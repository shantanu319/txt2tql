stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ Recovery 5 second interval;

create stream @APPNAME@_UserdataStream of Global.WAEvent;

create type @APPNAME@_Order_type(
id int,
order_id int,
zipcode int,
category String,
tablename string
);

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src1 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.order_%'
)
OUTPUT TO @APPNAME@_OrdersStream;

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src2 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_2',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.second_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream2;

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src3 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_3',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.third_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream3;

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src4 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_4',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.fourth_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream4;


Create CQ @APPNAME@_CQUser
insert into @APPNAME@_UserdataStream
select 
putuserdata (data,'Fileowner','FIRST_ORDER') from @APPNAME@_OrdersStream data;


Create CQ @APPNAME@_CQUser2
insert into @APPNAME@_UserdataStream
select 
putuserdata (data2,'Fileowner','SECOND_ORDER') from @APPNAME@_OrdersStream2 data2;


Create CQ @APPNAME@_CQUser3
insert into @APPNAME@_UserdataStream
select 
putuserdata (data3,'Fileowner','THIRD_ORDER') from @APPNAME@_OrdersStream3 data3;


Create CQ @APPNAME@_CQUser4
insert into @APPNAME@_UserdataStream
select 
putuserdata (data4,'Fileowner','FOURTH_ORDER') from @APPNAME@_OrdersStream4 data4;

create stream @APPNAME@_OrderTypedStream1 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream2 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream3 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream4 of @APPNAME@_Order_type;

CREATE CQ @APPNAME@_fin_cq
INSERT INTO @APPNAME@_OrderTypedStream1
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FIRST_ORDER';

CREATE CQ @APPNAME@_fin_cq2
INSERT INTO @APPNAME@_OrderTypedStream2
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'SECOND_ORDER';

CREATE CQ @APPNAME@_fin_cq3
INSERT INTO @APPNAME@_OrderTypedStream3
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'THIRD_ORDER';

CREATE CQ @APPNAME@_fin_cq4
INSERT INTO @APPNAME@_OrderTypedStream4
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FOURTH_ORDER';


create Target @APPNAME@_ADLSGen2_tgt1 using ADLSGen2Writer(
        filename:'event_data.csv',
        directory:'',
       	accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        uploadpolicy:'eventcount:8,interval:20s'
)
format using DSVFormatter (
    header:'true'
)
input from @APPNAME@_OrderTypedStream1; 

create Target @APPNAME@_ADLSGen2_tgt2 using ADLSGen2Writer(
        filename:'event_data.xml',
        directory:'',
		accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
   		uploadpolicy:'eventcount:8,interval:20s'
)
format using XMLFormatter (
  elementtuple: 'Order_id:id:order_id:zipcode:category:text=tablename',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from @APPNAME@_OrderTypedStream2; 

create Target @APPNAME@_ADLSGen2_tgt3 using ADLSGen2Writer(
        filename:'event_data.avro',
        directory:'',
		accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
  		uploadpolicy:'eventcount:8,interval:20s'
)
format using AvroFormatter (
  formatAs: 'Default',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA-FILE@'
)
input from @APPNAME@_OrderTypedStream3; 


create Target @APPNAME@_ADLSGen2_tgt4 using ADLSGen2Writer(
        filename:'event_data.json',
        directory:'',
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
  		uploadpolicy:'eventcount:8,interval:20s'
)
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@_OrderTypedStream4;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src1 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE SOURCE @APP_NAME@_src2 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE SOURCE @APP_NAME@_src3 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE SOURCE @APP_NAME@_src4 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE SOURCE @APP_NAME@_src5 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;


CREATE OR REPLACE TARGET @APP_NAME@_tgt1 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @APPNAME@_DBSource USING Global.OracleReader (
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  Compression: false,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Password_encrypted: 'false',
  SupportPDB: false,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  FetchSize: 1000,
  CDDLAction: 'Process',
  ConnectionURL: '192.168.56.3:1521:orcl',
  DictionaryMode: 'OnlineCatalog',
  QueueSize: 2048,
  CommittedTransactions: true,
  SetConservativeRange: false,
  CDDLCapture: false,
  Username: 'fan',
  Tables: 'FAN.S_BLOB',
  TransactionBufferType: 'Disk',
  Password: '9S5GnbGmBQNDD5c/baD0Tw==',
  TransactionBufferSpilloverSize: '100MB',
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  DatabaseRole: 'Primary' )
OUTPUT TO @APPNAME@_stream;

CREATE OR REPLACE TARGET @APPNAME@_target USING Global.BigQueryWriter (
  ColumnDelimiter: '|',
  projectId: 'striim-support',
  BatchPolicy: 'eventCount:1, Interval:1',
  NullMarker: 'NULL',
  streamingUpload: 'false',
  ServiceAccountKey: '/Users/fzhang/fan/u01/app/product/striim/striim_latest/UploadedFiles/striim-support-286429beb74d.json',
  Encoding: 'UTF-8',
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30',
  AllowQuotedNewLines: 'false',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  Tables: 'FAN.S_BLOB,Fan.s_blob columnmap(A=A,B=B,C=C,D=C,E=@metadata(OperationName));',
  TransportOptions: 'connectionTimeout=300, readTimeout=120',
  adapterName: 'BigQueryWriter',
  Mode: 'MERGE',
  StandardSQL: 'true',
  includeInsertId: 'true',
  QuoteCharacter: '\"' )
INPUT FROM @APPNAME@_stream;


END APPLICATION @APPNAME@;

--
-- Crash Recovery Test 1 on Four node all server cluster 
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP N4S4CR1Tester.N4S4CRTest1;
UNDEPLOY APPLICATION N4S4CR1Tester.N4S4CRTest1;
DROP APPLICATION N4S4CR1Tester.N4S4CRTest1 CASCADE;
CREATE APPLICATION N4S4CRTest1 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest1;

CREATE SOURCE CsvSourceN4S4CRTest1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest1;

CREATE FLOW DataProcessingN4S4CRTest1;

CREATE TYPE WactionTypeN4S4CRTest1 (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE WactionsN4S4CRTest1 CONTEXT OF WactionTypeN4S4CRTest1
EVENT TYPES ( WactionTypeN4S4CRTest1 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN4S4CRTest1
INSERT INTO WactionsN4S4CRTest1
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END FLOW DataProcessingN4S4CRTest1;

END APPLICATION N4S4CRTest1;

STOP APPLICATION oraddl;
UNDEPLOY APPLICATION oraddl;
DROP APPLICATION oraddl CASCADE;
CREATE APPLICATION oraddl recovery 5 second interval;
 
Create Source Ora Using OracleReader 
(
 Username:'@user-name@',
 Password:'@password@',
 ConnectionURL:'src_url',
 Tables:'QATEST.ORACLEDDL%',
 DictionaryMode:OfflineCatalog,
 DDLCaptureMode : 'All',
 FetchSize:1
) Output To LogminerStream;

Create Target tgt using DatabaseWriter 
(
 Username:'@username@',
 Password:'@password@',
 ConnectionURL:'TGT_URL',
 BatchPolicy:'EventCount:1,Interval:1',
 CommitPolicy:'EventCount:1,Interval:1',
 IgnorableExceptionCode: '1,2290,942',
 Tables :'QATEST.ORACLEDDL%,QATEST2.%'
) input from LogminerStream;

CREATE TARGET cdcDump USING LogWriter(
name:testOuput,
directory:'/Users/abinandan/product/IntegrationTests/target/test-classes/testNG/AllTargetWriters/OracleDDLDatabaseWriter/logs',
filename:'oraclecdc.log',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING JSONFormatter ()
INPUT FROM LogminerStream;
end application oraddl;
deploy application oraddl;
start application oraddl;

undeploy application dev15823;
alter application dev15823;

CREATE TYPE ModifyNotNull (
  x int,
  y string
);

CREATE STREAM FormattedStream OF ModifyNotNull;

CREATE  CQ InsertWactions
INSERT INTO FormattedStream
SELECT
    TO_INT(data[0]),
   	"notnull"
FROM LogminerStream;

Create or replace Target test using SysOut (name:test) input from FormattedStream;

CREATE OR REPLACE TARGET WriteCDCMySQL USING DatabaseWriter  ( 
  Username: '@USERNAME@',
  BatchPolicy: 'Eventcount:5,Interval:5',
  CommitPolicy: 'Eventcount:5,Interval:5',
  ConnectionURL: '@URL@',
  Tables: '@TABLES@',
  Checkpointtable: 'CHKPOINT',
  --IgnorableExceptionCode:'1062',
  Password: '@PASSWORD@'
 ) 
INPUT FROM FormattedStream;

END APPLICATION dev15823;
alter application dev15823 recompile;
deploy application dev15823;
start dev15823;

stop @APPNAME@;
undeploy application @APPNAME@;
--drop exceptionstore admin.Oracle12C_To_Oracle12CApp_ExceptionStore;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ use exceptionstore;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@',
    CommitPolicy: 'Interval:5'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;
create or replace cq @cq@
insert into @finalstream@
select exceptionType,action,appName,entityType,entityName,className,message,relatedActivity from @APPNAME@_ExceptionStore;

Create target @targetfile@ using filewriter (
filename:'@APPNAME@_file.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using jsonFormatter()
input from @finalstream@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop @APPNAME@_ExpStore;
undeploy application @APPNAME@_ExpStore;
drop application @APPNAME@_ExpStore cascade;
CREATE APPLICATION @APPNAME@_ExpStore;

CREATE TYPE @APPNAME@_ExpStore_CDCStreams_Type  (
  evtlist java.util.List  
 );

CREATE STREAM @APPNAME@_ExpStore_CDCStreams OF @APPNAME@_ExpStore_CDCStreams_Type;

CREATE CQ @APPNAME@_ReadFromExpStore 
INSERT INTO @APPNAME@_ExpStore_CDCStreams
select to_waevent(s.relatedObjects) as evtlist from admin.@APPNAME@_ExceptionStore [jumping 5 second] s;

CREATE STREAM @APPNAME@_ExpStore_CDCEventStream OF Global.WAEvent;

CREATE CQ @APPNAME@_ExpStore_GetCDCEvent 
INSERT INTO @APPNAME@_ExpStore_CDCEventStream
SELECT com.webaction.proc.events.WAEvent.makecopy(cdcevent) FROM @APPNAME@_ExpStore_CDCStreams a, iterator(a.evtlist) cdcevent;

CREATE CQ @APPNAME@_ExpStore_JoinDataCQ
INSERT INTO @APPNAME@_ExpStore_JoinedDataStream
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1])
        from @APPNAME@_ExpStore_CDCEventStream f;
        
CREATE OR REPLACE TARGET @APPNAME@_ExpStore_WriteToFileAsJSON USING FileWriter  ( 
  filename: 'expEvent_Oracle',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:1,interval:30',
  adapterName: 'FileWriter',
  directory: 'ExpStore_logs',
  rolloverpolicy: 'eventcount:6,interval:30s'
 ) 
FORMAT USING JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter',
  formatterName: 'JSONFormatter',
  jsonMemberDelimiter: '\n',
  jsonobjectdelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true'
 ) 
INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;
        
CREATE TARGET @APPNAME@_ExpStore_dbtarget USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1,Interval:1',
CommitPolicy:'Interval:1,Eventcount:1',
Tables:'@TargetTable@'
) INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;

END APPLICATION @APPNAME@_ExpStore;

deploy application @APPNAME@_ExpStore;
start @APPNAME@_ExpStore;

STOP APPLICATION @AppName@;
UNDEPLOY APPLICATION @AppName@;
DROP APPLICATION @AppName@ CASCADE;
CREATE APPLICATION @AppName@ recovery 1 second interval;
CREATE SOURCE @AppName@_Source USING FileReader (
	WildCard: 'posdata100.csv',
 directory: '@dir@',
	positionByEOF:false
	)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO @AppName@_Stream;

CREATE TYPE cdctypestream(
 id int,
 name String
);

create stream @StreamType@ of cdctypestream persist using Global.DefaultKafkaProperties;
CREATE OR REPLACE CQ CsvToPosData
INSERT INTO @StreamType@
SELECT
TO_INT(TO_STRING(data[0]).replaceAll("COMPANY ", "")),
data[1]
FROM @AppName@_Stream;
Deploy application @AppName@;
Start @AppName@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ recovery 5 second Interval;
CREATE SOURCE @srcName@ USING PostgreSQLReader  

(
  ReplicationSlotName:'@srcreplicationslot@',
  FilterTransactionBoundaries:'true',
  Username:'@srcusername@',
  Password_encrypted:false,
  ConnectionURL: '@srcurl@',
  adapterName:'PostgreSQLReader',
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  Password:'@srcpassword@',
  Tables:'@srcschema@.@srctable@'
) 
OUTPUT TO @outstreamname@ ;

CREATE OR REPLACE TARGET @tgtName@ USING DatabaseWriter  

(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'@tgtusername@',
  BatchPolicy:'EventCount:1,Interval:0',
  CommitPolicy:'EventCount:1,Interval:0',
  ConnectionURL:'@tgturl@',
  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
  Password:'@tgtpassword@'
) 
INPUT FROM @instreamname@;
End APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 1 second interval;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@'
 )
OUTPUT TO @APPNAME@Stream;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;


CREATE TYPE test_typeRecov 
(
node_new com.fasterxml.jackson.databind.JsonNode,
node_name com.fasterxml.jackson.databind.JsonNode,
node_addr com.fasterxml.jackson.databind.JsonNode
);

Create stream cqAsJSONNodeStreamRecov of test_typeRecov;

CREATE CQ GetPOAsJsonNodesRecov
INSERT into cqAsJSONNodeStreamRecov
select 
data.get('ACCTS-RECORD'),
data.get('ACCTS-RECORD').get('NAME'),
data.get('ACCTS-RECORD').get('ADDRESS3')
from @APPNAME@Stream js;

create type finaldtypeRecov
(ACCOUNT_NO int,
FIRST_NAME String,
LAST_NAME String,
ADDRESS1 String,
ADDRESS2 String,
CITY String,
STATE String,
ZIP_CODE int);

CREATE STREAM getdataStreamPSRecov OF finaldtypeRecov;

CREATE CQ getdataRecov
INSERT into getdataStreamPSRecov
select JSONGetInteger(x.node_new,"ACCOUNT-NO"),
JSONGetString(x.node_name,"FIRST-NAME"),
JSONGetString(x.node_name,"LAST-NAME"),
JSONGetString(x.node_new,"ADDRESS1"),
JSONGetString(x.node_new,"ADDRESS2"),
JSONGetString(x.node_addr,"CITY"),
JSONGetString(x.node_addr,"STATE"),
JSONGetInteger(x.node_addr,"ZIP-CODE")
from cqAsJSONNodeStreamRecov x;

create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:1000,Interval:50',
  CommitPolicy: 'EventCount:1000,Interval:50',
  Tables: 'QATEST.@table@'
)
input from getdataStreamPSRecov;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

--
-- Recovery Test 13 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same compound key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> CW(p#p) -> CQ -> WS
--

STOP Recov13Tester.RecovTest13;
UNDEPLOY APPLICATION Recov13Tester.RecovTest13;
DROP APPLICATION Recov13Tester.RecovTest13 CASCADE;
CREATE APPLICATION RecovTest13 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTest10Data.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  partKey String KEY,
  serialNumber int,
  partKey2 String KEY
);

CREATE TYPE WactionData (
  partKey String KEY,
  serialNumber int
);

CREATE STREAM DataStream OF CsvData PARTITION BY partKey, partKey2;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    TO_INT(data[1]),
    data[0]
FROM CsvStream;

CREATE JUMPING WINDOW DataStreamTwoItems
OVER DataStream KEEP 2 ROWS
PARTITION BY partKey, partKey2;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    first(partKey),
    to_int(first(serialNumber))
FROM DataStreamTwoItems
GROUP BY partKey, partKey2;

END APPLICATION RecovTest13;

CREATE OR REPLACE PROPERTYVARIABLE RetryPolicy='timeOut=00,retryInterval=1,maxRetries=3';
CREATE OR REPLACE PROPERTYVARIABLE KafkaBrokerAddress='localhost:9099';
CREATE OR REPLACE PROPERTYVARIABLE KafkaConfig='request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;';

STOP APPLICATION @Appname@;
UNDEPLOY APPLICATION @Appname@;
DROP APPLICATION @Appname@ CASCADE;
CREATE APPLICATION @Appname@ @Recovery@;
CREATE FLOW @Appname@AgentFlow;
--Partitioning source stream with meta (tablename)
Create Source @Appname@s1 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
 Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss1 partition by meta(@Appname@ss1,'TableName');
-- (id integer,name1 string,name2 string) partition by sleft(name1,1)
-- select TO_INT(data[0]),TO_STRING(data[1]),TO_STRING(data[2]);


--Partitioning source stream using meta (STARTSCN)
Create Source @Appname@s2 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss2 partition by meta(@Appname@ss2,'STARTSCN');



CREATE TYPE @Appname@OpTableDataType(
  TableName String,
  STARTSCN String,
  data java.util.HashMap
);

--Partitioning typed stream inline while creation using expr
CREATE STREAM @Appname@OracleTypedStream OF @Appname@OpTableDataType partition by sright(STARTSCN,2);
Create Source @Appname@s3 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss3 partition by meta(@Appname@ss3,'STARTSCN');

CREATE CQ @Appname@ParseOracleRawStream
  INSERT INTO @Appname@OracleTypedStream
  SELECT META(@Appname@ss3, 'TableName').toString(),META(@Appname@ss3, 'STARTSCN').toString(),
    DATA(@Appname@ss3)
  FROM @Appname@ss3;

--Partitioning source stream with types defined inline using meta expr
Create Source @Appname@s4 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss4 (id integer,NAME string,COL1 string)
-- partition by meta(ss4,'STARTSCN')
select TO_INT(data[0]),TO_STRING(data[2]),TO_STRING(data[3]);

CREATE CQ @Appname@cqss4
  INSERT INTO @Appname@ss4new Partition by sright(id,1)
  SELECT * FROM @Appname@ss4;

--Partitioning source stream -(waevent-s5) with field that has null value

CREATE STREAM @Appname@s5 of Global.WAEvent PARTITION BY meta(@Appname@s5, 'TransactionName');

Create Source @Appname@src5 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss5;

END FLOW @Appname@AgentFlow;


CREATE FLOW @Appname@ServerFlow;
--CREATE STREAM modifyStream OF Global.WAEvent partition by meta(modifyStream,'STARTSCN');
CREATE STREAM @Appname@modifyStream OF Global.WAEvent partition by sleft(data[0],1);
CREATE CQ @Appname@modifycq INSERT INTO @Appname@modifyStream
SELECT * FROM @Appname@ss5
MODIFY
(
data[0] = (TO_INT(data[0]) * 10000) / 100
);


--Partitioning source stream -(waevent) with modified fields
Create Source @Appname@s6 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss6;

CREATE CQ @Appname@putUserDatacq1
INSERT INTO @Appname@newss6
PARTITION BY userdata(@Appname@newss6, 'Modified_Date')
SELECT
putUserData(x,'Modified_Date',TO_STRING(DNOW(),'yyyy-MM-dd HH:mm:ss'))
FROM @Appname@ss6 x;

--Partitioning stream with userdata
CREATE STREAM @Appname@newss7 of Global.WAEvent PARTITION BY userdata(@Appname@newss7, 'Modified_Date');

Create Source @Appname@s7 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss7;

CREATE CQ @Appname@putUserDatacq2
INSERT INTO @Appname@newss7
SELECT
putUserData(x,'Modified_Date',TO_STRING(DNOW(),'yyyy-MM-dd HH:mm:ss'))
FROM @Appname@ss7 x;


create Target @Appname@KW1 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr01',
Mode:'Async',
KafkaConfig: '$KafkaConfig'
)
FORMAT USING jsonFormatter ()
input from @Appname@ss1;

create Target @Appname@KW2 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr02',
Mode:'Async',
KafkaConfig: '$KafkaConfig'
        )
FORMAT USING jsonFormatter ()
input from @Appname@ss2;

create Target @Appname@KW3 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr03',
Mode:'Async',
KafkaConfig: '$KafkaConfig'
        )
FORMAT USING jsonFormatter ()
input from @Appname@OracleTypedStream;

create Target @Appname@KW4 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr04',
Mode:'sync',
KafkaConfig: '$KafkaConfig'
        )
FORMAT USING jsonFormatter ()
input from @Appname@ss4new;

create Target @Appname@KW5 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr05',
Mode:'sync',
KafkaConfig: '$KafkaConfig'
        )
FORMAT USING jsonFormatter ()
input from @Appname@modifyStream;

create Target @Appname@KW6 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr06',
Mode:'sync',
KafkaConfig: '$KafkaConfig'
        )
FORMAT USING jsonFormatter ()
input from @Appname@newss6;

create Target @Appname@KW7 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr07',
Mode:'sync',
KafkaConfig: '$KafkaConfig'
        )
FORMAT USING jsonFormatter ()
input from @Appname@newss7;
END FLOW @Appname@ServerFlow;
end application @Appname@;
--deploy application @Appname@ with @Appname@AgentFlow in Agents, @Appname@ServerFlow in default;
deploy application @Appname@;
start @Appname@;

stop application @Appname@KR;
undeploy application @Appname@KR;
drop application @Appname@KR cascade;
create application @Appname@KR;
CREATE STREAM @Appname@KafkaStream of Global.jsonnodeEvent;
alter stream @Appname@KafkaStream PARTITION BY meta(@Appname@KafkaStream, 'PartitionID');
CREATE SOURCE @Appname@KafkaSource1 USING KafkaReader Version '2.1.0'
(
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr01',
startOffset:0
)
PARSE USING jsonParser ()
OUTPUT TO @Appname@KafkaStream;

create Target @Appname@t2 using filewriter(
	filename:'%n%',
	rolloverpolicy:'EventCount:5000000',
    directory:'FEATURE-DIR/logs/%@metadata(TopicName)%/%@metadata(PartitionID)%'
)
format using jsonFormatter (
)
input from @Appname@KafkaStream;

end application @Appname@KR;
deploy application @Appname@KR;
--start @Appname@KR;

STOP APPLICATION @Appname@FR;
UNDEPLOY APPLICATION @Appname@FR;
DROP APPLICATION @Appname@FR CASCADE;
CREATE APPLICATION @Appname@FR;
CREATE SOURCE @Appname@FS0 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS0;

CREATE SOURCE @Appname@FS1 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS1;

CREATE SOURCE @Appname@FS2 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS2;

CREATE SOURCE @Appname@FS3 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS3;

CREATE SOURCE @Appname@FS4 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS4;

CREATE SOURCE @Appname@FS5 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS5;

CREATE SOURCE @Appname@FS6 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS6;
end application @Appname@FR;
deploy application @Appname@FR;

--
-- Recovery Test 27 with two sources, two jumping time windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jt1W -> CQ1 -> WS
--   S2 -> Jt2W -> CQ2 -> WS
--

STOP Recov27Tester.RecovTest27;
UNDEPLOY APPLICATION Recov27Tester.RecovTest27;
DROP APPLICATION Recov27Tester.RecovTest27 CASCADE;
CREATE APPLICATION RecovTest27 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream1Second
OVER DataStream1 KEEP WITHIN 1 SECOND;

CREATE JUMPING WINDOW DataStream2Second
OVER DataStream2 KEEP WITHIN 2 SECOND;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data1ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream1Second p;

CREATE CQ Data2ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream2Second p;

END APPLICATION RecovTest27;

CREATE OR REPLACE EMBEDDINGGENERATOR @EMB_NAME@ USING @MODEL@ (
modelProvider: '@MODEL@',
modelName: '@MODEL_NAME@',
apiKey: '@API_KEY@'
);

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
alter APPLICATION @APPNAME@;
CREATE OR REPLACE SOURCE @APPNAME@src USING Global.FileReader 
(
  adapterName:'FileReader',
  rolloverstyle:'Default',
  blocksize:64,
  networkfilesystem:true,
  wildcard:'client_xref*',
  compressiontype:'gzip',
  includesubdirectories:false,
  directory:'@APPNAME@',
  skipbom:false,
  positionbyeof:false
)
PARSE USING Global.DSVParser 
(
  trimwhitespace:false,
  linenumber: '-1',
  columndelimittill: '-1',
  trimquote: true,
  ignoreemptycolumn: false,
  separator: ':',
  parserName: 'DSVParser',
  quoteset: '\"',
  handler:'com.webaction.proc.DSVParser_1_0',
  charset:'UTF-8',
  ignoremultiplerecordbegin:'true',
  ignorerowdelimiterinquote:false,
  columndelimiter:'|',
  blockascompleterecord:false,
  rowdelimiter:'\n',
  nocolumndelimiter:false,
  headerlineno:0,
  header:'true'
)                  
OUTPUT TO @APPNAME@STREAM;
ALTER APPLICATION @APPNAME@ RECOMPILE;
deploy application @APPNAME@;
start @APPNAME@;

Create Target @TARGET_NAME@ using HiveWriter
(
  ConnectionURL:'jdbc:hive2://dockerhost:10000/default',
  Username:'cloudera',
  Password:'cloudera',
  hadoopurl:'hdfs://dockerhost:9000',
  Mode:'initialload',
  mergepolicy:'eventcount:5,interval:1s',
  Tables:'QATEST.HIVE_EMP,default.hive_emp KEYCOLUMNS(id)',
  hadoopConfigurationPath:'/home/ubuntu/Product/IntegrationTests/TestData/hdfsconf/'
)
INPUT FROM @STREAM@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE TYPE @appname@CQOUT1_Type (
 companyName java.lang.String,
 merchantId java.lang.String,
 dateTime org.joda.time.DateTime,
 hourValue java.lang.String,
 amount java.lang.String,
 zip java.lang.String,
 FileName java.lang.String);

CREATE SOURCE @parquetsrc@ USING S3Reader (
    bucketname:'',
    objectnameprefix:'',
    foldername:''
     )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE OR REPLACE STREAM @appname@CQOUT1 OF @appname@CQOUT1_Type;
CREATE OR REPLACE CQ @appname@CQ_PQEvent
INSERT INTO @appname@CQOUT1
    Select
    data.get("companyName").toString(),
    data.get("merchantId").toString(),
    TO_DATE(data.get("dateTime").toString()),
    data.get("hourValue").toString(),
    data.get("amount").toString(),
    data.get("zip").toString(),
    metadata.get("FileName").toString()
    FROM @appname@Stream p;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using DSVFormatter ()
input from @appname@CQOUT1;

END APPLICATION @appname@;
deploy application @appname@ on @node@ in default;
start application @appname@;

CREATE TARGET @TARGET_NAME@ USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest',
	batchPolicy: 'EventCount: 10000,Interval:600s'
) INPUT FROM @STREAM@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;
CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'TargetPosDataXmlEC',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'eventcount:2000,sequence:00'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:zip:text=merchantname'
)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlEC_actual.log') input from TypedCSVStream;

end application DSV;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src1 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: '@ORACLE-URL@',
  Tables: '@SOURCE-TABLES@',
  Username: '@ORACLE-USERNAME@',
  Password: '@ORACLE-PASSWORD@'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt1 USING SnowflakeWriter

(
  ConnectionURL:'@SNOWFLAKE-URL@',
  username:'@SNOWFLAKE-USERNAME@',
  appendOnly:'false',
  Tables:'@TARGET-TABLES@',
  uploadpolicy:'eventcount:3,interval:10s',
  externalStageType:'local'
)
INPUT FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt2 USING SnowflakeWriter

(
  ConnectionURL:'@SNOWFLAKE-URL@',
  username:'@SNOWFLAKE-USERNAME@',
  appendOnly:'false',
  Tables:'@TARGET-TABLES@',
  uploadpolicy:'eventcount:3,interval:10s',
  externalStageType:'local'
)
INPUT FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt3 USING SnowflakeWriter

(
  ConnectionURL:'@SNOWFLAKE-URL@',
  username:'@SNOWFLAKE-USERNAME@',
  appendOnly:'false',
  Tables:'@TARGET-TABLES@',
  uploadpolicy:'eventcount:3,interval:10s',
  externalStageType:'local'
)
INPUT FROM @APP_NAME@_Stream;


END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

CREATE APPLICATION @AppName@;

CREATE OR REPLACE SOURCE ReadHTTPPort USING Global.HTTPReader  ( 
deferresponsetimeout: '10000ms', 
  keystore: '@dataFile@server.keystore.jks', 
  deferresponse: true, 
  keystorepassword: 'w@ct10n', 
  ipaddress: 'localhost', 
  threadcount: 10, 
  keystorepassword_encrypted: 'false', 
  adapterName: 'HTTPReader', 
  keystoretype: 'JKS', 
  portno: '@port@', 
  authenticateclient: true ) 
OUTPUT TO HTTPReaderOUT  ;

CREATE OR REPLACE TARGET sysout USING Global.SysOut  ( 
name: 'sysout' ) 
INPUT FROM HTTPReaderOUT;

END APPLICATION @AppName@;
deploy application @AppName@;
Start @AppName@;

stop APPLICATION RollOverTester.DSV;

UNDEPLOY APPLICATION RollOverTester.DSV;
DROP APPLICATION RollOverTester.DSV CASCADE;
CREATE APPLICATION DSV;

create source JsonSource using FileReader (
        directory:'@TEST-DATA-PATH@',
        WildCard:'TextInput.txt',
        positionByEOF:false,
        charset:'UTF-8'
)
 PARSE USING JSONParser ( 
  eventType: ''
 ) 
OUTPUT TO activeguard_rawstream;


CREATE OR REPLACE TYPE activeguard_parsed (
	-- Enriched, or not in AGI left navigation
	hash_key String KEY,
	client_id Long,
	event_type_id Long,
	type_code String,
	rule_id Long,
	rule_name String,
	rule_action String,
	sce_id  Long,
	message String,
	ref_list_match String,
	event_date DateTime,
	log_count Long,
	
	-- As seen in AGI
	-- frequently used, but not duplicated
	received_date DateTime,
	event_sub_source String,
					
	-- database
	sol_action String,
	comment String,
	comment_text String,
	database_name String,
	database_user_name String,
	entry_id String,
	obj_name String,
	os_user_name String,
	owner String,
	priv_used String,
	return_code String,
	session_id String,
	
	-- geo
	destination_country_code String,
	source_country_code String,
	
	-- network
	data_size String,
	destination String,
	destination_object String,
	destination_port String,
	sol_domain String,
	event_manager String,
	event_path String,
	event_source String,
	msg_no String,
	protocol_id String,
	service String,
	sol_source String,
	source_mac_address String,
	source_object String,
	source_port String,
	status_code String,
	target_path String,
	
	-- other
	command String,
	
	-- security
	affected_file String,
	object_action String,
	object_action_result String,
	object_id String,
	object_name String,
	object_path String,
	process_name String,
	
	-- user
	caller_user_name String,
	logon_type String,
	target_user_name String,
	user_agent String,
	user_name String,
	workstation String,
	
	-- web
	web_file String,
	web_method String,
	web_page String,
	web_protocol String
);


CREATE OR REPLACE STREAM activeguard_stream OF activeguard_parsed;

CREATE CQ Parseactiveguard_stream 
INSERT INTO activeguard_stream
SELECT

	-- Enriched, or not in AGI left navigation
	data.get('hash_key').textValue() as hash_key,
	data.get('client_id').longValue() as client_id,
	data.get('event_type_id').longValue() as event_type_id,
	data.get('type_code').textValue() as type_code,
	data.get('rule_id').longValue() as rule_id,
	data.get('rule_name').textValue() as rule_name,
	data.get('rule_action').textValue() as rule_action,
	data.get('sce_id').longValue() as sce_id,
	data.get('message').textValue() as message,
	data.get('ref_list_match').textValue() as ref_list_match,
	TO_DATE(data.get('event_date').textValue()) as event_date,
	data.get('log_count').longValue() as log_count,
	
	-- As seen in AGI
	-- frequently used, but not duplicated
	TO_DATE(data.get('received_date').textValue()) as received_date,
	data.get('event_sub_source').textValue() as event_sub_source,
					
	-- database
	data.get('sol_action').textValue() as sol_action,
	data.get('comment').textValue() as comment,
	data.get('comment_text').textValue() as comment_text,
	data.get('database_name').textValue() as database_name,
	data.get('database_user_name').textValue() as database_user_name,
	data.get('entry_id').textValue() as entry_id,
	data.get('obj_name').textValue() as obj_name,
	data.get('os_user_name').textValue() as os_user_name,
	data.get('owner').textValue() as owner,
	data.get('priv_used').textValue() as priv_used,
	data.get('return_code').textValue() as return_code,
	data.get('session_id').textValue() as session_id,
	
	-- geo
	data.get('destination_country_code').textValue() as destination_country_code,
	data.get('source_country_code').textValue() as source_country_code,
	
	-- network
	data.get('data_size').textValue() as data_size,
	data.get('destination').textValue() as destination,
	data.get('destination_object').textValue() as destination_object,
	data.get('destination_port').textValue() as destination_port,
	data.get('sol_domain').textValue() as sol_domain,
	data.get('event_manager').textValue() as event_manager,
	data.get('event_path').textValue() as event_path,
	data.get('event_source').textValue() as event_source,
	data.get('msg_no').textValue() as msg_no,
	data.get('protocol_id').textValue() as protocol_id,
	data.get('service').textValue() as service,
	data.get('sol_source').textValue() as sol_source,
	data.get('source_mac_address').textValue() as source_mac_address,
	data.get('source_object').textValue() as source_object,
	data.get('source_port').textValue() as source_port,
	data.get('status_code').textValue() as status_code,
	data.get('target_path').textValue() as target_path,
	
	-- other
	data.get('command').textValue() as command,
	
	-- security
	data.get('affected_file').textValue() as affected_file,
	data.get('object_action').textValue() as object_action,
	data.get('object_action_result').textValue() as object_action_result,
	data.get('object_id').textValue() as object_id,
	data.get('object_name').textValue() as object_name,
	data.get('object_path').textValue() as object_path,
	data.get('process_name').textValue() as process_name,
	
	-- user
	data.get('caller_user_name').textValue() as caller_user_name,
	data.get('logon_type').textValue() as logon_type,
	data.get('target_user_name').textValue() as target_user_name,
	data.get('user_agent').textValue() as user_agent,
	data.get('user_name').textValue() as user_name,
	data.get('workstation').textValue() as workstation,
	
	-- web
	data.get('web_file').textValue() as web_file,
	data.get('web_method').textValue() as web_method,
	data.get('web_page').textValue() as web_page,
	data.get('web_protocol').textValue() as web_protocol
	
FROM activeguard_rawstream;


CREATE TARGET alertf USING FileWriter ( directory: '@FEATURE-DIR@/logs', filename: 'TestOutput.txt', rolloverpolicy:'EventCount:10000,Interval:30s' )
 FORMAT USING JSONFormatter (
		members:'hash_key,client_id,event_type_id,type_code,rule_id,rule_name,rule_action,sce_id,message,ref_list_match,event_date,log_count,received_date,event_sub_source,sol_action,comment,comment_text,database_name,database_user_name,entry_id,obj_name,os_user_name,owner,priv_used,return_code,session_id,destination_country_code,source_country_code,data_size,destination,destination_object,destination_port,sol_domain,event_manager,event_path,event_source,msg_no,protocol_id,service,sol_source,source_mac_address,source_object,source_port,status_code,target_path,command,affected_file,object_action,object_action_result,object_id,object_name,object_path,process_name,caller_user_name,logon_type,target_user_name,user_agent,user_name,workstation,web_file,web_method,web_page,web_protocol',
 		rowdelimiter:'\n'
 )
--format using dsvformatter()
 INPUT FROM activeguard_stream;

 
END APPLICATION DSV;

/* *******************************************************************
 Ibasis App for monitoring network with pings to different locations 
****************************************************************** */

use global;
drop namespace PingMonitorBatchApp cascade;
create namespace PingMonitorBatchApp;
use PingMonitorBatchApp;

CREATE APPLICATION PingMonitorBatchApp;

create	flow PingMonitorBatchAppFlow;	

CREATE TYPE PingDateEntry(
    MaxTestDateTime  DateTime KEY
 );


CREATE TYPE PINGEVENT(
        SERVICE_NAME  String,
        TEST_DATETIME DateTime,
	SOURCE_VERIFIER_ALIAS String,
	TARGET_VERIFIER_ALIAS  String,
        DELAY_AVG float,
	PACKET_LOST float,
        JITTER_AVG float,
        SEQ_NO     long
);


CREATE CACHE PINGEVENTCACHE  using DatabaseReader (
   ConnectionURL:'jdbc:oracle:thin:@server1204v.ivanet.net:1960:prtld',
   Username:'xtract',
   Password:'xtract123',
   Query:'select SERVICE_NAME,TEST_DATETIME,SOURCE_VERIFIER_ALIAS,TARGET_VERIFIER_ALIAS,NVL(DELAY_AVG,0),NVL(PACKET_LOST,0),NVL(JITTER_AVG,0),SEQ_NO  from ir34_ping_site where TEST_DATETIME > (select NVL(to_date(\'1970-01-01\', \'YYYY-MM-DD\') + (MAX(MaxTestDateTime)/ 86400000), sysdate -10/24) from IR34SiteLastDate)order by TEST_DATETIME ASC'
 ) QUERY (keytomap:'SOURCE_VERIFIER_ALIAS', refreshinterval: '60 second', publishonrefresh: 'true') OF PINGEVENT;


create Target trace0 using CSVWriter (filename:'logs/PINGEVENTCACHE.csv') input from PINGEVENTCACHE;

CREATE WACTIONSTORE IR34ElasticSearchWS  
CONTEXT OF   PingDateEntry 
EVENT TYPES ( PingDateEntry  )
PERSIST IMMEDIATE
USING ( storageProvider: "elasticsearch" );


CREATE WACTIONSTORE IR34SiteLastDateWS CONTEXT OF PingDateEntry
 		EVENT TYPES ( PingDateEntry )
 	    PERSIST EVERY 50 second USING (
 	    JDBC_DRIVER:'oracle.jdbc.driver.OracleDriver',
 	    JDBC_URL:'jdbc:oracle:thin:@server1204v.ivanet.net:1960:prtld',
 	    JDBC_USER:'xtract',
 	    JDBC_PASSWORD:'xtract123',
 	    DDL_GENERATION:'create-or-extend-tables',
            CONTEXT_TABLE:'IR34SiteLastDate'
 	   );	
	   

CREATE CQ populateMaxDate
 INSERT INTO IR34SiteLastDateWS
 select TO_DATE(MAX(TO_LONG(TEST_DATETIME)))
 from  PINGEVENTCACHE
 , heartbeat(interval '20' second, 1 , 1) hb;


 
/* create Target trace1 using CSVWriter (filename:'logs/IR34SiteLastDateWS.csv') input from IR34SiteLastDateWS; */


CREATE TYPE DELAYTHRESHTYPE(
        SITE_KEY  STRING,
        SOURCE_SITE STRING,
        TARGET_SITE STRING,
        DELAY_THRESH FLOAT,
        PROFILE_ID   INT
);


CREATE CACHE DELAYTHRESHCACHE  USING DATABASEREADER (
   ConnectionURL:'jdbc:oracle:thin:@timstsnap05.ivanet.net:1521:timsrdg2',
   Username:'network_cfg',
   Password:'network_cfg',
   QUERY: 'select src.site_abbreviation || \'-\' || tgt.site_abbreviation as SITE_KEY, src.site_abbreviation as SOURCE_SITE, tgt.site_abbreviation as TARGET_SITE,D.DELAY as DELAY_THRESH, d.PROFILE_ID from site_attributes src, geography gsrc, threshold_delay d, site_attributes tgt, geography gtgt where  Src.GEO_ABBREVIATION = Gsrc.GEO_ABBREVIATION  and src.GEO_ABBREVIATION = D.SOURCE_VERIFIER_GEO  and tgt.GEO_ABBREVIATION = D.TARGET_VERIFIER_GEO  and tgt.GEO_ABBREVIATION = Gtgt.GEO_ABBREVIATION  and tgt.PROFILE_ID = D.PROFILE_ID  and d.profile_id = 1'
 ) QUERY (KEYTOMAP:'SITE_KEY', REFRESHINTERVAL: '60 SECOND', publishonrefresh: 'true') OF DELAYTHRESHTYPE;


CREATE TARGET TRACE2 USING CSVWRITER (FILENAME:'logs/DELAYTHRESHCACHE.CSV') INPUT FROM DELAYTHRESHCACHE;


CREATE TYPE ALERTENTRY(
	ALERTKEY STRING KEY,
 	TOLOC STRING,
 	FROMLOC STRING,	
        STATUS STRING,
	INTSTATUS INT,
	ACTUAL_DELAY INT,
	DELAY_THRESHOLD INT,
        TESTTIME DATETIME
 );

CREATE STREAM PingStreamRaw OF PINGEVENT;

CREATE CQ PROCESSSortCQ
 INSERT INTO PingStreamRaw
 select  SERVICE_NAME  ,
        TEST_DATETIME ,
        SOURCE_VERIFIER_ALIAS ,
        TARGET_VERIFIER_ALIAS  ,
        DELAY_AVG ,
        PACKET_LOST ,
        JITTER_AVG ,
        SEQ_NO     
from PINGEVENTCACHE,  heartbeat(interval '30' second) hb;

CREATE SORTER PingSorterStream OVER
PingStreamRaw ON TEST_DATETIME OUTPUT TO SortedPingStream
WITHIN 70 second  
OUTPUT ERRORS TO fooErrorStream;


 CREATE STREAM ALERTENTRYSTREAM OF ALERTENTRY;

 CREATE CQ PROCESSCQ1
 INSERT INTO ALERTENTRYSTREAM
 SELECT DW.SOURCE_VERIFIER_ALIAS + " - " + DW.TARGET_VERIFIER_ALIAS,
        DW.TARGET_VERIFIER_ALIAS, 
        DW.SOURCE_VERIFIER_ALIAS,         
        CASE WHEN DW.DELAY_AVG > DT.DELAY_THRESH THEN "WARNING" 
             ELSE "NORMAL" 
        END,
        CASE WHEN DW.DELAY_AVG > DT.DELAY_THRESH  THEN 2
             ELSE 1
        END,
        DW.DELAY_AVG,
        DT.DELAY_THRESH,
        DW.TEST_DATETIME
 FROM SortedPingStream DW  , DELAYTHRESHCACHE DT 
 WHERE DT.SOURCE_SITE = DW.SOURCE_VERIFIER_ALIAS
   AND DT.TARGET_SITE = DW.TARGET_VERIFIER_ALIAS;

CREATE TARGET TRACE4 USING CSVWRITER (FILENAME:'logs/ALERTENTRIES.CSV') INPUT FROM ALERTENTRYSTREAM;

CREATE WINDOW DELAYWARNWINDOW OVER  ALERTENTRYSTREAM KEEP WITHIN 20 minute ON TESTTIME;

CREATE TYPE ALERTAGGENTRY(
        ALERTKEY STRING KEY,
        ALERT_STATUS STRING,
        NUMBER_OF_VIOLATIONS INT,
        DELAY_THRESHOLD INT,
        FIRSTTIME DATETIME
 );



CREATE STREAM ALERTAGGSTREAM OF ALERTAGGENTRY;


CREATE CQ PROCAGGCQ1 
 	INSERT INTO ALERTAGGSTREAM
		SELECT  ALERTKEY,
                        CASE WHEN COUNT(*) > 5 THEN 'CRITICAL'
                             WHEN COUNT(*) >= 3 AND COUNT(*) <= 5 THEN 'MAJOR'
                             WHEN COUNT(*) >=1 AND COUNT(*) < 3 THEN 'MINOR'
                        ELSE 'NORMAL'
                        END ,
                        COUNT(*),
                        DELAY_THRESHOLD,
                        FIRST(TESTTIME)
		FROM DELAYWARNWINDOW   
                WHERE STATUS = 'WARNING'
		GROUP BY ALERTKEY, STATUS,DELAY_THRESHOLD;


-- new waction stores

CREATE WACTIONSTORE PING_DETAILS
  CONTEXT OF ALERTENTRY
  EVENT TYPES (ALERTENTRY )
  PERSIST EVERY 60 second USING (
  JDBC_DRIVER:'oracle.jdbc.driver.OracleDriver',
  JDBC_URL:'jdbc:oracle:thin:@timstsnap05.ivanet.net:1521:timsrdg2',
  JDBC_USER:'network_cfg', JDBC_PASSWORD:'network_cfg',
  DDL_GENERATION:'create-or-extend-tables',
--  DDL_GENERATION:'drop-and-create-tables',
  CONTEXT_TABLE:'PING_DETAILS'
  );

CREATE CQ STOREDETAILSCQ
       INSERT INTO PING_DETAILS
              SELECT * FROM DELAYWARNWINDOW
              WHERE STATUS <> 'NORMAL';

CREATE WACTIONSTORE PingStore CONTEXT OF ALERTAGGENTRY
        EVENT TYPES( ALERTAGGENTRY )
        PERSIST NONE USING ( ) ;

CREATE CQ LoadPingStoreCQ
                INSERT INTO PingStore
                SELECT *
                FROM ALERTAGGSTREAM s;

-- end of stores 



CREATE STREAM AlertStream OF Global.AlertEvent;


CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT 'FROM TO SITES',      
       ALERTKEY, 
CASE
WHEN ALERT_STATUS != 'NORMAL' THEN 'warning'
ELSE 'info' END,
CASE
WHEN  (DNOW()  > DADD(FIRSTTIME, DMINS(20)) OR ALERT_STATUS = 'NORMAL')   THEN 'cancel'
ELSE 'raise' END,
CASE
WHEN ALERT_STATUS  != 'NORMAL' THEN ALERT_STATUS + ' ALERT FOR ' + ALERTKEY + ' has delay of over threshold ' + TO_STRING(DELAY_THRESHOLD) + ' has ' + TO_STRING(NUMBER_OF_VIOLATIONS) + '  violations over limit over the hour ' + TO_STRING(FIRSTTIME,'MM/dd/yyyy HH:mm:ss')  ELSE ''
END
    FROM ALERTAGGSTREAM 
WHERE NUMBER_OF_VIOLATIONS > 1;
--    FROM RESULTSSTORE; 


CREATE TARGET TRACE3 USING CSVWRITER (FILENAME:'logs/ALERTS.CSV') INPUT FROM AlertStream;

CREATE SUBSCRIPTION PingAppEmailAlert
USING EmailAdapter (
smtpurl:'smtp.nyc.ibasis.net:25',
smtp_auth:'false',
starttls_enable:'false',
subject:"IR34 Alert from Webaction", emailList:"pgopal@ibasis.net,ed@webaction.com",senderEmail:"webactionsupport@ibasis.net"
)
INPUT FROM AlertStream;

CREATE SUBSCRIPTION PosAppWebAlert
USING WebAlertAdapter( )
INPUT FROM AlertStream;


END flow PingMonitorBatchAppFlow;	

/*	
CREATE TYPE AlarmThreshold(
	profile int,
	threshholdValue int
);

CREATE CACHE AlarmThresholdCache using DatabaseReader (
   ConnectionURL:'jdbc:oracle:thin:@10.1.110.128:1521:orcl',
   Username:'scott',
   Password:'tiger',
   Query:'select * from scott.ALARM_THRESHOLD'
 ) QUERY (keytomap:'profile', refreshinterval: '60000000') OF AlarmThreshold; 
 

CREATE TYPE pingEvent(
	toLoc String,
	fromLoc String,	
        delay float,
        jitter float,
	packloss float,
        dateTime DateTime
);

CREATE STREAM dataStream OF pingEvent;

 Create Target trace1 Using Sysout (name:'rawping') input From LCRStream;

 
 CREATE CQ renderEvents
 INSERT INTO dataStream
 SELECT
 	x.TOLOCATION, x.FROMLOCACTION,  TO_FLOAT(x.DELAYAMT), TO_FLOAT(x.JITTERAMT), TO_FLOAT(x.PACKLOSSAMT), DNOW()
 FROM LCRStream1 x;
 
CREATE  jumping WINDOW  dataWindow OVER dataStream KEEP WITHIN 30 second; 
 
 CREATE TYPE AlertEntry(
	alertKey String KEY,
 	toLoc String,
 	fromLoc String,	
    status String,
	intStatus int,
    dateTime DateTime
 );

 CREATE STREAM AlertEntryStream OF AlertEntry;

 CREATE CQ processCQ1
 INSERT INTO AlertEntryStream
 select dw.toLoc + dw.fromLoc,
        dw.toLoc, 
        dw.fromLoc,         
        CASE WHEN dw.delay > c.threshholdValue THEN "WARNING" 
             ELSE "NORMAL" 
        END,
        CASE WHEN dw.delay > c.threshholdValue THEN 2
             ELSE 1
        END,
        dw.dateTime
 from dataWindow dw, AlarmThresholdCache c;
	  	 
 Create Target trace2 Using Sysout (name:'alert') input From AlertEntryStream;
*/ 
 
END APPLICATION PingMonitorBatchApp;
 -- create dashboard using "ClientApps/Overstock/ProdIndexDash.json";

--
-- Crash Recovery Test 5 with Jumping window and partitioned on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION KStreamN2S2CR5Tester.KStreamN2S2CRTest5;
UNDEPLOY APPLICATION KStreamN2S2CR5Tester.KStreamN2S2CRTest5;
DROP APPLICATION KStreamN2S2CR5Tester.KStreamN2S2CRTest5 CASCADE;

DROP USER KStreamN2S2CR5Tester;
DROP NAMESPACE KStreamN2S2CR5Tester CASCADE;
CREATE USER KStreamN2S2CR5Tester IDENTIFIED BY KStreamN2S2CR5Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR5Tester;
CONNECT KStreamN2S2CR5Tester KStreamN2S2CR5Tester;

CREATE APPLICATION KStreamN2S2CRTest5 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest5;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest5 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest5;

CREATE FLOW DataProcessingKStreamN2S2CRTest5;

CREATE TYPE CsvDataTypeKStreamN2S2CRTest5 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvDataTypeKStreamN2S2CRTest5 PARTITION BY merchantId;

CREATE CQ CsvToDataKStreamN2S2CRTest5
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest5 CONTEXT OF CsvDataTypeKStreamN2S2CRTest5
EVENT TYPES ( CsvDataTypeKStreamN2S2CRTest5 )
@PERSIST-TYPE@

CREATE CQ DataToWactionKStreamN2S2CRTest5
INSERT INTO WactionsKStreamN2S2CRTest5
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingKStreamN2S2CRTest5;

END APPLICATION KStreamN2S2CRTest5;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE APPLICATION @APPNAME@ RECOVERY 10 SECOND INTERVAL;

 create flow Mysqlflow;
CREATE SOURCE MysqlToDBRoutersource USING MysqlReader
(
  Username: '',
  Password: '',
  Tables: '',
  ConnectionURL: '',
  Password_encrypted: 'false',
  connectionRetryPolicy: 'retryInterval=30, maxRetries=3'
)
OUTPUT TO RouterTestMasterStream;

end flow Mysqlflow;

CREATE OR REPLACE ROUTER RouterTestRs1 INPUT FROM RouterTestMasterStream s CASE
WHEN meta(s,"TableName").toString()='waction.source1' THEN ROUTE TO RouterTestTyped1,
WHEN meta(s,"TableName").toString()='waction.source2' THEN ROUTE TO RouterTestTyped2,
ELSE ROUTE TO RouterTestTypedElse;

CREATE TARGET MysqlToDBRoutertarget1 USING DatabaseWriter(
   Username: '',
   Password: '',
   Tables: '',
   ConnectionURL: '',
   Password_encrypted: 'false',
   connectionRetryPolicy: 'retryInterval=30, maxRetries=3'
) INPUT FROM RouterTestTyped1;


CREATE TARGET MysqlToDBRoutertarget2 USING DatabaseWriter(
    Username: '',
    Password: '',
    Tables: '',
    ConnectionURL: '',
    Password_encrypted: 'false',
    connectionRetryPolicy: 'retryInterval=30, maxRetries=3'
) INPUT FROM RouterTestTyped2;


end application @APPNAME@;
deploy application @APPNAME@ with Mysqlflow in AGENTs;
start application @APPNAME@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream1@RANDOM@ OF Global.waevent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc1 USING JMSReader (
  ProviderName: '',
  UserName: '',
  transactionpolicy: '',
  Provider: '',
  Ctx: '',
  EnableTransaction: '',
  QueueName: '',
  Topic:'',
  Password: '' )
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@PersistStream1@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt1 USING Global.FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING Global.DSVFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream1@RANDOM@;

CREATE STREAM @APPNAME@PersistStream2@RANDOM@ OF Global.JSONNodeEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc2 USING JMSReader (
  ProviderName: '',
  UserName: '',
  transactionpolicy: '',
  Provider: '',
  Ctx: '',
  EnableTransaction: '',
  QueueName: '',
  Topic:'',
  Password: '' )
PARSE USING JSONParser ()
OUTPUT TO @APPNAME@PersistStream2@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt2 USING Global.FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING Global.JSONFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream2@RANDOM@;

CREATE STREAM @APPNAME@PersistStream3@RANDOM@ OF Global.waevent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc3 USING JMSReader (
  ProviderName: '',
  UserName: '',
  transactionpolicy: '',
  Provider: '',
  Ctx: '',
  EnableTransaction: '',
  QueueName: '',
  Topic:'',
  Password: '' )
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@PersistStream3@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt3 USING Global.FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: ''  )
FORMAT USING Global.JSONFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream3@RANDOM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

create application TestApp;
create source CSVSource using FileReader (
	directory:'Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ TestCQ
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
	filename:'posdata_JSON',
	rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'
)
format using JSONFormatter (
	members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)

input from TypedCSVStream;
end application TestApp;

deploy application TestApp;
start application TestApp;

CREATE OR REPLACE APPLICATION @AppName@ USE EXCEPTIONSTORE TTL : '7d';

Create Source @AppName@_source Using OracleReader(
  Username:'@username@',
  Password:'@password@',
  ConnectionURL:'@url@',
  Tables:'@srctableName@',
  Fetchsize:1
)
Output To @AppName@_Stream;
CREATE OR REPLACE TARGET @AppName@_Target USING Global.SnowflakeWriter (
  streamingUpload: 'false',
  useConnectionProfile:'true',
  connectionProfileName: '@CP@',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  columnDelimiter: '|',
  tables: '@srctableName@,@trgtableName@',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:1000,interval:60s',
  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',
  externalStageType: 'Local',
  adapterName: 'SnowflakeWriter',
  fileFormatOptions: 'null_if = \"\"' )
INPUT FROM @AppName@_Stream;
END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

stop application app1PS;
undeploy application app1PS;
drop application app1PS cascade;

create application app1PS;

create target File_TargerPS using FileWriter
(
directory : '',
filename : ''
)
format using DSVFormatter()
input from Recoveryss1;

end application app1PS;

deploy application app1PS;
start application app1PS;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

CREATE SOURCE @SourceName@ USING MSSqlReader  ( 
TransactionSupport: false, 
  FetchTransactionMetadata: false, 
  autodisabletablecdc: true,
  Compression: false, 
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', 
  Password_encrypted: 'false', 
  ConnectionURL: '@ConnectionURL@', 
  Fetchsize: 0, 
  ConnectionPoolSize: 10, 
  StartPosition: 'EOF', 
  DatabaseName: 'qatest', 
  Username: '@UN@', 
  cdcRoleName: 'STRIIM_READER', 
  Password: '@PWD@', 
  Tables: '@sourcetable@', 
  FilterTransactionBoundaries: true, 
  SendBeforeImage: true, 
  ExcludedTables: 'qatest.CHKPOINT',
  AutoDisableTableCDC: false ) 
OUTPUT TO @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;


CREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'))) FROM @SRCINPUTSTREAM@ x; ;

CREATE TARGET @targetName@ USING DatabaseWriter  ( 
ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  ParallelThreads: '', 
  CheckPointTable: 'CHKPOINT', 
  Password_encrypted: 'false', 
  ConnectionURL: '@ConnectionURL@', 
  CommitPolicy: 'EventCount:1,Interval:60', 
  StatementCacheSize: '50', 
  DatabaseProviderType: 'Default', 
  Username: '@UN@', 
  Password: '@PWD@', 
  PreserveSourceTransactionBoundary: 'false', 
  BatchPolicy: 'EventCount:1,Interval:60', 
  Tables: '@TablemappingwithColmap@' ) 
INPUT FROM admin.sqlreader_cq_out;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

CREATE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src1 USING Global.GCSReader (
  _h_GCSQueryCoolingTime: '1',
  ServiceAccountKey: '',
  ProjectId: '',
  DownloadPolicy: '',
  BucketName: '',
  compressiontype: '',
  PollingInterval: ,
  FolderName: '',
  UseStreaming: false,
  StartTimestamp: '',
  ObjectFilter: '',
  ObjectDetectionMode: '',
  IncludeSubfolders: false )
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream1;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (
  objectname: '',
  foldername: '',
  bucketname: '',
  uploadpolicy: '' )
FORMAT USING DSVFormatter (
members:'data')
INPUT FROM @APPNAME@_Stream1;

CREATE OR REPLACE SOURCE @APPNAME@_src2 USING Global.GCSReader (
  _h_GCSQueryCoolingTime: '1',
  ServiceAccountKey: '',
  ProjectId: '',
  DownloadPolicy: '',
  BucketName: '',
  compressiontype: '',
  PollingInterval: ,
  FolderName: '',
  UseStreaming: false,
  StartTimestamp: '',
  ObjectFilter: '',
  ObjectDetectionMode: '',
  IncludeSubfolders: false )
PARSE USING Global.JSONParser ()
OUTPUT TO @APPNAME@_Stream2;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (
  objectname: '',
  foldername: '',
  bucketname: '',
  uploadpolicy: '' )
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_Stream2;

CREATE OR REPLACE SOURCE @APPNAME@_src3 USING GCSReader (
  _h_GCSQueryCoolingTime: '1',
  ServiceAccountKey: '',
  ProjectId: '',
  DownloadPolicy: '',
  BucketName: '',
  compressiontype: '',
  PollingInterval: ,
  FolderName: '',
  UseStreaming: false,
  StartTimestamp: '',
  ObjectFilter: '',
  ObjectDetectionMode: '',
  IncludeSubfolders: false )
PARSE USING AvroParser ()
OUTPUT TO @APPNAME@_Stream3;

CREATE CQ @APPNAME@_CQ3
INSERT INTO @APPNAME@_CQOut3
SELECT AvroToJson(data,false) FROM @APPNAME@_Stream3;

CREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (
  objectname: '',
  foldername: '',
  bucketname: '',
  uploadpolicy: '' )
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_CQOut3;

CREATE OR REPLACE SOURCE @APPNAME@_src4 USING Global.GCSReader (
  _h_GCSQueryCoolingTime: '1',
  ServiceAccountKey: '',
  ProjectId: '',
  DownloadPolicy: '',
  BucketName: '',
  compressiontype: '',
  PollingInterval: ,
  FolderName: '',
  UseStreaming: false,
  StartTimestamp: '',
  ObjectFilter: '',
  ObjectDetectionMode: '',
  IncludeSubfolders: false )
PARSE USING Global.XMLParser (
  rootnode: ''
)
OUTPUT TO @APPNAME@_Stream4;

CREATE OR REPLACE TARGET @APPNAME@_trgt4 USING S3Writer (
  objectname: '',
  foldername: '',
  bucketname: '',
  uploadpolicy: '' )
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_Stream4;

END APPLICATION @APPNAME@;

stop @AppName@;
Undeploy application @AppName@;
alter application @AppName@;
CREATE FLOW CP_Agent_flow;
CREATE OR REPLACE SOURCE CP_Oracle_source USING OracleReader ( 
  ConnectionURL: '@url@', 
  Tables: '@tables@', 
  Username: '@Username@', 
  Password: '@password@' ) 
OUTPUT TO CP_EndToEnd_SF_Adapter_Stream;
End Flow CP_Agent_flow;
alter application @AppName@ recompile;
DEPLOY APPLICATION @AppName@ with CP_Agent_flow on any in AGENTS;
start application @AppName@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ Recovery 5 second interval;
--create application @APPNAME@;

--CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaProps(zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11');
--CREATE STREAM @APPNAME@kperststream of Global.WAEvent PERSIST USING @APPNAME@KafkaProps;

create type @APPNAME@employee
(
id integer,
name String,
company String
);
CREATE STREAM @APPNAME@Hana_TypedStream of @APPNAME@employee;

CREATE OR REPLACE SOURCE @APPNAME@OnPrem_Oracle USING OracleReader  (
  Compression: false,
  StartTimestamp: 'null',
  SupportPDB: false,
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  FetchSize: 1,
  DDLCaptureMode: 'All',
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'localhost:1521:xe',
 Tables: 'QATEST.EMP%',
-- Tables: 'QATEST.OracToCql_alldatatypes',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  TransactionBufferType: 'Memory',
  DictionaryMode: 'OnlineCatalog',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '1MB'
 )
OUTPUT TO @APPNAME@kperststream ;

create stream @APPNAME@UserdataStream1 of Global.WAEvent;

Create CQ @APPNAME@CQUser
insert into @APPNAME@UserdataStream1
select putuserdata (data1,'OperationName',META(data1,'OperationName').toString()) from @APPNAME@kperststream data1;

Create CQ @APPNAME@CQUser_typed
insert into @APPNAME@Hana_TypedStream
select 
to_int(data[0]),
data[1],
data[2]
from @APPNAME@UserdataStream1 u 
where USERDATA(u,'OperationName').toString()=='INSERT' and meta(u,'TableName').toString()="QATEST.EMP3";


CREATE OR REPLACE TARGET @APPNAME@DBTarget USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'SYSTEM',
  Password_encrypted: 'false',
  --ParallelThreads: '4',
  BatchPolicy: 'EventCount:1000,Interval:60',
  CommitPolicy: 'EventCount:100,Interval:60',
  --ExcludedTables: 'QATEST.EMP2;QATEST.EMP3',
  ConnectionURL: 'jdbc:sap://10.77.21.116:39013/?databaseName=striim&currentSchema=QA',
  Tables: 'QA.EMP3',
  adapterName: 'DatabaseWriter',
  Password: 'XgsL2qpACEIHrXXh4SueCg=='
 ) 
INPUT FROM @APPNAME@Hana_TypedStream;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

use PosTester;
DROP WACTIONSTORE MerchantActivity;

CREATE TARGET @TARGET_NAME@ USING SpannerWriter (
  instanceId: 'qatest',
  BatchPolicy: 'EventCount:10000,Interval:60',
  Tables: 'QATEST.ORACLETOSPANNER_SOURCETABLE%,oracletospannerdb.OracleToSpannertarget',
  adapterName: 'SpannerWriter',
  ServiceAccountKey: '/Users/jenniffer/Product2/IntegrationTests/TestData/google-gcs.json'
) INPUT FROM @STREAM@;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;

 CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01;striim.test02;striim.test03',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream ;

  CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

create target AzureSQLDWHTarget using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream;


CREATE TARGET log USING LOGWRITER
(
 name:log,
 filename:'LOGFILENAME'
)
INPUT FROM data_stream;


END APPLICATION IR;

deploy application IR;
start IR;

STOP APPLICATION bq;
UNDEPLOY APPLICATION bq;
DROP APPLICATION bq CASCADE;
CREATE APPLICATION bq recovery 1 second interval;

Create Source s Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: 'dockerhost:1521:orcl',
 Tables:'qatest.src_multi_target01',
 FetchSize:1
) 
Output To ss;

CREATE TARGET t1 USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
	Tables:'qatest.src_multi_target01,qatest.tgt_multi_target01 columnmap(col1=ID,col2=NAME,OperationName=@METADATA(OperationName),TableName=@METADATA(TableName))',
	datalocation: 'US',
	nullmarker: 'aaaa',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:10, Interval:30'	
) INPUT FROM ss;

CREATE TARGET t2 USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
	Tables:'qatest.src_multi_target01,qatest.tgt_multi_target02',
	datalocation: 'US',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:10, Interval:30'	
) INPUT FROM ss;

CREATE TARGET t3 USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
	Tables:'qatest.src_multi_target01,qatest.tgt_multi_target03',
	datalocation: 'US',
	nullmarker: 'NOTNULL',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:10, Interval:30'	
) INPUT FROM ss;

CREATE or replace TARGET t4 USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
	Tables:'qatest.src_multi_target01,qatest.tgt_multi_target04 columnmap(OperationName=@METADATA(OperationName))',
	datalocation: 'US',
	nullmarker: 'NULL',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:10, Interval:30'	
) INPUT FROM ss;

END APPLICATION bq;
DEPLOY APPLICATION bq;
START APPLICATION bq;

STOP APPLICATION KafkaWPTester.KWApp;
STOP APPLICATION KafkaWPTester.KRApp;
UNDEPLOY APPLICATION KafkaWPTester.KWApp;
UNDEPLOY APPLICATION KafkaWPTester.KRApp;
DROP APPLICATION KafkaWPTester.KWApp CASCADE;
DROP APPLICATION KafkaWPTester.KRApp CASCADE;

CREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;
GRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;
CONNECT KafkaWPTester KafkaWPTester;


CREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE CSVSource USING FileReader (
	directory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',
    WildCard:'access_log_half',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;

CREATE TYPE AccessLogType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;

CREATE CQ AceeslogCQ
INSERT INTO TypedAccessLogStream
SELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]
FROM FileStream;

create Target DsvWriter using KafkaWriter VERSION '0.9.0' ( 
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V9dsvExistTopic',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream;


create Target JsonWriter using KafkaWriter VERSION '0.9.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V9jsonExistTopic',
	Mode:'Sync'
    )
 
FORMAT USING JSONFormatter ()
input from TypedAccessLogStream;


create Target AvroWriter using KafkaWriter VERSION '0.9.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V9avroExistTopic',
    Mode:'Sync'
    )
 
FORMAT USING AvroFormatter (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')
input from TypedAccessLogStream;


END APPLICATION KWApp;

DEPLOY APPLICATION KWApp on any in default;


-- Kafka Reader Apps

CREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;


CREATE TYPE AccessLogType2(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE Stream KafkaDSVReaderStream of AccessLogType2;
CREATE Stream KafkaJsonReaderStream of AccessLogType2;
CREATE Stream KafkaAvroReaderStream of AccessLogType2;


CREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '0.9.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V9dsvExistTopic',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;




CREATE CQ DSVtoKafkaDSVReaderStream
INSERT INTO KafkaDSVReaderStream
SELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]
FROM KafkaReaderStream1;

CREATE TARGET DSVDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V9_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaDSVReaderStream;

CREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '0.9.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V9jsonExistTopic',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;




CREATE CQ DSVtoKafkaJsonReaderStream
INSERT INTO KafkaJsonReaderStream
SELECT 
data.get('Col1').textValue(),
data.get('Col2').textValue(),
data.get('Col3').textValue(),
data.get('Col4').textValue(),
data.get('Col5').textValue(),
data.get('Col6').textValue(),
data.get('Col7').textValue(),
data.get('Col8').textValue(),
data.get('Col9').textValue(),
data.get('Col10').textValue()
FROM KafkaReaderStream2;

CREATE TARGET JSONDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V9_RT_JSON')
FORMAT USING DSVFormatter()
INPUT FROM KafkaJsonReaderStream;

CREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '0.9.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V9avroExistTopic',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'
)
OUTPUT TO KafkaReaderStream3;



CREATE Stream KRTypedStream of AccessLogType;

CREATE CQ DSVtoKafkaAvroReaderStream
INSERT INTO KafkaAvroReaderStream
SELECT    
data.get("Col1").toString(), 
data.get("Col2").toString(), 
data.get("Col3").toString(), 
data.get("Col4").toString(),
data.get("Col5").toString(), 
data.get("Col6").toString(), 
data.get("Col7").toString(), 
data.get("Col8").toString(),
data.get("Col9").toString(), 
data.get("Col10").toString()
FROM KafkaReaderStream3;

CREATE TARGET AVRODump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V9_RT_AVRO')
FORMAT USING DSVFormatter()
INPUT FROM KafkaAvroReaderStream;

END APPLICATION KRApp;
DEPLOY APPLICATION KRApp on any in default;

stop ORAToBigquery;
undeploy application ORAToBigquery;
drop application ORAToBigquery cascade;

CREATE APPLICATION ORAToBigquery RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE Rac11g USING OracleReader ( 
  SupportPDB: false,
  SendBeforeImage: true,
  ReaderType: 'LogMiner',
  CommittedTransactions: false,
  FetchSize: 1,
  Password: 'manager',
  DDLTracking: false,
  StartTimestamp: 'null',
  OutboundServerProcessName: 'WebActionXStream',
  OnlineCatalog: true,
  ConnectionURL: '192.168.33.10:1521/XE',
  SkipOpenTransactions: false,
  Compression: false,
  QueueSize: 40000,
  RedoLogfiles: 'null',
  Tables: 'SYSTEM.GGAUTHORIZATIONS',
  Username: 'system',
  FilterTransactionBoundaries: true,
  adapterName: 'OracleReader',
  XstreamTimeOut: 600,
  connectionRetryPolicy: 'timeOut=30, retryInterval=60, maxRetries=3'
 ) 
OUTPUT TO DataStream;

CREATE OR REPLACE TARGET Target1 USING SysOut ( 
  name: "dstream"
 ) 
INPUT FROM DataStream;

CREATE OR REPLACE TARGET Target2 using BigqueryWriter(
  BQServiceAccountConfigurationPath:"/Users/ravipathak/Downloads/big-querytest-1963ae421e90.json",
  projectId:"big-querytest",
  Tables: "SYSTEM.GGAUTHORIZATIONS,testing1.ggauthorisation",
  parallelismCount: 2,
  BatchPolicy: "eventCount:100000,Interval:0")
INPUT FROM DataStream;

END APPLICATION ORAToBigquery;

deploy application ORAToBigquery;
start ORAToBigquery;

STOP APPLICATION HW ;
undeploy application HW ;
drop application HW cascade;

CREATE APPLICATION HW Recovery 5 second interval;

CREATE  SOURCE S USING OrReader  ( 
  Username: 'miner',
  Password: '@miner',
  ConnectionURL: '@conn-url@',
  Tables: '@src@',
  FetchSize: 1) 
OUTPUT TO hivestream;

Create Target T using HiveWriter (
  ConnectionURL:'@hive-url@',
  Username:'@uname@', 
            Password:'@pwd@',
        --hadoopurl:'hdfs://localhost:9000/',
        hadoopurl:'hdfs://dockerhost:9000/',
	        Mode:'incremental',
	        mergepolicy: 'eventcount:100,interval:1s',
            Tables:'@tgt-table@',
            hadoopConfigurationPath:'/Users/saranyad/Documents/hello/'
 )
INPUT FROM hivestream;


END APPLICATION HW;
deploy application HW on all in default;

Start application HW;

CONNECT ADMIN abc;

stop application ADW;
undeploy application ADW;
drop application ADW cascade;
CREATE APPLICATION ADW;

CREATE  SOURCE OjetIL USING DatabaseReader  
 (
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'@SOURCE-TABLES@',
 FetchSize:2000
) 
OUTPUT TO InitialLoadStream;

CREATE TARGET AzureDWInitialLoad USING AzureSQLDWHWriter(
ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
)
INPUT FROM InitialLoadStream;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

--
-- Recovery Test 41 with two sources, two jumping count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5W/p -> CQ1 -> WS
--   S2 -> Jc6W/p -> CQ2 -> WS
--

STOP KStreamRecov41Tester.KStreamRecovTest41;
UNDEPLOY APPLICATION KStreamRecov41Tester.KStreamRecovTest41;
DROP APPLICATION KStreamRecov41Tester.KStreamRecovTest41 CASCADE;
DROP USER KStreamRecov41Tester;
DROP NAMESPACE KStreamRecov41Tester CASCADE;
CREATE USER KStreamRecov41Tester IDENTIFIED BY KStreamRecov41Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov41Tester;
CONNECT KStreamRecov41Tester KStreamRecov41Tester;

CREATE APPLICATION KStreamRecovTest41 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION KStreamRecovTest41;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@
 Using DatabaseReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectURL@',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP APPLICATION HW ;
undeploy application HW ;
drop application HW cascade;

CREATE APPLICATION HW Recovery 5 second interval;

CREATE  SOURCE S USING OracleReader  ( 
  Username: 'miner',
  Password: '@miner',
  ConnectionURL: '@conn-url@',
  Tables: '@src@',
  FetchSize: 1) 
OUTPUT TO hivestream;

Create Target T using HiveWriter (
  ConnectionURL:'@hive-url@',
  Username:'@uname@', 
            Password:'@pwd@',
            hadoopurl:'hdfs://dockerhost:9000/',
	        Mode:'incremental',
	        mergepolicy: 'eventcount:5,interval:1s',
            Tables:'@tgt-table@',
            hadoopConfigurationPath:'/Users/saranyad/Documents/hello/'
 )
INPUT FROM hivestream;


END APPLICATION HW;
deploy application HW on all in default;

Start application HW;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 10 second interval;
create source CSVSource using FileReader (
	directory:'@DIR@',
	WildCard:'@WILDCARD@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  curr String,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       data[6],
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target BlobT using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:30,interval:5s'
)
format using AvroFormatter (
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

--
-- Kafka Stream with KryoParser Kafka Reader without Recovery Test 2 
-- Bert Hashemi and Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> KS -> WS
-- S -> K -> CQ -> WS

STOP KStreamKryoParserTester2WOR.KStreamKryoParserTest2WOR;
UNDEPLOY APPLICATION KStreamKryoParserTester2WOR.KStreamKryoParserTest2WOR;
DROP APPLICATION KStreamKryoParserTester2WOR.KStreamKryoParserTest2WOR CASCADE;
DROP USER KStreamKryoParserTester2WOR;
DROP NAMESPACE KStreamKryoParserTester2WOR CASCADE;
CREATE USER KStreamKryoParserTester2WOR IDENTIFIED BY KStreamKryoParserTester2WOR;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamKryoParserTester2WOR;
CONNECT KStreamKryoParserTester2WOR KStreamKryoParserTester2WOR;

-- CREATE APPLICATION KStreamKryoParserTest2WOR RECOVERY 5 SECOND INTERVAL;
CREATE APPLICATION KStreamKryoParserTest2WOR;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'1');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE or REPLACE TYPE KafkaCsvStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF KafkaCsvStreamType 
EVENT TYPES ( KafkaCsvStreamType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE or REPLACE TYPE KafkaStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

--CREATE STREAM KafkaStream OF KafkaStreamType;
CREATE STREAM KafkaStream OF Global.waevent;

CREATE SOURCE KafkaSource USING KafkaReader
(
        brokerAddress:'localhost:9092',
        Topic:'KStreamKryoParserTester2WOR_KafkaCsvStream',
        PartitionIDList:'0',
        startOffset:0
)
PARSE USING KryoParser ()
OUTPUT TO KafkaStream;

CREATE WACTIONSTORE KRWactions CONTEXT OF KafkaStreamType
EVENT TYPES ( KafkaStreamType )
@PERSIST-TYPE@

CREATE CQ KRInsertWactions
INSERT INTO KRWactions
SELECT TO_STRING(data[1]) as merchantId,
    TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
    TO_DOUBLE(data[7]) as amount,
    TO_STRING(data[10]) as city 
FROM KafkaStream;

END APPLICATION KStreamKryoParserTest2WOR;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;

drop propertyset OrcToOrcPlatfm_App_KafkaPropset;
drop stream  OrcToOrcPlatfm_Stream CASCADE;


--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;

CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNEC4TION_URL@',
  Tables: '$table1',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'QATEST.ORCTOORCPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '$table2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@4',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'QATEST.ORCTOORCPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;

END APPLICATION @APPNAME@2;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;


CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using PostgreSQLReader
(
   adapterName: PostgreSQLReader,
   CDDLAction: Quiesce_Cascade,
   CDDLCapture: true,
   CDDLTrackingTable:'striim.ddlcapturetable',
   ConnectionURL: jdbc:postgresql://localhost:5432/qatest,
   FilterTransactionBoundaries: true,
   Password: w@ct10n,
   ReplicationSlotName:'test_slot',
   Tables: public.sample,
   Username: sa,
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;
CREATE  SOURCE @SourceName@ USING MySqlReader  ( 
  Username: '@Username@',
  Password: '@Password@',
  DatabaseName: 'qatest',
  connectionRetryPolicy: @ConnectionRetryPolicy@,
  ConnectionURL: '@ConnectionURL@',
  Tables: '@SourceTables@',
  ConnectionPoolSize: 1,
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @SRCINPUTSTREAM@;
create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

CREATE FLOW ServerFlow;


CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
Float_col Double,
BFLoat_col Double,
bdouble_col Double,
long_col String,
Table String,
Operation String
);

CREATE STREAM CDCFilteredStream OF LogType;

CREATE CQ ToFilteredStream
INSERT INTO CDCFilteredStream
SELECT data[0],
data[1],
data[2],
to_double(data[3]),
to_double(data[4]),
to_double(data[5]),
data[6],
META(a, "TableName"),
META(a, "OperationName")
from @STREAM@ a;



CREATE WINDOW CDCWindow
OVER CDCFilteredStream
KEEP 3 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

END FLOW ServerFlow;


CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
 )
INPUT FROM @STREAM@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @srcName@ USING OracleReader (
  Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@'
)
OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@ USING Global.SpannerWriter ( 
   Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
  InstanceID: '@instanceId@', 
  ServiceAccountKey: '@keyFileName@', 
  CheckpointTable: 'CHKPOINT', 
  ProjectId: '@projectId@', 
  PrivateServiceConnectEndpoint: '@privatelinkname@', 
  BatchPolicy: 'EventCount: 1, Interval: 60s') 
INPUT FROM @instreamname@;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;

drop propertyset PGtoPGPlatfm_App_KafkaPropset;
drop stream  PGToPGPlatfm_Stream CASCADE;


CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;
					
CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using PostgreSQLReader(
  ReplicationSlotName: 'slotname',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: '$table1',
  ExcludedTables:'public.chkpoint'
 ) OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using PostgreSQLReader( 
  ReplicationSlotName: 'slotname',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: '@TABLENAME@2',
  ExcludedTables:'public.chkpoint'
 ) OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'WACTION.PGToPGPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using PostgreSQLReader( 
  ReplicationSlotName: 'slotname',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: '$table2',
  ExcludedTables:'public.chkpoint'
 ) OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using PostgreSQLReader(
  ReplicationSlotName: 'slotname',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: '@TABLENAME@4',  
  ExcludedTables:'public.chkpoint'
 ) OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'WACTION.PGToPGPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;

END APPLICATION @APPNAME@2;

stop application GCSWriterTest;
undeploy application GCSWriterTest;
drop application GCSWriterTest cascade;
create application GCSWriterTest recovery 1 second interval;
Create Source OracleSource Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To OracleStream;

create Target OracleGCSTarget using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@',
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from OracleStream;

end application GCSWriterTest;
deploy application GCSWriterTest on all in default;
start application GCSWriterTest;

create or replace type @STREAM@details(
ID INT,
name String,
company String);

create or replace stream @STREAM@_TYPED of @STREAM@details PARTITION BY name;

Create or replace CQ @STREAM@detailsCQ
insert into @STREAM@_TYPED
select 
to_int(data[0]),data[1],data[2]
from @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING FileWriter  ( 
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
 ) Format using DSVFormatter()
INPUT FROM @STREAM@_TYPED;

stop application admin.@APPNAME@;
undeploy application admin.@APPNAME@;
alter application admin.@APPNAME@ AUTORESUME MAXRETRIES 5 RETRYINTERVAL 5;
Alter application admin.@APPNAME@ RECOMPILE;
Alter application admin.@APPNAME@;
create or replace source admin.@APPNAME@s using FileReader (
        directory:'Product/IntegrationTests/TestData/',
        wildcard:'posdata5L.csv',
        positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:',',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  header: true,
  separator:'~'
)
OUTPUT TO admin.@APPNAME@in_memory_rawStream;

Alter application admin.@APPNAME@ RECOMPILE;
deploy application admin.@APPNAME@;
start admin.@APPNAME@;

CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE MongoDB_Source USING Global.MongoDBReader(
collections: '@table@',
  ConnectionURL: '@connectionUrl@',
  mode: 'Incremental',
  Username: '@userName@',
  Password: '@Password@' )
Output To mongoDBRaw_Stream;

CREATE CQ JSON2StriimType
INSERT INTO jsonNodeEventStream
SELECT data.get("_id").toString() AS ID,
  data.get("firstname").toString() AS FirstName,
  data.get("lastname").toString() AS LastName,
  data.get("age").toString() AS Age
FROM mongoDBRaw_Stream;

CREATE TARGET MongoJson_FileDSV_Target USING Global.FileWriter ( 
  flushpolicy: 'EventCount:10000,Interval:30s',  
  directory: '@logs@',
  filename: '@BuiltinFunc@_Data', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING Global.DSVFormatter  ( 
   ) 
INPUT FROM jsonNodeEventStream;

End Application @AppName@;
Deploy application @AppName@;
Start Application @AppName@;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL ;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes1',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource2 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes2',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource3 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes3',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'TEST.user_chkpoint',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: 'QATEST.OracToCql_alldatatypes1,test.oractocq_alldatatypes columnmap(NumericToBigint=NumericToBigint);QATEST.OracToCql_alldatatypes2,test.oractocq_alldatatypes columnmap(NumericToBigint=NumericToBigint);QATEST.OracToCql_alldatatypes3,test.oractocq_alldatatypes columnmap(NumericToBigint=NumericToBigint)',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;


END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

STOP DSAPP;
UNDEPLOY APPLICATION DSAPP;
DROP APPLICATION DSAPP CASCADE;

CREATE APPLICATION DSAPP;

-- CacheWaction WACTIONSTORE is being loaded from DSCache

CREATE OR REPLACE WACTIONSTORE CacheWaction CONTEXT OF T1
EVENT TYPES ( T1 )
@PERSIST-TYPE@

CREATE CQ CQ2Derby
INSERT INTO CacheWaction
select * from C1
LINK SOURCE EVENT;

END APPLICATION DSAPP;
DEPLOY APPLICATION DSAPP;
START APPLICATION DSAPP;

stop application @APPNAME@Apps2;
undeploy application @APPNAME@Apps2;
drop application @APPNAME@Apps2 cascade;


stop application @APPNAME@Apps3;
undeploy application @APPNAME@Apps3;
drop application @APPNAME@Apps3 cascade;



stop application @APPNAME@Apps4;
undeploy application @APPNAME@Apps4;
drop application @APPNAME@Apps4 cascade;



stop application @APPNAME@Apps1;
undeploy application @APPNAME@Apps1;
drop application @APPNAME@Apps1 cascade;

CREATE OR REPLACE PROPERTYSET @APPNAME@Apps_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9099', kafkaversion:'0.11');

CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream1 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;
CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream2 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;
CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream3 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;
CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream4 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;


--**********************Application 1*******************
-- with 2 agent flow 
-- <agentflow1>source1->PS1<agentflow1>
-- <agentflow2>Source2->Inmemory1->cq1->PS2<agentflow2>

CREATE APPLICATION @APPNAME@Apps1 RECOVERY 5 SECOND INTERVAL;
create flow @APPNAME@agentflow;
CREATE OR REPLACE SOURCE @APPNAME@AppsDBSource1 USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@1',
  adapterName: 'OracleReader',
  Password: '@PASSWORD@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@USERNAME@',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@Apps_PS_Stream1;
end flow @APPNAME@agentflow;

create flow @APPNAME@agentflow2;
CREATE OR REPLACE SOURCE @APPNAME@AppsDBSource2 USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@2',
  adapterName: 'OracleReader',
  Password: '@PASSWORD@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@USERNAME@',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@Apps_PS_InmemoryStream1;

CREATE CQ @APPNAME@cq_Inmemory1
INSERT INTO @APPNAME@Apps_PS_Stream2
SELECT *
FROM @APPNAME@Apps_PS_InmemoryStream1;
end flow @APPNAME@agentflow2;

end application @APPNAME@Apps1;

-- ********************Application 2***********************
-- with 2 agent flow 
-- <agentflow3>source3->inmemory2<agentflow3>inmemory->cq2->ps3
-- <agentflow4>Source2->Inmemory3<agentflow4><serverflow1>inmemory->cq3->ps4<serverflow1>

CREATE APPLICATION @APPNAME@Apps2 RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@agentflow3;
CREATE OR REPLACE SOURCE @APPNAME@AppsDBSource3 USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@3',
  adapterName: 'OracleReader',
  Password: '@PASSWORD@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@USERNAME@',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@Apps_PS_InmemoryStream2;

CREATE CQ @APPNAME@cq_Inmemory2
INSERT INTO @APPNAME@Apps_PS_Stream3
SELECT *
FROM @APPNAME@Apps_PS_InmemoryStream2;

end flow @APPNAME@agentflow3;

create flow @APPNAME@agentflow4;
CREATE OR REPLACE SOURCE @APPNAME@AppsDBSource4 USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@4',
  adapterName: 'OracleReader',
  Password: '@PASSWORD@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@USERNAME@',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@Apps_PS_InmemoryStream3;

end flow @APPNAME@agentflow4;

create  flow @APPNAME@serverflow;
CREATE CQ @APPNAME@cq_Inmemory3
INSERT INTO @APPNAME@Apps_PS_Stream4
SELECT *
FROM @APPNAME@Apps_PS_InmemoryStream3;
end flow @APPNAME@serverflow;
END APPLICATION @APPNAME@Apps2;

-- ********************Application 3**************************
--WITH 3 TARGET FLOW
-- 1. <targetflow1> ps1->target1 <targetflow1>
-- 2. <targetflow2>ps2->target2 <targetflow2>
-- 3. <targetflow3> ps1,ps2->cq4 -> target3 <targetflow3>

CREATE APPLICATION @APPNAME@Apps3 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @APPNAME@TARGETFLOW1;
CREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget1 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@1,@TARGET_TABLE@1',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream1;
END FLOW @APPNAME@TARGETFLOW1;

CREATE FLOW @APPNAME@TARGETFLOW2;
CREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget2 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@2,@TARGET_TABLE@2',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream2;
END FLOW @APPNAME@TARGETFLOW2;

CREATE OR REPLACE STREAM @APPNAME@Apps_PS_InmemoryStream4 OF Global.waevent;

CREATE CQ @APPNAME@cq_ps1
INSERT INTO @APPNAME@Apps_PS_InmemoryStream4
SELECT *
FROM @APPNAME@Apps_PS_Stream1;

CREATE CQ @APPNAME@cq_ps2
INSERT INTO @APPNAME@Apps_PS_InmemoryStream4
SELECT *
FROM @APPNAME@Apps_PS_Stream2;

CREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget2 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@1,@TARGET_TABLE@5;@SOURCE_TABLE@2,@TARGET_TABLE@5',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_InmemoryStream4;

END APPLICATION @APPNAME@Apps3;


--********************Application 4************************
--WITH 3 TARGET FLOW
-- 1. <targetflow1> ps3->target4 <targetflow1>
-- 2. <targetflow2>ps4->target5 <targetflow2>
-- 3. <targetflow3> ps3,ps4->cq5 -> target6 <targetflow3>

CREATE APPLICATION @APPNAME@Apps4 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @APPNAME@TARGETFLOW3;
CREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget4 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@3,@TARGET_TABLE@3',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream3;
END FLOW @APPNAME@TARGETFLOW3;

CREATE FLOW @APPNAME@TARGETFLOW4;
CREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget5 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@4,@TARGET_TABLE@4',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream4;
END FLOW @APPNAME@TARGETFLOW4;

CREATE OR REPLACE STREAM @APPNAME@Apps_PS_InmemoryStream5 OF Global.waevent;

CREATE CQ @APPNAME@cq_ps3
INSERT INTO @APPNAME@Apps_PS_InmemoryStream5
SELECT *
FROM @APPNAME@Apps_PS_Stream3;

CREATE CQ @APPNAME@cq_ps4
INSERT INTO @APPNAME@Apps_PS_InmemoryStream5
SELECT *
FROM @APPNAME@Apps_PS_Stream4;


CREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget6 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@3,@TARGET_TABLE@6;@SOURCE_TABLE@4,@TARGET_TABLE@6',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_InmemoryStream5;

END APPLICATION @APPNAME@Apps4;

deploy application @APPNAME@Apps4 on ANY in Appsdg with @APPNAME@Targetflow3 in Targetdg1,@APPNAME@Targetflow4 in Targetdg;
start application @APPNAME@Apps4;

deploy application @APPNAME@Apps3 on ANY in Appsdg with @APPNAME@Targetflow2 in Targetdg1,@APPNAME@Targetflow1 in Targetdg;
start application @APPNAME@Apps3;

deploy application @APPNAME@Apps2 in Appsdg with @APPNAME@agentflow3 in agents, @APPNAME@agentflow4 in agents2, @APPNAME@serverflow in default;
start application @APPNAME@Apps2;

deploy application @APPNAME@Apps1 in Appsdg with @APPNAME@agentflow in agents, @APPNAME@agentflow2 in agents2;
start application @APPNAME@Apps1;

STOP twoNodeCacheRefresh.AppTest;
UNDEPLOY APPLICATION twoNodeCacheRefresh.AppTest;
DROP APPLICATION twoNodeCacheRefresh.AppTest cascade;

CREATE APPLICATION AppTest;

CREATE FLOW THESOURCEFLOW;
----------------------------------------------------

CREATE TYPE GenType
(
  GenID Integer KEY,
  GenCode double,
  GenState String,
  GenDate DateTime,
  GenLong Long
);


CREATE SOURCE GenSource USING StreamReader(
	OutputType: 'twoNodeCacheRefresh.GenType',
	noLimit: 'false',
	maxRows: 100,
	iterationDelay: 5,
	StringSet: 'GenState[CA-FL]',
	NumberSet: 'GenID[0-0]Linc,GenCode[99-99]Linc,GenLong[4999999-4999999]G'
)OUTPUT TO rawGenStream;


END FLOW THESOURCEFLOW;
----------------------------------------------------

CREATE FLOW MAINPROCESSFLOW;

CREATE TYPE theGenType
(
  theGenID Integer KEY,
  theGenCode double,
  theGenState String,
  theGenDate DateTime,
  theGenLong Long
);

CREATE STREAM GenStream OF theGenType;

CREATE CQ GenCQ1
INSERT INTO GenStream
SELECT TO_INT(data[0]),
	   TO_DOUBLE(data[1]),
	   data[2],
	   TO_DATE(data[3]),
	   TO_LONG(data[4])
FROM rawGenStream;


CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'addy.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false
) QUERY (keytomap:'zip', refreshinterval:'20 second', skipinvalid:'true') OF USAddressData;


CREATE TYPE MergedData
(
  country String,
  zip String,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String,
  theGenID Integer,
  theGenCode double,
  theGenState String,
  theGenDate DateTime,
  theGenLong Long
);

CREATE STREAM SanDiegoDataStream OF MergedData;
CREATE STREAM MiamiDataStream OF MergedData;

-- join the data
CREATE CQ SanDiegoCQData
INSERT INTO SanDiegoDataStream
SELECT z.country, z.zip, z.city, z.state, z.stateCode,
       z.fullCity, z.someNum, z.pad, z.latVal,
       z.longVal, z.empty, z.empty2, gs.theGenID,
       gs.theGenCode, gs.theGenState, gs.theGenDate, gs.theGenLong
       FROM GenStream gs, ZipLookup z where gs.theGenState = 'CA' AND z.zip LIKE '921%';

CREATE CQ MiamiCQData
INSERT INTO MiamiDataStream
SELECT z.country, z.zip, z.city, z.state, z.stateCode,
       z.fullCity, z.someNum, z.pad, z.latVal,
       z.longVal, z.empty, z.empty2, gs.theGenID,
       gs.theGenCode, gs.theGenState, gs.theGenDate, gs.theGenLong
       FROM GenStream gs, ZipLookup z where gs.theGenState = 'FL' AND z.zip LIKE '331%';


CREATE TYPE MiamiData
(
    zip String KEY,
    fullCity String,
    theGenCode double,
    theGenState String
);

CREATE WACTIONSTORE MiamiStore
CONTEXT OF MiamiData
EVENT TYPES(MergedData )
PERSIST NONE USING ( );


CREATE CQ MiamiStoreCQ
INSERT INTO MiamiStore
SELECT zip, fullCity, theGenCode, theGenState
FROM MiamiDataStream
LINK SOURCE EVENT;


CREATE TYPE SanDiegoData
(
    zip String KEY,
    fullCity String,
    theGenCode double,
    theGenState String
);

CREATE WACTIONSTORE SanDiegoStore
CONTEXT OF SanDiegoData
EVENT TYPES(MergedData )
PERSIST NONE USING ( );


CREATE CQ SanDiegoStoreCQ
INSERT INTO SanDiegoStore
SELECT zip, fullCity, theGenCode, theGenState
FROM SanDiegoDataStream
LINK SOURCE EVENT;


END FLOW MAINPROCESSFLOW;
----------------------------------------------------

END APPLICATION AppTest;

create Application netflow;
create source NetflowSource using UDPReader (
	IpAddress:'127.0.0.1',
	PortNo:'3546'
)
parse using NetflowParser (
)
OUTPUT TO NetflowStream;

create type NetflowV5_Type (
vers int,
sys_uptime long,
unix_sec long,
unix_nsec long,
flow_sequence long,
engine_type string,
engine_id string,

src_ip string,
dst_ip string,
next_hop  string,
input_idx int,
output_idx  int,

flow_pkt_cnt long,
l3_cnt long,
first_sys_uptime long,
last_sys_uptime long,
src_port int,

dst_port int,
unused1 string,
tcp_flg string,
protocol_type string,
tos string,

src_as int,
dst_as int,
src_mask string,
dst_mask string,
unused2 int

);

CREATE STREAM NetflowV5Stream of NetflowV5_Type;

CREATE CQ NetflowCQ
INSERT INTO NetflowV5Stream
SELECT META(x,'version'),META(x,'sys_uptime'),META(x,'unix_sec'),META(x,'unix_nsec'),META(x,'flow_sequence'),META(x,'engine_type').toString(),META(x,'engine_id').toString(),
       VALUE(x,'src_ip'), VALUE(x,'dst_ip'), VALUE(x,'next_hop'), VALUE(x,'input_idx'), VALUE(x,'output_idx'),
       VALUE(x,'flow_pkt_cnt'), VALUE(x,'l3_cnt'), VALUE(x,'first_sys_uptime'), VALUE(x,'last_sys_uptime'), VALUE(x,'src_port'),
       VALUE(x,'dst_port'), VALUE(x,'unused1').toString(), VALUE(x,'tcp_flg').toString(), VALUE(x,'protocol_type').toString(), VALUE(x,'tos').toString(),
       VALUE(x,'src_as'), VALUE(x,'dst_as'), VALUE(x,'src_mask').toString(), VALUE(x,'dst_mask').toString(), VALUE(x,'unused2')
FROM NetflowStream x;

create Target dump1 using LogWriter(name:'NetflowV5',fileName:'@FEATURE-DIR@/logs/netflow.out') input from NetflowV5Stream;
end Application netflow;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;;

create Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;

stop PatternMatching.CSV;
undeploy application PatternMatching.CSV;
drop application PatternMatching.CSV cascade;

create application CSV RECOVERY 5 SECOND INTERVAL;

create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'ctest.csv',
  columndelimiter:',',
  positionByEOF:false
)
OUTPUT TO CsvStream;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  TO_INT(data[0]) as UserId,
	TO_INT(data[1]) as temp1,
        TO_DOUBLE(data[2]) as temp2,
	TO_STRING(data[3]) as temp3
FROM CsvStream;

-- scenario 1.1 check pattern alterations
CREATE CQ TypeConversionCsvCQ1
INSERT INTO TypedStream1
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A | B  | C
define A = UserDataStream(temp1 = 20), B= UserDataStream(temp2 = 30.40), C= UserDataStream(temp3 = 'Bret')
PARTITION BY UserId;

-- scenario 1.2 check pattern permutation with partition by
CREATE CQ TypeConversionCsvCQ2
INSERT INTO TypedStream2
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A & B & C
define A = UserDataStream(temp1 = 10), B= UserDataStream(temp2 = 20.30), C= UserDataStream(temp3 = 'zalak')
PARTITION BY UserId;

-- scenario 1.3 check pattern quantifire with partition by
CREATE CQ TypeConversionCsvCQ3
INSERT INTO TypedStream3
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A ? B ? C
define A = UserDataStream(temp1 = 10), B= UserDataStream(temp2 = 20.30), C= UserDataStream(temp3 = 'zalak')
PARTITION BY UserId;

-- scenario 1.4 check pattern quantifire with grouping and partition by
CREATE CQ TypeConversionCsvCQ4
INSERT INTO TypedStream4
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A ( B ? C)
define A = UserDataStream(temp1 = 10), B= UserDataStream(temp2 = 20.30), C= UserDataStream(temp3 = 'zalak')
PARTITION BY UserId;

-- scenario 1.5 check pattern overlapping and alteration with grouping and partition by
CREATE CQ TypeConversionCsvCQ5
INSERT INTO TypedStream5
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN (A # B) | C
define A = UserDataStream(temp1 = 30), B= UserDataStream(temp2 = 20.30), C= UserDataStream(temp3 = 'Bret')
PARTITION BY UserId;

-- scenario 1.6 check pattern quantifire with grouping and two partition by
CREATE CQ TypeConversionCsvCQ6
INSERT INTO TypedStream6
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A ? B ? C
define A = UserDataStream(temp1 = 10), B= UserDataStream(temp2 = 30.40), C= UserDataStream(temp3 = 'Bret')
PARTITION BY temp1,temp2;

-- scenario 1.7 check pattern quantifire(0 or 1),<=,>= with partition by
CREATE CQ TypeConversionCsvCQ7
INSERT INTO TypedStream7
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A * B * C
define A = UserDataStream(temp1 <= 10), B= UserDataStream(temp2 >= 30.40), C= UserDataStream(temp3 = 'zalak')
PARTITION BY UserId,temp3;

-- scenario 1.8 check pattern alteration and permutation using between values with partition by
CREATE CQ TypeConversionCsvCQ8
INSERT INTO TypedStream8
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A | B & C
define A = UserDataStream(temp1 between 10 and 40), B= UserDataStream(temp2 between 10.40 and 30.50), C= UserDataStream(temp3 != 'zalak')
PARTITION BY temp3;

-- scenario 1.9 check pattern or using <,>,!= values with partition by
CREATE CQ TypeConversionCsvCQ9
INSERT INTO TypedStream9
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A  B  C
define A = UserDataStream(temp1 > 20), B= UserDataStream(temp2 < 60), C= UserDataStream(temp3 != 'prajkta')
PARTITION BY UserId;

-- scenario 1.10 check pattern {m,n} using != values with partition by
CREATE CQ TypeConversionCsvCQ10
INSERT INTO TypedStream10
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A{0,2} | B{1,3} & C
define A = UserDataStream(temp1 != 20), B= UserDataStream(temp2 != 40.10), C= UserDataStream(temp3 != 'bert')
PARTITION BY temp1;


CREATE WACTIONSTORE UserActivityInfo1
CONTEXT OF TypedStream1_Type
EVENT TYPES ( TypedStream1_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo2
CONTEXT OF TypedStream2_Type
EVENT TYPES ( TypedStream2_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo3
CONTEXT OF TypedStream3_Type
EVENT TYPES ( TypedStream3_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo4
CONTEXT OF TypedStream4_Type
EVENT TYPES ( TypedStream4_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo5
CONTEXT OF TypedStream5_Type
EVENT TYPES ( TypedStream5_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo6
CONTEXT OF TypedStream6_Type
EVENT TYPES ( TypedStream6_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo7
CONTEXT OF TypedStream7_Type
EVENT TYPES ( TypedStream7_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo8
CONTEXT OF TypedStream8_Type
EVENT TYPES ( TypedStream8_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo9
CONTEXT OF TypedStream9_Type
EVENT TYPES ( TypedStream9_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo10
CONTEXT OF TypedStream10_Type
EVENT TYPES ( TypedStream10_Type )
@PERSIST-TYPE@

create Target t2 using SysOut(name:AgentTyped) input from TypedStream1;
create Target t3 using SysOut(name:AgentTyped1) input from TypedStream2;

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction1
INSERT INTO UserActivityInfo1
SELECT * FROM TypedStream1
LINK SOURCE EVENT;

CREATE CQ UserWaction2
INSERT INTO UserActivityInfo2
SELECT * FROM TypedStream2
LINK SOURCE EVENT;

CREATE CQ UserWaction3
INSERT INTO UserActivityInfo3
SELECT * FROM TypedStream3
LINK SOURCE EVENT;

CREATE CQ UserWaction4
INSERT INTO UserActivityInfo4
SELECT * FROM TypedStream4
LINK SOURCE EVENT;

CREATE CQ UserWaction5
INSERT INTO UserActivityInfo5
SELECT * FROM TypedStream5
LINK SOURCE EVENT;

CREATE CQ UserWaction6
INSERT INTO UserActivityInfo6
SELECT * FROM TypedStream6
LINK SOURCE EVENT;

CREATE CQ UserWaction7
INSERT INTO UserActivityInfo7
SELECT * FROM TypedStream7
LINK SOURCE EVENT;

CREATE CQ UserWaction8
INSERT INTO UserActivityInfo8
SELECT * FROM TypedStream8
LINK SOURCE EVENT;

CREATE CQ UserWaction9
INSERT INTO UserActivityInfo9
SELECT * FROM TypedStream9
LINK SOURCE EVENT;

CREATE CQ UserWaction10
INSERT INTO UserActivityInfo10
SELECT * FROM TypedStream10
LINK SOURCE EVENT;

end application CSV;
deploy application csv;
start csv;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
 )
INPUT FROM @STREAM@2;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;

Create Source @SourceName@ Using OracleReader

(
  Username:'c##qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',
  Tables:'CDB$ROOT."C##QATEST".ojet_src;ORCLPDB.QATEST.ojet_src',
  _h_useClassic:false,
  Fetchsize:1,
  Compression: true,
  SupportPDB:true,
  ReplicationSlotName:'null'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',
  Username:'c##qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'CDB$ROOT."C##QATEST".ojet_src,CDB$ROOT."C##QATEST".ojet_tgt'
) INPUT FROM @SRCINPUTSTREAM@;

CREATE TARGET @targetName1@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orclpdb',
  Username:'qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'ORCLPDB.QATEST.ojet_src,ORCLPDB.QATEST.ojet_tgt'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

--
-- Recovery Test 1
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov1Tester.RecovTest1;
UNDEPLOY APPLICATION Recov1Tester.RecovTest1;
DROP APPLICATION Recov1Tester.RecovTest1 CASCADE;
CREATE APPLICATION RecovTest1 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END APPLICATION RecovTest1;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using Ojet
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@URL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

--
-- Crash Recovery Test 2 on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS
--

STOP APPLICATION N4S4CR2Tester.N4S4CRTest2;
UNDEPLOY APPLICATION N4S4CR2Tester.N4S4CRTest2;
DROP APPLICATION N4S4CR2Tester.N4S4CRTest2 CASCADE;
CREATE APPLICATION N4S4CRTest2 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest2;

CREATE SOURCE CsvSourceN4S4CRTest2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest2;

CREATE FLOW DataProcessingN4S4CRTest2;

CREATE TYPE WactionTypeN4S4CRTest2 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionTypeN4S4CRTest2;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN4S4CRTest2 CONTEXT OF WactionTypeN4S4CRTest2
EVENT TYPES ( WactionTypeN4S4CRTest2 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN4S4CRTest2
INSERT INTO WactionsN4S4CRTest2
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN4S4CRTest2;

END APPLICATION N4S4CRTest2;

stop application MSSQLTransactionSupportFTMFTBTrue;
undeploy application MSSQLTransactionSupportFTMFTBTrue;
drop application MSSQLTransactionSupportFTMFTBTrue cascade;

CREATE APPLICATION MSSQLTransactionSupportFTMFTBTrue recovery 1 second interval;

Create Source ReadFromMSSQL2
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
FetchTransactionMetadata:'true',
FilterTransactionBoundaries: true,
Compression:'false',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportFTMFTBTrueStream;


CREATE TARGET WriteToMSSQL2 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC@,@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportFTMFTBTrueStream;

CREATE TARGET MSSqlReaderOutput2 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportFTMFTBTrueStream; 


CREATE OR REPLACE TARGET MSSQLFileOut2 USING FileWriter  ( 
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  directory:'@FEATURE-DIR@/logs/',
  filename: 'TransactionSupportFTMFTBTrue.txt'
 ) 
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 ) 
INPUT FROM MSSQLTransactionSupportFTMFTBTrueStream;

END APPLICATION MSSQLTransactionSupportFTMFTBTrue;
deploy application MSSQLTransactionSupportFTMFTBTrue;
start application MSSQLTransactionSupportFTMFTBTrue;

--
-- Recovery Test 13 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same compound key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> CW(p#p) -> CQ -> WS
--

STOP Recov13Tester.RecovTest13;
UNDEPLOY APPLICATION Recov13Tester.RecovTest13;
DROP APPLICATION Recov13Tester.RecovTest13 CASCADE;
CREATE APPLICATION RecovTest13 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTest10Data.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  partKey String KEY,
  serialNumber int,
  partKey2 String KEY
);

CREATE TYPE WactionData (
  partKey String KEY,
  serialNumber int
);

CREATE STREAM DataStream OF CsvData PARTITION BY partKey, partKey2;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    TO_INT(data[1]),
    data[0]
FROM CsvStream;

CREATE JUMPING WINDOW DataStreamTwoItems
OVER DataStream KEEP 2 ROWS
PARTITION BY partKey, partKey2;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    first(partKey),
    to_int(first(serialNumber))
FROM DataStreamTwoItems
GROUP BY partKey, partKey2;

END APPLICATION RecovTest13;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ Recovery 5 second interval;
--create application @APPNAME@;

create type @APPNAME@employee
(
id integer,
name String,
company String
);
CREATE STREAM @APPNAME@cosmoscassandra_TypedStream of @APPNAME@employee;

CREATE OR REPLACE SOURCE @APPNAME@OnPrem_Oracle USING OracleReader  (
  Compression: false,
  StartTimestamp: 'null',
  SupportPDB: false,
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  FetchSize: 1,
  DDLCaptureMode: 'All',
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'localhost:1521:xe',
 Tables: 'QATEST.EMP%',
-- Tables: 'QATEST.OracToCql_alldatatypes',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  TransactionBufferType: 'Memory',
  DictionaryMode: 'OnlineCatalog',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '1MB'
 )
OUTPUT TO @APPNAME@kperststream ;

create stream @APPNAME@UserdataStream1 of Global.WAEvent;

Create CQ @APPNAME@CQUser
insert into @APPNAME@UserdataStream1
select putuserdata (data1,'OperationName',META(data1,'OperationName').toString()) from @APPNAME@kperststream data1;

Create CQ @APPNAME@CQUser_typed
insert into @APPNAME@cosmoscassandra_TypedStream
select 
to_int(data[0]),
data[1],
data[2]
from @APPNAME@UserdataStream1 u 
where USERDATA(u,'OperationName').toString()=='INSERT' and meta(u,'TableName').toString()="QATEST.EMP3";


CREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target1 USING CassandraCosmosDBWriter  ( 
  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --CheckPointTable: '',
  FlushPolicy: 'EventCount:1000,Interval:60',
  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',
  AccountKey:'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  --ConsistencyLevel:'fh',
  --port:'10505',
  Tables: 'test.emp_tgt',
  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',
  adapterName: 'CassandraCosmosDBWriter'
 ) 
INPUT FROM @APPNAME@cosmoscassandra_TypedStream;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'JsonTargetTI',
  directory:'@FEATURE-DIR@/logs/',
  sequence:'00',
  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:10-89s'
)
format using JSONFormatter (
  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)

input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetJsonTIAddition_actual.log') input from TypedCSVStream;

end application DSV;

STOP APPLICATION TQLwithinTqlTester.TQLwithinTqlApp;
UNDEPLOY APPLICATION TQLwithinTqlTester.TQLwithinTqlApp;
DROP APPLICATION TQLwithinTqlTester.TQLwithinTqlApp CASCADE;

CREATE APPLICATION TQLwithinTqlApp;

@@FEATURE-DIR@/tql/TQLwithinTQL2.tql;
@@FEATURE-DIR@/tql/TQLwithinTQL4.tql;

END APPLICATION TQLwithinTqlApp;

stop OracleTOFileWriterApp;
undeploy application OracleTOFileWriterApp;
drop application OracleTOFileWriterApp cascade;

CREATE APPLICATION OracleTOFileWriterApp recovery 5 second interval;

Create Source ReadFromOracle
Using OracleReader
(
Username:'qatest',
Password:'qatest',
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:xe',
Tables:'QATEST.VARRAY_AS_VARCHAR',
OnlineCatalog:true,
CommittedTransactions:true
)
Output To DataStream;


CREATE OR REPLACE TARGET FileWriter1 USING FileWriter  (
filename: 'OutFile.csv',
  directory:'@DIR@/%@metadata(TableName)%',
  rolloverpolicy: 'EventCount:6,Interval:60s'
)
format using DSVFormatter (
members: 'data'
)
input from DataStream;

create target tout using sysout(name:'out') input from DataStream;

CREATE OR REPLACE TARGET FileWriter2 USING FileWriter  (
filename: 'OutFile.json',
  directory:'@DIR@/%@metadata(TableName)%',
  rolloverpolicy: 'EventCount:6,Interval:60s'
)
format using JSONFormatter (
)
input from DataStream;


CREATE OR REPLACE TARGET FileWriter3 USING FileWriter  (
filename: 'OutFile.xml',
  directory:'@DIR@/%@metadata(TableName)%',
  rolloverpolicy: 'EventCount:6,Interval:60s'
)
format using XMLFormatter (
rootelement : 'data',
FormatColumnValueAS:'xmlelement')
input from DataStream;


CREATE OR REPLACE TARGET FileWriter4 USING FileWriter  (
  filename: 'OutFile.parquet',
  directory:'@DIR@/%@metadata(TableName)%',
  rolloverpolicy: 'EventCount:6,Interval:60s' 
  )
FORMAT USING ParquetFormatter  
( 
blocksize: '128000000',
  formatAs: 'Default',
  schemaFileName: '@DIR@/parquetSchema'
  )
INPUT FROM DataStream;

CREATE OR REPLACE TARGET FileWriter5 USING FileWriter  (
  filename: 'OutFile.avro',
  directory:'@DIR@/%@metadata(TableName)%',
  rolloverpolicy: 'EventCount:6,Interval:60s' 
  )
FORMAT USING AvroFormatter  
( 
schemaFileName : '@DIR@/varray.avsc'
  )
INPUT FROM DataStream;

END APPLICATION OracleTOFileWriterApp;
deploy application OracleTOFileWriterApp in default;
start OracleTOFileWriterApp;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 5 second interval;
create source CSVSource using FileReader (
	directory:'@DIR@',
	WildCard:'@WILDCARD@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target BlobT using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:50'
)
format using DSVFormatter (
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

--
-- Recovery Test 8
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov8Tester.RecovTest8;
UNDEPLOY APPLICATION Recov8Tester.RecovTest8;
DROP APPLICATION Recov8Tester.RecovTest8 CASCADE;
CREATE APPLICATION RecovTest8 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END APPLICATION RecovTest8;

use consoletest;
alter application noApp;

CREATE source CsvDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'posdata.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;


end application noApp;

alter application noApp recompile;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
filename:'',
directory:'@FEATURE-DIR@/logs/',
    sequence:'00',
rolloverpolicy:'eventcount:200,sequence:00'
)
format using DSVFormatter (

)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetEventCount_actual.log') input from TypedCSVStream;

end application DSV;

stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@;

CREATE SOURCE @SourceName@ USING MSSQLReader  ( 
ReaderType: 'LogMiner', 
  Password_encrypted: 'false', 
  DatabaseName: 'qatest',
  SupportPDB: false, 
  QuiesceMarkerTable: 'QUIESCEMARKER', 
  QueueSize: 2048, 
  CommittedTransactions: true, 
  Username: '@UserName@', 
  TransactionBufferType: 'Memory', 
  TransactionBufferDiskLocation: '.striim/LargeBuffer', 
  OutboundServerProcessName: 'WebActionXStream', 
  Password: '@Password@', 
  DDLCaptureMode: 'All', 
  Compression: false, 
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', 
  FetchSize: 1, 
  Tables: '@SourceTables@', 
  DictionaryMode: 'OnlineCatalog', 
  XstreamTimeOut: 600, 
  TransactionBufferSpilloverSize: '1MB', 
  StartTimestamp: 'null', 
  FilterTransactionBoundaries: true, 
  StartSCN: 'null', 
  ConnectionURL: '@ConnectionURL@', 
  SendBeforeImage: true ) 
OUTPUT TO @AppStream@  ;

CREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream SELECT data[2], to_string(to_date(data[2]), "dd-MMM-yy hh.mm.ss") FROM @AppStream@ o ;

CREATE  TARGET @targetsys@ USING Global.SysOut  ( 
name: 'ora1_sys' ) 
INPUT FROM admin.ZDT_cq_stream;

create Target @TargetFile@ using FileWriter(
  filename:'toStringOut.log',
  directory:'@FilePath@',
  rolloverpolicy:'eventcount:1000'
)
format using DSVFormatter (

)
input from admin.ZDT_cq_stream;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING CassandraWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

-- CacheTestWithDB.tql
-- Author: Gopi Putty; Date: Oct,11, 2016. 
-- A cache backed by db table.
/*
Oracle:
CREATE TABLE PRODUCT_INV ( SKU INT PRIMARY KEY NOT NULL,  NAME varchar2(20) );
INSERT INTO PRODUCT_INV (SKU, NAME)
*/

USE ADMIN;
STOP APPLICATION CACHETEST2.CACHEBACKEDWITHDB;
UNDEPLOY APPLICATION CACHETEST2.CACHEBACKEDWITHDB;
DROP APPLICATION CACHETEST2.CACHEBACKEDWITHDB CASCADE;
DROP NAMESPACE CACHETEST2 CASCADE; 
CREATE NAMESPACE CACHETEST2;
USE CACHETEST2; 

CREATE APPLICATION CACHEBACKEDWITHDB;

CREATE TYPE CTYPE(
  SKU int,
  Name String
);

CREATE STREAM skuStream OF CTYPE;

CREATE OR REPLACE CACHE BKRequestLookup USING DatabaseReader (
ConnectionURL:'jdbc:oracle:thin:@10.1.186.105:1521:orcl',
Username:'GOPI',
Password:'gopi',
Query: "
SELECT  PI.SKU, PI.NAME FROM GOPI.PRODUCT_INV PI"
) QUERY (keytomap:'SKU', refreshinterval: '60000000',charset:'UTF-8',replicas:1) OF CTYPE;

END APPLICATION CACHEBACKEDWITHDB;
DEPLOY APPLICATION CACHEBACKEDWITHDB;

USE ADMIN;

CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @APPNAME@_Source USING Global.MSSqlReader (
  TransactionSupport: false,
  _h_returnNumericAs: 'Double',
  Tables: 'qatest.T27342_Source',
  FetchTransactionMetadata: false,
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
  Compression: false,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Password_encrypted: 'false',
  Password: 'w3b@ct10n',
  StartPosition: 'NOW',
  adapterName: 'MSSqlReader',
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'qatest',
  Username: 'qatest',
  FetchSize: 0,
  IntegratedSecurity: false,
  FilterTransactionBoundaries: true,
  ConnectionPoolSize: 2,
  SendBeforeImage: true,
  AutoDisableTableCDC: false )
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_Target USING Global.AzureSQLDWHWriter (
  AccountName: 'testaswin',
  Tables: 'qatest.T27342_Source,dbo.test',
  Password_encrypted: 'false',
  AccountAccessKey_encrypted: 'false',
  CDDLAction: 'Process',
  StorageAccessDriverType: 'WASBS',
  ConnectionURL: 'jdbc:sqlserver://testaswin.database.windows.net:1433;database=testaswin',
  Username: 'testaswin',
  columnDelimiter: '|',
  Mode: 'MERGE',
  AccountAccessKey: 'MmlfH35Vc2mcOScbY2wnOyXol6deT8gtGA4XW3C5EXwwdFQEukP37RfHGWeUgMhfKsIvDvCHF/v3GF6frXGdYg==',
  Password: 'W3b@ct10n2020',
  uploadpolicy: 'eventcount:1,interval:5m' )
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_sysout USING Global.SysOut (
  name: 'sysout_1' )
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

STOP APPLICATION DBRTOCW;
UNDEPLOY APPLICATION DBRTOCW;
DROP APPLICATION DBRTOCW CASCADE;
CREATE APPLICATION DBRTOCW;

CREATE TYPE employee(
id String,
ename String
);

CREATE STREAM Oracle_ChangeDataStream of employee;

create source CSVSource using FileReader (
	directory:'/Users/jenniffer/Product2/IntegrationTests/TestData/',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (

)
OUTPUT TO FileStream;

CREATE CQ CQfilter
INSERT INTO Oracle_ChangeDataStream
select data[0],data[1] from FileStream;


create Target t2 using SysOut(name:OrgData) input from Oracle_ChangeDataStream;
CREATE OR REPLACE Target DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1000,Interval:60',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  Tables: 'test.employee',
  Password: 'cassandra',
  Password_encrypted: false
 )INPUT FROM Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;
DEPLOY APPLICATION DBRTOCW;
START APPLICATION DBRTOCW;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

create stream @stream1@ of Global.WAEvent;

CREATE TYPE @type@(
  id String,
  name String  
);

CREATE STREAM @typeStream@ OF @type@;

CREATE CQ @CQName1@
INSERT INTO @typeStream@
SELECT TO_STRING(p.data[0]), 
       TO_STRING(p.data[1])
FROM @SRCINPUTSTREAM@ p;

create Target @targetsys1@ using SysOut(name:TypeOut) input from @typeStream@;

create cq @CQName2@ 
insert into @stream1@
select convertTypedeventToWAevent(c, 'admin.@type@')
from @typeStream@ c;

create stream @CQOUTPUTSTREAM@ of Global.WAEvent;

CREATE OR REPLACE CQ @CQName@ INSERT INTO @CQOUTPUTSTREAM@ select @FUNCTION@ from @stream1@  s ;

create Target @targetsys2@ using SysOut(name:EventOut) input from @CQOUTPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:1',
  Tables: '@TargetTableMapping@'
 ) 
INPUT FROM @CQOUTPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;

drop propertyset OrcToOrcPlatfm_App_KafkaPropset;
drop stream  OrcToOrcPlatfm_Stream CASCADE;


--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;

CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNEC4TION_URL@',
  Tables: '$table1',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING AzureSQLDWHWriter  (
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'QATEST.ORCTOORCPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING AzureSQLDWHWriter  (
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
 )
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '$table2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@4',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING AzureSQLDWHWriter  (
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'QATEST.ORCTOORCPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING AzureSQLDWHWriter  (
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
 )
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;

END APPLICATION @APPNAME@2;

stop FileReaderToKuduWriter;
undeploy application FileReaderToKuduWriter;
drop application FileReaderToKuduWriter cascade;

CREATE APPLICATION FileReaderToKuduWriter recovery 5 second interval ;;

CREATE OR REPLACE SOURCE CSVPoller USING FileReader (
        directory:'/Users/Striim/',
        WildCard:'typetest.csv',
        positionByEOF:false
)
parse using DSVParser (
        header:'yes'
)
OUTPUT TO CsvStream;

CREATE OR REPLACE TYPE CSVStream_Type  ( ID1 String KEY,
ID2 String
);

CREATE OR REPLACE STREAM CSVTypeStream OF CSVStream_Type;

CREATE OR REPLACE CQ CQ1
INSERT INTO CSVTypeStream
SELECT TO_STRING(data[0]),TO_STRING(data[1])
FROM CsvStream;

CREATE TARGET WriteintoKudu using KuduWriter (
KuduClientConfig:'master.addresses->192.168.56.101:7051;socketreadtimeout->10000;operationtimeout->30000',
Tables: 'INTEGRATIONTEST',
BatchPolicy: 'EventCount:1,Interval:10') INPUT FROM CSVTypeStream;

END APPLICATION FileReaderToKuduWriter;
deploy application FileReaderToKuduWriter;
start FileReaderToKuduWriter;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE OR REPLACE TYPE @appname@CQOUT1_Type (
 companyName java.lang.String,
 merchantId java.lang.String,
 dateTime org.joda.time.DateTime,
 hourValue java.lang.String,
 amount java.lang.String,
 zip java.lang.String,
 FileName java.lang.String);

CREATE SOURCE @parquetsrc@ USING S3Reader (
    bucketname:'',
    objectnameprefix:''
     )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE OR REPLACE STREAM @appname@CQOUT1 OF @appname@CQOUT1_Type;
CREATE OR REPLACE CQ @appname@CQ_PQEvent
INSERT INTO @appname@CQOUT1
    Select
    data.get("companyName").toString(),
    data.get("merchantId").toString(),
    TO_DATE(data.get("dateTime").toString()),
    data.get("hourValue").toString(),
    data.get("amount").toString(),
    data.get("zip").toString(),
    metadata.get("FileName").toString()
    FROM @appname@Stream p;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using AvroFormatter (
schemaFileName: 'AvroS3Schema'
)
input from @appname@CQOUT1;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using JSONFormatter ()
INPUT FROM @appname@CQOUT1;


CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using JSONFormatter (
)
INPUT FROM @appname@CQOUT1;

CREATE OR REPLACE TARGET @dbtarget@ USING DatabaseWriter (
  Tables: '',
  ConnectionURL:'',
  Username:'',
  Password:'',
  CommitPolicy: 'EventCount:1,Interval:0',
  BatchPolicy:'EventCount:1,Interval:0'
)
INPUT FROM @appname@CQOUT1;

CREATE TARGET @bqtarget@ USING BigQueryWriter (
  Tables: '',
  projectId:'',
  BatchPolicy: 'eventCount:1, Interval:1',
  ServiceAccountKey: '',
   )
INPUT FROM @appname@CQOUT1;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@;
CREATE SOURCE @srcName@ USING Global.OracleReader ( 
  Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@') 
OUTPUT TO @instreamname@;

CREATE OR REPLACE TARGET @tgtName@ USING Global.ServiceNowWriter ( 
  MaxConnections: 20, 
  ClientSecret: '@clientsecret@', 
  ApplicationErrorCountThreshold: 0, 
  BatchPolicy: 'eventCount:10000, Interval:60', 
  ConnectionUrl: '@tgturl@', 
  Password: '@tgtpassword@', 
  BatchAPI: false,  
  ConnectionTimeOut: 60, 
  Mode: 'APPENDONLY', 
  ClientID: '@clientid@', 
  Tables: '@srcschema@.@srctable@,@tgttable@ COLUMNMAP()', 
  ConnectionRetries: 3, 
  useConnectionProfile: false, 
  UserName: '@tgtusername@', 
  adapterName: 'ServiceNowWriter' ) 
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop APPLICATION @AppName@;
Undeploy APPLICATION @AppName@;
drop APPLICATION @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @AgentFlow@1;
CREATE OR REPLACE SOURCE @SourceName@1 USING Global.MSJet (
  Tables: 'dbo.compsrc',
  username: 'qatest',
  DatabaseName: 'qatest',
  FetchTransactionMetadata: true,
  filterTransactionBoundaries: true,
  compression: false,
  ConnectionURL: '@ConnectionURL@1',
  Mode: '@mode@',
  CommittedTransactions: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
  SendBeforeImage: true,
  password: 'w3b@ct10n' )
OUTPUT TO @StreamName@;
END FLOW @AgentFlow@1;

CREATE FLOW @AgentFlow@2;
CREATE OR REPLACE SOURCE @SourceName@2 USING Global.MSJet (
  Tables: 'dbo.compsrc',
  username: 'qatest',
  DatabaseName: 'qatest',
  FetchTransactionMetadata: true,
  filterTransactionBoundaries: true,
  compression: false,
  ConnectionURL: '@ConnectionURL@2',
  CaptureType: '@captureType@',
  CommittedTransactions: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
  SendBeforeImage: true,
  password: 'w3b@ct10n' )
OUTPUT TO @StreamName@;
END FLOW @AgentFlow@2;

CREATE TARGET @SysTarget@ USING Global.SysOut (
  name: 'MS_CDC_SYSOUT' )
INPUT FROM @StreamName@;

CREATE FLOW @ServerFlow@1;

CREATE TARGET @TargetName@1 USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ParallelThreads: '',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: '@ConnectionURL@',
  StatementCacheSize: '50',
  DatabaseProviderType: 'Default',
  Username: 'qatest',
  Tables: 'dbo.compsrc,dbo.comptar',
  Password: 'w3b@ct10n',
  PreserveSourceTransactionBoundary: 'false',
  BatchPolicy: 'EventCount:1,Interval:60' )
INPUT FROM @StreamName@;

END FLOW @ServerFlow@1;

END APPLICATION @AppName@;
DEPLOY APPLICATION @AppName@ with @AgentFlow@1 in AGENTS, @AgentFlow@2 in AGENTS, @ServerFlow@1 on any in default;
START APPLICATION @AppName@;

stop application MySQLToSQLServer;
undeploy application MySQLToSQLServer;
drop application MySQLToSQLServer cascade;

CREATE APPLICATION MySQLToSQLServer recovery 1 second interval;

create source Src1ReadFromMySQL USING MySQLReader (
Username: 'root',
  Password: 'w@ct10n',
  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',
--Tables: 'waction.mytable1;waction.mytable2;qatest.mysqlmarker',
Tables: 'waction.Parent%;waction.Child%;',
--Tables: 'waction.mytable%',
BiDirectionalMarkerTable: 'waction.mysqlmarker',
compression: 'True',
sendBeforeImage:True
) OUTPUT TO App1Stream;


CREATE  TARGET Src1ReadFromMySQL_Out USING FileWriter  (
  filename: 'Src1ReadFromMySQL_Out.log',
flushpolicy: 'eventcount:1',
rolloverpolicy: 'eventcount:10000'
 )
FORMAT USING JSONFORMATTER  (
 )
INPUT FROM App1Stream;


CREATE TARGET WriteToMSSQL1 USING DatabaseWriter(
ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',
Username:'qatest',
Password:'w3b@ct10n',
BatchPolicy:'EventCount:8,Interval:5',
CommitPolicy:'EventCount:10,Interval:5',
BiDirectionalMarkerTable: 'qatest.mysqlmarker',
--Tables: 'waction.mytable1,qatest.mytable1;'
Tables: 'waction.Parent%,qatest.%;waction.Child%,qatest.%;'
--Tables: 'waction.mytable%,qatest.%'
)
INPUT FROM App1Stream;


--MSSQl tp MySQL

Create Source SrcReadFromMSSQL
Using MSSqlReader
(
Username:'qatest',
Password:'w3b@ct10n',
DatabaseName:'qatest',
ConnectionURL:'localhost:1433',
--Tables:'qatest.mytable1;qatest.mytable2;qatest.mysqlmarker;qatest.mysqlmarker2',
Tables:'qatest.Parent%;qatest.Child%',
--Tables:'qatest.mytable%',
BiDirectionalMarkerTable: 'qatest.mysqlmarker',
TransactionSupport: 'true',
FilterTransactionBoundaries: true,
Compression: 'True',
ConnectionPoolSize:1
)
Output To App2Stream;


CREATE  TARGET SrcReadFromMSSQL_Out USING FileWriter  (
  filename: 'SrcReadFromMSSQL_Out.log',
flushpolicy: 'eventcount:1',
rolloverpolicy: 'eventcount:10000'
 )
FORMAT USING JSONFORMATTER  (
 )
INPUT FROM App2Stream;


CREATE TARGET WriteToMySQL USING DatabaseWriter(
ConnectionURL:'jdbc:mysql://localhost:3306/waction',
Username:'root',
Password:'w@ct10n',
BatchPolicy:'EventCount:10,Interval:10',
CommitPolicy: 'EventCount:15,Interval:12',
BiDirectionalMarkerTable: 'waction.mysqlmarker',
--Tables: 'qatest.mytable1,waction.mytable1;qatest.mytable2,waction.mytable3;'
Tables: 'qatest.Parent_1,waction.Parent_1;qatest.Parent_2,waction.Parent_2;qatest.Child_1,waction.Child_1;qatest.Child_2,waction.Child_2;'
--Tables: 'qatest.mytable%,waction.%'
)
INPUT FROM App2Stream;

/*
CREATE OR REPLACE TARGET MSSQLDDLFileOut USING FileWriter  (
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  filename: 'MSSQL.txt'
 )
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 )
INPUT FROM App2Stream;

*/

END APPLICATION MySQLToSQLServer;
deploy application MySQLToSQLServer;
start application MySQLToSQLServer;

--
-- Crash Recovery Test 2 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS
--

STOP APPLICATION KStreamN2S2CR2Tester.KStreamN2S2CRTest2;
UNDEPLOY APPLICATION KStreamN2S2CR2Tester.KStreamN2S2CRTest2;
DROP APPLICATION KStreamN2S2CR2Tester.KStreamN2S2CRTest2 CASCADE;

DROP USER KStreamN2S2CR2Tester;
DROP NAMESPACE KStreamN2S2CR2Tester CASCADE;
CREATE USER KStreamN2S2CR2Tester IDENTIFIED BY KStreamN2S2CR2Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR2Tester;
CONNECT KStreamN2S2CR6Tester KStreamN2S2CR2Tester;

CREATE APPLICATION KStreamN2S2CRTest2 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest2;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest2;

CREATE FLOW DataProcessingKStreamN2S2CRTest2;

CREATE TYPE WactionTypeKStreamN2S2CRTest2 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionTypeKStreamN2S2CRTest2;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest2 CONTEXT OF WactionTypeKStreamN2S2CRTest2
EVENT TYPES ( WactionTypeKStreamN2S2CRTest2 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsKStreamN2S2CRTest2
INSERT INTO WactionsKStreamN2S2CRTest2
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingKStreamN2S2CRTest2;

END APPLICATION KStreamN2S2CRTest2;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  IgnorableExceptionCode: '1,TABLE_NOT_FOUND'
 )
INPUT FROM @STREAM@;

STOP APPLICATION snow2pg;
UNDEPLOY APPLICATION snow2pg;
DROP APPLICATION snow2pg CASCADE;


CREATE OR REPLACE APPLICATION snow2pg;

CREATE OR REPLACE SOURCE snow_pg USING Global.ServiceNowReader (
  Mode: 'InitialLoad',
  ServiceNow.ConnectionTimeOut: 60,
  ServiceNow.MaxConnections: 20,
  ServiceNow.FetchSize: 10000,
  ThreadPoolCount: '10',
  ServiceNow.ConnectionRetries: 3,
  PollingInterval: '1',
  ClientSecret: '6Wa-cv`I7x',
  Password: '^Pre&$EMO%6O.e_{96h+$R?rJd,=[4Vt=K)Szh?6g<J9D3,3zs8R;hpZqh]-3?C&.u-@GvSakPXH1:2eygbBDI>ou-z#GjBw[u8x',
  ServiceNow.Tables: 'u_empl',
  UserName: 'snr',
  ClientID: 'ce4fd5af894a11103d2c5c3a8fe075e1',
  adapterName: 'ServiceNowReader',
  ServiceNow.BatchAPI: false,
  ServiceNow.ConnectionUrl: 'https://dev84954.service-now.com/' )
OUTPUT TO sn;

CREATE TARGET pg USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'w@ct10n',
  Tables: 'u_empl,u_empl ColumnMap(name=u_name,age=u_age,address=u_address,sys_id=sys_id)',
  ParallelThreads: '',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  CommitPolicy: 'EventCount:1000,Interval:60',
  StatementCacheSize: '50',
  Username: 'waction',
  DatabaseProviderType: 'Postgres',
  BatchPolicy: 'EventCount:1000,Interval:60',
  PreserveSourceTransactionBoundary: 'false' )
INPUT FROM sn;

END APPLICATION snow2pg;
deploy application snow2pg;
start snow2pg;

STOP Jumping1Tester.Jumping1;
UNDEPLOY APPLICATION Jumping1Tester.Jumping1;
DROP APPLICATION Jumping1Tester.Jumping1 CASCADE;
CREATE APPLICATION Jumping1;

create source CsvSource1 using FileReader
(
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'WindowsTest.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
)
 parse using DSVParser
(
	header:'yes',
	columndelimiter:','
)
OUTPUT TO CsvStream1;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);
CREATE TYPE CsvData1 (
  zip double
);

CREATE TYPE WactionData1 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData2 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData3 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData5 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData6 (
  zip double
);
CREATE TYPE WactionData7 (
  zip double
);
CREATE TYPE WactionData8 (
  zip double
);

CREATE STREAM DataStream1 OF CsvData;

CREATE STREAM DataStream2 OF CsvData
PARTITION BY companyName;

CREATE STREAM DataStream3 OF CsvData
PARTITION BY city;

CREATE STREAM DataStream4 OF CsvData;
CREATE STREAM DataStream5 OF CsvData
PARTITION BY city;

CREATE STREAM DataStream6 OF CsvData1;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData3
INSERT INTO DataStream3
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData4
INSERT INTO DataStream4
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData5
INSERT INTO DataStream5
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData6
INSERT INTO DataStream6
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

CREATE CQ CsvToData7
INSERT INTO DataStream7
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

CREATE CQ CsvToData8
INSERT INTO DataStream8
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

-- Count based jumping window
CREATE JUMPING WINDOW DataStreamCount
OVER DataStream1 KEEP 5 ROWS;

-- Time based jumping window
CREATE JUMPING WINDOW DataStreamTime OVER DataStream2 KEEP
within 40 second
PARTITION BY companyName;

-- Attribute based jumping window
CREATE JUMPING WINDOW DataStreamAtrribute
OVER DataStream3 KEEP
range 5 minute
ON dateTime
PARTITION BY city;

-- Count + time based jumping window
CREATE JUMPING WINDOW DataStreamCountTime
OVER DataStream4 KEEP
5 rows
within 8 minute;

-- Attribute + time based jumping window
CREATE JUMPING WINDOW DataStreamAttributeTime
OVER DataStream5 KEEP
range 300 second
ON dateTime
within 5 minute
PARTITION BY city;

-- Attribute + time based jumping window using COUNT
CREATE JUMPING WINDOW DataStreamAttributeTime1
OVER DataStream5 KEEP
range 300 second
ON dateTime
within 5 minute;

CREATE WACTIONSTORE Wactions1 CONTEXT OF WactionData1
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions2 CONTEXT OF WactionData2
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions3 CONTEXT OF WactionData3
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions4 CONTEXT OF WactionData4
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions5 CONTEXT OF WactionData5
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions6 CONTEXT OF WactionData6
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions7 CONTEXT OF WactionData7
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions8 CONTEXT OF WactionData8
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE CQ Data1ToWaction
INSERT INTO Wactions1
SELECT
	p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamCount p;

CREATE CQ Data2ToWaction
INSERT INTO Wactions2
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamTime p
group by companyName;

CREATE CQ Data3ToWaction
INSERT INTO Wactions3
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamAtrribute p;

CREATE CQ Data4ToWaction
INSERT INTO Wactions4
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamCountTime p;

CREATE CQ Data5ToWaction
INSERT INTO Wactions5
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamAttributeTime p
group by city;

CREATE CQ Data6ToWaction
INSERT INTO Wactions6
SELECT
    count(*)
FROM DataStreamCount p;

CREATE CQ Data7ToWaction
INSERT INTO Wactions7
SELECT
    count(*)
FROM DataStreamCountTime p;

CREATE CQ Data8ToWaction
INSERT INTO Wactions8
SELECT
    count(*)
FROM DataStreamAttributeTime1 p;


END APPLICATION Jumping1;

stop application RedshiftColmap;
undeploy application RedshiftColmap;
drop application RedshiftColmap CASCADE;
create application RedshiftColmap recovery 1 second interval;
CREATE OR REPLACE SOURCE OracleSource USING OracleReader  (
  Username: 'miner',
  Password: 'miner',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.redshift_emp',
  FetchSize: 1
 )
OUTPUT TO LogminerStream;
CREATE  TARGET RedshiftTarget USING RedshiftWriter  (
  ConnectionURL: '@URL@',
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  bucketname: 'striim-s3',
  --accesskeyId: 'AKIAJFHRZ7VIH2GHON5A',
  --secretaccesskey: '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',
  S3IAMRole:'@IAMROLE@',
  Tables: 'QATEST.redshift_emp,qatest.EMPLOYEE COLUMNMAP(E_DOJ = DOJ, E_NAME = NAME,E_ID = ID)',
  uploadpolicy: 'eventcount:3,interval:5s',
  QuoteCharacter: '@QUOTECHARACTER@',
  Mode: 'incremental',
  ColumnDelimiter: '|'
 )
INPUT FROM LogminerStream;
END APPLICATION RedshiftColmap;
deploy application RedshiftColmap;
START application RedshiftColmap;

-- Creating a namespace ensures there won't be conflicts with the regular version of
-- PosApp. The only difference between this version and the regular version is
-- that the CQ that parses the source stream includes a PAUSE clauses that introduces a
-- 40-millisecond pause after each event is read, simulating the way the dashboard would
-- work with real-time data.
Stop PosAppOracle.PosAppOracle;
undeploy application PosAppOracle.PosAppOracle;
drop application PosAppOracle.PosAppOracle cascade;


-- The PosApp sample application demonstrates how a credit card
-- payment processor might use WebAction to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.

CREATE Application PosAppOracle;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING OracleReader (
OnlineCatalog:true,
FetchSize:1000,
QueueSize:2048,
CommittedTransactions:false,
Compression:false,
Username:'@LOGMINER-UNAME@',
Password:'@LOGMINER-PASSWORD@',
ConnectionURL:'@LOGMINER-URL@',
Tables:'@LOGMINER-SCHEMA@.POSDATA',
OnlineCatalog:true
) output to CsvStream;


-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells WebAction that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells WebAction to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvToPosData
--
-- A stream's type must be declared before the stream, and a CQ's
-- output stream must be defined before the CQ. Hence type-stream-CQ
-- sequences like the following are very common.

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATE(data[4]) as dateTime,
       DHOURS(TO_DATE(data[4])) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       To_String(data[9]) as zip
FROM CsvStream c;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP. These correspond to the merchantId,
-- dateTime, hourValue, amount, and zip fields in PosDataStream, as
-- defined by the PosData type.
--
-- The DATETIME field from the source is converted to both a DateTime
-- value, used as the event timestamp by the application, and an int,
-- which is used to look up historical hourly averages from the
-- HourlyAveLookup cache, discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)

-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateOnly
--
-- The PosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue int,
  hourlyAve int
);

CREATE CACHE HourlyAveLookup using DatabaseReader (
        ConnectionURL:'@READER-URL@',
        Username:'@READER-UNAME@',
        Password:'@READER-PASSWORD@',
        Table:'@READER-SCHEMA@.HOURLYDATA',
        FetchSize:12000
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId as merchantId,
       p.zip as zip,
       FIRST(p.dateTime) as startingTime,
       COUNT(p.merchantId) as count,
       SUM(p.amount) as totalAmount,
       l.hourlyAve/12 as hourlyAve,
       l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END as upperLimit,
       l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END as lowerLimit,
       '<NOTSET>' as category,
       '<NOTSET>' as status
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId as merchantId,
       zip as zip,
       startingTime as startingTime,
       count as count,
       totalAmount as totalAmount,
       hourlyAve as hourlyAve,
       upperLimit as upperLimit,
       lowerLimit as lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END as category,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END as status
FROM MerchantTxRateOnlyStream;


-- WAction store MerchantActivity
--
-- The following group of statements create and populate the MerchantActivity
-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced
-- with merchant details from NameLookup cache and with latitude and longitude
-- values from the USAddressData cache.

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count int,
  HourlyAve int,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);
CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count int,
  totalAmount double,
  hourlyAve int,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
@PERSIST-TYPE@

CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using DatabaseReader (
        ConnectionURL:'@READER-URL@',
        Username:'@READER-UNAME@',
        Password:'@READER-PASSWORD@',
        Table:'@READER-SCHEMA@.MERCHANTNAMES',
        FetchSize:12000
) QUERY (keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using DatabaseReader (
        ConnectionURL:'@READER-URL@',
        Username:'@READER-UNAME@',
        Password:'@READER-PASSWORD@',
        Table:'@READER-SCHEMA@.USADDRESSES',
        FetchSize:12000
) QUERY (keytomap:'zip') OF USAddressData;

CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;


-- CQ GenerateAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;


-- The following statement loads visualization (Dashboard) settings
-- from a file.


CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;


END APPLICATION PosAppOracle;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE application @APPNAME@ @Recovery@ AUTORESUME MAXRETRIES 2 RETRYINTERVAL 10;

create type @APPNAME@type1(
  companyName String,
  merchantId String,
  city string
);

create type @APPNAME@type2(
  c1 integer,
  c2 String,
  c3 string
);

create type @APPNAME@type3(
c1 integer
);

create type @APPNAME@type4(
c1 integer,
c2 integer
);

create stream @APPNAME@in_memory_typedStream of @APPNAME@type1 partition by city;
create stream @APPNAME@in_memory_typedStream_num of @APPNAME@type2;
create stream @APPNAME@in_memory_typedStream_num1 of @APPNAME@type2;
create stream @APPNAME@in_memory_typedStream_num2 of @APPNAME@type2;
create stream @APPNAME@in_memory_typedStream_num3 of @APPNAME@type2;
create stream @APPNAME@in_memory_typedStream_num4 of @APPNAME@type2;
create stream @APPNAME@in_memory_typedStream_num5 of @APPNAME@type2;
create stream @APPNAME@finalstream6 of @APPNAME@type4;

create source @APPNAME@s using FileReader (
        directory:'Product/IntegrationTests/TestData/',
        wildcard:'posdata5L.csv',
        positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:',',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  header: true,
  separator:'~'

)
OUTPUT TO @APPNAME@in_memory_rawStream;


create CQ @APPNAME@cq1
INSERT INTO @APPNAME@kps_waevent
SELECT *
FROM @APPNAME@in_memory_rawStream  ;

create CQ @APPNAME@cq2
INSERT INTO @APPNAME@in_memory_typedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", ""),
TO_STRING(data[1]),
TO_STRING(data[10])
FROM @APPNAME@kps_waevent ;

create CQ @APPNAME@cq3
INSERT INTO @APPNAME@in_memory_typedStream_num1
SELECT TO_INT(companyName) as c1, merchantId as c2,city as c3
FROM @APPNAME@in_memory_typedStream;
-- order by c3;

create CQ @APPNAME@cq4
INSERT INTO @APPNAME@in_memory_typedStream_num2
SELECT TO_INT(companyName) as c1, merchantId as c2,city as c3
FROM @APPNAME@in_memory_typedStream;

create CQ @APPNAME@cq5
INSERT INTO @APPNAME@in_memory_typedStream_num3
SELECT TO_INT(companyName) as c1, merchantId as c2,city as c3
FROM @APPNAME@in_memory_typedStream;

create CQ @APPNAME@cq6
INSERT INTO @APPNAME@in_memory_typedStream_num4
SELECT TO_INT(companyName) as c1, merchantId as c2,city as c3
FROM @APPNAME@in_memory_typedStream;

create CQ @APPNAME@cq7
INSERT INTO @APPNAME@in_memory_typedStream_num5
SELECT TO_INT(companyName) as c1, merchantId as c2,city as c3
FROM @APPNAME@in_memory_typedStream;

CREATE CQ @APPNAME@cq8
INSERT INTO @APPNAME@in_memory_typedStream_num6
SELECT TO_INT(companyName) as c1
FROM @APPNAME@in_memory_typedStream;


CREATE JUMPING WINDOW @APPNAME@DataStream1_100000Rows
OVER @APPNAME@in_memory_typedStream_num1 KEEP 100000 ROWS;


CREATE JUMPING WINDOW @APPNAME@DataStream2_100000Rows
OVER @APPNAME@in_memory_typedStream_num2 KEEP 100000 ROWS;


CREATE JUMPING WINDOW @APPNAME@DataStream3_100000Rows
OVER @APPNAME@in_memory_typedStream_num3 KEEP 100000 ROWS;


CREATE JUMPING WINDOW @APPNAME@DataStream4_100000Rows
OVER @APPNAME@in_memory_typedStream_num4 KEEP 100000 ROWS;


CREATE JUMPING WINDOW @APPNAME@DataStream5_100000Rows
OVER @APPNAME@in_memory_typedStream_num5 KEEP 100000 ROWS;

CREATE JUMPING WINDOW @APPNAME@DataStream6_100000Rows
OVER @APPNAME@in_memory_typedStream_num6 KEEP 100000 ROWS;

create CQ @APPNAME@cq9
INSERT INTO @APPNAME@finalstream1
SELECT c1 FROM @APPNAME@DataStream1_100000Rows sample by c1;

create CQ @APPNAME@cq10
INSERT INTO @APPNAME@finalstream2
SELECT c1 FROM @APPNAME@DataStream2_100000Rows sample by c1 selectivity 0.1;

create CQ @APPNAME@cq11
INSERT INTO @APPNAME@finalstream3
SELECT c1 FROM @APPNAME@DataStream3_100000Rows sample by c1 selectivity 0.25;


create CQ @APPNAME@cq12
INSERT INTO @APPNAME@finalstream4
SELECT c1 FROM @APPNAME@DataStream4_100000Rows sample by c1 selectivity 0.05;
--SELECT count(*) FROM @APPNAME@DataStream4Rows10000Seconds sample by c1 selectivity 0.05;

create CQ @APPNAME@cq13
INSERT INTO @APPNAME@finalstream5
SELECT c1 FROM @APPNAME@DataStream5_100000Rows sample by c1 selectivity 0.01;

create CQ @APPNAME@cq14
INSERT INTO @APPNAME@finalstream6
SELECT c1,c1 as c2 FROM @APPNAME@DataStream6_100000Rows sample by c1,c2 selectivity 0.01;

create target @APPNAME@target1 using filewriter (
filename:'FEATURE-DIR/logs/@APPNAME@target1.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using dsvFormatter()
input from @APPNAME@finalstream1;

create target @APPNAME@target2 using filewriter (
filename:'FEATURE-DIR/logs/@APPNAME@target2.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using dsvFormatter()
input from @APPNAME@finalstream2;

create target @APPNAME@target3 using filewriter (
filename:'FEATURE-DIR/logs/@APPNAME@target3.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using dsvFormatter()
input from @APPNAME@finalstream3;

create target @APPNAME@target4 using filewriter (
filename:'FEATURE-DIR/logs/@APPNAME@target4.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using dsvFormatter()
input from @APPNAME@finalstream4;

create target @APPNAME@target5 using filewriter (
filename:'FEATURE-DIR/logs/@APPNAME@target5.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using dsvFormatter()
input from @APPNAME@finalstream5;

CREATE WACTIONSTORE @APPNAME@Wactions1 CONTEXT OF @APPNAME@type3
EVENT TYPES ( @APPNAME@type2 )
USING ( storageProvider:'elasticsearch' );

CREATE WACTIONSTORE @APPNAME@Wactions2 CONTEXT OF @APPNAME@type3
EVENT TYPES ( @APPNAME@type2 )
USING ( storageProvider:'elasticsearch' );

CREATE WACTIONSTORE @APPNAME@Wactions3 CONTEXT OF @APPNAME@type3
EVENT TYPES ( @APPNAME@type2 )
USING ( storageProvider:'elasticsearch' );

CREATE WACTIONSTORE @APPNAME@Wactions4 CONTEXT OF @APPNAME@type4
EVENT TYPES ( @APPNAME@type4 )
USING ( storageProvider:'elasticsearch' );

CREATE WACTIONSTORE @APPNAME@Wactions5 CONTEXT OF @APPNAME@type4
EVENT TYPES ( @APPNAME@type4 )
USING ( storageProvider:'elasticsearch' );

--sampling twice: one in finalstream1 and another in select query.
CREATE CQ @APPNAME@cq15
INSERT INTO @APPNAME@Wactions1
SELECT FIRST(p.c1) FROM @APPNAME@finalstream1 p GROUP BY p.c1 sample by p.c1 ;

--sampling once: results will be same as target2 and target1.
CREATE CQ @APPNAME@cq16
INSERT INTO @APPNAME@Wactions2
SELECT * from @APPNAME@finalstream1 order by c1 desc limit 10000 ;

--sampling twice: one in finalstream1 and another in select query.
CREATE CQ @APPNAME@cq17
INSERT INTO @APPNAME@Wactions3
SELECT * from @APPNAME@finalstream1 order by c1 sample by c1;

--sampling using 2 fields, 2800 for single field and 332 for 2 field
CREATE CQ @APPNAME@cq18
INSERT INTO @APPNAME@Wactions4
SELECT c1,c1 from @APPNAME@finalstream6 order by c1 sample by c1;

--same as Wactions4 - here selectivity alone varies, so output is 8
CREATE CQ @APPNAME@cq19
INSERT INTO @APPNAME@Wactions5
SELECT c1,c1 from @APPNAME@finalstream6 order by c1 sample by c1 selectivity 0.0001;

end application @APPNAME@;
deploy application @APPNAME@;
--start @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

stop application @appname@Out;
undeploy application @appname@Out;
drop application @appname@Out cascade;

drop stream @appname@KafkaStream;
CREATE OR REPLACE PROPERTYSET @appname@KafkaPropset (zk.address:@keeper@, bootstrap.brokers:@broker@, partitions:'50');
CREATE STREAM @appname@KafkaStream OF Global.parquetevent PERSIST USING @appname@KafkaPropset;
 
CREATE APPLICATION @appname@;

CREATE OR REPLACE SOURCE @parquetsrc@ USING Global.FileReader ( 
  directory: '', 
  wildcard: '',
  positionbyeof: false ) 
PARSE USING Global.ParquetParser () 
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@KafkaStream
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;
    
END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE APPLICATION @appname@Out;

CREATE OR REPLACE TARGET @filetarget@ USING Global.FileWriter ( 
  directory: '', 
  filename: '' 
)
FORMAT USING Global.ParquetFormatter  ( 
  schemaFileName: 'parquetSchema' 
) 
INPUT FROM @appname@KafkaStream;

END APPLICATION @appname@Out;
deploy application @appname@Out on all in default;
start application @appname@Out;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'MINER.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@(
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:100,Interval:120',
  CommitPolicy: 'EventCount:100,Interval:120',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;


create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;

deploy application DBRTOCW on ANY in default;

start application DBRTOCW;

STOP application consoletest.noApp;
undeploy application consoletest.noApp;
drop application consoletest.noApp cascade;

DROP USER consoletest;
DROP NAMESPACE consoletest CASCADE;
CREATE USER consoletest IDENTIFIED BY consoletest;
REVOKE consoletest.admin from user consoletest;
GRANT create,drop ON deploymentgroup Global.consoletest To user consoletest;
GRANT all ON namespace Global.consoletest To user consoletest;
GRANT all ON Application consoletest.noApp To user consoletest;
REVOKE drop ON Application consoletest.noApp from user consoletest;
CONNECT consoletest consoletest;

create application noApp;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'posdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
directory:'@FEATURE-DIR@/logs/',
filename:'PermissionTarget',
sequence:'00',
rolloverpolicy:'eventcount:1222001'
)
format using DSVFormatter (

)
input from TypedCSVStream;
end application noApp;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
wildcard: '',
directory: '',
positionbyeof: false )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: ''
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

UNDEPLOY APPLICATION admin.LongRunningAppOracle;
DROP APPLICATION admin.LongRunningAppOracle cascade;

CREATE APPLICATION LongRunningAppOracle;


 --COMMENT::   Modify type attributes as desired.
 --COMMENT::   Type must be created first before creating a source using ranReader

CREATE TYPE RandomData(
  myName String,
  streetAddress String,
  bankName String,
  bankNumber int KEY,
  bankAmount double
);

CREATE source ranDataSource USING ranReader(
  OutputType:'LongRunningAppOracle.RandomData',
  TimeInterval:5,
  NoLimit:true,
  SampleSize:10000,
  DataKey:bankName,
  NumberOfUniqueKeys:500
) OUTPUT TO CSVDataStream;


CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4])
FROM CSVDataStream;


CREATE JUMPING WINDOW RandomData10Rows
OVER RandomDataStream KEEP 10 ROWS
PARTITION BY bankNumber;


CREATE TYPE myData(
  myName String,
  myAddress String,
  myBankName String,
  myBankNumber int KEY,
  myBankAmount double
);

CREATE STREAM myDataStream OF myData;

CREATE CQ GetMyData
INSERT INTO MyDataStream
SELECT myName, streetAddress, bankName, bankNumber, bankAmount
FROM RandomData10Rows WHERE bankNumber > 20000 AND bankNumber < 20300;


CREATE WACTIONSTORE MyDataActivity CONTEXT OF MyData
EVENT TYPES(myData )
PERSIST EVERY 10 second USING (
JDBC_DRIVER:'@WASTORE-DRIVER@',
JDBC_URL:'@WASTORE-URL@',
JDBC_USER:'@WASTORE-UNAME@',
JDBC_PASSWORD:'@WASTORE-PASSWORD@',
pu_name:oracle,
DDL_GENERATION:'drop-and-CREATE-tables'
);

Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
Select * from myDataStream
LINK SOURCE EVENT;


END APPLICATION LongRunningAppOracle;

STOP APPLICATION LoadMarketsAppTester.LoadMarketsApp;
UNDEPLOY APPLICATION LoadMarketsAppTester.LoadMarketsApp;
DROP APPLICATION LoadMarketsAppTester.LoadMarketsApp CASCADE;

CREATE APPLICATION LoadMarketsApp;

CREATE OR REPLACE SOURCE MarketSource USING FileReader (
 directory:'@TEST-DATA-PATH@',
  wildcard: 'marketlocations.json',
  blocksize: '64',
  charset: 'UTF-8',
  positionbyeof: 'false',
  rolloverpolicy: 'DefaultFileComparator',
  skipbom: 'true'
 )
 PARSE USING JSONParser (
  fieldName: 'locations'
 )
OUTPUT TO RawMarketLocationsStream;


CREATE OR REPLACE CQ ConvertRawShapes
INSERT INTO MarketStream
SELECT e.data.get("marketid").textValue() as id,
       e.data.get("marketname").textValue() as name,
       e.data.withArray("geopoints") as points,
       DNOW() as lastUpdated
FROM RawMarketLocationsStream e;

CREATE OR REPLACE CQ SplitLatLon
INSERT INTO PontStream
SELECT
  s.id as id,
  s.name as name,
  lat as lat ,
  lon as lon,
  s.lastUpdated as lastUpdated
FROM MarketStream s, iterator(s.points, (lat String, lon String)) a;

CREATE OR REPLACE WINDOW MarketWindow OVER MarketStream KEEP WITHIN 1 HOUR ON lastUpdated;


-- added by nambi ,Reason - for filtering out "lastUpdated", which has current time, is to have static expected results

CREATE OR REPLACE CQ SplitLatLon2
INSERT INTO PontStream2
SELECT
  s.id as id,
  s.name as name,
  lat as lat ,
  lon as lon
FROM MarketStream s, iterator(s.points, (lat String, lon String)) a;

CREATE TYPE wsData
(
  id String,
  name String,
  lat String,
  lon String
);


CREATE WACTIONSTORE oneWS
CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@

CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT
  id,
  name,
  lat ,
  lon
FROM PontStream2
LINK SOURCE EVENT;

END APPLICATION LoadMarketsApp;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.GGTrailReader (
  Tables:'@TABLES@',
  CDDLCapture: false,
  TrailDirectory: '@TRAIL_FILE_DIR@',
  TrailFilePattern: '@WILDCARD@',
  Compression: false,
  SupportColumnCharset: false,
  CDDLAction: 'Process',
  FilterTransactionBoundaries: true,
  adapterName: 'GGTrailReader',
  TrailByteOrder: '@ENDIAN@' )
OUTPUT TO @STREAM@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 USING Global.GGTrailReader (
  Tables:'@TABLES@',
  CDDLCapture: false,
  TrailDirectory: '@TRAIL_FILE_DIR@',
  TrailFilePattern: '@WILDCARD@',
  Compression: false,
  SupportColumnCharset: false,
  CDDLAction: 'Process',
  FilterTransactionBoundaries: true,
  adapterName: 'GGTrailReader',
  TrailByteOrder: '@ENDIAN@' )
OUTPUT TO @STREAM@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;


CREATE OR REPLACE SOURCE @APPNAME@CDC_Source1 USING OracleReader( 
  StartTimestamp: 'null',
  SupportPDB: false,
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  FetchSize: 1,
  DDLCaptureMode: 'All',
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.EMP',
  adapterName: 'OracleReader',
  Password: 'qatest',
  TransactionBufferType: 'Memory',
  DictionaryMode: 'OnlineCatalog',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '1MB',
  compression: true,
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@AppStream1;


CREATE OR REPLACE TARGET @APPNAME@sap_target USING DatabaseWriter( 
  DatabaseProviderType:'SAPHANA',
  ConnectionRetryPolicy: 'retryInterval=30,maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'SYSTEM',
  Password_encrypted: 'false',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:sap://10.77.21.116:39013/?databaseName=striim&currentSchema=QA',
  Tables: 'waction.crash_type,QA.CRASH_TYPES',
  adapterName: 'DatabaseWriter',
  --IgnorableExceptionCode: '',
  Password: 'Striim_SAP@123'
 ) 
INPUT FROM @APPNAME@AppStream1;


create or replace target @APPNAME@sys_tgt using sysout(
name:Foo2
)input from @APPNAME@AppStream1;

END APPLICATION @APPNAME@;

deploy application @APPNAME@;
start application @APPNAME@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING MySqlReader
(
  Compression: false,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'mysql://localhost:3306',
  DatabaseName: 'waction',
  Tables: 'waction.test01',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root'
)
OUTPUT TO @APPNAME@_SS;


CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'waction.test01,@DATASET@.% KEYCOLUMNS(id,name) COLUMNMAP(id=id,col1=col11,col2=col12)',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYVARIABLE Mode='sync';
CREATE OR REPLACE PROPERTYVARIABLE BatchPolicy='Size:900000,Interval:1';

create application KinesisTest;
create source CSVSource using FileReader (
	directory:'/home/dz/src/product/Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  companyName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0], data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	BatchPolicy: '$BatchPolicy',
    Mode: '$Mode'	
)
format using JSONFormatter (
)
input from TypedCSVStream;
end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[2]) = 'Null' ;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

stop application APP_KAFKASOURCE_AGG;
undeploy application APP_KAFKASOURCE_AGG;
alter application APP_KAFKASOURCE_AGG;

CREATE OR REPLACE CQ CQ_CALCULATE_HOURLY_TOTAL
INSERT INTO STREAM_CQ_CALCULATE_HOURLY_TOTAL
SELECT f.topic as topic, sum(f.rawdatacount) as TotalLast24hour, B.rawdatacount as TotalLast1hour FROM JUMP_WND_1EVT_1MIN h
       join SLIDE_WND_HOURLYTOTALS_KAFKADATA_FILE f on 1=1
       join (SELECT rawdatacount, topic,timerange from ET_HOURLYTOTALS_KAFKADATA_FILE,JUMP_WND_1EVT_30SEC where timerange = DHOURS(DNOW())-1) B on B.topic=f.topic
       Group by f.topic;

alter application APP_KAFKASOURCE_AGG recompile;

stop application GCSWriterTest;
undeploy application GCSWriterTest;
drop application GCSWriterTest cascade;
create application GCSWriterTest recovery 1 second interval;
create source GCS_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

create Target GCSTarget using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@',
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from CsvStream;

end application GCSWriterTest;
deploy application GCSWriterTest on all in default;
start application GCSWriterTest;

--
-- Recovery Multi Node Test 1 with uniqe objects to this App
-- Nicholas Keene, Bert Hashemi WebAction, Inc.
--
-- App with Single Source, CQ and Waction Store to be deployed on two or more node
-- Full app on node1 and one Source on node2 so source reads from node2 only.
--
-- S -> CQ -> WS
--


CREATE APPLICATION RecovTestMN01
RECOVERY 5 SECOND INTERVAL;



CREATE FLOW DataAcquisition;

CREATE SOURCE CsvSourceMN01 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamMN01;

END FLOW DataAcquisition;



CREATE FLOW DataProcessing;

CREATE TYPE WactionTypeMN01 (
  merchantId String,
  dateTime DateTime,
  amount double,
  city String,
  serverName String KEY
);

CREATE STREAM DataStreamMN01 OF WactionTypeMN01
PARTITION BY serverName;

CREATE CQ CsvToDataMN01
INSERT INTO DataStreamMN01
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10],
    data[11]
FROM CsvStreamMN01;

CREATE WINDOW DataWindowMN01
OVER DataStreamMN01 KEEP WITHIN 30 SECOND ON dateTime
PARTITION BY serverName;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionTypeMN01
EVENT TYPES ( WactionTypeMN01 )
PERSIST EVERY 1 second USING (
JDBC_DRIVER:'@WASTORE-DRIVER@',  JDBC_URL:'@WASTORE-URL@;CREATE=true',
JDBC_USER:'@WASTORE-UNAME@', JDBC_PASSWORD:'@WASTORE-PASSWORD@', pu_name:@WASTORE-TYPE@,
DDL_GENERATION:'create-or-extend-tables',  LOGGING_LEVEL:'SEVERE' );

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    *
FROM DataWindowMN01;

END FLOW DataProcessing;



END APPLICATION RecovTestMN01;


DEPLOY APPLICATION RecovTestMN01 WITH DataAcquisition ON ALL IN agents, DataProcessing ON ALL IN servers;

START RecovTestMN01;

STOP Mssqltobigquery;
UNDEPLOY APPLICATION Mssqltobigquery;
DROP APPLICATION Mssqltobigquery CASCADE;

CREATE APPLICATION Mssqltobigquery RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE Mssqltobigquery_source USING MSSqlReader
(
Username:'qatest',
Password:'w3b@ct10n',
DatabaseName:'qatest',
ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',
Tables:'qatest.srctb',
ConnectionPoolSize:1,
Compression:'true'
)
OUTPUT TO SS;


CREATE or replace TARGET Mssqltobigquery_Target USING BigQueryWriter (
ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  projectId:'striimqa-214712',
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:5,Interval:120',
  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', 
  AllowQuotedNewLines: 'false', 
  adapterName: 'BigQueryWriter', 
  optimizedMerge: 'true', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', 
  StandardSQL: 'true', 
  QuoteCharacter: '\"', 
  Tables: 'QATEST.srctb,.oratobqtgt',
  Mode: 'MERGE',
  StandardSQL: 'true',
  QuoteCharacter: '\"'
) INPUT FROM SS;

END APPLICATION Mssqltobigquery;
DEPLOY APPLICATION Mssqltobigquery;
START APPLICATION Mssqltobigquery;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@ RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE @parquetsrc@ USING FileReader (
wildcard: '',
directory: '',
positionbyeof: false )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: ''
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE APPLICATION  @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE  @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'JsonNodeEvent.json',
positionByEOF:false
)
PARSE USING Global.JSONParser (
 )  OUTPUT TO  @AppName@_rawstream;

CREATE CQ @BuiltinFunc@CQ
INSERT INTO  @BuiltinFunc@_Stream
SELECT @BuiltinFunc@(x, 'Sno', data.get("_id"), 'Name', data.get("firstname"))
FROM @AppName@_rawstream x;

CREATE OR REPLACE CQ cq1
INSERT INTO ClearUserData_Stream
SELECT
clearUserData(s1)
FROM @BuiltinFunc@_Stream s1;

CREATE OR REPLACE TARGET  @AppName@_FileTarget USING Global.FileWriter (
  flushpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
 directory: '@logs@',
  filename: '@BuiltinFunc@_JsonEventClearData',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  (
  handler: 'com.webaction.proc.JSONFormatter',
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  formatterName: 'JSONFormatter',
  jsonobjectdelimiter: '\n' )
INPUT FROM ClearUserData_Stream;

End application  @AppName@;
Deploy application  @AppName@;
Start application  @AppName@;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.OJet (
  PrimaryDatabasePassword: '@PRIMARY_PASSWORD@',
  ConnectionURL: '@DOWNSTREAM_URL@',
  PrimaryDatabaseUsername: '@PRIMARY_USER@',
  Password: '@DOWNSTREAM_PASSWORD@',
  DownstreamCaptureMode: 'REAL_TIME',
  DownstreamCapture: true,
  PrimaryDatabaseConnectionURL: '@PRIMARY_URL@',
  Tables: '@SOURCE_TABLES@',
  CDDLCapture: true,
  CDDLAction: 'Process',
  Username: '@DOWNSTREAM_USER@' )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET_SYS@ USING Global.SysOut (
  name: 'Out' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter (
  ConnectionURL: '@TARGET_URL@',
  Username: '@TARGET_USER@',
  Password: '@TARGET_PASSWORD@',
  CheckPointTable: 'CHKPOINT',
  CommitPolicy: 'EventCount:1',
  Tables: '@TARGET_TABLES@',
  BatchPolicy: 'EventCount:1' )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;
CREATE  SOURCE @FirstSourceName@ USING DatabaseReader  ( 
  ConnectionURL: '@SourceConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  Tables: '@SourceTable@',
  ReplicationSlotName: 'null'
 ) OUTPUT TO @SRCFirstINPUTSTREAM@;

 CREATE  SOURCE @MiddleSourceName@ USING DatabaseReader  ( 
  ConnectionURL: '@SourceConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  Tables: '@SourceTable@',
  ReplicationSlotName: 'null'
 )
OUTPUT TO @SRCMiddleINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCMiddleINPUTSTREAM@;

CREATE  TARGET @FirsttargetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
 ) 
 INPUT FROM @SRCFirstINPUTSTREAM@;

 CREATE  TARGET @MiddletargetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
 )
INPUT FROM @SRCMiddleINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

--
-- Recovery Test 22 with two sources, two sliding attribute windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sa5W -> CQ1 -> WS
-- S2 -> Sa6W -> CQ2 -> WS
--

STOP Recov22Tester.RecovTest22;
UNDEPLOY APPLICATION Recov22Tester.RecovTest22;
DROP APPLICATION Recov22Tester.RecovTest22 CASCADE;
CREATE APPLICATION RecovTest22 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION RecovTest22;

create Application UdpXml;
create source UdpXMLSource using UDPReader (
	IpAddress:'127.0.0.1',
	PortNo:'3546'
)
parse using XMLParser (
    RootNode:'/catalog/book'
)
OUTPUT TO UdpXMLStream;
create Target UdpDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/xmldata') input from UdpXMLStream;
end Application UdpXml;

--
-- Recovery Test 6 with sliding window and partitioned feature
-- Nicholas Keene, Bert Hashemi WebAction, Inc.
--
-- S -> CQ -> SW(partitioned) -> CQ(no aggregate) -> WS
--

STOP Recov6Tester.RecovTest6;
UNDEPLOY APPLICATION Recov6Tester.RecovTest6;
DROP APPLICATION Recov6Tester.RecovTest6 CASCADE;
CREATE APPLICATION RecovTest6 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvData PARTITION BY merchantId;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

CREATE WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION RecovTest6;

STOP APPLICATION OneAgentCQTester.CSV;
UNDEPLOY APPLICATION OneAgentCQTester.CSV;
DROP APPLICATION OneAgentCQTester.CSV cascade;

create application CSV;

CREATE FLOW AgentFlow;

create source CSVSource using CSVReader
(
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'StoreNames.csv',
  columndelimiter:',',
  positionByEOF:false
)OUTPUT TO CsvStream;

CREATE TYPE MyType (
Store_Id String KEY,
Store_Name String
);

CREATE STREAM TypedStream of MyType;

CREATE CQ TypeConversionCQ
INSERT INTO TypedStream
SELECT data[0], data[1]
from CsvStream;

END FLOW AgentFlow;

CREATE FLOW ServerFlow;

CREATE WACTIONSTORE StoreInfo CONTEXT OF MyType
EVENT TYPES ( MyType )
@PERSIST-TYPE@

CREATE CQ StoreWaction
INSERT INTO StoreInfo
SELECT * FROM TypedStream
LINK SOURCE EVENT;

END FLOW ServerFlow;

end application CSV;
DEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;
START CSV;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadpolicy:'EventCount:7'
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from @STREAM@;

end flow @APPNAME@_serverflow;

end application @APPNAME@;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;
Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream; 


create Target FileTarget using FileWriter(
    rolloverpolicy:'@UPLOAD-SIZE@',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using AvroFormatter (
schemafilename:'@charset@',
formatAs:'@mem@',
schemaregistryurl:'@head@'

)
input from TypedCSVStream;

end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

--
-- Recovery Test 24 with two sources, two sliding time-count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5a9W  -> CQ1 -> WS
-- S2 -> Sc6a11W -> CQ2 -> WS
--

STOP KStreamRecov24Tester.KStreamRecovTest24;
UNDEPLOY APPLICATION KStreamRecov24Tester.KStreamRecovTest24;
DROP APPLICATION KStreamRecov24Tester.KStreamRecovTest24 CASCADE;
DROP USER KStreamRecov24Tester;
DROP NAMESPACE KStreamRecov24Tester CASCADE;
CREATE USER KStreamRecov24Tester IDENTIFIED BY KStreamRecov24Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov24Tester;
CONNECT KStreamRecov24Tester KStreamRecov24Tester;

CREATE APPLICATION KStreamRecovTest24 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION KStreamRecovTest24;

STOP APPLICATION SystemTimeTester.SystemTimeWindows;
UNDEPLOY APPLICATION SystemTimeTester.SystemTimeWindows;
DROP APPLICATION SystemTimeTester.SystemTimeWindows cascade;


CREATE APPLICATION SystemTimeWindows;

CREATE TYPE RandomData(
bankNumber int KEY,
bankName String
);

CREATE  SOURCE ranDataSource USING StreamReader (
  OutputType: 'SystemTimeTester.RandomData',
  noLimit: 'false',
  isSeeded: 'true',
  maxRows: 0,
  iterations: 30,
  iterationDelay: 1000,
  StringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',
  NumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]R'
 )
OUTPUT TO CSVDataStream;

CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT data[0], data[1]
FROM CSVDataStream;

CREATE @WINDOWTYPE@ WINDOW tierone OVER RandomDataStream keep within 20 second;

CREATE STREAM onetwostream OF RandomData;

CREATE CQ onetwocq
INSERT INTO onetwostream
SELECT bankNumber,bankName
FROM tierone
where  bankNumber >= 300
order by bankName;

CREATE WACTIONSTORE MyDataActivity  CONTEXT OF RandomData
EVENT TYPES ( RandomData  )
@PERSIST-TYPE@

Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
SELECT bankNumber,bankName from @FROMSTREAM@
where  bankNumber >= 300
order by bankName
LINK SOURCE EVENT;

END APPLICATION SystemTimeWindows;
deploy application SystemTimeWindows;
start application SystemTimeWindows;

CREATE APPLICATION @APPNAME@ @RECOVERY@;

CREATE FLOW @APPNAME@AgentFlow;
CREATE OR REPLACE SOURCE @APPNAME@_src USING Global.GCSReader ()
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream;
END FLOW @APPNAME@AgentFlow;

CREATE FLOW @APPNAME@serverFlow;
CREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer ()
FORMAT USING DSVFormatter (
members:'data')
INPUT FROM @APPNAME@_Stream;
END FLOW @APPNAME@serverFlow;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ with @APPNAME@AgentFlow in Agents, @APPNAME@ServerFlow in default;
start application @APPNAME@;

STOP APPLICATION KafkaWPTester.KWApp;
STOP APPLICATION KafkaWPTester.KRApp;
UNDEPLOY APPLICATION KafkaWPTester.KWApp;
UNDEPLOY APPLICATION KafkaWPTester.KRApp;
DROP APPLICATION KafkaWPTester.KWApp CASCADE;
DROP APPLICATION KafkaWPTester.KRApp CASCADE;

CREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;
GRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;
CONNECT KafkaWPTester KafkaWPTester;


CREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE CSVSource USING FileReader (
	directory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',
    WildCard:'access_log_half',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;

CREATE TYPE AccessLogType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;

CREATE CQ AceeslogCQ
INSERT INTO TypedAccessLogStream
SELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]
FROM FileStream;

create Target DsvWriter using KafkaWriter VERSION '0.11.0' ( 
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V11dsvExistTopic',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream;


create Target JsonWriter using KafkaWriter VERSION '0.11.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V11jsonExistTopic',
	Mode:'Sync'
    )
 
FORMAT USING JSONFormatter ()
input from TypedAccessLogStream;


create Target AvroWriter using KafkaWriter VERSION '0.11.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V11avroExistTopic',
    Mode:'Sync'
    )
 
FORMAT USING AvroFormatter (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')
input from TypedAccessLogStream;


END APPLICATION KWApp;

DEPLOY APPLICATION KWApp on any in default;


-- Kafka Reader Apps

CREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;


CREATE TYPE AccessLogType2(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE Stream KafkaDSVReaderStream of AccessLogType2;
CREATE Stream KafkaJsonReaderStream of AccessLogType2;
CREATE Stream KafkaAvroReaderStream of AccessLogType2;


CREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '0.11.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V11dsvExistTopic',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;




CREATE CQ DSVtoKafkaDSVReaderStream
INSERT INTO KafkaDSVReaderStream
SELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]
FROM KafkaReaderStream1;

CREATE TARGET DSVDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V11_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaDSVReaderStream;

CREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '0.11.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V11jsonExistTopic',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;




CREATE CQ DSVtoKafkaJsonReaderStream
INSERT INTO KafkaJsonReaderStream
SELECT 
data.get('Col1').textValue(),
data.get('Col2').textValue(),
data.get('Col3').textValue(),
data.get('Col4').textValue(),
data.get('Col5').textValue(),
data.get('Col6').textValue(),
data.get('Col7').textValue(),
data.get('Col8').textValue(),
data.get('Col9').textValue(),
data.get('Col10').textValue()
FROM KafkaReaderStream2;

CREATE TARGET JSONDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V11_RT_JSON')
FORMAT USING DSVFormatter()
INPUT FROM KafkaJsonReaderStream;

CREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '0.11.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V11avroExistTopic',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'
)
OUTPUT TO KafkaReaderStream3;



CREATE Stream KRTypedStream of AccessLogType;

CREATE CQ DSVtoKafkaAvroReaderStream
INSERT INTO KafkaAvroReaderStream
SELECT    
data.get("Col1").toString(), 
data.get("Col2").toString(), 
data.get("Col3").toString(), 
data.get("Col4").toString(),
data.get("Col5").toString(), 
data.get("Col6").toString(), 
data.get("Col7").toString(), 
data.get("Col8").toString(),
data.get("Col9").toString(), 
data.get("Col10").toString()
FROM KafkaReaderStream3;

CREATE TARGET AVRODump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V11_RT_AVRO')
FORMAT USING DSVFormatter()
INPUT FROM KafkaAvroReaderStream;

END APPLICATION KRApp;
DEPLOY APPLICATION KRApp on any in default;

stop application reconnect;
undeploy application reconnect;
drop application reconnect cascade;
CREATE APPLICATION reconnect recovery 1 second interval;

CREATE  SOURCE mssqlsource USING MssqlReader  ( 
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  ConnectionURL: '@URL@',
  Tables: '@TABLE@',
  FetchSize: 1
 ) 
OUTPUT TO sqlstream;

CREATE TARGET dbtarget USING CassandraWriter(
  ConnectionURL:'@URL@',
  Username:'@USERNAME@',
  Password:'@PASSWORD@',
  ConnectionRetryPolicy: 'retryInterval=15s,maxRetries=2',
  BatchPolicy:'EventCount:5,Interval:30',
  CommitPolicy:'EventCount:5,Interval:30',
  Tables: '@TABLES@'
 ) INPUT FROM sqlstream;

 create Target tSysOut using Sysout(name:OrgData) input from sqlstream;
 end application reconnect;
 deploy application reconnect;
 start application reconnect;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

END APPLICATION @APPNAME@;

--
-- Recovery Test 33 with two sources, two sliding time windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> St1W/p -> CQ1 -> WS
-- S2 -> St2W/p -> CQ2 -> WS
--

STOP KStreamRecov33Tester.KStreamRecovTest33;
UNDEPLOY APPLICATION KStreamRecov33Tester.KStreamRecovTest33;
DROP APPLICATION KStreamRecov33Tester.KStreamRecovTest33 CASCADE;

DROP USER KStreamRecov33Tester;
DROP NAMESPACE KStreamRecov33Tester CASCADE;
CREATE USER KStreamRecov33Tester IDENTIFIED BY KStreamRecov33Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov33Tester;
CONNECT KStreamRecov33Tester KStreamRecov33Tester;

CREATE APPLICATION KStreamRecovTest33 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 1 SECOND
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 2 SECOND
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION KStreamRecovTest33;

CREATE OR REPLACE PROPERTYVARIABLE RetryPolicy='timeOut=00,retryInterval=1,maxRetries=3';
CREATE OR REPLACE PROPERTYVARIABLE KafkaBrokerAddress='localhost:9099';

STOP @WRITERAPPNAME@;
UNDEPLOY APPLICATION @WRITERAPPNAME@;
DROP APPLICATION @WRITERAPPNAME@ CASCADE;

CREATE APPLICATION @WRITERAPPNAME@ @Recovery@;
create flow AgentFlow;
CREATE SOURCE S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.oracle_kw_test%',
	FetchSize: '1',
	connectionRetryPolicy:'$RetryPolicy'
)
OUTPUT TO SS;
end flow AgentFlow;
create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;
create stream out_cq_select_SS_1 of global.waevent;
create stream out_cq_select_SS_2 of global.waevent;
create stream out_cq_select_SS_3 of global.waevent;
create stream out_cq_select_SS_4 of global.waevent;
create stream out_cq_select_SS_5 of global.waevent;
create stream out_cq_select_SS_6 of global.waevent;

CREATE OR REPLACE CQ cq_select_SS1 
INSERT INTO out_cq_select_SS_1
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST1';

CREATE OR REPLACE CQ cq_select_SS2 
INSERT INTO out_cq_select_SS_2
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST2';

CREATE OR REPLACE CQ cq_select_SS3 
INSERT INTO out_cq_select_SS_3
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST3';

CREATE OR REPLACE CQ cq_select_SS4 
INSERT INTO out_cq_select_SS_4
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST4';

CREATE OR REPLACE CQ cq_select_SS5 
INSERT INTO out_cq_select_SS_5
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST5';

CREATE OR REPLACE CQ cq_select_SS6 
INSERT INTO out_cq_select_SS_6
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST6';

create Target TARGET1 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_sync_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from out_cq_select_SS_1;

create Target TARGET2 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_sync_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from out_cq_select_SS_2;

create Target TARGET3 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_sync_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_sync_CQ.avsc')
input from out_cq_select_SS_3;

create Target TARGET4 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_Async_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(ROWID)',
Mode:'Async',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from out_cq_select_SS_4;

create Target TARGET5 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_Async_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(ROWID)',
Mode:'Async',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from out_cq_select_SS_5;

create Target TARGET6 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_Async_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(ROWID)',
Mode:'Async',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_Async_CQ.avsc')
input from out_cq_select_SS_6;

create Target TARGET7 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_sync',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from ss;

create Target TARGET8 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_sync',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from ss;

create Target TARGET9 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_sync',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_sync.avsc')
input from ss;

create Target TARGET10 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_Async',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'Async',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from ss;

create Target TARGET11 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_Async',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'Async',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from ss;

create Target TARGET12 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_Async',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'Async',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_Async.avsc')
input from ss;

end flow serverFlow;
end application @WRITERAPPNAME@;
deploy application @WRITERAPPNAME@;
start @WRITERAPPNAME@;



stop application @READERAPPNAME@;
undeploy application @READERAPPNAME@;
drop application @READERAPPNAME@ cascade;
CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @SOURCE@_DSV_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_sync_CQ',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;

CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
rolloverpolicy:'filesize:500M',
filename:'@READERAPPNAME@_@SOURCE@_dsv_sync_CQ')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream1;

CREATE SOURCE @SOURCE@_JSON_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_sync_CQ',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;

CREATE SOURCE @SOURCE@_AVRO_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_sync_CQ',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_sync_CQ.avsc'
)
OUTPUT TO KafkaReaderStream3;

CREATE SOURCE @SOURCE@_DSV_Async_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_Async_CQ',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream4;

CREATE SOURCE @SOURCE@_JSON_Async_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_Async_CQ',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream5;

CREATE SOURCE @SOURCE@_AVRO_Async_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_Async_CQ',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_Async_CQ.avsc'
)
OUTPUT TO KafkaReaderStream6;

CREATE SOURCE @SOURCE@_DSV_sync USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_sync',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream7;

CREATE TARGET kafkaDumpDSV_rawstream USING FileWriter(
name:kafkaOuputDSV_rawstream,
rolloverpolicy:'filesize:500M',
filename:'@READERAPPNAME@_@SOURCE@_dsv_sync')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream7;

CREATE SOURCE @SOURCE@_JSON_sync USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_sync',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream8;

CREATE SOURCE @SOURCE@_AVRO_sync USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_sync',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_sync.avsc'
)
OUTPUT TO KafkaReaderStream9;

CREATE SOURCE @SOURCE@_DSV_Async USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_Async',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream10;

CREATE SOURCE @SOURCE@_JSON_Async USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_Async',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream11;

CREATE SOURCE @SOURCE@_AVRO_Async USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_Async',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_Async.avsc'
)
OUTPUT TO KafkaReaderStream12;

end application @READERAPPNAME@;
deploy application @READERAPPNAME@;

stop application GCSWriterTest;
undeploy application GCSWriterTest;
drop application GCSWriterTest cascade;
create application GCSWriterTest recovery 1 second interval;
create source GS Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

create Target T using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@',
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from CsvStream;

end application GCSWriterTest;
deploy application GCSWriterTest on all in default;
start application GCSWriterTest;

-- The PosApp sample application demonstrates how a credit card
-- payment processor might use Striim to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.
STOP APPLICATION PosAppKafka.PosAppKafka;
UNDEPLOY APPLICATION PosAppKafka.PosAppKafka;
DROP APPLICATION PosAppKafka.PosAppKafka CASCADE;

CREATE APPLICATION PosAppKafka;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells Striim that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells Striim to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvToPosData

create type posdatatype(
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip string
);

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092');

create stream PosDataStream of posdatatype persist using KafkaProps;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT TO_STRING(data[1]),
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       TO_STRING(data[9])
FROM CsvStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP.
--
-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as
-- the types to be used in PosDataStream, which is created automatically.

-- The DATETIME field from the source is converted to both a dateTime
-- value, used as the event timestamp by the application, and (via the
-- function) an integer hourValue, which is used to look up
-- historical hourly averages from the HourlyAveLookup cache,
-- discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)
-- 17 (hourValue)
-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateOnly
--
-- The PosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startingTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;


-- WAction store MerchantActivity
--
-- The following group of statements create and populate the MerchantActivity
-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced
-- with merchant details from NameLookup cache and with latitude and longitude
-- values from the USAddressData cache.

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
@PERSIST-TYPE@

CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false
)
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;


CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

-- CQ GenerateAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;



--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM AlertStream;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;

END APPLICATION PosAppKafka;

--
-- Recovery Test 24 with two sources, two sliding time-count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5a9W  -> CQ1 -> WS
-- S2 -> Sc6a11W -> CQ2 -> WS
--

STOP Recov24Tester.RecovTest24;
UNDEPLOY APPLICATION Recov24Tester.RecovTest24;
DROP APPLICATION Recov24Tester.RecovTest24 CASCADE;
CREATE APPLICATION RecovTest24 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION RecovTest24;

STOP application JSONFormatterTester.DSV;
undeploy application JSONFormatterTester.DSV;
drop application JSONFormatterTester.DSV cascade;

create application DSV;

create source CSVSmallPosDataSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvSmallPosDataStream;

create source CSVPosDataSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'posdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvPosDataStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVPosDataStream of CSVType;
Create Stream TypedCSVSmallPosDataStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVPosDataStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvPosDataStream;

CREATE CQ CsvToSmallPosData
INSERT INTO TypedCSVSmallPosDataStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvSmallPosDataStream;



/**
* 3.4.5.b FileWriter JsonFileSize 333MB
**/
create Target JSONFormatterFileSize using FileWriter(
  filename:'JsonTargetFS',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'FileSizeRollingPolicy,filesize:333M,sequence:00'
)
format using JSONFormatter (
  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)
input from TypedCSVPosDataStream;

/**
* 3.5.1.b FileWriter Json EventCount 2000
**/
create Target JSONFormatterEventCount using FileWriter(
  filename:'JsonTargetEC',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'eventcount:2000,sequence:00'
)
format using JSONFormatter (
  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)
input from TypedCSVSmallPosDataStream;


/**
*  FileWriter Json events as array of json objects :true.
**/

create Target JSONFormatterEventCountT using FileWriter(
  filename:'JsonFormatterPropT',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'eventcount:500000,sequence:00'
)
format using JSONFormatter (
  members:'merchantname,merchantid,zip',
  jsonObjectDelimiter:'|',
  jsonMemberDelimiter:'~',
  EventsAsArrayOfJsonObjects : true
)
input from TypedCSVSmallPosDataStream;

/**
*  FileWriter Json events as array of json objects :false.
**/

create Target JSONFormatterEventCountF using FileWriter(
  filename:'JsonFormatterPropF',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'eventcount:500000,sequence:00'
)
format using JSONFormatter (
  members:'merchantname,merchantid,zip',
  jsonObjectDelimiter:'|',
  jsonMemberDelimiter:'~',
  EventsAsArrayOfJsonObjects : false
)
input from TypedCSVSmallPosDataStream;


create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizeBig_actual.log') input from TypedCSVPosDataStream;

end application DSV;

STOP APPLICATION HW ;
undeploy application HW ;
drop application HW cascade;

CREATE APPLICATION HW Recovery 5 second interval;

CREATE  SOURCE S USING OracleReader  ( 
  Username: 'miner',
  Password: '@miner',
  ConnectionURL: '@conn-url@',
  Tables: '@src@',
  FetchSize: 1) 
OUTPUT TO hivestream;

Create Target T using ClouderaHiveWriter (
  ConnectionURL:'@hive-url@',
  Username:'@uname@', 
            Password:'@pwd@',
            hadoopurl:'hdfs://dockerhost:9000/',
	        Mode:'incremental',
	        mergepolicy: 'eventcount:5,interval:1s',
            Tables:'@tgt-table@',
            hadoopConfigurationPath:'/Users/saranyad/Documents/hello/'
 )
INPUT FROM hivestream;


END APPLICATION HW;
deploy application HW on all in default;

Start application HW;

CREATE APPLICATION applicationApiSaas;

CREATE FLOW SaasSourceFlow;

CREATE SOURCE SaasLog4JSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'SaasMonitorLog4JOutput.xml',
  positionByEOF:false
)
PARSE USING XMLParser(
  rootnode:'/log4j:event',
  columnlist:'log4j:event/@timestamp,log4j:event/@level,log4j:event/log4j:message,log4j:event/log4j:locationInfo/@method'
)
OUTPUT TO SaasRawXMLStream;

CREATE TYPE SaasLog4JEvent (
    ts DateTime,
    level String,
    message String,
    method String
);
CREATE STREAM SaasLog4JStream OF SaasLog4JEvent;

CREATE CQ SaasGetLog4JData
INSERT INTO SaasLog4JStream
SELECT TO_DATE(TO_LONG(data[0])), data[1], data[2], data[3]
FROM SaasRawXMLStream;

CREATE STREAM SaasErrorStream OF SaasLog4JEvent;
CREATE STREAM SaasWarningStream OF SaasLog4JEvent;
CREATE STREAM SaasInfoStream OF SaasLog4JEvent;

CREATE CQ SaasGetErrors
INSERT INTO SaasErrorStream
SELECT log4j
FROM SaasLog4JStream log4j WHERE log4j.level = 'ERROR';

CREATE CQ SaasGetWarnings
INSERT INTO SaasWarningStream
SELECT log4j
FROM SaasLog4JStream log4j WHERE log4j.level = 'WARN';

CREATE CQ SaasGetInfo
INSERT INTO SaasInfoStream
SELECT log4j
FROM SaasLog4JStream log4j WHERE log4j.level = 'INFO';

END FLOW SaasSourceFlow;


CREATE FLOW SaasErrorFlow;

CREATE STREAM SaasErrorAlertStream OF Global.AlertEvent;

CREATE CQ SaasSendErrorAlerts
INSERT INTO SaasErrorAlertStream
SELECT 'ErrorAlert', ''+ts, 'error', 'raise', 'Error in log ' + message
FROM SaasErrorStream;

CREATE SUBSCRIPTION SaasErrorAlertSub USING WebAlertAdapter( ) INPUT FROM SaasErrorAlertStream;

END FLOW SaasErrorFlow;


CREATE FLOW SaasWarningFlow;

CREATE JUMPING WINDOW SaasWarningWindow
OVER SaasWarningStream KEEP WITHIN 30 SECOND ON ts;

CREATE STREAM SaasWarningAlertStream OF Global.AlertEvent;

CREATE CQ SaasSendWarningAlerts
INSERT INTO SaasWarningAlertStream
SELECT 'WarningAlert', ''+ts, 'warning', 'raise',
        COUNT(ts) + ' Warnings in log for api ' + method
FROM SaasWarningWindow
GROUP BY method
HAVING count(ts) > 25;
CREATE SUBSCRIPTION SaasWarningAlertSub USING WebAlertAdapter( ) INPUT FROM SaasWarningAlertStream;

END FLOW SaasWarningFlow;


CREATE FLOW SaasInfoFlow;

CREATE TYPE SaasApiCall (
  userId String,
  apiCall String,
  sobject String,
  ts DateTime,
  userName String,
  company String,
  userZip String,
  companyZip String
);
CREATE STREAM SaasApiStream OF SaasApiCall;

CREATE CQ SaasGetApiDetails
INSERT INTO SaasApiStream(userId,apiCall,sobject,ts)
SELECT MATCH(i.message, '\\\\[user=([a-zA-Z0-9]*)\\\\]'),
       MATCH(i.message, '\\\\[api=([a-zA-Z0-9]*)\\\\]'),
       MATCH(i.message, '\\\\[sobject=([a-zA-Z0-9]*)\\\\]'),
       i.ts
FROM SaasInfoStream i;

CREATE STREAM SaasApiEnrichedStream OF SaasApiCall;

CREATE TYPE SaasUserInfo (
  userId String,
  userName String,
  company String,
  userZip String,
  companyZip String
);
CREATE CACHE SaasUserLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'userLookup.txt',
  header: Yes,
  columndelimiter: ','
) QUERY (keytomap:'userId') OF SaasUserInfo;


CREATE CQ SaasGetUserDetails
INSERT INTO SaasApiEnrichedStream
SELECT a.userId, a.apiCall, a.sobject, a.ts, u.userName, u.company, u.userZip, u.companyZip
FROM SaasApiStream a, SaasUserLookup u
WHERE a.userId = u.userId;

END FLOW SaasInfoFlow;


CREATE FLOW SaasUserApiFlow;

CREATE JUMPING WINDOW SaasUserApiWindow
OVER SaasApiEnrichedStream KEEP WITHIN 5 SECOND ON ts
PARTITION BY userId;

CREATE TYPE SaasUserApiUsage (
  userId String key,
  userName String,
  userZip String,
  userLat double,
  userLong double,
  company String,
  apiCall String,
  count integer,
  state String,
  topObject String,
  ts DateTime
);
CREATE STREAM SaasUserApiUsageStream OF SaasUserApiUsage;

CREATE TYPE SaasUSAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);
CREATE CACHE SaasZipLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  header: Yes,
  columndelimiter: '	'
) QUERY (keytomap:'zip') OF SaasUSAddressData;


CREATE CQ SaasGetUserApiUsage
INSERT INTO SaasUserApiUsageStream
SELECT a.userId, a.userName, a.userZip, z.latVal, z.longVal, a.company,
       a.apiCall, COUNT(a.sobject),
       CASE WHEN COUNT(a.sobject) > 175 THEN 'HOT'
            ELSE 'COLD' END,
       MAXOCCURS(a.sobject),
       FIRST(a.ts)
FROM SaasUserApiWindow a, SaasZipLookup z
WHERE a.userZip = z.zip
GROUP BY a.userId, a.apiCall
HAVING FIRST(a.ts) IS NOT NULL;

CREATE TYPE SaasUserApiContext (
  userId String key,
  userName String,
  userZip String,
  userLat double,
  userLong double,
  company String,
  count integer,
  state String,
  topObject String,
  ts DateTime
);
CREATE WACTIONSTORE SaasUserApiActivity
CONTEXT OF SaasUserApiContext
EVENT TYPES ( SaasUserApiUsage )
@PERSIST-TYPE@

CREATE JUMPING WINDOW SaasUserWindow
OVER SaasUserApiUsageStream KEEP WITHIN 5 SECOND ON ts
PARTITION BY userId;

CREATE CQ SaasTrackUserApiSummary
INSERT INTO SaasUserApiActivity
SELECT a.userId, a.userName, a.userZip, a.userLat, a.userLong,
       a.company, SUM(count),
       CASE WHEN SUM(count) > 1000 THEN 'HOT'
            ELSE 'COLD' END,
       MAXOCCURS(a.topObject),
       FIRST(a.ts)
FROM SaasUserWindow a
GROUP BY a.userId
LINK SOURCE EVENT;

CREATE STREAM SaasUserApiAlertStream OF Global.AlertEvent;

CREATE CQ SaasSendUserApiAlerts
INSERT INTO SaasUserApiAlertStream
SELECT 'UserAPIAlert', ''+ts, 'warning', 'raise',
       'User ' + userName + ' has used api ' + apiCall + ' ' + count + ' times for ' + topObject
FROM SaasUserApiUsageStream
WHERE state = 'HOT' AND count > 300;

CREATE SUBSCRIPTION SaasUserApiAlertSub USING WebAlertAdapter( ) INPUT FROM SaasUserApiAlertStream;

END FLOW SaasUserApiFlow;


CREATE FLOW SaasCompanyApiFlow;

CREATE JUMPING WINDOW SaasCompanyApiWindow
OVER SaasApiEnrichedStream KEEP WITHIN 5 SECOND ON ts
PARTITION BY company;

CREATE TYPE SaasCompanyApiUsage (
  company String key,
  companyZip String,
  companyLat double,
  companyLong double,
  apiCall String,
  count integer,
  state String,
  topObject String,
  ts DateTime
);
CREATE STREAM SaasCompanyApiUsageStream OF SaasCompanyApiUsage;

CREATE CQ SaasGetCompanyApiUsage
INSERT INTO SaasCompanyApiUsageStream
SELECT a.company, a.companyZip, z.latVal, z.longVal,
       a.apiCall, COUNT(a.sobject),
       CASE WHEN COUNT(a.sobject) > 3000 THEN 'HOT'
            ELSE 'COLD' END,
       MAXOCCURS(a.sobject),
       FIRST(a.ts)
FROM SaasCompanyApiWindow a, SaasZipLookup z
WHERE a.companyZip = z.zip
GROUP BY a.company, a.apiCall
HAVING FIRST(a.ts) IS NOT NULL;

CREATE TYPE SaasCompanyApiContext (
  company String key,
  companyZip String,
  companyLat double,
  companyLong double,
  count integer,
  state String,
  topObject String,
  ts DateTime
);
CREATE JUMPING WINDOW SaasCompanyWindow
OVER SaasCompanyApiUsageStream KEEP WITHIN 5 SECOND ON ts
PARTITION BY company;

CREATE WACTIONSTORE SaasCompanyApiActivity
CONTEXT OF SaasCompanyApiContext
EVENT TYPES ( SaasCompanyApiUsage )
@PERSIST-TYPE@


CREATE CQ SaasTrackCompanyApiSummary
INSERT INTO SaasCompanyApiActivity
SELECT a.company, a.companyZip, a.companyLat, a.companyLong,
       SUM(a.count),
       CASE WHEN SUM(a.count) > 15000 THEN 'HOT'
            ELSE 'COLD' END,
       MAXOCCURS(a.topObject),
       FIRST(a.ts)
FROM SaasCompanyWindow a
GROUP BY a.company
LINK SOURCE EVENT;

END FLOW SaasCompanyApiFlow;


CREATE FLOW SaasApiFlow;

CREATE JUMPING WINDOW SaasApiWindow
OVER SaasApiEnrichedStream KEEP WITHIN 5 SECOND ON ts
PARTITION BY apiCall;

CREATE TYPE SaasApiUsage (
  apiCall String key,
  sobject String,
  count integer,
  state String,
  ts DateTime
);
CREATE STREAM SaasApiUsageStream OF SaasApiUsage;

CREATE CQ SaasGetApiUsage
INSERT INTO SaasApiUsageStream
SELECT a.apiCall, a.sobject,
       COUNT(a.userId),
       CASE WHEN COUNT(a.userId) > 3000 THEN 'HOT'
            ELSE 'COLD' END,
       FIRST(a.ts)
FROM SaasApiWindow a
GROUP BY a.apiCall, a.sobject
HAVING FIRST(a.ts) IS NOT NULL;

CREATE TYPE SaasApiContext (
  apiCall String key,
  count integer,
  state String,
  topObject String,
  ts DateTime
);

CREATE JUMPING WINDOW SaasApiAggWindow
OVER SaasApiUsageStream KEEP WITHIN 5 SECOND ON ts
PARTITION BY apiCall;

CREATE WACTIONSTORE SaasApiActivity
CONTEXT OF SaasApiContext
EVENT TYPES ( SaasApiUsage )
@PERSIST-TYPE@

CREATE CQ SaasTrackApiSummary
INSERT INTO SaasApiActivity
SELECT a.apiCall,
       SUM(a.count),
       CASE WHEN SUM(a.count) > 20000 THEN 'HOT'
            ELSE 'COLD' END,
       MAXOCCURS(a.sobject),
       first(a.ts)
FROM SaasApiAggWindow a
GROUP BY a.apiCall
LINK SOURCE EVENT;

END FLOW SaasApiFlow;

END APPLICATION applicationApiSaas;

--
-- Canon Test W70
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for a partitioned jumping count window
--
-- S -> JWc5p -> CQ -> WS
--


UNDEPLOY APPLICATION NameW70.W70;
DROP APPLICATION NameW70.W70 CASCADE;
CREATE APPLICATION W70 RECOVERY 5 SECOND INTERVAL;




CREATE FLOW DataAcquisitionW70;


CREATE SOURCE CsvSourceW70 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW70;


END FLOW DataAcquisitionW70;




CREATE FLOW DataProcessingW70;

CREATE TYPE DataTypeW70 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW70 OF DataTypeW70;

CREATE CQ CSVStreamW70_to_DataStreamW70
INSERT INTO DataStreamW70
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW70;

CREATE JUMPING WINDOW JWc5pW70
OVER DataStreamW70
KEEP 5 ROWS
PARTITION BY word;

CREATE WACTIONSTORE WactionStoreW70 CONTEXT OF DataTypeW70
EVENT TYPES ( DataTypeW70 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWc5pW70_to_WactionStoreW70
INSERT INTO WactionStoreW70
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc5pW70
GROUP BY word;

END FLOW DataProcessingW70;



END APPLICATION W70;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 5 second iNTERVAL;
create source @SOURCE@ using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'dynamicdirectory.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO @STREAM@;

Create Type CSVType_@APPNAME@ (
  id String,
  name String,
  department String,
  yoj String,
  moj String,
  doj int
);

Create Stream Typed@STREAM@ of CSVType_@APPNAME@;

CREATE CQ CsvToPosData@APPNAME@
INSERT INTO Typed@STREAM@
SELECT data[0],
       data[1],
       data[2],
       data[3],
       data[4],
       TO_INT(data[5])
FROM @STREAM@;

create Target @TARGET@ using ADLSGen2Writer(
    accountname:'',
	sastoken:'',
	filesystemname:'',
	filename:'',
	directory:'',
	uploadpolicy:'eventcount:5'

)format using DSVFormatter (
)
input from Typed@STREAM@;

end application @APPNAME@;

deploy application @APPNAME@;
start application @APPNAME@;

STOP PartiallyJumping1Tester.PartiallyJumping1;
UNDEPLOY APPLICATION PartiallyJumping1Tester.PartiallyJumping1;
DROP APPLICATION PartiallyJumping1Tester.PartiallyJumping1 CASCADE;
CREATE APPLICATION PartiallyJumping1;

create source CsvSource1 using FileReader
(
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'WindowsTest.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
)
 parse using DSVParser
(
	header:'yes',
	columndelimiter:','
)
OUTPUT TO CsvStream1;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);
CREATE TYPE CsvData1 (
  zip double
);

CREATE TYPE WactionData1 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData2 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData3 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData5 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData6 (
  zip double
);
CREATE TYPE WactionData7 (
  zip double
);

CREATE STREAM DataStream1 OF CsvData;

CREATE STREAM DataStream2 OF CsvData;

CREATE STREAM DataStream3 OF CsvData
PARTITION BY city;

CREATE STREAM DataStream4 OF CsvData;

CREATE STREAM DataStream5 OF CsvData
PARTITION BY city;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData3
INSERT INTO DataStream3
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData4
INSERT INTO DataStream4
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData5
INSERT INTO DataStream5
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData6
INSERT INTO DataStream6
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

CREATE CQ CsvToData7
INSERT INTO DataStream7
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

-- Count based partially jumping window
CREATE WINDOW DataStreamCount
OVER DataStream1 KEEP 5 ROWS
SLIDE 2
PARTITION BY companyName;

-- Time based partially jumping window
CREATE WINDOW DataStreamTime OVER DataStream2 KEEP
within 240 second
SLIDE 15 second;

-- Attribute based partially jumping window
CREATE WINDOW DataStreamAtrribute
OVER DataStream3 KEEP
RANGE 30 SECOND
ON dateTime
SLIDE 20 second;

-- Count + time based partially jumping window
CREATE WINDOW DataStreamCountTime
OVER DataStream4 KEEP
10 rows
within 150 second
SLIDE 10;

-- Attribute + time based partially jumping window
CREATE WINDOW DataStreamAttributeTime
OVER DataStream5 KEEP
range 50 second
ON dateTime
within 25 second
SLIDE 2 SECOND
PARTITION BY city;

CREATE WACTIONSTORE Wactions1 CONTEXT OF WactionData1
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions2 CONTEXT OF WactionData2
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions3 CONTEXT OF WactionData3
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions4 CONTEXT OF WactionData4
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions5 CONTEXT OF WactionData5
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions6 CONTEXT OF WactionData6
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions7 CONTEXT OF WactionData7
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE CQ Data1ToWaction
INSERT INTO Wactions1
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamCount p
group by companyName;

CREATE CQ Data2ToWaction
INSERT INTO Wactions2
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamTime p;

CREATE CQ Data3ToWaction
INSERT INTO Wactions3
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamAtrribute p;

CREATE CQ Data4ToWaction
INSERT INTO Wactions4
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamCountTime p;

CREATE CQ Data5ToWaction
INSERT INTO Wactions5
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamAttributeTime p
group by city;

CREATE CQ Data6ToWaction
INSERT INTO Wactions6
SELECT
    count(*)
FROM DataStreamCount p
group by companyName;

CREATE CQ Data7ToWaction
INSERT INTO Wactions7
SELECT
    count(*)
FROM DataStreamTime p;

END APPLICATION PartiallyJumping1;

CREATE STREAM @STREAM@_JSON OF Global.JsonNodeEvent;

CREATE TARGET @TARGET_NAME@ USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
) INPUT FROM @STREAM@_JSON;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING DSVFormatter (
members:'data')
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

stop application Postgres_To_PostgresApp;
undeploy application Postgres_To_PostgresApp;
drop application Postgres_To_PostgresApp cascade;

CREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE Postgres_Src1 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_1',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename0'
 ) 
OUTPUT TO Change_Data_Stream ;

CREATE OR REPLACE SOURCE Postgres_Src2 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_2',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename1'
 ) 
OUTPUT TO Change_Data_Stream ;


CREATE OR REPLACE SOURCE Postgres_Src3 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_3',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename2'
 ) 
OUTPUT TO Change_Data_Stream ;

CREATE OR REPLACE SOURCE Postgres_Src4 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_4',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename3'
 ) 
OUTPUT TO Change_Data_Stream ;


CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt1 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target0;public.tablename1, public.target0;public.tablename2, public.target0;public.tablename3, public.target0;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt2 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target1;public.tablename1, public.target1;public.tablename2, public.target1;public.tablename3, public.target1;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt3 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target2;public.tablename1, public.target2;public.tablename2, public.target2;public.tablename3, public.target2;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt4 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target3;public.tablename1, public.target3;public.tablename2, public.target3;public.tablename3, public.target3;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;


end application Postgres_To_PostgresApp;
deploy application Postgres_To_PostgresApp;
start Postgres_To_PostgresApp;

CREATE TYPE PosData(
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;
Create Source oracSource
 Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;
CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

Stop application DEV21349.app1;
Undeploy application DEV21349.app1;
Drop application DEV21349.app1 cascade;
Drop namespace DEV21349 cascade;
Create namespace DEV21349;
Use DEV21349;

CREATE APPLICATION app1;

CREATE CQ cq1 INSERT INTO cq1_outputstream select * from global.exceptionsstream exStream;

CREATE WACTIONSTORE exceptionsWS CONTEXT OF cq1_outputstream_Type @PERSIST-TYPE@;

CREATE CQ cq2 INSERT INTO exceptionsWS SELECT * FROM cq1_outputstream c;

CREATE OR REPLACE CQ readingWS_cq INSERT INTO outputstream2 select * from exceptionsWS;

CREATE OR REPLACE TARGET ExTarget USING Global.FileWriter  (
DataEncryptionKeyPassphrase: '',
  filename: 'ExceptionsOutPutFile',
  flushpolicy: 'EventCount:10000,Interval:30s',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  directory: '@TEST-DATA-PATH@',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  ( jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n' )
INPUT FROM outputstream2;

END APPLICATION app1;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 10 second interval;
create source CS using FileReader (
	directory:'@DIR@',
	WildCard:'@WILDCARD@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target T using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:50'
)
format using DSVFormatter (
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

--
-- Recovery Test 11
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS1
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS2
--

STOP Recov11Tester.RecovTest11;
UNDEPLOY APPLICATION Recov11Tester.RecovTest11;
DROP APPLICATION Recov11Tester.RecovTest11 CASCADE;
CREATE APPLICATION RecovTest11 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions1 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions2 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions1
INSERT INTO Wactions1
SELECT
    *
FROM DataStream5Minutes;

CREATE CQ InsertWactions2
INSERT INTO Wactions2
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION RecovTest11;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.WAEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader ()
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING Global.FileWriter ()
FORMAT USING Global.JSONFormatter  (
  members: 'data'
)
INPUT FROM @APPNAME@PersistStream@RANDOM@;

END APPLICATION @APPNAME@;

stop application ps1;
stop application ps2;
undeploy application ps1;
undeploy application ps2;
drop application ps1 cascade;
drop application ps2 cascade;
CREATE application ps1 recovery 5 second interval;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',
acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

create type type1(
  companyName String,
  merchantId String,
  city string
);

CREATE STREAM kps1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM kps2 OF Global.waevent persist using KafkaPropset;
CREATE STREAM kps3 OF Global.waevent persist using KafkaPropset;
CREATE STREAM kps4 OF Global.waevent persist using KafkaPropset;
CREATE STREAM kps5_typedStream OF type1 partition by city persist using KafkaPropset;

create source s using FileReader (
        directory:'/Users/saranyad/Product/IntegrationTests/TestData',
        wildcard:'posdata.csv',
        positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:',',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO kps;

CREATE CQ cq1
INSERT INTO kps1
SELECT *
FROM kps;

CREATE CQ cq2
INSERT INTO kps2
SELECT *
FROM kps;

CREATE CQ cq3
INSERT INTO kps3
SELECT *
FROM kps;

CREATE CQ cq4
INSERT INTO kps4
SELECT *
FROM kps;


CREATE CQ cq5
INSERT INTO kps5_typedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantId,
TO_STRING(data[10]) as city
FROM kps;

end application ps1;

CREATE application ps2 recovery 5 second interval;

create type type2(
  companyName String,
  merchantId String,
  city string
);

CREATE STREAM kps1_typedStream OF type2 partition by city;
CREATE STREAM kps2_typedStream OF type2 partition by city;
CREATE STREAM kps3_typedStream OF type2 partition by city;
CREATE STREAM kps4_typedStream OF type2 partition by city;

CREATE CQ cq6
INSERT INTO kps1_typedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantId,
TO_STRING(data[10]) as city
FROM kps1;

CREATE CQ cq7
INSERT INTO kps2_typedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantId,
TO_STRING(data[10]) as city
FROM kps2;

CREATE CQ cq8
INSERT INTO kps3_typedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantId,
TO_STRING(data[10]) as city
FROM kps3;

CREATE CQ cq9
INSERT INTO kps4_typedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantId,
TO_STRING(data[10]) as city
FROM kps4;

CREATE TARGET target1 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'QATEST.KPS1'
) INPUT FROM kps1_typedStream;

CREATE TARGET target2 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'QATEST.KPS2'
) INPUT FROM kps2_typedStream;

CREATE TARGET target3 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'QATEST.KPS3'
) INPUT FROM kps3_typedStream;

CREATE TARGET target4 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'QATEST.KPS4'
) INPUT FROM kps4_typedStream;

CREATE TARGET target5 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'QATEST.KPS5'
) INPUT FROM kps5_typedStream;

end application ps2;
--DEPLOY APPLICATION KafkaWriterApp with sourceFlow  ON ANY IN DEFAULT, targetFlow ON ALL IN DEFAULT;

CREATE APPLICATION IR recovery 5 second interval;

CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
 
  FetchSize: 5000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=t1',
 startPosition: '%=-1',
  PollingInterval: '20sec'
  )
  OUTPUT TO data_stream;
create target AzureSQLDWHTarget using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10000,interval:300s'
) INPUT FROM data_stream;
  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

END APPLICATION IR;
deploy application IR;
start IR;

stop @appname@;
undeploy application @appname@;
DROP APPLICATION @appname@ CASCADE;
CREATE APPLICATION @appname@;

CREATE SOURCE @appname@_src USING databaseReader  (
  Username: '@@',
  Password: '@@',
  ConnectionURL: '@@',
  Tables: '@@',
  FetchSize: '100'
 )
OUTPUT TO @appname@_ss;

--CREATE JUMPING WINDOW @appname@_win OVER @appname@_ss KEEP @winsize@ ROWS;

CREATE TYPE @appname@_MapType
    (   
       id INTEGER,
        name STRING,
        city  STRING
    );
    
CREATE EXTERNAL CACHE @appname@_cach (
  AdapterName: 'DatabaseReader',
    ConnectionURL: '@url@',
    UserName: '@uname@',
    Password: '@pwd@',
   Table: '@tablename@',
  FetchSize: 100,
  Columns: 'id,name,city',
  trimquote: false,
  KeyToMap: '@key@'
 )
 OF @appname@_MapType;
 
CREATE TYPE @appname@_MapTypenew
    (   id_t            INTEGER,
        name_t           STRING,
        city_t            STRING,
        id_c            INTEGER,
        name_c            STRING,
        city_c            STRING
    );
    
CREATE STREAM @appname@_JoinedData OF @appname@_MapTypenew;

CREATE CQ @appname@_JoinDataCQ
INSERT INTO @appname@_JoinedData
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_STRING(f.data[2]),
        z.id,
        z.name,
        z.city
FROM @appname@_ss f, @appname@_cach z
where TO_INT(f.data[0]) = z.id
@Ex@;

CREATE TARGET @appname@_tgt USING DatabaseWriter
(
  ConnectionURL:'@@',
  Username:'@@',
  Password:'@@',
  BatchPolicy:'Eventcount:10000,Interval:1',
  CommitPolicy:'Interval:1,Eventcount:10000',
  Tables:'@@'
) 
INPUT FROM @appname@_JoinedData;

END APPLICATION @appname@;
deploy application @appname@;
start @appname@;

stop application iteratortester.iteratorapp;
undeploy application iteratortester.iteratorapp;
drop application iteratortester.iteratorapp cascade;

CREATE APPLICATION iteratorapp;

create flow sourceFlow;

CREATE SOURCE JSONAccessLogSource USING FileReader(
  directory:'@TEST-DATA-PATH@',
  wildcard:'iterator.json',
  positionByEOF:'false'
)
parse using JSONParser (
) OUTPUT TO jsonSourceStream;

end flow sourceflow;

-- ******ARRAY LIST****** --
create flow processFlow;

create type cacheType (bankID string key, bankName string);
CREATE cache dsvcache USING FileReader (
directory:'@TEST-DATA-PATH@',
wildcard:'banks.csv',
blocksize: 10240,
positionByEOF:false
)
PARSE USING DSVParser (
header:No,
trimquote:false
) QUERY (keytomap:'bankID') OF cacheType;

CREATE TYPE listType (id integer KEY, bankname string, lst java.util.List);
CREATE TYPE listStoreType (id integer KEY, bankname string, lst java.util.List, lstoflst java.util.List);

CREATE STREAM listStream of listType partition by bankname;

CREATE JUMPING WINDOW listJWindow
OVER listStream
keep 3 rows;

CREATE WINDOW listWindow
OVER listStream
keep 3 rows;

CREATE WACTIONSTORE listStore CONTEXT OF listStoreType EVENT TYPES (listStoreType ) 
@PERSIST-TYPE@

create cq updatelistStream
insert into listStream
select TO_INT(bankID), bankName, makelist(bankID,bankName) as lst 
from dsvcache;

create cq updatelistStore 
insert into listStore
select ID, bankName, lst, makelist(lst,lst) from listStream
LINK SOURCE EVENT;

create stream listTargetStream( str String);

create cq updateListTarget
insert into listTargetStream
select itr
from listStream, iterator(listStream.lst) itr order by cast(itr as java.lang.Comparable);

-- CREATE TARGET listout USING SYSOUT(name:"list") input from listStream;

-- ******JsonNode****** --

--CREATE TYPE jsonType (id integer KEY,  lst com.fasterxml.jackson.databind.JsonNode);
CREATE TYPE jsonType (int integer, bankname string, lst com.fasterxml.jackson.databind.JsonNode key);

CREATE STREAM jsonStream of jsonType partition by bankname;

CREATE JUMPING WINDOW jsonJWindow
OVER jsonStream
keep 3 rows
partition by bankname;

CREATE WINDOW jsonWindow
OVER jsonStream
keep 3 rows;

CREATE WACTIONSTORE jsonStore CONTEXT OF jsonType EVENT TYPES (jsonType ) 
@PERSIST-TYPE@

create cq updateJsonStream
insert into jsonStream
--select TO_INT(MATCH(data[0], '.*\\\\s([0-9]+)')) , makeJSON('[{"x":"a"},{"x":"b","y":"c"},{"y":"c"}]') as lst 
select TO_INT(y.bankID), y.bankName, x.data 
from JSONSourceStream x, dsvcache y;

create cq updateJsonStore 
insert into jsonStore
select * from jsonStream
LINK SOURCE EVENT;

create stream jsonTargetStream( str String);

create cq updateJsonTarget
insert into jsonTargetStream
select to_string(itr.StringJSON)
from jsonStream, iterator(jsonStream.lst) itr order by to_string(itr.StringJSON);
CREATE TARGET jsonout USING SYSOUT(name:"jlist") input from jsonStream;

end flow processflow; 

end application iteratorapp;

CREATE APPLICATION jsonapp;

CREATE TYPE JSONAccessLogEntry (
    srcIp String KEY,
    userId String,
    sessionId String,
    accessTime long,
    request String,
    code int,
    size int,
    referrer String,
    userAgent String,
    responseTime int
);
CREATE STREAM JSONSourceStream OF JSONAccessLogEntry;

CREATE SOURCE JSONAccessLogSource USING FileReader(
  directory:'@TEST-DATA-PATH@',
  wildcard:'FileWithJSONTest.json',
  positionbyeof:false
)
parse using JSONParser (
  eventType:'admin.JSONAccessLogEntry'
) OUTPUT TO JSONSourceStream;

create Target JSONDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/jsondata') input FROM JSONSourceStream;

END APPLICATION jsonapp;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using S3Writer(
    bucketname:'@BUCKET@',
   objectname:'upgradeData.csv',
   foldername:'upgradefolder',
  uploadpolicy:'EventCount : 10000,Interval :1m '
)
format using DSVFormatter (
)
input from @STREAM@;

end application @APPNAME@;

STOP APPLICATION @WRITERAPPNAME@;
UNDEPLOY APPLICATION @WRITERAPPNAME@;
DROP APPLICATION @WRITERAPPNAME@ CASCADE;
create or replace PROPERTYVARIABLE SRC_PASSWORD='@PROP_VAR@';
CREATE APPLICATION @WRITERAPPNAME@ RECOVERY 10 SECOND INTERVAL;
-- USE EXCEPTIONSTORE;

CREATE SOURCE @SOURCE@ USING OracleReader
(
FetchSize:1,
Username:'@SOURCE_USER@',
Password:'85d7qFnwTW8=',
--Password:'$SRC_PASSWORD',
ConnectionURL:'@CONNECTION_URL@',
Tables:'@SOURCE_TABLE@',
password_encrypted: 'true'
)
OUTPUT TO @STREAM1@;

CREATE OR REPLACE TYPE @TYPE@( 
datae java.util.HashMap , 
TABLE_NAME java.lang.String , 
OPS_NAME java.lang.String , 
DB_TIMESTAMP java.lang.String  ,
COMMITSCN java.lang.String ,
SCN java.lang.String ,
REC_INS_TIME java.lang.String );

CREATE CQ @CQ1@
INSERT INTO @STREAM2@
SELECT  
CASE WHEN (META(c,"OperationName").toString() == "DELETE")
THEN putUserData(c, 'isDelete', 'true') 
ELSE
putUserData(c,'isDelete', 'false')
END
FROM @STREAM1@ c;

CREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;

CREATE OR REPLACE CQ @CQ2@ 
INSERT INTO @STREAM3@
SELECT 
data(e),
META(e,"TableName").toString() as TABLE_NAME,
META(e, "OperationName").toString() as OPS_NAME,
META(e, "TimeStamp").toString() as DB_TIMESTAMP,
META(e,"COMMITSCN").toString() as COMMITSCN ,
META(e,"SCN").toString() as  SCN ,
DNOW().toString() as REC_INS_TIME
FROM @STREAM1@ e;

create Target @TARGET1@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC1',
ParallelThreads:'',
PartitionKey:'@metadata(TableName)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from @STREAM1@;


create Target @TARGET2@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC2',
ParallelThreads:'2',
PartitionKey:'TABLE_NAME',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from @STREAM3@;

create Target @TARGET3@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC3',
ParallelThreads:'',
PartitionKey:'@userdata(isDelete)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest.avsc')
input from @STREAM2@;

end application @WRITERAPPNAME@;
deploy application @WRITERAPPNAME@;
start @WRITERAPPNAME@;
stop application @READERAPPNAME@;
undeploy application @READERAPPNAME@;
drop application @READERAPPNAME@ cascade;
CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC1',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;


CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
filename:'@READERAPPNAME@_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream1;

CREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC2',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;


CREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC3',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest.avsc'
)
OUTPUT TO KafkaReaderStream3;


end application @READERAPPNAME@;

Stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 1 second interval;
CREATE TYPE @APPNAME@_Test_1_type
( id java.lang.Integer KEY , 
status java.lang.String  
 );

CREATE OR REPLACE CACHE @APPNAME@_Test_1 USING DatabaseReader ( 
  --ConnectionURL: 'jdbc:sqlserver://@MSSQLIP@:@MSSQLPORT@',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433',
  DatabaseProviderType: 'Default',
  FetchSize: 1,
  Password:'@Password@',
  Query: 'Select id,status from [QATEST].[QATEST].[Test_1]',
  Username: '@Username@'
 ) 
QUERY ( 
  keytomap: 'id',
  skipinvalid: 'false'
 ) 
 OF @APPNAME@_Test_1_type;

CREATE TYPE @APPNAME@_Test_3_type
( id java.lang.Integer KEY , 
status java.lang.String  
 );

CREATE TYPE @APPNAME@_Test_2_type
( id java.lang.Integer KEY , 
status java.lang.String  
 );

CREATE OR REPLACE CACHE @APPNAME@_Test_3 USING DatabaseReader ( 
  --ConnectionURL: 'jdbc:sqlserver://@MSSQLIP@:@MSSQLPORT@',
    ConnectionURL: 'jdbc:sqlserver://localhost:1433',
  DatabaseProviderType: 'Default',
  FetchSize: 1,
  Password:'@Password@',
  Query: 'Select id,status from [QATEST].[QATEST].[Test_3]',
  Username: '@Username@'
 ) 
QUERY ( 
  keytomap: 'id',
  skipinvalid: 'false'
 ) 
 OF @APPNAME@_Test_3_type;
 
 CREATE OR REPLACE CACHE @APPNAME@_Test_2 USING DatabaseReader ( 
  --ConnectionURL: 'jdbc:sqlserver://@MSSQLIP@:@MSSQLPORT@',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433',
  DatabaseProviderType: 'Default',
  FetchSize: 1,
  Password:'@Password@',
  Query: 'Select id,status from [QATEST].[QATEST].[Test_2]',
  Username: '@Username@'
 ) 
QUERY ( 
  keytomap: 'id',
  skipinvalid: 'false',
  refreshinterval: '5 SECOND'
 ) 
 OF @APPNAME@_Test_2_type;
 

CREATE OR REPLACE SOURCE @APPNAME@_s1 USING MSSqlReader  ( 
  Compression: true,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.Test_Master',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
) 
OUTPUT TO @APPNAME@_ss1;

CREATE OR REPLACE SOURCE @APPNAME@_s2 USING MSSqlReader  ( 
  Compression: true,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qates%.Test_%',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
) 
OUTPUT TO @APPNAME@_ss2;

CREATE TYPE @APPNAME@_ExtractedFields_Type  ( 
id java.lang.Long , 
name java.lang.String , 
city java.lang.String , 
Test_1_id java.lang.Long , 
Test_3_id java.lang.Long ,
Test_2_id java.lang.Long 
 );

CREATE OR REPLACE STREAM @APPNAME@_ExtractedFields OF @APPNAME@_ExtractedFields_Type;


CREATE OR REPLACE CQ @APPNAME@_ExtractedFields_cq
INSERT INTO @APPNAME@_ExtractedFields
SELECT 
   b.data[0] as id,
   b.data[1] as name,
   b.data[2] as city,
   b.data[3] as Test_1_ID,
   b.data[4] as Test_3_ID,
   b.data[5] as Test_2_ID
FROM
@APPNAME@_ss1 b LEFT OUTER JOIN @APPNAME@_Test_2 s ON TO_INT(b.data[0]) = s.id
 LEFT OUTER JOIN 
@APPNAME@_Test_1 p ON TO_INT(b.data[3]) = p.id
 LEFT OUTER JOIN
@APPNAME@_Test_3 r ON TO_INT(b.data[4]) = r.id
WHERE (TO_STRING(META(b, "OperationName")) = "UPDATE" or TO_STRING(META(b, "OperationName")) = "INSERT")
limit 1;

CREATE  TARGET @APPNAME@_t1_dsv USING AzureEventHubWriter  ( 
  EventHubNamespace: 'EventHubWriterTest',
  EventHubName: 'test_01',
  E1P: 'true',
  SASPolicyName: 'RootManageSharedAccessKey',
  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
  PartitionKey: 'id',
  ConsumerGroup: 'test_01_cg',
  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
  --ParallelThreads: '2'
 ) 
FORMAT USING DSVFormatter  (  ) 
INPUT FROM @APPNAME@_ExtractedFields;

CREATE  TARGET @APPNAME@_t2_json USING AzureEventHubWriter  ( 
  EventHubNamespace: 'EventHubWriterTest',
  EventHubName: 'test_02',
  E1P: 'true',
  SASPolicyName: 'RootManageSharedAccessKey',
  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
  PartitionKey: 'id',
  ConsumerGroup: 'test_02_cg',
  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
  --ParallelThreads: '2'
 ) 
FORMAT USING JSONFormatter  (  ) 
INPUT FROM @APPNAME@_ExtractedFields;
CREATE  TARGET @APPNAME@_t3_avro USING AzureEventHubWriter  ( 
  EventHubNamespace: 'EventHubWriterTest',
  EventHubName: 'test_03',
  E1P: 'true',
  SASPolicyName: 'RootManageSharedAccessKey',
  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
  PartitionKey: 'id',
  ConsumerGroup: 'test_03_cg',
  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
  --ParallelThreads: '2'
 ) 
FORMAT USING AvroFormatter  (   schemaFileName: 'kafkaAvroTest_multipleReader.avsc'
 ) 
INPUT FROM @APPNAME@_ExtractedFields;

CREATE  TARGET @APPNAME@_t1_dsv_rawstream USING AzureEventHubWriter  ( 
  EventHubNamespace: 'EventHubWriterTest',
  EventHubName: 'test_04',
  E1P: 'true',
  SASPolicyName: 'RootManageSharedAccessKey',
  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
  PartitionKey: '@metadata(TableName)',
  ConsumerGroup: 'test_04_cg',
  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
  --ParallelThreads: '2'
 ) 
FORMAT USING DSVFormatter  (  ) 
INPUT FROM @APPNAME@_ss2;

CREATE  TARGET @APPNAME@_t2_json_rawstream USING AzureEventHubWriter  ( 
  EventHubNamespace: 'EventHubWriterTest',
  EventHubName: 'test_05',
  E1P: 'true',
  SASPolicyName: 'RootManageSharedAccessKey',
  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
  PartitionKey: '@metadata(TableName)',
  ConsumerGroup: 'test_05_cg',
  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
  --ParallelThreads: '2'
 ) 
FORMAT USING JSONFormatter  (  ) 
INPUT FROM @APPNAME@_ss2;

CREATE  TARGET @APPNAME@_t3_avro_rawstream USING AzureEventHubWriter  ( 
  EventHubNamespace: 'EventHubWriterTest',
  EventHubName: 'test_06',
  E1P: 'true',
  SASPolicyName: 'RootManageSharedAccessKey',
  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
  PartitionKey: '@metadata(TableName)',
  ConsumerGroup: 'test_06_cg',
  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
  --ParallelThreads: '2'
 ) 
FORMAT USING AvroFormatter  (   schemaFileName: 'kafkaAvroTest_multipleReader.avsc'
 ) 
INPUT FROM @APPNAME@_ss2;

END APPLICATION @APPNAME@;
Deploy application @APPNAME@;
start @APPNAME@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'TargetPosDataXmlTI',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:merchantid:text=merchantname'
)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlTI_actual.log') input from TypedCSVStream;

end application DSV;

STOP APPLICATION orrs;
UNDEPLOY APPLICATION orrs;
DROP APPLICATION orrs CASCADE;
CREATE APPLICATION orrs;
Create Type CSVType (
  merchantName String,
  companyname String
);

Create Stream TypedFileStream of CSVType;

create source CSVSource using FileReader (
	directory:'@DIRECTORY@',
	WildCard:'posdata5L.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	
)
OUTPUT TO FileStream;

CREATE CQ CsvToPosData
INSERT INTO TypedFileStream
SELECT TO_STRING(data[1]),TO_STRING(data[0])
FROM FileStream;

CREATE TARGET RSTarget USING RedshiftWriter
(
  ConnectionURL: '@TARGET_URL@',
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  bucketname: '@BUCKET-NAME@',
  --accesskeyId: '@ACCESS-KEY-ID@',
  --secretaccesskey: '@SECRET-ACCESS-KEY@',
   S3IAMRole:'@IAMROLE@',
  Tables: '@TABLES@',
  uploadpolicy: 'eventcount:10000, interval:1m'
) INPUT FROM TypedFileStream;
DEPLOY APPLICATION orrs;
START APPLICATION orrs;

stop MySQLToAzure;
undeploy application MySQLToAzure;
DROP APPLICATION MySQLToAzure CASCADE;
CREATE APPLICATION MySQLToAzure recovery 5 second interval;;

Create Source MySQLSOURCE Using MySQLReader
(
  Username: '@MYSQL-USERNAME@',
  Password: '@MYSQL-PASSWORD@',
  ConnectionURL: '@MYSQL-URL@',
  Database:'@MYSQL-DATABASE@',
  Tables: '@SOURCE_TABLES@'
) 
Output To str;


CREATE  TARGET t4 USING SysOut  ( 
  name: 'sqltors'
 ) 
INPUT FROM str;

create target MySQLAzureTarget using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

END APPLICATION MySQLToAzure;
deploy application MySQLToAzure;
start application MySQLToAzure;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ recovery 1 second interval;

CREATE SOURCE @SOURCENAME@ USING OracleReader
(
    Username: '@SRCUSERNAME@',
    Password: '@SRCPASSWORD@',
    ConnectionURL: '@SRCURL',
    Tables: '@SRCTABLE',
    FetchSize: '@FETCHSIZE@',
    CommittedTransactions: true
)

OUTPUT TO @STREAM@ ;

CREATE TARGET @TARGETNAME@ using DatabaseWriter
(
    ConnectionURL: '@TARGETURL',
    username: '@TARGETUSERNAME@',
    Password: '@TARGETPASSWORD@',
    Tables: '@TARGETTABLE@',
    BatchPolicy:'EventCount:1,Interval:1',
    CommitPolicy:'EventCount:1,Interval:1'
)
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using DatabaseReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@;

CREATE SOURCE @SourceName@ USING MySQLReader  ( 
ReaderType: 'LogMiner', 
  Password_encrypted: 'false', 
  DatabaseName: 'qatest',
  SupportPDB: false, 
  QuiesceMarkerTable: 'QUIESCEMARKER', 
  QueueSize: 2048, 
  CommittedTransactions: true, 
  Username: '@UserName@', 
  TransactionBufferType: 'Memory', 
  TransactionBufferDiskLocation: '.striim/LargeBuffer', 
  OutboundServerProcessName: 'WebActionXStream', 
  Password: '@Password@', 
  DDLCaptureMode: 'All', 
  Compression: false, 
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', 
  FetchSize: 1, 
  Tables: '@SourceTables@', 
  DictionaryMode: 'OnlineCatalog', 
  XstreamTimeOut: 600, 
  TransactionBufferSpilloverSize: '1MB', 
  FilterTransactionBoundaries: true, 
  StartSCN: 'null', 
  ConnectionURL: '@ConnectionURL@', 
  SendBeforeImage: true ) 
OUTPUT TO @AppStream@  ;

CREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream SELECT data[2], to_string(to_date(data[2]), "dd-MMM-yy hh.mm.ss") FROM @AppStream@ o ;

CREATE  TARGET @targetsys@ USING Global.SysOut  ( 
name: 'ora1_sys' ) 
INPUT FROM admin.ZDT_cq_stream;

create Target @TargetFile@ using FileWriter(
  filename:'toStringOut.log',
  directory:'@FilePath@',
  rolloverpolicy:'eventcount:1000'
)
format using DSVFormatter (

)
input from admin.ZDT_cq_stream;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ recovery 5 second interval;

CREATE OR REPLACE SOURCE @SOURCENAME@ USING IncrementalBatchReader  (
  FetchSize: 1000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: @startPosition@,
  PollingInterval: '20sec'
  )
  OUTPUT TO @STREAM@;

  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;

  CREATE TARGET @targetName@ USING DatabaseWriter(
    ConnectionURL:'@READER-URL@',
    Username:'@READER-UNAME@',
    Password:'@READER-PASSWORD@',
    BatchPolicy:'Eventcount:1000,Interval:1000',
    CommitPolicy:'Eventcount:1000,Interval:1000',
    Checkpointtable:'CHKPOINT',
    Tables:'@WATABLES@,@WATABLES@_target'
  ) INPUT FROM @STREAM@;

  END APPLICATION @APPNAME@;

  DEPLOY APPLICATION @APPNAME@;
  start application @APPNAME@;

CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (
  StartPosition: '@startPosition@',
  ConnectionURL: '@sourceURL@',
  Username: '@userName@',
  Tables: '@tables@',
  CheckColumn: '@checkColum@',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  Password: '@password@',
  Password_encrypted: 'false',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

stop application JMSWriter.JMS;
undeploy application JMSWriter.JMS;
drop application JMSWriter.JMS cascade;

create application JMS;
create source JMSCSVSource using FileReader (
        directory:'@TEST-DATA-PATH@',
        WildCard:'AdhocQueryData2.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target JmsTarget  using JMSWriter (
		Provider:'@JMSWRITERPROVIDER@',
		Ctx:'@JMSWRITERCONTEXT@',
		messagetype: @MESSAGETYPE@,
		UserName:'@JMSWRITERUSERNAME@',
		Password:'@JMSWRITERPASSWORD@',
		@DESTINATIONTYPE@)
format using @JMSTARGETFORMATTERTYPE@ (
@JMSTARGETFORMATTERMEMBERS@
)
input from TypedCSVStream;

end Application Jms;

Create Source @SOURCE_NAME@
 Using DatabaseReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
 Tables:'QATEST.DATE_TIME',
 --Query:"SELECT * FROM qatest.oracle_alldatatypes",
 FetchSize:10000,
 QueueSize:2048,
) Output To @STREAM@;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[2]) = 'Null' ;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE OR REPLACE SOURCE @parquetsrc@ USING Global.HDFSReader (
  wildcard: '',
  directory: '',
  hadoopurl: '',
  hadoopconfigurationpath: '',
  positionbyeof: false )
  PARSE USING ParquetParser (
   )
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING AvroFormatter  (
schemaFileName: 'AvroFileSchema'
)
INPUT FROM @appname@CqOut;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using AvroFormatter (
schemaFileName: 'AvroS3Schema'
)
input from @appname@CqOut;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using AvroFormatter (
schemaFileName: 'AvroAzureSchema'
)
INPUT FROM @appname@CqOut;

CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using AvroFormatter (
schemaFileName: 'AvroGCSSchema'
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@  RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @srcName@ USING OracleReader (
 Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@'
)
OUTPUT TO @outstreamname@;

CREATE  TARGET @tgtName@ USING Global.KinesisWriter ( 
  BatchPolicy: 'Size:900000,Interval:1', 
  streamName: '@streamname@', 
  accesskeyid: '@accesskeyid@', 
  Mode: 'Sync', 
  regionName: '@region@', 
  secretaccesskey: '@secretaccesskey@', 
  adapterName: 'KinesisWriter' ) 
FORMAT USING Global.DSVFormatter  ( 
  quotecharacter: '\"', 
  handler: 'com.webaction.proc.DSVFormatter', 
  columndelimiter: ',', 
  formatterName: 'DSVFormatter', 
  nullvalue: 'NULL', 
  usequotes: 'false', 
  rowdelimiter: '\n', 
  standard: 'none', 
  header: 'false' ) 
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

CREATE FLOW @STREAM@_SourceFlow;

CREATE SOURCE @SOURCE_NAME@ Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
   _h_ReturnDateTimeAs:'ZonedDateTime'
) OUTPUT TO @STREAM@;

END FLOW @STREAM@_SourceFlow;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using AzureblobWriter(
    accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:7'
)
format using DSVFormatter (
)
input from @STREAM@;

end application @APPNAME@;

STOP APPLICATION app1;
STOP APPLICATION app2;
STOP APPLICATION app3;
STOP APPLICATION app4;
STOP APPLICATION app5;
STOP APPLICATION app6;
STOP APPLICATION app7;
UNDEPLOY APPLICATION app1;
UNDEPLOY APPLICATION app2;
UNDEPLOY APPLICATION app3;
UNDEPLOY APPLICATION app4;
UNDEPLOY APPLICATION app5;
UNDEPLOY APPLICATION app6;
UNDEPLOY APPLICATION app7;
DROP APPLICATION app1 CASCADE;
DROP APPLICATION app2 CASCADE;
DROP APPLICATION app3 CASCADE;
DROP APPLICATION app4 CASCADE;
DROP APPLICATION app5 CASCADE;
DROP APPLICATION app6 CASCADE;
DROP APPLICATION app7 CASCADE;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',
acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

CREATE APPLICATION app1 RECOVERY 1 SECOND INTERVAL;

create flow sourceflow;

create type type1(
  id String,
  name String,
  city string
);

CREATE STREAM rawstream OF Global.waevent persist using KafkaPropset;
CREATE STREAM kps_typedStream OF type1 partition by city persist using KafkaPropset;
CREATE STREAM sourcestream of Global.waevent;

CREATE OR REPLACE SOURCE s USING oracleReader  (
  Username:'qatest',
  Password:'qatest',
  ConnectionURL:'localhost:1521/xe',
  Tables:'QATEST.test%',
  FetchSize:1
 )
OUTPUT TO rawstream;

end flow sourceflow;
create flow targetflow;
create cq cq1
INSERT INTO sourcestream
SELECT * from rawstream;

CREATE CQ cq2
INSERT INTO kps_typedStream
SELECT TO_STRING(data[0]),
TO_STRING(data[1]),
TO_STRING(data[2])FROM rawstream;
end flow targetflow;

end application app1;
-- deploy application app1;
-- deploy application app1 with sourceflow in AGENTS, targetflow on any in default;

CREATE APPLICATION app2 RECOVERY 1 SECOND INTERVAL;

CREATE TARGET app2_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS1'
) INPUT FROM rawstream;


end application app2;
deploy application app2;


CREATE APPLICATION app3 RECOVERY 1 SECOND INTERVAL;

CREATE TARGET app3_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test02,QATEST.KPS2'
) INPUT FROM rawstream;

end application app3;
deploy application app3;


CREATE APPLICATION app4 RECOVERY 1 SECOND INTERVAL;

CREATE TARGET app4_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test03,QATEST.KPS3'
) INPUT FROM rawstream;

end application app4;
deploy application app4;


CREATE APPLICATION app5 RECOVERY 1 SECOND INTERVAL;

CREATE TARGET app5_target1 USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'snappy',
KafkaConfig:'compression.type=snappy'
)
FORMAT USING DSVFormatter ()
INPUT FROM kps_typedStream;

CREATE TARGET app5_target2 USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'gzip',
KafkaConfig:'compression.type=gzip'
)
FORMAT USING DSVFormatter ()
INPUT FROM rawstream;

CREATE TARGET app5_target3 USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'lz4',
KafkaConfig:'compression.type=lz4'
)
FORMAT USING DSVFormatter ()
INPUT FROM rawstream;


end application app5;
deploy application app5;

CREATE APPLICATION app6 RECOVERY 1 SECOND INTERVAL;

CREATE TARGET app6_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test03,QATEST.KPS4'
) INPUT FROM rawstream;

end application app6;
deploy application app6;

CREATE APPLICATION app7 RECOVERY 1 SECOND INTERVAL;

CREATE TARGET app7_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS_NEW1;qatest.test02,QATEST.KPS_NEW2;qatest.test03,QATEST.KPS_NEW3;'
) INPUT FROM rawstream;

end application app7;
deploy application app7;

Create Source @SOURCE_NAME@ Using OracleReader
(
 Compression: false,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  --StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
) Output To @STREAM@;

IMPORT cern.colt.list.DoubleArrayList;

IMPORT com.webaction.runtime.LocationInfo;

IMPORT com.webaction.runtime.LocationArrayList;

UNDEPLOY APPLICATION FraudTester.FraudDetectionApp;
DROP APPLICATION FraudTester.FraudDetectionApp cascade;

CREATE APPLICATION FraudDetectionApp;

CREATE SOURCE CsvDataSource USING CSVReader
(
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'fraudPosData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE PosData
(
  merchantId String,
  pan String,
  datetime DateTime,
  hourValue int,
  amount double,
  zip String
);

CREATE STREAM PosDataStream OF PosData PARTITION BY merchantId;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT data[1], data[2], TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]), data[9]
FROM CsvStream;

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON datetime
PARTITION BY merchantId;

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE ZipLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  header: Yes,
  columndelimiter: '	',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

CREATE TYPE MerchantNameData(
  merchantId		String KEY,
  companyName		String
);

CREATE CACHE NameLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'MerchantNames.csv',
  header: Yes,
  columndelimiter: ',',
  trimquote:false
) QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE TYPE CustomerDetails(
  pan String KEY,
  fname String,
  lname String,
  address String,
  city String,
  state String,
  zip String,
  gender String
);

CREATE CACHE CustomerLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'customerdetails.csv',
  header: Yes,
  columndelimiter: ',',
  trimquote:false
) QUERY(keytomap:'pan') OF CustomerDetails;

CREATE TYPE PanWithMerchantData(
wKey String KEY,
pan String,
count int,
datetime DateTime,
locations com.webaction.runtime.LocationArrayList);

CREATE STREAM PanWithMerchantDataStream of PanWithMerchantData PARTITION BY wKey;

CREATE CQ CsvToPanWithMerchantData
INSERT into PanWithMerchantDataStream
SELECT p.pan+p.datetime, p.pan , count(*), p.datetime, locInfoList(z.latVal, z.longVal, z.city, z.zip, n.companyName)
FROM PosData5Minutes p, ZipLookup z, NameLookup n
WHERE p.zip = z.zip AND p.merchantId = n.merchantId group by p.pan having count(*) > 1;

CREATE TYPE PanWithMerchantAndCustomerData(
wKey String KEY,
pan String,
fname String,
lname String,
address String,
city String,
state String,
zip String,
gender String,
count int,
datetime DateTime,
locations com.webaction.runtime.LocationArrayList);

CREATE STREAM PanWithMerchantAndCustomerDataStream of PanWithMerchantAndCustomerData PARTITION BY wKey;

CREATE CQ PanWithMerchantToMerchantAndCustomerData
INSERT into PanWithMerchantAndCustomerDataStream SELECT ds.wKey, ds.pan, c.fname, c.lname, c.address, c.city, c.state, c.zip, c.gender, ds.count, ds.datetime, ds.locations
FROM PanWithMerchantDataStream ds, CustomerLookup c
WHERE ds.pan = c.pan;

CREATE TYPE AlertRecord(
  name String,
  keyVal String,
  severity String,
  flag String ,
  message String
);

CREATE STREAM AlertStream OF AlertRecord;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT p.pan, p.wKey,
       CASE
         WHEN p.count <= 2  THEN 'info'
      WHEN p.count > 2 THEN 'warning'
         ELSE 'error' END,
       CASE
         WHEN p.count > 1 THEN 'raise'
         ELSE 'cancel' END,
       CASE
         WHEN p.count <= 2 THEN textFromFields('PAN $2 has been used $10 times in a 5 minute period',p)
         WHEN p.count > 2 THEN textFromFields('[Abnormal Usage] PAN $2 has been used $10 times in a 5 minute period',p)
         ELSE ''
         END
FROM PanWithMerchantAndCustomerDataStream p;

CREATE TARGET SendAlert USING AlertSender(
 name: unusualActivity
) INPUT FROM AlertStream;

CREATE WACTIONSTORE FraudActivity CONTEXT OF PanWithMerchantAndCustomerData
EVENT TYPES ( PanWithMerchantAndCustomerData )
@PERSIST-TYPE@


Create CQ TrackFraudActivity
INSERT INTO FraudActivity
Select * from PanWithMerchantAndCustomerDataStream
LINK SOURCE EVENT;


END APPLICATION FraudDetectionApp;

undeploy application reconnect;
alter application reconnect;

create or replace TARGET dbtarget USING DatabaseWriter  (
  ConnectionURL:'@URL@',
  Username:'@USERNAME@',
  Password:'@PASSWORD@',
  ConnectionRetryPolicy: 'retryInterval=20s, maxRetries=3',
  BatchPolicy:'EventCount:5,Interval:30',
  CommitPolicy:'EventCount:5,Interval:30',
  Tables: '@TABLES@'
 )
INPUT FROM sqlstream;

alter application reconnect recompile;
deploy application reconnect;
start application reconnect;

STOP APPLICATION testApp;
UNDEPLOY APPLICATION testApp;
DROP APPLICATION testApp CASCADE;
-- DROP EXCEPTIONSTORE testApp_exceptionstore;

CREATE APPLICATION testApp WITH ENCRYPTION RECOVERY 10 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE OR REPLACE SOURCE testApp_Source USING GGTrailReader ( 

  SupportColumnCharset: false, 
  TrailFilePattern: 'd1*', 
  DefinitionFile: '/Users/gopinaths/Product/IntegrationTests/TestData/OGG/alldatatype/alldtype.def', 
  Compression: false, 
  TrailDirectory: '/Users/gopinaths/Product/IntegrationTests/TestData/OGG/alldatatype', 
  Tables: 'QATEST.ALLDTYPE', 
  FilterTransactionBoundaries: true, 
  ExcludedTables:'waction.CHKPOINT',
  TrailByteOrder: 'LittleEndian' ) 
OUTPUT TO testApp_Stream;

CREATE OR REPLACE TARGET testApp_Target USING BigQueryWriter  (
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  projectId:'striimqa-214712',
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:5,Interval:120',
  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', 
  AllowQuotedNewLines: 'false', 
  adapterName: 'BigQueryWriter', 
  optimizedMerge: 'true', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', 
  StandardSQL: 'true', 
  QuoteCharacter: '\"', 
  Tables: 'QATEST.ALLDTYPE,.oratobqtgt',
  Mode: 'MERGE',
  StandardSQL: 'true',
  QuoteCharacter: '\"'
  ) INPUT FROM testApp_Stream;

CREATE OR REPLACE TARGET testApp_SysOut USING Global.SysOut (name: 'wa') INPUT FROM testApp_Stream;

END APPLICATION testApp;
DEPLOY APPLICATION testApp;
START testApp;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING Global.DeltaLakeWriter (
  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.testaswin',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @STREAM@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE @SourceName@ USING PostgreSQLReader  (
  ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: '@UserName@',
  Password_encrypted: false,
  ConnectionURL: '@SourceConnectionURL@',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: '@Password@',
  Tables: '@SourceTable@'
 ) 
OUTPUT TO @SRCINPUTSTREAM@;


CREATE OR REPLACE TARGET @targetsys@ USING SysOut (name: 'ora12_out') INPUT FROM @SRCINPUTSTREAM@;


CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@UserName@',
  Password_encrypted: 'false',
  BatchPolicy: 'EventCount:1,Interval:60',
  CommitPolicy: 'EventCount:1,Interval:60',
  ConnectionURL: '@TargetConnectionURL@',
  Tables: '@SourceTable@,@TargetTable@',

  adapterName: 'PostgreSQLReader',
  Password: '@Password@'
 )
INPUT FROM @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

STOP AdhocTester.ws_one;
UNDEPLOY APPLICATION AdhocTester.ws_one;
DROP APPLICATION AdhocTester.ws_one cascade;

CREATE APPLICATION ws_one;

CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO QaStream;

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE ZipLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'AdhocQueryData.csv',
  header: Yes,
  columndelimiter: '	',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

Create TYPE wsData(
	CompanyNum String,
	CompanyName String KEY,
	CompanyCode int,
	Zip String
);


CREATE STREAM wsStream OF wsData;

--Select data from QaStream and insert into wsStream

CREATE CQ csvTowsData
INSERT INTO wsStream
SELECT  data[0],
    data[1],
    TO_INT(data[3]),
    data[9]
 FROM QaStream;




CREATE WACTIONSTORE oneWS CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@


CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT * FROM wsStream
LINK SOURCE EVENT;

END APPLICATION ws_one;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;

 CREATE OR REPLACE SOURCE Teradata_source_App2 USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01;striim.test02;striim.test03',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',
 startPosition: 'striim.test01=1;striim.test02=-1;%=0',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream1 ;

  CREATE OR REPLACE TARGET sys2 USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream1;

create target AzureSQLDWHTarget_app2 using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream1;


CREATE TARGET log USING LOGWRITER
(
 name:log,
 filename:'LOGFILENAME'
)
INPUT FROM data_stream1;


END APPLICATION IR;

deploy application IR;
start IR;

CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (
  StartPosition: '@startPosition@',
  ConnectionURL: '@sourceURL@',
  Username: '@userName@',
  Tables: '@tables@',
  CheckColumn: '@checkColum@',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  Password: '@password@',
  Password_encrypted: 'false',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (
  positionByEOF: false,
  wildcard: '',
  directory: '' )
PARSE USING XMLParserV2 (
  rootnode: 'document/MerchantName' )
OUTPUT TO @APPNAME@Stream;

CREATE CQ @APPNAME@CQ
INSERT INTO @APPNAME@CQStream
SELECT
data.attributeValue("merchantid") as merchantid,
data.getText() as MerchantName
FROM @APPNAME@Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@CQStream;

END APPLICATION @APPNAME@;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;

Create Source oracSource
 Using DatabaseReader
(
 Username:'oracle_username',
 Password:'oracle_password',
 ConnectionURL:'oracle_connection',
 Tables:'src_tables',
 Query:"SELECT * FROM qatest.oracle_alldatatypes",
 OnlineCatalog:true,
 FetchSize:10000,
 QueueSize:2048,
 CommittedTransactions:true,
 Compression:false
) Output To DataStream;
CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkuPDaTehAnDliNgmOdE:'DELETEANDINSERT',
tables: 'tgt_tables',
batchpolicy: 'EventCount:10000,Interval:30')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

STOP bq;
UNDEPLOY APPLICATION bq;
DROP APPLICATION bq CASCADE;

CREATE APPLICATION bq RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE BQ_source USING OracleReader
(
	Username:'qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
  OnlineCatalog:true,
  FetchSize:'1',
  Tables: 'QATEST.sourceTable'
)
OUTPUT TO SS;


CREATE or replace TARGET BQ_target USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
    Tables:'QATEST.sourceTable,qatest.% keycolumns(RONUM)',
    mode:'Appendonly',
    datalocation: 'US',
	nullmarker: 'NULL',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:100,Interval:10'	
) INPUT FROM ss;

CREATE OR REPLACE TARGET bq_SysOut USING Global.SysOut (name: 'wa') INPUT FROM ss;

END APPLICATION bq;
DEPLOY APPLICATION bq;
START APPLICATION bq;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName1@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM1@;
Create Source @SourceName2@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM2@;
Create Source @SourceName3@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM3@;
Create Source @SourceName4@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM4@;
Create Source @SourceName5@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM5@;
Create Source @SourceName6@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM6@;
Create Source @SourceName7@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM7@;
Create Source @SourceName8@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM8@;
Create Source @SourceName9@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM9@;
Create Source @SourceName10@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM10@;

CREATE TARGET @targetName1@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM1@;
CREATE TARGET @targetName2@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM2@;
CREATE TARGET @targetName3@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM3@;
CREATE TARGET @targetName4@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM4@;
CREATE TARGET @targetName5@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM5@;
CREATE TARGET @targetName6@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM6@;
CREATE TARGET @targetName7@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM7@;
CREATE TARGET @targetName8@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM8@;
CREATE TARGET @targetName9@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM9@;
CREATE TARGET @targetName10@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM10@;


END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

create Target @TARGET_NAME@ using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'eventCount:1000',
    ServiceAccountKey:'@file-path@'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @STREAM@;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;
Create Source OracleSource Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;


create target AzureTarget using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

create Target t2 using SysOut(name:Foo2) input from str;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

CREATE TARGET @TARGET_NAME@ USING SnowflakeWriter (
  ConnectionURL: '@SNOWFLAKE-URL@',
  username: '@SNOWFLAKE-USERNAME@',
  password: '@SNOWFLAKE-PASSWORD@',
  appendOnly:'false',
  Tables: '@TARGET-TABLES@'
 )
 INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

--
-- Crash Recovery Test 5 with Jumping window and partitioned on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION N4S4CR5Tester.N4S4CRTest5;
UNDEPLOY APPLICATION N4S4CR5Tester.N4S4CRTest5;
DROP APPLICATION N4S4CR5Tester.N4S4CRTest5 CASCADE;
CREATE APPLICATION N4S4CRTest5 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest5;

CREATE SOURCE CsvSourceN4S4CRTest5 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest5;

CREATE FLOW DataProcessingN4S4CRTest5;

CREATE TYPE CsvDataN4S4CRTest5 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvDataN4S4CRTest5 PARTITION BY merchantId;

CREATE CQ CsvToDataN4S4CRTest5
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsN4S4CRTest5 CONTEXT OF CsvDataN4S4CRTest5
EVENT TYPES ( CsvDataN4S4CRTest5 )
@PERSIST-TYPE@

CREATE CQ DataToWactionN4S4CRTest5
INSERT INTO WactionsN4S4CRTest5
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN4S4CRTest5;

END APPLICATION N4S4CRTest5;

STOP application XmlFormatterTester.DSV;
undeploy application XmlFormatterTester.DSV;
drop application XmlFormatterTester.DSV cascade;

create application DSV;
create source CSVSmallPosDataSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvSmallPosDataStream;

create source CSVPosDataSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'posdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvPosDataStream;


Create Type CSVPosDataType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVSmallPosDataStream of CSVPosDataType;
Create Stream TypedCSVPosDataStream of CSVPosDataType;


CREATE CQ CsvToSmallPosData
INSERT INTO TypedCSVSmallPosDataStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvSmallPosDataStream;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVPosDataStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvPosDataStream;

/**
*  3.2.1.b FileWriter XML TimeInterval
**/
create Target XmlFormatterTimeInterval using FileWriter(
  filename:'TargetPosDataXmlTIOpt',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:merchantid:text=merchantname',
  charset:'UTF-8'
)
input from TypedCSVSmallPosDataStream;

/**
* 3.2.1.c FileWriter XMLFileSize 101MB
**/
create Target XmlFormatterFileSize101 using FileWriter(
  filename:'TargetPosDataXmlFS',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'FileSizeRollingPolicy,filesize:101M,sequence:00'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:merchantid:text=merchantname',
  charset:'UTF-8'
)
input from TypedCSVPosDataStream;

/**
* 3.2.1.d FileWriter XMLDefaultFS 10 MB
**/
create Target XmlFormatterDefault using FileWriter(
  filename:'TargetPosDataXmlFSDefault',
  directory:'@FEATURE-DIR@/logs/',
    sequence:'00',
  rolloverpolicy:'FileSizeRollingPolicy'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:zip:text=merchantname',
  charset:'UTF-8'
)
input from TypedCSVSmallPosDataStream;


/**
* 3.2.1.e FileWriter XML EventCount 2000
**/
create Target XmlFormatterEventCount2000 using FileWriter(
  filename:'TargetPosDataXmlEC',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'eventcount:2000,sequence:00'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:zip:text=merchantname',
  charset:'UTF-8'
)
input from TypedCSVSmallPosDataStream;

create Target LogWriterSmallPosData using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlDFS_actual.log') input from TypedCSVSmallPosDataStream;

end application DSV;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @dsvsrc@ USING FileReader ( 
  directory: '', 
  wildcard: '', 
  positionbyeof: false ) 
PARSE USING DSVParser ()
OUTPUT TO @appname@OUT;

CREATE TARGET @prqttrgt@ USING FileWriter ( 
  filename: '', 
  directory: '', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING ParquetFormatter  ( 
  schemaFileName: '',
  compressiontype: 'GZIP',
  members:'data')
INPUT FROM @appname@OUT;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application app2PS;
undeploy application app2PS;
drop application app2PS cascade;

create application app2PS;

create target File_TargerPS2 using FileWriter
(
directory : '',
filename : ''
)
format using DSVFormatter()
input from KPSRss2;

end application app2PS;

deploy application app2PS;
start application app2PS;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:@EC@'
)
format using DSVFormatter (
)
input from @STREAM@;

STOP APPLICATION DatabaseWriterTester.DatabaseReaderApp;
UNDEPLOY APPLICATION DatabaseWriterTester.DatabaseReaderApp;
DROP APPLICATION DatabaseWriterTester.DatabaseReaderApp CASCADE;
STOP APPLICATION DatabaseWriterTester.DatabaseWriterApp;
UNDEPLOY APPLICATION DatabaseWriterTester.DatabaseWriterApp;
DROP APPLICATION DatabaseWriterTester.DatabaseWriterApp CASCADE;

CREATE APPLICATION DatabaseWriterApp;

CREATE SOURCE CSVSource USING FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'banks.csv',
positionByEOF:false
)
PARSE USING DSVParser (
header:'yes'
)
OUTPUT TO Orders;

CREATE TYPE OrdersData(
  id String Key,
  name String);

CREATE STREAM OrdersDataStream OF OrdersData;

CREATE CQ CsvToOrdersData
INSERT INTO OrdersDataStream (id, name)
SELECT data[0], data[1]
FROM Orders;

CREATE TARGET WriteJpaOracle USING DatabaseWriter(
DriverName:'oracle.jdbc.OracleDriver',
ConnectionURL:'@READER-URL@',
        Username:'@READER-UNAME@',
        Password:'@READER-PASSWORD@',
Tables:'QATEST.DB_WRITER_TEST',
eventType:'DatabaseWriterTester.OrdersData'
) INPUT FROM OrdersDataStream;

END APPLICATION DatabaseWriterApp;
DEPLOY APPLICATION DatabaseWriterApp;
START APPLICATION DatabaseWriterApp;

CREATE APPLICATION DatabaseReaderApp;

create source DatabaseReaderSource USING DatabaseReader
(
ConnectionURL:'@READER-URL@',
        Username:'@READER-UNAME@',
        Password:'@READER-PASSWORD@',
Tables:'QATEST.DB_WRITER_TEST',
   FetchSize: 1

)
OUTPUT TO DatabaseReaderTestStream;

create Target t using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/Databasewritertarget_RT') input from DatabaseReaderTestStream;


END APPLICATION DatabaseReaderApp;
DEPLOY APPLICATION DatabaseReaderApp;

-- detectFraudsterApp1.tql, detectFraudsterApp1_vis_settings.json, apiCallLogs.txt are needed to run this example.
-- gamelogs simulates api call logs for an online game. xferCur represents virtual currency transfer.
-- If this method is called by same user more than once in 1000 records its anomaly and hence waction.

IMPORT static com.webaction.runtime.converters.DateConverter.*;

UNDEPLOY APPLICATION admin.detectFraudsterApp1;
DROP APPLICATION admin.detectFraudsterApp1 CASCADE;
CREATE APPLICATION detectFraudsterApp1;


/* READ DATA FROM A CSV FILE. USE CSV-READER  */
CREATE source gameCSVDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@/persistence',
  header:No,
  wildcard:'apiCallLogs.txt',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO PCsvStream;

create jumping window apiwindow over PCsvStream keep 1000 rows;

CREATE TYPE apiCallData(
  date DateTime,
  apiName String,
  amount int,
  username String key
);

CREATE STREAM apiCallStream OF apiCallData;

/* FROM INPUT DATA READ ABOVE, CREATE EVENTS OF TYPE apiCallData
 * TO_INT(data[10]) > 100000 --> 3 WACTIONS
 * TO_INT(data[10]) > 50000 --> 8 WACTIONS
 * TO_INT(data[10]) > 25000 --> 11 WACTIONS
 * TO_INT(data[10]) > 20000 --> 14 WACTIONS
 * */

CREATE CQ gameCQ
INSERT INTO apiCallStream
SELECT
    TO_DATE(TO_LONG(data[3])),
    data[10],
    TO_INT(data[11]),
    data[12]
FROM apiwindow
where cast (data[10] as String)  =  'xferCur' and  TO_INT(data[11]) > 50000;

create TYPE apiCallContext(
  date DateTime,
  apiName String,
  amount int,
  username String key
);

/* PERSISTING TO DERBY  */
CREATE WACTIONSTORE fraudWactionsDerby
CONTEXT OF apiCallContext
EVENT TYPES (apiCallData )
PERSIST EVERY 2 second USING
(
JDBC_DRIVER:'@WASTORE-DRIVER@',
JDBC_URL:'@WASTORE-URL@;CREATE=true',
JDBC_USER:'@WASTORE-UNAME@',
JDBC_PASSWORD:'@WASTORE-PASSWORD@',
DDL_GENERATION:'drop-and-create-tables',
LOGGING_LEVEL:'SEVERE'
);


CREATE CQ populateFraudWactionsDerby
INSERT INTO fraudWactionsDerby
SELECT date, apiName, amount, username
FROM apiCallStream
LINK SOURCE EVENT;


/* PERSISTING TO MYSQL  */
/*
CREATE WACTIONSTORE fraudWactionsMySQL
CONTEXT OF apiCallContext
EVENT TYPES (apiCallData )
PERSIST EVERY 3 second USING
(
JDBC_DRIVER:'com.mysql.jdbc.Driver',
JDBC_URL:'jdbc:mysql://127.0.0.1:3306/test',
JDBC_USER:root,
JDBC_PASSWORD:'root',
DDL_GENERATION:'drop-and-create-tables',
LOGGING_LEVEL:'SEVERE'
);

CREATE CQ populateFraudWactionsMySQL
INSERT INTO fraudWactionsMySQL
SELECT date, apiName, amount, username
FROM apiCallStream
LINK SOURCE EVENT;
*/
/* PERSISTING TO MONGODB  */
/*
CREATE WACTIONSTORE fraudWactionsMongo
CONTEXT OF apiCallContext
EVENT TYPES (apiCallData )
PERSIST EVERY 5 second USING
(
NOSQL_PROPERTY:'localhost:27017',
TARGET_DATABASE:'org.eclipse.persistence.nosql.adapters.mongo.MongoPlatform',
DDL_GENERATION:'drop-and-create-tables',
DB_NAME:db
);


CREATE CQ populateFraudWactionsMongo
INSERT INTO fraudWactionsMongo
SELECT date, apiName, amount, username
FROM apiCallStream
LINK SOURCE EVENT;
*/



--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM PCsvStream;
--CREATE TARGET output2 USING SysOut(name : selectedinput) input FROM apiCallStream;


CREATE VISUALIZATION detectFraudsterApp1 "@TEST-DATA-PATH@/json/detectFraudsterApp1_vis_settings.json";

END APPLICATION detectFraudsterApp1;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

Create Source dataGenSrc USING MariaDBReader  ( 
  Username:'qatest',
  Password:'w3b@ct10n',
  ConnectionURL:'jdbc:mariadb://10.77.21.53:3306/qatest',
  Tables: '@tableNames@',
  ClusterSupport: 'Galera'
 ) 
 Output To LCRStream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;

END APPLICATION DataGenSampleApp;

--
-- Recovery Test 40 with two sources and two WactionStores. A variety of partitioned windows in between
-- assure that we are testing a complicated recovery scenario.
--
-- NOTE THIS APP IS INCONSISTENT AND NOT COMPATIBLE WITH THE CURRENT VERSION OF RECOVERY BECAUSE IT HAS COMBINING STREAMS
--
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> JWc2 -> JWc5 -> WS1
--   S2 -> JWc2 -> JWc7 -> WS2
--

STOP Recov40Tester.RecovTest40;
UNDEPLOY APPLICATION Recov40Tester.RecovTest40;
DROP APPLICATION Recov40Tester.RecovTest40 CASCADE;
CREATE APPLICATION RecovTest40 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStreamTop OF CsvData;

CREATE CQ Csv1ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ Csv2ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;








CREATE JUMPING WINDOW TopJWc2
OVER DataStreamTop KEEP 2 ROWS;







CREATE STREAM DataStreamLeft OF CsvData;
CREATE STREAM DataStreamRight OF CsvData;

CREATE CQ DataToLeft
INSERT INTO DataStreamLeft
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM TopJWc2 p;

CREATE CQ DataToRight
INSERT INTO DataStreamRight
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM TopJWc2 p;






CREATE JUMPING WINDOW LeftJWc5
OVER DataStreamLeft KEEP 5 ROWS;

CREATE JUMPING WINDOW RightJWc10
OVER DataStreamRight KEEP 10 ROWS;



CREATE WACTIONSTORE WactionsLeft CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsRight CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ ToWactionsLeft
INSERT INTO WactionsLeft
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM LeftJWc5 p;

CREATE CQ ToWactionsRight
INSERT INTO WactionsRight
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM RightJWc10 p;

END APPLICATION RecovTest40;

STOP @appName@;
UNDEPLOY application @appName@;
DROP application @appName@ cascade;

CREATE APPLICATION @appName@ @recovery@;

CREATE SOURCE @appName@_MongoDBReader USING MongoDBReader
(
  QuiesceOnILCompletion:'true',
  collections:'',
  userName:'',
  password:'',
  connectionUrl:'',
  mode:''
)
OUTPUT TO @appName@_MOut;

CREATE CQ @appName@_MCQ1
INSERT INTO @appName@_MCQOut1
SELECT
data.get('_id').textValue() as _id,
data.get('index').toString() as index,
data.get('isActive').toString() as isActive,
data.get('age').toString() as age,
data.get('balance').toString() as balance,
data.get('name').textValue() as name,
data.get('gender').textValue() as gender,
data.get('company').textValue() as company,
data.get('email').textValue() as email,
data.get('phone').textValue() as phone,
data.get('registered').textValue() as registered,
data.get('latitude').toString() as latitude,
data.get('longitude').toString() as longitude
FROM @appName@_MOut m;;

CREATE TARGET @appName@_AzureEventHubWriter USING AzureEventHubWriter
(
  SASKey:'',
  EventHubName:'',
  EventHubNamespace:'',
  SASPolicyName:'',
  BatchPolicy:'Size:1000000,Interval:10s',
  ConsumerGroup:'default'
)
FORMAT USING JSONFormatter  (
 )
INPUT FROM @appName@_MCQOut1;

END APPLICATION @appName@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.test01'
 ) 
OUTPUT TO @APPNAME@_SS;


CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'public.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12)',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

STOP APPLICATION testApp;
UNDEPLOY APPLICATION testApp;
DROP APPLICATION testApp CASCADE;
-- DROP EXCEPTIONSTORE testApp_exceptionstore;

CREATE APPLICATION testApp WITH ENCRYPTION RECOVERY 10 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE OR REPLACE SOURCE testApp_Source USING OracleReader  (
  Username:'qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
  OnlineCatalog:true,
  FetchSize:'1',
  Tables: 'QATEST.srctb'
  ) OUTPUT TO testApp_Stream  ;

CREATE OR REPLACE TARGET testApp_Target USING BigQueryWriter  (
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  projectId:'striimqa-214712',
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:5,Interval:120',
  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', 
  AllowQuotedNewLines: 'false', 
  adapterName: 'BigQueryWriter', 
  optimizedMerge: 'true', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', 
  StandardSQL: 'true', 
  QuoteCharacter: '\"', 
  Tables: 'QATEST.srctb,.oratobqtgt',
  Mode: 'MERGE',
  StandardSQL: 'true',
  _h_TransportOptions:'connectionTimeout=30s, readTimeout=12s',
  QuoteCharacter: '\"'
  ) INPUT FROM testApp_Stream;

CREATE OR REPLACE TARGET testApp_SysOut USING Global.SysOut (name: 'wa') INPUT FROM testApp_Stream;

END APPLICATION testApp;
DEPLOY APPLICATION testApp;
START testApp;

STOP APPLICATION eh;
UNDEPLOY APPLICATION eh;
DROP APPLICATION eh CASCADE;
CREATE APPLICATION eh @Recovery@;

Create Source s1_orcl_w Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: 'localhost:1521:orcl',
 Tables:'QATEST.TEST_%',
 FetchSize:1
) 
Output To sourcestream;

Create Source s2_orcl Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: 'localhost:1521:orcl',
 Tables:'QATEST.TEST_01',
 FetchSize:1
) 
Output To sourcestream;

Create Source s3_orcl Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: 'localhost:1521:orcl',
 Tables:'QATEST1.TEST_01',
 FetchSize:1
) 
Output To sourcestream;


CREATE CQ OperationType
INSERT INTO OpsStream
SELECT  
CASE WHEN (META(c,"OperationName").toString() == "DELETE")
THEN putUserData(c, 'isDelete', 'true') 
ELSE
putUserData(c,'isDelete', 'false')
END
FROM sourcestream c;



CREATE OR REPLACE SOURCE s4_orcl_ibr USING IncrementalBatchReader  ( 
  FetchSize: 1,
  StartPosition: '%=0',
  Username: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@//dockerhost:1521/orcl',
  Tables: 'QATEST.TEST_%',
  CheckColumn: '%=id',
  Password: 'qatest' ) 
OUTPUT TO sourcestream ;

CREATE SOURCE s5_fr USING FileReader (
	directory:'/Users/saranyad/Product/IntegrationTests/TestData/',
    WildCard:'banks.csv',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:',',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;


CREATE TYPE cdctype(
  id int,
  name String  
);

CREATE STREAM cdctypestream OF cdctype;

CREATE CQ cdcstreamcq
INSERT INTO cdctypestream
SELECT TO_INT(p.data[0]), 
       TO_STRING(p.data[1])
FROM FileStream p;


create cq cqtowaevent 
insert into sourcestream
select convertTypedeventToWAevent(c, 'admin.cdctype')
from cdctypestream c;


create Target t1_dsv using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_01',
	E1P:'true',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
	PartitionKey:'@metadata(TableName)',
	ConsumerGroup:'reader',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
	--ParallelThreads:'2'
)
format using DSVFormatter ( 
)
input from sourcestream;





create Target t2_json using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_02',
	E1P:'true',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
	PartitionKey:'@userdata(isDelete)',
	ConsumerGroup:'reader',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:30s'
	--ParallelThreads:'2'
)
format using JSONFormatter ( 
)
input from OpsStream;


create Target t3_avro using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_03',
	E1P:'true',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
	PartitionKey:'@metadata(TableName)',
	ConsumerGroup:'reader',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
	--ParallelThreads:'2'
)
format using AvroFormatter (schemaFileName:'kafkaAvroTest_multipleReader.avsc') 
input from sourcestream;


END APPLICATION eh;

DEPLOY APPLICATION eh on any in default;

start application eh;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE FLOW @APP_NAME@SrcFlow;

CREATE OR REPLACE SOURCE @APP_NAME@_src USING FileReader (
directory:'@DIRECTORY@',
WildCard:'posdata5L.csv',
positionByEOF:false
)
parse using DSVParser (
header:'no'
)
OUTPUT TO @APP_NAME@_Stream;
END FLOW @APP_NAME@SrcFlow;

CREATE FLOW @APP_NAME@TgtFlow;

CREATE OR REPLACE TYPE @APP_NAME@_Type  ( BUSINESS_NAME java.lang.String KEY,
MERCHANT_ID java.lang.String,
PRIMARY_ACCOUNT_NUMBER java.lang.String
 ) ;

CREATE OR REPLACE STREAM @APP_NAME@_Stream2 OF @APP_NAME@_Type;
CREATE OR REPLACE CQ @APP_NAME@_CQ
INSERT INTO @APP_NAME@_Stream2
SELECT data[0],data[1],data[2]
FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:50000,interval:50s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream2;

END FLOW @APP_NAME@TgtFlow;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@ WITH @APP_NAME@SrcFlow IN agents,@APP_NAME@TgtFlow IN default;
START APPLICATION @APP_NAME@;

create Target @TARGET_NAME@ using ADLSGen2Writer(
          accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'%@metadata(TableName)%',
        filename:'table.csv',
        uploadpolicy:'filesize:10M'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@',
	members:'Table=@metadata(TableName),OpName=@metadata(OperationName)'
)
input from @STREAM@;

stop application MSSQLTransactionSupportMultiReaderWriter;
undeploy application MSSQLTransactionSupportMultiReaderWriter;
drop application MSSQLTransactionSupportMultiReaderWriter cascade;

CREATE APPLICATION MSSQLTransactionSupportMultiReaderWriter recovery 1 second interval;

Create Source ReadFromMSSQL4
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
--AutoDisableTableCDC:'true',
FetchTransactionMetadata:'true',
FilterTransactionBoundaries: false,
Compression:'false',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportMultiReaderWriterStream1;

Create Source ReadFromMSSQL5
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
--AutoDisableTableCDC:'true',
FetchTransactionMetadata:'true',
FilterTransactionBoundaries: false,
Compression:'true',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportMultiReaderWriterStream2;

CREATE TARGET WriteToMSSQL4 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC,@@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportMultiReaderWriterStream1;

CREATE TARGET WriteToMSSQL5 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC,@@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportMultiReaderWriterStream2;

/*CREATE TARGET MSSqlReaderOutput4 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportStream; 


/*CREATE OR REPLACE TARGET MSSQLFileOut4 USING FileWriter  ( 
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  directory:'@FEATURE-DIR@/logs/',
  filename: 'TransactionSupportAutoDisableTableCdcTrue.txt'
 ) 
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 ) 
INPUT FROM MSSQLTransactionSupportMultiReaderWriterStream;
*/
END APPLICATION MSSQLTransactionSupportMultiReaderWriter;
deploy application MSSQLTransactionSupportMultiReaderWriter;
start application MSSQLTransactionSupportMultiReaderWriter;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSV1Source using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'RFC4180.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'no'
)
OUTPUT TO Csv1Stream;

create Target t using FileWriter(
  filename:'FileWriterStandardRFC4180',
  directory:'@FEATURE-DIR@/logs/',
  standard : 'RFC4180',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (
members:'data'
)
input from Csv1Stream;

end application DSV;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

create source @SourceName@ using MySQLReader
  (ConnectionURL: '@SourceConnectionURL@',
   Username:'@UserName@',
   Password:'@Password@',
   Tables: '@SourceTableName@'
)
output to @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
   ConnectionURL:'@TargetConnectionURL@',
   Username:'@UserName@',
   Password:'@Password@',
   BatchPolicy:'EventCount:1,Interval:0',
   Tables: '@SourceTableName@,@TargetTableName@'
 
 ) INPUT FROM @SRCINPUTSTREAM@;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

use PosTester;
alter application PosApp;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;

end application PosApp;

alter application PosApp recompile;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSV1Source using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO Csv1Stream;

Create Type CSV1Type (
  merchantId String,
  merchantName String
);

Create Stream TypedCSV1Stream of CSV1Type;

CREATE CQ CsvToMerchantNames
INSERT INTO TypedCSV1Stream
SELECT data[0],
       data[1]
FROM Csv1Stream;

create Target t using FileWriter(
  filename:'Merchant',
  sequence:'00',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00'
)
format using DSVFormatter (

)
input from TypedCSV1Stream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetTS_Merchant_actual.log') input from TypedCSV1Stream;

end application DSV;

stop application JMSWriter.JMS;
undeploy application JMSWriter.JMS;
drop application JMSWriter.JMS cascade;

create application JMS;
create source JMSCSVSource using FileReader (
        directory:'@TEST-DATA-PATH@',
        WildCard:'AdhocQueryData2.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target JmsTarget  using JMSWriter (
		Provider:'@JMSWRITERPROVIDER@',
		Ctx:'@JMSWRITERCONTEXT@',
		messagetype: @MESSAGETYPE@,
		UserName:'@JMSWRITERUSERNAME@',
		Password:'@JMSWRITERPASSWORD@',
		@DESTINATIONTYPE@)
format using @JMSTARGETFORMATTERTYPE@ (
@JMSTARGETFORMATTERMEMBERS@
)
input from TypedCSVStream;

--SECOND TARGET--

Create Type SecondType (
  zip String,
  city String
);

Create Stream SecondStream of SecondType;

CREATE CQ SecondCQ
INSERT INTO SecondStream
SELECT data[9],data[10]
FROM CsvStream;

create Target SecondTarget using JMSWriter (
		Provider:'@JMSWRITERPROVIDER@',
		Ctx:'@JMSWRITERCONTEXT@',
		messagetype: @MESSAGETYPE@,
		UserName:'@JMSWRITERUSERNAME@',
		Password:'@JMSWRITERPASSWORD@',
		@DESTINATIONTYPE@)
format using @SECONDTARGETFORMATTERTYPE@ (
@SECONDTARGETFORMATTERMEMBERS@
)
input from SecondStream;

end Application Jms;

drop application dropDeployApp cascade;
create application dropDeployApp;
deploy application admin.dropDeployApp on any in default;
drop application dropDeployApp;

--
-- Recovery Test 35 with two sources, two jumping count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5W/p -> CQ1 -> WS
--   S2 -> Jc6W/p -> CQ2 -> WS
--

STOP KStreamRecov35Tester.KStreamRecovTest35;
UNDEPLOY APPLICATION KStreamRecov35Tester.KStreamRecovTest35;
DROP APPLICATION KStreamRecov35Tester.KStreamRecovTest35 CASCADE;

DROP USER KStreamRecov35Tester;
DROP NAMESPACE KStreamRecov35Tester CASCADE;
CREATE USER KStreamRecov35Tester IDENTIFIED BY KStreamRecov35Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov35Tester;
CONNECT KStreamRecov35Tester KStreamRecov35Tester;

CREATE APPLICATION KStreamRecovTest35 RECOVERY 5 SECOND INTERVAL

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION KStreamRecovTest35;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @APPNAME@_src1 Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
   _h_ReturnDateTimeAs:'ZonedDateTime'
) Output To @APPNAME@_stream1;

Create Source @APPNAME@_src2 Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
   _h_ReturnDateTimeAs:'ZonedDateTime'
) Output To @APPNAME@_stream2;


create or replace stream @APPNAME@_combined_stream OF Global.WAEvent;

Create CQ @APPNAME@CQ1
insert into @APPNAME@_combined_stream
select
* from @APPNAME@_stream1;

Create CQ @APPNAME@CQ2
insert into @APPNAME@_combined_stream
select
* from @APPNAME@_stream2;



create Target @APPNAME@_tgt1 using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:100'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_combined_stream;

create Target @APPNAME@_tgt2 using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:100'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@',
	members: 'Table=@metadata(TableName),OpName=@metadata(OperationName)'
)
input from @APPNAME@_stream2;

create Target @APPNAME@_tgt3 using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:100'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_stream1;

create type @APPNAME@_type(
id int,
name String,
cost float,
TableName string,
OperationName String
);

create or replace stream @APPNAME@_typed_Stream of @APPNAME@_type;

Create CQ @APPNAME@_TypedCQ
insert into @APPNAME@_typed_Stream
select
to_int(data[0]),data[1],to_float(data[2]),
meta(@APPNAME@_stream2,'TableName'),
Meta(@APPNAME@_stream2,'OperationName') from @APPNAME@_stream2;


create Target @APPNAME@_tgt4 using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:100'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_typed_Stream;




end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( 
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: '@CDC_URL@',
  Tables: '@Source1Tables@' ) 
OUTPUT TO @APPNAME@cdcStream;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@1 ( 
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  FetchSize: 20,
  DatabaseProviderType: 'Default',
  Table: '@Source3Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')  
OF @APPNAME@cachetype;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@2 ( 
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  FetchSize: 10,
  DatabaseProviderType: 'Default',
  Table: '@Source2Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')  
OF @APPNAME@cachetype;

CREATE STREAM PosDataStream OF PosData;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;


CREATE TARGET DumpFileStream USING LogWriter(
name:testOuput1,
filename:'@FEATURE-DIR@/logs/TQLwithinTQL-console-out-RT.log'
) INPUT FROM PosDataStream;

--
-- Recovery Test 4
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS
-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS
--

STOP Recov4Tester.RecovTest4;
UNDEPLOY APPLICATION Recov4Tester.RecovTest4;
DROP APPLICATION Recov4Tester.RecovTest4 CASCADE;
CREATE APPLICATION RecovTest4 RECOVERY 50 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream OF CsvData;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION RecovTest4;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: '@ORACLE-URL@',
  Tables: '@SOURCE-TABLES@',
  Username: '@ORACLE-USERNAME@',
  Password: '@ORACLE-PASSWORD@'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt USING SnowflakeWriter

(
  ConnectionURL:'@SNOWFLAKE-URL@',
  username:'@SNOWFLAKE-USERNAME@',
  appendOnly:'false',
  Tables:'@TARGET-TABLES@',
  uploadpolicy:'eventcount:3,interval:10s',
  externalStageType:'local'
)
INPUT FROM @APP_NAME@_Stream;


END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

STOP APPLICATION testApp;
UNDEPLOY APPLICATION testApp;
DROP APPLICATION testApp CASCADE;
-- DROP EXCEPTIONSTORE testApp_exceptionstore;

CREATE APPLICATION testApp WITH ENCRYPTION RECOVERY 10 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE OR REPLACE SOURCE testApp_Source USING OracleReader  (
  Username:'qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
  OnlineCatalog:true,
  FetchSize:'1',
  Tables: 'QATEST.sourceTable'
  ) OUTPUT TO testApp_Stream  ;

CREATE OR REPLACE TARGET testApp_Target USING BigQueryWriter  (
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  projectId:'striimqa-214712',
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:5,Interval:120',
  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', 
  AllowQuotedNewLines: 'false', 
  adapterName: 'BigQueryWriter', 
  optimizedMerge: 'true', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', 
  StandardSQL: 'true', 
  QuoteCharacter: '\"', 
  Tables: 'QATEST.sourceTable,transOption_test.targettable',
  Mode: 'MERGE',
  StandardSQL: 'true',
  _h_TransportOptions:'connectionTimeout=30s, readTimeout=12s',
  QuoteCharacter: '\"'
  ) INPUT FROM testApp_Stream;

CREATE OR REPLACE TARGET testApp_SysOut USING Global.SysOut (name: 'wa') INPUT FROM testApp_Stream;

END APPLICATION testApp;
DEPLOY APPLICATION testApp;
START testApp;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING MSSqlReader
(
  Compression: false,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.test01',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
)
OUTPUT TO @APPNAME@_SS;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(id,col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'NULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true	,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

--
-- Recovery Test T20
-- Nicholas Keene, WebAction, Inc.
--
-- Snum -> CQ -> WS
--


UNDEPLOY APPLICATION NameT20.T20;
DROP APPLICATION NameT20.T20 CASCADE;
CREATE APPLICATION T20;




CREATE FLOW DataAcquisitionT20;


CREATE SOURCE CsvSourceT20 USING NumberSource ( 
  lowValue: '1',
  highValue: '1003',
  delayMillis: '10',
  delayNanos: '0',
  repeat: 'false'
 ) 
OUTPUT TO OutputStreamT20;


END FLOW DataAcquisitionT20;




CREATE FLOW DataProcessingT20;


Create Target OutputTargetT20
Using Sysout (name: 'OutputTargetT20')
Input From OutputStreamT20;


END FLOW DataProcessingT20;



END APPLICATION T20;

--This is a dummy file.
--Framework doesn't support to create an empty resource directory.
--Purpose of creating resource dir here is to store test logs in specific test related directory.

CREATE APPLICATION SourcePosApp;

CREATE SOURCE PosCsvDataSource USING FileReader ( 
  directory: '@TEST-DATA-PATH@', 
  wildcard: 'posdata.csv', 
  positionbyeof: false ) 
PARSE USING DSVParser ( 
  charset: 'UTF-8' )
OUTPUT TO PosCsvStream;

CREATE TARGET PosSourceDump using FileWriter(
  directory: '@FEATURE-DIR@/logs',  
  filename: 'SourcePosAppData',
  rolloverpolicy: 'EventCount:6000000'
   )
FORMAT USING Global.DSVFormatter (
  members: 'data',
  charset: 'UTF-8' ) 
 input from PosCsvStream;

CREATE Source PosHourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt',
  positionByEOF:false )
PARSE USING DSVParser ( 
    charset: 'UTF-8' )
OUTPUT TO PosCacheSource1;

CREATE TARGET PosCacheDump1 using FileWriter(
  directory: '@FEATURE-DIR@/logs',  
  filename: 'SourcePosCacheData1',
  rolloverpolicy: 'EventCount:6000000' ) 
FORMAT USING DSVFormatter (
    members: 'data', 
    charset: 'UTF-8') 
 input from PosCacheSource1;
 
CREATE Source PosNameLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'MerchantNames.csv',
  positionByEOF:false )
PARSE USING DSVParser ( 
    charset: 'UTF-8' )
 OUTPUT TO PosCacheSource2;

CREATE TARGET PosCacheDump2 using FileWriter(
  directory: '@FEATURE-DIR@/logs',  
  filename: 'SourcePosCacheData2',
  rolloverpolicy: 'EventCount:6000000' ) 
FORMAT USING DSVFormatter (
    members: 'data', 
    charset: 'UTF-8')
 input from PosCacheSource2;
 
CREATE Source PosZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  positionByEOF:false ) 
PARSE USING DSVParser ( 
    columndelimiter:'\t',
    charset: 'UTF-8' )
OUTPUT To PosCacheSource3;

CREATE TARGET PosCacheDump3 using FileWriter(
  directory: '@FEATURE-DIR@/logs',  
  filename: 'SourcePosCacheData3',
  rolloverpolicy: 'EventCount:6000000') 
FORMAT USING DSVFormatter (
    members: 'data', 
    charset: 'UTF-8')
INPUT from PosCacheSource3;

END APPLICATION SourcePosApp;

CREATE OR REPLACE PROPERTYVARIABLE Mode='sync';
CREATE OR REPLACE PROPERTYVARIABLE BatchPolicy='Size:900000,Interval:1';
create application KinesisTest;
create source CSVSource using FileReader (
	directory:'/home/dz/src/product/Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  companyName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0], data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	BatchPolicy: '$BatchPolicy',
    Mode: '$Mode'	
)
format using AvroFormatter (
	schemaFileName:'/Users/shikhar_nahar/Product/testwaevent.avsc'
)
input from TypedCSVStream;
end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

--
-- Recovery Test 14 with one source, a pattern matching CQ, and one WactionStore
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> PM-CQ -> WS
--

STOP Recov14Tester.RecovTest14;
UNDEPLOY APPLICATION Recov14Tester.RecovTest14;
DROP APPLICATION Recov14Tester.RecovTest14 CASCADE;
CREATE APPLICATION RecovTest14 RECOVERY 10 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  companyName String,
  merchantId String,
  dataCode int KEY,
  expDate String
);

CREATE TYPE WactionData (
companyName1 String KEY,
companyName2 String,
companyName3 String,
dataCode1 int,
dataCode2 int,
dataCode3 int
);

CREATE STREAM DataStream OF CsvData;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_INT(data[3]),
    data[5]
FROM CsvStream;





CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@


CREATE CQ PatternMatchCQ
INSERT INTO Wactions
SELECT A.companyName as companyName1,
        B.companyName as companyName2,
        C.companyName as companyName3,
        A.dataCode as dataCode1,
        B.dataCode as dataCode2,
        C.dataCode as dataCode3
from DataStream
MATCH_PATTERN A E*  B D* C
DEFINE
A=DataStream(dataCode = 0),
E=DataStream(datacode != 1),
B=DataStream(dataCode = 1),
D= Datastream(datacode != 2),
C=DataStream(dataCode = 2);






CREATE WINDOW RestartFromSpecificLocation
OVER CsvStream KEEP 8 ROWS;





END APPLICATION RecovTest14;

CREATE APPLICATION  @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE  @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'JsonNodeEvent.json',
positionByEOF:false
)
PARSE USING Global.JSONParser (
 )  OUTPUT TO  @AppName@_rawstream;

CREATE CQ @BuiltinFunc@CQ
INSERT INTO  @BuiltinFunc@_Stream
SELECT @BuiltinFunc@(x, 'Sno', data.get("_id"), 'Name', data.get("firstname"))
FROM @AppName@_rawstream x;

CREATE OR REPLACE CQ cq1
INSERT INTO RemoveUserData_Stream
SELECT
removeUserData(s1, 'Sno')
FROM @BuiltinFunc@_Stream s1;

CREATE OR REPLACE TARGET  @AppName@_FileTarget USING Global.FileWriter (
  flushpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
 directory: '@logs@',
  filename: '@BuiltinFunc@_JsonEventRemoveData',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  (
  handler: 'com.webaction.proc.JSONFormatter',
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  formatterName: 'JSONFormatter',
  jsonobjectdelimiter: '\n' )
INPUT FROM RemoveUserData_Stream;

End application  @AppName@;
Deploy application  @AppName@;
Start application  @AppName@;

-------------------------
-- The tql checks the auto conversion feature for multiple tables.
-- It covers all the data types supported by sqlmp (except "interval").
-------------------------

IMPORT static com.webaction.runtime.converters.DateConverter.*;

UNDEPLOY APPLICATION admin.SQLMPReaderApp;
DROP APPLICATION admin.SQLMPReaderApp cascade;

CREATE APPLICATION SQLMPReaderApp;
create source SQLMPSource using HPNonStopSQLMPReader (
    portno:2020,
	ipaddress:'10.10.196.103',
	Name:'intg',
	AuditTrails:'parallel',
	AgentPortNo:8012,
	AgentIpAddress:'10.10.197.150', 
    Tables:'$DATA06.MAHA.ESA;$DATA06.MAHA.ESB;$DATA06.MAHA.ESC'
) OUTPUT TO CDCStream,
ESAStream MAP (table:'\\RPC4.$DATA06.MAHA.ESA'),
ESBStream MAP (table:'\\RPC4.$DATA06.MAHA.ESB');


CREATE TYPE ESCStreamType(
C0 Short,
C1 Long,
C2 Long,
C3 String,
C4 String,
C5 Integer,
C6 Long,
C7 String,
C8 String,
C9 Double,
C10 Double,
OPR String,
TABLENAME String
);

CREATE STREAM ESCStream OF ESCStreamType;


CREATE JUMPING WINDOW SQLMPDataWindow
OVER ESCStream KEEP 5 ROWS
PARTITION BY OPR;

CREATE CQ ESCStreamCq
INSERT INTO ESCStream
SELECT TO_SHORT(data[0]),
    TO_LONG(data[1]),
    TO_LONG(data[2]),
    data[3],
    data[4],
    TO_INT(data[5]),
       TO_LONG(data[6]),
    data[7],
    data[8],
    TO_DOUBLE(data[9]),
    TO_DOUBLE(data[10]),
    META(x,"OperationName").toString(),
    META(x, "TableName").toString()
FROM CDCStream x
WHERE not(META(x,"OperationName").toString() = "BEGIN") AND not(META(x,"OperationName").toString() = "COMMIT") AND not(META(x, "TableName").toString() is null) AND META(x, "TableName").toString() = "\\\\RPC4.$DATA06.MAHA.ESC";


CREATE TYPE SQLMPOperationData(
    TableName String,
    OperationName String,
    Count Integer
);

CREATE STREAM SQLMPOperationDataStream OF SQLMPOperationData;

CREATE CQ SQLMPOperationCheck
INSERT INTO SQLMPOperationDataStream
SELECT x.TABLENAME,
CASE WHEN x.OPR = 'INSERT' THEN x.OPR
     WHEN x.OPR = 'DELETE' THEN x.OPR
     WHEN x.OPR = 'UPDATE' THEN x.OPR
     ELSE 'UNSUPPORTED OPREATION' END,
CASE WHEN x.OPR = 'INSERT' THEN COUNT(x.OPR)
     WHEN x.OPR = 'DELETE' THEN COUNT(x.OPR)
     WHEN x.OPR = 'UPDATE' THEN COUNT(x.OPR)
     ELSE 0 END
FROM SQLMPDataWindow x
GROUP BY OPR;

CREATE TARGET Log USING LogWriter(
  name:SQLMPReaderAppESA,
-- filename:'@FEATURE-DIR@/logs/SQLMPReaderAppESA.log'
  filename:'mp.log'
) INPUT FROM ESAStream;


CREATE TARGET Log1 USING LogWriter(
  name:SQLMPReaderAppESB,
--  filename:'@FEATURE-DIR@/logs/SQLMPReaderAppESB.log'
  filename:'mp1.log'
) INPUT FROM ESBStream;


CREATE TARGET OperationLog USING LogWriter(
  name:SQLMPReaderAppESC,
--  filename:'@FEATURE-DIR@/logs/SQLMPReaderOperationCheck.log'
  filename:'mp2.log'
) INPUT FROM SQLMPOperationDataStream;

END APPLICATION SQLMPReaderApp;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'smallposdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
	filename:'TargetPosDataXmlFSDefault',
	directory:'@FEATURE-DIR@/logs/',
    sequence:'00',
	rolloverpolicy:'FileSizeRollingPolicy'
)
format using XMLFormatter (
	rootelement:'document',
	elementtuple:'MerchantName:zip:text=merchantname'
)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlDFS_actual.log') input from TypedCSVStream;

end application DSV;

CREATE OR REPLACE APPLICATION @AppName@;

CREATE OR REPLACE SOURCE CP_Oracle_source USING OracleReader (
  ConnectionURL: '',
  Tables: '',
  Username: '',
  Password: '',
  Fetchsize: 1 )
OUTPUT TO CP_EndToEnd_SF_Adapter_Stream;

CREATE OR REPLACE TARGET CP_SF_Target USING Global.SnowflakeWriter (
  connectionProfileName: '',
  streamingUpload: 'false',
  StreamingConfiguration: 'MaxParallelRequests=5, MaxRequestSizeInMB=5, MaxRecordsPerRequest=10000',
  useConnectionProfile: 'true',
  externalStageConnectionProfileName: '',
  uploadPolicy: 'eventcount:10000,interval:5m',
  Tables: 'QATEST.Test_CP,SANJAYPRATAP.SAMPLESCHEMA.SAMPLE_PK')
INPUT FROM CP_EndToEnd_SF_Adapter_Stream;

END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

--
-- Recovery Test 1
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--


UNDEPLOY APPLICATION NameM00.M00;
DROP APPLICATION NameM00.M00 CASCADE;
CREATE APPLICATION M00 RECOVERY 5 SECOND INTERVAL;




CREATE FLOW DataAcquisitionM00;


CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;


END FLOW DataAcquisitionM00;




CREATE FLOW DataProcessingM00;


CREATE TYPE WactionTypeM00 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE WACTIONSTORE WactionsM00 CONTEXT OF WactionTypeM00
EVENT TYPES ( WactionTypeM00 KEY(word) )
@PERSIST-TYPE@

CREATE CQ InsertWactionsM00
INSERT INTO WactionsM00
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStream;


END FLOW DataProcessingM00;



END APPLICATION M00;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_DBSource USING Global.OracleReader (
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  Compression: false,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Password_encrypted: 'false',
  SupportPDB: false,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  FetchSize: 1000,
  CDDLAction: 'Process',
  ConnectionURL: '192.168.56.3:1521:orcl',
  DictionaryMode: 'OnlineCatalog',
  QueueSize: 2048,
  CommittedTransactions: true,
  SetConservativeRange: false,
  CDDLCapture: false,
  Username: 'fan',
  Tables: 'FAN.S_BLOB',
  TransactionBufferType: 'Disk',
  Password: '9S5GnbGmBQNDD5c/baD0Tw==',
  TransactionBufferSpilloverSize: '100MB',
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  DatabaseRole: 'Primary' )
OUTPUT TO @APPNAME@_stream;

CREATE OR REPLACE TARGET @APPNAME@_target USING Global.BigQueryWriter (
  ColumnDelimiter: '|',
  projectId: 'striim-support',
  BatchPolicy: 'eventCount:1, Interval:1',
  NullMarker: 'NULL',
  streamingUpload: 'false',
  ServiceAccountKey: '/Users/fzhang/fan/u01/app/product/striim/striim_latest/UploadedFiles/striim-support-286429beb74d.json',
  Encoding: 'UTF-8',
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30',
  AllowQuotedNewLines: 'false',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  Tables: 'FAN.S_BLOB,Fan.s_blob columnmap(A=A,B=B,C=C,D=C,E=@metadata(OperationName));',
  TransportOptions: 'connectionTimeout=300, readTimeout=120',
  adapterName: 'BigQueryWriter',
  Mode: 'APPENDONLY',
  StandardSQL: 'true',
  includeInsertId: 'true',
  QuoteCharacter: '\"' )
INPUT FROM @APPNAME@_stream;


END APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.GGTrailReader (
  Tables:'@TABLES@',
  CDDLCapture: false,
  TrailDirectory: '@TRAIL_FILE_DIR@',
  TrailFilePattern: '@WILDCARD@',
  Compression: false,
  SupportColumnCharset: false,
  CDDLAction: 'Process',
  FilterTransactionBoundaries: true,
  adapterName: 'GGTrailReader',
  TrailByteOrder: '@ENDIAN@' )
OUTPUT TO @STREAM@;

CREATE TARGET @SOURCE_NAME@_sysout USING Global.SysOut (
  name: '@SOURCE_NAME@_SysOut' )
INPUT FROM @STREAM@;

CREATE OR REPLACE  EMBEDDINGGENERATOR @EMB_NAME@ USING @MODEL@ (
'modelProvider': '@MODEL@',
'modelName': '@MODEL_NAME@',
'project': '@PROJECT@',
'location': '@LOCATION@',
'publisher': '@PUBLISHER@',
'serviceAccountKey': '@SERVICE_ACCOUNT_KEY@'
);

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 10 second interval;
CREATE SOURCE OracleSource USING OracleReader
(
    Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
 Tables:'@TABLES@',
    FetchSize: 1
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId int,
  curr String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT TO_INT(data[0]),
       data[1]
FROM CsvStream;

create Target BlobT using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:5,interval:5s'
)
format using AvroFormatter (
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING IncrementalBatchReader  ( 
  FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: '@CHECKCOLUMN@',
 startPosition: '%=-1',
  PollingInterval: '5sec'
  )
  OUTPUT TO @STREAM@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING MSSqlReader
(
  Compression: false,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.test01',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
)
OUTPUT TO @APPNAME@_SS;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'NULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING Global.DeltaLakeWriter (
  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.testaswin',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING Global.DeltaLakeWriter (
  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.testaswin',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING Global.DeltaLakeWriter (
  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.testaswin',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING Global.DeltaLakeWriter (
  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.testaswin',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @STREAM@;


CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;;

create Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.waevent PERSIST USING @APPNAME@KafkaPropset;

CREATE TYPE @APPNAME@posDataType (
 merchantId java.lang.String,
 companyName java.lang.String
 );

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (
  ProviderName: '',
  UserName: '',
  transactionpolicy: '',
  Provider: '',
  Ctx: '',
  EnableTransaction: '',
  QueueName: '',
  Topic:'',
  Password: '' )
PARSE USING XMLParser (
  rootnode:'/JMSXMLIN'
  )
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

CREATE STREAM @APPNAME@TypedStream OF @APPNAME@posDataType;

CREATE CQ @APPNAME@CQ
INSERT INTO @APPNAME@TypedStream
SELECT TO_STRING(data[0]) as merchantId,
  TO_STRING(data[1]) as companyName
FROM @APPNAME@PersistStream@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING Global.FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING JSONFormatter  ()
INPUT FROM @APPNAME@TypedStream;

END APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;

Create Source @SourceName@ Using Ojet

(
  Username:'c##qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',
  Tables:'CDB$ROOT."C##QATEST".ojet_src;ORCLPDB.QATEST.ojet_src',
  _h_useClassic:false,
  Fetchsize:1,
  Compression: true,
  SupportPDB:true,
  ReplicationSlotName:'null'
)
Output To @SRCINPUTSTREAM@;

CREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'))) FROM @SRCINPUTSTREAM@ x; ;


CREATE TARGET @targetName@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',
  Username:'c##qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'CDB$ROOT."C##QATEST".ojet_src,CDB$ROOT."C##QATEST".ojet_tgt'
) INPUT FROM admin.sqlreader_cq_out;



create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING YugabyteReader  (
 ReplicationSlotName: 'slotname',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 )
OUTPUT TO @STREAM@ ;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '@GRPPOLICY@',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'MultipleEvents',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
  filename: '%@metadata(FileName)%',
  directory: '@DIR@',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter()
INPUT FROM CCBStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

--
-- Recovery Test 32 with two sources, two sliding attribute windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sa5W/p -> CQ1 -> WS
-- S2 -> Sa6W/p -> CQ2 -> WS
--

STOP Recov32Tester.RecovTest32;
UNDEPLOY APPLICATION Recov32Tester.RecovTest32;
DROP APPLICATION Recov32Tester.RecovTest32 CASCADE;
CREATE APPLICATION RecovTest32 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION RecovTest32;

STOP OuterJoinTester.OJApp;
UNDEPLOY APPLICATION OuterJoinTester.OJApp;
DROP APPLICATION OuterJoinTester.OJApp cascade;

CREATE APPLICATION OJApp;

CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;

CREATE TYPE bankData
(
bankID Integer KEY,
bankName String
);

CREATE TYPE cardData
(
cardID Integer KEY,
cardName String
);

CREATE STREAM wsStream OF bankData;

CREATE CACHE cache1 USING CsvReader(
directory: '@TEST-DATA-PATH@',
wildcard: 'banks.csv',
header: No,
columndelimiter: ',',
trimquote: false
) QUERY (keytomap:'bankID') OF bankData;


CREATE CACHE cache2 USING CsvReader(
directory: '@TEST-DATA-PATH@',
wildcard: 'bankCards.csv',
header: No,
columndelimiter: ',',
trimquote: false
) QUERY (keytomap:'cardID') OF cardData;


CREATE WACTIONSTORE oneWS CONTEXT OF bankData
EVENT TYPES(bankData )
@PERSIST-TYPE@

CREATE CQ csvTobankData
INSERT INTO oneWS
SELECT TO_INT(data[0]), data[1] FROM QaStream;



END APPLICATION OJApp;

stop application DualGen;
undeploy application DualGen;
drop application DualGen cascade;
CREATE APPLICATION DualGen;

CREATE OR REPLACE TYPE DualEvent (
     Dummy DateTime,
    PhoneNo java.lang.String
);

CREATE OR REPLACE STREAM DualEvents OF DualEvent;

CREATE OR REPLACE CQ GenDual 
INSERT INTO DualEvents
SELECT
    TO_DATEF('28-FEB-22',"dd-MMM-yy") as Dummy,
    maskPhoneNumber('44 844 493 0787', "(\\\\d{0,4}\\\\s)(\\\\d{0,4}\\\\s)([0-9 ]+)", 1, 2) as PhoneNo
FROM
   heartbeat(interval 10 second) h;

   CREATE OR REPLACE CQ GenDual2 
INSERT INTO DualEvents
SELECT
   TO_DATEF('12/JAN/32',"dd/MMM/yy") as Dummy,
   maskPhoneNumber('12 345 678 9101', "(\\\\d{0,4}\\\\s)(\\\\d{0,4}\\\\s)([0-9 ]+)", 1, 2) as PhoneNo
FROM
   heartbeat(interval 30 second) h;

CREATE  SOURCE Orac_Src USING OracleReader  ( 
  Compression: false,
  DictionaryMode: 'OnlineCatalog',
  StartTimestamp: 'null',
  SupportPDB: false,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  DDLCaptureMode: 'All',
  CommittedTransactions: true,
  QueueSize: 2048,
  ReaderType: 'LogMiner',
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Username: 'miner',
  Password: 'miner',
  Tables: 'QATEST.HEARTBEAT',
  OutboundServerProcessName: 'WebActionXStream',
  Password_encrypted: false
 ) 
OUTPUT TO CDC_Events ;

CREATE OR REPLACE CQ cdcdual 
INSERT INTO DualEvents
SELECT
   TO_DATE(data[0]) as Dummy,
   data[1] as PhoneNo
FROM
   CDC_Events;


CREATE OR REPLACE TARGET DualSys USING SysOut  ( 
  name: 'heartbeat_out'
 ) 
INPUT FROM DualEvents;

CREATE TARGET DSVFormatterOut using FileWriter(
 filename:'HeartBeat_Reader.log',
 flushpolicy:'EventCount:16',
 rolloverpolicy:'EventCount:16,interval:117s')
FORMAT USING DSVFormatter ()
INPUT FROM DualEvents;


END APPLICATION DualGen;
deploy application DualGen;
start application DualGen;

CREATE APPLICATION @APPNAME@ USE EXCEPTIONSTORE TTL : '7d' ;

CREATE SOURCE @APPNAME@_Source USING Global.OracleReader (
  Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@', )
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_Target1 USING Global.SnowflakeWriter (
  connectionUrl: '@tgturl@',
    tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
    password: '@tgtpassword@',
    username: '@tgtusername@',
    appendOnly: 'true',
    uploadPolicy: 'eventcount:1,interval:5m',
    externalStageType: 'Local',
    adapterName: 'SnowflakeWriter' )
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_Target2 USING Global.SnowflakeWriter (
  connectionUrl: '@tgturl@',
    tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
    password: '@tgtpassword@',
    username: '@tgtusername@',
    appendOnly: 'false',
    optimizedMerge: 'false',
    uploadPolicy: 'eventcount:1,interval:5m',
    externalStageType: 'Local',
    adapterName: 'SnowflakeWriter' )
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_Target3 USING Global.SnowflakeWriter (
  connectionUrl: '@tgturl@',
    tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
    password: '@tgtpassword@',
    username: '@tgtusername@',
    optimizedMerge: 'true',
    uploadPolicy: 'eventcount:1,interval:5m',
    externalStageType: 'Local',
    adapterName: 'SnowflakeWriter' )
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

CREATE APPLICATION @AppName@;
CREATE FLOW Il_Agent_flow;
CREATE OR REPLACE SOURCE FileReader_Src USING FileReader  (
   WildCard: 'posdata100.csv',
  directory: '@dir@',
  positionbyeof: false)
 PARSE USING DSVParser  (
 )
OUTPUT TO CsvStream ;
END FLOW Il_Agent_flow;

Create Type CSVType (
  companyid String,
  merchantId String
);

CREATE STREAM TypedCSVStream OF CSVType;

CREATE OR REPLACE  CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT
TO_STRING(data[0]).replaceAll("COMPANY ", ""),
data[1]
FROM CsvStream;

CREATE OR REPLACE TARGET initialLoadPostgres_Trg USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:100,Interval:60',
  StatementCacheSize: '50',
  ConnectionURL: '@TrgUrl@',
  Username: '@TrgUserName@',
  BatchPolicy: 'EventCount:100,Interval:60',
  Tables: '@TrgTable@',
  Password: '@TrgPswd@',
  adapterName: 'DatabaseWriter' )
INPUT FROM TypedCSVStream;
END APPLICATION @AppName@;

use PosTester;
DROP CQ CsvToPosData;

-----------------------------------

stop application TargetServerApp3;
undeploy application TargetServerApp3;
drop application TargetServerApp3 cascade;


-- another app consuming from app running in agent

CREATE APPLICATION TargetServerApp3;
create flow flow4;

CREATE TARGET T4 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp3_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')
 FORMAT USING DSVFormatter ()
 INPUT FROM CsvStream;
end flow flow4;

END APPLICATION TargetServerApp3;
deploy application TargetServerApp3 with flow4 in default;

stop application dev15823;
undeploy application dev15823;
drop application dev15823 cascade;
CREATE APPLICATION dev15823 RECOVERY 1 SECOND INTERVAL;

CREATE  SOURCE OracleSource USING OracleReader  (
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  ConnectionURL: '@URL@',
  Tables: '@source-table@',
  FetchSize: 1
 )
OUTPUT TO LogminerStream;

--Create or replace Target test using SysOut (name:test) input from MySQLTestStream;

CREATE OR REPLACE TARGET WriteCDCMySQL USING DatabaseWriter  (
  Username: '@USERNAME@',
  BatchPolicy: 'Eventcount:5,Interval:300',
  CommitPolicy: 'Eventcount:5,Interval:300',
  ConnectionURL: '@URL@',
  Tables: '@TABLES@',
  Checkpointtable: 'CHKPOINT',
  Password: '@PASSWORD@'
 )
INPUT FROM LogminerStream;

END APPLICATION dev15823;
deploy application dev15823;
start dev15823;

STOP JSONRecoveryApp;

UNDEPLOY APPLICATION admin.JSONRecoveryApp;
DROP APPLICATION admin.JSONRecoveryApp cascade;

CREATE APPLICATION JSONRecoveryApp RECOVERY 1 SECOND INTERVAL;

CREATE TYPE Emptype (
firstName String,
lastName String );

CREATE STREAM EmpStream of Emptype;

create source CSVSource using FileReader (
	directory:'/Users/bhashemi/Product/IntegrationTests/TestData/jsonRecov',
	WildCard:'jsonRecov*.json',
	positionByEOF:false
) PARSE USING
JSONParser (
eventType:''
) OUTPUT TO EmpStream;

CREATE WACTIONSTORE jsonWactions CONTEXT OF Emptype
EVENT TYPES ( Emptype )
@PERSIST-TYPE@

CREATE CQ InsertjsonWactions
INSERT INTO jsonWactions
SELECT firstName, lastName
FROM EmpStream;

CREATE TARGET jsonRecovSYSOUT using SysOut(name:jsonrecov) INPUT FROM EmpStream;
END APPLICATION JSONRecoveryApp;

deploy application JSONRecoveryApp in default;
START JSONRecoveryApp;

stop Quiesce_CDC_BQ_TARGET;
undeploy application Quiesce_CDC_BQ_TARGET;
alter application Quiesce_CDC_BQ_TARGET;
Create or replace TARGET Quiesce_CDC_BigQueryTrg USING BigQueryWriter (
  serviceAccountKey: '@SERVICEACCOUNTKEY@',
  projectId:'@PROJECTID@',
  BatchPolicy:'Interval:10',
  _h_maxParallelStreamingRequests: '10',
  Tables:'QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE1'
)
INPUT FROM Quiesce_CDC_OrcStrm;
alter application Quiesce_CDC_BQ_TARGET recompile;
DEPLOY APPLICATION Quiesce_CDC_BQ_TARGET;
start application Quiesce_CDC_BQ_TARGET;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:'@eof@',
	charset:'@charset@'
)
parse using DSVParser (
	header:'@header@'
)
OUTPUT TO CsvStream;

create Target FileTarget using FileWriter(
    rolloverpolicy:'eventcount:100',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using DSVFormatter (
)
input from CsvStream;

end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

create application CSVToJSON;
create source CSVSource using FileReader (
	directory:'Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
	filename:'posdata_JSON',
	rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'
)
format using JSONFormatter (
	members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)

input from TypedCSVStream;
end application CSVToJSON;

deploy application CSVToJSON;
start application CSVToJSON;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.JsonNodeEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (
  ProviderName: '',
  Provider: '',
  Ctx: '',
  QueueName: '',
  Topic:'',
  UserName: '',
  Password: '',
  EnableTransaction: '',
  transactionpolicy: ''
 )
PARSE USING JSONParser ()
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING JSONFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream@RANDOM@;

END APPLICATION @APPNAME@;

create application JuniperSaLog;
create source JuniperSaLogSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'juniper-SA*',
	charset:'UTF-8',
	positionByEOF:false
) PARSE USING JuniperSA2000LogParser (
	columndelimitTill:5
) OUTPUT TO JuniperSaLogStream;
create Target JuniperSaDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/junipersa_log',charset:'UTF-8') input from JuniperSaLogStream;
end application JuniperSaLog;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

CREATE APPLICATION DSV;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE  SOURCE CSVSource USING FileReader (
  directory: '@TEST-DATA-PATH@',
  WildCard: 'posdata.csv',
  positionByEOF: false,
  charset: 'UTF-8'
 )
 PARSE USING DSVParser (
  header: 'yes'
 )
OUTPUT TO CsvStream;

CREATE OR REPLACE TARGET t USING FileWriter (
  filename: 'TargetDefault',
  directory:'@FEATURE-DIR@/logs/',
  sequence:'00',
  --flushinterval: '0',
  rolloverpolicy:'EventCount:5000000,Interval:200s'
 )
 format using DSVFormatter (

)
INPUT FROM TypedCSVStream;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetDefaultPD_actual.log') input from TypedCSVStream;

END APPLICATION DSV;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING DatabaseReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE CQ @APPNAME@CQ
INSERT INTO @APPNAME@CQStream
SELECT putUserData(x, 'city',data[6])
FROM @APPNAME@stream x;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@CQStream;

END APPLICATION @APPNAME@;

stop APPLICATION OrcToDWH;
undeploy APPLICATION OrcToDWH;
DROP APPLICATION OrcToDWH CASCADE;
CREATE APPLICATION OrcToDWH recovery 5 second interval;
Create Source OracleSource Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;


create target AzureTarget using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
        _h_TransportOptions:'connectionTimeout=30s, readTimeout=12s',
) INPUT FROM str;

END APPLICATION OrcToDWH;
deploy APPLICATION OrcToDWH;
start APPLICATION OrcToDWH;

drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

CREATE TYPE @APPNAME@type1 (
 companyName java.lang.String,
 merchantId java.lang.String,
 city java.lang.String);

CREATE STREAM @APPNAME@TypedStream OF @APPNAME@type1 PARTITION BY city;

CREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (
  wildcard: '',
  positionByEOF: false,
  directory: ''
  )
PARSE USING DSVParser (
header:'true'
)
OUTPUT TO @APPNAME@Stream;

CREATE OR REPLACE CQ @APPNAME@CQ
INSERT INTO @APPNAME@TypedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantID,
TO_STRING(data[10]) as city
FROM @APPNAME@Stream;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@TypedStream;

END APPLICATION @APPNAME@;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[0]) = '1' ;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@ RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE @parquetsrc@ USING FileReader (
wildcard: '',
directory: '',
positionbyeof: false )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: ''
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

create Target FileTarget using FileWriter(
    rolloverpolicy:'@UPLOAD-SIZE@',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using DSVFormatter (
charset:'@charset@',
members:'@mem@',
header:'@head@'

)
input from CsvStream;

end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

CREATE APPLICATION DSV RECOVERY 1 SECOND INTERVAL;

CREATE FLOW agentflow;

CREATE OR REPLACE SOURCE CSVSource USING Global.FileReader (
  charset: 'UTF-8',
  adapterName: 'FileReader',
  rolloverstyle: 'Default',
  positionByEOF: false,
  WildCard: 'posdata.csv',
  blocksize: 64,
  skipbom: true,
  includesubdirectories: false,
  directory: 'Samples/AppData' )
PARSE USING Global.DSVParser (
  trimwhitespace: false,
  linenumber: '-1',
  columndelimiter: ',',
  columndelimittill: '-1',
  trimquote: true,
  ignoreemptycolumn: false,
  separator: ':',
  parserName: 'DSVParser',
  quoteset: '\"',
  handler: 'com.webaction.proc.DSVParser_1_0',
  charset: 'UTF-8',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  blockascompleterecord: false,
  rowdelimiter: '\n',
  nocolumndelimiter: false,
  headerlineno: 0,
  header: true )
OUTPUT TO CsvStream;

END FLOW agentflow;

CREATE OR REPLACE TARGET t USING Global.FileWriter (
  directory: '@TEST-DATA-PATH@',
  flushpolicy: 'EventCount:10000,Interval:30s',
  members: 'data',
  rolloveronddl: 'true',
  adapterName: 'FileWriter',
  rolloverpolicy: 'TimeIntervalRollingPolicy,rotationinterval:5s',
  filename: 'Foo' )
FORMAT USING Global.DSVFormatter  (
  quotecharacter: '\"',
  handler: 'com.webaction.proc.DSVFormatter',
  columndelimiter: ',',
  formatterName: 'DSVFormatter',
  nullvalue: 'NULL',
  usequotes: 'false',
  rowdelimiter: '\n',
  standard: 'none',
  header: 'false' )
INPUT FROM CsvStream;

END APPLICATION DSV;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.test01',
	FetchSize: '1',
	connectionRetryPolicy:'timeOut=00,retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace @APPNAME@_TARGET T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'Eventcount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true		
) INPUT FROM @APPNAME@_ss;
--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE OR REPLACE SOURCE @parquetsrc@ USING Global.HDFSReader (
  wildcard: '',
  directory: '',
  hadoopurl: '',
  hadoopconfigurationpath: '',
  positionbyeof: false )
  PARSE USING ParquetParser (
   )
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: 'ParquetFileSchema'
)
INPUT FROM @appname@CqOut;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using ParquetFormatter (
schemaFileName: 'ParquetS3Schema'
)
input from @appname@CqOut;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using ParquetFormatter (
schemaFileName: 'ParquetAzureSchema'
)
INPUT FROM @appname@CqOut;

CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using ParquetFormatter (
schemaFileName: 'ParquetGCSSchema'
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

--
-- Crash Recovery Test 6 with Jumping window and partitioned on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> KafkaStream -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION N2S2CR6Tester.N2S2CRTest6;
UNDEPLOY APPLICATION N2S2CR6Tester.N2S2CRTest6;
DROP APPLICATION N2S2CR6Tester.N2S2CRTest6 CASCADE;
CREATE APPLICATION N2S2CRTest6 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest6;

CREATE SOURCE CsvSourceN2S2CRTest6 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;




CREATE TYPE CsvDataTypeN2S2CRTest6 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream of CsvDataTypeN2S2CRTest6 using KafkaProps;

CREATE CQ TransferToKafka
INSERT INTO KafkaCsvStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;








END FLOW DataAcquisitionN2S2CRTest6;

CREATE FLOW DataProcessingN2S2CRTest6;

CREATE STREAM DataStream OF CsvDataTypeN2S2CRTest6 PARTITION BY merchantId;

CREATE CQ CsvToDataN2S2CRTest6
INSERT INTO DataStream
SELECT
    *
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsN2S2CRTest6 CONTEXT OF CsvDataTypeN2S2CRTest6
EVENT TYPES ( CsvDataTypeN2S2CRTest6 )
@PERSIST-TYPE@

CREATE CQ DataToWactionN2S2CRTest6
INSERT INTO WactionsN2S2CRTest6
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN2S2CRTest6;

END APPLICATION N2S2CRTest6;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE FLOW @APPNAME@AgentFlow;

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.waevent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (
  ProviderName: '',
  UserName: '',
  transactionpolicy: '',
  Provider: '',
  Ctx: '',
  EnableTransaction: '',
  QueueName: '',
  Password: '' )
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@PersistStream@RANDOM@;
END FLOW @APPNAME@AgentFlow;

CREATE FLOW @APPNAME@serverFlow;
CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING JSONFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream@RANDOM@;
END FLOW @APPNAME@serverFlow;

END APPLICATION @APPNAME@;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING AvroFormatter (
schemaFileName: '@SCHEMAFILE@'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING DSVFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

END APPLICATION @APPNAME@;

STOP APPLICATION orrs;
UNDEPLOY APPLICATION orrs;
DROP APPLICATION orrs CASCADE;
CREATE APPLICATION orrs;
Create Source oraSource1 Using DatabaseReader
(
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'QATEST.ORACLETOREDSHIFTIL1;QATEST.ORACLETOREDSHIFTIL2',
 FilterTransactionBoundaries:true,
 FetchSize:1000
) Output To LCRStream1;

Create Source oraSource2 Using DatabaseReader
(
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'QATEST.ORACLETOREDSHIFTIL3;QATEST.ORACLETOREDSHIFTIL4',
 FilterTransactionBoundaries:true,
 FetchSize:1000
) Output To LCRStream2;

CREATE TARGET RSTarget USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: 'access_key',
	  --secretaccesskey: 'secret_access',
	  Tables: 'QATEST.%,QATEST.%',
	   S3IAMRole:'@IAMROLE@',
	  uploadpolicy:'eventcount:1000,interval:1m'
	) INPUT FROM LCRStream1;

CREATE TARGET RSTarget2 USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: 'access_key',
	  --secretaccesskey: 'secret_access',
	  Tables: 'QATEST.%,QATEST.%',
	   S3IAMRole:'@IAMROLE@',
	  Tables: 'QATEST.%,QATEST.%',
	  uploadpolicy:'eventcount:1000,interval:1m'
	) INPUT FROM LCRStream2;

END APPLICATION orrs;
DEPLOY APPLICATION orrs;
START APPLICATION orrs;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE OR REPLACE APPLICATION @APPNAME@ recovery 5 second interval;

CREATE FLOW @APPNAME@_Agent_flow;

CREATE OR REPLACE SOURCE @SourceName@1 USING Global.MSJet (
  Tables: 'dbo.compsrc',
    username: 'qatest',
    DatabaseName: 'qatest',
    FetchTransactionMetadata: true,
    filterTransactionBoundaries: true,
    compression: true,
    ConnectionURL: '10.211.55.3:1433',
    CommittedTransactions: true,
    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
    SendBeforeImage: true,
    password: 'w3b@ct10n' )
OUTPUT TO @SRCINPUTSTREAM@;

END FLOW @APPNAME@_Agent_flow;

CREATE FLOW @APPNAME@_Agent_flow1;

CREATE OR REPLACE SOURCE @SourceName@2 USING Global.MSJet (
  Tables: 'dbo.compsrc',
    username: 'qatest',
    DatabaseName: 'qatest',
    FetchTransactionMetadata: true,
    filterTransactionBoundaries: true,
    compression: false,
    ConnectionURL: '10.211.55.3:1433',
    CommittedTransactions: true,
    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
    SendBeforeImage: true,
    password: 'w3b@ct10n' )
OUTPUT TO @SRCINPUTSTREAM@;

END FLOW @APPNAME@_Agent_flow1;

CREATE FLOW @APPNAME@_server_flow;

CREATE OR REPLACE TARGET @targetName@ USING Global.DatabaseWriter
(
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  BatchPolicy:'EventCount:10,Interval:60',
  CommitPolicy:'EventCount:10,Interval:60',
  ParallelThreads:'',
  CheckPointTable:'CHKPOINT',
  Password_encrypted:'false',
  Tables:'qatest.MSJEtsrc1,qatest.MSJEtar1;qatest.MSJEtsrc2,qatest.MSJEtar2;',
  CDDLAction:'Process',
  Password:'w3b@ct10n',
  StatementCacheSize:'50',
  ConnectionURL:'jdbc:sqlserver://10.211.55.3:1433;databaseName=qatest',
  DatabaseProviderType:'Default',
  Username:'qatest',
  PreserveSourceTransactionBoundary:'false',
  adapterName:'DatabaseWriter'
)
INPUT FROM @SRCINPUTSTREAM@;

CREATE TARGET @targetsys@ USING Global.SysOut (
  name: '@targetsys@' )
INPUT FROM @SRCINPUTSTREAM@;

END FLOW @APPNAME@_server_flow;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@ with @APPNAME@_Agent_flow in AGENTS, @APPNAME@_Agent_flow1 in AGENTS ,@APPNAME@_server_flow on any in default;
START APPLICATION @APPNAME@;

STOP APPLICATION @Appname@;
UNDEPLOY APPLICATION @Appname@;
DROP APPLICATION @Appname@ CASCADE;

CREATE APPLICATION @Appname@ RECOVERY 10 SECOND INTERVAL;

CREATE  SOURCE @Appname@source USING MySQLReader
(
Username: '',
Password: '',
ConnectionURL: '',
Tables: 'waction.TABLE_TEST_%',
FetchSize:1
)
OUTPUT TO @Appname@MasterStream;

CREATE OR REPLACE ROUTER @Appname@Rs1 INPUT FROM @Appname@MasterStream s CASE
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_1' THEN ROUTE TO @Appname@Typed1,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_2' THEN ROUTE TO @Appname@Typed2,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_3' THEN ROUTE TO @Appname@Typed3,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_4' THEN ROUTE TO @Appname@Typed4,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_5' THEN ROUTE TO @Appname@Typed5,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_6' THEN ROUTE TO @Appname@Typed6,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_7' THEN ROUTE TO @Appname@Typed7,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_8' THEN ROUTE TO @Appname@Typed8,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_9' THEN ROUTE TO @Appname@Typed9,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_10' THEN ROUTE TO @Appname@Typed10,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_11' THEN ROUTE TO @Appname@Typed11,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_12' THEN ROUTE TO @Appname@Typed12,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_13' THEN ROUTE TO @Appname@Typed13,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_14' THEN ROUTE TO @Appname@Typed14,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_15' THEN ROUTE TO @Appname@Typed15,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_16' THEN ROUTE TO @Appname@Typed16,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_17' THEN ROUTE TO @Appname@Typed17,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_18' THEN ROUTE TO @Appname@Typed18,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_19' THEN ROUTE TO @Appname@Typed19,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_20' THEN ROUTE TO @Appname@Typed20,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_21' THEN ROUTE TO @Appname@Typed21,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_22' THEN ROUTE TO @Appname@Typed22,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_23' THEN ROUTE TO @Appname@Typed23,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_24' THEN ROUTE TO @Appname@Typed24,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_25' THEN ROUTE TO @Appname@Typed25,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_26' THEN ROUTE TO @Appname@Typed26,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_27' THEN ROUTE TO @Appname@Typed27,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_28' THEN ROUTE TO @Appname@Typed28,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_29' THEN ROUTE TO @Appname@Typed29,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_30' THEN ROUTE TO @Appname@Typed30,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_31' THEN ROUTE TO @Appname@Typed31,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_32' THEN ROUTE TO @Appname@Typed32,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_33' THEN ROUTE TO @Appname@Typed33,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_34' THEN ROUTE TO @Appname@Typed34,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_35' THEN ROUTE TO @Appname@Typed35,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_36' THEN ROUTE TO @Appname@Typed36,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_37' THEN ROUTE TO @Appname@Typed37,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_38' THEN ROUTE TO @Appname@Typed38,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_39' THEN ROUTE TO @Appname@Typed39,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_40' THEN ROUTE TO @Appname@Typed40,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_41' THEN ROUTE TO @Appname@Typed41,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_42' THEN ROUTE TO @Appname@Typed42,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_43' THEN ROUTE TO @Appname@Typed43,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_44' THEN ROUTE TO @Appname@Typed44,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_45' THEN ROUTE TO @Appname@Typed45,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_46' THEN ROUTE TO @Appname@Typed46,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_47' THEN ROUTE TO @Appname@Typed47,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_48' THEN ROUTE TO @Appname@Typed48,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_49' THEN ROUTE TO @Appname@Typed49,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_50' THEN ROUTE TO @Appname@Typed50,
/*WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_51' THEN ROUTE TO @Appname@Typed51,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_52' THEN ROUTE TO @Appname@Typed52,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_53' THEN ROUTE TO @Appname@Typed53,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_54' THEN ROUTE TO @Appname@Typed54,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_55' THEN ROUTE TO @Appname@Typed55,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_56' THEN ROUTE TO @Appname@Typed56,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_57' THEN ROUTE TO @Appname@Typed57,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_58' THEN ROUTE TO @Appname@Typed58,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_59' THEN ROUTE TO @Appname@Typed59,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_60' THEN ROUTE TO @Appname@Typed60,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_61' THEN ROUTE TO @Appname@Typed61,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_62' THEN ROUTE TO @Appname@Typed62,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_63' THEN ROUTE TO @Appname@Typed63,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_64' THEN ROUTE TO @Appname@Typed64,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_65' THEN ROUTE TO @Appname@Typed65,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_66' THEN ROUTE TO @Appname@Typed66,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_67' THEN ROUTE TO @Appname@Typed67,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_68' THEN ROUTE TO @Appname@Typed68,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_69' THEN ROUTE TO @Appname@Typed69,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_70' THEN ROUTE TO @Appname@Typed70,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_71' THEN ROUTE TO @Appname@Typed71,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_72' THEN ROUTE TO @Appname@Typed72,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_73' THEN ROUTE TO @Appname@Typed73,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_74' THEN ROUTE TO @Appname@Typed74,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_75' THEN ROUTE TO @Appname@Typed75,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_76' THEN ROUTE TO @Appname@Typed76,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_77' THEN ROUTE TO @Appname@Typed77,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_78' THEN ROUTE TO @Appname@Typed78,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_79' THEN ROUTE TO @Appname@Typed79,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_80' THEN ROUTE TO @Appname@Typed80,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_81' THEN ROUTE TO @Appname@Typed81,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_82' THEN ROUTE TO @Appname@Typed82,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_83' THEN ROUTE TO @Appname@Typed83,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_84' THEN ROUTE TO @Appname@Typed84,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_85' THEN ROUTE TO @Appname@Typed85,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_86' THEN ROUTE TO @Appname@Typed86,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_87' THEN ROUTE TO @Appname@Typed87,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_88' THEN ROUTE TO @Appname@Typed88,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_89' THEN ROUTE TO @Appname@Typed89,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_90' THEN ROUTE TO @Appname@Typed90,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_91' THEN ROUTE TO @Appname@Typed91,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_92' THEN ROUTE TO @Appname@Typed92,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_93' THEN ROUTE TO @Appname@Typed93,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_94' THEN ROUTE TO @Appname@Typed94,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_95' THEN ROUTE TO @Appname@Typed95,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_96' THEN ROUTE TO @Appname@Typed96,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_97' THEN ROUTE TO @Appname@Typed97,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_98' THEN ROUTE TO @Appname@Typed98,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_99' THEN ROUTE TO @Appname@Typed99,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_1000100' THEN ROUTE TO @Appname@Typed100,*/
ELSE ROUTE TO @Appname@TypedElse;



CREATE OR REPLACE cq @Appname@cq1
INSERT INTO combinedstream
select s FROM @Appname@Typed1 s;

CREATE OR REPLACE cq @Appname@cq2
INSERT INTO combinedstream
select s FROM @Appname@Typed2 s;

CREATE OR REPLACE cq @Appname@cq3
INSERT INTO combinedstream
select s FROM @Appname@Typed3 s;

CREATE OR REPLACE cq @Appname@cq4
INSERT INTO combinedstream
select s FROM @Appname@Typed4 s;

CREATE OR REPLACE cq @Appname@cq5
INSERT INTO combinedstream
select s FROM @Appname@Typed5 s;

CREATE OR REPLACE cq @Appname@cq6
INSERT INTO combinedstream
select s FROM @Appname@Typed6 s;

CREATE OR REPLACE cq @Appname@cq7
INSERT INTO combinedstream
select s FROM @Appname@Typed7 s;

CREATE OR REPLACE cq @Appname@cq8
INSERT INTO combinedstream
select s FROM @Appname@Typed8 s;

CREATE OR REPLACE cq @Appname@cq9
INSERT INTO combinedstream
select s FROM @Appname@Typed9 s;

CREATE OR REPLACE cq @Appname@cq10
INSERT INTO combinedstream
select s FROM @Appname@Typed10 s;


CREATE OR REPLACE cq @Appname@cq11
INSERT INTO combinedstream
select s FROM @Appname@Typed11 s;

CREATE OR REPLACE cq @Appname@cq12
INSERT INTO combinedstream
select s FROM @Appname@Typed12 s;

CREATE OR REPLACE cq @Appname@cq13
INSERT INTO combinedstream
select s FROM @Appname@Typed13 s;

CREATE OR REPLACE cq @Appname@cq14
INSERT INTO combinedstream
select s FROM @Appname@Typed14 s;

CREATE OR REPLACE cq @Appname@cq15
INSERT INTO combinedstream
select s FROM @Appname@Typed15 s;

CREATE OR REPLACE cq @Appname@cq16
INSERT INTO combinedstream
select s FROM @Appname@Typed16 s;

CREATE OR REPLACE cq @Appname@cq117
INSERT INTO combinedstream
select s FROM @Appname@Typed17 s;

CREATE OR REPLACE cq @Appname@cq18
INSERT INTO combinedstream
select s FROM @Appname@Typed18 s;

CREATE OR REPLACE cq @Appname@cq19
INSERT INTO combinedstream
select s FROM @Appname@Typed19 s;

CREATE OR REPLACE cq @Appname@cq20
INSERT INTO combinedstream
select s FROM @Appname@Typed20 s;

CREATE OR REPLACE cq @Appname@cq21
INSERT INTO combinedstream
select s FROM @Appname@Typed21 s;

CREATE OR REPLACE cq @Appname@cq22
INSERT INTO combinedstream
select s FROM @Appname@Typed22 s;

CREATE OR REPLACE cq @Appname@cq23
INSERT INTO combinedstream
select s FROM @Appname@Typed23 s;

CREATE OR REPLACE cq @Appname@cq24
INSERT INTO combinedstream
select s FROM @Appname@Typed24 s;

CREATE OR REPLACE cq @Appname@cq25
INSERT INTO combinedstream
select s FROM @Appname@Typed25 s;

CREATE OR REPLACE cq @Appname@cq26
INSERT INTO combinedstream
select s FROM @Appname@Typed26 s;

CREATE OR REPLACE cq @Appname@cq27
INSERT INTO combinedstream
select s FROM @Appname@Typed27 s;

CREATE OR REPLACE cq @Appname@cq28
INSERT INTO combinedstream
select s FROM @Appname@Typed28 s;

CREATE OR REPLACE cq @Appname@cq29
INSERT INTO combinedstream
select s FROM @Appname@Typed29 s;

CREATE OR REPLACE cq @Appname@cq30
INSERT INTO combinedstream
select s FROM @Appname@Typed30 s;

CREATE OR REPLACE cq @Appname@cq31
INSERT INTO combinedstream
select s FROM @Appname@Typed31 s;

CREATE OR REPLACE cq @Appname@cq32
INSERT INTO combinedstream
select s FROM @Appname@Typed32 s;

CREATE OR REPLACE cq @Appname@cq33
INSERT INTO combinedstream
select s FROM @Appname@Typed33 s;

CREATE OR REPLACE cq @Appname@cq34
INSERT INTO combinedstream
select s FROM @Appname@Typed34 s;

CREATE OR REPLACE cq @Appname@cq35
INSERT INTO combinedstream
select s FROM @Appname@Typed35 s;

CREATE OR REPLACE cq @Appname@cq36
INSERT INTO combinedstream
select s FROM @Appname@Typed36 s;

CREATE OR REPLACE cq @Appname@cq37
INSERT INTO combinedstream
select s FROM @Appname@Typed37 s;

CREATE OR REPLACE cq @Appname@cq38
INSERT INTO combinedstream
select s FROM @Appname@Typed38 s;

CREATE OR REPLACE cq @Appname@cq39
INSERT INTO combinedstream
select s FROM @Appname@Typed39 s;

CREATE OR REPLACE cq @Appname@cq40
INSERT INTO combinedstream
select s FROM @Appname@Typed40 s;

CREATE OR REPLACE cq @Appname@cq41
INSERT INTO combinedstream
select s FROM @Appname@Typed41 s;

CREATE OR REPLACE cq @Appname@cq42
INSERT INTO combinedstream
select s FROM @Appname@Typed42 s;

CREATE OR REPLACE cq @Appname@cq43
INSERT INTO combinedstream
select s FROM @Appname@Typed43 s;

CREATE OR REPLACE cq @Appname@cq44
INSERT INTO combinedstream
select s FROM @Appname@Typed44 s;

CREATE OR REPLACE cq @Appname@cq45
INSERT INTO combinedstream
select s FROM @Appname@Typed45 s;

CREATE OR REPLACE cq @Appname@cq46
INSERT INTO combinedstream
select s FROM @Appname@Typed46 s;

CREATE OR REPLACE cq @Appname@cq47
INSERT INTO combinedstream
select s FROM @Appname@Typed47 s;

CREATE OR REPLACE cq @Appname@cq48
INSERT INTO combinedstream
select s FROM @Appname@Typed48 s;

CREATE OR REPLACE cq @Appname@cq49
INSERT INTO combinedstream
select s FROM @Appname@Typed49 s;

CREATE OR REPLACE cq @Appname@cq50
INSERT INTO combinedstream
select s FROM @Appname@Typed50 s;
/*
CREATE OR REPLACE cq @Appname@cq51
INSERT INTO combinedstream
select s FROM @Appname@Typed51 s;

CREATE OR REPLACE cq @Appname@cq52
INSERT INTO combinedstream
select s FROM @Appname@Typed52 s;

CREATE OR REPLACE cq @Appname@cq53
INSERT INTO combinedstream
select s FROM @Appname@Typed53 s;

CREATE OR REPLACE cq @Appname@cq54
INSERT INTO combinedstream
select s FROM @Appname@Typed54 s;

CREATE OR REPLACE cq @Appname@cq55
INSERT INTO combinedstream
select s FROM @Appname@Typed55 s;

CREATE OR REPLACE cq @Appname@cq56
INSERT INTO combinedstream
select s FROM @Appname@Typed56 s;

CREATE OR REPLACE cq @Appname@cq57
INSERT INTO combinedstream
select s FROM @Appname@Typed57 s;

CREATE OR REPLACE cq @Appname@cq58
INSERT INTO combinedstream
select s FROM @Appname@Typed58 s;

CREATE OR REPLACE cq @Appname@cq59
INSERT INTO combinedstream
select s FROM @Appname@Typed59 s;

CREATE OR REPLACE cq @Appname@cq60
INSERT INTO combinedstream
select s FROM @Appname@Typed60 s;

CREATE OR REPLACE cq @Appname@cq61
INSERT INTO combinedstream
select s FROM @Appname@Typed61 s;

CREATE OR REPLACE cq @Appname@cq62
INSERT INTO combinedstream
select s FROM @Appname@Typed62 s;

CREATE OR REPLACE cq @Appname@cq63
INSERT INTO combinedstream
select s FROM @Appname@Typed63 s;

CREATE OR REPLACE cq @Appname@cq64
INSERT INTO combinedstream
select s FROM @Appname@Typed64 s;

CREATE OR REPLACE cq @Appname@cq65
INSERT INTO combinedstream
select s FROM @Appname@Typed65 s;

CREATE OR REPLACE cq @Appname@cq66
INSERT INTO combinedstream
select s FROM @Appname@Typed66 s;

CREATE OR REPLACE cq @Appname@cq67
INSERT INTO combinedstream
select s FROM @Appname@Typed67 s;

CREATE OR REPLACE cq @Appname@cq68
INSERT INTO combinedstream
select s FROM @Appname@Typed68 s;

CREATE OR REPLACE cq @Appname@cq69
INSERT INTO combinedstream
select s FROM @Appname@Typed69 s;

CREATE OR REPLACE cq @Appname@cq70
INSERT INTO combinedstream
select s FROM @Appname@Typed70 s;

CREATE OR REPLACE cq @Appname@cq71
INSERT INTO combinedstream
select s FROM @Appname@Typed71 s;

CREATE OR REPLACE cq @Appname@cq72
INSERT INTO combinedstream
select s FROM @Appname@Typed72 s;

CREATE OR REPLACE cq @Appname@cq73
INSERT INTO combinedstream
select s FROM @Appname@Typed73 s;

CREATE OR REPLACE cq @Appname@cq74
INSERT INTO combinedstream
select s FROM @Appname@Typed74 s;

CREATE OR REPLACE cq @Appname@cq75
INSERT INTO combinedstream
select s FROM @Appname@Typed75 s;

CREATE OR REPLACE cq @Appname@cq76
INSERT INTO combinedstream
select s FROM @Appname@Typed76 s;

CREATE OR REPLACE cq @Appname@cq77
INSERT INTO combinedstream
select s FROM @Appname@Typed77 s;

CREATE OR REPLACE cq @Appname@cq78
INSERT INTO combinedstream
select s FROM @Appname@Typed78 s;

CREATE OR REPLACE cq @Appname@cq79
INSERT INTO combinedstream
select s FROM @Appname@Typed79 s;

CREATE OR REPLACE cq @Appname@cq80
INSERT INTO combinedstream
select s FROM @Appname@Typed80 s;

CREATE OR REPLACE cq @Appname@cq81
INSERT INTO combinedstream
select s FROM @Appname@Typed81 s;

CREATE OR REPLACE cq @Appname@cq82
INSERT INTO combinedstream
select s FROM @Appname@Typed82 s;

CREATE OR REPLACE cq @Appname@cq83
INSERT INTO combinedstream
select s FROM @Appname@Typed83 s;

CREATE OR REPLACE cq @Appname@cq84
INSERT INTO combinedstream
select s FROM @Appname@Typed84 s;

CREATE OR REPLACE cq @Appname@cq85
INSERT INTO combinedstream
select s FROM @Appname@Typed85 s;

CREATE OR REPLACE cq @Appname@cq86
INSERT INTO combinedstream
select s FROM @Appname@Typed86 s;

CREATE OR REPLACE cq @Appname@cq87
INSERT INTO combinedstream
select s FROM @Appname@Typed87 s;

CREATE OR REPLACE cq @Appname@cq88
INSERT INTO combinedstream
select s FROM @Appname@Typed88 s;

CREATE OR REPLACE cq @Appname@cq89
INSERT INTO combinedstream
select s FROM @Appname@Typed89 s;

CREATE OR REPLACE cq @Appname@cq90
INSERT INTO combinedstream
select s FROM @Appname@Typed90 s;

CREATE OR REPLACE cq @Appname@cq91
INSERT INTO combinedstream
select s FROM @Appname@Typed91 s;

CREATE OR REPLACE cq @Appname@cq92
INSERT INTO combinedstream
select s FROM @Appname@Typed92 s;

CREATE OR REPLACE cq @Appname@cq93
INSERT INTO combinedstream
select s FROM @Appname@Typed93 s;

CREATE OR REPLACE cq @Appname@cq94
INSERT INTO combinedstream
select s FROM @Appname@Typed94 s;

CREATE OR REPLACE cq @Appname@cq95
INSERT INTO combinedstream
select s FROM @Appname@Typed95 s;

CREATE OR REPLACE cq @Appname@cq96
INSERT INTO combinedstream
select s FROM @Appname@Typed96 s;

CREATE OR REPLACE cq @Appname@cq97
INSERT INTO combinedstream
select s FROM @Appname@Typed97 s;

CREATE OR REPLACE cq @Appname@cq98
INSERT INTO combinedstream
select s FROM @Appname@Typed98 s;

CREATE OR REPLACE cq @Appname@cq99
INSERT INTO combinedstream
select s FROM @Appname@Typed99 s;

CREATE OR REPLACE cq @Appname@cq100
INSERT INTO combinedstream
select s FROM @Appname@Typed100 s;

CREATE OR REPLACE cq @Appname@cq101
INSERT INTO combinedstream
select s FROM @Appname@TypedElse s;*/

CREATE OR REPLACE ROUTER Rs2 INPUT FROM combinedstream s CASE
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_1' THEN ROUTE TO New@Appname@Typed1,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_2' THEN ROUTE TO New@Appname@Typed2,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_3' THEN ROUTE TO New@Appname@Typed3,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_4' THEN ROUTE TO New@Appname@Typed4,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_5' THEN ROUTE TO New@Appname@Typed5,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_6' THEN ROUTE TO New@Appname@Typed6,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_7' THEN ROUTE TO New@Appname@Typed7,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_8' THEN ROUTE TO New@Appname@Typed8,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_9' THEN ROUTE TO New@Appname@Typed9,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_10' THEN ROUTE TO New@Appname@Typed10,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_11' THEN ROUTE TO New@Appname@Typed11,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_12' THEN ROUTE TO New@Appname@Typed12,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_13' THEN ROUTE TO New@Appname@Typed13,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_14' THEN ROUTE TO New@Appname@Typed14,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_15' THEN ROUTE TO New@Appname@Typed15,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_16' THEN ROUTE TO New@Appname@Typed16,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_17' THEN ROUTE TO New@Appname@Typed17,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_18' THEN ROUTE TO New@Appname@Typed18,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_19' THEN ROUTE TO New@Appname@Typed19,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_20' THEN ROUTE TO New@Appname@Typed20,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_21' THEN ROUTE TO New@Appname@Typed21,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_22' THEN ROUTE TO New@Appname@Typed22,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_23' THEN ROUTE TO New@Appname@Typed23,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_24' THEN ROUTE TO New@Appname@Typed24,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_25' THEN ROUTE TO New@Appname@Typed25,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_26' THEN ROUTE TO New@Appname@Typed26,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_27' THEN ROUTE TO New@Appname@Typed27,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_28' THEN ROUTE TO New@Appname@Typed28,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_29' THEN ROUTE TO New@Appname@Typed29,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_30' THEN ROUTE TO New@Appname@Typed30,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_31' THEN ROUTE TO New@Appname@Typed31,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_32' THEN ROUTE TO New@Appname@Typed32,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_33' THEN ROUTE TO New@Appname@Typed33,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_34' THEN ROUTE TO New@Appname@Typed34,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_35' THEN ROUTE TO New@Appname@Typed35,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_36' THEN ROUTE TO New@Appname@Typed36,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_37' THEN ROUTE TO New@Appname@Typed37,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_38' THEN ROUTE TO New@Appname@Typed38,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_39' THEN ROUTE TO New@Appname@Typed39,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_40' THEN ROUTE TO New@Appname@Typed40,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_41' THEN ROUTE TO New@Appname@Typed41,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_42' THEN ROUTE TO New@Appname@Typed42,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_43' THEN ROUTE TO New@Appname@Typed43,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_44' THEN ROUTE TO New@Appname@Typed44,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_45' THEN ROUTE TO New@Appname@Typed45,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_46' THEN ROUTE TO New@Appname@Typed46,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_47' THEN ROUTE TO New@Appname@Typed47,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_48' THEN ROUTE TO New@Appname@Typed48,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_49' THEN ROUTE TO New@Appname@Typed49,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_50' THEN ROUTE TO New@Appname@Typed50,
/*WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_51' THEN ROUTE TO New@Appname@Typed51,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_52' THEN ROUTE TO New@Appname@Typed52,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_53' THEN ROUTE TO New@Appname@Typed53,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_54' THEN ROUTE TO New@Appname@Typed54,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_55' THEN ROUTE TO New@Appname@Typed55,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_56' THEN ROUTE TO New@Appname@Typed56,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_57' THEN ROUTE TO New@Appname@Typed57,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_58' THEN ROUTE TO New@Appname@Typed58,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_59' THEN ROUTE TO New@Appname@Typed59,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_60' THEN ROUTE TO New@Appname@Typed60,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_61' THEN ROUTE TO New@Appname@Typed61,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_62' THEN ROUTE TO New@Appname@Typed62,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_63' THEN ROUTE TO New@Appname@Typed63,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_64' THEN ROUTE TO New@Appname@Typed64,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_65' THEN ROUTE TO New@Appname@Typed65,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_66' THEN ROUTE TO New@Appname@Typed66,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_67' THEN ROUTE TO New@Appname@Typed67,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_68' THEN ROUTE TO New@Appname@Typed68,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_69' THEN ROUTE TO New@Appname@Typed69,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_70' THEN ROUTE TO New@Appname@Typed70,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_71' THEN ROUTE TO New@Appname@Typed71,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_72' THEN ROUTE TO New@Appname@Typed72,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_73' THEN ROUTE TO New@Appname@Typed73,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_74' THEN ROUTE TO New@Appname@Typed74,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_75' THEN ROUTE TO New@Appname@Typed75,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_76' THEN ROUTE TO New@Appname@Typed76,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_77' THEN ROUTE TO New@Appname@Typed77,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_78' THEN ROUTE TO New@Appname@Typed78,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_79' THEN ROUTE TO New@Appname@Typed79,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_80' THEN ROUTE TO New@Appname@Typed80,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_81' THEN ROUTE TO New@Appname@Typed81,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_82' THEN ROUTE TO New@Appname@Typed82,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_83' THEN ROUTE TO New@Appname@Typed83,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_84' THEN ROUTE TO New@Appname@Typed84,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_85' THEN ROUTE TO New@Appname@Typed85,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_86' THEN ROUTE TO New@Appname@Typed86,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_87' THEN ROUTE TO New@Appname@Typed87,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_88' THEN ROUTE TO New@Appname@Typed88,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_89' THEN ROUTE TO New@Appname@Typed89,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_90' THEN ROUTE TO New@Appname@Typed90,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_91' THEN ROUTE TO New@Appname@Typed91,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_92' THEN ROUTE TO New@Appname@Typed92,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_93' THEN ROUTE TO New@Appname@Typed93,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_94' THEN ROUTE TO New@Appname@Typed94,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_95' THEN ROUTE TO New@Appname@Typed95,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_96' THEN ROUTE TO New@Appname@Typed96,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_97' THEN ROUTE TO New@Appname@Typed97,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_98' THEN ROUTE TO New@Appname@Typed98,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_99' THEN ROUTE TO New@Appname@Typed99,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_1000100' THEN ROUTE TO New@Appname@Typed100,*/
ELSE ROUTE TO New@Appname@TypedElse;

CREATE OR REPLACE cq New@Appname@cq1
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed1 s;

CREATE OR REPLACE cq New@Appname@cq2
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed2 s;

CREATE OR REPLACE cq New@Appname@cq3
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed3 s;

CREATE OR REPLACE cq New@Appname@cq4
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed4 s;

CREATE OR REPLACE cq New@Appname@cq5
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed5 s;

CREATE OR REPLACE cq New@Appname@cq6
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed6 s;

CREATE OR REPLACE cq New@Appname@cq7
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed7 s;

CREATE OR REPLACE cq New@Appname@cq8
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed8 s;

CREATE OR REPLACE cq New@Appname@cq9
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed9 s;

CREATE OR REPLACE cq New@Appname@cq10
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed10 s;


CREATE OR REPLACE cq New@Appname@cq11
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed11 s;

CREATE OR REPLACE cq New@Appname@cq12
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed12 s;

CREATE OR REPLACE cq New@Appname@cq13
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed13 s;

CREATE OR REPLACE cq New@Appname@cq14
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed14 s;

CREATE OR REPLACE cq New@Appname@cq15
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed15 s;

CREATE OR REPLACE cq New@Appname@cq16
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed16 s;

CREATE OR REPLACE cq New@Appname@cq117
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed17 s;

CREATE OR REPLACE cq New@Appname@cq18
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed18 s;

CREATE OR REPLACE cq New@Appname@cq19
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed19 s;

CREATE OR REPLACE cq New@Appname@cq20
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed20 s;

CREATE OR REPLACE cq New@Appname@cq21
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed21 s;

CREATE OR REPLACE cq New@Appname@cq22
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed22 s;

CREATE OR REPLACE cq New@Appname@cq23
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed23 s;

CREATE OR REPLACE cq New@Appname@cq24
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed24 s;

CREATE OR REPLACE cq New@Appname@cq25
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed25 s;

CREATE OR REPLACE cq New@Appname@cq26
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed26 s;

CREATE OR REPLACE cq New@Appname@cq27
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed27 s;

CREATE OR REPLACE cq New@Appname@cq28
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed28 s;

CREATE OR REPLACE cq New@Appname@cq29
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed29 s;

CREATE OR REPLACE cq New@Appname@cq30
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed30 s;

CREATE OR REPLACE cq New@Appname@cq31
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed31 s;

CREATE OR REPLACE cq New@Appname@cq32
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed32 s;

CREATE OR REPLACE cq New@Appname@cq33
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed33 s;

CREATE OR REPLACE cq New@Appname@cq34
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed34 s;

CREATE OR REPLACE cq New@Appname@cq35
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed35 s;

CREATE OR REPLACE cq New@Appname@cq36
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed36 s;

CREATE OR REPLACE cq New@Appname@cq37
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed37 s;

CREATE OR REPLACE cq New@Appname@cq38
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed38 s;

CREATE OR REPLACE cq New@Appname@cq39
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed39 s;

CREATE OR REPLACE cq New@Appname@cq40
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed40 s;

CREATE OR REPLACE cq New@Appname@cq41
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed41 s;

CREATE OR REPLACE cq New@Appname@cq42
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed42 s;

CREATE OR REPLACE cq New@Appname@cq43
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed43 s;

CREATE OR REPLACE cq New@Appname@cq44
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed44 s;

CREATE OR REPLACE cq New@Appname@cq45
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed45 s;

CREATE OR REPLACE cq New@Appname@cq46
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed46 s;

CREATE OR REPLACE cq New@Appname@cq47
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed47 s;

CREATE OR REPLACE cq New@Appname@cq48
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed48 s;

CREATE OR REPLACE cq New@Appname@cq49
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed49 s;

CREATE OR REPLACE cq New@Appname@cq50
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed50 s;
/*
CREATE OR REPLACE cq New@Appname@cq51
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed51 s;

CREATE OR REPLACE cq New@Appname@cq52
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed52 s;

CREATE OR REPLACE cq New@Appname@cq53
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed53 s;

CREATE OR REPLACE cq New@Appname@cq54
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed54 s;

CREATE OR REPLACE cq New@Appname@cq55
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed55 s;

CREATE OR REPLACE cq New@Appname@cq56
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed56 s;

CREATE OR REPLACE cq New@Appname@cq57
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed57 s;

CREATE OR REPLACE cq New@Appname@cq58
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed58 s;

CREATE OR REPLACE cq New@Appname@cq59
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed59 s;

CREATE OR REPLACE cq New@Appname@cq60
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed60 s;

CREATE OR REPLACE cq New@Appname@cq61
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed61 s;

CREATE OR REPLACE cq New@Appname@cq62
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed62 s;

CREATE OR REPLACE cq New@Appname@cq63
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed63 s;

CREATE OR REPLACE cq New@Appname@cq64
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed64 s;

CREATE OR REPLACE cq New@Appname@cq65
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed65 s;

CREATE OR REPLACE cq New@Appname@cq66
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed66 s;

CREATE OR REPLACE cq New@Appname@cq67
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed67 s;

CREATE OR REPLACE cq New@Appname@cq68
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed68 s;

CREATE OR REPLACE cq New@Appname@cq69
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed69 s;

CREATE OR REPLACE cq New@Appname@cq70
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed70 s;

CREATE OR REPLACE cq New@Appname@cq71
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed71 s;

CREATE OR REPLACE cq New@Appname@cq72
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed72 s;

CREATE OR REPLACE cq New@Appname@cq73
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed73 s;

CREATE OR REPLACE cq New@Appname@cq74
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed74 s;

CREATE OR REPLACE cq New@Appname@cq75
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed75 s;

CREATE OR REPLACE cq New@Appname@cq76
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed76 s;

CREATE OR REPLACE cq New@Appname@cq77
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed77 s;

CREATE OR REPLACE cq New@Appname@cq78
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed78 s;

CREATE OR REPLACE cq New@Appname@cq79
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed79 s;

CREATE OR REPLACE cq New@Appname@cq80
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed80 s;

CREATE OR REPLACE cq New@Appname@cq81
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed81 s;

CREATE OR REPLACE cq New@Appname@cq82
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed82 s;

CREATE OR REPLACE cq New@Appname@cq83
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed83 s;

CREATE OR REPLACE cq New@Appname@cq84
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed84 s;

CREATE OR REPLACE cq New@Appname@cq85
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed85 s;

CREATE OR REPLACE cq New@Appname@cq86
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed86 s;

CREATE OR REPLACE cq New@Appname@cq87
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed87 s;

CREATE OR REPLACE cq New@Appname@cq88
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed88 s;

CREATE OR REPLACE cq New@Appname@cq89
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed89 s;

CREATE OR REPLACE cq New@Appname@cq90
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed90 s;

CREATE OR REPLACE cq New@Appname@cq91
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed91 s;

CREATE OR REPLACE cq New@Appname@cq92
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed92 s;

CREATE OR REPLACE cq New@Appname@cq93
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed93 s;

CREATE OR REPLACE cq New@Appname@cq94
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed94 s;

CREATE OR REPLACE cq New@Appname@cq95
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed95 s;

CREATE OR REPLACE cq New@Appname@cq96
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed96 s;

CREATE OR REPLACE cq New@Appname@cq97
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed97 s;

CREATE OR REPLACE cq New@Appname@cq98
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed98 s;

CREATE OR REPLACE cq New@Appname@cq99
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed99 s;

CREATE OR REPLACE cq New@Appname@cq100
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed100 s;

CREATE OR REPLACE cq New@Appname@cq101
INSERT INTO Newcombinedstream
select s FROM New@Appname@TypedElse s;*/

create target systar using sysout(name:'out')INPUT FROM Newcombinedstream;

CREATE OR REPLACE TARGET @Appname@Target USING DatabaseWriter (
ConnectionURL:'jdbc:mysql://162.222.179.3:3306/waction',
Username:'root',
Password:'w@ct10n',
Tables:'waction.TABLE_TEST_%;waction.RESULTTABLE',
IgnorableExceptioncode : 'NO_OP_DELETE,NO_OP_UPDATE,NO_OP_PKUPDATE',
CommitPolicy:'Eventcount:1000,Interval:3600',
BatchPolicy:'eventCount:1000,Interval:3600'
)
INPUT FROM Newcombinedstream;

end application @Appname@;
deploy application @Appname@;
start application @Appname@;

CREATE OR REPLACE APPLICATION @AppFeature@;

CREATE OR REPLACE SOURCE initialLoad_Src USING Global.DatabaseReader (
  FetchSize: 100,
  QuiesceOnILCompletion: false,
  Tables: '@SrcTable@',
  adapterName: 'DatabaseReader',
  Password: '@SrcPswd@',
  Username: '@SrcUserName@',
  ConnectionURL: '@SrcUrl@'
   )
OUTPUT TO @AppName@_Stream;

CREATE OR REPLACE TARGET initialLoadPostgres_Trg USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:100,Interval:60',
  StatementCacheSize: '50',
  ConnectionURL: '@TrgUrl@',
  Username: '@TrgUserName@',
  BatchPolicy: 'EventCount:100,Interval:60',
  Tables: '@SrcTable@,@TrgTable@',
  Password: '@TrgPswd@',
  adapterName: 'DatabaseWriter' )
INPUT FROM @AppName@_Stream;

END APPLICATION @AppName@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:100,Interval:3',
  CommitPolicy: 'EventCount:100,Interval:3',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@1;

CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@1;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE APPLICATION @APPNAME@ WITH ENCRYPTION @Recovery@ USE EXCEPTIONSTORE;
create flow @APPNAME@_SourceFlow;
CREATE SOURCE @APPNAME@_s USING FileReader(
  directory:'Samples/AppData',
  wildcard:'PO.JSON',
  positionByEOF:false
)
parse using JSONParser (
) OUTPUT TO @APPNAME@_ss1;
end flow @APPNAME@_SourceFlow;
create target @APPNAME@_t using sysout (name:ss1) input from @APPNAME@_ss1;
end application @APPNAME@;
deploy application @APPNAME@ @DP@;
start @APPNAME@;

Stop @APPNAME@_App;
undeploy application @APPNAME@_App;
drop application @APPNAME@_App cascade;

CREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;
CREATE SOURCE @APPNAME@_Src USING Global.Ojet(
  Username:'@SOURCE_USER@',
  Password:'@SOURCE_PASSWORD@',
  ConnectionURL:'@SOURCE_URL@',
  Tables:'@SOURCE_TABLES@',
  ConnectionRetryPolicy:'@AUTO_CONNECTION_RETRY@',
  OJetConfig: '{ "OJET" : [ "retriable_errors:ORA-26804" ] }'
)
OUTPUT TO @APPNAME@_Stream;

CREATE TARGET @APPNAME@_SYS USING SysOut (
  name: '@APPNAME@_SYS' )
INPUT FROM @APPNAME@_Stream;

End APPLICATION @APPNAME@_App;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;


CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using PostgreSQLReader
(
   adapterName: PostgreSQLReader,
   CDDLAction: Quiesce_Cascade,
   CDDLCapture: true,
   CDDLTrackingTable:'striim.ddlcapturetable',
   ConnectionURL: jdbc:postgresql://localhost:5432/qatest,
   FilterTransactionBoundaries: true,
   Password: w@ct10n,
   ReplicationSlotName:'test_slot',
   Tables: public.PGMultiDownstream_src,
   Username: sa,
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;

CREATE APPLICATION @APPNAME3@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName1@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME3@;
DEPLOY APPLICATION @APPNAME3@;
START APPLICATION @APPNAME3@;

--
-- Crash Recovery Test 7 with Jumping window and partitioned on two node cluster with one agent
-- Bert Hashemi, WebAction, Inc.
--
-- S -> KafkaStream -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION N2S2CR7Tester.N2S2CRTest7;
UNDEPLOY APPLICATION N2S2CR7Tester.N2S2CRTest7;
DROP APPLICATION N2S2CR7Tester.N2S2CRTest7 CASCADE;
CREATE APPLICATION N2S2CRTest7 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest7;

CREATE SOURCE CsvSourceN2S2CRTest7 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;


CREATE TYPE CsvDataTypeN2S2CRTest7 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream of CsvDataTypeN2S2CRTest7 using KafkaProps;

CREATE CQ TransferToKafka
INSERT INTO KafkaCsvStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

END FLOW DataAcquisitionN2S2CRTest7;





CREATE FLOW DataProcessingN2S2CRTest7;

CREATE STREAM DataStream OF CsvDataTypeN2S2CRTest7 PARTITION BY merchantId;

CREATE CQ CsvToDataN2S2CRTest7
INSERT INTO DataStream
SELECT
    *
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsN2S2CRTest7 CONTEXT OF CsvDataTypeN2S2CRTest7
EVENT TYPES ( CsvDataTypeN2S2CRTest7 )
@PERSIST-TYPE@

CREATE CQ DataToWactionN2S2CRTest7
INSERT INTO WactionsN2S2CRTest7
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN2S2CRTest7;

END APPLICATION N2S2CRTest7;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

CREATE  SOURCE @QuerySource@1 USING DatabaseReader  (
  ReplicationSlotName:'@slotname1@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@1;
create Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;
 CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@1;

CREATE  SOURCE @QuerySource@2 USING DatabaseReader  (
  ReplicationSlotName:'@slotname2@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@2;
create Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;
 CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@2;

CREATE  SOURCE @QuerySource@3 USING DatabaseReader  (
  ReplicationSlotName:'@slotname3@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@3;
create Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;
 CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@3;

CREATE  SOURCE @QuerySource@4 USING DatabaseReader  (
  ReplicationSlotName:'@slotname4@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@4;
create Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;
 CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@4;

CREATE  SOURCE @QuerySource@5 USING DatabaseReader  (
  ReplicationSlotName:'@slotname5@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@5;
create Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;
 CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@5;

CREATE  SOURCE @QuerySource@6 USING DatabaseReader  (
  ReplicationSlotName:'@slotname6@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@6;
create Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;
 CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@6;

CREATE  SOURCE @QuerySource@7 USING DatabaseReader  (
  ReplicationSlotName:'@slotname7@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@7;
create Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;
 CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@7;

CREATE  SOURCE @QuerySource@8 USING DatabaseReader  (
  ReplicationSlotName:'@slotname8@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@8;
create Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;
 CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@8;

CREATE  SOURCE @QuerySource@9 USING DatabaseReader  (
  ReplicationSlotName:'@slotname9@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@9;
create Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;
 CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@9;

CREATE  SOURCE @QuerySource@10 USING DatabaseReader  (
  ReplicationSlotName:'@slotname10@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@10;
create Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;
 CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@10;
end application @APPNAME@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING XMLParser (
  rootnode: ''
)
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

stop application @APPNAME@Apps2;
undeploy application @APPNAME@Apps2;
drop application @APPNAME@Apps2 cascade;


stop application @APPNAME@Apps3;
undeploy application @APPNAME@Apps3;
drop application @APPNAME@Apps3 cascade;



stop application @APPNAME@Apps4;
undeploy application @APPNAME@Apps4;
drop application @APPNAME@Apps4 cascade;



stop application @APPNAME@Apps1;
undeploy application @APPNAME@Apps1;
drop application @APPNAME@Apps1 cascade;

CREATE OR REPLACE PROPERTYSET @APPNAME@Apps_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9099', kafkaversion:'0.11');

CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream1 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;
CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream3 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;
CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream4 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;


--**********************Application 1*******************
-- with 2 source flow 
-- <sourceflow1>source1->PS1<sourceflow1>
-- <sourceflow2>Source2->Inmemory1<sourceflow2>
-- Inmemomry1->cq1->PS1

CREATE APPLICATION @APPNAME@Apps1 RECOVERY 5 SECOND INTERVAL;
create flow @APPNAME@serverflow;
CREATE OR REPLACE SOURCE @APPNAME@AppsDBSource1 USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@1',
  adapterName: 'OracleReader',
  Password: '@PASSWORD@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@USERNAME@',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@Apps_PS_Stream1;
end flow @APPNAME@serverflow;

create flow @APPNAME@serverflow2;
CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream2 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;
CREATE OR REPLACE SOURCE @APPNAME@AppsDBSource2 USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@2;@SOURCE_TABLE@3',
  adapterName: 'OracleReader',
  Password: '@PASSWORD@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@USERNAME@',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@Apps_PS_InmemoryStream1;
end flow @APPNAME@serverflow2;

CREATE CQ @APPNAME@cq_Inmemory1
INSERT INTO @APPNAME@Apps_PS_Stream2
SELECT *
FROM @APPNAME@Apps_PS_InmemoryStream1;

end application @APPNAME@Apps1;

-- ********************Application 2***********************
-- PS1->Target1

CREATE APPLICATION @APPNAME@Apps2 RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE TARGET @APPNAME@Apps2DBTarget1 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:5,Interval:0',
  CommitPolicy: 'EventCount:5,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@1,@TARGET_TABLE@1',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream1;
END APPLICATION @APPNAME@Apps2;

-- ********************Application 3**************************
--WITH 3 TARGET FLOW
-- 1. <targetflow1> cq->ps3->target2 <targetflow1>
-- 2. cq->ps4 <targetflow2>ps4->target3 <targetflow2>
-- 3. <targetflow3> ps2 -> target4 <targetflow3>

CREATE APPLICATION @APPNAME@Apps3 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @APPNAME@TARGETFLOW1;
CREATE CQ @APPNAME@cq_ps1
INSERT INTO @APPNAME@Apps_PS_Stream3
SELECT *
FROM @APPNAME@Apps_PS_Stream2
WHERE META(@APPNAME@Apps_PS_Stream2,'TableName').toString() == '@SOURCE_TABLE@2';
CREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget2 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@2,@TARGET_TABLE@2',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream3;
END FLOW @APPNAME@TARGETFLOW1;

CREATE CQ @APPNAME@cq_ps2
INSERT INTO @APPNAME@Apps_PS_Stream4
SELECT *
FROM @APPNAME@Apps_PS_Stream2
WHERE META(@APPNAME@Apps_PS_Stream2,'TableName').toString() == '@SOURCE_TABLE@3';

CREATE FLOW @APPNAME@TARGETFLOW2;
CREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget3 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:1000,Interval:0',
  CommitPolicy: 'EventCount:1000,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@3,@TARGET_TABLE@3',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream4;
END FLOW @APPNAME@TARGETFLOW2;

CREATE FLOW @APPNAME@TARGETFLOW3;
CREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget4 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:50,Interval:0',
  CommitPolicy: 'EventCount:50,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@2,@TARGET_TABLE@4;@SOURCE_TABLE@3,@TARGET_TABLE@4',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream2;
END FLOW @APPNAME@TARGETFLOW3;

END APPLICATION @APPNAME@Apps3;


--********************Application 4************************
-- PS2->cq->Inmemory2 <targetflow4> Inmemory2->target5<targetflow4>
-- PS3->Target6

CREATE APPLICATION @APPNAME@Apps4 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE STREAM @APPNAME@Apps_PS_InmemoryStream2 OF Global.waevent;

CREATE CQ @APPNAME@cq_Ps_Inmemory1
INSERT INTO @APPNAME@Apps_PS_InmemoryStream2
SELECT *
FROM @APPNAME@Apps_PS_Stream2
WHERE META(@APPNAME@Apps_PS_Stream2,'TableName').toString() == '@SOURCE_TABLE@3';

create flow @APPNAME@targetflow4;
CREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget5 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:10,Interval:0',
  CommitPolicy: 'EventCount:10,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@3,@TARGET_TABLE@5',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_InmemoryStream2;
end flow @APPNAME@targetflow4;

CREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget6 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@2,@TARGET_TABLE@6',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream3;

END APPLICATION @APPNAME@Apps4;

deploy application @APPNAME@Apps4 on ANY in Appsdg with @APPNAME@Targetflow4 in Targetdg;
start application @APPNAME@Apps4;

deploy application @APPNAME@Apps3 on ANY in Appsdg with @APPNAME@Targetflow3 in Targetdg,@APPNAME@Targetflow2 in Targetdg,@APPNAME@Targetflow1 in Targetdg;
start application @APPNAME@Apps3;

deploy application @APPNAME@Apps2 on ANY in default;
start application @APPNAME@Apps2;

deploy application @APPNAME@Apps1 in Appsdg with @APPNAME@serverflow in sourcedg, @APPNAME@serverflow2 in sourcedg2;
start application @APPNAME@Apps1;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING SnowflakeWriter (
  ConnectionURL: '@SNOWFLAKE-URL@',
  username: '@SNOWFLAKE-USERNAME@',
  password: '@SNOWFLAKE-PASSWORD@',
  appendOnly:'false',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  Tables: '@TARGET-TABLES@',
  uploadpolicy:'eventcount:10,interval:5s'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING SnowflakeWriter (
  ConnectionURL: '@SNOWFLAKE-URL@',
  username: '@SNOWFLAKE-USERNAME@',
  password: '@SNOWFLAKE-PASSWORD@',
  appendOnly:'false',
  BatchPolicy: 'Interval:10',
  CommitPolicy: 'Interval:10',
  Tables: '@TARGET-TABLES@',
  uploadpolicy:'eventcount:10,interval:5s'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING SnowflakeWriter (
  ConnectionURL: '@SNOWFLAKE-URL@',
  username: '@SNOWFLAKE-USERNAME@',
  password: '@SNOWFLAKE-PASSWORD@',
  appendOnly:'false',
  BatchPolicy: 'eventCount:100000,Interval:20',
  CommitPolicy: 'eventCount:100000,Interval:20',
  Tables: '@TARGET-TABLES@',
  uploadpolicy:'eventcount:10,interval:5s'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING SnowflakeWriter (
  ConnectionURL: '@SNOWFLAKE-URL@',
  username: '@SNOWFLAKE-USERNAME@',
  password: '@SNOWFLAKE-PASSWORD@',
  appendOnly:'false',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  Tables: '@TARGET-TABLES@',
  uploadpolicy:'eventcount:10,interval:5s'
 )
INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)
INPUT FROM @STREAM@;;

create Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;

--
-- Recovery Test 34 with two sources, two sliding time-count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5a9W/p  -> CQ1 -> WS
-- S2 -> Sc6a11W/p -> CQ2 -> WS
--

STOP KStreamRecov3Tester.KStreamRecovTest34;
UNDEPLOY APPLICATION KStreamRecov3Tester.KStreamRecovTest34;
DROP APPLICATION KStreamRecov34Tester.KStreamRecovTest34 CASCADE;

DROP USER KStreamRecov34Tester;
DROP NAMESPACE KStreamRecov34Tester CASCADE;
CREATE USER KStreamRecov34Tester IDENTIFIED BY KStreamRecov34Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov34Tester;
CONNECT KStreamRecov34Tester KStreamRecov34Tester;

CREATE APPLICATION KStreamRecovTest34 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION KStreamRecovTest34;

--
-- Recovery Test 5 with Jumping window and partitioned
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP KStreamRecov5Tester.KStreamRecovTest5;
UNDEPLOY APPLICATION KStreamRecov5Tester.KStreamRecovTest5;
DROP APPLICATION KStreamRecov5Tester.KStreamRecovTest5 CASCADE;
DROP USER KStreamRecov5Tester;
DROP NAMESPACE KStreamRecov5Tester CASCADE;
CREATE USER KStreamRecov5Tester IDENTIFIED BY KStreamRecov5Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov5Tester;
CONNECT KStreamRecov5Tester KStreamRecov5Tester;

CREATE APPLICATION KStreamRecovTest5 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvData PARTITION BY merchantId;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION KStreamRecovTest5;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING S3Reader (
    bucketname:'',
    objectnameprefix:'',
     )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: 'ParquetFileSchema'
)
INPUT FROM @appname@CqOut;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using ParquetFormatter (
schemaFileName: 'ParquetS3Schema'
)
input from @appname@CqOut;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using ParquetFormatter (
schemaFileName: 'ParquetAzureSchema'
)
INPUT FROM @appname@CqOut;

CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using ParquetFormatter (
schemaFileName: 'ParquetGCSSchema'
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE OR REPLACE PROPERTYVARIABLE Partitionkey1='@metadata(RecordOffset)';
CREATE OR REPLACE PROPERTYVARIABLE Partitionkey2='Col1';
CREATE OR REPLACE PROPERTYVARIABLE OperationTimeout='500000';
CREATE OR REPLACE PROPERTYVARIABLE ConnectionRetryPolicy='Retries:5,RetryBackOff:1m';

STOP APPLICATION EH;
UNDEPLOY APPLICATION EH;
DROP APPLICATION EH CASCADE;
CREATE APPLICATION EH @Recovery@;
CREATE SOURCE s USING FileReader (
	directory:'Product/IntegrationTests/TestData/',
    WildCard:'posdata.csv',
	positionByEOF:false
	)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO ss;

CREATE TYPE userType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String
);

CREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;

CREATE CQ cq1
INSERT INTO userDefinedTypedStream
SELECT data[0],data[1],data[2], data[3],data[4]
FROM ss;

create Target t1 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	Partitionkey:'$Partitionkey1',
	--ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'$OperationTimeout',
	ConnectionRetryPolicy:'$ConnectionRetryPolicy'
)
format using AvroFormatter (
	schemaFileName:'kafkaAvroTest1.avsc')
input from ss;

create Target t2 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	--ParallelThreads:'2',
	Partitionkey:'$Partitionkey2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'$ConnectionRetryPolicy'
)
format using AvroFormatter (
	schemaFileName:'kafkaAvroTest2.avsc')
input from userDefinedTypedStream;

END APPLICATION EH;
DEPLOY APPLICATION EH;
start application EH;


STOP APPLICATION ER;
UNDEPLOY APPLICATION ER;
DROP APPLICATION ER CASCADE;
CREATE APPLICATION ER;
CREATE SOURCE ER_S1 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING avroParser (
schemaFileName:'kafkaAvroTest1.avsc'
)OUTPUT TO ER_SS1;
CREATE SOURCE ER_S2 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING avroParser (
schemaFileName:'kafkaAvroTest2.avsc'
)OUTPUT TO ER_SS2;

create Type CustType 
(writerdata com.fasterxml.jackson.databind.JsonNode
--TopicName java.lang.String,
--PartitionID java.lang.String
);

Create Stream datastream1 of CustType;
Create Stream datastream2 of CustType;

CREATE CQ CustCQ1
INSERT INTO datastream1
SELECT AvroToJson(s1.data)
--metadata.get("TopicName").toString() AS TopicName,
--metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS1 s1;

CREATE CQ CustCQ2
INSERT INTO datastream2
SELECT AvroToJson(s2.data)
--metadata.get("TopicName").toString() AS TopicName,
--metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS2 s2;

create Target ER_t1 using FileWriter (
filename:'FT1_5L_AVRO_RT',
directory:'FEATURE-DIR/logs/',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from datastream1;

create Target ER_t2 using FileWriter (
filename:'FT2_5L_AVRO_RT',
directory:'FEATURE-DIR/logs/',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from datastream2;
end application ER;
deploy application ER;

STOP AdhocTester.ws_one;
UNDEPLOY APPLICATION AdhocTester.ws_one;
DROP APPLICATION AdhocTester.ws_one cascade;

CREATE APPLICATION ws_one;


CREATE SOURCE wsSource USING CSVReader
(
directory:'@TEST-DATA-PATH@',
header: Yes,
wildcard:'AdhocQueryData2.csv',
columndelimiter:',',
blocksize: 10240,
positionByEOF:false
) OUTPUT TO QaStream;

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE ZipLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'AdhocQueryData.csv',
  header: Yes,
  columndelimiter: '	',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

Create TYPE wsData(
	CompanyNum String,
	CompanyName String KEY,
	CompanyCode int,
	Zip String
);


CREATE STREAM wsStream OF wsData;

--Select data from QaStream and insert into wsStream

CREATE CQ csvTowsData
INSERT INTO wsStream
SELECT  data[0],
    data[1],
    TO_INT(data[3]),
    data[9]
 FROM QaStream;




CREATE WACTIONSTORE oneWS CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@


CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT * FROM wsStream
LINK SOURCE EVENT;

END APPLICATION ws_one;

STOP APPLICATION KafkaWPTester.KWApp;
STOP APPLICATION KafkaWPTester.KRApp;
UNDEPLOY APPLICATION KafkaWPTester.KWApp;
UNDEPLOY APPLICATION KafkaWPTester.KRApp;
DROP APPLICATION KafkaWPTester.KWApp CASCADE;
DROP APPLICATION KafkaWPTester.KRApp CASCADE;

CREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;
GRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;
CONNECT KafkaWPTester KafkaWPTester;


CREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE CSVSource USING FileReader (
	directory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',
    WildCard:'access_log_half',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;

CREATE TYPE AccessLogType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;

CREATE CQ AceeslogCQ
INSERT INTO TypedAccessLogStream
SELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]
FROM FileStream;

create Target DsvWriter using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V10dsvExistTopic',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream;


create Target JsonWriter using KafkaWriter VERSION '0.10.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V10jsonExistTopic',
	Mode:'Sync'
    )
 
FORMAT USING JSONFormatter ()
input from TypedAccessLogStream;


create Target AvroWriter using KafkaWriter VERSION '0.10.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V10avroExistTopic',
    Mode:'Sync'
    )
 
FORMAT USING AvroFormatter (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')
input from TypedAccessLogStream;


END APPLICATION KWApp;

DEPLOY APPLICATION KWApp on any in default;


-- Kafka Reader Apps

CREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;


CREATE TYPE AccessLogType2(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE Stream KafkaDSVReaderStream of AccessLogType2;
CREATE Stream KafkaJsonReaderStream of AccessLogType2;
CREATE Stream KafkaAvroReaderStream of AccessLogType2;


CREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V10dsvExistTopic',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;




CREATE CQ DSVtoKafkaDSVReaderStream
INSERT INTO KafkaDSVReaderStream
SELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]
FROM KafkaReaderStream1;

CREATE TARGET DSVDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V10_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaDSVReaderStream;

CREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V10jsonExistTopic',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;




CREATE CQ DSVtoKafkaJsonReaderStream
INSERT INTO KafkaJsonReaderStream
SELECT 
data.get('Col1').textValue(),
data.get('Col2').textValue(),
data.get('Col3').textValue(),
data.get('Col4').textValue(),
data.get('Col5').textValue(),
data.get('Col6').textValue(),
data.get('Col7').textValue(),
data.get('Col8').textValue(),
data.get('Col9').textValue(),
data.get('Col10').textValue()
FROM KafkaReaderStream2;

CREATE TARGET JSONDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V10_RT_JSON')
FORMAT USING DSVFormatter()
INPUT FROM KafkaJsonReaderStream;

CREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V10avroExistTopic',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'
)
OUTPUT TO KafkaReaderStream3;



CREATE Stream KRTypedStream of AccessLogType;

CREATE CQ DSVtoKafkaAvroReaderStream
INSERT INTO KafkaAvroReaderStream
SELECT    
data.get("Col1").toString(), 
data.get("Col2").toString(), 
data.get("Col3").toString(), 
data.get("Col4").toString(),
data.get("Col5").toString(), 
data.get("Col6").toString(), 
data.get("Col7").toString(), 
data.get("Col8").toString(),
data.get("Col9").toString(), 
data.get("Col10").toString()
FROM KafkaReaderStream3;

CREATE TARGET AVRODump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V10_RT_AVRO')
FORMAT USING DSVFormatter()
INPUT FROM KafkaAvroReaderStream;

END APPLICATION KRApp;
DEPLOY APPLICATION KRApp on any in default;

--
-- Recovery Test 42 with two sources and two WactionStores. A variety of partitioned windows in between
-- assure that we are testing a complicated recovery scenario.
--
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Stream -> JWc5 -> WS1
--   S2 -> Stream -> JWc10 -> WS2
--

STOP Recov42Tester.RecovTest42;
UNDEPLOY APPLICATION Recov42Tester.RecovTest42;
DROP APPLICATION Recov42Tester.RecovTest42 CASCADE;
CREATE APPLICATION RecovTest42 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10242,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10242,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM DataStreamTop OF CsvData using KafkaProps;

CREATE CQ Csv1ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ Csv2ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;










CREATE JUMPING WINDOW LeftJWc5
OVER DataStreamTop KEEP 5 ROWS;

CREATE JUMPING WINDOW RightJWc10
OVER DataStreamTop KEEP 10 ROWS;



CREATE WACTIONSTORE WactionsLeft CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsRight CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ ToWactionsLeft
INSERT INTO WactionsLeft
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM LeftJWc5 p;

CREATE CQ ToWactionsRight
INSERT INTO WactionsRight
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM RightJWc10 p;

END APPLICATION RecovTest42;

--
-- Crash Recovery Test 3 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW -> CQ(aggregate) -> WS
--

STOP APPLICATION KStreamN2S2CR3Tester.KStreamN2S2CRTest3;
UNDEPLOY APPLICATION KStreamN2S2CR3Tester.KStreamN2S2CRTest3;
DROP APPLICATION KStreamN2S2CR3Tester.KStreamN2S2CRTest3 CASCADE;

DROP USER KStreamN2S2CR3Tester;
DROP NAMESPACE KStreamN2S2CR3Tester CASCADE;
CREATE USER KStreamN2S2CR3Tester IDENTIFIED BY KStreamN2S2CR3Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR3Tester;
CONNECT KStreamN2S2CR3Tester KStreamN2S2CR3Tester;

CREATE APPLICATION KStreamN2S2CRTest3 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest3;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest3 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest3;

CREATE FLOW DataProcessingKStreamN2S2CRTest3;

CREATE TYPE WactionTypeKStreamN2S2CRTest3 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount int,
  city String
);

CREATE STREAM DataStream OF WactionTypeKStreamN2S2CRTest3;

CREATE CQ CsvToDataKStreamN2S2CRTest3
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_INT(TO_DOUBLE(data[7])),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest3 CONTEXT OF WactionTypeKStreamN2S2CRTest3
EVENT TYPES ( WactionTypeKStreamN2S2CRTest3 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsKStreamN2S2CRTest3
INSERT INTO WactionsKStreamN2S2CRTest3
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

END FLOW DataProcessingKStreamN2S2CRTest3;

END APPLICATION KStreamN2S2CRTest3;

use PosTester;
DROP TYPE MerchantTxRate;

-----------------------------------
stop application SourceAgentApp;
undeploy application SourceAgentApp;

stop application TargetServerApp1;
undeploy application TargetServerApp1;

stop application TargetServerApp2;
undeploy application TargetServerApp2;

stop application TargetServerApp3;
undeploy application TargetServerApp3;

drop application SourceAgentApp cascade;
drop application TargetServerApp1 cascade;
drop application TargetServerApp2 cascade;
drop application TargetServerApp3 cascade;


CREATE APPLICATION SourceAgentApp;

create flow flow1;
create source CSVSource using FileReader (
directory: '@TEST-DATA-PATH@/tmp',
WildCard:'mybanks*',
positionByEOF: true,
charset:'UTF-8'
) parse using DSVParser (header:'no')
OUTPUT TO CsvStream;
end flow flow1;

--CREATE TARGET T USING Sysout(name:'sysout1') INPUT FROM CsvStream;

END APPLICATION SourceAgentApp;

DEPLOY APPLICATION SourceAgentApp with flow1 in AGENTS;

-- Fisrt app consuming from app running in agent
CREATE APPLICATION TargetServerApp1;
create flow flow2;

CREATE TARGET T2 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp1_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')
FORMAT USING JSONFormatter ()
INPUT FROM CsvStream;
end flow flow2;

END APPLICATION TargetServerApp1;
deploy application TargetServerApp1 with flow2 in default;


-- another app consuming from app running in agent

CREATE APPLICATION TargetServerApp2;
create flow flow3;

CREATE TARGET T3 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp2_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')
FORMAT USING JSONFormatter ()
INPUT FROM CsvStream;
end flow flow3;

END APPLICATION TargetServerApp2;
deploy application TargetServerApp2 with flow3 in default;

-- another app consuming from app running in agent. This app will be deployed and started after other apps started.

CREATE APPLICATION TargetServerApp3;
create flow flow4;

CREATE TARGET T4 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp3_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')
 FORMAT USING JSONFormatter ()
 INPUT FROM CsvStream;
end flow flow4;

END APPLICATION TargetServerApp3;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '@GRPPOLICY@',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'MultipleEvents',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
  filename: '%@metadata(FileName)%',
  directory: '@DIR@',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter  ()
INPUT FROM CCBStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

stop application @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;

CREATE OR REPLACE APPLICATION @AppName@;

CREATE SOURCE @AppName@_MssqlSource USING MSSqlReader
(
   Username:'@userName@',
  Tables: '@tableName@', 
  Password:'@password@',
 DatabaseName:'qatest',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
 AutoDisableTableCDC:false,
 connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5'
)
OUTPUT TO @AppName@_Stream;

CREATE OR REPLACE CQ @AppName@_Cq
INSERT INTO InputData
select userdata, metadata,
CASE WHEN meta(h,'OperationName').toString() = 'DELETE' THEN data(h) ELSE before(h) END AS before , 
CASE WHEN meta(h,'OperationName').toString() = 'DELETE' THEN NULL ELSE data(h) END AS data,
CASE WHEN meta(h,'OperationName').toString() = 'UPDATE' THEN data(h) ELSE before(h) END AS before ,
CASE WHEN meta(h,'OperationName').toString() = 'INSERT' THEN data(h) ELSE before(h) END AS before
from  @AppName@_Stream  h where meta(h,'TableName').tostring() = 'qatest.MetaDataTable';;


CREATE OR REPLACE TARGET @AppName@_FileWriterTarget USING Global.FileWriter ( 
  flushpolicy: 'EventCount:10000,Interval:30s', 
  adapterName: 'FileWriter', 
  directory: '@logsDir@',
  filename: '@FileName@', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING Global.JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter', 
  jsonMemberDelimiter: '\n', 
  EventsAsArrayOfJsonObjects: 'true', 
  formatterName: 'JSONFormatter', 
  jsonobjectdelimiter: '\n' ) 
INPUT FROM InputData;

END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE @SOURCE@ USING MSSQLReader
 (
   Username: '@LOGMINER-UNAME@',
   Password: '@LOGMINER-PASSWORD@',
   ConnectionURL: '@LOGMINER-URL@',
   DatabaseName:'qatest',
   Tables: '@SOURCE_TABLE@',
    Compression:false,
    AutoDisableTableCDC:false,FetchTransactionMetadata:true,
    StartPosition:'EOF'
 )
OUTPUT TO @STREAM@;


CREATE OR REPLACE TARGET @TARGET@1 USING Global.DeltaLakeWriter (
  uploadPolicy: 'eventcount:10,interval:10s',
  Mode: 'MERGE',
  hostname: ' ',
  Tables: ' ',
  stageLocation: '/',
  CDDLAction: 'Process',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken: ' ',
  connectionUrl: ' '  )

INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@2 USING Global.DeltaLakeWriter (
  uploadPolicy: 'eventcount:10,interval:10s',
  Mode: 'MERGE',
  optimizedMerge: 'true',
  hostname: ' ',
  Tables: ' ',
  stageLocation: '/',
  CDDLAction: 'Process',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken: ' ',
  connectionUrl: ' '  )

INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@3 USING Global.DeltaLakeWriter (
  uploadPolicy: 'eventcount:10,interval:10s',
  Mode: 'APPENDONLY',
  hostname: ' ',
  Tables: ' ',
  stageLocation: '/',
  CDDLAction: 'Process',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken: ' ',
  connectionUrl: ' '  )

INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

-- Creating a namespace ensures there won't be conflicts with the regular version of
-- PosApp. The only difference between this version and the regular version is
-- that the CQ that parses the source stream includes a PAUSE clauses that introduces a
-- 40-millisecond pause after each event is read, simulating the way the dashboard would
-- work with real-time data.
Stop PosAppImplicit.PosAppImplicit;
undeploy application PosAppImplicit.PosAppImplicit;
drop application PosAppImplicit.PosAppImplicit cascade;


-- The PosApp sample application demonstrates how a credit card
-- payment processor might use WebAction to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.

CREATE Application PosAppImplicit;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells WebAction that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells WebAction to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvToPosData
--
-- A stream's type must be declared before the stream, and a CQ's
-- output stream must be defined before the CQ. Hence type-stream-CQ
-- sequences like the following are very common.

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       To_String(data[9]) as zip
FROM CsvStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP. These correspond to the merchantId,
-- dateTime, hourValue, amount, and zip fields in PosDataStream, as
-- defined by the PosData type.
--
-- The DATETIME field from the source is converted to both a DateTime
-- value, used as the event timestamp by the application, and an int,
-- which is used to look up historical hourly averages from the
-- HourlyAveLookup cache, discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)

-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateOnly
--
-- The PosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue int,
  hourlyAve int
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId as merchantId,
       p.zip as zip,
       FIRST(p.dateTime) as startingTime,
       COUNT(p.merchantId) as count,
       SUM(p.amount) as totalAmount,
       l.hourlyAve/12 as hourlyAve,
       l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END as upperLimit,
       l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END as lowerLimit,
       '<NOTSET>' as category,
       '<NOTSET>' as status
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId as merchantId,
       zip as zip,
       startingTime as startingTime,
       count as count,
       totalAmount as totalAmount,
       hourlyAve as hourlyAve,
       upperLimit as upperLimit,
       lowerLimit as lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END as category,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END as status
FROM MerchantTxRateOnlyStream;


-- WAction store MerchantActivity
--
-- The following group of statements create and populate the MerchantActivity
-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced
-- with merchant details from NameLookup cache and with latitude and longitude
-- values from the USAddressData cache.

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count int,
  HourlyAve int,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);
CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count int,
  totalAmount double,
  hourlyAve int,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
@PERSIST-TYPE@

CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false
)
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;


-- CQ GenerateAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;


-- The following statement loads visualization (Dashboard) settings
-- from a file.


--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM AlertStream;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;


END APPLICATION PosAppImplicit;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
  directory: '',
  positionByEOF: false,
  WildCard: '')
PARSE USING ParquetParser ()
OUTPUT TO @appname@Streams;

CREATE CQ @appname@CQ1
INSERT INTO @appname@out
SELECT convertParquetEventToWAEvent(o) FROM @appname@Streams o;

CREATE TARGET @filetarget@ USING Global.FileWriter (
  directory: '',
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000,Interval:30s',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  filename: '',
  DataEncryptionKeyPassphrase_encrypted: 'true',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  members: 'data',
  jsonobjectdelimiter: '\n' )
INPUT FROM @appname@out;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE OR REPLACE APPLICATION OtelMonitoringApp;

CREATE OR REPLACE TYPE MonitorBatchEvent_Type (
items java.util.ArrayList
);
CREATE OR REPLACE STREAM MonitorBatchStream OF MonitorBatchEvent_Type;

CREATE OR REPLACE TYPE MonitorEvent_Type (
item com.webaction.runtime.monitor.MonitorEvent
);
CREATE OR REPLACE STREAM MonitorEventStream OF MonitorEvent_Type;


CREATE OR REPLACE CQ MonitorBatchCQ
INSERT INTO MonitorBatchStream
select filterMonEventsForOtelWriter(ms,'INPUT','MEMORY_USED_PERCENT','DISK_FREE','CPU_PER_CORE_PCT','LAG_END2END', 'STATUS_CHANGE') from global.MonitoringSourceStream ms;

CREATE CQ MonitorEventCQ
INSERT INTO MonitorEventStream
SELECT ri FROM MonitorBatchStream m, ITERATOR(m.items) ri;

CREATE OR REPLACE Target OTelWriter1 USING StriimOTelWriter INPUT FROM MonitorEventStream;

END APPLICATION OtelMonitoringApp;
deploy application OtelMonitoringApp;
start OtelMonitoringApp;

CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  (
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: '@CDC_URL@',
  DatabaseName:'@DatabaseName@',
  Tables: '@Source1Tables@' )
OUTPUT TO @APPNAME@cdcStream;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@1 (
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
  FetchSize: 20,
  DatabaseProviderType: 'Default',
  Table: '@Source3Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')
OF @APPNAME@cachetype;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@2 (
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
  FetchSize: 10,
  DatabaseProviderType: 'Default',
  Table: '@Source2Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')
OF @APPNAME@cachetype;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;


CREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@DataSrc USING OracleReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
)
OUTPUT TO @APPNAME@DataStream;

CREATE OR REPLACE TARGET @APPNAME@DataTrgt USING MongoDBWriter (
  ConnectionURL: '',
  Username: '',
  Password: '',
  collections: ''
  AuthDB: '',
  batchpolicy: 'EventCount:1000, Interval:30',
 )
INPUT FROM @APPNAME@DataStream;

CREATE OR REPLACE SOURCE @APPNAME@_src USING MongoDBReader (
  ConnectionURL: '',
  Username: '',
  password: '',
  authDB: '',
  collections: '',
  mode: 'Incremental'
  )
OUTPUT TO @APPNAME@stream;

CREATE CQ @APPNAME@CQ
INSERT INTO @APPNAME@CQSTREAM
SELECT data.get("NUM_COL").toString() AS NUM_COL,
  data.get("CHAR_COL").toString() AS CHAR_COL,
  data.get("VARCHAR2_COL").toString() AS VARCHAR2_COL,
  data.get("FLOAT_COL").toString() AS FLOAT_COL,
  data.get("BINARY_FLOAT_COL").toString() AS BINARY_FLOAT_COL,
  data.get("BINARY_DOUBLE_COL").toString() AS BINARY_DOUBLE_COL,
  data.get("DATE_COL").toString() AS DATE_COL,
  data.get("TIMESTAMP_COL").toString() AS TIMESTAMP_COL
FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING AvroFormatter (
schemaFileName: '@SCHEMAFILE@'
)
INPUT FROM @APPNAME@CQSTREAM;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING DSVFormatter ()
INPUT FROM @APPNAME@CQSTREAM;

END APPLICATION @APPNAME@;

CREATE OR REPLACE TARGET  @TARGET_NAME@ USING CassandraCosmosDBWriter  (
  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --CheckPointTable: '',
  FlushPolicy: 'EventCount:1,Interval:60',
  AccountEndpoint: 'cassandracosmostest.cassandra.cosmos.azure.com',
  AccountKey: 'pqDZvVgbdSCg7VzIzD77dAhPG2odGRZPLhAQA1qnZbAKoIDk6RuQX5r2phbRQFnR1l54qxOcvBXNdz8DeijYIg==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  --ConsistencyLevel:'fh',
  --port:'10505',
  Tables: 'QATEST.Source1,test.target1',
  --Tables: 'QATEST.OracToCql_alldatatypes,test.tgt_data',
  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',
  adapterName: 'CassandraCosmosDBWriter'
 )
 INPUT FROM @STREAM@;

stop application MySQLToOracle;
undeploy application MySQLToOracle;
drop application MySQLToOracle cascade;

CREATE APPLICATION MySQLToOracle recovery 5 second interval;

create source Src1ReadFromMySQL USING MySQLReader (
Username: 'root',
  Password: 'w@ct10n',
  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',
--Tables: 'waction.mytable1;waction.mytable2;qatest.mysqlmarker',
Tables: 'waction.Parent%;waction.Child%;',
--Tables: 'waction.mytable%',
BiDirectionalMarkerTable: 'waction.mysqlmarker',
compression:'false',
sendBeforeImage:True
) OUTPUT TO App1Stream;

/*
CREATE TARGET MySQLReaderOutput
USING SysOut(name:MySQLReaderOutput)
INPUT FROM App1Stream;
*/

CREATE TARGET MySQLReaderOutput using FileWriter(
  filename:'MySQLReaderOutput.log',
  flushpolicy: 'EventCount:1, Interval:30s',
  rolloverpolicy: 'Eventcount:1000, rolloverpolicy:3000s')
FORMAT USING JsonFormatter ()

INPUT FROM App1Stream;


CREATE TARGET WriteToOracle USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'EventCount:10,Interval:5',
CommitPolicy:'EventCount:15,Interval:10',
BiDirectionalMarkerTable: 'qatest.mysqlmarker',
--Tables: 'waction.mytable1,qatest.mytable1;'
--Tables: 'waction.table_1_1,qatest.table_1_1;waction.table_2_1,qatest.table_2_1;waction.table_1_2,qatest.table_1_2;waction.table_2_2,qatest.table_2_2'
--Tables: 'waction.mytable%,qatest.%'
Tables: 'waction.Parent%,qatest.%;waction.Child%,qatest.%'

)
INPUT FROM App1Stream;



--Orcle to MySQL

Create Source SrcReadFromOracle
Using OracleReader
(
Username:'qatest',
Password:'qatest',
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
--Tables:'qatest.mytable1;qatest.mytable2;qatest.mysqlmarker;qatest.mysqlmarker2',
Tables:'QATEST.PARENT%;QATEST.CHILD%',
--Tables:'qatest.mytable%',
BiDirectionalMarkerTable: 'QATEST.MYSQLMARKER',
FilterTransactionBoundaries: true,
compression:'false',
FetchSize: 1

)
Output To App2Stream;

/*
CREATE TARGET MSSqlReaderOutput
USING SysOut(name:MSSqlReaderOutput)
INPUT FROM App2Stream;
*/

CREATE TARGET OracleReaderOutput using FileWriter(
  filename:'OracleReaderOutput.log',
    flushpolicy: 'EventCount:1, Interval:30s',
  rolloverpolicy: 'Eventcount:1000, rolloverpolicy:3000s')
FORMAT USING JsonFormatter ()

INPUT FROM App2Stream;



CREATE TARGET WriteToMySQL1 USING DatabaseWriter(
ConnectionURL:'jdbc:mysql://localhost:3306/waction',
Username:'root',
Password:'w@ct10n',
BatchPolicy:'EventCount:10,Interval:10',
CommitPolicy: 'EventCount:15,Interval:12',
BiDirectionalMarkerTable: 'waction.mysqlmarker',
--Tables: 'qatest.mytable1,waction.mytable1;qatest.mytable2,waction.mytable3;'
Tables: 'QATEST.PARENT_1,waction.Parent_1;QATEST.PARENT_2,waction.Parent_2;QATEST.CHILD_1,waction.Child_1;QATEST.CHILD_2,waction.Child_2;'
--Tables: 'qatest.mytable%,waction.%'
)
INPUT FROM App2Stream;




END APPLICATION MySQLToOracle;
deploy application MySQLToOracle;
start application MySQLToOracle;

STOP APPLICATION KafkaWPTester.KWApp;
STOP APPLICATION KafkaWPTester.KRApp;
UNDEPLOY APPLICATION KafkaWPTester.KWApp;
UNDEPLOY APPLICATION KafkaWPTester.KRApp;
DROP APPLICATION KafkaWPTester.KWApp CASCADE;
DROP APPLICATION KafkaWPTester.KRApp CASCADE;

CREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;
GRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;
CONNECT KafkaWPTester KafkaWPTester;


CREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE CSVSource USING FileReader (
	directory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',
    WildCard:'access_log_half',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;

CREATE TYPE AccessLogType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;

CREATE CQ AceeslogCQ
INSERT INTO TypedAccessLogStream
SELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]
FROM FileStream;

create Target DsvWriter using KafkaWriter VERSION '2.1.0' ( 
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V2dsvExistTopic',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream;


create Target JsonWriter using KafkaWriter VERSION '2.1.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V2jsonExistTopic',
	Mode:'Sync'
    )
 
FORMAT USING JSONFormatter ()
input from TypedAccessLogStream;


create Target AvroWriter using KafkaWriter VERSION '2.1.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V2avroExistTopic',
    Mode:'Sync'
    )
 
FORMAT USING AvroFormatter (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')
input from TypedAccessLogStream;


END APPLICATION KWApp;

DEPLOY APPLICATION KWApp on any in default;


-- Kafka Reader Apps

CREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;


CREATE TYPE AccessLogType2(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE Stream KafkaDSVReaderStream of AccessLogType2;
CREATE Stream KafkaJsonReaderStream of AccessLogType2;
CREATE Stream KafkaAvroReaderStream of AccessLogType2;


CREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V2dsvExistTopic',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;




CREATE CQ DSVtoKafkaDSVReaderStream
INSERT INTO KafkaDSVReaderStream
SELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]
FROM KafkaReaderStream1;

CREATE TARGET DSVDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V2_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaDSVReaderStream;

CREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V2jsonExistTopic',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;




CREATE CQ DSVtoKafkaJsonReaderStream
INSERT INTO KafkaJsonReaderStream
SELECT 
data.get('Col1').textValue(),
data.get('Col2').textValue(),
data.get('Col3').textValue(),
data.get('Col4').textValue(),
data.get('Col5').textValue(),
data.get('Col6').textValue(),
data.get('Col7').textValue(),
data.get('Col8').textValue(),
data.get('Col9').textValue(),
data.get('Col10').textValue()
FROM KafkaReaderStream2;

CREATE TARGET JSONDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V2_RT_JSON')
FORMAT USING DSVFormatter()
INPUT FROM KafkaJsonReaderStream;

CREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V2avroExistTopic',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'
)
OUTPUT TO KafkaReaderStream3;



CREATE Stream KRTypedStream of AccessLogType;

CREATE CQ DSVtoKafkaAvroReaderStream
INSERT INTO KafkaAvroReaderStream
SELECT    
data.get("Col1").toString(), 
data.get("Col2").toString(), 
data.get("Col3").toString(), 
data.get("Col4").toString(),
data.get("Col5").toString(), 
data.get("Col6").toString(), 
data.get("Col7").toString(), 
data.get("Col8").toString(),
data.get("Col9").toString(), 
data.get("Col10").toString()
FROM KafkaReaderStream3;

CREATE TARGET AVRODump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V2_RT_AVRO')
FORMAT USING DSVFormatter()
INPUT FROM KafkaAvroReaderStream;

END APPLICATION KRApp;
DEPLOY APPLICATION KRApp on any in default;

--
-- Recovery Test 37 with two sources, two jumping time windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jt1W/p -> CQ1 -> WS
--   S2 -> Jt2W/p -> CQ2 -> WS
--

STOP KStreamRecov37Tester.KStreamRecovTest37;
UNDEPLOY APPLICATION KStreamRecov37Tester.KStreamRecovTest37;
DROP APPLICATION KStreamRecov37Tester.KStreamRecovTest37 CASCADE;

DROP USER KStreamRecov37Tester;
DROP NAMESPACE KStreamRecov37Tester CASCADE;
CREATE USER KStreamRecov37Tester IDENTIFIED BY KStreamRecov37Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov37Tester;
CONNECT KStreamRecov37Tester KStreamRecov37Tester;

CREATE APPLICATION KStreamRecovTest37 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 1 SECOND
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 2 SECOND
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION KStreamRecovTest37;

STOP IS2noder.IS2Node;
UNDEPLOY APPLICATION IS2noder.IS2Node;
DROP APPLICATION IS2noder.IS2Node CASCADE;

CREATE APPLICATION IS2Node;

CREATE FLOW ISFLOW1;
----------------------------------------------------
CREATE source implicitSOurce USING FileReader (
      directory:'@TEST-DATA-PATH@',
      columndelimiter: ',',
      wildcard:'ISdata.csv',
      blocksize: 10240,
      positionByEOF:false
)
PARSE USING DSVParser (
      header:False,
      trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity double,
  size long,
  currentDate DateTime);

END FLOW ISFLOW1;
----------------------------------------------------

CREATE FLOW ISFLOW2;

CREATE CACHE cache1 USING CsvReader(
  directory: '@TEST-DATA-PATH@',
  wildcard: 'ISdata.csv',
  header: false,
  columndelimiter: ',',
  trimquote: false
  ) QUERY (keytomap:'productID') OF Atm;


CREATE STREAM newStream OF Atm;


CREATE CQ newCQ
INSERT INTO newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]), TO_DATE(data[5]) FROM
CsvStream;

CREATE WINDOW win1
OVER newStream
KEEP 50 rows;


CREATE CQ newCQ2
INSERT INTO newStream2
SELECT productID as A , stateID AS B, productWeight AS C, quantity AS D, size AS E, currentDate AS F FROM
newStream;


CREATE CQ newCQ3
INSERT INTO newStream3 PARTITION BY A
SELECT A,B,C,D,E,F FROM newStream2 order by C,D
link source event;

CREATE CQ newCQ4
INSERT INTO newStream4
SELECT count(productID),currentDate FROM newStream ORDER BY currentDate
link source event;

CREATE CQ newCQ5
INSERT INTO newStream5
SELECT x.*, y.* from cache1 x, newStream y WHERE x.productweight > 6 ORDER BY x.currentDate;


CREATE WACTIONSTORE WS1 CONTEXT OF Atm
EVENT TYPES(Atm );

CREATE CQ newCQ6
INSERT INTO WS1
SELECT * FROM newStream WHERE productID = '001';

CREATE CQ newCQ7
INSERT INTO newStream6
SELECT aa.productID FROM WS1 [push] aa, cache1 bb;


CREATE CQ newCQ8
INSERT INTO newStream7
SELECT Sum(X.size) FROM (Select size from win1 where productweight > 5) X;

END FLOW ISFLOW2;
----------------------------------------------------

END APPLICATION IS2Node;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'posdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'JsonTarget',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'FileSizeRollingPolicy,filesize:333M,sequence:00'
)
format using JSONFormatter (
  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)

input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizeBig_actual.log') input from TypedCSVStream;
end application DSV;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSV1Source using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO Csv1Stream;

Create Type CSV1Type (
  merchantId String,
  merchantName String
);

Create Stream TypedCSV1Stream of CSV1Type;

CREATE CQ CsvToMerchantNames
INSERT INTO TypedCSV1Stream
SELECT data[0],
       data[1]
FROM Csv1Stream;

create Target t using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'@FEATURE-DIR@/logs/',
  compressiontype : 'garbage',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (

)
input from TypedCSV1Stream;

end application DSV;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

Create Source dataGenSrc Using MSSqlReader
(
 Username:'qatest',
 Password:'w3b@ct10n',
 DatabaseName:'qatest',
 ConnectionURL:'localhost:1433',
 Tables:'@tableNames@', 
 ConnectionPoolSize:1,
 FetchSize:1,
 StartPosition:'EOF'
 )
 Output To LCRStream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;

END APPLICATION DataGenSampleApp;

deploy application DataGenSampleApp;
start  application DataGenSampleApp;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ Recovery 5 second interval;

create stream @APPNAME@_UserdataStream of Global.WAEvent;

create type @APPNAME@_Order_type(
id int,
order_id int,
zipcode int,
category String,
tablename string
);

CREATE OR REPLACE SOURCE @APPNAME@S1 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.order_%'
)
OUTPUT TO @APPNAME@_OrdersStream;

CREATE OR REPLACE SOURCE @APPNAME@S2 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_2',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.second_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream2;

CREATE OR REPLACE SOURCE @APPNAME@S3 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_3',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.third_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream3;

CREATE OR REPLACE SOURCE @APPNAME@S4 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_4',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.fourth_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream4;


Create CQ @APPNAME@_CQUser
insert into @APPNAME@_UserdataStream
select 
putuserdata (data,'Fileowner','FIRST_ORDER') from @APPNAME@_OrdersStream data;


Create CQ @APPNAME@_CQUser2
insert into @APPNAME@_UserdataStream
select 
putuserdata (data2,'Fileowner','SECOND_ORDER') from @APPNAME@_OrdersStream2 data2;


Create CQ @APPNAME@_CQUser3
insert into @APPNAME@_UserdataStream
select 
putuserdata (data3,'Fileowner','THIRD_ORDER') from @APPNAME@_OrdersStream3 data3;


Create CQ @APPNAME@_CQUser4
insert into @APPNAME@_UserdataStream
select 
putuserdata (data4,'Fileowner','FOURTH_ORDER') from @APPNAME@_OrdersStream4 data4;

create stream @APPNAME@_OrderTypedStream1 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream2 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream3 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream4 of @APPNAME@_Order_type;

CREATE CQ @APPNAME@_fin_cq
INSERT INTO @APPNAME@_OrderTypedStream1
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FIRST_ORDER';

CREATE CQ @APPNAME@_fin_cq2
INSERT INTO @APPNAME@_OrderTypedStream2
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'SECOND_ORDER';

CREATE CQ @APPNAME@_fin_cq3
INSERT INTO @APPNAME@_OrderTypedStream3
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'THIRD_ORDER';

CREATE CQ @APPNAME@_fin_cq4
INSERT INTO @APPNAME@_OrderTypedStream4
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FOURTH_ORDER';


create Target @APPNAME@T1 using ADLSGen2Writer(
        filename:'event_data.csv',
        directory:'%category%/%tablename%',
       	accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        uploadpolicy:'eventcount:8,interval:20s'
)
format using DSVFormatter (
    header:'true'
)
input from @APPNAME@_OrderTypedStream1; 

create Target @APPNAME@T2 using ADLSGen2Writer(
        filename:'event_data.xml',
        directory:'%category%/%tablename%',
		accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
   		uploadpolicy:'eventcount:8,interval:20s'
)
format using XMLFormatter (
  elementtuple: 'Order_id:id:order_id:zipcode:category:text=tablename',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from @APPNAME@_OrderTypedStream2; 

create Target @APPNAME@T3 using ADLSGen2Writer(
        filename:'event_data.avro',
        directory:'%category%/%tablename%',
		accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
  		uploadpolicy:'eventcount:8,interval:20s'
)
format using AvroFormatter (
  formatAs: 'Default',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA-FILE@'
)
input from @APPNAME@_OrderTypedStream3; 


create Target @APPNAME@T4 using ADLSGen2Writer(
        filename:'event_data.json',
        directory:'%category%/%tablename%',
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
  		uploadpolicy:'eventcount:8,interval:20s'
)
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@_OrderTypedStream4;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

stop application ADW;
undeploy application ADW;
drop application ADW cascade;
CREATE APPLICATION ADW;

CREATE  SOURCE OracleInitialLoad USING DatabaseReader  
 (
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'@SOURCE-TABLES@',
 FetchSize:2000
) 
OUTPUT TO InitialLoadStream;

CREATE TARGET AzureDWInitialLoad USING AzureSQLDWHWriter(
ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
)
INPUT FROM InitialLoadStream;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR;

 CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01;striim.test02;striim.test03',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',
 startPosition: 'striim.test01=1;striim.test02=-1;%=0',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream ;

  CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

create target AzureSQLDWHTarget using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream;


CREATE TARGET log USING LOGWRITER
(
 name:log,
 filename:'LOGFILENAME'
)
INPUT FROM data_stream;


END APPLICATION IR;

deploy application IR;
start IR;

STOP APPLICATION RouterTester.RouterApp;
UNDEPLOY APPLICATION RouterTester.RouterApp;
DROP APPLICATION RouterTester.RouterApp CASCADE;

--DROP namespace RouterTester;
--CREATE namespace RouterTester;
--USE RouterTester;

CREATE APPLICATION RouterApp RECOVERY 20 SECOND INTERVAL;

CREATE OR REPLACE SOURCE CsvDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'posdata100.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO InputStreamNonTyped;

CREATE TYPE MyDataType (
  colId String,
  colStr String,
  colInt int,
  colFloat float
);

CREATE OR REPLACE STREAM InputStreamTyped of MyDataType;

CREATE OR REPLACE CQ  CreateTypedStreamCQ
INSERT INTO InputStreamTyped (colId, colStr, colInt, colFloat)
SELECT
  TO_STRING(src.data[1]) as colId,
  TO_STRING(src.data[0]) as colStr,
  TO_INT(src.data[3]) as colInt,
  TO_FLOAT(src.data[7]) as colFloat
FROM InputStreamNonTyped src;

-- Router on the stream of Generated type. It tests the predicate with expressions on
-- integer, float and string types.
CREATE OR REPLACE ROUTER TestRouterTyped INPUT FROM InputStreamTyped CASE
WHEN colInt <= 3 and colFloat     < 100  and colStr like 'COMPANY%' THEN ROUTE TO StreamTyped1,
WHEN colInt <= 6 and colFloat * 3 < 200  and colStr like 'COMPANY%' THEN ROUTE TO StreamTyped2,
ELSE ROUTE TO StreamTypedElse;

-- Router on the stream of Non-generated Type. Predicates with expressions over
-- integer and float types.
CREATE OR REPLACE ROUTER TestRouterNonTyped INPUT FROM InputStreamNonTyped CASE
WHEN TO_INT(data[3]) <= 3 and TO_FLOAT(data[7])     < 100 and TO_STRING(data[0]) like 'COMPANY%' THEN ROUTE TO StreamNonTyped1,
WHEN TO_INT(data[3]) <= 6 and TO_FLOAT(data[7]) * 3 < 200 and TO_STRING(data[0]) like 'COMPANY%' THEN ROUTE TO StreamNonTyped2,
ELSE ROUTE TO StreamNonTypedElse;

-- Output Streams to populate the waction stores (for non-typed stream only).
CREATE OR REPLACE STREAM StreamWactions1 of MyDataType;
CREATE OR REPLACE STREAM StreamWactions2 of MyDataType;
CREATE OR REPLACE STREAM StreamWactionsElse of MyDataType;

CREATE OR REPLACE CQ ConvertCQNonTyped1
INSERT INTO StreamWactions1 (colId, colStr, colInt, colFloat)
SELECT
  TO_STRING(src.data[1]) as colId,
  TO_STRING(src.data[0]) as colStr,
  TO_INT(src.data[3]) as colInt,
  TO_FLOAT(src.data[7]) * 3 as colFloat
FROM StreamNonTyped1 src;

CREATE OR REPLACE CQ ConvertCQNonTyped2
INSERT INTO StreamWactions2 (colId, colStr, colInt, colFloat)
SELECT
  TO_STRING(src.data[1]) as colId,
  TO_STRING(src.data[0]) as colStr,
  TO_INT(src.data[3]) as colInt,
  TO_FLOAT(src.data[7]) * 3 as colFloat
FROM StreamNonTyped2 src;

CREATE OR REPLACE CQ ConvertCQNonTypedElse
INSERT INTO StreamWactionsElse (colId, colStr, colInt, colFloat)
SELECT
  TO_STRING(src.data[1]) as colId,
  TO_STRING(src.data[0]) as colStr,
  TO_INT(src.data[3]) as colInt,
  TO_FLOAT(src.data[7]) * 3 as colFloat
FROM StreamNonTypedElse src;

CREATE TYPE WactionsTypedType1 (
  colId  String,
  colStr String,
  colInt int,
  colFloat float
);

CREATE TYPE WactionsTypedType2 (
  colId  String,
  colStr String,
  colInt int,
  colFloat float
);

CREATE TYPE WactionsTypedTypeElse (
  colId  String,
  colStr String,
  colInt int,
  colFloat float
);

CREATE TYPE WactionsNonTypedType1 (
  colId  String,
  colStr String,
  colInt int,
  colFloat float
);

CREATE TYPE WactionsNonTypedType2 (
  colId  String,
  colStr String,
  colInt int,
  colFloat float
);

CREATE TYPE WactionsNonTypedTypeElse (
  colId  String,
  colStr String,
  colInt int,
  colFloat float
);


CREATE WACTIONSTORE WactionsTyped1 CONTEXT OF WactionsTypedType1
EVENT TYPES ( WactionsTypedType1 )
@PERSIST-TYPE@;

CREATE WACTIONSTORE WactionsTyped2 CONTEXT OF WactionsTypedType2
EVENT TYPES ( WactionsTypedType2 )
@PERSIST-TYPE@;

CREATE WACTIONSTORE WactionsTypedElse CONTEXT OF WactionsTypedTypeElse
EVENT TYPES ( WactionsTypedTypeElse )
@PERSIST-TYPE@;

CREATE WACTIONSTORE WactionsNonTyped1 CONTEXT OF WactionsNonTypedType1
EVENT TYPES ( WactionsNonTypedType1 )
@PERSIST-TYPE@;

CREATE WACTIONSTORE WactionsNonTyped2 CONTEXT OF WactionsNonTypedType2
EVENT TYPES ( WactionsNonTypedType2 )
@PERSIST-TYPE@;

CREATE WACTIONSTORE WactionsNonTypedElse CONTEXT OF WactionsNonTypedTypeElse
EVENT TYPES ( WactionsNonTypedTypeElse )
@PERSIST-TYPE@;

CREATE CQ InsertWactionsTyped1
INSERT INTO WactionsTyped1
SELECT *
FROM StreamTyped1;

CREATE CQ InsertWactionsTyped2
INSERT INTO WactionsTyped2
SELECT *
FROM StreamTyped2;

CREATE CQ InsertWactionsTypedElse
INSERT INTO WactionsTypedElse
SELECT *
FROM StreamTypedElse;

CREATE CQ InsertWactionsNonTyped1
INSERT INTO WactionsNonTyped1
SELECT *
FROM StreamWactions1;

CREATE CQ InsertWactionsNonTyped2
INSERT INTO WactionsNonTyped2
SELECT *
FROM StreamWactions2;

CREATE CQ InsertWactionsNonTypedElse
INSERT INTO WactionsNonTypedElse
SELECT *
FROM StreamWactionsElse;

END APPLICATION RouterApp;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[0]) = '1' ;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

--
-- Recovery Test 26 with two sources, two jumping attribute windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W -> CQ1 -> WS
-- S2 -> Ja6W -> CQ2 -> WS
--

STOP Recov26Tester.RecovTest26;
UNDEPLOY APPLICATION Recov26Tester.RecovTest26;
DROP APPLICATION Recov26Tester.RecovTest26 CASCADE;
CREATE APPLICATION RecovTest26 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION RecovTest26;

create Target @TARGET_NAME@ using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:50'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@',
	members:'Table=@metadata(TableName),OpName=@metadata(OperationName)'
)
input from @STREAM@;

STOP APPLICATION @WRITERAPPNAME@;
UNDEPLOY APPLICATION @WRITERAPPNAME@;
DROP APPLICATION @WRITERAPPNAME@ CASCADE;
create or replace PROPERTYVARIABLE SRC_PASSWORD='@SOURCE_PASS@';
CREATE APPLICATION @WRITERAPPNAME@ RECOVERY 10 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE SOURCE @SOURCE@ USING OracleReader
(
FetchSize:1,
Username:'@SOURCE_USER@',
Password:'$SRC_PASSWORD',
ConnectionURL:'@CONNECTION_URL@',
Tables:'@SOURCE_TABLE@',
password_encrypted: 'true'
)
OUTPUT TO @STREAM1@;

CREATE OR REPLACE TYPE @TYPE@( 
datae java.util.HashMap , 
TABLE_NAME java.lang.String , 
OPS_NAME java.lang.String , 
DB_TIMESTAMP java.lang.String  ,
COMMITSCN java.lang.String ,
SCN java.lang.String ,
REC_INS_TIME java.lang.String );

CREATE CQ @CQ1@
INSERT INTO @STREAM2@
SELECT  
CASE WHEN (META(c,"OperationName").toString() == "DELETE")
THEN putUserData(c, 'isDelete', 'true') 
ELSE
putUserData(c,'isDelete', 'false')
END
FROM @STREAM1@ c;

CREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;

CREATE OR REPLACE CQ @CQ2@ 
INSERT INTO @STREAM3@
SELECT 
data(e),
META(e,"TableName").toString() as TABLE_NAME,
META(e, "OperationName").toString() as OPS_NAME,
META(e, "TimeStamp").toString() as DB_TIMESTAMP,
META(e,"COMMITSCN").toString() as COMMITSCN ,
META(e,"SCN").toString() as  SCN ,
DNOW().toString() as REC_INS_TIME
FROM @STREAM1@ e;

create Target @TARGET1@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC1',
ParallelThreads:'',
PartitionKey:'@metadata(TableName)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from @STREAM1@;


create Target @TARGET2@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC2',
ParallelThreads:'2',
PartitionKey:'TABLE_NAME',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from @STREAM3@;

create Target @TARGET3@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC3',
ParallelThreads:'',
PartitionKey:'@userdata(isDelete)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest.avsc')
input from @STREAM2@;

end application @WRITERAPPNAME@;
deploy application @WRITERAPPNAME@;
start @WRITERAPPNAME@;
stop application @READERAPPNAME@;
undeploy application @READERAPPNAME@;
drop application @READERAPPNAME@ cascade;
CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC1',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;


CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
filename:'@READERAPPNAME@_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream1;

CREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC2',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;


CREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC3',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest.avsc'
)
OUTPUT TO KafkaReaderStream3;


end application @READERAPPNAME@;

create or replace propertyvariable TEAMSREFTOKEN = '@refershtoken@';
 create or replace propertyvariable TEAMSCLIENTSECRET = '@clientSecret@';
 create or replace propertyvariable TEAMSCHANNELURL = '@channelUrl@';
 create or replace propertyvariable TEAMSCLIENTID = '@clientID@';

CREATE APPLICATION @AppName@ WITH ENCRYPTION EXCEPTIONHANDLER (AdapterException: 'IGNORE', ArithmeticException: 'IGNORE', ClassCastException: 'IGNORE', ConnectionException: 'IGNORE', InvalidDataException: 'IGNORE', NullPointerException: 'IGNORE', NumberFormatException: 'IGNORE', SystemException: 'IGNORE', UnExpectedDDLException: 'IGNORE', UnknownException: 'IGNORE')  USE EXCEPTIONSTORE TTL : '7d' ;

CREATE OR REPLACE SOURCE OracleReader_AlertSource USING oraclereader (
  ConnectionURL: '@ConnectionURl@',
  Tables: '@table@',
  Password: '@password@',
  Username: '@username@' )
OUTPUT TO AlertUpgradeStream;

CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE STREAM SlackAndTeamsStream OF Global.SlackAlertEvent;

CREATE OR REPLACE TARGET SmartAlertUpgradeSysOut USING SysOut (
  name: 'SmartAlertUpgradeSysOut' )
INPUT FROM AlertUpgradeStream;

CREATE OR REPLACE CQ GenerateAlertsForWebAndEmail
INSERT INTO AlertStream
SELECT data[1],
 data[1],
 'info',
 'raise',
'Welcome\n to Striim'
FROM AlertUpgradeStream;


 CREATE OR REPLACE CQ GenerateAlertsForSlackAndTeams
 INSERT INTO SlackAndTeamsStream
 SELECT data[1],
  data[1],
 'info',
 'raise',
 'Welcome\n to Striim',
 'TestChannel'
 FROM AlertUpgradeStream;


CREATE TARGET TeamsAlertSender USING Global.TeamsAlertAdapter (
  refreshToken: '$TEAMSREFTOKEN',
  clientSecret: '$TEAMSCLIENTSECRET',
  channelURL: '$TEAMSCHANNELURL',
  clientID: '$TEAMSCLIENTID' )
INPUT FROM SlackAndTeamsStream;


CREATE SUBSCRIPTION WebAlertSender USING WebAlertAdapter (
  isSubscription: 'true',
  channelName: 'admin_PosAppWebAlert' )
INPUT FROM AlertStream;


CREATE TARGET slackalertSender USING Global.SlackAlertAdapter (
  OauthToken: '@oauth@',
  ChannelName: '@slackChannel@',
  OauthToken_encrypted: 'false' )
INPUT FROM SlackAndTeamsStream;

CREATE SUBSCRIPTION EmailAlertsender USING EmailAdapter (
  smtpurl: '@smtpUrl@',
  starttls_enable: '@starttls@',
  SMTPUSER: '@smtpuser@',
  SMTPPASSWORD: '@stmpPwsd@',
  emailList: '@emailList@',
  subject: '@subject@',
  senderEmail: '@senderEmail@',
  SMTPPASSWORD_encrypted: 'false')
INPUT FROM AlertStream;

END APPLICATION @AppName@;

stop application BigqueryBulkLoadMonMetrics;
undeploy application BigqueryBulkLoadMonMetrics;
drop application BigqueryBulkLoadMonMetrics cascade;

CREATE APPLICATION BigqueryBulkLoadMonMetrics;

CREATE FLOW BigqueryBulkLoadMonMetrics_SourceFlow;

CREATE SOURCE BigqueryBulkLoadMonMetrics_DBSource USING DatabaseReader ( 
  Username: 'qatest', 
  DatabaseProviderType: 'ORACLE', 
  FetchSize: 10000, 
  Password_encrypted: 'false', 
  QuiesceOnILCompletion: 'true', 
  Password: 'JVaLv3ZpgQDY8R2ZxS38xg==', 
  Tables: 'QATEST.EMPLOYEE', 
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe' ) 
OUTPUT TO BigqueryBulkLoadMonMetrics_OutputStream;

END FLOW BigqueryBulkLoadMonMetrics_SourceFlow;

CREATE OR REPLACE TARGET BigqueryBulkLoadMonMetrics_BigQueryTarget1 USING BigQueryWriter ( 
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  streamingUpload: 'false', 
  projectId: 'striimqa-214712', 
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:1000000, Interval:90', 
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30', 
  AllowQuotedNewLines: 'false', 
  optimizedMerge: 'false', 
  TransportOptions: 'connectionTimeout=300, readTimeout=120', 
  adapterName: 'BigQueryWriter', 
  Mode: 'MERGE', 
  Tables: 'QATEST.EMPLOYEE,DEV22862jen.sample', 
  StandardSQL: 'true', 
  includeInsertId: 'true', 
  QuoteCharacter: '\"', 
  ServiceAccountKey: '/Users/jenniffer/Product2/IntegrationTests/TestData/google-gcs.json' ) 
INPUT FROM BigqueryBulkLoadMonMetrics_OutputStream;

END APPLICATION BigqueryBulkLoadMonMetrics;

deploy application BigqueryBulkLoadMonMetrics;
start application BigqueryBulkLoadMonMetrics;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ recovery 5 second Interval;

CREATE SOURCE @srcName@ USING Global.S3Reader ( 
 secretaccesskey: '@secretaccesskey@', 
  foldername: '@sourcefoldername@', 
  blocksize: 64, 
  bucketname: '@bucketname@', 
  objectnameprefix: '@sourcefilename@', 
  accesskeyid: '@accesskeyid@' ) 
PARSE USING Global.DSVParser ( 
  trimwhitespace: false, 
  commentcharacter: '', 
  linenumber: '-1', 
  columndelimiter: ',', 
  trimquote: true, 
  columndelimittill: '-1', 
  ignoreemptycolumn: false, 
  separator: ':', 
  quoteset: '\"', 
  charset: 'UTF-8', 
  ignoremultiplerecordbegin: 'true', 
  ignorerowdelimiterinquote: false, 
  header: false, 
  blockascompleterecord: false, 
  rowdelimiter: '\n', 
  nocolumndelimiter: false, 
  headerlineno: 0 ) 
OUTPUT TO @outstreamname@;

CREATE OR REPLACE TARGET @tgtName@ USING Global.S3Writer ( 
  UploadConfigValueSeparator: '=', 
  UploadConfigPropertySeparator: ',', 
  objectname: '@targetfilename@', 
  secretaccesskey: '@secretaccesskey@', 
  ParallelThreads: '', 
  rolloveronddl: 'true', 
  bucketname: '@bucketname@', 
  foldername: '@targetfoldername@', 
  uploadpolicy: 'eventcount:5,interval:60s', 
  accesskeyid: '@accesskeyid@' ) 
FORMAT USING Global.DSVFormatter  ( 
  quotecharacter: '\"', 
  columndelimiter: ',', 
  members: 'data', 
  nullvalue: 'NULL', 
  usequotes: 'false', 
  rowdelimiter: '\n', 
  standard: 'none', 
  header: 'false' ) 
INPUT FROM @instreamname@;
End APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

--
-- Recovery Test 1
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--


UNDEPLOY APPLICATION NameT00.T00;
DROP APPLICATION NameT00.T00 CASCADE;
CREATE APPLICATION T00 RECOVERY 5 SECOND INTERVAL;




CREATE FLOW DataAcquisitionT00;


CREATE SOURCE CsvSourceT00 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO OutputStreamT00;


END FLOW DataAcquisitionT00;




CREATE FLOW DataProcessingT00;


Create Target OutputTargetT00
Using Sysout (name: 'OutputTargetT00')
Input From OutputStreamT00;


END FLOW DataProcessingT00;



END APPLICATION T00;

STOP APPLICATION DBRTOCW;
UNDEPLOY APPLICATION DBRTOCW;
DROP APPLICATION DBRTOCW CASCADE;
CREATE APPLICATION DBRTOCW;

create source CSVSource using FileReader (
	directory:'/Users/jenniffer/Product2/IntegrationTests/TestData/',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	
)
OUTPUT TO FileStream
(
id String,
ename String
)
select 
data[2],
data[0];

create Target t2 using SysOut(name:OrgData) input from FileStream;
CREATE OR REPLACE Target DBTarget USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1000,Interval:60',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  Tables: 'test.employee',
  Password: 'cassandra',
  Password_encrypted: false
 )INPUT FROM FileStream;

END APPLICATION DBRTOCW;
DEPLOY APPLICATION DBRTOCW;
START APPLICATION DBRTOCW;

--
-- Canon Test W50
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for a partitioned sliding count window
--
-- S -> SWc5p -> CQ -> WS
--


UNDEPLOY APPLICATION NameW50.W50;
DROP APPLICATION NameW50.W50 CASCADE;
CREATE APPLICATION W50 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW50;

CREATE SOURCE CsvSourceW50 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW50;

END FLOW DataAcquisitionW50;


CREATE FLOW DataProcessingW50;

CREATE TYPE DataTypeW50 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW50 OF DataTypeW50;

CREATE CQ CSVStreamW50_to_DataStreamW50
INSERT INTO DataStreamW50
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW50;

CREATE WINDOW SWc5pW50
OVER DataStreamW50
KEEP 5 ROWS
PARTITION BY word;

CREATE WACTIONSTORE WactionStoreW50 CONTEXT OF DataTypeW50
EVENT TYPES ( DataTypeW50 KEY(word) )
@PERSIST-TYPE@

CREATE CQ SWc5pW50_to_WactionStoreW50
INSERT INTO WactionStoreW50
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM SWc5pW50
GROUP BY word;

END FLOW DataProcessingW50;



END APPLICATION W50;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;
CREATE OR REPLACE SOURCE @SourceName@ Using OJet
(
  Username: '@Username@',
  Password: '@Password@',
  ConnectionURL: '@ConnectionURL@',
  connectionRetryPolicy: @ConnectionRetryPolicy@,
  Tables: '@SourceTables@',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  Password_encrypted: 'false',
  CommittedTransactions: true,
  adapterName: 'OJet',
)OUTPUT TO @SRCINPUTSTREAM@;
create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadpolicy:'EventCount:7'
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from @STREAM@;
end application @APPNAME@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'JsonTargetEC',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'eventcount:2000,sequence:00'
)
format using JSONFormatter (
  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)

input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetJsonECBig_actual.log') input from TypedCSVStream;

end application DSV;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;

Create Source @SourceName@ Using Ojet

(
  Username:'c##qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',
  Tables:'CDB$ROOT."C##QATEST".ojet_src;ORCLPDB.QATEST.ojet_src',
  _h_useClassic:false,
  Fetchsize:1,
  Compression: true,
  SupportPDB:true,
  ReplicationSlotName:'null'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',
  Username:'c##qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'CDB$ROOT."C##QATEST".ojet_src,CDB$ROOT."C##QATEST".ojet_tgt'
) INPUT FROM @SRCINPUTSTREAM@;

CREATE TARGET @targetName1@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orclpdb',
  Username:'qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'ORCLPDB.QATEST.ojet_src,ORCLPDB.QATEST.ojet_tgt'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

--
-- Recovery Test 8
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov8Tester.RecovTest8;
UNDEPLOY APPLICATION Recov8Tester.RecovTest8;
DROP APPLICATION Recov8Tester.RecovTest8 CASCADE;
CREATE APPLICATION RecovTest8 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END APPLICATION RecovTest8;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW ;

CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: 'QATEST.orac_1000COL',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:60',
  CommitPolicy: 'EventCount:1,Interval:60',
  ConnectionURL: '@TARGET_URL@',
  Tables: 'QATEST.orac_1000COL,test.cassandra_1500col columnmap(field1=f1,field2=f2,field3=f3,field4=f4,field5=f5,field6=f6,field7=f7,field8=f8,field9=f9,field10=f10,field11=f11,field12=f12,field13=f13,field14=f14,field15=f15,field16=f16,field17=f17,field18=f18,field19=f19,field20=f20,field21=f21,field22=f22,field23=f23,field24=f24,field25=f25,field26=f26,field27=f27,field28=f28,field29=f29,field30=f30,field31=f31,field32=f32,field33=f33,field34=f34,field35=f35,field36=f36,field37=f37,field38=f38,field39=f39,field40=f40,field41=f41,field42=f42,field43=f43,field44=f44,field45=f45,field46=f46,field47=f47,field48=f48,field49=f49,field50=f50,field51=f51,field52=f52,field53=f53,field54=f54,field55=f55,field56=f56,field57=f57,field58=f58,field59=f59,field60=f60,field61=f61,field62=f62,field63=f63,field64=f64,field65=f65,field66=f66,field67=f67,field68=f68,field69=f69,field70=f70,field71=f71,field72=f72,field73=f73,field74=f74,field75=f75,field76=f76,field77=f77,field78=f78,field79=f79,field80=f80,field81=f81,field82=f82,field83=f83,field84=f84,field85=f85,field86=f86,field87=f87,field88=f88,field89=f89,field90=f90,field91=f91,field92=f92,field93=f93,field94=f94,field95=f95,field96=f96,field97=f97,field98=f98,field99=f99,field100=f100,field101=f101,field102=f102,field103=f103,field104=f104,field105=f105,field106=f106,field107=f107,field108=f108,field109=f109,field110=f110,field111=f111,field112=f112,field113=f113,field114=f114,field115=f115,field116=f116,field117=f117,field118=f118,field119=f119,field120=f120,field121=f121,field122=f122,field123=f123,field124=f124,field125=f125,field126=f126,field127=f127,field128=f128,field129=f129,field130=f130,field131=f131,field132=f132,field133=f133,field134=f134,field135=f135,field136=f136,field137=f137,field138=f138,field139=f139,field140=f140,field141=f141,field142=f142,field143=f143,field144=f144,field145=f145,field146=f146,field147=f147,field148=f148,field149=f149,field150=f150,field151=f151,field152=f152,field153=f153,field154=f154,field155=f155,field156=f156,field157=f157,field158=f158,field159=f159,field160=f160,field161=f161,field162=f162,field163=f163,field164=f164,field165=f165,field166=f166,field167=f167,field168=f168,field169=f169,field170=f170,field171=f171,field172=f172,field173=f173,field174=f174,field175=f175,field176=f176,field177=f177,field178=f178,field179=f179,field180=f180,field181=f181,field182=f182,field183=f183,field184=f184,field185=f185,field186=f186,field187=f187,field188=f188,field189=f189,field190=f190,field191=f191,field192=f192,field193=f193,field194=f194,field195=f195,field196=f196,field197=f197,field198=f198,field199=f199,field200=f200,field201=f201,field202=f202,field203=f203,field204=f204,field205=f205,field206=f206,field207=f207,field208=f208,field209=f209,field210=f210,field211=f211,field212=f212,field213=f213,field214=f214,field215=f215,field216=f216,field217=f217,field218=f218,field219=f219,field220=f220,field221=f221,field222=f222,field223=f223,field224=f224,field225=f225,field226=f226,field227=f227,field228=f228,field229=f229,field230=f230,field231=f231,field232=f232,field233=f233,field234=f234,field235=f235,field236=f236,field237=f237,field238=f238,field239=f239,field240=f240,field241=f241,field242=f242,field243=f243,field244=f244,field245=f245,field246=f246,field247=f247,field248=f248,field249=f249,field250=f250,field251=f251,field252=f252,field253=f253,field254=f254,field255=f255,field256=f256,field257=f257,field258=f258,field259=f259,field260=f260,field261=f261,field262=f262,field263=f263,field264=f264,field265=f265,field266=f266,field267=f267,field268=f268,field269=f269,field270=f270,field271=f271,field272=f272,field273=f273,field274=f274,field275=f275,field276=f276,field277=f277,field278=f278,field279=f279,field280=f280,field281=f281,field282=f282,field283=f283,field284=f284,field285=f285,field286=f286,field287=f287,field288=f288,field289=f289,field290=f290,field291=f291,field292=f292,field293=f293,field294=f294,field295=f295,field296=f296,field297=f297,field298=f298,field299=f299,field300=f300,field301=f301,field302=f302,field303=f303,field304=f304,field305=f305,field306=f306,field307=f307,field308=f308,field309=f309,field310=f310,field311=f311,field312=f312,field313=f313,field314=f314,field315=f315,field316=f316,field317=f317,field318=f318,field319=f319,field320=f320,field321=f321,field322=f322,field323=f323,field324=f324,field325=f325,field326=f326,field327=f327,field328=f328,field329=f329,field330=f330,field331=f331,field332=f332,field333=f333,field334=f334,field335=f335,field336=f336,field337=f337,field338=f338,field339=f339,field340=f340,field341=f341,field342=f342,field343=f343,field344=f344,field345=f345,field346=f346,field347=f347,field348=f348,field349=f349,field350=f350,field351=f351,field352=f352,field353=f353,field354=f354,field355=f355,field356=f356,field357=f357,field358=f358,field359=f359,field360=f360,field361=f361,field362=f362,field363=f363,field364=f364,field365=f365,field366=f366,field367=f367,field368=f368,field369=f369,field370=f370,field371=f371,field372=f372,field373=f373,field374=f374,field375=f375,field376=f376,field377=f377,field378=f378,field379=f379,field380=f380,field381=f381,field382=f382,field383=f383,field384=f384,field385=f385,field386=f386,field387=f387,field388=f388,field389=f389,field390=f390,field391=f391,field392=f392,field393=f393,field394=f394,field395=f395,field396=f396,field397=f397,field398=f398,field399=f399,field400=f400,field401=f401,field402=f402,field403=f403,field404=f404,field405=f405,field406=f406,field407=f407,field408=f408,field409=f409,field410=f410,field411=f411,field412=f412,field413=f413,field414=f414,field415=f415,field416=f416,field417=f417,field418=f418,field419=f419,field420=f420,field421=f421,field422=f422,field423=f423,field424=f424,field425=f425,field426=f426,field427=f427,field428=f428,field429=f429,field430=f430,field431=f431,field432=f432,field433=f433,field434=f434,field435=f435,field436=f436,field437=f437,field438=f438,field439=f439,field440=f440,field441=f441,field442=f442,field443=f443,field444=f444,field445=f445,field446=f446,field447=f447,field448=f448,field449=f449,field450=f450,field451=f451,field452=f452,field453=f453,field454=f454,field455=f455,field456=f456,field457=f457,field458=f458,field459=f459,field460=f460,field461=f461,field462=f462,field463=f463,field464=f464,field465=f465,field466=f466,field467=f467,field468=f468,field469=f469,field470=f470,field471=f471,field472=f472,field473=f473,field474=f474,field475=f475,field476=f476,field477=f477,field478=f478,field479=f479,field480=f480,field481=f481,field482=f482,field483=f483,field484=f484,field485=f485,field486=f486,field487=f487,field488=f488,field489=f489,field490=f490,field491=f491,field492=f492,field493=f493,field494=f494,field495=f495,field496=f496,field497=f497,field498=f498,field499=f499,field500=f500,field501=f501,field502=f502,field503=f503,field504=f504,field505=f505,field506=f506,field507=f507,field508=f508,field509=f509,field510=f510,field511=f511,field512=f512,field513=f513,field514=f514,field515=f515,field516=f516,field517=f517,field518=f518,field519=f519,field520=f520,field521=f521,field522=f522,field523=f523,field524=f524,field525=f525,field526=f526,field527=f527,field528=f528,field529=f529,field530=f530,field531=f531,field532=f532,field533=f533,field534=f534,field535=f535,field536=f536,field537=f537,field538=f538,field539=f539,field540=f540,field541=f541,field542=f542,field543=f543,field544=f544,field545=f545,field546=f546,field547=f547,field548=f548,field549=f549,field550=f550,field551=f551,field552=f552,field553=f553,field554=f554,field555=f555,field556=f556,field557=f557,field558=f558,field559=f559,field560=f560,field561=f561,field562=f562,field563=f563,field564=f564,field565=f565,field566=f566,field567=f567,field568=f568,field569=f569,field570=f570,field571=f571,field572=f572,field573=f573,field574=f574,field575=f575,field576=f576,field577=f577,field578=f578,field579=f579,field580=f580,field581=f581,field582=f582,field583=f583,field584=f584,field585=f585,field586=f586,field587=f587,field588=f588,field589=f589,field590=f590,field591=f591,field592=f592,field593=f593,field594=f594,field595=f595,field596=f596,field597=f597,field598=f598,field599=f599,field600=f600,field601=f601,field602=f602,field603=f603,field604=f604,field605=f605,field606=f606,field607=f607,field608=f608,field609=f609,field610=f610,field611=f611,field612=f612,field613=f613,field614=f614,field615=f615,field616=f616,field617=f617,field618=f618,field619=f619,field620=f620,field621=f621,field622=f622,field623=f623,field624=f624,field625=f625,field626=f626,field627=f627,field628=f628,field629=f629,field630=f630,field631=f631,field632=f632,field633=f633,field634=f634,field635=f635,field636=f636,field637=f637,field638=f638,field639=f639,field640=f640,field641=f641,field642=f642,field643=f643,field644=f644,field645=f645,field646=f646,field647=f647,field648=f648,field649=f649,field650=f650,field651=f651,field652=f652,field653=f653,field654=f654,field655=f655,field656=f656,field657=f657,field658=f658,field659=f659,field660=f660,field661=f661,field662=f662,field663=f663,field664=f664,field665=f665,field666=f666,field667=f667,field668=f668,field669=f669,field670=f670,field671=f671,field672=f672,field673=f673,field674=f674,field675=f675,field676=f676,field677=f677,field678=f678,field679=f679,field680=f680,field681=f681,field682=f682,field683=f683,field684=f684,field685=f685,field686=f686,field687=f687,field688=f688,field689=f689,field690=f690,field691=f691,field692=f692,field693=f693,field694=f694,field695=f695,field696=f696,field697=f697,field698=f698,field699=f699,field700=f700,field701=f701,field702=f702,field703=f703,field704=f704,field705=f705,field706=f706,field707=f707,field708=f708,field709=f709,field710=f710,field711=f711,field712=f712,field713=f713,field714=f714,field715=f715,field716=f716,field717=f717,field718=f718,field719=f719,field720=f720,field721=f721,field722=f722,field723=f723,field724=f724,field725=f725,field726=f726,field727=f727,field728=f728,field729=f729,field730=f730,field731=f731,field732=f732,field733=f733,field734=f734,field735=f735,field736=f736,field737=f737,field738=f738,field739=f739,field740=f740,field741=f741,field742=f742,field743=f743,field744=f744,field745=f745,field746=f746,field747=f747,field748=f748,field749=f749,field750=f750,field751=f751,field752=f752,field753=f753,field754=f754,field755=f755,field756=f756,field757=f757,field758=f758,field759=f759,field760=f760,field761=f761,field762=f762,field763=f763,field764=f764,field765=f765,field766=f766,field767=f767,field768=f768,field769=f769,field770=f770,field771=f771,field772=f772,field773=f773,field774=f774,field775=f775,field776=f776,field777=f777,field778=f778,field779=f779,field780=f780,field781=f781,field782=f782,field783=f783,field784=f784,field785=f785,field786=f786,field787=f787,field788=f788,field789=f789,field790=f790,field791=f791,field792=f792,field793=f793,field794=f794,field795=f795,field796=f796,field797=f797,field798=f798,field799=f799,field800=f800,field801=f801,field802=f802,field803=f803,field804=f804,field805=f805,field806=f806,field807=f807,field808=f808,field809=f809,field810=f810,field811=f811,field812=f812,field813=f813,field814=f814,field815=f815,field816=f816,field817=f817,field818=f818,field819=f819,field820=f820,field821=f821,field822=f822,field823=f823,field824=f824,field825=f825,field826=f826,field827=f827,field828=f828,field829=f829,field830=f830,field831=f831,field832=f832,field833=f833,field834=f834,field835=f835,field836=f836,field837=f837,field838=f838,field839=f839,field840=f840,field841=f841,field842=f842,field843=f843,field844=f844,field845=f845,field846=f846,field847=f847,field848=f848,field849=f849,field850=f850,field851=f851,field852=f852,field853=f853,field854=f854,field855=f855,field856=f856,field857=f857,field858=f858,field859=f859,field860=f860,field861=f861,field862=f862,field863=f863,field864=f864,field865=f865,field866=f866,field867=f867,field868=f868,field869=f869,field870=f870,field871=f871,field872=f872,field873=f873,field874=f874,field875=f875,field876=f876,field877=f877,field878=f878,field879=f879,field880=f880,field881=f881,field882=f882,field883=f883,field884=f884,field885=f885,field886=f886,field887=f887,field888=f888,field889=f889,field890=f890,field891=f891,field892=f892,field893=f893,field894=f894,field895=f895,field896=f896,field897=f897,field898=f898,field899=f899,field900=f900,field901=f901,field902=f902,field903=f903,field904=f904,field905=f905,field906=f906,field907=f907,field908=f908,field909=f909,field910=f910,field911=f911,field912=f912,field913=f913,field914=f914,field915=f915,field916=f916,field917=f917,field918=f918,field919=f919,field920=f920,field921=f921,field922=f922,field923=f923,field924=f924,field925=f925,field926=f926,field927=f927,field928=f928,field929=f929,field930=f930,field931=f931,field932=f932,field933=f933,field934=f934,field935=f935,field936=f936,field937=f937,field938=f938,field939=f939,field940=f940,field941=f941,field942=f942,field943=f943,field944=f944,field945=f945,field946=f946,field947=f947,field948=f948,field949=f949,field950=f950,field951=f951,field952=f952,field953=f953,field954=f954,field955=f955,field956=f956,field957=f957,field958=f958,field959=f959,field960=f960,field961=f961,field962=f962,field963=f963,field964=f964,field965=f965,field966=f966,field967=f967,field968=f968,field969=f969,field970=f970,field971=f971,field972=f972,field973=f973,field974=f974,field975=f975,field976=f976,field977=f977,field978=f978,field979=f979,field980=f980,field981=f981,field982=f982,field983=f983,field984=f984,field985=f985,field986=f986,field987=f987,field988=f988,field989=f989,field990=f990,field991=f991,field992=f992,field993=f993,field994=f994,field995=f995,field996=f996,field997=f997,field998=f998,field999=f999,field1000=f1000,field1001=f501,field1002=f2,field1003=f3,field1004=f4,field1005=f5,field1006=f6,field1007=f7,field1008=f8,field1009=f9,field1010=f10,field1011=f11,field1012=f12,field1013=f13,field1014=f14,field1015=f15,field1016=f16,field1017=f17,field1018=f18,field1019=f19,field1020=f20,field1021=f21,field1022=f22,field1023=f23,field1024=f24,field1025=f25,field1026=f26,field1027=f27,field1028=f28,field1029=f29,field1030=f30,field1031=f31,field1032=f32,field1033=f33,field1034=f34,field1035=f35,field1036=f36,field1037=f37,field1038=f38,field1039=f39,field1040=f40,field1041=f41,field1042=f42,field1043=f43,field1044=f44,field1045=f45,field1046=f46,field1047=f47,field1048=f48,field1049=f49,field1050=f50,field1051=f51,field1052=f52,field1053=f53,field1054=f54,field1055=f55,field1056=f56,field1057=f57,field1058=f58,field1059=f59,field1060=f60,field1061=f61,field1062=f62,field1063=f63,field1064=f64,field1065=f65,field1066=f66,field1067=f67,field1068=f68,field1069=f69,field1070=f70,field1071=f71,field1072=f72,field1073=f73,field1074=f74,field1075=f75,field1076=f76,field1077=f77,field1078=f78,field1079=f79,field1080=f80,field1081=f81,field1082=f82,field1083=f83,field1084=f84,field1085=f85,field1086=f86,field1087=f87,field1088=f88,field1089=f89,field1090=f90,field1091=f91,field1092=f92,field1093=f93,field1094=f94,field1095=f95,field1096=f96,field1097=f97,field1098=f98,field1099=f99,field1100=f100,field1101=f101,field1102=f102,field1103=f103,field1104=f104,field1105=f105,field1106=f106,field1107=f107,field1108=f108,field1109=f109,field1110=f110,field1111=f111,field1112=f112,field1113=f113,field1114=f114,field1115=f115,field1116=f116,field1117=f117,field1118=f118,field1119=f119,field1120=f120,field1121=f121,field1122=f122,field1123=f123,field1124=f124,field1125=f125,field1126=f126,field1127=f127,field1128=f128,field1129=f129,field1130=f130,field1131=f131,field1132=f132,field1133=f133,field1134=f134,field1135=f135,field1136=f136,field1137=f137,field1138=f138,field1139=f139,field1140=f140,field1141=f141,field1142=f142,field1143=f143,field1144=f144,field1145=f145,field1146=f146,field1147=f147,field1148=f148,field1149=f149,field1150=f150,field1151=f151,field1152=f152,field1153=f153,field1154=f154,field1155=f155,field1156=f156,field1157=f157,field1158=f158,field1159=f159,field1160=f160,field1161=f161,field1162=f162,field1163=f163,field1164=f164,field1165=f165,field1166=f166,field1167=f167,field1168=f168,field1169=f169,field1170=f170,field1171=f171,field1172=f172,field1173=f173,field1174=f174,field1175=f175,field1176=f176,field1177=f177,field1178=f178,field1179=f179,field1180=f180,field1181=f181,field1182=f182,field1183=f183,field1184=f184,field1185=f185,field1186=f186,field1187=f187,field1188=f188,field1189=f189,field1190=f190,field1191=f191,field1192=f192,field1193=f193,field1194=f194,field1195=f195,field1196=f196,field1197=f197,field1198=f198,field1199=f199,field1200=f200,field1201=f201,field1202=f202,field1203=f203,field1204=f204,field1205=f205,field1206=f206,field1207=f207,field1208=f208,field1209=f209,field1210=f210,field1211=f211,field1212=f212,field1213=f213,field1214=f214,field1215=f215,field1216=f216,field1217=f217,field1218=f218,field1219=f219,field1220=f220,field1221=f221,field1222=f222,field1223=f223,field1224=f224,field1225=f225,field1226=f226,field1227=f227,field1228=f228,field1229=f229,field1230=f230,field1231=f231,field1232=f232,field1233=f233,field1234=f234,field1235=f235,field1236=f236,field1237=f237,field1238=f238,field1239=f239,field1240=f240,field1241=f241,field1242=f242,field1243=f243,field1244=f244,field1245=f245,field1246=f246,field1247=f247,field1248=f248,field1249=f249,field1250=f250,field1251=f251,field1252=f252,field1253=f253,field1254=f254,field1255=f255,field1256=f256,field1257=f257,field1258=f258,field1259=f259,field1260=f260,field1261=f261,field1262=f262,field1263=f263,field1264=f264,field1265=f265,field1266=f266,field1267=f267,field1268=f268,field1269=f269,field1270=f270,field1271=f271,field1272=f272,field1273=f273,field1274=f274,field1275=f275,field1276=f276,field1277=f277,field1278=f278,field1279=f279,field1280=f280,field1281=f281,field1282=f282,field1283=f283,field1284=f284,field1285=f285,field1286=f286,field1287=f287,field1288=f288,field1289=f289,field1290=f290,field1291=f291,field1292=f292,field1293=f293,field1294=f294,field1295=f295,field1296=f296,field1297=f297,field1298=f298,field1299=f299,field1300=f300,field1301=f301,field1302=f302,field1303=f303,field1304=f304,field1305=f305,field1306=f306,field1307=f307,field1308=f308,field1309=f309,field1310=f310,field1311=f311,field1312=f312,field1313=f313,field1314=f314,field1315=f315,field1316=f316,field1317=f317,field1318=f318,field1319=f319,field1320=f320,field1321=f321,field1322=f322,field1323=f323,field1324=f324,field1325=f325,field1326=f326,field1327=f327,field1328=f328,field1329=f329,field1330=f330,field1331=f331,field1332=f332,field1333=f333,field1334=f334,field1335=f335,field1336=f336,field1337=f337,field1338=f338,field1339=f339,field1340=f340,field1341=f341,field1342=f342,field1343=f343,field1344=f344,field1345=f345,field1346=f346,field1347=f347,field1348=f348,field1349=f349,field1350=f350,field1351=f351,field1352=f352,field1353=f353,field1354=f354,field1355=f355,field1356=f356,field1357=f357,field1358=f358,field1359=f359,field1360=f360,field1361=f361,field1362=f362,field1363=f363,field1364=f364,field1365=f365,field1366=f366,field1367=f367,field1368=f368,field1369=f369,field1370=f370,field1371=f371,field1372=f372,field1373=f373,field1374=f374,field1375=f375,field1376=f376,field1377=f377,field1378=f378,field1379=f379,field1380=f380,field1381=f381,field1382=f382,field1383=f383,field1384=f384,field1385=f385,field1386=f386,field1387=f387,field1388=f388,field1389=f389,field1390=f390,field1391=f391,field1392=f392,field1393=f393,field1394=f394,field1395=f395,field1396=f396,field1397=f397,field1398=f398,field1399=f399,field1400=f400,field1401=f401,field1402=f402,field1403=f403,field1404=f404,field1405=f405,field1406=f406,field1407=f407,field1408=f408,field1409=f409,field1410=f410,field1411=f411,field1412=f412,field1413=f413,field1414=f414,field1415=f415,field1416=f416,field1417=f417,field1418=f418,field1419=f419,field1420=f420,field1421=f421,field1422=f422,field1423=f423,field1424=f424,field1425=f425,field1426=f426,field1427=f427,field1428=f428,field1429=f429,field1430=f430,field1431=f431,field1432=f432,field1433=f433,field1434=f434,field1435=f435,field1436=f436,field1437=f437,field1438=f438,field1439=f439,field1440=f440,field1441=f441,field1442=f442,field1443=f443,field1444=f444,field1445=f445,field1446=f446,field1447=f447,field1448=f448,field1449=f449,field1450=f450,field1451=f451,field1452=f452,field1453=f453,field1454=f454,field1455=f455,field1456=f456,field1457=f457,field1458=f458,field1459=f459,field1460=f460,field1461=f461,field1462=f462,field1463=f463,field1464=f464,field1465=f465,field1466=f466,field1467=f467,field1468=f468,field1469=f469,field1470=f470,field1471=f471,field1472=f472,field1473=f473,field1474=f474,field1475=f475,field1476=f476,field1477=f477,field1478=f478,field1479=f479,field1480=f480,field1481=f481,field1482=f482,field1483=f483,field1484=f484,field1485=f485,field1486=f486,field1487=f487,field1488=f488,field1489=f489,field1490=f490,field1491=f491,field1492=f492,field1493=f493,field1494=f494,field1495=f495,field1496=f496,field1497=f497,field1498=f498,field1499=f499,field1500=f500)',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

stop PatternMatchingTcp.CSV;
undeploy application PatternMatchingTcp.CSV;
drop application PatternMatchingTCp.CSV cascade;

create application CSV;

create source TCPSource using TCPReader
(
  IpAddress:'127.0.0.1',
  PortNo:'3549'
)

PARSE USING DSVParser
(
header:'false',
metadata:'@TEST-DATA-PATH@/ctest-TCP.csv',endian : false

)
OUTPUT TO TcpStream;

create Target t1 using SysOut(name:Typed1) input from TcpStream;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  TO_INT(data[0]) as UserId,
	    TO_INT(data[1]) as temp1,
        TO_DOUBLE(data[2]) as temp2,
	    TO_STRING(data[3]) as temp3
FROM TcpStream;

-- scenario 1.1 check pattern using timer within 10 seconds and wait
CREATE CQ TypeConversionTCPCQ1
INSERT INTO TypedStream1
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN T A (W | B | C)
define T = timer(interval 10 second),
A = UserDataStream(temp1 >= 20), B= UserDataStream(temp2 < 30.40), C= UserDataStream(temp3 = 'Bret'), W = wait(T)
PARTITION BY UserId;

-- scenario 1.2 check pattern using timer within 20 seconds
CREATE CQ TypeConversionTCPCQ2
INSERT INTO TypedStream2
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN T A C
define T = timer(interval 20 second), B= UserDataStream(temp2 < 30.40), C= UserDataStream(temp3 = 'zalak'),
A = UserDataStream(temp1 >= 20)
PARTITION BY UserId;

-- scenario 1.3 check pattern using timer within 5 seconds with between values
CREATE CQ TypeConversionTCPCQ3
INSERT INTO TypedStream3
SELECT UserId as typeduserid,
	   A.temp1 as typedtemp1
from UserDataStream
MATCH_PATTERN T A
define T = timer(interval 5 second),
A = UserDataStream(temp1 between 10 and 40)
PARTITION BY UserId;

-- scenario 1.4 check pattern using timer which match no events
CREATE CQ TypeConversionTCPCQ4
INSERT INTO TypedStream4
SELECT UserId as typeduserid
from UserDataStream
MATCH_PATTERN T W
define T = timer(interval 50 second), W = wait(T)
PARTITION BY UserId;

-- scenario 1.5 check pattern using stop timer
CREATE CQ TypeConversionTCPCQ5
INSERT INTO TypedStream5
SELECT UserId as typeduserid,
       A.temp1 as typedtemp1,
       B.temp2 as typedtemp2
from UserDataStream
MATCH_PATTERN T A C T2 B
define
T = timer(interval 50 second),
A = UserDataStream(temp1 between 10 and 40),
C = stoptimer(T),
T2 = timer(interval 30 second),
B = UserDataStream(temp2 >= 20)
PARTITION BY UserId;

CREATE WACTIONSTORE UserActivityInfoTcp1
CONTEXT OF TypedStream1_Type
EVENT TYPES ( TypedStream1_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTcp2
CONTEXT OF TypedStream2_Type
EVENT TYPES ( TypedStream2_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTcp3
CONTEXT OF TypedStream3_Type
EVENT TYPES ( TypedStream3_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTcp4
CONTEXT OF TypedStream4_Type
EVENT TYPES ( TypedStream4_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTcp5
CONTEXT OF TypedStream5_Type
EVENT TYPES ( TypedStream5_Type )
@PERSIST-TYPE@

create Target t2 using SysOut(name:Typed2) input from TypedStream1;

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction1
INSERT INTO UserActivityInfoTcp1
SELECT * FROM TypedStream1
LINK SOURCE EVENT;

CREATE CQ UserWaction2
INSERT INTO UserActivityInfoTcp2
SELECT * FROM TypedStream2
LINK SOURCE EVENT;

CREATE CQ UserWaction3
INSERT INTO UserActivityInfoTcp3
SELECT * FROM TypedStream3
LINK SOURCE EVENT;

CREATE CQ UserWaction4
INSERT INTO UserActivityInfoTcp4
SELECT * FROM TypedStream4
LINK SOURCE EVENT;

CREATE CQ UserWaction5
INSERT INTO UserActivityInfoTcp5
SELECT * FROM TypedStream5
LINK SOURCE EVENT;

end application CSV;
deploy application csv;
start csv;

--
-- Kafka Stream Recovery Test 1 with FileWriter as Target
-- Amudha, Striim, Inc.
--
-- S -> CQ -> KS -> WS

STOP KStreamRecov1Tester.KStreamRecovTest1wfwr;
UNDEPLOY APPLICATION KStreamRecov1Tester.KStreamRecovTest1wfwr;
DROP APPLICATION KStreamRecov1Tester.KStreamRecovTest1wfwr CASCADE;
DROP USER KStreamRecov1Tester;
DROP NAMESPACE KStreamRecov1Tester CASCADE;
CREATE USER KStreamRecov1Tester IDENTIFIED BY KStreamRecov1Tester;
-- GRANT 'Global:create,drop:deploymentgroup:*' TO USER KStreamRecov1Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov1Tester;
CONNECT KStreamRecov1Tester KStreamRecov1Tester;

CREATE APPLICATION KStreamRecovTest1wfwr RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE or REPLACE TYPE CsvStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM TypedStream OF CsvStreamType; 

CREATE CQ InsertEvents
INSERT INTO TypedStream
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE TARGET FileWrt USING FILEWRITER (
	directory:'@FEATURE-DIR@/logs/',
	FILENAME:'FileKafkaStream.log',
	flushpolicy:'eventcount:1'
--	rolloverpolicy:'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter(

) 
INPUT FROM TypedStream;


END APPLICATION KStreamRecovTest1wfwr;
DEPLOY APPLICATION KStreamRecovTest1wfwr;
START APPLICATION KStreamRecovTest1wfwr;

create Target @TARGET_NAME@ using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (

)
input from @STREAM@;

CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatc (
  StartPosition: '@startPosition@',
  ConnectionURL: '@sourceURL@',
  Username: '@userName@',
  Tables: '@tables@',
  CheckColumn: '@checkColum@',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  Password: '@password@',
  Password_encrypted: 'false',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

STOP APPLICATION EH;
UNDEPLOY APPLICATION EH;
DROP APPLICATION EH CASCADE;
CREATE APPLICATION EH @Recovery@;
CREATE SOURCE s USING FileReader (
	directory:'Product/IntegrationTests/TestData/',
    WildCard:'posdata.csv',
	positionByEOF:false
	)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO ss;

CREATE TYPE userType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String
);

CREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;

CREATE CQ cq1
INSERT INTO userDefinedTypedStream
SELECT data[0],data[1],data[2], data[3],data[4]
FROM ss;

create Target t1 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	Partitionkey:'@metadata(RecordOffset)',
	--ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using xmlFormatter(
rootelement:'data')
input from ss;

create Target t2 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	Partitionkey:'Col1',
	--ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using xmlFormatter(
    rootelement:'document',
	elementtuple:'Col1:Col2:text=Col1'
)
input from userDefinedTypedStream;

END APPLICATION EH;
DEPLOY APPLICATION EH;
start application EH;

STOP APPLICATION EH;
UNDEPLOY APPLICATION EH;
DROP APPLICATION EH CASCADE;
CREATE APPLICATION EH recovery 5 second interval;
create flow AgentFlow;
CREATE SOURCE EH_SOURCE USING MySQLReader (
  Compression: true,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: '@CONNECTION_URL@',
  DatabaseName: 'waction',
  Tables: 'waction.Test01',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)
OUTPUT TO EH_SS;
end flow AgentFlow;
create flow serverFlow;
create Target EH_TARGET using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_01',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_01_cg',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:1000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter()
input from EH_SS;
end flow serverFlow;

END APPLICATION EH;

--DEPLOY APPLICATION EH;
deploy application eh with AgentFlow in Agents, ServerFlow in default;

start application EH;

STOP APPLICATION ER;
UNDEPLOY APPLICATION ER;
DROP APPLICATION ER CASCADE;
CREATE APPLICATION ER;
CREATE STREAM ER_SS1 OF Global.JsonNodeEvent;

CREATE SOURCE ER_S1 USING AzureEventHubReader (
    EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_01',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
    ConsumerGroup: 'test_01_cg',
	startSeqNo:'0'
	)
PARSE USING jsonparser ()
OUTPUT TO ER_SS1;


create Target ER_t1 using FileWriter (
filename:'MYSQL_EH_ER_JSON_RT',
directory:'FEATURE-DIR/logs/',
rolloverpolicy:'eventcount:5000000'
)
format using jsonFormatter (members:'data')
input from ER_SS1;
end application ER;
deploy application ER;

STOP APPLICATION BooleanExpressionsTester.BooleanExpressionsApp;
UNDEPLOY APPLICATION BooleanExpressionsTester.BooleanExpressionsApp;
DROP APPLICATION BooleanExpressionsTester.BooleanExpressionsApp CASCADE;

CREATE APPLICATION BooleanExpressionsApp;

CREATE source wsSource USING FileReader (
directory:'@TEST-DATA-PATH@',
wildcard:'bool.csv',
blocksize: 10240,
positionByEOF:false
)
PARSE USING DSVParser (
header:No,
trimquote:false
) OUTPUT TO QaStream;

CREATE TYPE wsData
(
bankbool boolean,
bankID integer key
);


CREATE WACTIONSTORE oneWS
CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@



CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT To_boolean(data[0]),to_int(data[1]) FROM QaStream
LINK SOURCE EVENT;


END APPLICATION BooleanExpressionsApp;

CREATE TYPE PosData(
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);
CREATE STREAM PosDataStream OF PosData PARTITION BY merchantId;

create flow @STREAM@Flow1;
create type @STREAM@type (id string, name string);

CREATE OR REPLACE STREAM @STREAM@2 OF @STREAM@type;
CREATE OR REPLACE STREAM @STREAM@3 OF @STREAM@type;
CREATE OR REPLACE STREAM @STREAM@4 OF @STREAM@type;

create cq @STREAM@cq1
insert into @STREAM@2
select 
TO_STRING(data[0]).replaceAll("COMPANY ", ""),
data[1]
from @STREAM@;

end flow @STREAM@Flow1;


create flow @STREAM@Flow2;

create cq @STREAM@cq2
insert into @STREAM@3
select * 
from @STREAM@2 where id is not null ;

create cq @STREAM@cq3
insert into @STREAM@4
select * 
from @STREAM@3 where id is not null ;

end flow @STREAM@Flow2;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING FileWriter( 
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
   ) 
FORMAT USING dsvFormatter  ()
INPUT FROM @STREAM@4;

create namespace ns1;
create namespace ns2;

use ns1;

create application group group1;

CREATE or REPLACE  APPLICATION testgroupapp1 RECOVERY 15 SECOND INTERVAL;
end application testgroupapp1;

CREATE or REPLACE  APPLICATION testgroupapp2 RECOVERY 15 SECOND INTERVAL;
end application testgroupapp2;

use ns2;
create application group group2;

CREATE or REPLACE  APPLICATION testgroupapp1 RECOVERY 15 SECOND INTERVAL;
end application testgroupapp1;

CREATE or REPLACE  APPLICATION testgroupapp2 RECOVERY 15 SECOND INTERVAL;
end application testgroupapp2;

alter application group group1 add ns1.testgroupapp1,testgroupapp1;

use ns1;
alter application group group2 add testgroupapp2,ns2.testgroupapp2;

use admin;

create user PosTester identified by PosTester;

drop role PosTester.enduser; drop role PosTester.useradmin;
drop role PosTester.dev; drop user PosTester cascade;
drop namespace PosTester;
create user PosTester identified by PosTester;
drop role PosTester.enduser; drop role PosTester.useradmin;
drop role PosTester.dev; drop user PosTester cascade;
drop namespace PosTester;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING JSONParser ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE CQ @CQName@ INSERT INTO @CQOUTPUTSTREAM@ select @FUNCTION@ from @SRCINPUTSTREAM@ s ;

create Target @targetsys@ using SysOut(name:Foo2) input from @CQOUTPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:1',
  Tables: '@TargetTableMapping@'
 ) 
INPUT FROM @CQOUTPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ USE EXCEPTIONSTORE TTL : '7d' ;

CREATE SOURCE @APPNAME@_DBSource USING DatabaseReader (
  Tables: '"qatest"."dbo"."emp"',
  Username: 'qatest',
  Password: '5wZ8jZNAU1dzU0bPbxhATA==',
  FetchSize: 10000,
  Password_encrypted: 'false',
  QuiesceOnILCompletion: 'true',
  DatabaseProviderType: 'SQLSERVER',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;DatabaseName=qatest' )
OUTPUT TO @APPNAME@_OutputStream;

CREATE OR REPLACE TARGET @APPNAME@_Target USING Global.BigQueryWriter (
  ColumnDelimiter: '|',
  NullMarker: 'NULL',
  streamingUpload: 'false',
  projectId: 'striimqa-214712',
  Encoding: 'UTF-8',
  batchPolicy: 'eventcount:10,interval:60',
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30',
  AllowQuotedNewLines: 'false',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  TransportOptions: 'connectionTimeout=300, readTimeout=120',
  adapterName: 'BigQueryWriter',
  Mode: 'APPENDONLY',
  ServiceAccountKey: 'Platform/UploadedFiles/google-gcs.json',
  StandardSQL: 'true',
  includeInsertId: 'true',
  QuoteCharacter: '\"',
  Tables: '"qatest"."dbo"."%",DEV_30875.%' )
INPUT FROM @APPNAME@_OutputStream;

END APPLICATION @APPNAME@;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;
Create Source oracSource
 Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;

CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

STOP APPLICATION SystemTimeTester.SystemTimeWindows;
UNDEPLOY APPLICATION SystemTimeTester.SystemTimeWindows;
DROP APPLICATION SystemTimeTester.SystemTimeWindows cascade;

CREATE APPLICATION SystemTimeWindows;

CREATE TYPE RandomData(
bankNumber int KEY,
bankName String
);


CREATE SOURCE ranDataSource using StreamReader(
OutputType: 'SystemTimeTester.RandomData',
noLimit: 'false',
isSeeded: 'true',
maxRows: 0,
iterations: 30,
iterationDelay: 1000,
StringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',
NumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]R'
)OUTPUT TO CSVDataStream;

CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT (data[0]), data[1]
FROM CSVDataStream;

CREATE @WINDOWTYPE@ WINDOW tierone OVER RandomDataStream keep within 20 second;

CREATE STREAM onetwostream OF RandomData;

CREATE CQ onetwocq
INSERT INTO onetwostream
SELECT bankNumber,bankName
FROM tierone
where bankName ='bofa'
order by bankNumber;

CREATE @WINDOWTYPE@ WINDOW tiertwo OVER onetwostream keep within 40 second;

CREATE STREAM twothreestream OF RandomData;

CREATE CQ twothreecq
INSERT INTO twothreestream
SELECT bankNumber,bankName
FROM tierTwo
where bankName ='bofa'
order by bankNumber;

CREATE @WINDOWTYPE@ WINDOW tierthree OVER twothreestream keep within 1 minute;

CREATE WACTIONSTORE MyDataActivity
CONTEXT OF RandomData
EVENT TYPES(RandomData )
@PERSIST-TYPE@

Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
SELECT bankNumber,bankName from @FROMSTREAM@
where bankName ='bofa'
order by bankNumber
LINK SOURCE EVENT;


END APPLICATION SystemTimeWindows;
deploy application SystemTimeWindows;
start application SystemTimeWindows;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

stop application @appname@Out;
undeploy application @appname@Out;
drop application @appname@Out cascade;

drop stream @appname@PSStream;
CREATE OR REPLACE PROPERTYSET @appname@KafkaPropset (zk.address:'localhost:@keeperport@', bootstrap.brokers:'localhost:@brokerport@', partitions:'50');
CREATE STREAM @appname@PSStream@rand@ OF Global.JSONNodeEvent PERSIST USING @appname@KafkaPropset;

CREATE APPLICATION @appname@ recovery 5 second interval;;

CREATE OR REPLACE SOURCE @cobolsrc@ USING FileReader (
  wildcard: '',
  positionbyeof: false,
  directory: ''
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: 'ProcessRecordAsEvent',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'SingleEvent',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'Level01',
  copybookFileName: ''
   )
OUTPUT TO @appname@PSStream@rand@;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE APPLICATION @appname@Out recovery 5 second interval;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
  filename: '',
  directory: '',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter  (
  members: 'data',
  EventsAsArrayOfJsonObjects: 'true'
 )
INPUT FROM @appname@PSStream@rand@;

END APPLICATION @appname@Out;
deploy application @appname@Out on all in default;

STOP application consoleconsoletest.noApp;
undeploy application consoleconsoletest.noApp;
drop application consoleconsoletest.noApp cascade;

DROP USER consoletest;
DROP NAMESPACE consoletest CASCADE;
CREATE USER consoletest IDENTIFIED BY consoletest;

--Creating two roles
Create role consoletest.R1;
Create role consoletest.R2;

--Granting consoletest.dev to new role R1 and granting single role of Drop to R2
Grant consoletest.dev to role consoletest.R1;
Grant deploy on application consoletest.* to role consoletest.dev;
Grant undeploy on application consoletest.* to role consoletest.dev;
Revoke update on Target consoletest.noApp from role consoletest.dev;
Revoke select on WACTIONSTORE consoletest.* from role consoletest.dev;
Revoke consoletest.admin from user consoletest;
Grant drop on application consoletest.* to role consoletest.R2;

--Granting new roles to the user
Grant consoletest.R1 to user consoletest;
Grant consoletest.R2 to user consoletest;

--Creating application
Create application consoletest.noApp;

CREATE TYPE consoletest.Atm(
productID String KEY,
stateID String,
productWeight int,
quantity double,
size long,
currentDate DateTime);

CREATE source consoletest.implicitSOurce USING FileReader (
directory:'@TEST-DATA-PATH@',
columndelimiter: ',',
wildcard:'ISdata.csv',
blocksize: 10240,
positionByEOF:false
)
PARSE USING DSVParser (
header:False,
trimquote:false
) OUTPUT TO consoletest.CsvStream;

CREATE TYPE consoletest.wsType(
quantity double KEY,
currentDate DateTime
);

CREATE STREAM consoletest.newStream OF consoletest.Atm;

CREATE CQ consoletest.newCQ
INSERT INTO consoletest.newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]),TO_DATE(data[5]) FROM
consoletest.CsvStream;

CREATE WINDOW consoletest.win1
OVER consoletest.newStream
keep within 3 minute;

CREATE STREAM consoletest.newStream2 of consoletest.wsType;

CREATE WACTIONSTORE consoletest.WS1 CONTEXT OF consoletest.wsType
EVENT TYPES(consoletest.wsType );

Create cq consoletest.newCQ2
insert into consoletest.ws1 (quantity,currentDate)
select quantity, currentDate from consoletest.newStream;

Create Target consoletest.Trace Using Sysout (name: 'Trace') Input From consoletest.newStream;

End Application consoletest.noApp;


Connect consoletest consoletest;


Alter application noApp;

create or replace type consoletest.wsType(quantity double KEY, size long, currentDate DateTime);

Create or replace Target consoletest.Trace1 Using Sysout (name:'Trace1') Input From consoletest.newStream;

end application consoletest.noApp;
alter application consoletest.noApp recompile;

Deploy application consoletest.noApp;
Start application consoletest.noApp;

-- The following step should fail as expected 
Select count (*) from WS1;

stop application MSSQLTransactionSupportCompression;
undeploy application MSSQLTransactionSupportCompression;
drop application MSSQLTransactionSupportCompression cascade;

CREATE APPLICATION MSSQLTransactionSupportCompression recovery 1 second interval;

Create Source ReadFromMSSQL7
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
FetchTransactionMetadata:'true',
FilterTransactionBoundaries: true,
Compression:'true',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportCompressionStream;


CREATE TARGET WriteToMySQL7 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC@,@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportCompressionStream;

CREATE TARGET MSSqlReaderOutput7 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportCompressionStream; 


CREATE OR REPLACE TARGET MSSQLFileOut7 USING FileWriter  ( 
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  directory:'@FEATURE-DIR@/logs/',
  filename: 'TransactionSupportMSSQLToMySQLCompressionOn.txt'
 ) 
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 ) 
INPUT FROM MSSQLTransactionSupportCompressionStream;

END APPLICATION MSSQLTransactionSupportCompression;
deploy application MSSQLTransactionSupportCompression;
start application MSSQLTransactionSupportCompression;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;


CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using OracleReader
(
  Compression:true,
  StartTimestamp:'null',
  FetchSize:1,
  CommittedTransactions:true,
  QueueSize:2048,
  FilterTransactionBoundaries:true,
  Password_encrypted:'false',
  SendBeforeImage:true,
  XstreamTimeOut:600,
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE',
  adapterName:'OracleReader',
  Password:'qatest',
  DictionaryMode:'OfflineCatalog',
  FilterTransactionState:true,
  connectionRetryPolicy:'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType:'LogMiner',
  Username:'qatest',
  OutboundServerProcessName:'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic:true,
  CDDLAction:'Quiesce_Cascade',
  CDDLCapture:'true'
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING BigqueryWriter  
(
  serviceAccountKey:'/Users/hariharasudhan/Downloads/google-gcs.json',
  projectId:'striimqa-214712',
  datalocation:'US',
  Tables:'public.dbr_pg234567890123456789source1,public.dbr_pg234567890123456789Target1;public.dbr_pg234567890123456789source2,public.dbr_pg234567890123456789Target2',
  BatchPolicy:"eventCount:1,Interval:90",
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;

--
-- Kafka Stream Recovery Test 1
-- Bert Hashemi and Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> KS -> WS

STOP KStreamAvroRecov1Tester.KStreamAvroRecovTest1;
UNDEPLOY APPLICATION KStreamAvroRecov1Tester.KStreamAvroRecovTest1;
DROP APPLICATION KStreamAvroRecov1Tester.KStreamAvroRecovTest1 CASCADE;
DROP USER KStreamAvroRecov1Tester;
DROP NAMESPACE KStreamAvroRecov1Tester CASCADE;
CREATE USER KStreamAvroRecov1Tester IDENTIFIED BY KStreamAvroRecov1Tester;
-- GRANT 'Global:create,drop:deploymentgroup:*' TO USER KStreamAvroRecov1Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamAvroRecov1Tester;
CONNECT KStreamAvroRecov1Tester KStreamAvroRecov1Tester;

CREATE APPLICATION KStreamAvroRecovTest1 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250', dataformat:'avro');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE or REPLACE TYPE KafkaCsvStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF KafkaCsvStreamType 
EVENT TYPES ( KafkaCsvStreamType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

END APPLICATION KStreamAvroRecovTest1;

stop application CDCTester.CDCTest;
undeploy application CDCTester.CDCTest;
drop application CDCTester.CDCTest cascade;

create application CDCTest;

Create Source Rac11g Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'QATEST.SAMPLETEST2',
 FetchSize:1,
 QueueSize:2048
)
Output To LCRStream;


end application CDCTest;
deploy application CDCTest;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@
 Using Ojet
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@'
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  directory:'@FEATURE-DIR@/logs/',
  filename:'RoundUPPosData',
  rolloverpolicy:'FileSizeRollingPolicy,filesize:2.5M,sequence:00'
)
format using DSVFormatter (

)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizeRoundUp_actual.log') input from TypedCSVStream;

end application DSV;

use PosTester;
DROP WINDOW PosData5Minutes;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

stop application APP_KAFKASOURCE_AGG;
undeploy application APP_KAFKASOURCE_AGG;
drop application APP_KAFKASOURCE_AGG cascade;

CREATE APPLICATION APP_KAFKASOURCE_AGG;

CREATE OR REPLACE TYPE STREAM_CQ_CALCULATELAG_Type (
 topic java.lang.String KEY,
 TotalTopicLag java.lang.Integer,
 lastdatatime org.joda.time.DateTime,
 status java.lang.String,
 is_green java.lang.Integer);

CREATE OR REPLACE TYPE STREAM_CQ_JOIN_KAFKA_SOURCES1_Type (
 Company java.lang.String KEY,
 TotalLast24hour java.lang.Integer,
 TotalLast1hour java.lang.Integer,
 TotalTopicLag java.lang.Integer,
 lastdatatime org.joda.time.DateTime,
 status java.lang.String,
 is_green java.lang.Integer,
 latitude java.lang.String,
 longitude java.lang.String,
 topic java.lang.String,
 city_name java.lang.String,
 city_id java.lang.Integer);

CREATE OR REPLACE TYPE STREAM_CQ_CALCULATE_HOURLYTOTALS_Type (
 topic java.lang.String KEY,
 TotalLast24hour java.lang.Integer,
 TotalLast1hour java.lang.Integer);

CREATE OR REPLACE CQ CQ_GET_LASTHOUR
INSERT INTO STREAM_CQ_GET_LASTHOUR
SELECT rawdatacount, topic,timerange from  ET_HOURLYTOTALS_KAFKADATA_FILE,JUMP_WND_1EVT_30SEC where timerange = DHOURS(DNOW())-1;

CREATE OR REPLACE CQ CQ_CALCULATE_HOURLY_TOTAL
INSERT INTO STREAM_CQ_CALCULATE_HOURLY_TOTAL
SELECT f.topic as topic, sum(f.rawdatacount) as TotalLast24hour, B.rawdatacount as TotalLast1hour FROM JUMP_WND_1EVT_1MIN h
   join SLIDE_WND_HOURLYTOTALS_KAFKADATA_FILE f on 1=1
   join STREAM_CQ_GET_LASTHOUR B on B.topic=f.topic
   Group by f.topic;

CREATE OR REPLACE EVENTTABLE ET_KAFKA_HOURLY_TOTAL USING STREAM (
  name: 'STREAM_CQ_CALCULATE_HOURLY_TOTAL' )
QUERY (
  keytomap: 'topic',
  persistPolicy: 'true' )
OF STREAM_CQ_CALCULATE_HOURLY_TOTAL_Type;

END APPLICATION APP_KAFKASOURCE_AGG;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using DatabaseReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 1 second interval;

create flow agentflow;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@'
  --recordSelector: 'ARDPLKBX-RECORD:ARDPLKBX-RECORD-TYPE=ARDPLKBX-RECORD'
  --recordSelector: 'OH:MOH-SEG-ID=OH, OH2:OH2-SEG-ID=OH2, OHU:MOH-SEG-ID=OHU, OR1:OR1-SEG-ID=OR1, OR2:OR2-SEG-ID=OR2, OR3:OR3-SEG-ID=OR3, OR4:OR4-SEG-ID=OR4, OHM:OHM-SEG-ID=OHM, OD:OD-SEG-ID=OD, ODU:ODU-SEG-ID=ODU, OD1:OD1-SEG-ID=OD1, ODM:ODM-SEG-ID=ODM, OT:OT-SEG-ID=OT'
)
OUTPUT TO @APPNAME@Stream;

end flow agentflow;

create flow serverflow;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;


CREATE TYPE test_typeRe 
(
node_new com.fasterxml.jackson.databind.JsonNode,
node_name com.fasterxml.jackson.databind.JsonNode,
node_addr com.fasterxml.jackson.databind.JsonNode
);

Create stream cqAsJSONNodeStreamRe of test_typeRe;

CREATE CQ GetPOAsJsonNodesRe
INSERT into cqAsJSONNodeStreamRe
select 
data.get('ACCTS-RECORD'),
data.get('ACCTS-RECORD').get('NAME'),
data.get('ACCTS-RECORD').get('ADDRESS3')
from @APPNAME@Stream js;

create type finaldtypeRe
(ACCOUNT_NO int,
FIRST_NAME String,
LAST_NAME String,
ADDRESS1 String,
ADDRESS2 String,
CITY String,
STATE String,
ZIP_CODE int);

CREATE STREAM getdataStreamPS OF finaldtypeRe;

CREATE CQ getdataRe
INSERT into getdataStreamPS
select JSONGetInteger(x.node_new,"ACCOUNT-NO"),
JSONGetString(x.node_name,"FIRST-NAME"),
JSONGetString(x.node_name,"LAST-NAME"),
JSONGetString(x.node_new,"ADDRESS1"),
JSONGetString(x.node_new,"ADDRESS2"),
JSONGetString(x.node_addr,"CITY"),
JSONGetString(x.node_addr,"STATE"),
JSONGetInteger(x.node_addr,"ZIP-CODE")
from cqAsJSONNodeStreamRe x;

create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:1000,Interval:50',
  CommitPolicy: 'EventCount:1000,Interval:50',
  Tables: 'QATEST.@table@'
)
input from getdataStreamPS;
end flow serverflow;

end application @APPNAME@;
deploy application @APPNAME@ with agentflow on any in agents,serverflow in default; 
start application @APPNAME@;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;


stop OracleToKudu_ExpStore;
undeploy application OracleToKudu_ExpStore;
drop application OracleToKudu_ExpStore cascade;

--drop exceptionstore admin.OracleToKudu_ExceptionStore;
CREATE APPLICATION OracleToKudu use exceptionstore;
Create Source oracSource Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;

CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;



CREATE APPLICATION OracleToKudu_ExpStore;

CREATE TYPE OracleToKudu_ExpStore_CDCStreams_Type  (
 exceptionType java.lang.String,
  action java.lang.String,
  appName java.lang.String,
  entityType java.lang.String,
  entityName java.lang.String,
  className java.lang.String,
  message java.lang.String 
 );

CREATE STREAM OracleToKudu_ExpStore_CDCStreams OF OracleToKudu_ExpStore_CDCStreams_Type;

CREATE CQ OracleToKudu_ReadFromExpStore
INSERT INTO OracleToKudu_ExpStore_CDCStreams
select s.exceptionType,s.action,s.appName,s.entityType,s.entityName,s.className,s.message from admin.OracleToKudu_ExceptionStore [jumping 10 second] s;
        
CREATE OR REPLACE TARGET OracleToKudu_ExpStore_WriteToFileAsJSON USING FileWriter  ( 
  filename: 'expEvent_Kudu.log',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:2,interval:30',
  adapterName: 'FileWriter',
  directory: 'ExpStore_logs',
  rolloverpolicy: 'eventcount:2,interval:30s'
 ) 
FORMAT USING JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter',
  formatterName: 'JSONFormatter',
  jsonMemberDelimiter: '\n',
  jsonobjectdelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true'
 ) 
INPUT FROM OracleToKudu_ExpStore_CDCStreams;
        
END APPLICATION OracleToKudu_ExpStore;

deploy application OracleToKudu_ExpStore;
start OracleToKudu_ExpStore;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@ (a String, b String ,c String,g String)

SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]) where TO_String(data[0]) > '1' ;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW ;

CREATE STREAM Oracle_ChangeDataStream of Global.WAEvent;

CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: false,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.56.101:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream1;


CREATE CQ CQfilter
INSERT INTO Oracle_ChangeDataStream
select putuserdata (data1,'IntToInt', data[0]) from Oracle_ChangeDataStream1 data1
where (META(data1, 'OperationName').toString() =='INSERT' or META(data1, 'OperationName').toString() =='UPDATE');

CREATE STREAM Oracle_DataStream of Global.WAEvent;

CREATE CQ CQfilter1
INSERT INTO Oracle_DataStream
select * from Oracle_ChangeDataStream c where to_int(USERDATA(c, 'IntToInt'))<4;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:60',
  CommitPolicy: 'EventCount:1,Interval:60',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  Tables: 'QATEST.OracToCql_alldatatypes,test.oractocq_alldatatypes columnmap(IntToInt=IntToInt)',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_DataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

drop namespace hubspot cascade force;
create namespace hubspot;
use hubspot;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ RECOVERY 30 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;

CREATE FLOW @AppName@_SourceFlow;

CREATE SOURCE @srcName@ USING Global.HubSpotReader ( 
  PollingInterval: '1m', 
  StartPosition: '%=-1', 
  Tables: '@objectname@s__c', 
  ThreadPoolCount: '10', 
  ConnectionPoolSize: '20', 
  ClientSecret_encrypted: 'false', 
  RefreshToken: '', 
  RefreshToken_encrypted: 'false', 
  Mode: 'automated', 
  useConnectionProfile: false,
  AuthMode: 'PrivateAppToken', 
  PrivateAppToken: '@accesstoken@', 
  ClientSecret: '', 
  ClientId: '', 
  PrivateAppToken_encrypted: 'false', 
  MigrateSchema: true ) 
OUTPUT TO @outstreamname@;

END FLOW @AppName@_SourceFlow;

CREATE TARGET @tgtName@ USING Global.BigQueryWriter ( 
  projectId: '@projectId@',
  batchPolicy: 'eventcount:10000,interval:2', 
  streamingUpload: 'true', 
  Mode: 'MERGE', 
  CDDLAction : 'process',
  CDDLOptions: '{\"CreateTable\":{\"action\":\"IgnoreIfExists\",\"options\":[{\"CreateSchema\":{\"action\":\"IgnoreIfExists\"}}]}}', 
  ServiceAccountKey: '@keyFileName@', 
  Tables: '%,@tgtschema@.%' ) 
INPUT FROM @instreamname@;

End APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

Stop application @externalCacheName@;
undeploy application @externalCacheName@;
drop application @externalCacheName@ cascade;
drop EXCEPTIONSTORE @externalCacheName@_Exceptionstore;

drop application @ApplicationName@ cascade;

CREATE APPLICATION @ApplicationName@ RECOVERY 5 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;

CREATE OR REPLACE SOURCE @SourceName@ USING Global.DatabaseReader
(
  Username: '@Username@',
  Password: '@Password@',
  ConnectionURL: '@ConnectionURL@',
  Tables: '@SourceTableName@',
  FetchSize: 1
)
Output To @SourceName@_st;

CREATE TYPE @TYPEName@_type (
 id Integer,
 email STRING,
 address STRING
 );

CREATE EXTERNAL CACHE @externalCacheName@ (
  keytomap: 'id',
  DatabaseProviderType: 'Default',
  Table: '@LookUpTableName@',
  AdapterName: 'DatabaseReader',
  ConnectionURL: '@ConnectionURL@',
  FetchSize: 1,
  Password_encrypted: 'false',
  Columns: 'id,email,address',
  Password: '@Password@',
  connectionRetryPolicy: 'timeOut=5, retryInterval=5, maxRetries=5',
  Username: '@Username@' )
OF @TYPEName@_type;


CREATE CQ @CQName@
INSERT INTO @TargetName@_st
SELECT t1.data[0] as ID,t1.data[1] as Name,t1.data[2] as Deptarment,t2.email as Email,t2.address as address
FROM @SourceName@_st t1 left outer join @externalCacheName@ t2
on TO_INT(t1.data[0]) = t2.id;


CREATE OR REPLACE TARGET @TargetName@ USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  BatchPolicy: 'EventCount:1,Interval:10',
  CheckPointTable: 'CHKPOINT',
  ConnectionURL: '@ConnectionURL@',
  Password_encrypted: 'false',
  CDDLAction: 'Process',
  Password: '@Password@',
  CommitPolicy: 'EventCount:1,Interval:30',
  StatementCacheSize: '50',
  DatabaseProviderType: 'Default',
  PreserveSourceTransactionBoundary: 'false',
  Tables: '@TargetTableName@',
  Username: '@Username@',
  adapterName: 'DatabaseWriter' )
INPUT FROM @TargetName@_st;

END APPLICATION @ApplicationName@;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.OJet (
  PrimaryDatabasePassword: '@PRIMARY_PASSWORD@',
  ConnectionURL: '@DOWNSTREAM_URL@',
  PrimaryDatabaseUsername: '@PRIMARY_USER@',
  Password: '@DOWNSTREAM_PASSWORD@',
  DownstreamCaptureMode: 'REAL_TIME',
  DownstreamCapture: true,
  PrimaryDatabaseConnectionURL: '@PRIMARY_URL@',
  Tables: '@SOURCE_TABLES@',
  CDDLCapture: true,
  Username: '@DOWNSTREAM_USER@' )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET_SYS@ USING Global.SysOut (
  name: 'Out' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter (
  ConnectionURL: '@TARGET_URL@',
  Username: '@TARGET_USER@',
  Password: '@TARGET_PASSWORD@',
  CheckPointTable: 'CHKPOINT',
  CommitPolicy: 'EventCount:1',
  Tables: '@TARGET_TABLES@',
  BatchPolicy: 'EventCount:1' )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

STOP APPLICATION EnvvarTester.envVar;
UNDEPLOY APPLICATION EnvvarTester.envVar;
DROP APPLICATION EnvvarTester.envVar CASCADE;

CREATE APPLICATION envVar;


CREATE SOURCE AccessLogSource USING FileReader (
directory:'@TEST-DATA-PATH@/envVar',
wildcard:'$FILENAME',
blocksize: $BLOCKSIZE,
positionByEOF:false
)
PARSE USING DSVParser (
columndelimiter:' ',
ignoreemptycolumn:'Yes',
quoteset:'[]~"',
separator:'~'
)
OUTPUT TO RawAccessStream;


END APPLICATION envVar;
DEPLOY APPLICATION envVar on any in default;
START envVar;

stop Quiesce_IL;
undeploy application Quiesce_IL;
drop application Quiesce_IL cascade;
CREATE APPLICATION Quiesce_IL USE EXCEPTIONSTORE TTL : '7d' ;
CREATE FLOW Quiesce_IL_flow;
Create Source Quiesce_IL_Oraclesrc Using databasereader(
 Username:'@USERNAME@',
 Password:'@PASSWORD@',
 ConnectionURL:'@CONNECTION_URL@',
 Tables:'QATEST.QUIESCE_TABLE1;QATEST.QUIESCE_TABLE2',
 QuiesceOnILCompletion: 'true',
 _h_fetchexactrowcount: 'true'
)
Output To Quiesce_IL_OrcStrm;
END FLOW Quiesce_IL_flow;

CREATE TARGET Quiesce_IL_BigQueryTrg USING BigQueryWriter (
  serviceAccountKey: '@SERVICEACCOUNTKEY@',
  projectId:'@PROJECTID@',
  Tables:'QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE1;QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE2'
)

INPUT FROM Quiesce_IL_OrcStrm;
END APPLICATION Quiesce_IL;
DEPLOY APPLICATION Quiesce_IL;
start application Quiesce_IL;

stop Quiesce_CDC;
undeploy application Quiesce_CDC;
drop application Quiesce_CDC cascade;
CREATE APPLICATION Quiesce_CDC recovery 1 second interval USE EXCEPTIONSTORE TTL : '7d' ;
CREATE stream Quiesce_CDC_OrcStrm of global.waevent persist using Global.DefaultKafkaProperties;
CREATE FLOW Quiesce_CDC_flow;
Create Source Quiesce_CDC_Oraclesrc Using oraclereader(
 Username:'@USERNAME@',
 Password:'@PASSWORD@',
 ConnectionURL:'@CONNECTION_URL@',
 Tables:'QATEST.QUIESCE_TABLE1;QATEST.QUIESCE_TABLE2',
 _h_fetchexactrowcount: 'true'
)
Output To Quiesce_CDC_OrcStrm;
END FLOW Quiesce_CDC_flow;
END APPLICATION Quiesce_CDC;
DEPLOY APPLICATION Quiesce_CDC;
start application Quiesce_CDC;

stop Quiesce_CDC_BQ_TARGET;
undeploy application Quiesce_CDC_BQ_TARGET;
drop application Quiesce_CDC_BQ_TARGET cascade;
CREATE APPLICATION Quiesce_CDC_BQ_TARGET recovery 1 second interval USE EXCEPTIONSTORE TTL : '7d' ;
CREATE TARGET Quiesce_CDC_BigQueryTrg USING BigQueryWriter (
  serviceAccountKey: '@SERVICEACCOUNTKEY@',
  projectId:'@PROJECTID@',
  BatchPolicy:'Interval:10',
  _h_maxParallelStreamingRequests: '10',
  Tables:'QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE1;QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE2'
)
INPUT FROM Quiesce_CDC_OrcStrm;
END APPLICATION Quiesce_CDC_BQ_TARGET;
DEPLOY APPLICATION Quiesce_CDC_BQ_TARGET;
start application Quiesce_CDC_BQ_TARGET;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SourceName@ Using MSSqlReader
(
 Username:'qatest',
 Password:'w3b@ct10n',
 DatabaseName:'qatest',
 ConnectionURL:'localhost:1433',
 Tables:'qatest.source1',
 ConnectionPoolSize:1,
 StartPosition:'EOF'
 ) Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter ( 
CheckPointTable: 'CHKPOINT',
Username: 'qatest',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',
Tables: 'QATEST.SOURCE1,qatest.target1',
Password: 'qatest'
)INPUT FROM @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetsys@ USING SysOut (name: 'ora12_out') INPUT FROM @SRCINPUTSTREAM@;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

--
-- Recovery Test 21 with two sources, two sliding count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5W -> CQ1 -> WS
-- S2 -> Sc6W -> CQ2 -> WS
--

STOP KStreamRecov21Tester.KStreamRecovTest21;
UNDEPLOY APPLICATION KStreamRecov21Tester.KStreamRecovTest21;
DROP APPLICATION KStreamRecov21Tester.KStreamRecovTest21 CASCADE;
DROP USER KStreamRecov21Tester;
DROP NAMESPACE KStreamRecov21Tester CASCADE;
CREATE USER KStreamRecov21Tester IDENTIFIED BY KStreamRecov21Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov21Tester;
CONNECT KStreamRecov21Tester KStreamRecov21Tester;

CREATE APPLICATION KStreamRecovTest21 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION KStreamRecovTest21;

CREATE APPLICATION @APPNAME@;

CREATE TYPE @APPNAME@type1 (
 companyName java.lang.String,
 merchantId java.lang.String,
 city java.lang.String);

CREATE STREAM @APPNAME@TypedStream OF @APPNAME@type1 PARTITION BY city;

CREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (
  wildcard: '',
  positionByEOF: false,
  directory: ''
  )
PARSE USING DSVParser (
header:'true'
)
OUTPUT TO @APPNAME@Stream;

CREATE OR REPLACE CQ @APPNAME@CQ
INSERT INTO @APPNAME@TypedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantID,
TO_STRING(data[10]) as city
FROM @APPNAME@Stream;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@TypedStream;

END APPLICATION @APPNAME@;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;
Create Source oracSource Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;

CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@  RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @srcName@ USING OracleReader (
 Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@'
)
OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@ USING Global.DeltaLakeWriter (
  personalAccessToken:'@tgtpassword@',
  hostname:'@tgthostname@',
  stageLocation:'/',
  Mode:'MERGE',
  AuthenticationType: 'PersonalAccessToken',
  Tables:'@srcschema@.@srctable@,@tgtschema@.@tgttable@',
  adapterName:'DeltaLakeWriter',
  personalAccessToken_encrypted:'false',
  optimizedMerge:'false',
  uploadPolicy:'eventcount:1,interval:10s',
  connectionUrl:'@tgturl@',
  IgnorableExceptionCode:'TABLE_NOT_FOUND',
  externalStageType:'DBFSROOT'
)
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second Interval ;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectURL@',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;;

Create Source MSSqlsource Using MSSqlReader
(
 Username:'@SQL-USERNAME',
 Password:'@SQL-PASSWORD',
 DatabaseName:'@DATABASE-NAME@',
 ConnectionURL: '@SQLSERVER-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'1'
) 
Output To str;

create target MssqlAzureTarget using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

CREATE FLOW @STREAM@_SourceFlow;

CREATE SOURCE @SOURCE_NAME@ USING MySQLReader (
 Username: '@READER-UNAME@',
Password: '@READER-PASSWORD@',
ConnectionURL: '@CDC-READER-URL@',
Tables: @WATABLES@,
sendBeforeImage:'true',
FilterTransactionBoundaries: 'true'
) OUTPUT TO @STREAM@;

END FLOW @STREAM@_SourceFlow;

STOP APPLICATION EH;
UNDEPLOY APPLICATION EH;
DROP APPLICATION EH CASCADE;
CREATE APPLICATION EH @Recovery@;
CREATE SOURCE s USING FileReader (
	directory:'Product/IntegrationTests/TestData/',
    WildCard:'posdata.csv',
	positionByEOF:false
	)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO ss;

CREATE TYPE userType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String
);

CREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;

CREATE CQ cq1
INSERT INTO userDefinedTypedStream
SELECT data[0],data[1],data[2], data[3],data[4]
FROM ss;

create Target t1 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	Partitionkey:'@metadata(RecordOffset)',
	ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using dsvFormatter()
input from ss;

create Target t2 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	Partitionkey:'Col1',
	ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using dsvFormatter()
input from userDefinedTypedStream;

END APPLICATION EH;
DEPLOY APPLICATION EH;
start application EH;

STOP APPLICATION ER;
UNDEPLOY APPLICATION ER;
DROP APPLICATION ER CASCADE;
CREATE APPLICATION ER;
CREATE SOURCE ER_S1 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:0	
	)
PARSE USING DSVParser (
)OUTPUT TO ER_SS1;
CREATE SOURCE ER_S2 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:0	
	)
PARSE USING DSVParser (
)OUTPUT TO ER_SS2;

CREATE TYPE CustType1(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
TopicName String,
PartitionID String
);

CREATE TYPE CustType2(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
TopicName String,
PartitionID String
);

CREATE Stream DSVReaderStream1 of CustType1;
CREATE Stream DSVReaderStream2 of CustType2;

CREATE CQ CustType_CQ1
INSERT INTO DSVReaderStream1
SELECT data[5],data[6],data[7],data[8],data[9],data[10],
metadata.get("TopicName").toString() AS TopicName,
metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS1;

CREATE CQ CustType_CQ2
INSERT INTO DSVReaderStream2
SELECT data[0],data[1],data[2],data[3],data[4],
metadata.get("TopicName").toString() AS TopicName,
metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS2;

create Target ER_t1 using FileWriter (
filename:'FT1_5L_DSV_RT',
directory:'FEATURE-DIR/logs/',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from DSVReaderStream1;

create Target ER_t2 using FileWriter (
filename:'FT2_5L_DSV_RT',
directory:'FEATURE-DIR/logs/',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from DSVReaderStream2;
end application ER;
deploy application ER;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 10 second interval;
create source CSVSource using FileReader (
	directory:'@DIR@',
	WildCard:'@WILDCARD@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  curr String,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       data[6],
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target BlobT using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	uploadpolicy:'EventCount:100,interval:5s'
)
format using XMLFormatter (
rootelement:'document',
elementtuple:'MerchantName:merchantId:text=merchantname'
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ @RECOVERY@;
create source @SOURCE@ using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'dynamicdirectory.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO @STREAM@;

create Target @TARGET@ using ADLSGen2Writer(
    accountname:'',
	sastoken:'',
	filesystemname:'',
	filename:'',
	directory:'',
	uploadpolicy:'eventcount:5000'
)format using DSVFormatter (
 members: 'data'
)
input from @STREAM@;

end application @APPNAME@;

deploy application @APPNAME@;
start application @APPNAME@;

--
-- Canon Test W60
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for a partitioned sliding attribute window
--
-- S -> SWa5p -> CQ -> WS
--


UNDEPLOY APPLICATION NameW60.W60;
DROP APPLICATION NameW60.W60 CASCADE;
CREATE APPLICATION W60 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW60;

CREATE SOURCE CsvSourceW60 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW60;

END FLOW DataAcquisitionW60;




CREATE FLOW DataProcessingW60;

CREATE TYPE DataTypeW60 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW60 OF DataTypeW60;

CREATE CQ CSVStreamW60_to_DataStreamW60
INSERT INTO DataStreamW60
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW60;

CREATE WINDOW SWa5pW60
OVER DataStreamW60
KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY word;

CREATE WACTIONSTORE WactionStoreW60 CONTEXT OF DataTypeW60
EVENT TYPES ( DataTypeW60 KEY(word) )
@PERSIST-TYPE@

CREATE CQ SWa5pW60_to_WactionStoreW60
INSERT INTO WactionStoreW60
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM SWa5pW60
GROUP BY word;

END FLOW DataProcessingW60;



END APPLICATION W60;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
 )
INPUT FROM @STREAM@;

--
-- Canon Test W02
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for an unpartitioned jumping count window
--
-- S -> JWc101uW02 -> CQ -> WS
--


UNDEPLOY APPLICATION NameW02.W02;
DROP APPLICATION NameW02.W02 CASCADE;
CREATE APPLICATION W02 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW02;


CREATE SOURCE CsvSourceW02 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW02;


END FLOW DataAcquisitionW02;



CREATE FLOW DataProcessingW02;

CREATE TYPE DataTypeW02 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW02 OF DataTypeW02;

CREATE CQ CSVStreamW02_to_DataStreamW02
INSERT INTO DataStreamW02
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW02;

CREATE JUMPING WINDOW JWc101uW02
OVER DataStreamW02
KEEP 101 ROWS;

CREATE WACTIONSTORE WactionStoreW02 CONTEXT OF DataTypeW02
EVENT TYPES ( DataTypeW02 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWc101uW02_to_WactionStoreW02
INSERT INTO WactionStoreW02
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc101uW02;

END FLOW DataProcessingW02;



END APPLICATION W02;

CREATE OR REPLACE TARGET @TARGET_NAME@ using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
		BatchPolicy: 'EventCount:1',
  		CommitPolicy: 'EventCount:1',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@2 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
  		BatchPolicy: 'Interval:10',
  		CommitPolicy: 'Interval:10',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@3 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
		BatchPolicy: 'eventCount:100000,Interval:20',
		CommitPolicy: 'eventCount:100000,Interval:20',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@4 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
  		BatchPolicy: 'EventCount:1',
		CommitPolicy: 'EventCount:1',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
) INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;;

create Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;

stop application SalesForceReaderTest;
undeploy application SalesForceReaderTest;
drop application SalesForceReaderTest cascade;

CREATE APPLICATION SalesForceReaderTest recovery 5 second interval;

CREATE OR REPLACE SOURCE SFPoller USING SalesForceReader 
(
  sObjects:'newobj__c',
  --sObject:'Campaign',
  pollingInterval:'1 min',
  autoAuthTokenRenewal:'true',
  Username:'siddhika@webaction.com',
  Password:'webaction@1234',
  securityToken:'qhNbKKmafFpanz8Y2oiM89UhR',
  consumerKey:'3MVG9ZL0ppGP5UrBayz85eLnPg69gWLaE8pA3uzwcFCZ9s.J0mgE7AKvPCEhTaop4uYRbBaDnGXHjnLmngG6P',
  consumerSecret:'2500119200751301808',
  apiEndPoint:'https://ap2.salesforce.com',
  mode:'InitialLoad',
  startTimestamp:null
)
OUTPUT TO DataStream;

create target tout using sysout(name : 'out')input from DataStream;

/*
CREATE TARGET dbtar USING DatabaseWriter( 
	BatchPolicy:'EventCount:100,Interval:10',
	CommitPolicy:'EventCount:100,Interval:10',
	ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
	Username:'qatest',
	Password:'qatest',
  Tables:'Position__c,QATEST.POS columnmap(Id=Id,numnum=numnum__c)'
  -- Tables: 'newobj__c,QATEST.SFTABLE1 columnmap(Id=Id,CHECKBOOL=checkbool__c,CURR=curr__c,DT=dt__c,TIME1=time1__c,DtTime=DtTime__c,EMAILID=emailid__c,NUM=num__c,PERCNT=percnt__c,PHN=phn__c,TXTLONG=txtlong__c,URL1=url1__c,TXT=txt__c)'
   ---,loc__latitude=loc__latitude__s,loc__longitude=loc__longitude__s)' 
) INPUT FROM DataStream;

*/
CREATE OR REPLACE TARGET Target2 using FileWriter
(
  filename:'Obj123.json',
  directory:'/Users/siddhika/Product/',
  rolloverpolicy:'EventCount:1'
)
FORMAT USING JSONFormatter ()
INPUT FROM DataStream;

END APPLICATION SalesForceReaderTest;
DEPLOY APPLICATION SalesForceReaderTest on any in default;
START SalesForceReaderTest;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.test01',
  connectionRetryPolicy:'retryInterval=1,maxRetries=3'
 ) 
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'public.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true	
) INPUT FROM @APPNAME@_ss;
--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

create source @SourceName1@ USING IncrementalBatchReader
(
  FetchSize: 10,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: '@startPosition@',
  PollingInterval: '20sec'
)
OUTPUT TO @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:@targetsys@) input from @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
  ConnectionURL:'@READER-URL@',
  Username:'@READER-UNAME@',
  Password:'@READER-PASSWORD@',
  BatchPolicy:'Eventcount:1,Interval:1',
  CommitPolicy:'Eventcount:1,Interval:1',
  Checkpointtable:'RGRN_CHKPOINT',
  Tables:'@WATABLES@,@WATABLES@_target'
) INPUT FROM @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;

DEPLOY APPLICATION @APPNAME@;
start application @APPNAME@;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
drop application @APPNAME1@ cascade;

CREATE APPLICATION @APPNAME1@;

create source @SourceName2@ USING IncrementalBatchReader
(
FetchSize: 10,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn1@,
  startPosition: '@startPosition1@',
  PollingInterval: '20sec'
)
OUTPUT TO @SRCINPUTSTREAM1@;

create Target @targetsys1@ using SysOut(name:@targetsys1@) input from @SRCINPUTSTREAM1@;

CREATE TARGET @targetName1@ USING DatabaseWriter(
  ConnectionURL:'@READER-URL@',
  Username:'@READER-UNAME@',
  Password:'@READER-PASSWORD@',
  BatchPolicy:'Eventcount:1,Interval:1',
  CommitPolicy:'Eventcount:1,Interval:1',
  Checkpointtable:'RGRN_CHKPOINT',
  Tables:'@WATABLES_target'
) INPUT FROM @SRCINPUTSTREAM1@;

END APPLICATION @APPNAME1@;

DEPLOY APPLICATION @APPNAME1@;
start application @APPNAME1@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;

Create Source @SourceName@ Using Ojet

(
  Username:'c##qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',
  Tables:'CDB$ROOT."C##QATEST".ojet_src;ORCLPDB.QATEST.ojet_src',
  _h_useClassic:false,
  Fetchsize:1,
  Compression: true,
  SupportPDB:true,
  ReplicationSlotName:'null'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',
  Username:'c##qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'CDB$ROOT."C##QATEST".ojet_src,CDB$ROOT."C##QATEST".ojet_tgt'
) INPUT FROM @SRCINPUTSTREAM@;



create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ recovery 5 second interval;
--create application @APPNAME@;

--create flow agentflow;
CREATE OR REPLACE SOURCE @APPNAME@_Src USING SpannerBatchReader  (
  DatabaseProviderType: 'Default',
  pollingInterval: '5ms',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  ConnectionURL: 'jdbc:cloudspanner:/projects/bigquerywritertest/instances/testspanner/databases/spannertestdb?credentials=/Users/jenniffer/Downloads/abc.json',
  Tables: 'Recovery_Timestam%',
  --_h_mode:'InitialLoad',
--  VendorConfiguration:'_h_SpannerReadStaleness=MAX_STALENESS 20s',
  adapterName: 'SpannerBatchReader',
    StartPosition: '%=0',
  CheckColumn: '%=id'
 )
OUTPUT TO @APPNAME@_Output_Stream;
--end flow agentflow;

CREATE TARGET @APPNAME@_tgt USING SpannerWriter (
	Tables: 'spannersource,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	BatchPolicy: 'EventCount: 1; Interval: 1s',
	instanceId: 'qatest'
) INPUT FROM @APPNAME@_Output_Stream;

create Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_Output_Stream;

end application @APPNAME@;
deploy application @APPNAME@;
--deploy application @APPNAME@ with agentflow in agents;
start application @APPNAME@;

STOP NamedQTester.NamedQueryApp;
UNDEPLOY APPLICATION NamedQTester.NamedQueryApp;
DROP APPLICATION NamedQTester.NamedQueryApp cascade;

CREATE APPLICATION NamedQueryApp;

CREATE TYPE cacheType( EventID String, 
			Word String, 
			datetime Datetime);

CREATE CACHE adhcache using CSVReader (
  directory: '@TEST-DATA-PATH@/',
  wildcard: 'Canon1000.csv',
  header: Yes,
  columndelimiter: ',',
  trimquote:false
) QUERY (keytomap:'EventID') OF cacheType;



END APPLICATION NamedQueryApp;
DEPLOY APPLICATION NamedQueryApp;
START APPLICATION NamedQueryApp;

DROP NAMEDQUERY NamedQTester.nqone;
DROP NAMEDQUERY NamedQTester.nqtwo;
DROP NAMEDQUERY NamedQTester.nqthree;
CREATE NAMEDQUERY nqone select * from adhcache;
CREATE NAMEDQUERY nqtwo select * from adhcache;
CREATE NAMEDQUERY nqthree select * from adhcache;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING ADLSReader ()
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE CQ @APPNAME@_CQ
INSERT INTO @APPNAME@_CQOut
SELECT
    data[0] as BusinessName,
    data[1] as MerchantId,
    data[2] as PosDataCode,
    data[3] as AccNumber,
    data[4] as DateTime,
    data[5] as ExpDate,
    data[6] as CurrencyCode,
    data[7] as AuthAmount,
    data[8] as TerminalId,
    data[9] as Zip,
    data[10] as City
FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_KafkaTarget USING Global.KafkaWriter VERSION @KAFKA_VERSION@(
  brokerAddress: '',
  Topic: '',
  Mode: 'Sync' )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_BlobTarget USING Global.AzureBlobWriter (
  containername: '',
  blobname: '',
  accountaccesskey: '',
  accountname: '',
  foldername: '' )
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_DWHTarget USING Global.BigQueryWriter (
  Tables: '',
  BatchPolicy: '',
  projectId: '',
  ServiceAccountKey: '' )
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_NoSqlTarget USING Global.MongoDBWriter (
  AuthDB: 'admin',
  ConnectionURL: '',
  Username: '',
  collections: '',
  Password: '' )
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_FileTarget USING Global.FileWriter (
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '',
  filename: '' )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_OLTPTarget USING Global.DatabaseWriter (
  ConnectionURL: '',
  Password: '',
  Username: '',
  Tables: '' )
INPUT FROM @APPNAME@_CQOut;

CREATE SOURCE @APPNAME@_KafkaSource USING KafkaReader VERSION @KAFKA_VERSION@ (
  brokerAddress: '',
  Topic: '',
  startOffset: '0' )
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@_Stream2;

CREATE OR REPLACE TARGET @APPNAME@_KafkaFileTarget USING Global.FileWriter (
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '',
  filename: '' )
FORMAT USING JSONFormatter(
members:'data')
INPUT FROM @APPNAME@_Stream2;

END APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING FileReader  (
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '/Users/jenniffer/Downloads',
  skipbom: true,
  wildcard: 'dk000000000'
 )
 PARSE USING GGTrailParser  (
  handler: 'com.webaction.proc.GGTrailParser_1_0',
  metadata: '@METADATA@',
  FilterTransactionBoundaries: true,
  TrailByteOrder: '@BYTEORDDER@',
  Tables: '@TABLES@',
  parserName: 'GGTrailParser',
  _h_ReturnDateTimeAs: '@DATETIME@',
  Compression:'@COMPRESSION@'
 )OUTPUT to @STREAM@;

STOP APPLICATION DBWriterTester.DBWriterApp;
UNDEPLOY APPLICATION DBWriterTester.DBWriterApp;
DROP APPLICATION DBWriterTester.DBWriterApp CASCADE;

DROP USER DBWriterTester;
DROP NAMESPACE DBWriterTester CASCADE;
CREATE USER DBWriterTester IDENTIFIED BY DBWriterTester;
GRANT create,drop ON deploymentgroup Global.* To user DBWriterTester;
CONNECT DBWriterTester DBWriterTester;

CREATE APPLICATION DBWriterApp;
CREATE OR REPLACE SOURCE Source_DBReader USING OracleReader  (
-- StartTimestamp: '@CDC-STARTUPTIME@',
Username: 'miner',
Password: 'miner',
ConnectionURL: '//10.1.186.110:1521/orcl',
TABLES: 'qatest.alltype1;qatest.dbr_marker;',
FetchSize: '1',
committedtransactions: true,
CatalogMode: 'Offline',
BatchPolicy: 'eventCount:10'
)
OUTPUT TO DBReaderStrm;
CREATE OR REPLACE TARGET Target_DBWriter USING DatabaseWriter  (
Tables: 'QATEST.ALLTYPE1,QATEST.ALLTYPE1;QATEST.DBR_MARKER,QATEST.DBR_MARKER;',
Username: 'qatest',
PasSword: 'qatest',
ConnecTionURL: 'jdbc:oracle:thin:@//10.1.110.142:1521/orcl',
BatchPolicy: 'eventCount:10'
)
INPUT FROM DBReaderStrm;
END APPLICATION DBWriterApp;
deploy application DBWriterApp;
start application DBWriterApp;

--
-- Recovery Test 6 with sliding window and partitioned feature
-- Nicholas Keene, Bert Hashemi WebAction, Inc.
--
-- S -> CQ -> SW(partitioned) -> CQ(no aggregate) -> WS
--

STOP KStreamRecov6Tester.KStreamRecovTest6;
UNDEPLOY APPLICATION KStreamRecov6Tester.KStreamRecovTest6;
DROP APPLICATION KStreamRecov6Tester.KStreamRecovTest6 CASCADE;
DROP USER KStreamRecov6Tester;
DROP NAMESPACE KStreamRecov6Tester CASCADE;
CREATE USER KStreamRecov6Tester IDENTIFIED BY KStreamRecov6Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov6Tester;
CONNECT KStreamRecov6Tester KStreamRecov6Tester;

CREATE APPLICATION KStreamRecovTest6 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvData PARTITION BY merchantId;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION KStreamRecovTest6;

stop Oracle_IRLogWriter;
undeploy application Oracle_IRLogWriter;
drop application Oracle_IRLogWriter cascade;

CREATE APPLICATION Oracle_IRLogWriter;

CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.autotest01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.autotest01=id',
 startPosition: 'striim.autotest01=2',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream;

  create type AutoType(
  ID string,
  name string,
  company string,
  country string
);

CREATE STREAM CDCdata_stream OF AutoType;

CREATE CQ Lookup
INSERT INTO CDCdata_stream
select data[0],data[1],data[2],data[3] from data_stream;

CREATE  TARGET AzureSQLDWHTarget USING AzureSQLDWHWriter  ( 
  ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
  username: 'striim',
  password: 'W3b@ct10n',
   accountname: 'striimqatestdonotdelete',
   Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
   Tables: 'STRIIM.AUTOTEST01',
  uploadpolicy: 'eventcount:1,interval:10s'
 ) 
INPUT FROM CDCdata_stream;

CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM CDCdata_stream;


END APPLICATION Oracle_IRLogWriter;
deploy application Oracle_IRLogWriter;
start Oracle_IRLogWriter;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW ;

 create flow myagentflow;

CREATE OR REPLACE SOURCE DBSource USING DatabaseReader  ( 
  Username: '@SOURCE_USER@',
  Password_encrypted: false,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Query: "SELECT * FROM QATEST.OracToCql_alldatatypes",
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SOURCE_PASS@'
 ) 
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource2 USING DatabaseReader  ( 
  Username: '@SOURCE_USER@',
  Password_encrypted: false,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Query: "SELECT * FROM QATEST.OracToCql_alldatatypes2",
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SOURCE_PASS@'
 ) 
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource3 USING DatabaseReader  ( 
  Username: '@SOURCE_USER@',
  Password_encrypted: false,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Query: "SELECT * FROM QATEST.OracToCql_alldatatypes3",
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SOURCE_PASS@'
 ) 
OUTPUT TO Oracle_ChangeDataStream;

end flow myagentflow;

create flow myserverflow;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  Tables: '',
  Password: 'cassandra',
  Password_encrypted: false
 ) 
INPUT FROM Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

end flow myserverflow;

END APPLICATION DBRTOCW;

deploy application DBRTOCW on ALL in default with myagentflow on all in Agents, myserverflow on all in  default;

start DBRTOCW;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@;


CREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW ;

 

CREATE OR REPLACE SOURCE DBSource USING DatabaseReader  ( 
  Username: '@SOURCE_USER@',
  Password_encrypted: false,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Query: "SELECT * FROM qatest.MssqlToCql_alldatatypes",
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SOURCE_PASS@'
 ) 
OUTPUT TO Oracle_ChangeDataStream;


CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1000,Interval:0',
  CommitPolicy: 'EventCount:1000,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes1',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource2 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes2',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource3 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes3',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget2 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget3 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:10,Interval:60',
  CommitPolicy: 'EventCount:10,Interval:60',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget4 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:11,Interval:120',
  CommitPolicy: 'EventCount:11,Interval:120',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget5 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:3,Interval:120',
  CommitPolicy: 'EventCount:3,Interval:120',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  ExcludedTables:'QATEST.ORACTOCQL_ALLDATATYPES',
  Password: '+hbb060plSWQwscvI105cg==',
  Password_encrypted: true
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE TARGET FWTarget USING FileWriter(
	name:CassandraOuput,
	filename:'OracToFw.log',
	flushpolicy : 'interval:120,eventcount:3',
	rolloverpolicy : 'interval:300s'
)
FORMAT USING DSVFormatter()
INPUT FROM Oracle_ChangeDataStream;
create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;


END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
directory:'@FEATURE-DIR@/logs',
filename:'PosDataFS',
rolloverpolicy:'filesize:1M,sequence:00'
)
format using DSVFormatter (

)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizePosData_actual.log') input from TypedCSVStream;

end application DSV;

create application KinesisTest;
CREATE OR REPLACE SOURCE OS USING OracleReader (
  Username: 'miner',
  Password: 'miner',
  ConnectionURL: '192.168.1.113:1521:ORCL',
  TABLES: 'QATEST.H_REGION;QATEST.H_NATION;QATEST.H_CUSTOMER',
  FetchSize: '1'
 )
OUTPUT TO DDLCDCStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	accesskeyid:'ACCESS_KEY',
	secretaccesskey:'SECRET_KEY'
)
format using JSONFormatter (
)
input from DDLCDCStream;

end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

create or replace Target EH_TARGET using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_01',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_01_cg',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from EH_SS;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

CREATE Source dataGenSrc Using PostgreSQLReader  ( 
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: true,
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://127.0.0.1:5432/webaction?stringtype=unspecified',
  Tables:'@tableNames@',
  adapterName: 'PostgreSQLReader',
  Password: 'xFzvJYZf1b8=',
  Password_encrypted: 'true'
 ) 
Output To LCRStream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;

END APPLICATION DataGenSampleApp;

STOP APPLICATION routerApp;
UNDEPLOY APPLICATION routerApp;
DROP APPLICATION routerApp CASCADE;


CREATE APPLICATION routerApp;

CREATE  SOURCE OraSource USING OracleReader  (
Username: 'qatest',
Password: 'qatest',
ConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',
Tables: 'QATEST.TGT_T%',
 FetchSize:'100'
)
OUTPUT TO MasterStream1;

CREATE OR REPLACE ROUTER tablerouter1 INPUT FROM MasterStream1 s CASE
WHEN meta(s,"TableName").toString()='QATEST.TGT_T1' THEN ROUTE TO ss1,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T2' THEN ROUTE TO ss2,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T3' THEN ROUTE TO ss3,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T4' THEN ROUTE TO ss4,
ELSE ROUTE TO ss_else;

create Target FileTarget_1 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'
)
FORMAT USING dsvFormatter ()
input from ss1;

create Target FileTarget_2 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'
)
FORMAT USING dsvFormatter ()
input from ss2;

create Target FileTarget_3 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from ss3;

create Target FileTarget_4 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from ss4;


create Target FileTarget_5 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from ss_else;


end application routerApp;
deploy application routerApp;
start routerApp;

STOP cacheRefresher.cacheApp;
UNDEPLOY APPLICATION cacheRefresher.cacheApp;
DROP APPLICATION cacheRefresher.cacheApp cascade;

CREATE APPLICATION cacheApp;


CREATE TYPE bankData
(
bankID Integer KEY,
bankName String
);

CREATE CACHE cache1 USING CsvReader(
directory: '@TEST-DATA-PATH@',
wildcard: 'banks_cache.csv',
header: No,
columndelimiter: ',',
trimquote: false,
positionbyEOF: false
) QUERY (keytomap:'bankID', refreshinterval:'20 second') OF bankData;



END APPLICATION cacheApp;

STOP APPLICATION LongRunningQueryTester.LongRunningApp;
UNDEPLOY APPLICATION LongRunningQueryTester.LongRunningApp;
DROP APPLICATION LongRunningQueryTester.LongRunningApp cascade;

CREATE APPLICATION LongRunningApp;


 --COMMENT::   Modify type attributes as desired.
 --COMMENT::   Type must be created first before creating a source using ranReader

CREATE TYPE RandomData(
  myName String,
  streetAddress String,
  bankName String,
  bankNumber int KEY,
  bankAmount double
);

CREATE source ranDataSource USING ranReader(
  OutputType:'LongRunningQueryTester.RandomData',
  TimeInterval:5,
  NoLimit:true,
  SampleSize:10000,
  DataKey:bankName,
  NumberOfUniqueKeys:500
) OUTPUT TO CSVDataStream;


CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4])
FROM CSVDataStream;


CREATE JUMPING WINDOW RandomData10Rows
OVER RandomDataStream KEEP 10 ROWS
PARTITION BY bankNumber;


CREATE TYPE myData(
  myName String,
  myAddress String,
  myBankName String,
  myBankNumber int KEY,
  myBankAmount double
);

CREATE STREAM myDataStream OF myData;

CREATE CQ GetMyData
INSERT INTO MyDataStream
SELECT myName, streetAddress, bankName, bankNumber, bankAmount
FROM RandomData10Rows WHERE bankNumber > 20000 AND bankNumber < 20300;


CREATE WACTIONSTORE MyDataActivity CONTEXT OF MyData
EVENT TYPES(myData )
PERSIST EVERY 60 second USING (
JDBC_DRIVER:'@WASTORE-DRIVER@',
JDBC_URL:'@WASTORE-URL@',
JDBC_USER:'@WASTORE-UNAME@',
JDBC_PASSWORD:'@WASTORE-PASSWORD@',
DDL_GENERATION:'drop-and-CREATE-tables'
);


Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
Select * from myDataStream
LINK SOURCE EVENT;


END APPLICATION LongRunningApp;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src1 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream1;

CREATE OR REPLACE CQ @APP_NAME@_CQ1
INSERT INTO @APP_NAME@_Stream6
SELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream1 d;

CREATE OR REPLACE SOURCE @APP_NAME@_src2 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream2;

CREATE OR REPLACE CQ @APP_NAME@_CQ2
INSERT INTO @APP_NAME@_Stream6
SELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream2 d;

CREATE OR REPLACE SOURCE @APP_NAME@_src3 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream3;

CREATE OR REPLACE CQ @APP_NAME@_CQ3
INSERT INTO @APP_NAME@_Stream6
SELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream3 d;

CREATE OR REPLACE SOURCE @APP_NAME@_src4 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream4;

CREATE OR REPLACE CQ @APP_NAME@_CQ4
INSERT INTO @APP_NAME@_Stream6
SELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream4 d;

CREATE OR REPLACE SOURCE @APP_NAME@_src5 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream5;

CREATE OR REPLACE CQ @APP_NAME@_CQ5
INSERT INTO @APP_NAME@_Stream6
SELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream5 d;


CREATE OR REPLACE TARGET @APP_NAME@_tgt1 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream6;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 _h_returnDateTimeAs: 'ZonedDateTime',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING CosmosDBWriter  (
  BatchPolicy: 'EventCount:1000,Interval:1',
  ServiceEndpoint: 'https://souvik.documents.azure.com:443/',
  ConnectionPoolSize: '10',
  AccessKey: 'z1CfmzAy5QwB5MN7bWIinM9gYmJn7zWo1wOaadvaCErqaTCqlb7srpAx7muPYWhJwnYq3plOQoBNENn1xPmkfQ==',
  adapterName: 'CosmosDBWriter',
  Collections: 'QATEST.TEST,db2.TEST keycolumns(id)',
  ConnectionRetryPolicy: 'RetryInterval:1,MaxRetries:0',
  KeySeparator: ':',
  AccessKey_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE APPLICATION @AppName@ RECOVERY 1 MINUTE INTERVAL AUTORESUME MAXRETRIES 2 RETRYINTERVAL 60;

CREATE source @AppName@_PosData USING FileReader (
  WildCard: 'posdata100.csv',
  directory: '@TestDir@',
positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO @AppName@_CsvStream;

CREATE OR REPLACE CQ CsvPosAppCq 
INSERT INTO FromCsvPosAppCq 
SELECT TO_STRING(data[1]) as merchantId,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM @AppName@_CsvStream;


CREATE TARGET FileWriterTarget USING Global.FileWriter ( 
 
  flushpolicy: 'EventCount:1000,Interval:30s', 
  directory: '@logs@',
  filename: '@Filename@', 
  rolloverpolicy: 'EventCount:1000,Interval:30s' ) 
FORMAT USING Global.DSVFormatter  ( 
  quotecharacter: '\"', 
  columndelimiter: ',', 
  nullvalue: 'NULL', 
  usequotes: 'false', 
  rowdelimiter: '\n', 
  standard: 'none', 
  header: 'false' ) 
INPUT FROM FromCsvPosAppCq;

end application @AppName@;
deploy application @AppName@;
start application @AppName@;

create source @SOURCE_NAME@ USING MariaDBReader 
(
Username: '@READER-UNAME@',
Password: '@READER-PASSWORD@',
ConnectionURL: '@CDC-READER-URL@',
Tables: @WATABLES@,
sendBeforeImage:'true',
FilterTransactionBoundaries: 'true'
) 
OUTPUT TO @STREAM@;

use PosTester;
DROP CACHE HourlyAveLookup;

--
-- Recovery Test 21 with two sources, two sliding count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5W -> CQ1 -> WS
-- S2 -> Sc6W -> CQ2 -> WS
--

STOP Recov21Tester.RecovTest21;
UNDEPLOY APPLICATION Recov21Tester.RecovTest21;
DROP APPLICATION Recov21Tester.RecovTest21 CASCADE;
CREATE APPLICATION RecovTest21 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION RecovTest21;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using S3Writer(
    bucketname:'@BUCKET@',
   objectname:'upgradeData.csv',
   foldername:'upgradefolder',
  uploadpolicy:'EventCount : 10000,Interval :1m '
)
format using DSVFormatter (
)
input from @STREAM@;

end flow @APPNAME@_serverflow;

end application @APPNAME@;

CREATE  TARGET @TARGET_NAME@ USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@1;

 CREATE  TARGET @TARGET_NAME@2 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@2;

 CREATE  TARGET @TARGET_NAME@3 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@3;

 CREATE  TARGET @TARGET_NAME@4 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@4;

 CREATE  TARGET @TARGET_NAME@5 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@5;

 CREATE  TARGET @TARGET_NAME@6 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@6;

 CREATE  TARGET @TARGET_NAME@7 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@7;

 CREATE  TARGET @TARGET_NAME@8 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@8;

 CREATE  TARGET @TARGET_NAME@9 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@9;

 CREATE  TARGET @TARGET_NAME@10 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@10;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src1 USING Global.GCSReader ()
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream1;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer ()
FORMAT USING DSVFormatter (
members:'data')
INPUT FROM @APPNAME@_Stream1;

CREATE OR REPLACE SOURCE @APPNAME@_src2 USING Global.GCSReader ()
PARSE USING Global.JSONParser ()
OUTPUT TO @APPNAME@_Stream2;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_Stream2;

CREATE OR REPLACE SOURCE @APPNAME@_src3 USING GCSReader ()
PARSE USING AvroParser ()
OUTPUT TO @APPNAME@_Stream3;

CREATE CQ @APPNAME@_CQ3
INSERT INTO @APPNAME@_CQOut3
SELECT AvroToJson(data,false) FROM @APPNAME@_Stream3;

CREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_CQOut3;

CREATE OR REPLACE SOURCE @APPNAME@_src4 USING Global.GCSReader ()
PARSE USING Global.XMLParser ()
OUTPUT TO @APPNAME@_Stream4;

CREATE OR REPLACE TARGET @APPNAME@_trgt4 USING S3Writer ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_Stream4;

END APPLICATION @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ Recovery 5 second interval;

create stream @APPNAME@_UserdataStream of Global.WAEvent;

create type @APPNAME@_Order_type(
id int,
order_id int,
zipcode int,
category String,
tablename string
);

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src1 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.order_%'
)
OUTPUT TO @APPNAME@_OrdersStream;

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src2 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_2',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.second_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream2;

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src3 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_3',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.third_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream3;

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src4 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_4',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.fourth_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream4;


Create CQ @APPNAME@_CQUser
insert into @APPNAME@_UserdataStream
select 
putuserdata (data,'Fileowner','FIRST_ORDER') from @APPNAME@_OrdersStream data;


Create CQ @APPNAME@_CQUser2
insert into @APPNAME@_UserdataStream
select 
putuserdata (data2,'Fileowner','SECOND_ORDER') from @APPNAME@_OrdersStream2 data2;


Create CQ @APPNAME@_CQUser3
insert into @APPNAME@_UserdataStream
select 
putuserdata (data3,'Fileowner','THIRD_ORDER') from @APPNAME@_OrdersStream3 data3;


Create CQ @APPNAME@_CQUser4
insert into @APPNAME@_UserdataStream
select 
putuserdata (data4,'Fileowner','FOURTH_ORDER') from @APPNAME@_OrdersStream4 data4;

create stream @APPNAME@_OrderTypedStream1 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream2 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream3 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream4 of @APPNAME@_Order_type;

CREATE CQ @APPNAME@_fin_cq
INSERT INTO @APPNAME@_OrderTypedStream1
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FIRST_ORDER';

CREATE CQ @APPNAME@_fin_cq2
INSERT INTO @APPNAME@_OrderTypedStream2
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'SECOND_ORDER';

CREATE CQ @APPNAME@_fin_cq3
INSERT INTO @APPNAME@_OrderTypedStream3
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'THIRD_ORDER';

CREATE CQ @APPNAME@_fin_cq4
INSERT INTO @APPNAME@_OrderTypedStream4
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FOURTH_ORDER';


create Target @APPNAME@_ADLSGen1_tgt1 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'%category%/%tablename%',
        datalakestorename:'striimdlstest.azuredatalakestore.net',
        clientid:'94195e09-651c-431e-8556-59343c99cc05',
        authtokenendpoint:'https://login.microsoftonline.com/71bfeed5-1905-43da-a4a4-49d8490731da/oauth2/token',
        clientkey:'Vt7Reaamli1DXpqa3kY1+VTzQuEQrvchs5PJ3VNVmfM=',
  rolloverpolicy:'eventcount:8,interval:20s'
)
format using DSVFormatter (
    header:'true'
)
input from @APPNAME@_OrderTypedStream1; 

create Target @APPNAME@_ADLSGen1_tgt2 using ADLSGen1Writer(
        filename:'event_data.xml',
        directory:'%category%/%tablename%',
        datalakestorename:'striimdlstest.azuredatalakestore.net',
        clientid:'94195e09-651c-431e-8556-59343c99cc05',
        authtokenendpoint:'https://login.microsoftonline.com/71bfeed5-1905-43da-a4a4-49d8490731da/oauth2/token',
        clientkey:'Vt7Reaamli1DXpqa3kY1+VTzQuEQrvchs5PJ3VNVmfM=',
	rolloverpolicy:'eventcount:8,interval:20s'
)
format using XMLFormatter (
  elementtuple: 'Order_id:id:order_id:zipcode:category:text=tablename',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from @APPNAME@_OrderTypedStream2; 

create Target @APPNAME@_ADLSGen1_tgt3 using ADLSGen1Writer(
        filename:'event_data.avro',
        directory:'%category%/%tablename%',
        datalakestorename:'striimdlstest.azuredatalakestore.net',
        clientid:'94195e09-651c-431e-8556-59343c99cc05',
        authtokenendpoint:'https://login.microsoftonline.com/71bfeed5-1905-43da-a4a4-49d8490731da/oauth2/token',
        clientkey:'Vt7Reaamli1DXpqa3kY1+VTzQuEQrvchs5PJ3VNVmfM=',
  rolloverpolicy:'eventcount:8,interval:20s'
)
format using AvroFormatter (
  formatAs: 'Default',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA-FILE@'
)
input from @APPNAME@_OrderTypedStream3; 


create Target @APPNAME@_ADLSGen1_tgt4 using ADLSGen1Writer(
        filename:'event_data.json',
        directory:'%category%/%tablename%',
        datalakestorename:'striimdlstest.azuredatalakestore.net',
        clientid:'94195e09-651c-431e-8556-59343c99cc05',
        authtokenendpoint:'https://login.microsoftonline.com/71bfeed5-1905-43da-a4a4-49d8490731da/oauth2/token',
        clientkey:'Vt7Reaamli1DXpqa3kY1+VTzQuEQrvchs5PJ3VNVmfM=',
  rolloverpolicy:'eventcount:8,interval:20s'
)
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@_OrderTypedStream4;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

create application DhcpLog;
create source DHCPLogSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'Dhcp*',
	charset:'UTF-8',
	positionByEOF:false
) PARSE USING DHCPLogParser (
	rowdelimiter:'\r\n',
	LineNumber:33
)
OUTPUT TO DHCPLogStream;
create Target DHCPDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/dhcp_log') input from DHCPLogStream;
end application DhcpLog;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_scnRange: 1000,
 _h_eoffDelay: 10,
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.56.101:1521/orcl',
  Tables: 'QATEST.oracle_200',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;


create stream xferredDataStream1 of Global.WAEvent;

CREATE CQ CQ1
insert into xferredDataStream1
select  putuserdata (data1,'ID', data[0]) from Oracle_ChangeDataStream data1;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:4,Interval:60',
  CommitPolicy: 'EventCount:4,Interval:60',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  IgnorableExceptionCode:'PRIMARY KEY',
  Tables: 'QATEST.oracle_200,test.cassandra_200 columnmap(field1=field1,field2=field2,field3=field3,field4=field4,field5=field5,field6=field6,field7=field7,field8=field8,field9=field9,field10=field10,field11=field11,field12=field12,field13=field13,field14=field14,field15=field15,field16=field16,field17=field17,field18=field18,field19=field19,field20=field20,field21=field21,field22=field22,field23=field23,field24=field24,field25=field25,field26=field26,field27=field27,field28=field28,field29=field29,field30=field30,field31=field31,field32=field32,field33=field33,field34=field34,field35=field35,field36=field36,field37=field37,field38=field38,field39=field39,field40=field40,field41=field41,field42=field42,field43=field43,field44=field44,field45=field45,field46=field46,field47=field47,field48=field48,field49=field49,field50=field50,field51=field51,field52=field52,field53=field53,field54=field54,field55=field55,field56=field56,field57=field57,field58=field58,field59=field59,field60=field60,field61=field61,field62=field62,field63=field63,field64=field64,field65=field65,field66=field66,field67=field67,field68=field68,field69=field69,field70=field70,field71=field71,field72=field72,field73=field73,field74=field74,field75=field75,field76=field76,field77=field77,field78=field78,field79=field79,field80=field80,field81=field81,field82=field82,field83=field83,field84=field84,field85=field85,field86=field86,field87=field87,field88=field88,field89=field89,field90=field90,field91=field91,field92=field92,field93=field93,field94=field94,field95=field95,field96=field96,field97=field97,field98=field98,field99=field99,field100=field100,field101=field101,field102=field102,field103=field103,field104=field104,field105=field105,field106=field106,field107=field107,field108=field108,field109=field109,field110=field110,field111=field111,field112=field112,field113=field113,field114=field114,field115=field115,field116=field116,field117=field117,field118=field118,field119=field119,field120=field120,field121=field121,field122=field122,field123=field123,field124=field124,field125=field125,field126=field126,field127=field127,field128=field128,field129=field129,field130=field130,field131=field131,field132=field132,field133=field133,field134=field134,field135=field135,field136=field136,field137=field137,field138=field138,field139=field139,field140=field140,field141=field141,field142=field142,field143=field143,field144=field144,field145=field145,field146=field146,field147=field147,field148=field148,field149=field149,field150=field150,field151=@METADATA(OperationName),field152=field1,field153=@METADATA(SQLRedoLength),field154=@METADATA(SEQUENCE),field155=@METADATA(SegmentName),field156=@METADATA(OperationType),field157=@METADATA(TxnUserID),field158=@METADATA(ThreadID),field159=@METADATA(TxnUserID),field160=@USERDATA(ID),field161=@USERDATA(ID),field162=@USERDATA(ID),field163=@USERDATA(ID),field164=field3,field165=field150,field166=field151)',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM xferredDataStream1;

create Target t2 using SysOut(name:Foo2) input from xferredDataStream1;

END APPLICATION DBRTOCW;

deploy application DBRTOCW in  default;

start application DBRTOCW;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

create source @SourceName1@ USING IncrementalBatchReader
(
  FetchSize: 10,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: '@startPosition@',
  PollingInterval: '20sec'
)
OUTPUT TO @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:@targetsys@) input from @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
  ConnectionURL:'@READER-URL@',
  Username:'@READER-UNAME@',
  Password:'@READER-PASSWORD@',
  BatchPolicy:'Eventcount:1,Interval:1',
  CommitPolicy:'Eventcount:1,Interval:1',
  Checkpointtable:'RGRN_CHKPOINT',
  Tables:'@WATABLES@,@WATABLES@_target'
) INPUT FROM @SRCINPUTSTREAM@;

create source @SourceName2@ USING IncrementalBatchReader
(
FetchSize: 10,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn1@,
  startPosition: '@startPosition1@',
  PollingInterval: '20sec'
)
OUTPUT TO @SRCINPUTSTREAM1@;

create Target @targetsys1@ using SysOut(name:@targetsys1@) input from @SRCINPUTSTREAM1@;

CREATE TARGET @targetName1@ USING DatabaseWriter(
  ConnectionURL:'@READER-URL@',
  Username:'@READER-UNAME@',
  Password:'@READER-PASSWORD@',
  BatchPolicy:'Eventcount:1,Interval:1',
  CommitPolicy:'Eventcount:1,Interval:1',
  Checkpointtable:'RGRN_CHKPOINT',
  Tables:'@WATABLES_target'
) INPUT FROM @SRCINPUTSTREAM1@;

END APPLICATION @APPNAME@;

DEPLOY APPLICATION @APPNAME@;
start application @APPNAME@;

stop application BigqueryBulkLoadMonMetrics_cdc;
undeploy application BigqueryBulkLoadMonMetrics_cdc;
drop application BigqueryBulkLoadMonMetrics_cdc cascade;

CREATE APPLICATION BigqueryBulkLoadMonMetrics_cdc;

CREATE FLOW BigqueryBulkLoadMonMetrics_cdc_SourceFlow;

CREATE SOURCE BigqueryBulkLoadMonMetrics_cdc_DBSource USING oracleReader ( 
  Username: 'qatest', 
  FetchSize: 10000, 
  Password_encrypted: 'false', 
  Password: 'JVaLv3ZpgQDY8R2ZxS38xg==', 
  Tables: 'QATEST.EMPLOYEE', 
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe' ) 
OUTPUT TO BigqueryBulkLoadMonMetrics_cdc_OutputStream;

END FLOW BigqueryBulkLoadMonMetrics_cdc_SourceFlow;

CREATE OR REPLACE TARGET BigqueryBulkLoadMonMetrics_cdc_BigQueryTarget1 USING BigQueryWriter ( 
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  streamingUpload: 'false', 
  projectId: 'striimqa-214712', 
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:100000, Interval:90', 
   AllowQuotedNewLines: 'false', 
  optimizedMerge: 'false', 
  TransportOptions: 'connectionTimeout=300, readTimeout=120', 
  adapterName: 'BigQueryWriter', 
  Mode: 'MERGE', 
  StandardSQL: 'true', 
  includeInsertId: 'true', 
  QuoteCharacter: '\"', 
  ServiceAccountKey: '/Users/jenniffer/Product2/IntegrationTests/TestData/google-gcs.json' ) 
INPUT FROM BigqueryBulkLoadMonMetrics_cdc_OutputStream;

END APPLICATION BigqueryBulkLoadMonMetrics_cdc;

deploy application BigqueryBulkLoadMonMetrics_cdc;
start application BigqueryBulkLoadMonMetrics_cdc;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  _h_BindEmptyStringasNull : 'true'
 )
INPUT FROM @STREAM@;

CREATE APPLICATION tungstenAppNoComments;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)

PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;


CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startingTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
@PERSIST-TYPE@


CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false,
  trimwhitespace:true
) 
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'zip') OF USAddressData;


CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;
        
END APPLICATION tungstenAppNoComments;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.AvroEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING Global.JMSReader (
  ProviderName: '',
  Provider: '',
  Ctx: '',
  QueueName: '',
  Topic:'',
  UserName: '',
  Password: '',
  EnableTransaction: '',
  transactionpolicy: ''
  )
PARSE USING Global.ParquetParser ()
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

END APPLICATION @APPNAME@;

Stop IR;
Undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;
CREATE OR REPLACE SOURCE Teradata_source1 USING IncrementalBatchReader  ( 
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.test01=id',
  startPosition: '%=0',
  PollingInterval: '20sec'
 )
OUTPUT TO data_stream1;

CREATE OR REPLACE SOURCE Teradata_source2 USING IncrementalBatchReader  ( 
  FetchSize: 10000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.test01=t1',
  startPosition: '%=0',
  PollingInterval: '20sec' )
OUTPUT TO data_stream2;

CREATE OR REPLACE SOURCE Teradata_source3 USING IncrementalBatchReader  ( 
  FetchSize: 10000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.test01=id',
  startPosition: '%=1',
  PollingInterval: '20sec'
 )
OUTPUT TO data_stream3;

CREATE OR REPLACE SOURCE Teradata_source4 USING IncrementalBatchReader  ( 
  FetchSize: 10000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.test01=t1',
  startPosition: '%=1',
  PollingInterval: '20sec' )
OUTPUT TO data_stream4;
CREATE OR REPLACE SOURCE Teradata_source5 USING IncrementalBatchReader  ( 
  FetchSize: 10000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.test01=id',
  startPosition: '%=1',
  PollingInterval: '20sec'
 )
OUTPUT TO data_stream5;

CREATE OR REPLACE SOURCE Teradata_source6 USING IncrementalBatchReader  ( 
  FetchSize: 10000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.test01=t1',
  startPosition: '%=0',
  PollingInterval: '20sec' )
OUTPUT TO data_stream5;


create target AzureSQLDWHTarget1 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test4 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream1;


create target AzureSQLDWHTarget2 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test5',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream2;

create target AzureSQLDWHTarget3 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test5',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream3;

create target AzureSQLDWHTarget4 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test5',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream4;

create target AzureSQLDWHTarget5 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test5',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream5;



create target AzureSQLDWHTarget6 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test5',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream5;


END APPLICATION IR;
deploy application IR on all in default;
start application IR;

stop application app1PS;
undeploy application app1PS;
drop application app1PS cascade;

create application app1PS;

create target File_TargerPS using FileWriter
(
directory : '',
filename : ''
)
format using DSVFormatter()
input from KPSRss1;

end application app1PS;

deploy application app1PS;
start application app1PS;

STOP cacheCase;
UNDEPLOY APPLICATION cacheCase;
DROP APPLICATION cacheCase cascade;

CREATE APPLICATION cacheCase;


CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity double,
  size long,
  currentDate DateTime);

CREATE CACHE cAcHe1 USING CsvReader(
  directory: '@TEST-DATA-PATH@',
  wildcard: 'ISdata.csv',
  header: false,
  columndelimiter: ',',
  trimquote: false
  ) QUERY (keytomap:'PRODUCTID') OF Atm;


CREATE WACTIONSTORE WS1 CONTEXT OF Atm
EVENT TYPES
(Atm );


CREATE CQ cq1
INSERT INTO ws1
SELECT * FROM CACHE1;

END APPLICATION cacheCase;
DEPLOY APPLICATION cacheCase;
START cacheCase;

--
-- Kafka Stream with KryoParser and Kafka Reader Recovery Test 1
-- Bert Hashemi and Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> KS -> WS
-- S -> K -> CQ -> WS

STOP KStreamKryoParser1Tester.KStreamKryoParserTest1;
UNDEPLOY APPLICATION KStreamKryoParser1Tester.KStreamKryoParserTest1;
DROP APPLICATION KStreamKryoParser1Tester.KStreamKryoParserTest1 CASCADE;
DROP USER KStreamKryoParser1Tester;
DROP NAMESPACE KStreamKryoParser1Tester CASCADE;
CREATE USER KStreamKryoParser1Tester IDENTIFIED BY KStreamKryoParser1Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamKryoParser1Tester;
CONNECT KStreamKryoParser1Tester KStreamKryoParser1Tester;

CREATE APPLICATION KStreamKryoParserTest1 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'1');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE or REPLACE TYPE KafkaCsvStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF KafkaCsvStreamType 
EVENT TYPES ( KafkaCsvStreamType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE or REPLACE TYPE KafkaStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

--CREATE STREAM KafkaTypedStream OF KafkaStreamType;

CREATE STREAM KafkaStream OF Global.waevent;

CREATE SOURCE KafkaSource USING KafkaReader Version '0.8.0'
(
        brokerAddress:'localhost:9092',
        Topic:'KStreamKryoParser1Tester_KafkaCsvStream',
        PartitionIDList:'0',
        startOffset:0
)
PARSE USING StriimParser ()
OUTPUT TO KafkaStream;

CREATE WACTIONSTORE KRWactions CONTEXT OF KafkaStreamType
EVENT TYPES ( KafkaStreamType )
@PERSIST-TYPE@

CREATE CQ KRInsertWactions
INSERT INTO KRWactions
SELECT TO_STRING(data[1]) as merchantId,
    TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
    TO_DOUBLE(data[7]) as amount,
    TO_STRING(data[10]) as city 
FROM KafkaStream;

/*
CREATE CQ CQ2KafkaTypedStream
INSERT INTO KafkaTypedStream
SELECT TO_STRING(data[1]) as merchantId,
    TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
    TO_DOUBLE(data[7]) as amount,
    TO_STRING(data[10]) as city 
FROM KafkaStream;
*/

END APPLICATION KStreamKryoParserTest1;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ RECOVERY 1 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @SourceName@ USING PostgreSQLReader  ( 
 ReplicationSlotName: 'striim_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src'
 ) 
OUTPUT TO @SRCINPUTSTREAM@ ;

CREATE OR REPLACE SOURCE @SourceName@1 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src'
 ) 
OUTPUT TO @SRCINPUTSTREAM@1 ;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy:'EventCount:1000,Interval:60',
CommitPolicy:'EventCount:1000,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@1 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy:'EventCount:1000,Interval:60',
CommitPolicy:'EventCount:1000,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM @SRCINPUTSTREAM@1;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

Stop Teradata_LogWriter;
Undeploy application Teradata_LogWriter;
drop application Teradata_LogWriter cascade;

CREATE APPLICATION Teradata_LogWriter WITH ENCRYPTION recovery 5 second interval;

CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.TDSOURCE',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.TEST01=ID;',
  PollingInterval: '5sec',
  ReturnDateTimeAs: 'String',
  startPosition:'striim.test01=0'
  )
  OUTPUT TO data_stream;

  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

CREATE TARGET BinaryDump USING LogWriter(
  name: 'TeraData',
  filename:'TeraData.log'
)INPUT FROM data_stream;

END APPLICATION Teradata_LogWriter;

deploy application Teradata_LogWriter in default;

start application Teradata_LogWriter;

stop @APPNAME@;
undeploy application @APPNAME@;
--drop exceptionstore admin.MySQL_To_MySQLApp_ExceptionStore;
drop application @APPNAME@ cascade;
create application @APPNAME@ use exceptionstore;

create source @SourceName@ using MySQLReader
  (ConnectionURL: '@SourceConnectionURL@',
   Username:'@UserName@',
   Password:'@Password@',
   Tables: '@SourceTableName@'
)
output to @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
   ConnectionURL:'@TargetConnectionURL@',
   Username:'@UserName@',
   Password:'@Password@',
   BatchPolicy:'EventCount:1,Interval:0',
   Tables: '@SourceTableName@,@TargetTableName@',
   CommitPolicy: 'Interval:5'
 ) INPUT FROM @SRCINPUTSTREAM@;

create or replace cq @cq@
insert into @finalstream@
select exceptionType,action,appName,entityType,entityName,className,message,relatedActivity from @APPNAME@_ExceptionStore;

Create target @targetfile@ using filewriter (
filename:'@APPNAME@_file.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using jsonFormatter()
input from @finalstream@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop @APPNAME@_ExpStore;
undeploy application @APPNAME@_ExpStore;
drop application @APPNAME@_ExpStore cascade;
CREATE APPLICATION @APPNAME@_ExpStore;

CREATE TYPE @APPNAME@_ExpStore_CDCStreams_Type  (
  evtlist java.util.List  
 );

CREATE STREAM @APPNAME@_ExpStore_CDCStreams OF @APPNAME@_ExpStore_CDCStreams_Type;

CREATE CQ @APPNAME@_ReadFromExpStore 
INSERT INTO @APPNAME@_ExpStore_CDCStreams
select to_waevent(s.relatedObjects) as evtlist from admin.@APPNAME@_ExceptionStore [jumping 5 second] s;

CREATE STREAM @APPNAME@_ExpStore_CDCEventStream OF Global.WAEvent;

CREATE CQ @APPNAME@_ExpStore_GetCDCEvent 
INSERT INTO @APPNAME@_ExpStore_CDCEventStream
SELECT com.webaction.proc.events.WAEvent.makecopy(cdcevent) FROM @APPNAME@_ExpStore_CDCStreams a, iterator(a.evtlist) cdcevent;

CREATE CQ @APPNAME@_ExpStore_JoinDataCQ
INSERT INTO @APPNAME@_ExpStore_JoinedDataStream
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1])
        from @APPNAME@_ExpStore_CDCEventStream f;
        
CREATE OR REPLACE TARGET @APPNAME@_ExpStore_WriteToFileAsJSON USING FileWriter  ( 
  filename: '@APPNAME@_file',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:1,interval:30',
  adapterName: 'FileWriter',
  directory: 'ExpStore_logs',
  rolloverpolicy: 'eventcount:6,interval:30s'
 ) 
FORMAT USING JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter',
  formatterName: 'JSONFormatter',
  jsonMemberDelimiter: '\n',
  jsonobjectdelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true'
 ) 
INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;
        
CREATE TARGET @APPNAME@_ExpStore_dbtarget USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1,Interval:1',
CommitPolicy:'Interval:1,Eventcount:1',
Tables:'@TargetTable@'
) INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;

END APPLICATION @APPNAME@_ExpStore;

deploy application @APPNAME@_ExpStore;
start @APPNAME@_ExpStore;

CREATE OR REPLACE TARGET @appName@_AzureEventHubWriter USING AzureEventHubWriter (
  SASKey: '',
  EventHubName: '',
  EventHubNamespace: '',
  SASPolicyName: '',
  BatchPolicy: 'Size:1000000,Interval:10s' )
FORMAT USING JSONFormatter  (
 )
INPUT FROM @appName@_MCQOut1;

STOP application FileWriterDSVTester.DSV;
undeploy application FileWriterDSVTester.DSV;
drop application FileWriterDSVTester.DSV cascade;

create application DSV;

create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;


CREATE SOURCE CSVPosDataSource USING FileReader (
  directory: '@TEST-DATA-PATH@',
  WildCard: 'posdata.csv',
  positionByEOF: false,
  charset: 'UTF-8'
 )
 PARSE USING DSVParser (
  header: 'yes'
 )
OUTPUT TO PosDataCsvStream;

create source CSVMerchantNamesSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvMerchantNamesStream;

create source CSVSmallRetailSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallretaildata2M.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvSmallRetailStream;


Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Type CSVMerchantNamesType (
  merchantId String,
  merchantName String
);

Create Type CSVSmallRetailType (
storeId String,
nameId String,
city String,
state String
);

Create Stream TypedCSVStream of CSVType;
Create Stream TypedPosDataCSVStream of CSVType;
Create Stream TypedCSVMerchantNamesStream of CSVMerchantNamesType;
Create Stream TypedCSVSmallRetailStream of CSVSmallRetailType;


CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;


CREATE CQ CsvPosData
INSERT INTO TypedPosDataCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM PosDataCsvStream;


CREATE CQ CsvToMerchantNames
INSERT INTO TypedCSVMerchantNamesStream
SELECT data[0],
       data[1]
FROM CsvMerchantNamesStream;


CREATE CQ CsvToSmallRetailData
INSERT INTO TypedCSVSmallRetailStream
SELECT data[0],
       data[1],
       data[2],
       data[3]
FROM CsvSmallRetailStream;

/**
* 3.4.5.c FileWriter DSV ParserNegativeEC 100
**/
create Target DSVNegativeEventCount using FileWriter(
filename:'EventNCDefault',
directory:'@FEATURE-DIR@/logs/',
sequence:'00',
--filelimit: '5',
rolloverpolicy:'eventcount:-100',
buffersize:1
)
format using DSVFormatter (

)
input from TypedCSVSmallRetailStream;


/**
* 3.3.1.c FileWriter DSV TimeInterval
**/
create Target DSVTimeIntervalRollingPolicy using FileWriter(
  filename:'MerchantTIRP',
  sequence:'00',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00',
  buffersize:1
)
format using DSVFormatter (

)
input from TypedCSVMerchantNamesStream;

/**
* 3.4.6.d FileWriter DSV DefaultRP
**/
create Target DSVDefaultRollingPolicy using FileWriter(
directory:'@FEATURE-DIR@/logs/',
filename:'PosData',
rolloverpolicy:'EventCount:5000000',
buffersize:1
)
format using DSVFormatter (

)
input from TypedCSVStream;

/**
* 3.4.5.d FileWriter DSV EventCount 100
**/
create Target DSVEventCountDecimal using FileWriter(
filename:'Events',
directory:'@FEATURE-DIR@/logs/',
rolloverpolicy:'eventcount:200,sequence:00',
buffersize:1
)
format using DSVFormatter (

)
input from TypedCSVStream;

/**
* 3.3.1.a FileWriter DSVFileSize 1MB
**/
create Target DSVFileSize using FileWriter(
directory:'@FEATURE-DIR@/logs',
filename:'PosDataFS',
rolloverpolicy:'FileSizeRollingPolicy,filesize:1M,sequence:00',
buffersize:1
)
format using DSVFormatter (

)
input from TypedCSVStream;

/**
* 3.4.5.a FileWriter DSVFileSizeRoundUp 3MB
**/
create Target DSVFileSizeDecimal using FileWriter(
  directory:'@FEATURE-DIR@/logs/',
  filename:'RoundUPPosData',
  rolloverpolicy:'FileSizeRollingPolicy,filesize:2.5M,sequence:00',
  buffersize:1
)
format using DSVFormatter (

)
input from TypedCSVStream;


/**
* 3.1.1.b FileWriter DSV EventCount
**/

CREATE OR REPLACE TARGET DSVEventCount USING FileWriter (
  filename: 'TargetDefault',
  directory:'@FEATURE-DIR@/logs/',
  sequence:'00',
  flushinterval: '0',
  rolloverpolicy:'EventCount:5000000',
  buffersize:1
 )
 format using DSVFormatter (

)
INPUT FROM TypedPosDataCSVStream;


create Target TargetSmallPosData using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizePosDataDefault_actual.log') input from TypedCSVStream;

end application DSV;

STOP APPLICATION KafkaWPTester.KWApp;
STOP APPLICATION KafkaWPTester.KRApp;
UNDEPLOY APPLICATION KafkaWPTester.KWApp;
UNDEPLOY APPLICATION KafkaWPTester.KRApp;
DROP APPLICATION KafkaWPTester.KWApp CASCADE;
DROP APPLICATION KafkaWPTester.KRApp CASCADE;

CREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;
GRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;
CONNECT KafkaWPTester KafkaWPTester;


CREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE CSVSource USING FileReader (
	directory:'/Users/saranyad/Product/IntegrationTests/TestData/kafka_tmp',
    WildCard:'mybanks*',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:',',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;


CREATE TYPE AccessLogType(
Col1 String,
Col2 String
);

CREATE STREAM TypedAccessLogStream1 OF AccessLogType;
CREATE STREAM TypedAccessLogStream2 OF AccessLogType;
CREATE STREAM TypedAccessLogStream3 OF AccessLogType;
CREATE STREAM TypedAccessLogStream4 OF AccessLogType;
CREATE STREAM TypedAccessLogStream5 OF AccessLogType;
CREATE STREAM TypedAccessLogStream6 OF AccessLogType;
CREATE STREAM TypedAccessLogStream7 OF AccessLogType;
CREATE STREAM TypedAccessLogStream8 OF AccessLogType;
CREATE STREAM TypedAccessLogStream9 OF AccessLogType;
CREATE STREAM TypedAccessLogStream10 OF AccessLogType;
CREATE STREAM TypedAccessLogStream11 OF AccessLogType;
CREATE STREAM TypedAccessLogStream12 OF AccessLogType;
CREATE STREAM TypedAccessLogStream13 OF AccessLogType;
CREATE STREAM TypedAccessLogStream14 OF AccessLogType;
CREATE STREAM TypedAccessLogStream15 OF AccessLogType;
CREATE STREAM TypedAccessLogStream16 OF AccessLogType;
CREATE STREAM TypedAccessLogStream17 OF AccessLogType;
CREATE STREAM TypedAccessLogStream18 OF AccessLogType;
CREATE STREAM TypedAccessLogStream19 OF AccessLogType;
CREATE STREAM TypedAccessLogStream20 OF AccessLogType;

CREATE CQ AcceeslogCQ
INSERT INTO TypedAccessLogStream
SELECT 
TO_STRING(data[0]) as Col1,
TO_STRING(data[1]) as Col2
FROM FileStream ; 

CREATE CQ AcceeslogCQ1
INSERT INTO TypedAccessLogStream1
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS WHERE FS.Col1 = '1'; 


create Target KW1 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test01',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream1;

CREATE CQ AcceeslogCQ2
INSERT INTO TypedAccessLogStream2
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '2'; 


create Target KW2 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test02',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream2;

CREATE CQ AcceeslogCQ3
INSERT INTO TypedAccessLogStream3
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '3'; 


create Target KW3 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test03',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream3;

CREATE CQ AcceeslogCQ4
INSERT INTO TypedAccessLogStream4
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '4'; 


create Target KW4 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test04',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream4;

CREATE CQ AcceeslogCQ5
INSERT INTO TypedAccessLogStream5
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '5'; 


create Target KW5 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test05',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream5;

CREATE CQ AcceeslogCQ6
INSERT INTO TypedAccessLogStream6
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '6'; 


create Target KW6 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test06',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream6;

CREATE CQ AcceeslogCQ7
INSERT INTO TypedAccessLogStream7
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '7'; 


create Target KW7 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test07',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream7;

CREATE CQ AcceeslogCQ8
INSERT INTO TypedAccessLogStream8
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '8'; 


create Target KW8 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test08',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream8;

CREATE CQ AcceeslogCQ9
INSERT INTO TypedAccessLogStream9
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '9'; 


create Target KW9 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test09',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream9;

CREATE CQ AcceeslogCQ10
INSERT INTO TypedAccessLogStream10
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '10'; 


create Target KW10 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test10',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream10;

CREATE CQ AcceeslogCQ11
INSERT INTO TypedAccessLogStream11
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '11'; 


create Target KW11 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test11',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream11;

CREATE CQ AcceeslogCQ12
INSERT INTO TypedAccessLogStream12
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '12'; 


create Target KW12 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test12',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream12;

CREATE CQ AcceeslogCQ13
INSERT INTO TypedAccessLogStream13
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '13'; 


create Target KW13 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test13',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream13;

CREATE CQ AcceeslogCQ14
INSERT INTO TypedAccessLogStream14
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '14'; 


create Target KW14 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test14',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream14;

CREATE CQ AcceeslogCQ15
INSERT INTO TypedAccessLogStream15
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '15'; 


create Target KW15 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test15',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream15;

CREATE CQ AcceeslogCQ16
INSERT INTO TypedAccessLogStream16
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '16'; 


create Target KW16 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test16',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream16;

CREATE CQ AcceeslogCQ17
INSERT INTO TypedAccessLogStream17
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '17'; 


create Target KW17 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test17',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream17;

CREATE CQ AcceeslogCQ18
INSERT INTO TypedAccessLogStream18
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '18'; 


create Target KW18 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test18',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream18;

CREATE CQ AcceeslogCQ19
INSERT INTO TypedAccessLogStream19
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '19'; 


create Target KW19 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test19',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream19;

CREATE CQ AcceeslogCQ20
INSERT INTO TypedAccessLogStream20
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '20'; 


create Target KW20 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test20',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream20;


END APPLICATION KWApp;

DEPLOY APPLICATION KWApp on any in default;



















-- Kafka Reader Apps

CREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;

CREATE SOURCE KR1 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test01',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;



CREATE SOURCE KR2 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test02',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream2;


CREATE SOURCE KR3 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test03',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream3;


CREATE SOURCE KR4 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test04',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream4;


CREATE SOURCE KR5 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test05',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream5;


CREATE SOURCE KR6 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test06',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream6;


CREATE SOURCE KR7 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test07',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream7;


CREATE SOURCE KR8 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test08',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream8;


CREATE SOURCE KR9 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test09',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream9;


CREATE SOURCE KR10 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test10',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream10;


CREATE SOURCE KR11 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test11',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream11;



CREATE SOURCE KR12 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test12',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream12;


CREATE SOURCE KR13 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test13',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream13;


CREATE SOURCE KR14 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test14',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream14;


CREATE SOURCE KR15 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test15',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream15;


CREATE SOURCE KR16 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test16',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream16;


CREATE SOURCE KR17 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test17',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream17;


CREATE SOURCE KR18 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test18',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream18;


CREATE SOURCE KR19 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test19',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream19;


CREATE SOURCE KR20 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test20',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream20;




CREATE TARGET DumpKafkaReaderStream1 USING FileWriter(
  name:KafkaROuput1,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_1',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream1;


CREATE TARGET DumpKafkaReaderStream2 USING FileWriter(
  name:KafkaROuput2,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_2',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream2;


CREATE TARGET DumpKafkaReaderStream3 USING FileWriter(
  name:KafkaROuput3,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_3',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream3;


CREATE TARGET DumpKafkaReaderStream4 USING FileWriter(
  name:KafkaROuput4,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_4',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream4;


CREATE TARGET DumpKafkaReaderStream5 USING FileWriter(
  name:KafkaROuput5,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_5',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream5;


CREATE TARGET DumpKafkaReaderStream6 USING FileWriter(
  name:KafkaROuput6,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_6',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream6;


CREATE TARGET DumpKafkaReaderStream7 USING FileWriter(
  name:KafkaROuput7,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_7',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream7;


CREATE TARGET DumpKafkaReaderStream8 USING FileWriter(
  name:KafkaROuput8,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_8',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream8;


CREATE TARGET DumpKafkaReaderStream9 USING FileWriter(
  name:KafkaROuput9,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_9',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream9;


CREATE TARGET DumpKafkaReaderStream10 USING FileWriter(
  name:KafkaROuput10,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_10',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream10;


CREATE TARGET DumpKafkaReaderStream11 USING FileWriter(
  name:KafkaROuput11,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_11',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream11;


CREATE TARGET DumpKafkaReaderStream12 USING FileWriter(
  name:KafkaROuput12,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_12',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream12;


CREATE TARGET DumpKafkaReaderStream13 USING FileWriter(
  name:KafkaROuput13,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_13',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream13;


CREATE TARGET DumpKafkaReaderStream14 USING FileWriter(
  name:KafkaROuput14,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_14',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream14;


CREATE TARGET DumpKafkaReaderStream15 USING FileWriter(
  name:KafkaROuput15,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_15',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream15;


CREATE TARGET DumpKafkaReaderStream16 USING FileWriter(
  name:KafkaROuput16,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_16',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream16;


CREATE TARGET DumpKafkaReaderStream17 USING FileWriter(
  name:KafkaROuput17,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_17',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream17;


CREATE TARGET DumpKafkaReaderStream18 USING FileWriter(
  name:KafkaROuput18,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_18',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream18;


CREATE TARGET DumpKafkaReaderStream19 USING FileWriter(
  name:KafkaROuput19,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_19',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream19;


CREATE TARGET DumpKafkaReaderStream20 USING FileWriter(
  name:KafkaROuput20,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_20',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream20;

END APPLICATION KRApp;
DEPLOY APPLICATION KRApp on any in default;

CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'data.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO @AppName@_rawstream;


CREATE CQ @BuiltinFunc@CQ
INSERT INTO @BuiltinFunc@_Stream
SELECT @BuiltinFunc@(x, 'Last_Date', data[5], 'Country', data[10])
FROM @AppName@_rawstream x;

CREATE OR REPLACE CQ cq1
INSERT INTO RemoveUserData_Stream
SELECT
removeUserData(s1, 'Last_Date')
FROM @BuiltinFunc@_Stream s1;


CREATE OR REPLACE TARGET @AppName@_FileTarget USING Global.FileWriter ( 
  flushpolicy: 'EventCount:10000,Interval:30s', 
  adapterName: 'FileWriter', 
  directory: '@logs@',
  filename: '@BuiltinFunc@_RemoveData', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING Global.JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter', 
  jsonMemberDelimiter: '\n', 
  EventsAsArrayOfJsonObjects: 'true', 
  formatterName: 'JSONFormatter', 
  jsonobjectdelimiter: '\n' ) 
INPUT FROM RemoveUserData_Stream;

End application @AppName@;
Deploy application @AppName@; 
Start application @AppName@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@ recovery 1 second interval;

CREATE SOURCE @parquetsrc@ USING FileReader (
  directory: '',
  positionByEOF: false,
  WildCard: '' )
PARSE USING ParquetParser (
 )
OUTPUT TO @appname@Streams;

CREATE OR REPLACE CQ @appname@CQOrder3
INSERT INTO @appname@Stream3
SELECT
PUTUSERDATA(s,'schemaName',s.data.getSchema().getName())
FROM @appname@Streams s;

CREATE TARGET @adlstarget@ USING Global.ADLSGen2Writer (
    accountname:'',
  	sastoken:'',
  	filesystemname:'',
  	filename:'',
  	directory:'',
  	uploadpolicy:'eventcount:10' )

format using AvroFormatter (
)
INPUT FROM @appname@Stream3;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

--
-- Crash Recovery Test 5 with Jumping window and partitioned on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION N2S2CR5Tester.N2S2CRTest5;
UNDEPLOY APPLICATION N2S2CR5Tester.N2S2CRTest5;
DROP APPLICATION N2S2CR5Tester.N2S2CRTest5 CASCADE;
CREATE APPLICATION N2S2CRTest5 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest5;

CREATE SOURCE CsvSourceN2S2CRTest5 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN2S2CRTest5;

CREATE FLOW DataProcessingN2S2CRTest5;

CREATE TYPE CsvDataTypeN2S2CRTest5 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvDataTypeN2S2CRTest5 PARTITION BY merchantId;

CREATE CQ CsvToDataN2S2CRTest5
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsN2S2CRTest5 CONTEXT OF CsvDataTypeN2S2CRTest5
EVENT TYPES ( CsvDataTypeN2S2CRTest5 )
@PERSIST-TYPE@

CREATE CQ DataToWactionN2S2CRTest5
INSERT INTO WactionsN2S2CRTest5
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN2S2CRTest5;

END APPLICATION N2S2CRTest5;

STOP APPLICATION SystemTimeTester.SystemTimeWindows;
UNDEPLOY APPLICATION SystemTimeTester.SystemTimeWindows;
DROP APPLICATION SystemTimeTester.SystemTimeWindows cascade;

CREATE APPLICATION SystemTimeWindows;

CREATE TYPE RandomData(
myName String,
streetAddress String,
bankName String,
bankNumber int KEY,
bankAmount double
);


CREATE SOURCE ranDataSource using StreamReader(
OutputType: 'SystemTimeTester.RandomData',
noLimit: 'false',
isSeeded: 'true',
maxRows: 0,
iterations: 30,
iterationDelay: 1000,
StringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',
NumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]R'
)OUTPUT TO CSVDataStream;

CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4])
FROM CSVDataStream;

CREATE @WINDOWTYPE@ WINDOW tierone OVER RandomDataStream keep within 20 second;

CREATE STREAM onetwostream OF RandomData;

CREATE CQ onetwocq
INSERT INTO onetwostream
SELECT *
FROM tierone;

CREATE @WINDOWTYPE@ WINDOW tiertwo OVER onetwostream keep within 40 second;

CREATE STREAM twothreestream OF RandomData;

CREATE CQ twothreecq
INSERT INTO twothreestream
SELECT *
FROM tiertwo;

CREATE @WINDOWTYPE@ WINDOW tierthree OVER twothreestream keep within 1 minute;

CREATE WACTIONSTORE MyDataActivity
CONTEXT OF RandomData
EVENT TYPES(RandomData )
@PERSIST-TYPE@

Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
Select * from @FROMSTREAM@
LINK SOURCE EVENT;


END APPLICATION SystemTimeWindows;
deploy application SystemTimeWindows;
start application SystemTimeWindows;

create application JuniperLog;
create source JuniperLogSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'juniper-NSM*',
	charset:'UTF-8',
	positionByEOF:false
) PARSE USING JuniperNSMLogParser (
	trimwhitespace: yes
)
OUTPUT TO JuniperLogStream;
create Target JuniperDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/junipernsm_log') input from JuniperLogStream;
end application JuniperLog;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[0]) != '2';

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

-- stop application Metadata_Application;
-- undeploy application Metadata_Application;
drop application Metadata_Application force;
CREATE APPLICATION Metadata_Application;

CREATE OR REPLACE TYPE app1_AccessSource_Type (
 timestamp org.joda.time.DateTime,
 src_ip java.lang.String,
 dest_ip java.lang.String,
 http_code java.lang.String);


CREATE OR REPLACE SOURCE app1_AccessSource USING ContinuousGenerator ( 
  OutputType: 'admin.TYPE.app1_AccessSource_Type', 
  Throughput: 'Unrestricted' ) 
OUTPUT TO app1_OutputStream;


CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

END APPLICATION Metadata_Application;



-- stop application app2;
-- undeploy application Metadata_Application2;
drop application Metadata_Application2 force;

CREATE APPLICATION Metadata_Application2;

CREATE OR REPLACE CQ app2_Query 
INSERT INTO app2_AccessStream 
SELECT *
FROM app1_AccessStream;

END APPLICATION Metadata_Application2;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;


create Target DBRTOCW_t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;

deploy application DBRTOCW on ANY in default;

start application DBRTOCW;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@ MAP (table: '@SOURCE_SCHEMA@.@SOURCE_TABLE@1')
SELECT NUM_COL,CHAR_COL,VARCHAR2_COL,LONG_COL,DATE_COL,TIMESTAMP_COL where TO_INT(NUM_COL) > 1;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.test01',
	FetchSize: '1'
)
OUTPUT TO @APPNAME@_SS;


CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(id,name) COLUMNMAP(id=id,col1=col11,col2=col12)',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true		
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

--
-- Crash Recovery Test 3 on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW -> CQ(aggregate) -> WS
--

STOP APPLICATION N4S4CR3Tester.N4S4CRTest3;
UNDEPLOY APPLICATION N4S4CR3Tester.N4S4CRTest3;
DROP APPLICATION N4S4CR3Tester.N4S4CRTest3 CASCADE;
CREATE APPLICATION N4S4CRTest3 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest3;

CREATE SOURCE CsvSourceN4S4CRTest3 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest3;

CREATE FLOW DataProcessingN4S4CRTest3;

CREATE TYPE WactionTypeN4S4CRTest3 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount int,
  city String
);

CREATE STREAM DataStream OF WactionTypeN4S4CRTest3;

CREATE CQ CsvToDataN4S4CRTest3
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_INT(TO_DOUBLE(data[7])),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN4S4CRTest3 CONTEXT OF WactionTypeN4S4CRTest3
EVENT TYPES ( WactionTypeN4S4CRTest3 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN4S4CRTest3
INSERT INTO WactionsN4S4CRTest3
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

END FLOW DataProcessingN4S4CRTest3;

END APPLICATION N4S4CRTest3;

drop user dtest;
drop namespace dtest cascade;

create user dtest identified by test;
grant Global.appuser to user dtest;
connect dtest test;
create cq permissioncq select * from banker.oneWS where ( bankID = :bI);

Stop application PosAppToFW;
UNDEPLOY APPLICATION PosAppToFW;
DROP APPLICATION PosAppToFW CASCADE;

CREATE APPLICATION PosAppToFW RECOVERY 1 MINUTE INTERVAL AUTORESUME MAXRETRIES 2 RETRYINTERVAL 60;

CREATE source CsvPosDataSource USING FileReader (
  WildCard: '',
  directory: '',
positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE OR REPLACE CQ CsvPosAppCq 
INSERT INTO FromCsvPosAppCq 
SELECT TO_STRING(data[1]) as merchantId,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;


CREATE TARGET FileWriterTarget USING Global.FileWriter ( 
 
  flushpolicy: 'EventCount:1000,Interval:30s', 
  directory: '',
  filename: '' ) 
FORMAT USING Global.DSVFormatter  ( 
  quotecharacter: '\"', 
  columndelimiter: ',', 
  usequotes: 'false', 
  rowdelimiter: '\n', 
  standard: 'none', 
  header: 'false' ) 
INPUT FROM FromCsvPosAppCq;

end application PosAppToFW;
deploy application PosAppToFW;
start application PosAppToFW;

create or replace PROPERTYVARIABLE SRC_PASSWORD='@PROP_VAR@';
CREATE APPLICATION @WRITERAPPNAME@ RECOVERY 10 SECOND INTERVAL;
-- USE EXCEPTIONSTORE;

CREATE SOURCE @SOURCE@ USING Ojet
(
FetchSize:1,
Username:'@SOURCE_USER@',
Password:'@pass@',
--Password:'$SRC_PASSWORD',
ConnectionURL:'@CONNECTION_URL@',
Tables:'@SOURCE_TABLE@',
)
OUTPUT TO @STREAM1@;

CREATE OR REPLACE TYPE @TYPE@( 
datae java.util.HashMap , 
TABLE_NAME java.lang.String , 
OPS_NAME java.lang.String , 
DB_TIMESTAMP java.lang.String  ,
COMMITSCN java.lang.String ,
SCN java.lang.String ,
REC_INS_TIME java.lang.String );

CREATE CQ @CQ1@
INSERT INTO @STREAM2@
SELECT  
CASE WHEN (META(c,"OperationName").toString() == "DELETE")
THEN putUserData(c, 'isDelete', 'true') 
ELSE
putUserData(c,'isDelete', 'false')
END
FROM @STREAM1@ c;

CREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;

CREATE OR REPLACE CQ @CQ2@ 
INSERT INTO @STREAM3@
SELECT 
data(e),
META(e,"TableName").toString() as TABLE_NAME,
META(e, "OperationName").toString() as OPS_NAME,
META(e, "TimeStamp").toString() as DB_TIMESTAMP,
META(e,"COMMITSCN").toString() as COMMITSCN ,
META(e,"SCN").toString() as  SCN ,
DNOW().toString() as REC_INS_TIME
FROM @STREAM1@ e;

create Target @TARGET1@ using KafkaWriter VERSION @kafakaversion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC1',
ParallelThreads:'',
PartitionKey:'@metadata(TableName)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from @STREAM1@;

create Target @TARGET2@ using KafkaWriter VERSION @kafakaversion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC2',
ParallelThreads:'2',
PartitionKey:'TABLE_NAME',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from @STREAM3@;

create Target @TARGET3@ using KafkaWriter VERSION @kafakaversion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC3',
ParallelThreads:'',
PartitionKey:'@userdata(isDelete)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest.avsc')
input from @STREAM2@;

end application @WRITERAPPNAME@;

CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafakaversion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC1',
        startOffset:0       
)
PARSE USING DSVParser ()
OUTPUT TO KafkaReaderStream1;

CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
filename:'@READERAPPNAME@_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream1;

CREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafakaversion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC2',
        startOffset:0          
)
PARSE USING JSONParser ()
OUTPUT TO KafkaReaderStream2;

CREATE TARGET kafkaDumpJSON USING FileWriter(
filename:'@READERAPPNAME@_RT_JSON')
FORMAT USING JSONFormatter()
INPUT FROM KafkaReaderStream2;

CREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafakaversion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC3',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest.avsc'
)
OUTPUT TO KafkaReaderStream3;

CREATE CQ CQAvro_Json 
INSERT INTO Avro_Json  
SELECT AvroToJSON(u.data) FROM KafkaReaderStream3 u;;

CREATE TARGET kafkaDumpAVRO USING FileWriter(
filename:'@READERAPPNAME@_RT_AVRO')
FORMAT USING Global.JSONFormatter  ()
INPUT FROM Avro_Json;

end application @READERAPPNAME@;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR;

Create Source s1 Using IncrementalBatchReader (
 FetchSize: 1,
  Username: 'striim',
  Password: 'o4l1uMpwIDQ=',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.autotest01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.autotest01=id',
  startPosition: '%=0'
 )
OUTPUT TO data_stream1;

create source s2 using IncrementalBatchReader (
FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.autotest02',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.autotest02=id',
  startPosition: '%=0'
 )
OUTPUT TO data_stream2;

create source s3 using IncrementalBatchReader (
FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.autotest03',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.autotest03=id',
  startPosition: '%=0'
 )
OUTPUT TO data_stream3;

Create Type EventType (
ID int,
PIN int
);

CREATE STREAM insertData1  of EventType;
CREATE STREAM deleteData1 of EventType;
CREATE STREAM joinData1 of EventType;
CREATE STREAM joinData2 of EventType;
CREATE STREAM deleteData2 of EventType;
CREATE STREAM OutStream of EventType;

CREATE CQ cq1 INSERT INTO insertData1  SELECT TO_INT(data[0]),TO_INT(data[1]) FROM data_stream1;

CREATE CQ cq2 INSERT INTO deleteData1 SELECT TO_INT(data[0]),TO_INT(data[1]) FROM data_stream2;

CREATE CQ cq3 INSERT INTO joinData1 SELECT TO_INT(data[0]),TO_INT(data[1]) FROM data_stream3;

CREATE JUMPING WINDOW DataWin1 OVER deleteData1 KEEP 1 ROWS;

CREATE CQ TypedEventKeyCQ6 INSERT INTO deleteData2 SELECT * from DataWin1;

CREATE JUMPING WINDOW DataWin2 OVER joinData1 KEEP 1 ROWS;

CREATE CQ TypedEventKeyCQ5 INSERT INTO joinData2 SELECT * from DataWin2;

CREATE EVENTTABLE ETABLE1 using STREAM ( NAME: 'insertData1 ' )
DELETE using STREAM ( NAME: 'deleteData1')
QUERY (keytomap:"ID", persistPolicy: 'true') OF EventType;

CREATE CQ cq4 INSERT INTO OutStream SELECT B.ID,B.PIN FROM joinData2 A, ETABLE1 B where A.ID=B.ID;

CREATE TARGET EventTableFW USING FileWriter
(filename:'BasicIR_RT.log',
 rolloverpolicy: 'EventCount:1000000')
FORMAT USING DSVFormatter () INPUT FROM OutStream;

create target Target_Azure using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'STRIIM',
        password: 'W3b@ct10n',
        AccountName: 'striimqatestdonotdelete',
        accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables:'dbo.autotest01',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM OutStream;

END APPLICATION IR;
deploy application IR in default;
start IR;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.test01',
	FetchSize: '1',
	connectionRetryPolicy:'timeOut=00,retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(id,col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'Eventcount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true		
) INPUT FROM @APPNAME@_ss;
--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

CREATE APPLICATION @APPNAME@ RECOVERY 5 second interval;
CREATE SOURCE @APPNAME@_src USING OracleReader ()
OUTPUT TO @APPNAME@_Stream;

CREATE TARGET @APPNAME@_trgt USING AzureBlobWriter()
format using DSVFormatter ()
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

CREATE APPLICATION  @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE  @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'JsonNodeEvent.json',
positionByEOF:false
)
PARSE USING Global.JSONParser (
 )  OUTPUT TO  @AppName@_rawstream;


CREATE CQ @BuiltinFunc@CQ
INSERT INTO  @BuiltinFunc@_Stream
SELECT @BuiltinFunc@(x, 'Sno', data.get("_id"), 'Name', data.get("firstname"))
FROM @AppName@_rawstream x;

CREATE OR REPLACE TARGET  @AppName@_FileTarget USING Global.FileWriter (
  flushpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
 directory: '@logs@',
  filename: '@BuiltinFunc@_JsonNodeEventData',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  (
  handler: 'com.webaction.proc.JSONFormatter',
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  formatterName: 'JSONFormatter',
  jsonobjectdelimiter: '\n' )
INPUT FROM @BuiltinFunc@_Stream;

End application  @AppName@;
Deploy application  @AppName@;
Start application  @AppName@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[0]) != '2';

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

create Target @TARGET_NAME@ using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (

)
input from @STREAM@;

stop @appname@;
undeploy application @appname@;
DROP APPLICATION @appname@ CASCADE;
CREATE APPLICATION @appname@;

CREATE SOURCE @appname@_src USING databaseReader  (
  Username: '@@',
  Password: '@@',
  ConnectionURL: '@@',
  Tables: '@@',
  FetchSize: '100'
 )
OUTPUT TO @appname@_ss;

----1st set of window and cache

CREATE JUMPING WINDOW @appname@_win OVER @appname@_ss KEEP @winsize@ ROWS;

CREATE TYPE @appname@_MapType
    (   
       id INTEGER,
        name STRING,
        city  STRING
    );
    
CREATE EXTERNAL CACHE @appname@_cach (
  AdapterName: 'DatabaseReader',
    ConnectionURL: '@url@',
    UserName: '@uname@',
    Password: '@pwd@',
   Table: '@tablename@2',
  FetchSize: 100,
  Columns: 'id,name,city',
  trimquote: false,
  KeyToMap: '@key@'
 )
 OF @appname@_MapType;   
 
CREATE TYPE @appname@_MapTypenew
    (   id_t            INTEGER,
        name_t           STRING,
        city_t            STRING,
        id_c            INTEGER,
        name_c            STRING,
        city_c            STRING
    );
    
CREATE STREAM @appname@_JoinedData OF @appname@_MapTypenew;

CREATE CQ @appname@_JoinDataCQ
INSERT INTO @appname@_JoinedData
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_STRING(f.data[2]),
        z.id,
        z.name,
        z.city
FROM @appname@_win f, @appname@_cach z
where TO_INT(f.data[0]) = z.id
@Ex@;

----2nd set of window and cache

CREATE JUMPING WINDOW @appname@_win2 OVER @appname@_ss KEEP @winsize@ ROWS;
 
 CREATE EXTERNAL CACHE @appname@_cach2 (
  AdapterName: 'DatabaseReader',
    ConnectionURL: '@url@',
    UserName: '@uname@',
    Password: '@pwd@',
   Table: '@tablename@3',
  FetchSize: 100,
  Columns: 'id,name,city',
  trimquote: false,
  KeyToMap: '@key@'
 )
 OF @appname@_MapType;
 
CREATE CQ @appname@_JoinDataCQ2
INSERT INTO @appname@_JoinedData
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_STRING(f.data[2]),
        z.id,
        z.name,
        z.city
FROM @appname@_win2 f, @appname@_cach2 z
where TO_INT(f.data[0]) = z.id
@Ex@;


----3rd set of window and cache

CREATE JUMPING WINDOW @appname@_win3 OVER @appname@_ss KEEP @winsize@ ROWS;
 
 CREATE EXTERNAL CACHE @appname@_cach3 (
  AdapterName: 'DatabaseReader',
    ConnectionURL: '@url@',
    UserName: '@uname@',
    Password: '@pwd@',
   Table: '@tablename@4',
  FetchSize: 100,
  Columns: 'id,name,city',
  trimquote: false,
  KeyToMap: '@key@'
 )
 OF @appname@_MapType;
 
CREATE CQ @appname@_JoinDataCQ3
INSERT INTO @appname@_JoinedData
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_STRING(f.data[2]),
        z.id,
        z.name,
        z.city
FROM @appname@_win3 f, @appname@_cach3 z
where TO_INT(f.data[0]) = z.id
@Ex@;


CREATE TARGET @appname@_tgt USING DatabaseWriter
(
  ConnectionURL:'@@',
  Username:'@@',
  Password:'@@',
  BatchPolicy:'Eventcount:10000,Interval:1',
  CommitPolicy:'Interval:1,Eventcount:10000',
  Tables:'@@'
) 
INPUT FROM @appname@_JoinedData;

END APPLICATION @appname@;
deploy application @appname@;
start @appname@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING MSSqlReader
(
  Compression: false,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.test01',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'NULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true	,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;
--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

--
-- Canon Test W72
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for sliding count window
--
-- S -> SWc5p -> CQ1(aggregate) -> WS
-- S -> SWc2p -> CQ1(aggregate) -> WS
--


UNDEPLOY APPLICATION NameW72.W72;
DROP APPLICATION NameW72.W72 CASCADE;
CREATE APPLICATION W72 RECOVERY 5 SECOND INTERVAL;




CREATE FLOW DataAcquisitionW72;


CREATE SOURCE CsvSourceW72 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW72;


END FLOW DataAcquisitionW72;




CREATE FLOW DataProcessingW72;

CREATE TYPE DataTypeW72 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW72 OF DataTypeW72
PARTITION BY word;

CREATE CQ CSVStreamW72_to_DataStreamW72
INSERT INTO DataStreamW72
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW72;

CREATE JUMPING WINDOW JWc5pW72
OVER DataStreamW72
KEEP 5 ROWS
PARTITION BY word;

CREATE JUMPING WINDOW JWc2pW72
OVER DataStreamW72
KEEP 2 ROWS
PARTITION BY word;

CREATE WACTIONSTORE WactionStoreW72 CONTEXT OF DataTypeW72
EVENT TYPES ( DataTypeW72 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWc5pW72_to_WactionStoreW72
INSERT INTO WactionStoreW72
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc5pW72 p;

CREATE CQ JWc2pW72_to_WactionStoreW72
INSERT INTO WactionStoreW72
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc2pW72;

END FLOW DataProcessingW72;



END APPLICATION W72;

STOP APPLICATION ORACLETOBIGQUERY;
UNDEPLOY APPLICATION ORACLETOBIGQUERY;
DROP APPLICATION ORACLETOBIGQUERY CASCADE;

--create application 
CREATE APPLICATION OracleToBigquery RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE OracleSource USING OracleReader (
 ConnectionURL: '192.168.123.30:1521:ORCL',
 Tables: 'QATEST.E1PLOADTEST',
 Username: 'qatest',
 Password: 'qatest',
 FetchSize:100,
 OnlineCatalog:true,
 QueueSize:2000,
 CommittedTransactions:true,
 Compression:false
) OUTPUT TO CDCStream;

CREATE OR REPLACE TARGET bqtables using BigqueryWriter(
 BQServiceAccountConfigurationPath:"/Users/ravipathak/Downloads/bqtest-e287bcb47998.json",
 projectId:"bqtest-158706",
 Tables: "QATEST.E1PLOADTEST,issues.DEV11070",
 BatchPolicy: "eventCount:100,Interval:1")
INPUT FROM CDCStream;

CREATE OR REPLACE TARGET T1 using SysOut(
name : "some text"
)
INPUT FROM CDCStream;

END APPLICATION OracleToBigquery;

DEPLOY APPLICATION OracleToBigquery;
START APPLICATION OracleToBigquery;

--
-- Crash Recovery Test 3 on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW -> CQ(aggregate) -> WS
--

STOP APPLICATION N4S4CR3Tester.N4S4CRTest3;
UNDEPLOY APPLICATION N4S4CR3Tester.N4S4CRTest3;
DROP APPLICATION N4S4CR3Tester.N4S4CRTest3 CASCADE;
CREATE APPLICATION N4S4CRTest3 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest3;

CREATE SOURCE CsvSourceN4S4CRTest3 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest3;

CREATE FLOW DataProcessingN4S4CRTest3;

CREATE TYPE WactionTypeN4S4CRTest3 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount int,
  city String
);

CREATE STREAM DataStream OF WactionTypeN4S4CRTest3;

CREATE CQ CsvToDataN4S4CRTest3
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_INT(TO_DOUBLE(data[7])),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN4S4CRTest3 CONTEXT OF WactionTypeN4S4CRTest3
EVENT TYPES ( WactionTypeN4S4CRTest3 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN4S4CRTest3
INSERT INTO WactionsN4S4CRTest3
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

END FLOW DataProcessingN4S4CRTest3;

END APPLICATION N4S4CRTest3;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;


CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',
					acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

CREATE OR REPLACE STREAM @APPNAME@_stream OF Global.waevent persist using @APPNAME@_KafkaPropset;

CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @APPNAME@_stream2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNEC4TION_URL@',
  Tables: '$table1',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING SnowflakeWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'QATEST.ORCTOSFPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING SnowflakeWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo) input from @STREAM@;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @APPNAME@_stream3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '$table2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@4',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING SnowflakeWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'QATEST.ORCTOSFPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING SnowflakeWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@3;

END APPLICATION @APPNAME@2;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;
Create Source OS Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;

CREATE TARGET T using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.WAEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (
  ProviderName: '',
  Provider: '',
  Ctx: '',
  QueueName: '',
  Topic:'',
  UserName: '',
  Password: '',
  EnableTransaction: '',
  transactionpolicy: ''
 )
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING DSVFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream@RANDOM@;

END APPLICATION @APPNAME@;

stop MySQLToHBase;
undeploy application MySQLToHBase;
drop application MySQLToHBase cascade;

CREATE APPLICATION MySQLToHBase RECOVERY 5 SECOND INTERVAL;;

CREATE SOURCE MySQLCDCIn USING MySQLReader (
Username:'root',
Password:'root',
ConnectionURL:'mysql://192.168.123.14:3306',
Database:'qatest',
Tables:'qatest.BUSINESS'
)
OUTPUT TO DataStream;

CREATE TARGET MySQLCDCOut
USING SysOut(name:MySQLCDC)
INPUT FROM DataStream;

CREATE OR REPLACE TARGET Target2 using HBaseWriter(
  HBaseConfigurationPath:"/Users/ravipathak/soft/hbase-1.1.5/conf/hbase-site.xml",
  Tables: "qatest.BUSINESS,PKTEST.data",
  PKUpdateHandlingMode: "DELETEANDINSERT",
  BatchPolicy: "eventCount:1")
INPUT FROM DataStream;

END APPLICATION MySQLToHBase;

deploy application MySQLToHBase;
start MySQLToHBase;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

cCREATE TARGET @TARGET@ USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: 'access_key',
	  --secretaccesskey: 'secret_access',
	  Tables: 'QATEST.%,QATEST.%',
	   S3IAMRole:'@IAMROLE@',
	uploadpolicy:'EventCount:7'
	) INPUT FROM @STREAM@;
	
end flow @APPNAME@_serverflow;

end application @APPNAME@;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@ (a String, b String ,c String,g String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]) where TO_String(data[0]) > '1' ;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

stop application ADW1;
undeploy application ADW1;
drop application ADW1 cascade;
CREATE APPLICATION ADW1;

CREATE  SOURCE SqlServerInitialLoad1 USING DatabaseReader  
 (
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'@SOURCE-TABLES@',
 FetchSize:2000
) 
OUTPUT TO InitialLoadStream1;

CREATE TARGET AzureDWInitialLoad1 USING AzureSQLDWHWriter(
ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
)
INPUT FROM InitialLoadStream1;

END APPLICATION ADW1;
deploy application ADW1;
start application ADW1;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

CREATE SOURCE @APPNAME@_Source USING MSSqlReader
(
  Compression: false,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.test01',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
)
OUTPUT TO @APPNAME@_stream;

CREATE TARGET @APPNAME@_Target USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	BatchPolicy: 'EventCount: 1; Interval: 1s',
	instanceId: 'qatest'
) INPUT FROM @APPNAME@_stream;

create Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_stream;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

--
-- Recovery Test 23 with two sources, two sliding time windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> St1W -> CQ1 -> WS
-- S2 -> St2W -> CQ2 -> WS
--

STOP KStreamRecov23Tester.KStreamRecovTest23;
UNDEPLOY APPLICATION KStreamRecov23Tester.KStreamRecovTest23;
DROP APPLICATION KStreamRecov23Tester.KStreamRecovTest23 CASCADE;
DROP USER KStreamRecov23Tester;
DROP NAMESPACE KStreamRecov23Tester CASCADE;
CREATE USER KStreamRecov23Tester IDENTIFIED BY KStreamRecov23Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov23Tester;
CONNECT KStreamRecov23Tester KStreamRecov23Tester;

CREATE APPLICATION KStreamRecovTest23 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 1 SECOND;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 2 SECOND;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION KStreamRecovTest23;

stop OracleReaderToDBWriter;
undeploy application OracleReaderToDBWriter;
drop application OracleReaderToDBWriter cascade;

CREATE APPLICATION  OracleReaderToDBWriter RECOVERY 1 MINUTE INTERVAL AUTORESUME MAXRETRIES 2 RETRYINTERVAL 60;

CREATE FLOW Hz_Agent_flow;

Create Source Oraclesrc
 Using OracleReader
(
 Username:'',
 Password:'',
 ConnectionURL:'',
 Tables:'',
 Fetchsize:1
)
Output To OrcStrm;

END FLOW Hz_Agent_flow;


CREATE TARGET OracleTrg USING DatabaseWriter(
ConnectionURL:'',
  Username:'',
  Password:'',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: ''
) 
INPUT FROM OrcStrm;

END APPLICATION OracleReaderToDBWriter;
DEPLOY APPLICATION OracleReaderToDBWriter with Hz_Agent_flow on any in AGENTS;
start application OracleReaderToDBWriter;

-- The PosApp sample application demonstrates how a credit card
-- payment processor might use Striim to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.

stop admin.PosApp;
undeploy application admin.PosApp;
drop application admin.PosApp cascade;

CREATE APPLICATION PosApp;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING FileReader (
  directory:'Samples/Customer/PosApp/appData',
  wildcard:'$wildcard',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells Striim that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells Striim to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvToPosData

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP.
--
-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as
-- the types to be used in PosDataStream, which is created automatically.

-- The DATETIME field from the source is converted to both a dateTime
-- value, used as the event timestamp by the application, and (via the
-- function) an integer hourValue, which is used to look up
-- historical hourly averages from the HourlyAveLookup cache,
-- discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)
-- 17 (hourValue)
-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateOnly
--
-- The PosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: 'Samples/Customer/PosApp/appData',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startingTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;


-- WAction store MerchantActivity
--
-- The following group of statements create and populate the MerchantActivity
-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced
-- with merchant details from NameLookup cache and with latitude and longitude
-- values from the USAddressData cache.

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );


CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'Samples/Customer/PosApp/appData',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false
)
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: 'Samples/Customer/PosApp/appData',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;


-- CQ GenerateAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;



--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM AlertStream;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;


END APPLICATION PosApp;


CREATE DASHBOARD USING "Samples/Customer/PosApp/PosAppDashboard.json";

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]);

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

STOP BuiltInTester.test_todate_builtinfunc;
UNDEPLOY APPLICATION BuiltInTester.test_todate_builtinfunc;
DROP APPLICATION BuiltInTester.test_todate_builtinfunc CASCADE;

CREATE APPLICATION test_todate_builtinfunc;

CREATE OR REPLACE TYPE date_cq_Type (
 gapdate org.joda.time.DateTime,
 randomdate org.joda.time.DateTime);

CREATE SOURCE date_file USING FileReader (
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  directory: '@TEST-DATA-PATH@',
  skipbom: true,
  wildcard: 'anomalyBound.csv'
 )
 PARSE USING DSVParser  (
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: '-1',
  nocolumndelimiter: false,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: true,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 )
OUTPUT TO date_file_stream;

CREATE OR REPLACE WACTIONSTORE datef_ws CONTEXT OF date_cq_Type EVENT TYPES ( date_cq_Type ) USING ( storageProvider: 'elasticsearch' );

CREATE OR REPLACE CQ date_cq INSERT INTO datef_ws SELECT TO_DATEF("1983-04-01", 'yyyy-MM-dd') as gapdate, TO_DATEF("2021-02-18", 'yyyy-MM-dd') as randomdate FROM date_file_stream d;

END APPLICATION test_todate_builtinfunc;

stop application GGTrailReaderApp;
undeploy application GGTrailReaderApp;
drop application GGTrailReaderApp cascade;

create application GGTrailReaderApp recovery 5 second interval;

create source GGTrailSource using GGTrailReader (
tRaildIrectory:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario1',
tRAilfilepattern:'n1*',
positionByEOF:false,
FilterTransactionBoundaries: true,
DefinitionFile:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario1/Scn1_beforeddl.def',
captureCDdl: true,
CDDLAction:'Process',
--CDDLAction:'Ignore',
TrailByTeOrder:'LittleEndian',
recoveryInterval: 5
)
OUTPUT TO GGTrailStream;

create Target t2 using SysOut(name:Foo2) input from GGTrailStream;

CREATE TARGET WriteCDCOracle1 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost/orcl',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1,Interval:1',
CommitPolicy:'Eventcount:1,Interval:1',
Checkpointtable:'RGRN_CHKPOINT',
Tables:'QATEST.GGDDL1,QATEST.GGDDL1_TGT'
) INPUT FROM GGTrailStream1;


end application GGTrailReaderApp;

deploy application GGTrailReaderApp;
start application GGTrailReaderApp;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@;

CREATE SOURCE @SOURCE_NAME@2 USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;

Create Source @SourceName@ Using OracleReader

(
  Username:'c##qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',
  Tables:'CDB$ROOT."C##QATEST".ojet_src;ORCLPDB.QATEST.ojet_src',
  _h_useClassic:false,
  Fetchsize:1,
  Compression: true,
  SupportPDB:true,
  ReplicationSlotName:'null'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',
  Username:'c##qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'CDB$ROOT."C##QATEST".ojet_src,CDB$ROOT."C##QATEST".ojet_tgt'
) INPUT FROM @SRCINPUTSTREAM@;



create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

CREATE APPLICATION FileSource;

CREATE  TYPE FileStr2_Type  ( seq java.lang.Integer
 );

CREATE STREAM FileStr2 OF FileStr2_Type;

CREATE OR REPLACE JUMPING WINDOW FileDataAggregation OVER FileStr2 KEEP 1000000 ROWS;

CREATE OR REPLACE SOURCE FileSource USING FileReader  (
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '@TEST-DATA-PATH@/Validate-Striim',
  skipbom: true,
  wildcard: 'FiletoRead.txt'
 )
 PARSE USING DSVParser  (
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: '-1',
  nocolumndelimiter: true,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: false,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 )
OUTPUT TO FileStr1 ;

CREATE OR REPLACE CQ FileDataConvert
INSERT INTO FileStr2
SELECT TO_INT(data[0]) as seq
FROM FileStr1;

CREATE  TYPE FileStr3_Type  ( SUMFileDataAggregationseq java.lang.Long );

CREATE STREAM FileStr3 OF FileStr3_Type;

CREATE OR REPLACE TARGET FileWrite USING FileWriter  (
  filename: 'SourceResults',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:10000,interval:30',
  adapterName: 'FileWriter',
  directory: '@FEATURE-DIR@/logs',
  rolloverpolicy: 'eventcount:10000,interval:30s'
 )
FORMAT USING DSVFormatter  (   nullvalue: 'NULL',
  standard: 'none',
  handler: 'com.webaction.proc.DSVFormatter',
  formatterName: 'DSVFormatter',
  usequotes: 'false',
  rowdelimiter: '\n',
  quotecharacter: '\"',
  header: 'false',
  columndelimiter: ','
 )
INPUT FROM FileStr3;

CREATE OR REPLACE CQ SumAggregat
INSERT INTO FileStr3
SELECT SUM(FileDataAggregation.seq)
FROM FileDataAggregation;

END APPLICATION FileSource;

--
-- Canon Test W32
-- Nicholas Keene, WebAction, Inc.
--
-- Test having one source leading to two data paths with unpartitioned
-- jumping count windows leading to one waction store.
--
-- S -> JWc5u -> CQ5 -> WS
-- S -> JWc2u -> CQ2 -> WS
--


UNDEPLOY APPLICATION NameW32.W32;
DROP APPLICATION NameW32.W32 CASCADE;
CREATE APPLICATION W32 RECOVERY 5 SECOND INTERVAL;




CREATE FLOW DataAcquisitionW32;


CREATE SOURCE CsvSourceW32 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW32;


END FLOW DataAcquisitionW32;




CREATE FLOW DataProcessingW32;

CREATE TYPE DataTypeW32 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW32 OF DataTypeW32;

CREATE CQ CSVStreamW32_to_DataStreamW32
INSERT INTO DataStreamW32
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW32;

CREATE JUMPING WINDOW JWc5uW32
OVER DataStreamW32
KEEP 5 ROWS;

CREATE JUMPING WINDOW JWc2uW32
OVER DataStreamW32
KEEP 2 ROWS;

CREATE WACTIONSTORE WactionStoreW32 CONTEXT OF DataTypeW32
EVENT TYPES ( DataTypeW32 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWc5uW32_to_WactionStoreW32
INSERT INTO WactionStoreW32
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc5uW32;

CREATE CQ JWc2uW32_to_WactionStoreW32
INSERT INTO WactionStoreW32
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc2uW32;

END FLOW DataProcessingW32;



END APPLICATION W32;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@  RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @srcName@ USING OracleReader (
 Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@'
)
OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@ USING AzureEventHubWriter (
  SASKey:'@saasKey@',
  EventHubNamespace:'@eventhubNamespace@',
  ConsumerGroup:'@consumerGrp@',
  SASPolicyName:'RootManageSharedAccessKey',
  E1P:'true',
  OperationTimeout:'1m',
  ConnectionRetryPolicy:'Retries:0,RetryBackOff:1m',
  EventHubName:'@eventhub@',
  BatchPolicy:'Size:1000000,Interval:1m'
)
FORMAT USING JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  members: 'data',
  jsonobjectdelimiter: '\n' )
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

create source @SOURCE_NAME@ USING MySQLReader 
(
Username: '@READER-UNAME@',
Password: '@READER-PASSWORD@',
ConnectionURL: 'jdbc:mysql://127.0.0.1:3306/@DBName@',
Tables: @WATABLES@,
sendBeforeImage:'true',
FilterTransactionBoundaries: 'true'
) 
OUTPUT TO @STREAM@;

STOP APPLICATION VerticaTester.VW;
UNDEPLOY APPLICATION VerticaTester.VW;
DROP APPLICATION VerticaTester.VW CASCADE;
CREATE APPLICATION VW;

CREATE SOURCE CSVSource USING CSVReader
(
  directory:'@TEST-DATA-PATH@',
  header:No,
  wildcard:'trade.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStream;


CREATE TYPE VerticaStream_t
(
  NameField   String,
  ValueField  String
);

CREATE STREAM VerticaStream OF VerticaStream_t;

CREATE CQ VW_q
  INSERT INTO VerticaStream
  SELECT DATA[0],
         DATA[1]
  FROM CsvStream;

CREATE TARGET WriteToVertica USING VerticaWriter
(
  dbHostName:'@HOST@',
  dbUser:'@USER@',
  dbPassword: '@PASS@',
  dbName: '@DBNAME@',
  tableName: '@SCHEMA@.TRADE'
)
INPUT FROM VerticaStream;

END APPLICATION VW;

create application ConsoleApplication;
create type someType(zip Int);
drop application ConsoleApplication;

Undeploy application bq;
alter application bq;
CREATE OR REPLACE SOURCE S USING OracleReader  ( 
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.TABLE_TEST_1000100',
  DictionaryMode: offlineCatalog,
  FetchSize: '1'
 ) 
OUTPUT TO SS;
alter application bq recompile;
deploy application bq;

create Target @TARGET_NAME@ using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (

)
input from @STREAM@;

CREATE OR REPLACE APPLICATION @appname@ @recovery@ AUTORESUME MAXRETRIES 2 RETRYINTERVAL 60;

CREATE OR REPLACE SOURCE @appname@src USING Global.FileReader (
  adapterName: 'FileReader',
  rolloverstyle: 'Default',
  blocksize: 64,
  networkfilesystem: true,
  wildcard: @wildcard@,
  compressiontype: 'gzip',
  includesubdirectories: false,
  directory: @inp_directory@,
  skipbom: false,
  positionbyeof: false )
PARSE USING Global.DSVParser (
  trimwhitespace: false,
  linenumber: '-1',
  columndelimittill: '-1',
  trimquote: true,
  ignoreemptycolumn: false,
  separator: ':',
  parserName: 'DSVParser',
  quoteset: '\"',
  handler: 'com.webaction.proc.DSVParser_1_0',
  charset: 'UTF-8',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  columndelimiter: '|',
  blockascompleterecord: false,
  rowdelimiter: '\n',
  nocolumndelimiter: false,
  headerlineno: 0,
  header: true )                  
OUTPUT TO @appname@STREAM;
                                                                                                         
CREATE OR REPLACE CQ @appname@CQ
INSERT INTO @appname@CQ_Out
SELECT replaceStringRegex(f,'^$','NULL')
FROM @appname@STREAM f;

CREATE OR REPLACE CQ @appname@CQ1
INSERT INTO @appname@CQ_Out1
SELECT
CASE when TO_STRING(data[0])!='NULL' THEN TO_STRING(data[0]) ELSE data[0] END AS CLIENT_ID,
CASE when TO_STRING(data[1])!='NULL' THEN TO_STRING(data[1]) ELSE data[1] END AS ACCOUNT_NAME,
CASE when TO_STRING(data[2])!='NULL' THEN TO_DATE(data[2], 'MM/dd/yyyy') ELSE data[2] END AS EFFECTIVE_DATE,
CASE when TO_STRING(data[3])!='NULL' THEN TO_DATE(data[3], 'MM/dd/yyyy') ELSE data[3] END AS EXPIRATION_DATE,
CASE when TO_STRING(data[4])!='NULL' THEN TO_STRING(data[4]) ELSE data[4] END AS XREF,
CASE when TO_STRING(data[5])!='NULL' THEN TO_STRING(data[5]) ELSE data[5] END AS XREF_TYPE,
CASE when TO_STRING(data[6])!='NULL' THEN TO_STRING(data[6]) ELSE data[6] END AS XREF_DESCRIPTION,
CASE when TO_STRING(data[7])!='NULL' THEN TO_LONG(data[7]) ELSE data[7] END AS AUD_REC_ID,
CASE when TO_STRING(data[8])!='NULL' THEN TO_LONG(data[8]) ELSE data[8] END AS X_CLIENT_ID,
CASE when TO_STRING(data[9])!='NULL' THEN TO_LONG(data[9]) ELSE data[9] END AS X_XREF_ID,
CASE when TO_STRING(data[10])!='NULL' THEN TO_DATE(data[10], 'MM/dd/yyyy HH:mm:ss') ELSE data[10] END AS AUDIT_DATE,
DNOW() AS UDP_DB2_INSERT_TIMESTAMP,
DNOW() AS UDP_DB2_UPDATE_TIMESTAMP
FROM @appname@CQ_Out f;

CREATE OR REPLACE TARGET @appname@tgt USING Global.FileWriter (
  rolloverpolicy: 'EventCount:150,Interval:120s',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  flushpolicy: 'EventCount:150',
  filename: @tar_filename@,
  directory: @tar_directory@ )
FORMAT USING Global.JSONFormatter  (
   members:'CLIENT_ID,ACCOUNT_NAME,EXPIRATION_DATE,XREF,XREF_TYPE,XREF_DESCRIPTION,AUD_REC_ID,X_CLIENT_ID,X_XREF_ID'

)
INPUT FROM @appname@CQ_Out1;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application APP_KAFKA_DATASOURCES;
undeploy application APP_KAFKA_DATASOURCES;
drop application APP_KAFKA_DATASOURCES cascade;

CREATE APPLICATION APP_KAFKA_DATASOURCES;

CREATE OR REPLACE SOURCE SRC_FR_KAFKA_HOURLYTOTALS USING Global.FileReader (
  adapterName: 'FileReader',
  rolloverstyle: 'Default',
  blocksize: 64,
  skipbom: true,
  wildcard: 'kafka_hourly_total*.txt',
  directory: '@confDir@',
  includesubdirectories: false,
  positionbyeof: false ) 
PARSE USING Global.DSVParser (
  trimwhitespace: false,
  linenumber: '-1',
  columndelimittill: '-1',
  trimquote: true,
  ignoreemptycolumn: false,
  parserName: 'DSVParser',
  quoteset: '\"',
  handler: 'com.webaction.proc.DSVParser_1_0',
  charset: 'UTF-8',
  columndelimiter: ':',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  separator: ',',
  header: false,
  blockascompleterecord: false,
  rowdelimiter: '\n',
  nocolumndelimiter: false,
  headerlineno: 0 )
OUTPUT TO STREAM_SRC_FR_KAFKA_HOURLYTOTALS;

CREATE OR REPLACE TYPE SRC_FILECACHE_KAFKA_HOURLYTOTALS_Type (
 topic java.lang.String KEY,
 timerange java.lang.Integer,
 rawdatacount java.lang.Integer);

CREATE OR REPLACE CQ CQ_FR_KAFKA_HOURLYTOTALS_COLMAP
INSERT INTO STREAM_CQ_FR_KAFKA_HOURLYTOTALS_COLMAP
SELECT to_string(data[0]) as topic, to_int(data[1]) as timerange, to_int(data[2]) as rawdatacount, to_string(to_string(data[0])+ '-' + to_string(data[1])) as TopicTimeRange FROM STREAM_SRC_FR_KAFKA_HOURLYTOTALS s;

CREATE OR REPLACE EVENTTABLE ET_HOURLYTOTALS_KAFKADATA_FILE USING STREAM (
  name: 'STREAM_CQ_FR_KAFKA_HOURLYTOTALS_COLMAP' )
QUERY (
  keytomap: 'TopicTimeRange',
  replicas: 'all',
  persistPolicy: 'true' )
OF STREAM_CQ_FR_KAFKA_HOURLYTOTALS_COLMAP_Type;

CREATE WINDOW SLIDE_WND_HOURLYTOTALS_KAFKADATA_FILE OVER STREAM_CQ_FR_KAFKA_HOURLYTOTALS_COLMAP
KEEP WITHIN 1 DAY
PARTITION BY timerange;

END APPLICATION APP_KAFKA_DATASOURCES;

stop IR;
undeploy application IR;
drop application IR cascade;

CREATE APPLICATION IR;

CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.autotest01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.autotest01=id',
 startPosition: 'striim.autotest01=2',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream;

  create type AutoType(
  ID int,
  col1 int,
  col2 string,
  t1 DateTime,
  t2 DateTime
);

CREATE STREAM CDCdata_stream OF AutoType;

CREATE CQ Lookup
INSERT INTO CDCdata_stream
select data[0],data[1],data[2],data[3],data[4] from data_stream;

CREATE  TARGET AzureSQLDWHTarget1 USING AzureSQLDWHWriter  ( 
  ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
  username: 'striim',
  password: 'W3b@ct10n',
  storageaccount: 'striimqatestdonotdelete',
  accesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
  Tables: 'STRIIM.AUTOTEST01',
  uploadpolicy: 'eventcount:1,interval:10s'
 ) 
INPUT FROM CDCdata_stream;

CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM CDCdata_stream;


END APPLICATION IR;
deploy application IR;
start IR;

STOP APPLICATION @appname@routerApp;
UNDEPLOY APPLICATION @appname@routerApp;
DROP APPLICATION @appname@routerApp CASCADE;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',
  acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

CREATE APPLICATION @appname@routerApp;

CREATE  SOURCE @appname@OraSource USING OracleReader  (
Username: 'qatest',
Password: 'qatest',
ConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',
Tables: 'QATEST.TGT_T%',
 FetchSize:'100'
)
OUTPUT TO @appname@MasterStream1;

-- CREATE STREAM @appname@ss1 OF Global.waevent persist using Global.DefaultKafkaProperties;
-- CREATE STREAM @appname@ss2 OF Global.waevent persist using Global.DefaultKafkaProperties;
-- CREATE STREAM @appname@ss3 OF Global.waevent persist using Global.DefaultKafkaProperties;

CREATE STREAM @appname@ss1 OF Global.waevent PERSIST USING KafkaPropset;
CREATE STREAM @appname@ss2 OF Global.waevent PERSIST USING KafkaPropset;
CREATE STREAM @appname@ss3 OF Global.waevent PERSIST USING KafkaPropset;

CREATE OR REPLACE ROUTER @appname@tablerouter1 INPUT FROM @appname@MasterStream1 s CASE
WHEN meta(s,"TableName").toString()='QATEST.TGT_T1' THEN ROUTE TO @appname@ss1,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T2' THEN ROUTE TO @appname@ss2,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T3' THEN ROUTE TO @appname@ss3,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T4' THEN ROUTE TO @appname@ss4,
ELSE ROUTE TO @appname@ss_else;

create Target @appname@FileTarget_1 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'
)
FORMAT USING dsvFormatter ()
input from @appname@ss1;

create Target @appname@FileTarget_2 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'
)
FORMAT USING dsvFormatter ()
input from @appname@ss2;

create Target @appname@FileTarget_3 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from @appname@ss3;

create Target @appname@FileTarget_4 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from @appname@ss4;


create Target @appname@FileTarget_5 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from @appname@ss_else;


end application @appname@routerApp;
deploy application @appname@routerApp;
start @appname@routerApp;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@@RECOVERY_PROP@;

CREATE OR REPLACE SOURCE @APP_NAME@_src USING FileReader (
directory:'@DIRECTORY@',
WildCard:'posdata5L.csv',
positionByEOF:false
)
parse using DSVParser (
header:true
)
OUTPUT TO @APP_NAME@_Stream;


CREATE OR REPLACE CQ @APP_NAME@_CQ
INSERT INTO @APP_NAME@_Stream2
SELECT data[0] as BUSINESS_NAME,data[1] as MERCHANT_ID,data[2] as PRIMARY_ACCOUNT,
data[3] as POS_DATA_CODE,data[4] as DATE_TIME,data[5] as EXP_DATE,data[6] as CURRENCY_CODE,data[7] as AUTH_AMOUNT,
data[8] as TERMINAL_ID,data[9] as ZIP,data[10] as CITY
FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt USING KuduWriter (
  kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
  pkupdatehandlingmode:'@MODE@',
  tables: '@TARGET_TABLES@',
  batchpolicy: 'EventCount:1,Interval:0'
 )
INPUT FROM @APP_NAME@_Stream2;


END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

CREATE TARGET @TARGET@ USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: 'access_key',
	  --secretaccesskey: 'secret_access',
	  Tables: 'QATEST.%,QATEST.%',
	   S3IAMRole:'@IAMROLE@',
	uploadpolicy:'EventCount:7'
	) INPUT FROM @STREAM@;

end application @APPNAME@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ RECOVERY 30 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;

CREATE FLOW @AppName@_SourceFlow;

CREATE SOURCE @srcName@ USING ZendeskReader ( 
  MaxConnections: 20, 
  Password_encrypted: 'false', 
  ThreadPoolCount: '10', 
  ConnectionTimeOut: 60, 
  ConnectionPoolSize: '20', 
  MigrateSchema: 'true', 
  Mode: 'Automated', 
  authflow: 'false', 
  Username: '@srcusername@', 
  Password: '@srcpassword@', 
  ConnectionRetries: '3', 
  ZendeskObjects: 'sessions;settings;targets;tickets', 
  AccessToken: '', 
  ConnectionUrl: '@connectionUrl@' ) 
OUTPUT TO @outstreamname@;

END FLOW @AppName@_SourceFlow;

CREATE TARGET @tgtName@ USING Global.BigQueryWriter ( 
  Tables: '%,zendeskallresize.%', 
  projectId: '@projectId@', 
  batchPolicy: 'eventcount:10000,interval:60', 
  ServiceAccountKey: '@keyFileName@', 
  streamingUpload: 'true', 
  Mode: 'MERGE', 
  CDDLOptions: '{\"CreateTable\":{\"action\":\"IgnoreIfExists\",\"options\":[{\"CreateSchema\":{\"action\":\"IgnoreIfExists\"}}]}}' ) 
INPUT FROM @outstreamname@;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_SourceFlow;
CREATE OR REPLACE SOURCE @SOURCE@ USING Ojet  (
  FilterTransactionBoundaries: true,
  ConnectionURL: '@OCI-URL@',
  Tables: '@SOURCE_TABLE@',
  Password: '@OJET-PASSWORD@',
  fetchsize: 1,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Username: '@OJET-UNAME@'
 )
OUTPUT TO @STREAM@;
end flow @APPNAME@_SourceFlow;
create flow @APPNAME@_TargetFlow;
CREATE OR REPLACE TARGET @TARGET@1 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'false',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@2 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'false',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'true',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@3 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'true',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;
end flow @APPNAME@_TargetFlow;
END APPLICATION @APPNAME@;

create application CSVToXML;
create source CSVSource using FileReader (
	directory:'Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
	filename:'posdata_XML',
	rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'
)
format using XMLFormatter (
	rootelement:'document',
	elementtuple:'MerchantName:merchantid:text=merchantname'
)
input from TypedCSVStream;
end application CSVToXML;

deploy application CSVToXML;
start application CSVToXML;

--
-- Recovery Test 7
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov7Tester.RecovTest7;
UNDEPLOY APPLICATION Recov7Tester.RecovTest7;
DROP APPLICATION Recov7Tester.RecovTest7 CASCADE;
CREATE APPLICATION RecovTest7 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM wacStream OF WactionType;

CREATE CQ InsertWactions
INSERT INTO wacStream
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE WINDOW waWindow
OVER wacStream KEEP WITHIN 1 SECOND ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT *
FROM waWindow
LINK SOURCE EVENT;

END APPLICATION RecovTest7;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@2;

DROP TYPE MerchantActivityContext;
DROP TYPE MerchantTxRate;
DROP WACTIONSTORE MerchantActivity;

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count int,
  HourlyAve int,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count int,
  totalAmount double,
  hourlyAve int,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);

-- Loading WACTIONSTORE MerchantActivity to be accessed by PosAppWS.tql

CREATE WACTIONSTORE MerchantActivity CONTEXT OF DS.MerchantActivityContext
EVENT TYPES ( DS.MerchantTxRate )
@PERSIST-TYPE@

use PosTester;
DROP STREAM MerchantTxRateOnlyStream;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 10 second interval;
CREATE SOURCE OracleSource USING OracleReader
(
    Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
 Tables:'@TABLES@',
    FetchSize: 1
)
OUTPUT TO CsvStream;

Create Type CSVType (
  tablename String,
  data java.util.HashMap  
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT TO_LOWER(META(s, "TableName").toString()) as tablename,
       DATA(s) as data
FROM CsvStream s;

create Target BlobT using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:5,interval:5s'
)
format using AvroFormatter (
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

--
-- Recovery Test 42 with two sources and two WactionStores. A variety of partitioned windows in between
-- assure that we are testing a complicated recovery scenario.
--
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Stream -> JWc5 -> WS1
--   S2 -> Stream -> JWc10 -> WS2
--

STOP Recov42Tester.RecovTest42;
UNDEPLOY APPLICATION Recov42Tester.RecovTest42;
DROP APPLICATION Recov42Tester.RecovTest42 CASCADE;
CREATE APPLICATION RecovTest42 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10242,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10242,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM DataStreamTop OF CsvData using KafkaProps;

CREATE CQ Csv1ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ Csv2ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;










CREATE JUMPING WINDOW LeftJWc5
OVER DataStreamTop KEEP 5 ROWS;

CREATE JUMPING WINDOW RightJWc10
OVER DataStreamTop KEEP 10 ROWS;



CREATE WACTIONSTORE WactionsLeft CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsRight CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ ToWactionsLeft
INSERT INTO WactionsLeft
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM LeftJWc5 p;

CREATE CQ ToWactionsRight
INSERT INTO WactionsRight
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM RightJWc10 p;

END APPLICATION RecovTest42;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@ Using Ojet
(
Username:'@OJET-UNAME@',
Password:'@OJET-PASSWORD@',
ConnectionURL:'@OCI-URL@',
Tables:'@SourceTable@',
)
Output To @SRCINPUTSTREAM@;
CREATE OR REPLACE STREAM Rstream1 OF Global.WAEvent;
CREATE OR REPLACE OPEN PROCESSOR Open_Processor1 USING PartialRecordPolicy
(
Username: 'qatest_ojet',
Password: 'qatest_ojet',
ConnectionURL: 'jdbc:oracle:oci:@//localhost:1525/orcl',
Tables: 'QATEST.Ojetsrc(ROWIDCOL)'
)
INSERT INTO Rstream1
FROM @SRCINPUTSTREAM@;
CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
Username:'@UN@',
Password:'@PWD@',
BatchPolicy:'EventCount:1,Interval:1',
Tables: '@Tablemapping@'
)INPUT FROM Rstream1;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

--
-- Crash Recovery Test 7 with Jumping window and partitioned on two node cluster with one agent
-- Bert Hashemi, WebAction, Inc.
--
-- S -> KafkaStream -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION KStreamN2S2CR7Tester.KStreamN2S2CRTest7;
UNDEPLOY APPLICATION KStreamN2S2CR7Tester.KStreamN2S2CRTest7;
DROP APPLICATION KStreamN2S2CR7Tester.KStreamN2S2CRTest7 CASCADE;
DROP USER KStreamN2S2CR7Tester;
DROP NAMESPACE KStreamN2S2CR7Tester CASCADE;
CREATE USER KStreamN2S2CR7Tester IDENTIFIED BY KStreamN2S2CR7Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR7Tester;
CONNECT KStreamN2S2CR6Tester KStreamN2S2CR7Tester;

CREATE APPLICATION KStreamN2S2CRTest7 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest7;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream of CsvDataTypeKStreamN2S2CRTest7 using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest7 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;


CREATE TYPE CsvDataTypeKStreamN2S2CRTest7 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE CQ TransferToKafka
INSERT INTO KafkaCsvStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest7;

CREATE FLOW DataProcessingKStreamN2S2CRTest7;

CREATE STREAM DataStream OF CsvDataTypeKStreamN2S2CRTest7 PARTITION BY merchantId;

CREATE CQ CsvToDataKStreamN2S2CRTest7
INSERT INTO DataStream
SELECT
    *
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest7 CONTEXT OF CsvDataTypeKStreamN2S2CRTest7
EVENT TYPES ( CsvDataTypeKStreamN2S2CRTest7 )
@PERSIST-TYPE@

CREATE CQ DataToWactionKStreamN2S2CRTest7
INSERT INTO WactionsKStreamN2S2CRTest7
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingKStreamN2S2CRTest7;

END APPLICATION KStreamN2S2CRTest7;

alter application oraddl;
Create or replace Source Ora Using OracleReader 
(
 Username:'@user-name@',
 Password:'@password@',
 ConnectionURL:'src_url',
 Tables:'QATEST.ORACLEDDL%',
 DictionaryMode:OfflineCatalog,
 DDLCaptureMode : 'All',
 FetchSize:2
) Output To LogminerStream;
ALTER APPLICATION oraddl RECOMPILE;
deploy application oraddl;
START application oraddl;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING XMLParserV2 ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE CQ @APPNAME@_CQ
INSERT INTO @APPNAME@_CQOut
SELECT
data.attributeValue("merchantid") as merchantID,
data.getText() as companyName
FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_CQOut;

END APPLICATION @APPNAME@;

CREATE APPLICATION DsPosApp;

CREATE OR REPLACE CQ CQPosApp
INSERT INTO PosStrm
select * from DS.MerchantActivity
LINK SOURCE EVENT;

-- WACTIONSTORE PosWaction is being loaded from MerchantActivity from DSWaction.tql

CREATE WACTIONSTORE PosWaction CONTEXT OF DS.MerchantActivityContext
EVENT TYPES ( DS.MerchantTxRate )
@PERSIST-TYPE@


CREATE CQ CQPosWaction
INSERT INTO PosWaction
select * from DS.PosStrm
LINK SOURCE EVENT;


END APPLICATION DsPosApp;
DEPLOY APPLICATION DsPosApp;
START APPLICATION DsPosApp;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ recovery 1 second interval;

CREATE OR REPLACE SOURCE @SOURCENAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)

OUTPUT TO @STREAM@ ;

CREATE TARGET @TARGETNAME@ using DatabaseWriter
(
    ConnectionURL: '@TARGETURL',
    username: '@TARGETUSERNAME@',
    Password: '@TARGETPASSWORD@',
    Tables: '@TARGETTABLE@',
    BatchPolicy:'EventCount:1,Interval:1',
    CommitPolicy:'EventCount:1,Interval:1'
)
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

CREATE FLOW @STREAM@_SourceFlow;

CREATE SOURCE @SOURCE_NAME@ USING MSSqlReader (
 Username:'@READER-UNAME@',
 Password:'@READER-PASSWORD@',
 DatabaseName:'@DB-NAME@',
 ConnectionURL:'@CDC-READER-URL@',
 Tables:@WATABLES@,
 ConnectionPoolSize:2,
 Compression:false,
 StartPosition:'EOF'
) OUTPUT TO @STREAM@;

END FLOW @STREAM@_SourceFlow;

Stop application DEV20814.ValidateFile;
Undeploy application DEV20814.ValidateFile;
Drop application DEV20814.ValidateFile cascade;

CREATE APPLICATION ValidateFile;

CREATE STREAM AlertFileStream OF Global.AlertEvent;

CREATE SUBSCRIPTION FileAlert USING WebAlertAdapter( ) INPUT FROM AlertFileStream;

CREATE STREAM jsonFileStream OF Global.JsonNodeEvent;

CREATE  SOURCE readFromFile USING FileReader  (
  blocksize: 64,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  directory:'@TEST-DATA-PATH@',
  skipbom: true,
  wildcard: 'Postgres*'
 )
 PARSE USING JSONParser  (
 )
OUTPUT TO jsonFileStream ;

CREATE OPEN PROCESSOR alertFileOP USING AlertGenerator
(
   messagePrefix:'File: ',
   severity:'info',
   flag:'notify'
)
INSERT INTO AlertFileStream
FROM jsonFileStream;

END APPLICATION ValidateFile;

stop application GCSWriterTest;
undeploy application GCSWriterTest;
drop application GCSWriterTest cascade;
create application GCSWriterTest recovery 1 second interval;
create source GCS_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;
create Target GCSTarget using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@',
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
--members:'data'
)
input from TypedCSVStream;

end application GCSWriterTest;
deploy application GCSWriterTest on all in default;
start application GCSWriterTest;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create or replace propertyvariable ms_pass='@SOURCE_PASS@';
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE @SOURCE@ USING MSSQLReader  ( 
  FilterTransactionBoundaries: true,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Password: '$ms_pass',
  Password_encrypted: 'true',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Username: '@SOURCE_USER@',
  DatabaseName: 'qatest',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@ USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;


END APPLICATION @APPNAME@;

deploy application @APPNAME@ on ANY in default;

start application @APPNAME@;

create target @TARGET_NAME@ using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
) INPUT FROM @STREAM@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using PostgreSQLReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application @appName@;
undeploy application @appName@;
drop application @appName@ cascade;

CREATE OR REPLACE APPLICATION @appName@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @appName@_Source USING PostgreSQLReader (
Username: '@Username@',
ConnectionURL: '@ConnectionURL@',
ReplicationSlotName: '@ReplicationSlotName@',
Tables: '@srcTable@',
Password: '@Password@' )
OUTPUT TO @appName@_Stream;

CREATE OR REPLACE TARGET @appName@_Target USING Global.BigQueryWriter (
ColumnDelimiter: '|',
NullMarker: 'NULL',
streamingUpload: 'false',
projectId: '@projectId@',
Encoding: 'UTF-8',
ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 ,
maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30',
AllowQuotedNewLines: 'false',
CDDLAction: 'Process',
Tables: '@srcTable@,@tgtTable@',
optimizedMerge: 'false',
TransportOptions: 'connectionTimeout=300,
readTimeout=120',
adapterName: 'BigQueryWriter',
Mode: 'APPENDONLY',   StandardSQL: 'true',
ServiceAccountKey: '@GCS-AuthPath@',
BatchPolicy: 'eventCount:100',
QuoteCharacter: '\"' )
INPUT FROM @appName@_Stream;
END APPLICATION @appName@;

alter APPLICATION DBRTOCW;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes1',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource2 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes2',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource3 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes3',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget2 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget3 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:60',
  CommitPolicy: 'EventCount:1,Interval:60',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget4 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:120',
  CommitPolicy: 'EventCount:1,Interval:120',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget5 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:120',
  CommitPolicy: 'EventCount:1,Interval:120',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  ExcludedTables:'QATEST.ORACTOCQL_ALLDATATYPES',
  Password: '+hbb060plSWQwscvI105cg==',
  Password_encrypted: true
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET FWTarget USING FileWriter(
	name:CassandraOuput,
	filename:'OracToFw.log',
	flushpolicy : 'interval:120,eventcount:11',
	rolloverpolicy : 'interval:300s'
)
FORMAT USING DSVFormatter()
INPUT FROM Oracle_ChangeDataStream;
create  or replace Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;


END APPLICATION DBRTOCW;
ALTER APPLICATION DBRTOCW RECOMPILE;
deploy application DBRTOCW in default;
start DBRTOCW;

use PosTester;
alter application PosApp;

CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

end application PosApp;
alter application PosApp recompile;

stop Quiesce_CDC;
undeploy application Quiesce_CDC;
alter application Quiesce_CDC;
CREATE or replace FLOW Quiesce_CDC_flow;
Create or replace Source Quiesce_CDC_Oraclesrc Using oraclereader(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',
 Tables:'QATEST.QUIESCE_TABLE1',
 _h_fetchexactrowcount: 'true'
)
Output To Quiesce_CDC_OrcStrm;
END FLOW Quiesce_CDC_flow;
alter application Quiesce_CDC recompile;
DEPLOY APPLICATION Quiesce_CDC;
start application Quiesce_CDC;

undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;
Create Source @APPNAME@_s Using DatabaseReader
(
 Username:'@UNAME@',
 Password:'@PASSWORD@',
 ConnectionURL:'@URL@',
 --Query: "SELECT * FROM QATEST.AUTHORIZATIONS",
 Tables:'QATEST.HDFS_IL_%',
 FetchSize:1,
 QuiesceOnILCompletion: true
)
Output To @APPNAME@_ss;


create Target @APPNAME@_t using HDFSWriter(
	hadoopurl:'hdfs://localhost:9000/',
	directory: '%@METADATA(TableName)%',
	filename:'%@METADATA(TableName)%',
    rolloverpolicy: 'filesize:500M',
    flushpolicy:'@FLUSHPOLICY@',
    hadoopConfigurationPath:'@CONF@'
	)
format using DSVFormatter (
)input from @APPNAME@_ss;

--create Target t2 using SysOut(name:Foo2) input from DataStream;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadpolicy:'EventCount:7'
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from @STREAM@;

end flow @APPNAME@_serverflow;

end application @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

--CREATE APPLICATION @APPNAME@;
create application @APPNAME@ Recovery 5 second Interval;

--create or replace flow @APPNAME@_agentflow;

CREATE OR REPLACE SOURCE @APPNAME@_source USING OracleReader  (
  Compression: false,
  StartTimestamp: 'null',
  SupportPDB: false,
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  FetchSize: 1,
  DDLCaptureMode: 'All',
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'localhost:1521:xe',
 Tables: 'QATEST.Source1',
-- Tables: 'QATEST.OracToCql_alldatatypes',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  TransactionBufferType: 'Memory',
  DictionaryMode: 'OnlineCatalog',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '1MB'
 )
OUTPUT TO @APPNAME@_stream ;

--end flow @APPNAME@_@APPNAME@_agentflow;

CREATE OR REPLACE TARGET @APPNAME@_target USING CassandraCosmosDBWriter  (
  AccountEndpoint: 'cassandracosmostest.cassandra.cosmos.azure.com',
  AccountKey: 'pqDZvVgbdSCg7VzIzD77dAhPG2odGRZPLhAQA1qnZbAKoIDk6RuQX5r2phbRQFnR1l54qxOcvBXNdz8DeijYIg==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  Tables: 'QATEST.Source1,test.target1',
  adapterName: 'CassandraCosmosDBWriter'
 )
 INPUT FROM @APPNAME@_stream;

 END APPLICATION @APPNAME@;

deploy application @APPNAME@;
 --deploy application @APPNAME@ with agentflow in agents;
 start application @APPNAME@;

STOP APPLICATION TQLwithinTqlTester.TQLwithinTqlApp;
UNDEPLOY APPLICATION TQLwithinTqlTester.TQLwithinTqlApp;
DROP APPLICATION TQLwithinTqlTester.TQLwithinTqlApp CASCADE;

CREATE APPLICATION TQLwithinTqlApp;

@@FEATURE-DIR@/tql/TQLwithinTQL5.tql;

END APPLICATION TQLwithinTqlApp;

--
-- Recovery Test 32 with two sources, two sliding attribute windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sa5W/p -> CQ1 -> WS
-- S2 -> Sa6W/p -> CQ2 -> WS
--

STOP KStreamRecov32Tester.KStreamRecovTest32;
UNDEPLOY APPLICATION KStreamRecov32Tester.KStreamRecovTest32;
DROP APPLICATION KStreamRecov32Tester.KStreamRecovTest32 CASCADE;

DROP USER KStreamRecov32Tester;
DROP NAMESPACE KStreamRecov32Tester CASCADE;
CREATE USER KStreamRecov32Tester IDENTIFIED BY KStreamRecov32Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov32Tester;
CONNECT KStreamRecov32Tester KStreamRecov32Tester;

CREATE APPLICATION KStreamRecovTest32 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION KStreamRecovTest32;

--
-- Recovery Test 37 with two sources, two jumping time windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jt1W/p -> CQ1 -> WS
--   S2 -> Jt2W/p -> CQ2 -> WS
--

STOP Recov37Tester.RecovTest37;
UNDEPLOY APPLICATION Recov37Tester.RecovTest37;
DROP APPLICATION Recov37Tester.RecovTest37 CASCADE;
CREATE APPLICATION RecovTest37 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 1 SECOND
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 2 SECOND
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION RecovTest37;

IMPORT static com.webaction.runtime.converters.DateConverter.*;

DROP APPLICATION SampleCDCReaderApp cascade;

CREATE APPLICATION SampleCDCReaderApp;
create source CDCSource using SampleReader (
	AgentPortNo:'2012',
	AgentIpAddress:'127.0.0.1',
	portno:'2020',
	ipaddress:'10.10.196.103',
	Name:'testsession',
    dataFile:'../conf/data.csv',
    metaFile:'../conf/metadata.csv',
	Tables:'POS;STU') output to dbstream,
	CheckUnSupportedTypeStream MAP (table:'STU');

CREATE TYPE CDCPosData(
    BUSINESSNAME String,
    BUSINESSNAME_HEX String,
    PRIMARYACCOUNTNUMBER String,
    POSDATACODE Integer,
    DATETIME org.joda.time.DateTime,
    EXPDATE String,
    CURRENCYCODE String,
    AUTHAMOUNT Float,
    TERMINALID String,
    ZIP String,
    CITY String,
    OPR String,
    TABLENAME String
);

CREATE STREAM CDCPosDataStream OF CDCPosData;

CREATE JUMPING WINDOW CDCPosDataWindow
OVER CDCPosDataStream KEEP 9 ROWS
PARTITION BY OPR;

CREATE CQ CDCCsvToPosData
INSERT INTO CDCPosDataStream
SELECT TO_STRING(data[0],"UTF-8"),
	   TO_HEX(data[0]),
       data[2],
       TO_INT(data[3]),
	   data[4],
	   data[5],
       data[6],
       TO_FLOAT(data[7]),
	   data[8],
       data[9],
	   data[10],
	META(x,"OperationName").toString(),
	META(x, "TableName").toString()
FROM dbstream x
WHERE not(META(x,"OperationName").toString() = "BEGIN") AND not(META(x,"OperationName").toString() = "COMMIT") AND not(META(x, "TableName").toString() is null) AND META(x, "TableName").toString() = "POS";

CREATE TYPE CDCSampleOperationData(
    TableName String,
    OperationName String,
    Count Integer
);

CREATE STREAM CDCSampleOperationDataStream OF CDCSampleOperationData;
-- PARTITION BY OperationName;


CREATE CQ CDCSampleOperationCheck
INSERT INTO CDCSampleOperationDataStream
SELECT x.TABLENAME,
CASE WHEN x.OPR = 'INSERT' THEN x.OPR
     WHEN x.OPR = 'DELETE' THEN x.OPR
     WHEN x.OPR = 'UPDATE' THEN x.OPR
     ELSE 'UNSUPPORTED OPREATION' END,
CASE WHEN x.OPR = 'INSERT' THEN COUNT(x.OPR)
     WHEN x.OPR = 'DELETE' THEN COUNT(x.OPR)
     WHEN x.OPR = 'UPDATE' THEN COUNT(x.OPR)
     ELSE 0 END
FROM CDCPosDataWindow x
GROUP BY OPR;

CREATE TARGET CDCOperationLog USING LogWriter(
	name:FILECDCP,
	filename:'@FEATURE-DIR@/logs/SampleCDCReaderOperationCheck.log'
--	filename:'a1.log'
) INPUT FROM CDCSampleOperationDataStream;

CREATE TARGET CDCLog USING LogWriter(
  name:SampleCDCReaderApp,
filename:'@FEATURE-DIR@/logs/SampleCDCReaderApp.log'
--  filename:'a.log'
) INPUT FROM CDCPosDataStream;

CREATE TARGET CDCLog1 USING LogWriter(
  name:SampleCDCReaderApp,
  filename:'@FEATURE-DIR@/logs/UnsupportedColumn.log'
--  filename:'a2.log'
) INPUT FROM CDCCheckUnSupportedTypeStream;

END APPLICATION SampleCDCReaderApp;

--deploy application SampleCDCReaderApp.SampleCDCReaderApp in default;

--
-- Recovery Test 5 with Jumping window and partitioned
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP Recov5Tester.RecovTest5;
UNDEPLOY APPLICATION Recov5Tester.RecovTest5;
DROP APPLICATION Recov5Tester.RecovTest5 CASCADE;
CREATE APPLICATION RecovTest5 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvData PARTITION BY merchantId;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION RecovTest5;

stop @appname@;
undeploy application @appname@;
DROP APPLICATION @appname@ CASCADE;
CREATE APPLICATION @appname@;

CREATE SOURCE @appname@_src USING databaseReader  (
  Username: '@@',
  Password: '@@',
  ConnectionURL: '@@',
  Tables: '@@',
  FetchSize: '100'
 )
OUTPUT TO @appname@_ss;

CREATE JUMPING WINDOW @appname@_win OVER @appname@_ss KEEP @winsize@ ROWS;

CREATE TYPE @appname@_MapType
    (   
       id INTEGER,
        name STRING,
        city  STRING
    );
    
CREATE EXTERNAL CACHE @appname@_cach (
  AdapterName: 'DatabaseReader',
    ConnectionURL: '@url@',
    UserName: '@uname@',
    Password: '@pwd@',
   Table: '@tablename@',
  FetchSize: 100,
  Columns: 'id,name,city',
  trimquote: false,
  KeyToMap: '@key@'
 )
 OF @appname@_MapType;
 
CREATE TYPE @appname@_MapTypenew
    (   id_t            INTEGER,
        name_t           STRING,
        city_t            STRING,
        id_c            INTEGER,
        name_c            STRING,
        city_c            STRING
    );
    
CREATE STREAM @appname@_JoinedData OF @appname@_MapTypenew;

CREATE CQ @appname@_JoinDataCQ
INSERT INTO @appname@_JoinedData
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_STRING(f.data[2]),
        z.id,
        z.name,
        z.city
FROM @appname@_win f, @appname@_cach z
where TO_INT(f.data[0]) = z.id
@Ex@;

CREATE TARGET @appname@_tgt USING DatabaseWriter
(
  ConnectionURL:'@@',
  Username:'@@',
  Password:'@@',
  BatchPolicy:'Eventcount:10000,Interval:1',
  CommitPolicy:'Interval:1,Eventcount:10000',
  Tables:'@@'
) 
INPUT FROM @appname@_JoinedData;

END APPLICATION @appname@;
deploy application @appname@;
start @appname@;

STOP OuterJoinTester.OJApp;
UNDEPLOY APPLICATION OuterJoinTester.OJApp;
DROP APPLICATION OuterJoinTester.OJApp cascade;

CREATE APPLICATION OJApp;


CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;


CREATE source wsSource2 USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'bankCards.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO stream2;



CREATE TYPE bankData
(
bankID Integer KEY,
bankName String
);

CREATE TYPE cardData
(
cardID Integer KEY,
cardName String
);


CREATE STREAM wsStream OF bankData;
CREATE STREAM wsStream2 OF cardData;


--Select data from QaStream and insert into wsStream

CREATE CQ csvTobankData
INSERT INTO wsStream
SELECT TO_INT(data[0]), data[1] FROM QaStream;

CREATE CQ csvTobankData2
INSERT INTO wsStream2
SELECT TO_INT(data[0]), data[1] FROM stream2;

CREATE JUMPING WINDOW win1 OVER wsStream KEEP 20 rows;


CREATE JUMPING WINDOW win2 OVER wsStream2 KEEP 4 rows;

END APPLICATION OJApp;

stop application reconnect;
undeploy application reconnect;
drop application reconnect cascade;
CREATE APPLICATION reconnect recovery 1 second interval;

CREATE  SOURCE OracleSource USING OracleReader  ( 
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  ConnectionURL: '@URL@',
  Tables: 'qatest.ReconnectTestSource;qatest.ReconnectTestSourceDummy',
  FetchSize: '@FETCHSIZE@'
 )output to sqlstream;
 --output to sqlstream MAP (table:'qatest.ReconnectTestSource');

CREATE TARGET dbtarget USING DatabaseWriter(
  ConnectionURL:'@URL@',
  Username:'@USERNAME@',
  Password:'@PASSWORD@',
  ConnectionRetryPolicy: 'retryInterval=10s, maxRetries=4',
  BatchPolicy:'EventCount:5000,Interval:30',
  CommitPolicy:'EventCount:30000,Interval:30',
  Tables: '@TABLES@'
 ) INPUT FROM sqlstream;
 
 create target filetgt using filewriter(
 filename:'recovering_validation',
 rolloverpolicy:'eventcount:1000000,interval:100m',
 flushpolicy:'eventcount:1000000,interval:100m'
 )format using dsvformatter()
 input from sqlstream;

--create Target tSysOut using Sysout(name:OrgData) input from sqlstream;
 end application reconnect;
 deploy application reconnect;
 start application reconnect;

CREATE OR REPLACE PROPERTYVARIABLE RetryPolicy='timeOut=10,retryInterval=10,maxRetries=30';
CREATE OR REPLACE PROPERTYVARIABLE KafkaBrokerAddress='localhost:9099';
--CREATE OR REPLACE PROPERTYVARIABLE KafkaConfig='batch.size=1048576;linger.ms=30000';

STOP @WRITERAPPNAME@;
UNDEPLOY APPLICATION @WRITERAPPNAME@;
DROP APPLICATION @WRITERAPPNAME@ CASCADE;

CREATE APPLICATION @WRITERAPPNAME@ Recovery 5 second interval;
create flow @WRITERAPPNAME@AgentFlow;
CREATE SOURCE @WRITERAPPNAME@S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.oracle_kw_quiesce_simple_test%',
	FetchSize: '1',
	connectionRetryPolicy:'$RetryPolicy',
	DictionaryMode: onlineCatalog
)
OUTPUT TO @WRITERAPPNAME@SS;
end flow @WRITERAPPNAME@AgentFlow;
create flow @WRITERAPPNAME@serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;
create stream @WRITERAPPNAME@out_cq_select_SS_1 of global.waevent;

CREATE OR REPLACE CQ @WRITERAPPNAME@cq_select_SS1 
INSERT INTO @WRITERAPPNAME@out_cq_select_SS_1
select *
from @WRITERAPPNAME@ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_QUIESCE_SIMPLE_TEST';


create Target @WRITERAPPNAME@TARGET1 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@WRITERAPPNAME@_dsv_sync_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(COMMIT_TIMESTAMP)',
Mode:'sync',
KafkaConfig: 'batch.size=1048576;linger.ms=300000;')
FORMAT USING dsvFormatter ()
input from @WRITERAPPNAME@ss;

create Target @WRITERAPPNAME@TARGET2 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@WRITERAPPNAME@_json_sync_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(COMMIT_TIMESTAMP)',
Mode:'sync',
KafkaConfig: 'batch.size=1048576;linger.ms=300000;')
FORMAT USING jsonFormatter ()
input from @WRITERAPPNAME@out_cq_select_SS_1;

create Target @WRITERAPPNAME@TARGET3 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@WRITERAPPNAME@_avro_sync_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(COMMIT_TIMESTAMP)',
Mode:'sync',
KafkaConfig: 'batch.size=1048576;linger.ms=300000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTestQuiesce_sync_CQ.avsc')
input from @WRITERAPPNAME@out_cq_select_SS_1;

end flow @WRITERAPPNAME@serverFlow;
end application @WRITERAPPNAME@;
@DEPLOYAPP@;
start @WRITERAPPNAME@;


stop application @READERAPPNAME@;
undeploy application @READERAPPNAME@;
drop application @READERAPPNAME@ cascade;
CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @READERAPPNAME@_DSV_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@WRITERAPPNAME@_dsv_sync_CQ',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO @READERAPPNAME@KafkaReaderStream1;

CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
rolloverpolicy:'filesize:500M',
filename:'@READERAPPNAME@_dsv_sync_CQ')
FORMAT USING DSVFormatter()
INPUT FROM @READERAPPNAME@KafkaReaderStream1;

CREATE SOURCE @READERAPPNAME@_JSON_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@WRITERAPPNAME@_json_sync_CQ',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO @READERAPPNAME@KafkaReaderStream2;

CREATE SOURCE @READERAPPNAME@_AVRO_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@WRITERAPPNAME@_avro_sync_CQ',
        startOffset:0          
)
PARSE USING AvroParser (
schemaFileName:'KafkaAvroTestQuiesce_sync_CQ.avsc')
OUTPUT TO @READERAPPNAME@KafkaReaderStream3;


end application @READERAPPNAME@;
deploy application @READERAPPNAME@;

CREATE OR REPLACE APPLICATION @AppName@;

Create Source @AppName@_source Using OracleReader(
  Username:'@username@',
  Password:'@password@',
  ConnectionURL:'@url@',
  Tables:'@srctableName@',
  Fetchsize:1
)
Output To @AppName@_Stream;
CREATE OR REPLACE TARGET @AppName@_SF_Target USING Global.SnowflakeWriter (
  streamingUpload: 'false',
  useConnectionProfile:'true',
  connectionProfileName: 'admin.@SFCP@',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  columnDelimiter: '|',
  tables: '@srctableName@,@trgtableName@',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:1,interval:5m',
  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',
  externalStageType: 'Local',
  adapterName: 'SnowflakeWriter',
  fileFormatOptions: 'null_if = \"\"' )
INPUT FROM @AppName@_Stream;

CREATE OR REPLACE TARGET @AppName@_SF_Target2 USING Global.SnowflakeWriter (
  streamingUpload: 'false',
  useConnectionProfile:'true',
  connectionProfileName: 'admin.@SFCP@',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  columnDelimiter: '|',
  tables: '@srctableName@,@trgtableName@',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:1,interval:5m',
  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',
  externalStageType: 'Local',
  adapterName: 'SnowflakeWriter',
  fileFormatOptions: 'null_if = \"\"' )
INPUT FROM @AppName@_Stream;

END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING HL7v2Parser (
  EnableMessageValidation: false,
  MLLPDelimited: false
  )
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING XMLFormatter (
    rootelement:"document",
    charset: "UTF-8"
    )
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

stop application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
	directory:'@TEST-DATA-PATH@',
        WildCard:'banks.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'no'
)
OUTPUT TO CsvStream;

CREATE TARGET DBDump USING FileWriter(
filename:'@FEATURE-DIR@/logs/JsonRaw_RT.log', rolloverpolicy:'EventCount:10000,Interval:30s' )Format using JSONFormatter()
INPUT FROM CsvStream;
end application DSV;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.DatabaseReader (
  FetchSize: 1,
  ConnectionURL: '@ORACLE-URL@',
  Tables: '@SOURCE-TABLES@',
  Username: '@ORACLE-USERNAME@',
  Password: '@ORACLE-PASSWORD@'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt USING SnowflakeWriter

(
  ConnectionURL:'@SNOWFLAKE-URL@',
  username:'@SNOWFLAKE-USERNAME@',
  appendOnly:'false',
  Tables:'@TARGET-TABLES@',
  uploadpolicy:'eventcount:3,interval:10s',
  externalStageType:'local'
)
INPUT FROM @APP_NAME@_Stream;


END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

Stop application appname;
undeploy application appname;
drop application appname cascade;

create application appname use exceptionstore;

  


CREATE SOURCE OracleSource USING OracleReader  ( 
Username: 'qatest', 
  Tables: 'QATEST.TEST01', 
  FetchSize: 100, 
  Password_encrypted: 'false', 
  ConnectionURL: 'localhost:1521:xe', 
  DictionaryMode: 'OnlineCatalog', 
  queuesize: 25000, 
  Password: 'qatest'
  ) 
OUTPUT TO SS_oracle;

CREATE TYPE CDCtestnMapType
    (   id INTEGER,
        name STRING,
        cost DOUBLE
    );


CREATE EXTERNAL CACHE c_ext_oracle (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
    DatabaseProviderType: 'Default',
  FetchSize: 1,
  Password: 'qatest',
  Columns: 'id,name,cost',
  Table: 'QATEST.SRCCACHE12',
  trimquote: false,
  Username: 'qatest',
  keytomap: 'id',
  AdapterName:'DatabaseReader',
  connectionRetryPolicy: 'timeOut=5, retryInterval=5, maxRetries=10'
 )
OF CDCtestnMapType;
 

CREATE EXTERNAL CACHE c_oracle  (
 ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  DatabaseProviderType: 'Default',
  FetchSize: 1,
  Password: 'qatest',
  Columns: 'id,name,cost',
  Table: 'QATEST.SRCCACHE12',
  trimquote: false,
  Username: 'qatest',
  keytomap: 'id',
  AdapterName:'DatabaseReader',
  connectionRetryPolicy: 'timeOut=5, retryInterval=5, maxRetries=10' 
 )
 OF  CDCtestnMapType;
 
 CREATE TYPE CDCtestnMapTypenew
    (   id_t INTEGER,
        name_t STRING,
        cost_t DOUBLE,
        id_c INTEGER,
        name_c STRING,
        cost_c DOUBLE
    );
    
create stream JoinedDataStreamOracle of CDCtestnMapTypenew;
create stream JoinedDataStreamOraclenull of CDCtestnMapTypenew;

    
CREATE CQ JoinDataCQOracle1
INSERT INTO JoinedDataStreamOracle
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_DOUBLE(f.data[2]),
        z.id,
        z.name,
        z.cost
FROM SS_oracle f inner join c_ext_oracle z
on TO_INT(f.data[0]) = z.id;

CREATE CQ JoinDataCQOracle2 
INSERT INTO JoinedDataStreamOraclenull
SELECT  TO_INT(s.data[0]),
        TO_STRING(s.data[1]),
        TO_DOUBLE(s.data[2]),
        z.id,
        z.name,
        z.cost
FROM c_oracle f left outer join c_ext_oracle z
on z.id=f.id RIGHT OUTER JOIN SS_oracle s
on  TO_INT(s.data[0])=z.id
WHERE (TO_STRING(META(s, "OperationName")) = "INSERT");



CREATE TARGET Target_DBWriter USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1000,Interval:60',
CommitPolicy:'Eventcount:10,Interval:60',
Tables:'QATEST.test03'
) INPUT FROM JoinedDataStreamOracle;

CREATE TARGET Target_DBWriter2 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1000,Interval:60',
CommitPolicy:'Eventcount:10,Interval:60',
Tables:'QATEST.test04'
) INPUT FROM JoinedDataStreamOraclenull;

end application appname;
deploy application appname;
start appname;

CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( 
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: '@CDC_URL@',
  Tables: '@Source1Tables@',
  FetchSize: 1) 
OUTPUT TO @APPNAME@cdcStream;

--
-- Recovery Test 40 with two sources and two WactionStores. A variety of partitioned windows in between
-- assure that we are testing a complicated recovery scenario.
--
-- NOTE THIS APP IS INCONSISTENT AND NOT COMPATIBLE WITH THE CURRENT VERSION OF RECOVERY BECAUSE IT HAS COMBINING STREAMS
--
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> JWc2 -> JWc5 -> WS1
--   S2 -> JWc2 -> JWc7 -> WS2
--

STOP Recov40Tester.RecovTest40;
UNDEPLOY APPLICATION Recov40Tester.RecovTest40;
DROP APPLICATION Recov40Tester.RecovTest40 CASCADE;
CREATE APPLICATION RecovTest40 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStreamTop OF CsvData;

CREATE CQ Csv1ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ Csv2ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;








CREATE JUMPING WINDOW TopJWc2
OVER DataStreamTop KEEP 2 ROWS;







CREATE STREAM DataStreamLeft OF CsvData;
CREATE STREAM DataStreamRight OF CsvData;

CREATE CQ DataToLeft
INSERT INTO DataStreamLeft
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM TopJWc2 p;

CREATE CQ DataToRight
INSERT INTO DataStreamRight
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM TopJWc2 p;






CREATE JUMPING WINDOW LeftJWc5
OVER DataStreamLeft KEEP 5 ROWS;

CREATE JUMPING WINDOW RightJWc10
OVER DataStreamRight KEEP 10 ROWS;



CREATE WACTIONSTORE WactionsLeft CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsRight CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ ToWactionsLeft
INSERT INTO WactionsLeft
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM LeftJWc5 p;

CREATE CQ ToWactionsRight
INSERT INTO WactionsRight
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM RightJWc10 p;

END APPLICATION RecovTest40;

CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'data.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO @AppName@_rawstream;

CREATE CQ @BuiltinFunc@CQ
INSERT INTO @BuiltinFunc@Stream
SELECT updateUserData(x, 'Last_Date', data[5], 'Country', data[10])
FROM @AppName@_rawstream x;

CREATE OR REPLACE CQ cq1
INSERT INTO clearUserData_Stream
SELECT
clearUserData(s1)
FROM @BuiltinFunc@Stream s1;

CREATE OR REPLACE TARGET @AppName@_FileTarget USING Global.FileWriter ( 
  flushpolicy: 'EventCount:10000,Interval:30s', 
  adapterName: 'FileWriter', 
  directory: '@logs@',
  filename: '@BuiltinFunc@_ClearData', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING Global.JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter', 
  jsonMemberDelimiter: '\n', 
  EventsAsArrayOfJsonObjects: 'true', 
  formatterName: 'JSONFormatter', 
  jsonobjectdelimiter: '\n' ) 
INPUT FROM clearUserData_Stream;

End application @AppName@;
Deploy application @AppName@; 
Start application @AppName@;

Stop @APPNAME@_App;
undeploy application @APPNAME@_App;
drop application @APPNAME@_App cascade;

CREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;
CREATE SOURCE @APPNAME@_Source USING Global.Ojet

(
  Username:'@SOURCE_USER@',
  Password:'@SOURCE_PASSWORD@',
  ConnectionURL:'@SOURCE_URL@',
  Tables:'@SOURCE_TABLES@'
)
OUTPUT TO @APPNAME@_Stream;

CREATE TARGET @APPNAME@_SYS USING SysOut (
  name: '@APPNAME@_SYS' )
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_CDB_Target USING DatabaseWriter

(
  Username:'@TARGET_CDB_USER@',
  ConnectionURL:'@TARGET_CDB_URL@',
  Tables:'@TARGET_CDB_TABLES@',
  Password:'@TARGET_CDB_PASSWORD@',
  BatchPolicy:'EventCount:1,Interval:1',
  CommitPolicy:'EventCount:1,Interval:1'
)
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_PDB_Target USING DatabaseWriter

(
  Username:'@TARGET_PDB_USER@',
  ConnectionURL:'@TARGET_PDB_URL@',
  Tables:'@TARGET_PDB_TABLES@',
  Password:'@TARGET_PDB_PASSWORD@',
  BatchPolicy:'EventCount:1,Interval:1',
  CommitPolicy:'EventCount:1,Interval:1'
)
INPUT FROM @APPNAME@_Stream;

End APPLICATION @APPNAME@_App;

Stop IR;
Undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;
create flow AgentFlow;
CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.119/DBS_PORT=1025',
  Tables: 'striim.upgrade01',
  CheckColumn:'striim.upgrade01=t1',
  startPosition:'striim.upgrade01=2018-09-20 06:43:59',
  ReturnDateTimeAs:'string'
  )
OUTPUT TO data_stream1;
end flow AgentFlow;

create flow serverFlow;
create target AzureSQLDWHTarget using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test26,dbo.test26',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream1;

end flow serverFlow;
END APPLICATION IR;
deploy application IR with AgentFlow in Agents, ServerFlow in default;
start application IR;

--
-- Recovery Test 31 with two sources, two sliding count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5W/p -> CQ1 -> WS
-- S2 -> Sc6W/p -> CQ2 -> WS
--

STOP KStreamRecov31Tester.KStreamRecovTest31;
UNDEPLOY APPLICATION KStreamRecov31Tester.KStreamRecovTest31;
DROP APPLICATION KStreamRecov31Tester.KStreamRecovTest31 CASCADE;

DROP USER KStreamRecov31Tester;
DROP NAMESPACE KStreamRecov31Tester CASCADE;
CREATE USER KStreamRecov31Tester IDENTIFIED BY KStreamRecov31Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov31Tester;
CONNECT KStreamRecov31Tester KStreamRecov31Tester;

CREATE APPLICATION KStreamRecovTest31 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION KStreamRecovTest31;

STOP APPLICATION ORACLETOBIGQUERY;
UNDEPLOY APPLICATION ORACLETOBIGQUERY;
DROP APPLICATION ORACLETOBIGQUERY CASCADE;

--create application 
CREATE APPLICATION ORACLETOBIGQUERY RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE OracleSource USING OracleReader (
 ConnectionURL: '192.168.123.12:1521/ORCL',
 Tables: 'QATEST.ORATOBIGQALLDATATYPE',
 Username: 'qatest',
 Password: 'qatest',
 FetchSize:1
) OUTPUT TO CDCStream;

CREATE OR REPLACE TARGET bqtables using BigqueryWriter(
 BQServiceAccountConfigurationPath:"/Users/karthikmurugan/Downloads/bqtest-540227c31980.json",
 projectId:"bqtest-158706",
 Tables: "QATEST.ORATOBIGQALLDATATYPE,QATEST.ORATOBIGQALLDATATYPE",
 BatchPolicy: "eventCount:1,Interval:90")
INPUT FROM CDCStream;


CREATE OR REPLACE TARGET T1 using SysOut(name : "some text") INPUT FROM CDCStream;

END APPLICATION ORACLETOBIGQUERY;

DEPLOY APPLICATION ORACLETOBIGQUERY;
START APPLICATION ORACLETOBIGQUERY;

create flow serverFlow;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@;

end flow serverFlow;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using AzureblobWriter(
    accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:7'
)
format using DSVFormatter (
)
input from @STREAM@;

end flow @APPNAME@_serverflow;

end application @APPNAME@;

--
-- Recovery Test 36 with two sources, two jumping attribute windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W/p -> CQ1 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS
--

STOP Recov36Tester.RecovTest36;
UNDEPLOY APPLICATION Recov36Tester.RecovTest36;
DROP APPLICATION Recov36Tester.RecovTest36 CASCADE;

DROP USER KStreamRecov36Tester;
DROP NAMESPACE KStreamRecov36Tester CASCADE;
CREATE USER KStreamRecov36Tester IDENTIFIED BY KStreamRecov36Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov36Tester;
CONNECT KStreamRecov36Tester KStreamRecov36Tester;

CREATE APPLICATION KStreamRecovTest36 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION KStreamRecovTest36;

--
-- Recovery Test 1
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov1Tester.RecovTest1;
UNDEPLOY APPLICATION Recov1Tester.RecovTest1;
DROP APPLICATION Recov1Tester.RecovTest1 CASCADE;
CREATE APPLICATION RecovTest1 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END APPLICATION RecovTest1;

CREATE SOURCE @SOURCE_NAME@ USING Global.FileReader (
  positionbyeof: false )
PARSE USING Global.DSVParser (
 )
OUTPUT TO @STREAM@;

CREATE TARGET @SOURCE_NAME@_sysout USING Global.SysOut (
  name: '@SOURCE_NAME@_sysout' )
INPUT FROM @STREAM@;

Create Source @SOURCE_NAME@ Using MSJet
(
 Username:'@READER-UNAME@',
 Password:'@READER-PASSWORD@',
 DatabaseName:'@DB_NAME@',
 ConnectionURL:'@CONN_URL@',
 Tables:@WATABLES@,
 compression:'@COMP@'
)
OUTPUT TO @STREAM@;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@ @APP_PROPERTY@ USE EXCEPTIONSTORE;

CREATE OR REPLACE STREAM @APP_NAME@_DataStreamFromCQ OF Global.WAEvent;

Create Source @APP_NAME@_Source Using @SOURCE_ADAPTER@ (

) OUTPUT TO @APP_NAME@_DataStream;

CREATE OR REPLACE CQ @APP_NAME@_CQ INSERT INTO @APP_NAME@_DataStreamFromCQ SELECT * FROM @APP_NAME@_DataStream s where META(s,"TableName") is not null AND META(s,"TableName").toString().trim().isEmpty() == false;

CREATE TARGET @APP_NAME@_Target USING @TARGET_ADAPTER@ ( 

) INPUT FROM @APP_NAME@_DataStreamFromCQ;

CREATE OR REPLACE TARGET @APP_NAME@_SysOut_ReadFromSource USING Global.SysOut ( 
	name: '@APP_NAME@_SysOutWA_Source' 
) INPUT FROM @APP_NAME@_DataStream;

CREATE OR REPLACE TARGET @APP_NAME@_SysOut_WriteToTarget USING Global.SysOut ( 
	name: '@APP_NAME@_SysOutWA_Target' 
) INPUT FROM @APP_NAME@_DataStreamFromCQ;

END APPLICATION @APP_NAME@;

DEPLOY APPLICATION @APP_NAME@ IN DEFAULT;
START APPLICATION @APP_NAME@;

CREATE TARGET @TARGET_NAME@ using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkuPDaTehAnDliNgmOdE:'DELETEANDINSERT',
tables: 'QATEST.ORCLALLDATATYPES,@TARGET_TABLE@ ColumnMap(Log_Id=LOG_ID,Description=Description,LogChar=LogChar,LogVarchar2=LogVarchar2,LogNumber=LogNumber,LogBinaryFloat=LogBinaryFloat,LogBinaryDouble=LogBinaryDouble,LogFloat=LogFloat,LogDate=LogDate,LogTimestamp=LogTimestamp,LogNchar=LogNchar,LogNvarchar2=LogNvarchar2,LogTimezone=LogTimezone,LogTimelocal=LogTimelocal,LogInterval=LogInterval,LogInterval2=LogInterval2,LogInteger=LogInteger)',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM @STREAM@;

STOP AES;
UNDEPLOY APPLICATION AES;
DROP APPLICATION AES CASCADE;

CREATE APPLICATION AES;


CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity double,
  size long,
  currentDate DateTime);

CREATE source implicitSOurce USING FileReader (
      directory:'@TEST-DATA-PATH@',
      columndelimiter: ',',
      wildcard:'ISdata.csv',
      blocksize: 10240,
      positionByEOF:false
)
PARSE USING DSVParser (
      header:False,
      trimquote:false
) OUTPUT TO CsvStream; 

CREATE TYPE wsType(
  quantity double KEY,
  currentDate DateTime
  );


CREATE STREAM newStream OF Atm;

CREATE CQ newCQ
INSERT INTO newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]), TO_DATE(data[5]) FROM
CsvStream;


CREATE WINDOW win1
OVER newStream
keep within 3 minute;

CREATE STREAM newStream2 of wsType;



CREATE WACTIONSTORE WS1 CONTEXT OF wsType
EVENT TYPES (wsType );


Create cq newCQ2
insert into ws1 (quantity,currentDate)
select quantity, currentDate from newStream;


END APPLICATION AES;
deploy APPLICATION AES;

stop application Postgres_To_Filewriter;
undeploy application Postgres_To_Filewriter;
drop application Postgres_To_Filewriter cascade;

CREATE APPLICATION Postgres_To_Filewriter RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE Postgres_Src USING PostgreSQLReader  ( 
  ReplicationSlotName: '',
  FilterTransactionBoundaries: 'true',
  Username: '',
  Password_encrypted: false,
  ConnectionURL: '',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: '',
  Tables: ''
 ) 
OUTPUT TO Change_Data_Stream;


CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Change_Data_Stream;
 
 CREATE TARGET BinaryDump USING LogWriter(
  name: 'PostgresCDCData',
  filename:'PostgresCDCData.log'
)INPUT FROM Change_Data_Stream;
 

end application Postgres_To_Filewriter;
deploy application Postgres_To_Filewriter;
start Postgres_To_Filewriter;

STOP APPLICATION @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;
CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9099', kafkaversion:'0.11');

CREATE FLOW @APPNAME@_AgentFlow;

CREATE STREAM @APPNAME@_PS_Stream1 OF Global.waevent persist using  @APPNAME@_KafkaPropset;
CREATE STREAM @APPNAME@_PS_Stream2 OF Global.waevent persist using  @APPNAME@_KafkaPropset;
CREATE STREAM @APPNAME@_PS_Stream3 OF Global.waevent persist using  @APPNAME@_KafkaPropset;
CREATE STREAM @APPNAME@_PS_Stream4 OF Global.waevent persist using  @APPNAME@_KafkaPropset;


create source @APPNAME@_source using FileReader (
directory:'/Users/jenniffer/Product2/IntegrationTests/TestData/OGG/Recovery',
WildCard:'lg*.gz',
positionByEOF:false,
compressiontype:'gzip',
recoveryInterval: 5
) parse using GGTrailParser (
metadata:'@META-FILE@'
)
OUTPUT TO @APPNAME@_Stream;

END FLOW @APPNAME@_AgentFlow;

CREATE CQ @APPNAME@_CQ1
INSERT INTO @APPNAME@_PS_Stream1
SELECT *
FROM @APPNAME@_Stream sm
WHERE META(sm, 'TableName').toString() = 'MINER.CUSTOMER';

CREATE CQ @APPNAME@_CQ2
INSERT INTO @APPNAME@_PS_Stream2
SELECT *
FROM @APPNAME@_Stream sm
WHERE META(sm, 'TableName').toString() = 'MINER.CUSTOMER';

CREATE CQ @APPNAME@_CQ3
INSERT INTO @APPNAME@_PS_Stream3
SELECT *
FROM @APPNAME@_Stream sm
WHERE META(sm, 'TableName').toString() = 'MINER.CUSTOMER';

CREATE CQ @APPNAME@_CQ4
INSERT INTO @APPNAME@_PS_Stream4
SELECT *
FROM @APPNAME@_Stream sm
WHERE META(sm, 'TableName').toString() = 'MINER.CUSTOMER';


CREATE FLOW @APPNAME@_ServerFlow;

CREATE TARGET @APPNAME@_target1 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'MINER.CUSTOMER,QATEST.CUSTOMER_TARGET_AG4'
) INPUT FROM @APPNAME@_PS_Stream1;

CREATE TARGET @APPNAME@_target2 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'MINER.CUSTOMER,QATEST.CUSTOMER_TARGET_AG4'
) INPUT FROM @APPNAME@_PS_Stream2;

CREATE TARGET @APPNAME@_target3 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'MINER.CUSTOMER,QATEST.CUSTOMER_TARGET_AG4'
) INPUT FROM @APPNAME@_PS_Stream3;

CREATE TARGET @APPNAME@_target4 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'MINER.CUSTOMER,QATEST.CUSTOMER_TARGET_AG4'
) INPUT FROM @APPNAME@_PS_Stream4;

END FLOW @APPNAME@_ServerFlow;

end application @APPNAME@;

deploy application @APPNAME@ with @APPNAME@_AgentFlow in AGENTS, @APPNAME@_ServerFlow on any in default;

start application @APPNAME@;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 10 second interval;
create source CSVSource using FileReader (
	directory:'@DIR@',
	WildCard:'@WILDCARD@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  curr String,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       data[6],
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target BlobT using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:50,interval:5s'
)
format using JSONFormatter (
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

stop application GCSWriterTest;
undeploy application GCSWriterTest;
drop application GCSWriterTest cascade;
create application GCSWriterTest recovery 1 second interval;
create source GCS_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

create target sys using sysout(name:'raw')input from CsvStream;

create Target GCSTarget using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    foldername:'@foldername@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@'    
)
format using AvroFormatter (
)
input from CsvStream;

end application GCSWriterTest;
deploy application GCSWriterTest on all in default;
start application GCSWriterTest;

stop application Postgres_To_PostgresApp;
undeploy application Postgres_To_PostgresApp;
drop application Postgres_To_PostgresApp cascade;

CREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;

CREATE FLOW agentflow;

CREATE OR REPLACE SOURCE Postgres_Src1 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_1',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename0'
 ) 
OUTPUT TO Change_Data_Stream ;

CREATE OR REPLACE SOURCE Postgres_Src2 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_2',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename1'
 ) 
OUTPUT TO Change_Data_Stream ;


CREATE OR REPLACE SOURCE Postgres_Src3 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_3',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename2'
 ) 
OUTPUT TO Change_Data_Stream ;

CREATE OR REPLACE SOURCE Postgres_Src4 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_4',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename3'
 ) 
OUTPUT TO Change_Data_Stream ;

end flow agentflow;

create flow serverflow;

CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt1 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target0;public.tablename1, public.target0;public.tablename2, public.target0;public.tablename3, public.target0;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt2 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target1;public.tablename1, public.target1;public.tablename2, public.target1;public.tablename3, public.target1;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt3 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target2;public.tablename1, public.target2;public.tablename2, public.target2;public.tablename3, public.target2;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt4 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target3;public.tablename1, public.target3;public.tablename2, public.target3;public.tablename3, public.target3;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

end flow serverflow;
end application Postgres_To_PostgresApp;
deploy application Postgres_To_PostgresApp with agentflow on any in agents, serverflow in default;
start Postgres_To_PostgresApp;







stop application Postgres_To_PostgresApp2;
undeploy application Postgres_To_PostgresApp2;
drop application Postgres_To_PostgresApp2 cascade;

CREATE APPLICATION Postgres_To_PostgresApp2 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW agentflow2;

CREATE OR REPLACE SOURCE Postgres_Src21 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_1',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename0'
 ) 
OUTPUT TO Change_Data_Stream2 ;

CREATE OR REPLACE SOURCE Postgres_Src22 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_2',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename1'
 ) 
OUTPUT TO Change_Data_Stream2 ;


CREATE OR REPLACE SOURCE Postgres_Src23 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_3',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename2'
 ) 
OUTPUT TO Change_Data_Stream2 ;

CREATE OR REPLACE SOURCE Postgres_Src24 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_4',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename3'
 ) 
OUTPUT TO Change_Data_Stream2 ;

end flow agentflow2;

create flow serverflow2;

CREATE OR REPLACE TARGET Postgres_Sys2 USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Change_Data_Stream2;

CREATE OR REPLACE TARGET PostgreSQL_Tgt21 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target0;public.tablename1, public.target0;public.tablename2, public.target0;public.tablename3, public.target0;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream2;

CREATE OR REPLACE TARGET PostgreSQL_Tgt22 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target1;public.tablename1, public.target1;public.tablename2, public.target1;public.tablename3, public.target1;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream2;

CREATE OR REPLACE TARGET PostgreSQL_Tgt23 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target2;public.tablename1, public.target2;public.tablename2, public.target2;public.tablename3, public.target2;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream2;

CREATE OR REPLACE TARGET PostgreSQL_Tgt24 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target3;public.tablename1, public.target3;public.tablename2, public.target3;public.tablename3, public.target3;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream2;

end flow serverflow2;

end application Postgres_To_PostgresApp2;
deploy application Postgres_To_PostgresApp2 with agentflow2 on any in agents, serverflow2 in default;
start Postgres_To_PostgresApp2;

--
-- Recovery Test 20 with two sources going to one wactionstore
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> CQ1 -> WS
-- S2 -> CQ2 -> WS
--

STOP KStreamRecov20Tester.KStreamRecovTest20;
UNDEPLOY APPLICATION KStreamRecov20Tester.KStreamRecovTest20;
DROP APPLICATION KStreamRecov20Tester.KStreamRecovTest20 CASCADE;
DROP USER KStreamRecov20Tester;
DROP NAMESPACE KStreamRecov20Tester CASCADE;
CREATE USER KStreamRecov20Tester IDENTIFIED BY KStreamRecov20Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov20Tester;
CONNECT KStreamRecov20Tester KStreamRecov20Tester;

CREATE APPLICATION KStreamRecovTest20 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions1
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream1;

CREATE CQ InsertWactions2
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream2;

END APPLICATION KStreamRecovTest20;

stop ORAToBigquery;
undeploy application ORAToBigquery;
drop application ORAToBigquery cascade;
CREATE APPLICATION ORAToBigquery RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE Rac11g USING OracleReader ( 
  SupportPDB: false,
  SendBeforeImage: true,
  ReaderType: 'LogMiner',
  CommittedTransactions: false,
  FetchSize: 1,
  Password: 'manager',
  DDLTracking: false,
  StartTimestamp: 'null',
  OutboundServerProcessName: 'WebActionXStream',
  OnlineCatalog: true,
  ConnectionURL: '192.168.33.10:1521/XE',
  SkipOpenTransactions: false,
  Compression: false,
  QueueSize: 40000,
  RedoLogfiles: 'null',
  Tables: 'SYSTEM.GGAUTHORIZATIONS',
  Username: 'system',
  FilterTransactionBoundaries: true,
  adapterName: 'OracleReader',
  XstreamTimeOut: 600,
  connectionRetryPolicy: 'timeOut=30, retryInterval=60, maxRetries=3'
 ) 
OUTPUT TO DataStream;
CREATE OR REPLACE TARGET Target1 USING SysOut ( 
  name: "dstream"
 ) 
INPUT FROM DataStream;
CREATE OR REPLACE TARGET Target2 USING BigqueryWriter  ( 
  BQServiceAccountConfigurationPath: '/Users/ravipathak/Downloads/abc.json',
  projectId: 'big-querytest',
  Tables: 'SYSTEM.GGAUTHORIZATIONS,testing1.ggauthorisation',
  parallelismCount: 2,
  BatchPolicy: 'eventCount:100000,Interval:0'
 ) 
INPUT FROM DataStream;
END APPLICATION ORAToBigquery;
deploy application ORAToBigquery;
start ORAToBigquery;

--spool on to '/Users/jeyaselvan/Product/spoolfile.log';
select * from Oracle12C_To_Oracle12CApp_ExceptionStore;
--spool off;

create Application UdpDsv;
create source UdpDsvCSVSource using UDPReader (
	IpAddress:'127.0.0.1',
	PortNo:'3546',
	charset: 'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO UdpDsvCsvStream;
create Target UdpDsvDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/userLookup') input from UdpDsvCsvStream;
end Application UdpDsv;

stop APPLICATION @AppName@;
Undeploy APPLICATION @AppName@;
drop APPLICATION @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @AgentFlow@;

CREATE OR REPLACE SOURCE @SourceName@ USING Global.MSJet (
  Tables: 'dbo.compsrc',
  username: 'qatest',
  DatabaseName: 'qatest',
  FetchTransactionMetadata: true,
  filterTransactionBoundaries: true,
  compression: false,
   Mode: '@mode@',
  ConnectionURL: '@ConnectionURL@',
  CommittedTransactions: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
  SendBeforeImage: true,
  password: 'w3b@ct10n' )
OUTPUT TO @StreamName@;
END FLOW @AgentFlow@;

CREATE TARGET @SysTarget@ USING Global.SysOut (
  name: 'MS_CDC_SYSOUT' )
INPUT FROM @StreamName@;

CREATE FLOW @ServerFlow@;

CREATE TARGET @TargetName@ USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ParallelThreads: '',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: '@ConnectionURL@',
  StatementCacheSize: '50',
  DatabaseProviderType: 'Default',
  Username: 'qatest',
  Tables: 'dbo.compsrc,dbo.comptar',
  Password: 'w3b@ct10n',
  PreserveSourceTransactionBoundary: 'false',
  BatchPolicy: 'EventCount:1,Interval:60' )
INPUT FROM @StreamName@;

create Target @TargetName@_File using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (

)
INPUT FROM @StreamName@;

END FLOW @ServerFlow@;

END APPLICATION @AppName@;
DEPLOY APPLICATION @AppName@ with @AgentFlow@ in AGENTS ,@ServerFlow@ on any in default;
START APPLICATION @AppName@;

--
-- Crash Recovery Test 2 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS
--

STOP APPLICATION N2S2CR2Tester.N2S2CRTest2;
UNDEPLOY APPLICATION N2S2CR2Tester.N2S2CRTest2;
DROP APPLICATION N2S2CR2Tester.N2S2CRTest2 CASCADE;
CREATE APPLICATION N2S2CRTest2 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest2;

CREATE SOURCE CsvSourceN2S2CRTest2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN2S2CRTest2;

CREATE FLOW DataProcessingN2S2CRTest2;

CREATE TYPE WactionTypeN2S2CRTest2 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionTypeN2S2CRTest2;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN2S2CRTest2 CONTEXT OF WactionTypeN2S2CRTest2
EVENT TYPES ( WactionTypeN2S2CRTest2 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN2S2CRTest2
INSERT INTO WactionsN2S2CRTest2
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN2S2CRTest2;

END APPLICATION N2S2CRTest2;

Stop @APPNAME@_App;
undeploy application @APPNAME@_App;
drop application @APPNAME@_App cascade;

CREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;
CREATE SOURCE @APPNAME@_Src USING Global.OracleReader(
  FetchSize:'1',
  Username:'@SOURCE_USER@',
  Password:'@SOURCE_PASSWORD@',
  ConnectionURL:'@SOURCE_URL@',
  Tables:'@SOURCE_TABLES@',
  ConnectionRetryPolicy:'@AUTO_CONNECTION_RETRY@'
)
OUTPUT TO @APPNAME@_Stream;

CREATE TARGET @APPNAME@_SYS USING SysOut (
  name: '@APPNAME@_SYS' )
INPUT FROM @APPNAME@_Stream;

End APPLICATION @APPNAME@_App;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;
CREATE  SOURCE @SourceName@ USING MSSqlReader  ( 
  Username: '@Username@',
  Password: '@Password@',
  DatabaseName: 'qatest',
  connectionRetryPolicy: @ConnectionRetryPolicy@,
  ConnectionURL: '@ConnectionURL@',
  Tables: '@SourceTables@',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @SRCINPUTSTREAM@;
create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

CREATE APPLICATION @AppName@;

CREATE SOURCE @AppName@_FileReaderSource USING FileReader ( 
  wildcard: 'posdata.csv', 
  positionByEOF: false, 
  blocksize: 10240, 
  directory: 'Samples/PosApp/appData' ) 
PARSE USING DSVParser ( 
  trimquote: false, 
  header: 'Yes' ) 
OUTPUT TO @AppName@_CsvStream;

CREATE TYPE n1 (
 c1 java.lang.Integer KEY,
 _plan java.lang.String AS "plan");

CREATE OR REPLACE TARGET @AppName@_NullWriterTrg USING NullWriter ( 
 ) 
INPUT FROM @AppName@_CsvStream;

END APPLICATION @AppName@;
DEPLOY APPLICATION @AppName@;
START APPLICATION @AppName@;

STOP APPLICATION orrs;
UNDEPLOY APPLICATION orrs;
DROP APPLICATION orrs CASCADE;
CREATE APPLICATION orrs;
Create Source oraSource Using DatabaseReader
(
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'QATEST.ORACLETOREDSHIFTIL1;QATEST.ORACLETOREDSHIFTIL2',
 FilterTransactionBoundaries:true,
 FetchSize:1000
) Output To LCRStream;

CREATE TARGET RSTarget USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: 'access_key',
	  --secretaccesskey: 'secret_access',
	  S3IAMRole:'@IAMROLE@',
	  Tables: 'QATEST.ORACLETOREDSHIFT%,QATEST.ORACLETOREDSHIFT%',
	  uploadpolicy:'eventcount:1000,interval:1m'
	) INPUT FROM LCRStream;
	
END APPLICATION orrs;
DEPLOY APPLICATION orrs;
START APPLICATION orrs;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.XMLNodeEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (
  ProviderName: '',
  UserName: '',
  transactionpolicy: '',
  Provider: '',
  Ctx: '',
  EnableTransaction: '',
  QueueName: '',
  Topic:'',
  Password: '' )
PARSE USING XMLParserV2 (
  rootnode:'/JMSXMLIN'
  )
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

CREATE CQ @APPNAME@CQ
INSERT INTO @APPNAME@XMLStream
SELECT
  data.element("companyName").attributeValue("merchantId") as merchantId,
  data.element("companyName").getText() as companyName
FROM @APPNAME@PersistStream@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING Global.FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@XMLStream;

END APPLICATION @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCENAME@ USING IncrementalBatchReader  (
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: @startPosition@,
  PollingInterval: '120sec'
  )
  OUTPUT TO @STREAM@;

  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;

  CREATE TARGET @targetName@ USING DatabaseWriter(
    ConnectionURL:'@READER-URL@',
    Username:'@READER-UNAME@',
    Password:'@READER-PASSWORD@',
    BatchPolicy:'Eventcount:1,Interval:1',
    CommitPolicy:'Eventcount:1,Interval:1',
    Checkpointtable:'CHKPOINT',
    Tables:'@WATABLES@,@WATABLES@_target'
  ) INPUT FROM @STREAM@;

  END APPLICATION @APPNAME@;

  DEPLOY APPLICATION @APPNAME@;
  start application @APPNAME@;

STOP APPLICATION orrs;
UNDEPLOY APPLICATION orrs;
DROP APPLICATION orrs CASCADE;
CREATE APPLICATION orrs recovery 5 second interval;
Create Source OraSource Using OracleReader 
	(
	 Username:'user-name',	
	 Password:'password',
	 ConnectionURL: 'src_url',
	 Tables:'src_table',
	 FilterTransactionBoundaries:true,
	 FetchSize:'fetch-size'
	) Output To LCRStream;
	
	CREATE TARGET RSTarget USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: '@ACCESS-KEY-ID@',
	  --secretaccesskey: '@SECRET-ACCESS-KEY@',
	  S3IAMRole:'@IAMROLE@',
	  Tables: 'tgt_table',
	  uploadpolicy:'eventcount:300,interval:1m'
	) INPUT FROM LCRStream;
END APPLICATION orrs;
DEPLOY APPLICATION orrs;
START APPLICATION orrs;

CREATE or replace TARGET @TARGET_NAME@ USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:10',
StandardSQL:true	
) INPUT FROM @STREAM@;

stop tpcc;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;


Create Source @SourceName@
 Using OracleReader
(
 Username:'@READER-UNAME@',
 Password:'@READER-PASSWORD@',
 ConnectionURL:'@CDC-READER-URL@',
 Tables: '@WATABLES-SRC@',
 FetchSize:1,
 QueueSize:25000,
 CommittedTransactions:true,
 Compression:true,
 CaptureDDL: true,
 SendBeforeImage:true
) Output To @SRCINPUTSTREAM@;


create Target @targetsys@ using SysOut(name:OrgData) input from DataStream;

CREATE TARGET @targetName@ USING databasewriter(
  Username: '@WRITER-UNAME@',
  Password: '@WRITER-PASSWORD@',
  ConnectionURL:'@WRITER-URL@',
  BatchPolicy:'EventCount:1,Interval:1',
  CommitPolicy:'EventCount:1,Interval:1',
  Tables: '@WATABLES-SRC@,@WATABLES-TGT@'
) INPUT FROM @SRCINPUTSTREAM@;


END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ recovery 5 second Interval;

CREATE SOURCE @srcName@ USING SalesForceReader ( 
  customObjects: false, 
  autoAuthTokenRenewal: 'true',
  pollingInterval: '1 min', 
  sObjects: '@srcobject@', 
  useConnectionProfile: 'false',
  consumerSecret: '@srcconsumersecret@',
  consumerKey: '@srcconsumerkey@', 
  Username: '@srcusername@',
  Password: '@srcpassword@',
  mode: 'Automated',
  connectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  securityToken: '@srcsecuritytoken@',
  apiEndPoint: '@srcapiurl@',
  MigrateSchema: true, 
  threadPoolSize: 5
)

OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@ USING Global.DeltaLakeWriter (
  personalAccessToken:'@tgtpassword@',
  hostname:'@tgthostname@',
  stageLocation:'/',
  Mode:'MERGE',
  AuthenticationType: 'PersonalAccessToken',
  Tables:'@srcschema@,@tgtschema@.@tgttable@ COLUMNMAP()',
  adapterName:'DeltaLakeWriter',
  personalAccessToken_encrypted:'false',
  optimizedMerge:'false',
  uploadPolicy:'eventcount:1,interval:10s',
  connectionUrl:'@tgturl@',
  IgnorableExceptionCode:'TABLE_NOT_FOUND',
  externalStageType:'DBFSROOT'
)
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

 create flow myagentflow;

CREATE OR REPLACE SOURCE @APPNAME@DB_emp1 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO @APPNAME@Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE @APPNAME@DB_emp2 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO @APPNAME@Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE @APPNAME@DB_emp3 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO @APPNAME@Oracle_ChangeDataStream;

end flow myagentflow;

create flow myserverflow;

CREATE OR REPLACE TARGET @APPNAME@DB_etarget USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:100,Interval:10',
  CommitPolicy: 'EventCount:100,Interval:10',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  Tables: '',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM @APPNAME@Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from @APPNAME@Oracle_ChangeDataStream;

end flow myserverflow;

END APPLICATION @APPNAME@;

deploy application @APPNAME@ on ALL in default with myagentflow on all in Agents, myserverflow on all in  default;

start @APPNAME@;

--
-- Crash Recovery Test 1 on two node cluster with Kafka Stream
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP APPLICATION KStreamN2S2CR1Tester.KStreamN2S2CRTest1;
UNDEPLOY APPLICATION KStreamN2S2CR1Tester.KStreamN2S2CRTest1;
DROP APPLICATION KStreamN2S2CR1Tester.KStreamN2S2CRTest1 CASCADE;

DROP USER KStreamN2S2CR1Tester;
DROP NAMESPACE KStreamN2S2CR1Tester CASCADE;
CREATE USER KStreamN2S2CR1Tester IDENTIFIED BY KStreamN2S2CR1Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR1Tester;
CONNECT KStreamN2S2CR1Tester KStreamN2S2CR1Tester;

CREATE APPLICATION KStreamN2S2CRTest1 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest1;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest1;

CREATE FLOW DataProcessingKStreamN2S2CRTest1;

CREATE TYPE WactionTypeKStreamN2S2CRTest1 (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest1 CONTEXT OF WactionTypeKStreamN2S2CRTest1
EVENT TYPES ( WactionTypeKStreamN2S2CRTest1 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsKStreamN2S2CRTest1
INSERT INTO WactionsKStreamN2S2CRTest1
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

END FLOW DataProcessingKStreamN2S2CRTest1;

END APPLICATION KStreamN2S2CRTest1;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName1@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM1@;
Create Source @SourceName2@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM2@;
Create Source @SourceName3@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM3@;
Create Source @SourceName4@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM4@;
Create Source @SourceName5@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM5@;
Create Source @SourceName6@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM6@;
Create Source @SourceName7@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM7@;
Create Source @SourceName8@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM8@;
Create Source @SourceName9@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM9@;
Create Source @SourceName10@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM10@;

CREATE TARGET @targetName1@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM1@;
CREATE TARGET @targetName2@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM2@;
CREATE TARGET @targetName3@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM3@;
CREATE TARGET @targetName4@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM4@;
CREATE TARGET @targetName5@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM5@;
CREATE TARGET @targetName6@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM6@;
CREATE TARGET @targetName7@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM7@;
CREATE TARGET @targetName8@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM8@;
CREATE TARGET @targetName9@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM9@;
CREATE TARGET @targetName10@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM10@;


END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop application MSSQLTransactionSupportFTBTrue;
undeploy application MSSQLTransactionSupportFTBTrue;
drop application MSSQLTransactionSupportFTBTrue cascade;

CREATE APPLICATION MSSQLTransactionSupportFTBTrue recovery 1 second interval;

Create Source ReadFromMSSQL1
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
FetchTransactionMetadata:'false',
FilterTransactionBoundaries: true,
Compression:'false',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportFTBTrueStream;


CREATE TARGET WriteToMSSQL1 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC@,@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportFTBTrueStream;

CREATE TARGET MSSqlReaderOutput1 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportFTBTrueStream; 


CREATE OR REPLACE TARGET MSSQLFileOut1 USING FileWriter  ( 
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  directory:'@FEATURE-DIR@/logs/',
  filename: 'TransactionSupportFTBTrue.txt'
 ) 
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 ) 
INPUT FROM MSSQLTransactionSupportFTBTrueStream;

END APPLICATION MSSQLTransactionSupportFTBTrue;
deploy application MSSQLTransactionSupportFTBTrue;
start application MSSQLTransactionSupportFTBTrue;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
wildcard: '',
directory: '',
positionbyeof: false )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING AvroFormatter  (
schemaFileName: 'AvroFileSchema'
)
INPUT FROM @appname@CqOut;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using AvroFormatter (
schemaFileName: 'AvroS3Schema'
)
input from @appname@CqOut;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using AvroFormatter (
schemaFileName: 'AvroAzureSchema'
)
INPUT FROM @appname@CqOut;

CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using AvroFormatter (
schemaFileName: 'AvroGCSSchema'
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu RECOVERY 5 SECOND INTERVAL;
Create Source oracSource
 Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;
CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
ConnectionRetryPolicy: 'retryInterval=40,maxRetries=7',
batchpolicy: 'EventCount:20,Interval:60')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( 
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: '@CDC_URL@',
  Tables: '@Source1Tables@' ) 
OUTPUT TO @APPNAME@cdcStream;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@1 ( 
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',
  FetchSize: 20,
  DatabaseProviderType: 'Default',
  Table: '@Source3Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')  
OF @APPNAME@cachetype;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@2 ( 
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',
  FetchSize: 10,
  DatabaseProviderType: 'Default',
  Table: '@Source2Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')  
OF @APPNAME@cachetype;

STOP bq;
UNDEPLOY APPLICATION bq;
DROP APPLICATION bq CASCADE;

CREATE APPLICATION bq RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:orcl',
	Tables: 'QATEST.TABLE_TEST_1000001',
	FetchSize: '1'
)
OUTPUT TO SS;


CREATE or replace TARGET T USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
    Tables:'QATEST.TABLE_TEST_1000001,qatest.% keycolumns(RONUM)',
    mode:'Appendonly',
    datalocation: 'US',
	nullmarker: 'defaultNULL',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:100,Interval:10'	
) INPUT FROM ss;

END APPLICATION bq;
DEPLOY APPLICATION bq;
START APPLICATION bq;

STOP TestAlertsEmail.TestAlertsEmailApp;
UNDEPLOY APPLICATION TestAlertsEmail.TestAlertsEmailApp;
DROP APPLICATION TestAlertsEmail.TestAlertsEmailApp CASCADE;

CREATE APPLICATION TestAlertsEmailApp;

CREATE source rawSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:No,
  wildcard:'alerts_csv.txt',
  coldelimiter:' ',
  positionByEOF:false
) OUTPUT TO rawStream;

CREATE STREAM MyAlertStream OF Global.AlertEvent;
CREATE CQ GenerateMyAlerts
INSERT INTO MyAlertStream (name, keyVal, severity, flag, message)
SELECT "Testing Alerts", data[0], data[1], data[2], data[3]
FROM rawStream s;
CREATE TARGET output2 USING SysOut(name : alertsrecevied) input FROM MyAlertStream;

CREATE SUBSCRIPTION alertSubscription USING EmailAdapter
(SMTPUSER:'zalakalerts@gmail.com',
SMTPPASSWORD:'paloalto',
smtpurl:'smtp.gmail.com',
starttls_enable:'true',
smtp_auth:'true',
subject:"Test Email Alerts With Security Enabled",
emailList:"siddhika@striim.com,s.henry@striim.com,saranya@striim.com,invalidmailid.@striim.com",
userIds:'admin',
threadCount:"5",
senderEmail:"doga@striim.com"
)
INPUT FROM MyAlertStream;

END APPLICATION TestAlertsEmailApp;
DEPLOY APPLICATION TestAlertsEmail.TestAlertsEmailApp;
START TestAlertsEmail.TestAlertsEmailApp;

STOP banker.bankApp;
UNDEPLOY APPLICATION banker.bankApp;
DROP APPLICATION banker.bankApp cascade;

CREATE APPLICATION bankApp;


CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;

CREATE TYPE cacheType( merchantId String, 
			hourValue int, 
			hourlyAve int);

CREATE TYPE wsData
(
bankID Integer KEY,
bankName String
);

CREATE CACHE adhcache using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'banks.csv',
  header: Yes,
  columndelimiter: ',',
  trimquote:false
) QUERY (keytomap:'bankID') OF wsData;


CREATE STREAM wsStream OF wsData;

--Select data from QaStream and insert into wsStream

CREATE CQ csvTowsData
INSERT INTO wsStream
SELECT TO_INT(data[0]),data[1] FROM QaStream;

--create jumping window over data in wsStream

CREATE WACTIONSTORE oneWS
CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@

--get data from wsStream and place into wactionStore oneWS
CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT * FROM wsStream
LINK SOURCE EVENT;


END APPLICATION bankApp;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING MySqlReader
(
  Compression: false,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'mysql://localhost:3306',
  DatabaseName: 'waction',
  Tables: 'waction.test01',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root',
  connectionRetryPolicy:'retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'waction.test01,@DATASET@.% KEYCOLUMNS(col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;

--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

stop application @APPNAME@2;
undeploy application @APPNAME@2;
drop application @APPNAME@2 cascade;

create application @APPNAME@ recovery 1 second interval;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@'
)
OUTPUT TO @APPNAME@Stream;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;



CREATE TYPE test_typeJo (
 GG String,
CORP String,
DIV String,
YR FLOAT,
WK INT,
SOURCEID String,
JENUM INT,
ACCTPRIME String,
ACCTSUB String,
AMTTYPE String,
FAC String,
DEPT String,
SECT String,
REF String,
AMT FLOAT,
UNITAMT INT,
POSTINGDATE String,
EFFECTIVEDATE String,
SOURCEDESC String,
SEQUENCENUMBER INT,
SYSTEMN String
);

Create stream cqAsJSONNodeStreamJo of test_typeJo;

CREATE CQ cqAsJSONNodeStreamJo
INSERT into JSONNodeStreamJo
    select 
    data.get('GG'),
data.get('CORP'),
data.get('DIV'),
TO_FLOAT(data.get('YR')),
TO_INT(data.get('WK')),
data.get('SOURCE-ID'),
TO_INT(data.get('JE-NUM')),
data.get('ACCT-PRIME'),
data.get('ACCT-SUB'),
data.get('AMT-TYPE'),
data.get('FAC'),
data.get('DEPT'),
data.get('SECT'),
data.get('REF'),
TO_FLOAT(data.get('AMT')),
TO_INT(data.get('UNIT-AMT')),
data.get('POSTING-DATE'),
data.get('EFFECTIVE-DATE'),
data.get('SOURCE-DESC'),
TO_INT(data.get('SEQUENCE-NUMBER')),
data.get('SYSTEM-N')
from @APPNAME@Stream js;

create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:100,Interval:5',
  CommitPolicy: 'EventCount:100,Interval:5',
  Tables: 'QATEST.@table@'
)
input from JSONNodeStreamJo;

end application @APPNAME@;

-----------------------------------------------
create application @APPNAME@2 recovery 1 second interval;

create source @APPNAME@2_SRC Using FileReader(
	directory:'@DIRECTORY2@',
	WildCard:'@FILENAME2@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP12@',
  dataFileFont: '@PROP22@',
  copybookSplit: '@PROP32@',
  dataFileOrganization: '@PROP42@',
  copybookDialect: '@PROP52@', 
  skipIndent:'@PROP62@',
  DatahandlingScheme:'@PROP72@'
 )
OUTPUT TO @APPNAME@2Stream;

create Target @APPNAME@2Target using FileWriter(
    filename :'@FILE2@',
    directory : '@FOLDER2@'
)
format using JsonFormatter (
)
input from @APPNAME@2Stream;


CREATE TYPE test_typeRe 
(
node_new com.fasterxml.jackson.databind.JsonNode,
node_name com.fasterxml.jackson.databind.JsonNode,
node_addr com.fasterxml.jackson.databind.JsonNode
);

Create stream cqAsJSONNodeStreamRe of test_typeRe;

CREATE CQ GetPOAsJsonNodesRe
INSERT into cqAsJSONNodeStreamRe
select 
data.get('ACCTS-RECORD'),
data.get('ACCTS-RECORD').get('NAME'),
data.get('ACCTS-RECORD').get('ADDRESS3')
from @APPNAME@2Stream js;

create type finaldtypeRe
(ACCOUNT_NO int,
FIRST_NAME String,
LAST_NAME String,
ADDRESS1 String,
ADDRESS2 String,
CITY String,
STATE String,
ZIP_CODE int);

CREATE STREAM getdataStreamPS OF finaldtypeRe;

CREATE CQ getdataRe
INSERT into getdataStreamPS
select JSONGetInteger(x.node_new,"ACCOUNT-NO"),
JSONGetString(x.node_name,"FIRST-NAME"),
JSONGetString(x.node_name,"LAST-NAME"),
JSONGetString(x.node_new,"ADDRESS1"),
JSONGetString(x.node_new,"ADDRESS2"),
JSONGetString(x.node_addr,"CITY"),
JSONGetString(x.node_addr,"STATE"),
JSONGetInteger(x.node_addr,"ZIP-CODE")
from cqAsJSONNodeStreamRe x;

create Target @APPNAME@2DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:1000,Interval:50',
  CommitPolicy: 'EventCount:1000,Interval:50',
  Tables: 'QATEST.@table2@'
)
input from getdataStreamPS;

end application @APPNAME@2;
deploy application @APPNAME@2 on all in default;
deploy application @APPNAME@ on all in default;

start application @APPNAME@2;
start application @APPNAME@;

stop application AzureDLSGen1_sanity;
undeploy application AzureDLSGen1_sanity;
drop application AzureDLSGen1_sanity cascade;


create application AzureDLSGen1_sanity recovery 5 Second interval;
create source CSVSource using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'dynamicdirectory.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO CsvStream;

create Target WriteToADLSGen1 using ADLSGen1Writer(
        filename:'',
        directory:'',
        datalakestorename:'',
        clientid:'',
        authtokenendpoint:'',
        clientkey:'',
		rolloverpolicy:'eventcount:100000'
)
format using DSVFormatter (
)
input from CsvStream; 

end application AzureDLSGen1_sanity;

deploy application AzureDLSGen1_sanity;
start application AzureDLSGen1_sanity;

create application access;

create source AALAccessSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'access_log',
  charset:'UTF-8',
  positionByEOF:false
) PARSE USING AALParser (
  columndelimiter:' ',
  IgnoreEmptyColumn:'Yes'
) OUTPUT TO AalAccessStream;

create Target AALAccessDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/logdata') input from AalAccessStream;

end application access;

undeploy application RedshiftColmap;
alter application RedshiftColmap;

CREATE OR REPLACE SOURCE OracleSource USING OracleReader  (
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: '@SOURCE_TABLES@',
  FetchSize: 1
 ) Output To LogminerStream;
     
CREATE OR REPLACE TARGET RedshiftTarget USING RedshiftWriter
	(
	  ConnectionURL: '@TARGET-URL@',
	  Username: '@TARGET-UNAME@',
	  Password: '@TARGET-PASSWORD@',
	  bucketname: '@BUCKETNAME@',
	  --accesskeyId: '@ACCESS-KEY-ID@',
	  --secretaccesskey: '@SECRET-ACCESS-KEY@',
	  S3IAMRole:'@IAMROLE@',
	  Tables: '@TARGET-TABLES@',
	  uploadpolicy:'eventcount:1,interval:5s',
	  Mode:'incremental'
	) INPUT FROM LogminerStream;
	
END APPLICATION RedshiftColmap;
ALTER APPLICATION RedshiftColmap RECOMPILE;
deploy application RedshiftColmap;
START application RedshiftColmap;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE APPLICATION @APPNAME@ WITH ENCRYPTION @Recovery@ USE EXCEPTIONSTORE;
CREATE SOURCE @APPNAME@_s USING FileReader(
  directory:'Samples/AppData',
  wildcard:'PO.JSON',
  positionByEOF:false
)
parse using JSONParser (
) OUTPUT TO @APPNAME@_ss1;

--Using JSONNew() to create empty json node (j1)
CREATE CQ @APPNAME@_cq1 INSERT INTO @APPNAME@_ss2 SELECT JSONNew() j1,* FROM @APPNAME@_ss1 ;
--Using JSONSet() to set value to the empty json node (j1) and naming it as (j2)
CREATE CQ @APPNAME@_cq2 INSERT INTO @APPNAME@_ss3 SELECT JSONSet(j1, "Mission","Test") j2,* FROM @APPNAME@_ss2 ;
--Using JSONFrom() to create new json node (j3) using string
CREATE CQ @APPNAME@_cq3 INSERT INTO @APPNAME@_ss4 SELECT JSONFrom('{"PONumber":1600,"Reference":"ABULL-20140421","Requestor":"Alexis Bull","User":"ABULL","CostCenter":"A50","ShippingInstructions":{"name":"Alexis Bull","Address":{"street":"200 Sporting Green","city":"South San Francisco","state":"CA","zipCode":99236,"country":"United States of America"},"Phone":[{"type":"Office","number":"909-555-7307"},{"type":"Mobile","number":"415-555-1234"}]},"Special Instructions":null,"AllowPartialShipment":false,"LineItems":[{"ItemNumber":1,"Part":{"Description":"One Magic Christmas","UnitPrice":19.95,"UPCCode":13131092899},"Quantity":9,"Barcodes":[{"type":"Home","number":"9-555-7307"},{"type":"Office","number":"9-555-1234"}]},{"ItemNumber":2,"Part":{"Description":"Lethal Weapon","UnitPrice":19.95,"UPCCode":85391628927},"Quantity":5,"Barcodes":[{"type":"Home","number":"10-555-7307"},{"type":"Office","number":"10-555-1234"}]}]}')j3,* FROM @APPNAME@_ss3 ;
--Using JSONSet() to set new element to already existing node (data) - naming it as j4
CREATE CQ @APPNAME@_cq4 INSERT INTO @APPNAME@_ss5 SELECT JSONSet(data, "FileName","Dummy") j4,* FROM @APPNAME@_ss4 ;
--Using JSONArrayAdd() to Add element to existing JsonArray - naming it as j5
CREATE CQ @APPNAME@_cq5 INSERT INTO @APPNAME@_ss6 SELECT JSONArrayAdd(data.get("ShippingInstructions").get("Phone"),"work") j5,* FROM @APPNAME@_ss5 ;
--Using JSONArrayInsert() to Add element to existing JsonArray by specifying Index - naming it as j6
CREATE CQ @APPNAME@_cq6 INSERT INTO @APPNAME@_ss7 SELECT JSONArrayInsert(data.get("ShippingInstructions").get("Phone"),1,"residence") j6,* FROM @APPNAME@_ss6 ;
--Using JSONRemove() to Remove element from existing Json node 'data' - naming it as j7
CREATE CQ @APPNAME@_cq7 INSERT INTO @APPNAME@_ss8 SELECT JSONRemove(data,"Requestor") j7,* FROM @APPNAME@_ss7 ;
--Using JSONSetAll() to Add collection of objects (from metadata) to existing Json node 'data' - naming it as j8
CREATE CQ @APPNAME@_cq8 INSERT INTO @APPNAME@_ss9 SELECT JSONSetAll(j4, metadata) j8,* FROM @APPNAME@_ss8;
--Using JSONSetAll() in empty json node j1 - naming it as j9
CREATE CQ @APPNAME@_cq9 INSERT INTO @APPNAME@_ss10 SELECT JSONSetAll(j1, metadata) j9,* FROM @APPNAME@_ss9;
--Using JSONSetAll() with simple json node j2 - naming it as j10
CREATE CQ @APPNAME@_cq10 INSERT INTO @APPNAME@_ss11 SELECT JSONSetAll(j2, metadata) j10,* FROM @APPNAME@_ss10;
--Using JSONSetAll() with json node which has the same field/values to verify overriding
CREATE CQ @APPNAME@_cq11 INSERT INTO @APPNAME@_ss12 SELECT JSONSetAll(j10, metadata) j11,* FROM @APPNAME@_ss11;
create target @APPNAME@_t using sysout (name:ss1) input from @APPNAME@_ss12;

CREATE Stream @APPNAME@_str1 (
     PODetails com.fasterxml.jackson.databind.JsonNode,
     Phone com.fasterxml.jackson.databind.JsonNode,
     LineItems com.fasterxml.jackson.databind.JsonNode,
     BarCodes com.fasterxml.jackson.databind.JsonNode,
     Addr com.fasterxml.jackson.databind.JsonNode
);

CREATE CQ @APPNAME@_cq12
INSERT into @APPNAME@_str1
    select
    j8,
    j8.get('ShippingInstructions').get('Phone'),
    j8.get('LineItems'),
    j8.get('LineItems').get('Barcodes'),
    j8.get('ShippingInstructions').get('Address')
from @APPNAME@_ss12;

CREATE Stream @APPNAME@_str2(
     PONumber Integer,
     Reference String,
     Usr String,
     CostCenter String,
     Name String,
     street String,
     city String,
     state String,
     zipCode Integer,
     country String,
     officephone String,
     mobilephone String,
     SplInstruction String,
     AllowPartialShipment boolean,
     PhoneType String,
     PhoneNo String,
     ItemNumber Integer,
     ItemDesc String,
     UnitProce Double,
     UPCCode Double,
     Quantity Integer,
     BarcodeType String,
     BarcodeNo String
);

CREATE CQ @APPNAME@_cq13
INSERT into @APPNAME@_str2
SELECT

    /* PO Details */
    JSONGetInteger(x.PODetails,"PONumber"),
    JSONGetString(x.PODetails,"Reference"),
    JSONGetString(x.PODetails,"User"),
    JSONGetString(x.PODetails,"CostCenter"),

    /* Shipping Details */
    JSONGetString(x.PODetails.get('ShippingInstructions'),"name"),
    JSONGetString(x.Addr,"street"),
    JSONGetString(x.Addr,"city"),
    JSONGetString(x.Addr,"state"),
    JSONGetInteger(x.Addr,"zipCode"),
    JSONGetString(x.Addr,"country"),

    /* Phone nos by array position*/
    JSONGetString(x.Phone.get(0),"number"),
    JSONGetString(x.Phone.get(1),"number"),

    /*Others*/
    JSONGetString(x.PODetails,"Special Instructions"),
    JSONGetboolean(x.PODetails,"AllowPartialShipment"),

    /* Phone nos using iterator*/
    JSONGetString(pho,"type"),
    JSONGetString(pho,"number"),

    /*Item Specific one */
    JSONGetInteger(Items,"ItemNumber"),
    JSONGetString(Items.get('Part'),"Description"),
    JSONGetDouble(Items.get('Part'),"UnitPrice"),
    JSONGetDouble(Items.get('Part'),"UPCCode"),
    JSONGetInteger(Items,"Quantity"),

    /*Barcode specific values */
    JSONGetString(Barcode,"type"),
    JSONGetString(Barcode,"number")

from @APPNAME@_str1 x, iterator(x.LineItems) Items, iterator(x.Phone) pho, iterator(Items.Barcodes) Barcode;

create Target @APPNAME@_JSONNew_T using FileWriter (
filename:'@APPNAME@_JSONNew_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j1)
input from @APPNAME@_ss2;

create Target @APPNAME@_JSONSet1_T using FileWriter (
filename:'@APPNAME@_JSONSet1_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j2)
input from @APPNAME@_ss3;

create Target @APPNAME@_JSONFrom_T using FileWriter (
filename:'@APPNAME@_JSONFrom_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j3)
input from @APPNAME@_ss4;

create Target @APPNAME@_JSONSet2_T using FileWriter (
filename:'@APPNAME@_JSONSet2_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j4)
input from @APPNAME@_ss5;

create Target @APPNAME@_JSONArrayAdd_T using FileWriter (
filename:'@APPNAME@_JSONArrayAdd_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j5)
input from @APPNAME@_ss6;

create Target @APPNAME@_JSONArrayInsert_T using FileWriter (
filename:'@APPNAME@_JSONArrayInsert_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j6)
input from @APPNAME@_ss7;

create Target @APPNAME@_JSONRemove_T using FileWriter (
filename:'@APPNAME@_JSONRemove_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j7)
input from @APPNAME@_ss8;

create Target @APPNAME@_JSONSetAll1_T using FileWriter (
filename:'@APPNAME@_JSONSetAll1_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j8)
input from @APPNAME@_ss9;

create Target @APPNAME@_JSONSetAll2_T using FileWriter (
filename:'@APPNAME@_JSONSetAll2_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j9)
input from @APPNAME@_ss10;

create Target @APPNAME@_JSONSetAll3_T using FileWriter (
filename:'@APPNAME@_JSONSetAll3_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j10)
input from @APPNAME@_ss11;

create Target @APPNAME@_JSONSetAll4_T using FileWriter (
filename:'@APPNAME@_JSONSetAll4_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j11)
input from @APPNAME@_ss12;

create Target @APPNAME@_PO_T using FileWriter (
filename:'@APPNAME@_PO_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000'
)
format using dsvFormatter()
input from @APPNAME@_str2;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

Stop @APPNAME@_App;
undeploy application @APPNAME@_App;
drop application @APPNAME@_App cascade;

CREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;
CREATE SOURCE @APPNAME@_Src USING Global.OracleReader(
  FetchSize:'1',
  Username:'@SOURCE_USER@',
  Password:'@SOURCE_PASSWORD@',
  ConnectionURL:'@SOURCE_URL@',
  Tables:'@SOURCE_TABLES@',
  _h_useClassic:'true',
  ConnectionRetryPolicy:'@AUTO_CONNECTION_RETRY@'
)
OUTPUT TO @APPNAME@_Stream;

CREATE TARGET @APPNAME@_SYS USING SysOut (
  name: '@APPNAME@_SYS' )
INPUT FROM @APPNAME@_Stream;

End APPLICATION @APPNAME@_App;

Stop Teradata_LogWriter;
Undeploy application Teradata_LogWriter;
drop application Teradata_LogWriter cascade;

CREATE APPLICATION Teradata_LogWriter recovery 5 second interval;

CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.TDSOURCE',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.TEST01=ID;',
  PollingInterval: '5sec',
  ReturnDateTimeAs: 'String',
  startPosition:'striim.test01=0'
  )
  OUTPUT TO data_stream;

  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

CREATE TARGET BinaryDump USING LogWriter(
  name: 'TeraData',
  filename:'TeraData.log',
  flushpolicy:'EventCount:100,Interval:30s'
)INPUT FROM data_stream;

END APPLICATION Teradata_LogWriter;

deploy application Teradata_LogWriter in default;

start application Teradata_LogWriter;

STOP APPLICATION @Appname@;
UNDEPLOY APPLICATION @Appname@;
DROP APPLICATION @Appname@ CASCADE;
unload OPEN PROCESSOR "OP_SCM_PATH";

CREATE APPLICATION @Appname@ RECOVERY 10 SECOND INTERVAL;

Create flow AgentFlow;

CREATE  SOURCE @Appname@s USING databaseReader  (
Username: 'qatest',
Password: 'qatest',
ConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',
Tables: 'QATEST.TGT_T%')
OUTPUT TO @Appname@Stream1;

CREATE SOURCE @Appname@sd USING databaseReader  (
Username: 'qatest',
Password: 'qatest',
ConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',
Tables: 'QATEST.TGT_T%')
OUTPUT TO @Appname@Stream2;

LOAD OPEN PROCESSOR "OP_SCM_PATH";

CREATE STREAM  @Appname@outStream OF Global.WAEvent;

CREATE OPEN PROCESSOR @Appname@op USING TUPLECONVERTER (
   magicprop:"ASIA"
)
INSERT INTO @Appname@outStream
FROM @Appname@Stream1;

CREATE OR REPLACE ROUTER @Appname@router1 INPUT FROM @Appname@outStream s CASE
WHEN meta(s,"TableName").toString()='QATEST.OPA_INPUTR1' THEN ROUTE TO @Appname@ss1,
WHEN meta(s,"TableName").toString()='QATEST.OPA_INPUTR2' THEN ROUTE TO @Appname@ss2,
ELSE ROUTE TO @Appname@ss_else;


CREATE OR REPLACE ROUTER @Appname@router2 INPUT FROM @Appname@Stream2 s CASE
WHEN meta(s,"TableName").toString()='QATEST.OPA_OUTPUTR1' THEN ROUTE TO @Appname@ss1,
WHEN meta(s,"TableName").toString()='QATEST.OPA_OUTPUTR2' THEN ROUTE TO @Appname@ss2,
ELSE ROUTE TO @Appname@ss_else;

end flow AgentFlow;

create flow nflow;

create Target @Appname@Target_1 using FileWriter
(
directory: 'testApr22',
filename:'%@metadata(TableName)%',
flushpolicy:'eventCount:1000,Interval:90s',
RollOverPolicy:'eventCount:1000,Interval:90s'
)
FORMAT USING dsvFormatter ()
input from @Appname@ss1;

create Target @Appname@Target_2 using FileWriter
(
directory: 'testApr22',
filename:'%@metadata(TableName)%',
flushpolicy:'eventCount:1000,Interval:90s',
RollOverPolicy:'eventCount:1000,Interval:90s'
)
FORMAT USING dsvFormatter ()
input from @Appname@ss2;

end flow nflow;

end application @Appname@;
deploy application @Appname@ with AgentFlow in agents, nflow in default;
start @Appname@;

CREATE FLOW ServerFlow;

CREATE TARGET @TARGET_NAME@_sysout USING Global.SysOut (
  name: '@TARGET_NAME@_SysOut' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1'
 )
INPUT FROM @STREAM@;
END FLOW ServerFlow;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
  directory: '',
  positionByEOF: false,
  WildCard: '' )
PARSE USING ParquetParser (
 )
OUTPUT TO @appname@Streams;

CREATE STREAM @appname@Stream3 OF Global.ParquetEvent;

CREATE STREAM @appname@Stream4 OF Global.ParquetEvent;

CREATE OR REPLACE CQ @appname@CQOrder3
INSERT INTO @appname@Stream3
SELECT
PUTUSERDATA(s,'schemaName',s.data.getSchema().getName())
FROM @appname@Streams s;

CREATE OR REPLACE CQ @appname@CQOrder4
INSERT INTO @appname@Stream4
SELECT
PUTUSERDATA(s2, 'customFilename', Userdata(s2, 'schemaName').toString().concat(".test"))
FROM @appname@Stream3 s2;

CREATE OR REPLACE TARGET @filetarget@ USING Global.FileWriter (
  flushpolicy: 'EventCount:10000,Interval:30s',
  directory: '',
  filename: '',
  rolloverpolicy: 'eventcount:10' )
FORMAT USING ParquetFormatter  (
schemafilename:''
)
INPUT FROM @appname@Stream4;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;;

Create Source Ojetsrc1 Using Ojet
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source Ojetsrc2 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source Ojetsrc3 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source Ojetsrc4 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source Ojetsrc5 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

create target AzureTarget using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',  
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;
END APPLICATION ADW;
deploy application ADW;
start application ADW;

STOP liveStream;
UNDEPLOY APPLICATION liveStream;
DROP APPLICATION liveStream CASCADE;

CREATE APPLICATION liveStream;

CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity int,
  size int);

--SOURCE DESCRIPTION
---------------------------------------------
--OutputType: name of the corresponding TYPE
--noLimit: produce infinite data
--maxRows: (if noLimit = false) produces specified rows
--iterations (optional): 1 iteration = populating Type attributes once.
--iterationDelay (ms)(optional): delay between iterations (0 if none)

-- ** either maxRows or iterations must be 0 **

--StringSet: columns of type String. column values within brackets (seperate values by dash), seperate columns by comma
--NumberSet: columns of type Int, Double, Long. supply a range between brackets, followed by G (Gaussian) or R (Random) distribution.
---------------------------------------------

CREATE SOURCE liveSource using StreamReader(
  OutputType: 'eventLister.Atm',
  noLimit: 'false',
  maxRows: 20,
  iterations: 0,
  iterationDelay: 100,
  StringSet: 'productID[001-002-003-004],stateID[AS-CA-WA-NY]',
  NumberSet: 'productWeight[3-3]R,quantity[20-20]R,size[250-250]R'
  )OUTPUT TO CsvStream;

CREATE STREAM newStream OF Atm;


CREATE CQ newCQ
INSERT INTO newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_INT(data[3]), TO_INT(data[4]) FROM
CsvStream;

CREATE WACTIONSTORE streamActivity CONTEXT OF Atm
EVENT TYPES ( Atm )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ newCQ2
INSERT INTO streamActivity
SELECT * FROM newStream
link source event;

END APPLICATION liveStream;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

CREATE SOURCE @SourceName@ USING PostgreSQLReader  ( 
ReplicationSlotName: 'test_slot',
adapterName: 'PostgreSQLReader',
TransactionSupport: false, 
  FetchTransactionMetadata: false, 
  Compression: false, 
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', 
  Password_encrypted: 'false', 
  ConnectionURL: '@ConnectionURL@', 
  Fetchsize: 0, 
  ConnectionPoolSize: 10, 
  StartPosition: 'EOF', 
  Username: '@UN@', 
  cdcRoleName: 'STRIIM_READER', 
  Password: '@PWD@', 
  Tables: 'qatest.%', 
  FilterTransactionBoundaries: true, 
  SendBeforeImage: true, 
  AutoDisableTableCDC: false ) 
OUTPUT TO @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;


CREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'), 'OpTime',META(x, 'TimeStamp'))) FROM @SRCINPUTSTREAM@ x; ;

CREATE TARGET @targetName@ USING DatabaseWriter  ( 
ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  ParallelThreads: '', 
  CheckPointTable: 'CHKPOINT', 
  Password_encrypted: 'false', 
  ConnectionURL: '@ConnectionURL@', 
  CommitPolicy: 'EventCount:1,Interval:60', 
  StatementCacheSize: '50', 
  DatabaseProviderType: 'Default', 
  Username: '@UN@', 
  Password: '@PWD@', 
  PreserveSourceTransactionBoundary: 'false', 
  BatchPolicy: 'EventCount:1,Interval:60', 
  Tables: '@TableMapping@' ) 
INPUT FROM admin.sqlreader_cq_out;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP ttlTester.ttlApp;
UNDEPLOY APPLICATION ttlTester.ttlApp;
DROP APPLICATION ttlTester.ttlApp cascade;

CREATE APPLICATION ttlApp;


CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;


CREATE TYPE wsData
(
bankID Integer KEY,
bankName String
);


CREATE STREAM wsStream OF wsData;

--Select data from QaStream and insert into wsStream

CREATE CQ csvTowsData
INSERT INTO wsStream
SELECT TO_INT(data[0]),data[1] FROM QaStream;

--create jumping window over data in wsStream

CREATE WACTIONSTORE oneWS
CONTEXT OF wsData
EVENT TYPES(wsData )
PERSIST IMMEDIATE USING ( storageProvider: 'elasticsearch', elasticsearch.time_to_live: '10000ms' ) ;

--get data from wsStream and place into wactionStore oneWS
CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT * FROM wsStream
LINK SOURCE EVENT;


END APPLICATION ttlApp;

CREATE APPLICATION BankDataApp;

CREATE TYPE MoreBankData (
  bankID java.lang.Integer KEY,
  bankName java.lang.String,
  bankRouting java.lang.Long,
  bankAmount java.lang.Double
);

CREATE STREAM BankDataStream OF MoreBankData;

CREATE TARGET SysOut USING Global.SysOut ( 
  name: 'SysOut'
) 
INPUT FROM BankDataStream;

END APPLICATION BankDataApp;

DEPLOY APPLICATION BankDataApp;
START BankDataApp;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '@GRPPOLICY@',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'MultipleEvents',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE TARGET MongoTarget USING MongoDBWriter (
  Username: '',
  AuthDB: 'admin',
  ConnectionURL: '',
  batchpolicy: 'EventCount:10, Interval:30',
  Password: '',
  collections: '' )
INPUT FROM CCBStream;

create Target BlobTarget using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,Interval:30s'
)
format using JSONFormatter (
)
INPUT FROM CCBStream;


CREATE OR REPLACE TARGET GCSTarget USING GCSWriter (
    bucketname:'',
    objectname:'',
    projectId:'',
    uploadPolicy:'EventCount:10,Interval:30s'
)
format using JSONFormatter ()
INPUT FROM CCBStream;

create Target KafkaTarget using KafkaWriter VERSION @KAFKAVERSION@ (
brokerAddress:'',
Topic:'',
Mode: 'Sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;linger.ms=30000'
)
FORMAT USING JSONFormatter (
members:'data')
input from CCBStream;

CREATE TARGET S3Target USING Global.S3Writer (
  bucketname: '',
  objectname: '',
  uploadpolicy: 'EventCount:10,Interval:30s' )
FORMAT USING Global.JSONFormatter  ()
INPUT FROM CCBStream;

create source KafkaSource using KafkaReader VERSION @KAFKAVERSION@(
brokerAddress:'',
	Topic:''
)
parse using JSONParser ()
output to KafkaStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:10,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING JSONFormatter  ()
INPUT FROM KafkaStream;


end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ recovery 5 second interval;

CREATE OR REPLACE SOURCE @SOURCENAME@ USING IncrementalBatchReader  (
  FetchSize: 1000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: @startPosition@,
  PollingInterval: '20sec'
  )
  OUTPUT TO @STREAM@;

  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;

  CREATE TARGET @targetName@ USING DatabaseWriter(
    ConnectionURL:'@READER-URL@',
    Username:'@READER-UNAME@',
    Password:'@READER-PASSWORD@',
    BatchPolicy:'Eventcount:1000,Interval:1000',
    CommitPolicy:'Eventcount:1000,Interval:1000',
    Checkpointtable:'CHKPOINT',
    Tables:'@WATABLES@,@WATABLES@_target'
  ) INPUT FROM @STREAM@;

  END APPLICATION @APPNAME@;

  DEPLOY APPLICATION @APPNAME@;
  start application @APPNAME@;

CREATE APPLICATION @WRITERAPPNAME@ RECOVERY 1 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

CREATE SOURCE @SOURCE@ USING Ojet
(
FetchSize:1,
Username:'@SOURCE_USER@',
Password:'@pass@',
--Password:'$SRC_PASSWORD',
ConnectionURL:'@CONNECTION_URL@',
Tables:'@SOURCE_TABLE@',
)
OUTPUT TO @STREAM1@;


end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

CREATE OR REPLACE TYPE @TYPE@( 
datae java.util.HashMap , 
TABLE_NAME java.lang.String , 
OPS_NAME java.lang.String , 
DB_TIMESTAMP java.lang.String  ,
COMMITSCN java.lang.String ,
SCN java.lang.String ,
REC_INS_TIME java.lang.String );

CREATE CQ @CQ1@
INSERT INTO @STREAM2@
SELECT  
CASE WHEN (META(c,"OperationName").toString() == "DELETE")
THEN putUserData(c, 'isDelete', 'true') 
ELSE
putUserData(c,'isDelete', 'false')
END
FROM @STREAM1@ c;

CREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;

CREATE OR REPLACE CQ @CQ2@ 
INSERT INTO @STREAM3@
SELECT 
data(e),
META(e,"TableName").toString() as TABLE_NAME,
META(e, "OperationName").toString() as OPS_NAME,
META(e, "TimeStamp").toString() as DB_TIMESTAMP,
META(e,"COMMITSCN").toString() as COMMITSCN ,
META(e,"SCN").toString() as  SCN ,
DNOW().toString() as REC_INS_TIME
FROM @STREAM1@ e;

create Target @TARGET1@ using KafkaWriter VERSION @kafakaversion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC1',
ParallelThreads:'',
PartitionKey:'@metadata(TableName)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from @STREAM1@;


create Target @TARGET2@ using KafkaWriter VERSION @kafakaversion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC2',
ParallelThreads:'2',
PartitionKey:'TABLE_NAME',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from @STREAM3@;

create Target @TARGET3@ using KafkaWriter VERSION @kafakaversion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC3',
ParallelThreads:'',
PartitionKey:'@userdata(isDelete)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest.avsc')
input from @STREAM2@;

end application @WRITERAPPNAME@;

CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafakaversion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC1',
        startOffset:0       
)
PARSE USING DSVParser ()
OUTPUT TO KafkaReaderStream1;

CREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafakaversion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC2',
        startOffset:0          
)
PARSE USING JSONParser ()
OUTPUT TO KafkaReaderStream2;

CREATE TARGET kafkaDumpJSON USING FileWriter(
filename:'@READERAPPNAME@_RT_JSON')
FORMAT USING JSONFormatter()
INPUT FROM KafkaReaderStream2;

CREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafakaversion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC3',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest.avsc'
)
OUTPUT TO KafkaReaderStream3;

CREATE TARGET kafkaDumpAVRO USING FileWriter(
filename:'@READERAPPNAME@_RT_AVRO')
FORMAT USING AvroFormatter(
    schemaFileName:'@avro_schema_file@'
)
INPUT FROM KafkaReaderStream3;

end flow @APPNAME@_serverflow;
end application @READERAPPNAME@;

--
-- Crash Recovery Test 4 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS
-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS
--

STOP APPLICATION KStreamN2S2CR4Tester.KStreamN2S2CRTest4;
UNDEPLOY APPLICATION KStreamN2S2CR4Tester.KStreamN2S2CRTest4;
DROP APPLICATION KStreamN2S2CR4Tester.KStreamN2S2CRTest4 CASCADE;

DROP USER KStreamN2S2CR4Tester;
DROP NAMESPACE KStreamN2S2CR4Tester CASCADE;
CREATE USER KStreamN2S2CR4Tester IDENTIFIED BY KStreamN2S2CR4Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR4Tester;
CONNECT KStreamN2S2CR4Tester KStreamN2S2CR4Tester;

CREATE APPLICATION KStreamN2S2CRTest4 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest4;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest4 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest4;

CREATE FLOW DataProcessingKStreamN2S2CRTest4;

CREATE TYPE CsvDataKStreamN2S2CRTest4 (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionTypeKStreamN2S2CRTest4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream OF CsvDataKStreamN2S2CRTest4;

CREATE CQ CsvToDataKStreamN2S2CRTest4
INSERT INTO DataStream
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest4 CONTEXT OF WactionTypeKStreamN2S2CRTest4
EVENT TYPES ( CsvDataKStreamN2S2CRTest4 )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO WactionsKStreamN2S2CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO WactionsKStreamN2S2CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END FLOW DataProcessingKStreamN2S2CRTest4;

END APPLICATION KStreamN2S2CRTest4;

CREATE OR REPLACE APPLICATION @AppName@;

Create Source @AppName@_source Using OracleReader(
  Username:'@username@',
  Password:'@password@',
  ConnectionURL:'@url@',
  Tables:'@srctableName@',
  Fetchsize:1
)
Output To @AppName@_Stream;
CREATE OR REPLACE TARGET @AppName@_SF_Target USING Global.SnowflakeWriter (
  streamingUpload: 'false',
  useConnectionProfile:'true',
  connectionProfileName: 'admin.@SFCP@',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  columnDelimiter: '|',
  tables: '@srctableName@,@trgtableName@',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:1,interval:5m',
  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',
  externalStageType: 'Local',
  adapterName: 'SnowflakeWriter',
  fileFormatOptions: 'null_if = \"\"' )
INPUT FROM @AppName@_Stream;


CREATE OR REPLACE TARGET @AppName@_DB_Target USING Global.DeltaLakeWriter (
connectionProfileName: 'admin.@DBCP@',
useConnectionProfile:'true',
  Tables: '@srctableName@,@DBtrgtableName@',
  uploadPolicy: 'eventcount:100000,interval:60s'
)

INPUT FROM @AppName@_Stream;
END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadpolicy:'EventCount:7'
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from @STREAM@;
end application @APPNAME@;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;
Create Source Ojetsrc Using Ojet
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;


create target AzureTarget using AzureSQLDWHWriter (
ConnectionURL: '@SQLDW-URL@',
username: '@SQLDW-USERNAME@',
password: '@SQLDW-PASSWORD@',
AccountName: '@STORAGEACCOUNT@',
AccountAccessKey: '@ACCESSKEY@',
Tables: '@TARGET-TABLES@',
uploadpolicy:'@EVENT-COUNT@',
Mode:'Merge'
) INPUT FROM str;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

Stop application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

CREATE APPLICATION DSV;

CREATE FLOW HTTPsource;

CREATE SOURCE HTTPSOURCE USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'http.txt',
  skipbom: true,
  rolloverpolicy: 'DefaultFileComparator',
  blocksize: '64',
  charset: 'UTF-8',
  positionbyeof: false
 )
 PARSE USING DSVParser (
  trimquote: true,
  columndelimiter: ' ',
  rowdelimiter: '\n',
  header: false,
  quoteset: '{}'
 )
OUTPUT TO RawHTTPStream;

END FLOW HTTPSource;

CREATE FLOW main;

CREATE TYPE HTTPLogEntry (
     start_time String,
     srcIp      String KEY,
     port       String,
     method     String,
     url        String,
     error_num  String,
     status     String,
     end_time   String,
     host       String
 );

CREATE STREAM HTTPStream OF HTTPLogEntry;

CREATE CQ ParseHTTPLog
  INSERT INTO HTTPStream
  SELECT  MATCH(data[1],'start\\s+(\\w+.\\w+)'),
          matchIP(data[2]),
          MATCH(data[3],'port\\W+(\\w+)'),
          MATCH(data[4],'method\\W+(\\w+)'),
          data[5],
          data[7],
          data[8],
          data[9],
          data[10]
  FROM RawHTTPStream;

create Target t using FileWriter(
  filename:'XmlTrimQuote',
  sequence:'00',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:10s,sequence:00'
)
format using DSVFormatter (

)
input from HTTPStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TrimQuoteTest_actual.log') input from HTTPStream;

END FLOW main;

END APPLICATION DSV;

CREATE CQ @CQ_NAME@
INSERT INTO @EMB_STREAM@
@SELECT_QUERY@
FROM @STREAM@ e;

CREATE TARGET @CQ_NAME@_sysout USING Global.SysOut (
  name: '@CQ_NAME@_sysout' )
INPUT FROM @EMB_STREAM@;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;;

Create Source Ojetsrc Using Ojet
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;


create target AzureTarget1 using AzureSQLDWHWriter (
		CoNNectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',  
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;


create target AzureTarget2 using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        UserName: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@', 
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;


create target AzureTarget3 using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        UserName: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@', 
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

create target AzureTarget4 using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        UserName: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@', 
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

--
-- Recovery Test 12 with two sources, two jumping attribute windows, one wactionstore with recovery, and another wactionstore without -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W/p -> CQ1 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS2 (no persists)
--

STOP Recov12Tester.RecovTest12;
UNDEPLOY APPLICATION Recov12Tester.RecovTest12;
DROP APPLICATION Recov12Tester.RecovTest12 CASCADE;
CREATE APPLICATION RecovTest12 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsNoPersist CONTEXT OF WactionData
EVENT TYPES ( CsvData )
		PERSIST NONE USING ( ) ;

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWactionNoPersist
INSERT INTO WactionsNoPersist
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION RecovTest12;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@ConnectionURL@',
 Tables:'@SourceTables@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;


CREATE CQ @cqName@ INSERT INTO admin.oraclereader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'), 'OpTime',META(x, 'TimeStamp'))) FROM @SRCINPUTSTREAM@ x; ;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@ConnectionURL@',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@TableMapping@'
) INPUT FROM oraclereader_cq_out;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @srcName@ USING OracleReader (
 Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@'
)
OUTPUT TO @outstreamname@;

CREATE OR REPLACE TARGET @tgtName@ USING DatabaseWriter
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'@tgtusername@',
  BatchPolicy:'EventCount:1,Interval:0',
  CommitPolicy:'EventCount:1,Interval:0',
  ConnectionURL:'@tgturl@',
  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
  Password:'@tgtpassword@'
)
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP LngIS2noder.LongRunningISStreamApp;
UNDEPLOY APPLICATION LngIS2noder.LongRunningISStreamApp;
DROP APPLICATION LngIS2noder.LongRunningISStreamApp CASCADE;

CREATE APPLICATION LongRunningISStreamApp;

CREATE FLOW ISFLOW1;
----------------------------------------------------
CREATE source implicitSOurce USING StreamReader
(
   OutputType: 'LngIS2noder.Atm',
   noLimit: 'true',
   maxRows: 1000,
   iterationDelay: 50,
   StringSet: 'productID[001-002-003-004],stateID[AS-CA-WA-NY],currentDate[2014-2015]',
   NumberSet: 'productWeight[1-10]R,quantity[50.5-160.1]G,size[763872-4778823]L'
) OUTPUT TO CsvStream;


CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity double,
  size long,
  currentDate org.joda.time.DateTime);

END FLOW ISFLOW1;
----------------------------------------------------

CREATE FLOW ISFLOW2;

CREATE CACHE cache1 USING CsvReader(
  directory: '@TEST-DATA-PATH@',
  wildcard: 'ISdata.csv',
  header: false,
  columndelimiter: ',',
  trimquote: false
  ) QUERY (keytomap:'productID') OF Atm;


CREATE STREAM newStream OF Atm;


CREATE CQ newCQ
INSERT INTO newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]), TO_DATE(data[5]) FROM
CsvStream;

CREATE WINDOW win1
OVER newStream
KEEP 50 rows;


CREATE CQ newCQ2
INSERT INTO newStream2
SELECT productID as A , stateID AS B, productWeight AS C, quantity AS D, size AS E, currentDate AS F FROM
newStream;


CREATE CQ newCQ3
INSERT INTO newStream3 PARTITION BY A
SELECT A,B,C,D,E,F FROM newStream2 order by C,D
link source event;

CREATE CQ newCQ4
INSERT INTO newStream4
SELECT count(productID),currentDate FROM newStream ORDER BY currentDate
link source event;

CREATE CQ newCQ5
INSERT INTO newStream5
SELECT x.*, y.* from cache1 x, newStream y WHERE x.productweight > 6 ORDER BY x.currentDate;


CREATE WACTIONSTORE WS1 CONTEXT OF Atm
EVENT TYPES(Atm );

CREATE CQ newCQ6
INSERT INTO WS1
SELECT * FROM newStream WHERE productID = '001';

CREATE CQ newCQ7
INSERT INTO newStream6
SELECT aa.productID FROM WS1 [push] aa, cache1 bb;


CREATE CQ newCQ8
INSERT INTO newStream7
SELECT Sum(X.size) FROM (Select size from win1 where productweight > 5) X;


END FLOW ISFLOW2;
----------------------------------------------------

END APPLICATION LongRunningISStreamApp;

--
-- Kafka Stream Recovery Test 11
-- Bert Hashemi and Nicholas Keene, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS1
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS2
--

STOP Recov11Tester.KStreamRecovTest11;
UNDEPLOY APPLICATION Recov11Tester.KStreamRecovTest11;
DROP APPLICATION Recov11Tester.KStreamRecovTest11 CASCADE;
DROP USER KStreamRecov11Tester;
DROP NAMESPACE KStreamRecov11Tester CASCADE;
CREATE USER KStreamRecov11Tester IDENTIFIED BY KStreamRecov11Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov11Tester;
CONNECT KStreamRecov11Tester KStreamRecov11Tester;

CREATE APPLICATION KStreamRecovTest11 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions1 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions2 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions1
INSERT INTO Wactions1
SELECT
    *
FROM DataStream5Minutes;

CREATE CQ InsertWactions2
INSERT INTO Wactions2
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION KStreamRecovTest11;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src1 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;


CREATE OR REPLACE TARGET @APP_NAME@_tgt1 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt2 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt3 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt4 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt5 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

use admin;
drop namespace @namespace@ cascade;
create namespace @namespace@;
use @namespace@;
CREATE APPLICATION @appName@;
create flow @flowName@;
CREATE SOURCE @appName@_s1 USING Global.FileReader (
  wildcard: 'posdata100.csv',
  blocksize: 64,
  directory: '@TestDataDir@',
  positionByEOF:false)
PARSE USING Global.DSVParser ()
OUTPUT TO @appName@_st1;
end flow srcFlow;
create target @appName@_t1 using NullWriter() input from @appName@_st1;
END APPLICATION @appName@;

STOP APPLICATION App1;
UNDEPLOY APPLICATION App1;
DROP APPLICATION App1 CASCADE;
CREATE APPLICATION App1;
CREATE FLOW AgentFlow;
CREATE OR REPLACE SOURCE App1_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App1_SampleStream;
END FLOW AgentFlow;
CREATE FLOW ServerFlow;
CREATE OR REPLACE TARGET App1_NullTarget using NullWriter()
INPUT FROM App1_SampleStream;
END FLOW ServerFlow;
END APPLICATION App1;
deploy application App1 on any in ServerDG1 with AgentFlow on any in Agents, ServerFlow on any in ServerDG1;
START APPLICATION App1;

STOP APPLICATION App2;
UNDEPLOY APPLICATION App2;
DROP APPLICATION App2 CASCADE;
CREATE APPLICATION App2;
CREATE FLOW AgentFlow2;
CREATE OR REPLACE SOURCE App2_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App2_SampleStream;
END FLOW AgentFlow2;
CREATE FLOW ServerFlow2;
CREATE OR REPLACE TARGET App2_NullTarget using NullWriter()
INPUT FROM App2_SampleStream;
END FLOW ServerFlow2;
END APPLICATION App2;
deploy application App2 on any in ServerDG1 with AgentFlow2 on any in Agents, ServerFlow2 on any in ServerDG1;
START APPLICATION App2;

STOP APPLICATION App3;
UNDEPLOY APPLICATION App3;
DROP APPLICATION App3 CASCADE;
CREATE APPLICATION App3;
CREATE OR REPLACE SOURCE App3_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App3_SampleStream;
CREATE OR REPLACE TARGET App3_NullTarget using NullWriter()
INPUT FROM App3_SampleStream;
END APPLICATION App3;
DEPLOY APPLICATION App3;
START APPLICATION App3;

STOP APPLICATION App4;
UNDEPLOY APPLICATION App4;
DROP APPLICATION App4 CASCADE;
CREATE APPLICATION App4;
CREATE OR REPLACE SOURCE App4_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App4_SampleStream;
CREATE OR REPLACE TARGET App4_NullTarget using NullWriter()
INPUT FROM App4_SampleStream;
END APPLICATION App4;
DEPLOY APPLICATION App4 ON ONE IN ServerDG1;
START APPLICATION App4;

STOP APPLICATION App5;
UNDEPLOY APPLICATION App5;
DROP APPLICATION App5 CASCADE;
CREATE APPLICATION App5;
CREATE OR REPLACE SOURCE App5_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App5_SampleStream;
CREATE OR REPLACE TARGET App5_NullTarget using NullWriter()
INPUT FROM App5_SampleStream;
END APPLICATION App5;
DEPLOY APPLICATION App5 ON ONE IN ServerDG1;
START APPLICATION App5;

STOP APPLICATION App6;
UNDEPLOY APPLICATION App6;
DROP APPLICATION App6 CASCADE;
CREATE APPLICATION App6;
CREATE OR REPLACE SOURCE App6_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App6_SampleStream;
CREATE OR REPLACE TARGET App6_NullTarget using NullWriter()
INPUT FROM App6_SampleStream;
END APPLICATION App6;
DEPLOY APPLICATION App6 ON ONE IN ServerDG1;
START APPLICATION App6;

STOP APPLICATION App7;
UNDEPLOY APPLICATION App7;
DROP APPLICATION App7 CASCADE;
CREATE APPLICATION App7;
CREATE OR REPLACE SOURCE App7_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App7_SampleStream;
CREATE OR REPLACE TARGET App7_NullTarget using NullWriter()
INPUT FROM App7_SampleStream;
END APPLICATION App7;
DEPLOY APPLICATION App7 ON ONE IN ServerDG1;
START APPLICATION App7;

STOP APPLICATION App8;
UNDEPLOY APPLICATION App8;
DROP APPLICATION App8 CASCADE;
CREATE APPLICATION App8;
CREATE OR REPLACE SOURCE App8_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App8_SampleStream;
CREATE OR REPLACE TARGET App8_NullTarget using NullWriter()
INPUT FROM App8_SampleStream;
END APPLICATION App8;
DEPLOY APPLICATION App8 ON ONE IN ServerDG1;
START APPLICATION App8;


STOP APPLICATION App9;
UNDEPLOY APPLICATION App9;
DROP APPLICATION App9 CASCADE;
CREATE APPLICATION App9;
CREATE OR REPLACE SOURCE App9_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App9_SampleStream;
CREATE OR REPLACE TARGET App9_NullTarget using NullWriter()
INPUT FROM App9_SampleStream;
END APPLICATION App9;
DEPLOY APPLICATION App9;
START APPLICATION App9;

STOP APPLICATION App10;
UNDEPLOY APPLICATION App10;
DROP APPLICATION App10 CASCADE;
CREATE APPLICATION App10;
CREATE OR REPLACE SOURCE App10_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App10_SampleStream;
CREATE OR REPLACE TARGET App10_NullTarget using NullWriter()
INPUT FROM App10_SampleStream;
END APPLICATION App10;
DEPLOY APPLICATION App10;
START APPLICATION App10;

STOP APPLICATION App11;
UNDEPLOY APPLICATION App11;
DROP APPLICATION App11 CASCADE;
CREATE APPLICATION App11;
CREATE OR REPLACE SOURCE App11_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App11_SampleStream;
CREATE OR REPLACE TARGET App11_NullTarget using NullWriter()
INPUT FROM App11_SampleStream;
END APPLICATION App11;
DEPLOY APPLICATION App11;
START APPLICATION App11;

STOP APPLICATION App12;
UNDEPLOY APPLICATION App12;
DROP APPLICATION App12 CASCADE;
CREATE APPLICATION App12;
CREATE OR REPLACE SOURCE App12_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App12_SampleStream;
CREATE OR REPLACE TARGET App12_NullTarget using NullWriter()
INPUT FROM App12_SampleStream;
END APPLICATION App12;
DEPLOY APPLICATION App12;
START APPLICATION App12;

STOP APPLICATION App13;
UNDEPLOY APPLICATION App13;
DROP APPLICATION App13 CASCADE;
CREATE APPLICATION App13;
CREATE FLOW AgentFlow13;
CREATE OR REPLACE SOURCE App13_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App13_SampleStream;
END FLOW AgentFlow13;
CREATE FLOW ServerFlow13;
CREATE OR REPLACE TARGET App13_NullTarget using NullWriter()
INPUT FROM App13_SampleStream;
END FLOW ServerFlow13;
END APPLICATION App13;
deploy application App13 on any in ServerDG1 with AgentFlow13 on any in Agents, ServerFlow13 on any in ServerDG1;
START APPLICATION App13;

STOP APPLICATION OJETTOBIGQUERY;
UNDEPLOY APPLICATION OJETTOBIGQUERY;
DROP APPLICATION OJETTOBIGQUERY CASCADE;

--create application 
CREATE APPLICATION OJETTOBIGQUERY RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE ojetSrc USING Ojet (
 ConnectionURL: '192.168.123.12:1521/ORCL',
 Tables: 'QATEST.OJETTOBIGQALLDATATYPE',
 Username: 'qatest',
 Password: 'qatest',
 FetchSize:1
) OUTPUT TO CDCStream;

CREATE OR REPLACE TARGET bqtables using BigqueryWriter(
 serviceAccountKey:"/Users/karthikmurugan/Downloads/bqtest-540227c31980.json",
 projectId:"bqtest-158706",
 datalocation: 'US',
 Tables: "QATEST.OJETTOBIGQALLDATATYPE,QATEST.OJETTOBIGQALLDATATYPE",
 BatchPolicy: "eventCount:1,Interval:90")
INPUT FROM CDCStream;


CREATE OR REPLACE TARGET T1 using SysOut(name :Foo2out) INPUT FROM CDCStream;

END APPLICATION OJETTOBIGQUERY;

DEPLOY APPLICATION OJETTOBIGQUERY;
START APPLICATION OJETTOBIGQUERY;

-- The PosAppAgent sample application demonstrates how a credit card
-- payment processor might use Striim to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.


CREATE APPLICATION PosAppAgent;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosAppAgent application.

-- source CsvAgentDataSource

CREATE FLOW AgentFlow;

CREATE source CsvAgentDataSource USING FileReader (
  directory: 'C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvAgentStream;

END FLOW AgentFlow;

-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells Striim that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells Striim to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvAgentStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvAgentToPosDataCq

CREATE FLOW ProcessFlow;

CREATE CQ CsvAgentToPosDataCq
INSERT INTO PosDataAgentStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvAgentStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP.
--
-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as
-- the types to be used in PosDataAgentStream, which is created automatically.

-- The DATETIME field from the source is converted to both a dateTime
-- value, used as the event timestamp by the application, and (via the
-- function) an integer hourValue, which is used to look up
-- historical hourly averages from the HourlyAgentAveLookup cache,
-- discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)
-- 17 (hourValue)
-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateAgentOnly
--
-- The AgentPosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAgentAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAgentAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW AgentPosData5Minutes
OVER PosDataAgentStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantAgentHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAgentAveLookup using FileReader (
  directory: 'C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'merchantId') OF MerchantAgentHourlyAve;

CREATE TYPE MerchantTxRateAgent(
  merchantId String KEY,
  zip String,
  startTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateAgentOnlyStream OF MerchantTxRateAgent PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateAgentOnly
INSERT INTO MerchantTxRateAgentOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM AgentPosData5Minutes p, HourlyAgentAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAgentAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateAgentWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE STREAM MerchantTxRateAgentWithStatusStream OF MerchantTxRateAgent;

CREATE CQ GenerateMerchantTxRateAgentWithStatus
INSERT INTO MerchantTxRateAgentWithStatusStream
SELECT merchantId,
       zip,
       startTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateAgentOnlyStream;


-- WAction store MerchantActivityAgent
--
-- The following group of statements create and populate the MerchantActivityAgent
-- WAction store. Data from the MerchantTxRateAgentWithStatusStream is enhanced
-- with merchant details from NameLookupAgent cache and with latitude and longitude
-- values from the USAddressDataAgent cache.

CREATE TYPE MerchantActivityAgentContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivityAgent CONTEXT OF MerchantActivityAgentContext
EVENT TYPES ( MerchantTxRateAgent )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );


CREATE TYPE MerchantAgentNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressDataAgent(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookupAgent using FileReader (
  directory:'C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false,
  trimwhitespace:true
)
QUERY(keytomap:'merchantId') OF MerchantAgentNameData;

CREATE CACHE ZipLookupAgent using FileReader (
  directory: 'C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'zip') OF USAddressDataAgent;


CREATE CQ GenerateWactionAgentContext
INSERT INTO MerchantActivityAgent
SELECT  m.merchantId,
        m.startTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateAgentWithStatusStream m, NameLookupAgent n, ZipLookupAgent z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

-- CQ GenerateAgentAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertAgentStream OF Global.AlertEvent;

CREATE CQ GenerateAgentAlerts
INSERT INTO AlertAgentStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateAgentWithStatusStream m, NameLookupAgent n
WHERE m.merchantId = n.merchantId;

CREATE SUBSCRIPTION AgentAlertSub USING WebAlertAdapter( ) INPUT FROM AlertAgentStream;

END FLOW ProcessFlow;

END APPLICATION PosAppAgent;

DEPLOY APPLICATION PosAppAgent with AgentFlow in AGENTS, ProcessFlow in default;

-- CREATE DASHBOARD USING "C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData/PosAppDashboard.json";

stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@;

CREATE SOURCE @SourceName@ USING OracleReader  (
ReaderType: 'LogMiner',
  Password_encrypted: 'false',
  SupportPDB: false,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  QueueSize: 2048,
  CommittedTransactions: true,
  Username: '@UserName@',
  TransactionBufferType: 'Memory',
  _h_ReturnDateTimeAs: 'ZonedDateTime',
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  OutboundServerProcessName: 'WebActionXStream',
  Password: '@Password@',
  DDLCaptureMode: 'All',
  Compression: false,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  FetchSize: 1,
  Tables: '@SourceTables@',
  DictionaryMode: 'OnlineCatalog',
  XstreamTimeOut: 600,
  TransactionBufferSpilloverSize: '1MB',
  StartTimestamp: 'null',
  FilterTransactionBoundaries: true,
  StartSCN: null,
  ConnectionURL: '@ConnectionURL@',
  SendBeforeImage: true )
OUTPUT TO @AppStream@  ;

CREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream SELECT data[2], to_string(data[2], "dd-MMM-yy hh.mm.ss") FROM @AppStream@ o ;

CREATE  TARGET @targetsys@ USING Global.SysOut  (
name: 'ora1_sys' )
INPUT FROM admin.ZDT_cq_stream;

create Target @TargetFile@ using FileWriter(
  filename:'toStringOut.log',
  directory:'@FilePath@',
  rolloverpolicy:'eventcount:1000'
)
format using DSVFormatter (

)
input from admin.ZDT_cq_stream;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.OJet (
  PrimaryDatabasePassword: '@PRIMARY_PASSWORD@',
  ConnectionURL: '@DOWNSTREAM_URL@',
  PrimaryDatabaseUsername: '@PRIMARY_USER@',
  Password: '@DOWNSTREAM_PASSWORD@',
  DownstreamCaptureMode: 'REAL_TIME',
  DownstreamCapture: true,
  PrimaryDatabaseConnectionURL: '@PRIMARY_URL@',
  Tables: '@SOURCE_TABLES@',
  Username: '@DOWNSTREAM_USER@'
  )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET_SYS@ USING Global.SysOut (
  name: 'Out' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter (
  ConnectionURL: '@TARGET_URL@',
  Username: '@TARGET_USER@',
  Password: '@TARGET_PASSWORD@',
  CheckPointTable: 'CHKPOINT',
  CommitPolicy: 'EventCount:1',
  Tables: '@TARGET_TABLES@',
  BatchPolicy: 'EventCount:1' )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

stop application ManyToManyADLSGen2;
undeploy application ManyToManyADLSGen2;
drop application ManyToManyADLSGen2 cascade;

create application ManyToManyADLSGen2 Recovery 5 second interval;


create type csv_type_gen2(
id String,
name String,
seq String
);

create type Order_type_gen2(
id String,
Name String,
Company String
);



create source CSVSource_multi_gen2 using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'Canon1000_All.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO CsvStream_user_gen2;

create source CSVSource2_gen2 using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'portfolio.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO CsvStream_gen2;

create source CSVSource3_gen2 using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'DataCenterData.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO CsvStream_gen2;


CREATE SOURCE OraSource1_gen2 USING OracleReader
(
 Username:'miner',
 Password:'miner',
 ConnectionURL:'localhost:1521:xe',
 Tables:'QATEST.CUSTOMER1,QATEST.CUSTOMER2,QATEST.CUSTOMER3',
 OnlineCatalog:true,
 FetchSize:10000,
 QueueSize:2148,
 CommittedTransactions:false,
 Compression:true
)
OUTPUT TO OrdersStream_gen2;

CREATE SOURCE OraSource2_gen2 USING OracleReader
(
 Username:'miner',
 Password:'miner',
 ConnectionURL:'localhost:1521:xe',
 Tables:'QATEST.CUSTOMER4,QATEST.CUSTOMER5',
 OnlineCatalog:true,
 FetchSize:10000,
 QueueSize:2148,
 CommittedTransactions:false,
 Compression:true
)
OUTPUT TO OrdersStream_gen2;


create Target ADLSGen2_tgt1 using ADLSGen2Writer(
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatest',
        directory:'%@metadata(FileName)%',
        filename:'event_data.csv',
        uploadpolicy:'eventcount:5'
)
FORMAT USING JSONFormatter()
input from CsvStream_gen2; 

create Target ADLSGen2_tgt2 using ADLSGen2Writer(
          accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'%@metadata(TableName)%',
        filename:'table.csv',
        uploadpolicy:'filesize:10M',
        compressiontype: 'true'
)
format using DSVFormatter (
)
input from OrdersStream_gen2; 

create stream UserdataStream_gen2 of Global.WAEvent;

Create CQ CQUser_gen2
insert into UserdataStream_gen2
select 
putuserdata (data1,'Fileowner',data[1]) from CsvStream_user_gen2 data1;

create stream CSVTypedStream1_gen2 of csv_type_gen2;
create stream CSVTypedStream2_gen2 of csv_type_gen2;
create stream CSVTypedStream3_gen2 of csv_type_gen2;

CREATE CQ cq1_gen2
INSERT INTO CSVTypedStream1_gen2
SELECT data[0],
data[1],
data[2]
FROM UserdataStream_gen2
WHERE USERDATA(UserdataStream_gen2,'Fileowner').toString() == 'Lorem';

CREATE CQ cq2
INSERT INTO CSVTypedStream2_gen2
SELECT data[0],
data[1],
data[2]
FROM UserdataStream_gen2
WHERE USERDATA(UserdataStream_gen2,'Fileowner').toString() == 'doloremque';

CREATE CQ cq3
INSERT INTO CSVTypedStream3_gen2
SELECT data[0],
data[1],
data[2]
FROM UserdataStream_gen2
WHERE USERDATA(UserdataStream_gen2,'Fileowner').toString() == 'accusantium';

create Target ADLSGen2_tgt3 using ADLSGen2Writer(
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'1up_%name%',
        filename:'many_event_data.csv',
        uploadpolicy:'eventcount:74'
)
format using XMLFormatter (
  elementtuple: 'Eventname:name:id:seq:text=name',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from CSVTypedStream1_gen2; 

create Target ADLSGen2_tgt4 using ADLSGen2Writer(
         accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'2up_%name%',
        filename:'many_event_data.csv',
        uploadpolicy:'interval:10s'
)
format using XMLFormatter (
  elementtuple: 'Eventname:name:id:seq:text=name',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from CSVTypedStream2_gen2; 

create Target ADLSGen2_tgt5 using ADLSGen2Writer(
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'3up_%name%',
        filename:'many_event_data.csv',
        uploadpolicy:'eventcount:10,intervals:30s'
)
format using XMLFormatter (
  elementtuple: 'Eventname:name:id:seq:text=name',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from CSVTypedStream3_gen2; 


create stream OrderTypedStream1_gen2 of Order_type_gen2;
create stream OrderTypedStream2_gen2 of Order_type_gen2;
create stream OrderTypedStream3_gen2 of Order_type_gen2;

CREATE CQ cq1_gen2_db
INSERT INTO OrderTypedStream1_gen2
SELECT data[0],
data[1],
data[2]
FROM OrdersStream_gen2
WHERE META(OrdersStream_gen2,'TableName').toString() == 'QATEST.CUSTOMER1';

CREATE CQ cq2_gen2_db
INSERT INTO OrderTypedStream2_gen2
SELECT data[0],
data[1],
data[2]
FROM OrdersStream_gen2
WHERE META(OrdersStream_gen2,'TableName').toString() == 'QATEST.CUSTOMER2';

CREATE CQ cq3_gen2_db
INSERT INTO OrderTypedStream3_gen2
SELECT data[0],
data[1],
data[2]
FROM OrdersStream_gen2
WHERE META(OrdersStream_gen2,'TableName').toString() == 'QATEST.CUSTOMER3';


create Target ADLSGen2_tgt6 using ADLSGen2Writer(
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'Customer1',
        filename:'many_event_data.csv',
        uploadpolicy:'eventcount:10000'
)
format using AvroFormatter (
  formatAs: 'Default',
  handler: 'com.webaction.proc.AvroFormatter',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA@1.avsc'
)
input from OrderTypedStream1_gen2; 

create Target ADLSGen2_tgt7 using ADLSGen2Writer(
       accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'Customer2',
        filename:'many_event_data.csv',
        uploadpolicy:'eventcount:10000'
)
format using AvroFormatter (
  formatAs: 'Default',
  handler: 'com.webaction.proc.AvroFormatter',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA@2.avsc'
)
input from OrderTypedStream2_gen2; 

create Target ADLSGen2_tgt8 using ADLSGen2Writer(
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'Customer3',
        filename:'many_event_data.csv',
        uploadpolicy:'eventcount:10000'
)
format using AvroFormatter (
  formatAs: 'Default',
  handler: 'com.webaction.proc.AvroFormatter',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA@3.avsc'
)
input from OrderTypedStream3_gen2; 

end application ManyToManyADLSGen2;

deploy application ManyToManyADLSGen2;
start application ManyToManyADLSGen2;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;

CREATE FLOW OracleTokudu1;
	Create Source oracSource1
	 Using OracleReader
	(
	 Username:'@LOGMINER-UNAME@',
 	Password:'@LOGMINER-PASSWORD@',
 	ConnectionURL:'@LOGMINER-URL@',
 	Tables:'@SOURCE_TABLES@',
 	OnlineCatalog:true,
 	FetchSize:1
	) Output To DataStream1;

	CREATE TARGET WriteintoKudu1 using KuduWriter (
	kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
	pkupdatehandlingmode:'@MODE@',
	tables: '@TARGET_TABLES@',
	batchpolicy: 'EventCount:1,Interval:0')
	INPUT FROM DataStream1;
END FLOW OracleTokudu1;

CREATE FLOW OracleTokudu2;
	Create Source oracSource2
	 Using OracleReader
	(
	 Username:'@LOGMINER-UNAME@',
 	Password:'@LOGMINER-PASSWORD@',
 	ConnectionURL:'@LOGMINER-URL@',
 	Tables:'@SOURCE_TABLES@',
 	OnlineCatalog:true,
 	FetchSize:1
	) Output To DataStream2;

	CREATE TARGET WriteintoKudu2 using KuduWriter (
	kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
	pkupdatehandlingmode:'@MODE@',
	tables: '@TARGET_TABLES@',
	batchpolicy: 'EventCount:10,Interval:10')
	INPUT FROM DataStream2;
END FLOW OracleTokudu2;

CREATE FLOW OracleTokudu3;
	Create Source oracSource3
	 Using OracleReader
	(
	 Username:'@LOGMINER-UNAME@',
 	Password:'@LOGMINER-PASSWORD@',
 	ConnectionURL:'@LOGMINER-URL@',
 	Tables:'@SOURCE_TABLES@',
 	OnlineCatalog:true,
 	FetchSize:1
	) Output To DataStream3;

	CREATE TARGET WriteintoKudu3 using KuduWriter (
	kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
	pkupdatehandlingmode:'@MODE@',
	tables: '@TARGET_TABLES@',
	batchpolicy: 'EventCount:5,Interval:120')
	INPUT FROM DataStream3;
END FLOW OracleTokudu3;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

use PosTester;
alter application PosApp;

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
@PERSIST-TYPE@

end application PosApp;

alter application PosApp recompile;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@ Using Ojet
(
Username:'@OJET-UNAME@',
Password:'@OJET-PASSWORD@',
ConnectionURL:'@OCI-URL@',
Tables:'@SourceTable@',
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
Username:'@UN@',
Password:'@PWD@',
BatchPolicy:'EventCount:1,Interval:1',
Tables: '@Tablemapping@'
)INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP application AlterTester.DSV;
undeploy application AlterTester.DSV;
drop application AlterTester.DSV cascade;


create application DSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;


end application DSV;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@ recovery 5 SECOND Interval;
CREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: '@ORACLE-URL@',
  Tables: '@SOURCE-TABLES@',
  Username: '@ORACLE-USERNAME@',
  Password: '@ORACLE-PASSWORD@'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt USING SnowflakeWriter

(
  ConnectionURL:'@SNOWFLAKE-URL@',
  username:'@SNOWFLAKE-USERNAME@',
  appendOnly:'false',
  Tables:'@TARGET-TABLES@',
  uploadpolicy:'eventcount:3,interval:10s',
  externalStageType:'local'
)
INPUT FROM @APP_NAME@_Stream;


END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

CREATE SOURCE @SOURCE_NAME@ USING Global.incREmEnTALBatchrEADer (
  StartPosition: '@startPosition@',
  ConnectionURL: '@sourceURL@',
  Username: '@userName@',
  Tables: '@tables@',
  CheckColumn: '@checkColum@',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  Password: '@password@',
  Password_encrypted: 'false',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 5 Second interval;
create source @APPNAME@_Source using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'dynamicdirectory.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO @APPNAME@_Stream;

Create Type @APPNAME@_type (
  id String,
  name String,
  department String,
  yoj String,
  moj String,
  doj int
);

Create Stream @APPNAME@_typedstream of @APPNAME@_type;

CREATE CQ @APPNAME@_cq
INSERT INTO @APPNAME@_typedstream
SELECT data[0],
       data[1],
       data[2],
       data[3],
       data[4],
       TO_INT(data[5])
FROM @APPNAME@_stream;

create Target @APPNAME@_target using ADLSGen1Writer(
        filename:'',
        directory:'',
        datalakestorename:'',
        clientid:'',
        authtokenendpoint:'',
        clientkey:'',
		rolloverpolicy:'eventcount:5'
)
format using DSVFormatter (
)
input from @APPNAME@_typedstream; 

end application @APPNAME@;

deploy application @APPNAME@;
start application @APPNAME@;

CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @APPNAME@_src USING Global.GCSReader (
  _h_GCSQueryCoolingTime: '1',
  ServiceAccountKey: '',
  ProjectId: '',
  DownloadPolicy: '',
  BucketName: '',
  FolderName: '',
  ObjectFilter: '',
  ObjectDetectionMode: '',
  IncludeSubfolders: false )
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE CQ @APPNAME@_CQ
INSERT INTO @APPNAME@_CQOut
SELECT
    data[0] as BusinessName,
    data[1] as MerchantId,
    data[2] as PosDataCode,
    data[3] as AccNumber,
    data[4] as DateTime,
    data[5] as ExpDate,
    data[6] as CurrencyCode,
    data[7] as AuthAmount,
    data[8] as TerminalId,
    data[9] as Zip,
    data[10] as City
FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_KafkaTarget USING Global.KafkaWriter VERSION @KAFKA_VERSION@(
  brokerAddress: '',
  Topic: '',
  Mode: 'Sync' )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_HDFSTarget USING Global.HDFSWriter (
  flushpolicy: '',
  rolloverpolicy: '',
  directory: '',
  hadoopurl: '',
  hadoopconfigurationpath: '',
  filename: '')
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_BlobTarget USING Global.AzureBlobWriter (
  containername: '',
  blobname: '',
  accountaccesskey: '',
  accountname: '',
  foldername: '' )
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_DWHTarget USING Global.BigQueryWriter (
  Tables: '',
  BatchPolicy: '',
  projectId: '',
  ServiceAccountKey: '' )
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_NoSqlTarget USING Global.MongoDBWriter (
  AuthDB: 'admin',
  ConnectionURL: '',
  Username: '',
  collections: '',
  Password: '' )
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_FileTarget USING Global.FileWriter (
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '',
  filename: '' )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_OLTPTarget USING Global.DatabaseWriter (
  ConnectionURL: '',
  Password: '',
  Username: '',
  Tables: '' )
INPUT FROM @APPNAME@_CQOut;

CREATE SOURCE @APPNAME@_KafkaSource USING KafkaReader VERSION @KAFKA_VERSION@ (
  brokerAddress: '',
  Topic: '',
  startOffset: '0' )
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@_Stream2;

CREATE OR REPLACE TARGET @APPNAME@_KafkaFileTarget USING Global.FileWriter (
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '',
  filename: '' )
FORMAT USING JSONFormatter(
members:'data')
INPUT FROM @APPNAME@_Stream2;

END APPLICATION @APPNAME@;

CREATE APPLICATION @APPNAME@;

CREATE TYPE @APPNAME@type1 (
 companyName java.lang.String,
 merchantId java.lang.String
 );

CREATE STREAM @APPNAME@TypedStream OF @APPNAME@type1;

CREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (
  wildcard: '',
  positionByEOF: false,
  directory: ''
  )
PARSE USING DSVParser (
header:'true'
)
OUTPUT TO @APPNAME@Stream;

CREATE OR REPLACE CQ @APPNAME@CQ
INSERT INTO @APPNAME@TypedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantId
FROM @APPNAME@Stream;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING XMLFormatter(
  rootelement:'JMSXMLIN',
  elementtuple:'companyName:merchantId:text=companyName'
)
INPUT FROM @APPNAME@TypedStream;

END APPLICATION @APPNAME@;

stop ROLLUPMON_CDC;
undeploy application ROLLUPMON_CDC;
alter application ROLLUPMON_CDC;
CREATE or replace FLOW ROLLUPMON_CDC_flow;
Create or replace Source ROLLUPMON_CDC_Oraclesrc Using oraclereader(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',
 Tables:'QATEST.ROLLUPMON_TABLE1;QATEST.ROLLUPMON_TABLE2;QATEST.ROLLUPMON_TABLE3;QATEST.ROLLUPMON_TABLE4;QATEST.ROLLUPMON_TABLE5',
 Fetchsize:1000,
 connectionRetryPolicy:'maxRetries=4',
 _h_fetchexactrowcount: 'true'
)
Output To ROLLUPMON_CDC_OrcStrm;
END FLOW ROLLUPMON_CDC_flow;
alter application ROLLUPMON_CDC recompile;
DEPLOY APPLICATION ROLLUPMON_CDC;
start application ROLLUPMON_CDC;

--
-- Recovery Test 50 is a simple Kafka test
-- Nicholas Keene WebAction, Inc.
--
-- S -> CQ -> KSu -> JWc10 -> WS
--

STOP Recov50Tester.RecovTest50;
UNDEPLOY APPLICATION Recov50Tester.RecovTest50;
DROP APPLICATION Recov50Tester.RecovTest50 CASCADE;
CREATE APPLICATION RecovTest50 RECOVERY 5 SECOND INTERVAL;

CREATE or REPLACE TYPE KafkaType(
  value java.lang.Long KEY  
);

CREATE SOURCE KafkaSource USING NumberSource ( 
  lowValue: '1',
  highValue: '1003',
  delayMillis: '10',
  delayNanos: '0',
  repeat: 'false'
 ) 
OUTPUT TO NumberSourceOut;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaStream OF KafkaType using KafkaProps;

CREATE OR REPLACE CQ KafkaStreamPopulate 
INSERT INTO KafkaStream
SELECT data[1]
FROM NumberSourceOut;

CREATE JUMPING WINDOW SizeTenWindow
OVER KafkaStream KEEP 10 ROWS;


CREATE WACTIONSTORE Wactions CONTEXT of KafkaType
@PERSIST-TYPE@

CREATE CQ WactionsPopulate
INSERT INTO Wactions
SELECT * FROM SizeTenWindow;

END APPLICATION RecovTest50;

--
-- Recovery Test 50 is a simple Kafka test
-- Nicholas Keene WebAction, Inc.
--
-- S -> CQ -> KSu -> JWc10 -> WS
--

STOP Recov50Tester.RecovTest50;
UNDEPLOY APPLICATION Recov50Tester.RecovTest50;
DROP APPLICATION Recov50Tester.RecovTest50 CASCADE;
CREATE APPLICATION RecovTest50 RECOVERY 5 SECOND INTERVAL;

CREATE or REPLACE TYPE KafkaType(
  value java.lang.Long KEY  
);

CREATE SOURCE KafkaSource USING NumberSource ( 
  lowValue: '1',
  highValue: '1003',
  delayMillis: '10',
  delayNanos: '0',
  repeat: 'false'
 ) 
OUTPUT TO NumberSourceOut;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaStream OF KafkaType using KafkaProps;

CREATE OR REPLACE CQ KafkaStreamPopulate 
INSERT INTO KafkaStream
SELECT data[1]
FROM NumberSourceOut;

CREATE JUMPING WINDOW SizeTenWindow
OVER KafkaStream KEEP 10 ROWS;


CREATE WACTIONSTORE Wactions CONTEXT of KafkaType
@PERSIST-TYPE@

CREATE CQ WactionsPopulate
INSERT INTO Wactions
SELECT * FROM SizeTenWindow;

END APPLICATION RecovTest50;

use PosTester;
alter application PosApp;

CREATE CACHE HourlyAveLookup using CSVReader (
  directory: 'Samples/Customer/PosApp/appData',
  wildcard: 'hourlyData.txt',
  header: Yes,
  columndelimiter: ',',
  trimquote:false
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

end application PosApp;

alter application PosApp recompile;

--
-- Canon Test W30
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for an unpartitioned jumping count window
--
-- S -> JWc5u -> CQ -> WS
--


UNDEPLOY APPLICATION NameW30.W30;
DROP APPLICATION NameW30.W30 CASCADE;
CREATE APPLICATION W30 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW30;


CREATE SOURCE CsvSourceW30 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW30;


END FLOW DataAcquisitionW30;



CREATE FLOW DataProcessingW30;

CREATE TYPE DataTypeW30 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW30 OF DataTypeW30;

CREATE CQ CSVStreamW30_to_DataStreamW30
INSERT INTO DataStreamW30
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW30;

CREATE JUMPING WINDOW JWc5uW30
OVER DataStreamW30
KEEP 5 ROWS;

CREATE WACTIONSTORE WactionStoreW30 CONTEXT OF DataTypeW30
EVENT TYPES ( DataTypeW30 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWc5uW30_to_WactionStoreW30
INSERT INTO WactionStoreW30
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc5uW30;

END FLOW DataProcessingW30;



END APPLICATION W30;

--
-- Crash Recovery Test 1 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP APPLICATION N2S2CR1Tester.N2S2CRTest1;
UNDEPLOY APPLICATION N2S2CR1Tester.N2S2CRTest1;
DROP APPLICATION N2S2CR1Tester.N2S2CRTest1 CASCADE;
CREATE APPLICATION N2S2CRTest1 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest1;

CREATE SOURCE CsvSourceN2S2CRTest1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN2S2CRTest1;

CREATE FLOW DataProcessingN2S2CRTest1;

CREATE TYPE WactionTypeN2S2CRTest1 (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE WactionsN2S2CRTest1 CONTEXT OF WactionTypeN2S2CRTest1
EVENT TYPES ( WactionTypeN2S2CRTest1 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN2S2CRTest1
INSERT INTO WactionsN2S2CRTest1
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END FLOW DataProcessingN2S2CRTest1;

END APPLICATION N2S2CRTest1;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE Source @SourceName@ USING Global.MSSqlReader (
  PollingInterval: 5,
  FetchTransactionMetadata: false,
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  TransactionSupport: true,
  StartPosition: 'NOW',
  Tables: 'dbo.sourceTable',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n',
  ConnectionPoolSize: 10,
  Compression: true,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'qatest',
  Username: 'qatest',
  FetchSize: 0,
  IntegratedSecurity: false,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  AutoDisableTableCDC: false )
OUTPUT TO @SRCINPUTSTREAM@;

CREATE TARGET @targetsys@ USING Global.SysOut (
  name: 'sysout' )
INPUT FROM @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
  BatchPolicy: 'EventCount:10000,Interval:30',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:10000,Interval:100',
  StatementCacheSize: '50',
  Password: 'w3b@ct10n',
  Username: 'qatest',
  IgnorableExceptionCode: '547,DUPLICATE_ROW_EXISTS,NO_OP_UPDATE,NO_OP_DELETE,NO_OP_PKUPDATE',
  DatabaseProviderType: 'SQLServer',
  PreserveSourceTransactionBoundary: 'false',
  Tables: 'dbo.sourceTable,dbo.targetTable',
  VendorConfiguration: 'enableIdentityInsert=true',
  adapterName: 'DatabaseWriter' )
INPUT FROM @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;

STOP APPLICATION OneAgentTester.CSV;
UNDEPLOY APPLICATION OneAgentTester.CSV;
DROP APPLICATION OneAgentTester.CSV cascade;

DROP USER OneAgentTester;
DROP NAMESPACE OneAgentTester CASCADE;
CREATE USER OneAgentTester IDENTIFIED BY OneAgentTester;
GRANT create,drop ON deploymentgroup Global.* To user OneAgentTester;
CONNECT OneAgentTester OneAgentTester;

CREATE APPLICATION CSV;

CREATE FLOW AgentFlow;
create source CSVSource using FileReader
(
directory:'@TEST-DATA-PATH@',
wildcard:'StoreNames.csv',
positionByEOF:false
)
parse using DSVParser
(
header:'yes',
columndelimiter:','
)
OUTPUT TO CsvStream;
END FLOW AgentFlow;

CREATE FLOW ServerFlow;
CREATE TARGET myout using LogWriter(name: OneAgentSource, filename:'@FEATURE-DIR@/logs/log.txt') input from CsvStream;
END FLOW ServerFlow;

END APPLICATION CSV;
DEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;
START CSV;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @kafkasrc@ USING KafkaReader VERSION @KAFKAVERSION@ (
  brokerAddress: '',
  Topic: '',
  startOffset: '0' )
PARSE USING AvroParser (
  schemaregistryurl: 'http://localhost:8081/' )
OUTPUT TO @appname@Stream2;

CREATE TYPE @appname@ElementsOfNativeRecord1 (
 datarecord com.fasterxml.jackson.databind.JsonNode,
 before com.fasterxml.jackson.databind.JsonNode,
 metadata com.fasterxml.jackson.databind.JsonNode,
 userdata com.fasterxml.jackson.databind.JsonNode,
 datapresenceinfo com.fasterxml.jackson.databind.JsonNode,
 beforepresenceinfo com.fasterxml.jackson.databind.JsonNode);

CREATE TYPE @appname@completeRecord1 (
 completedata com.fasterxml.jackson.databind.JsonNode);

CREATE STREAM @appname@NativeRecordStream1 OF @appname@ElementsOfNativeRecord1;

CREATE STREAM @appname@CompleteRecordInJSONStream1 OF @appname@completeRecord1;

CREATE TARGET @filetarget@ USING FileWriter (
  filename: 'kafkaout',
  directory: ''
  rolloverpolicy: 'EventCount:10' )
FORMAT USING JSONFormatter  (
  members: 'datarecord' )
INPUT FROM @appname@NativeRecordStream1;

CREATE CQ @appname@GetNativeRecordInJSONCQ1
INSERT INTO @appname@NativeRecordStream1
SELECT
 completedata.get("data"),
 completedata.get("before"),
 completedata.get("metadata"),
 completedata.get("userdata"),
 completedata.get("datapresenceinfo"),
 completedata.get("beforepresenceinfo")
FROM @appname@CompleteRecordInJSONStream1;

CREATE CQ @appname@CQ2
INSERT INTO @appname@CompleteRecordInJSONStream1
SELECT
 AvroToJson(y.data)
 from @appname@Stream2 y;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

STOP WStester.WSusingApp;
UNDEPLOY APPLICATION WStester.WSusingApp;
DROP APPLICATION WStester.WSusingApp CASCADE;

CREATE APPLICATION WSusingApp;

CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity int,
  size int);

CREATE SOURCE liveSource using StreamReader(
  OutputType: 'WStester.Atm',
  noLimit: 'false',
  maxRows: 20,
  iterations: 0,
  iterationDelay: 100,
  StringSet: 'productID[001-002-003-004],stateID[AS-CA-WA-NY]',
  NumberSet: 'productWeight[3-3]R,quantity[20-20]R,size[250-250]R'
  )OUTPUT TO CsvStream;


CREATE STREAM newStream OF Atm;

CREATE CQ newCQ
INSERT INTO newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_INT(data[3]), TO_INT(data[4]) FROM
CsvStream;

create target myOut8 using SysOut(name : testOut8) input from newStream;

CREATE WACTIONSTORE streamActivityMEM CONTEXT OF Atm
EVENT TYPES ( Atm )
USING MEMORY;

CREATE WACTIONSTORE streamActivityES CONTEXT OF Atm
EVENT TYPES ( Atm );

CREATE WACTIONSTORE streamActivityES2 CONTEXT OF Atm
EVENT TYPES ( Atm )
USING ( storageProvider: 'elasticsearch' );

CREATE WACTIONSTORE streamActivityDerby CONTEXT OF Atm
EVENT TYPES ( Atm )
USING (
storageProvider:'jdbc',
persistence_interval:'10 sec',
JDBC_DRIVER:'@WASTORE-DRIVER@',
JDBC_URL:'@WASTORE-URL@',
JDBC_USER:'@WASTORE-UNAME@',
JDBC_PASSWORD:'@WASTORE-PASSWORD@',
DDL_GENERATION:'drop-and-create-tables',
LOGGING_LEVEL:'SEVERE');

CREATE CQ newCQ2
INSERT INTO streamActivityMEM
SELECT * FROM newStream
link source event;

CREATE CQ newCQ3
INSERT INTO streamActivityES
SELECT * FROM newStream
link source event;

CREATE CQ newCQ4
INSERT INTO streamActivityES2
SELECT * FROM newStream
link source event;

CREATE CQ newCQ5
INSERT INTO streamActivityDerby
SELECT * FROM newStream
link source event;

END APPLICATION WSusingApp;

--
-- Recovery Test 28 with two sources, two jumping time-count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5a9W  -> CQ1 -> WS
--   S2 -> Jc6a11W -> CQ2 -> WS
--

STOP Recov28Tester.RecovTest28;
UNDEPLOY APPLICATION Recov28Tester.RecovTest28;
DROP APPLICATION Recov28Tester.RecovTest28 CASCADE;
CREATE APPLICATION RecovTest28 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION RecovTest28;

use admin;
drop application ns1.testgroupapp1 cascade;
drop application ns1.testgroupapp2 cascade;
drop application ns2.testgroupapp1 cascade;
drop application ns2.testgroupapp2 cascade;
drop namespace ns1 CASCADE;
drop namespace ns2 CASCADE;

DROP APPLICATION @APP_NAME@1 FORCE;
DROP APPLICATION @APP_NAME@2 FORCE;
DROP APPLICATION @APP_NAME@3 FORCE;


CREATE APPLICATION @APP_NAME@1 WITH ENCRYPTION USE EXCEPTIONSTORE TTL : '7d';
CREATE OR REPLACE PROPERTYSET @APP_NAME@1_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',
acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');
CREATE STREAM @APP_NAME@_In1 OF Global.waevent persist using @APP_NAME@1_KafkaPropset;

CREATE OR REPLACE SOURCE @APP_NAME@1_Source USING @SOURCE_ADAPTER@  ( 
  Tables:'@SRC_TABLE@'
) OUTPUT TO @APP_NAME@_In1;

CREATE OR REPLACE TARGET @APP_NAME@1_Target USING @TARGET_ADAPTER@  ( 
  Tables: '@SRC_TABLE@, @TGT_TABLE@1', 
  CheckPointTable: '@CHK_TABLE@', 
) INPUT FROM @APP_NAME@_In1;

CREATE TARGET @APP_NAME@1_SysOut
USING SysOut(name:@APP_NAME@1Sys)
INPUT FROM @APP_NAME@_In1;

CREATE TARGET @APP_NAME@1_FileWriter USING filewriter
(filename:'@APP_NAME@_FW1.log'
)
format using dsvFormatter()
INPUT FROM @APP_NAME@_In1;

END APPLICATION @APP_NAME@1;

DEPLOY APPLICATION @APP_NAME@1;
START APPLICATION @APP_NAME@1;

CREATE APPLICATION @APP_NAME@2;

CREATE OR REPLACE SOURCE @APP_NAME@2_Source USING @SOURCE_ADAPTER@  ( 
  Tables:'@SRC_TABLE@'
) OUTPUT TO @APP_NAME@_In2;

CREATE OR REPLACE TARGET @APP_NAME@2_Target USING @TARGET_ADAPTER@  ( 
  Tables: '@SRC_TABLE@, @TGT_TABLE@2', 
  CheckPointTable: '@CHK_TABLE@'
) INPUT FROM @APP_NAME@_In2;

CREATE TARGET @APP_NAME@2_SysOut
USING SysOut(name:@APP_NAME@2Sys)
INPUT FROM @APP_NAME@_In2;

CREATE TARGET @APP_NAME@2_FileWriter USING filewriter
(filename:'@APP_NAME@_FW2.log'
)
format using dsvFormatter()
INPUT FROM @APP_NAME@_In2;

END APPLICATION @APP_NAME@2;

DEPLOY APPLICATION @APP_NAME@2;
START APPLICATION @APP_NAME@2;

CREATE APPLICATION @APP_NAME@3;
CREATE FLOW @APP_NAME@_SOURCEFLOW;
CREATE OR REPLACE SOURCE @APP_NAME@3_Source USING @SOURCE_ADAPTER@  ( 
  Tables:'@SRC_TABLE@'
) OUTPUT TO @APP_NAME@_In3;
END FLOW @APP_NAME@_SOURCEFLOW;

CREATE FLOW @APP_NAME@_TARGETFLOW;
CREATE OR REPLACE TARGET @APP_NAME@3_Target USING @TARGET_ADAPTER@  ( 
  Tables: '@SRC_TABLE@, @TGT_TABLE@3', 
  CheckPointTable: '@CHK_TABLE@', 
) INPUT FROM @APP_NAME@_In3;

CREATE TARGET @APP_NAME@3_SysOut
USING SysOut(name:@APP_NAME@3Sys)
INPUT FROM @APP_NAME@_In3;

CREATE TARGET @APP_NAME@3_FileWriter USING filewriter
(filename:'@APP_NAME@_FW3.log'
)
format using dsvFormatter()
INPUT FROM @APP_NAME@_In3;
END FLOW @APP_NAME@_TARGETFLOW;

END APPLICATION @APP_NAME@3;

DEPLOY APPLICATION @APP_NAME@3;
START APPLICATION @APP_NAME@3;

STOP APPLICATION @APP_NAME@1;
STOP APPLICATION @APP_NAME@2;

UNDEPLOY APPLICATION @APP_NAME@1;
UNDEPLOY APPLICATION @APP_NAME@2;

DROP APPLICATION @APP_NAME@1 CASCADE;
DROP APPLICATION @APP_NAME@2 CASCADE;

CREATE APPLICATION @APP_NAME@1;

CREATE OR REPLACE SOURCE @APP_NAME@1_Source USING @SOURCE_ADAPTER@  ( 
  Tables:'@SRC_TABLE@'
) OUTPUT TO @APP_NAME@_In1;

CREATE OR REPLACE TARGET @APP_NAME@1_Target USING @TARGET_ADAPTER@  ( 
  Tables: '@SRC_TABLE@, @TGT_TABLE@1', 
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  CheckPointTable: '@CHK_TABLE@', 
) INPUT FROM @APP_NAME@_In1;

CREATE TARGET @APP_NAME@_SysOut1
USING SysOut(name:@APP_NAME@1Sys)
INPUT FROM @APP_NAME@_In1;

END APPLICATION @APP_NAME@1;

DEPLOY APPLICATION @APP_NAME@1;
START APPLICATION @APP_NAME@1;


CREATE APPLICATION @APP_NAME@2;

CREATE OR REPLACE SOURCE @APP_NAME@2_Source USING @SOURCE_ADAPTER@  ( 
  Tables:'@SRC_TABLE@'
) OUTPUT TO @APP_NAME@_In2;

CREATE OR REPLACE TARGET @APP_NAME@2_Target USING @TARGET_ADAPTER@  ( 
  Tables: '@SRC_TABLE@, @TGT_TABLE@2', 
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  CheckPointTable: '@CHK_TABLE@'
) INPUT FROM @APP_NAME@_In2;

CREATE TARGET @APP_NAME@_SysOut2
USING SysOut(name:@APP_NAME@2Sys)
INPUT FROM @APP_NAME@_In2;

END APPLICATION @APP_NAME@2;

DEPLOY APPLICATION @APP_NAME@2;
START APPLICATION @APP_NAME@2;

--
-- Recovery Test 22 with two sources, two sliding attribute windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sa5W -> CQ1 -> WS
-- S2 -> Sa6W -> CQ2 -> WS
--

STOP KStreamRecov22Tester.KStreamRecovTest22;
UNDEPLOY APPLICATION KStreamRecov22Tester.KStreamRecovTest22;
DROP APPLICATION KStreamRecov22Tester.KStreamRecovTest22 CASCADE;
DROP USER KStreamRecov22Tester;
DROP NAMESPACE KStreamRecov22Tester CASCADE;
CREATE USER KStreamRecov22Tester IDENTIFIED BY KStreamRecov22Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov22Tester;
CONNECT KStreamRecov22Tester KStreamRecov22Tester;

CREATE APPLICATION KStreamRecovTest22 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION KStreamRecovTest22;