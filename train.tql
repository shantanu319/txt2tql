CREATE or replace TARGET @TARGET_NAME@ USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:10',
StandardSQL:true	
) INPUT FROM @STREAM@;

stop application Postgres_To_PostgresApp;
undeploy application Postgres_To_PostgresApp;
drop application Postgres_To_PostgresApp cascade;
CREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;
create type pkFlag_type
(
TableName String,
PK_UPDATE String,
OperationName String
);
CREATE STREAM Postgres_TypedStream of pkFlag_type;
CREATE OR REPLACE SOURCE Postgres_Src USING PostgreSQLReader  ( 
  ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgres_src'
 ) 
OUTPUT TO Postgres_Change_Data_Stream;
create CQ Cqfilter 
insert into Postgres_TypedStream
select 
META(u,'TableName').toString(),
META(u,'PK_UPDATE').toString(),
META(u,'OperationName').toString()
from Postgres_Change_Data_Stream u;
CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'postgres_PK_Out'
 ) INPUT FROM Postgres_TypedStream;
CREATE  TARGET Postgres_FW USING FileWriter  ( 
  filename: 'Postgres_PKOut.log',
  directory: '/Users/jenniffer/Product2/IntegrationTests/target/test-classes/testNG/PostgreSQLReader/logs'
 ) 
FORMAT USING DSVFormatter  (  ) 
INPUT FROM Postgres_TypedStream;
end application Postgres_To_PostgresApp;
deploy application Postgres_To_PostgresApp;
start Postgres_To_PostgresApp;

STOP APPLICATION App1;
UNDEPLOY APPLICATION App1;
DROP APPLICATION App1 CASCADE;
CREATE APPLICATION App1;
CREATE FLOW AgentFlow;
CREATE OR REPLACE SOURCE App1_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App1_SampleStream;
END FLOW AgentFlow;
CREATE OR REPLACE TARGET App1_NullTarget using NullWriter()
INPUT FROM App1_SampleStream;
END APPLICATION App1;
deploy application App1 with AgentFlow on any in AGENTS;
START APPLICATION App1;

STOP APPLICATION App2;
UNDEPLOY APPLICATION App2;
DROP APPLICATION App2 CASCADE;
CREATE APPLICATION App2;
CREATE FLOW AgentFlow2;
CREATE OR REPLACE SOURCE App2_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App2_SampleStream;
END FLOW AgentFlow2;
CREATE OR REPLACE TARGET App2_NullTarget using NullWriter()
INPUT FROM App2_SampleStream;
END APPLICATION App2;
deploy application App2 with AgentFlow2 on any in AGENTS;
START APPLICATION App2;

STOP APPLICATION App3;
UNDEPLOY APPLICATION App3;
DROP APPLICATION App3 CASCADE;
CREATE APPLICATION App3;
CREATE OR REPLACE SOURCE App3_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App3_SampleStream;
CREATE OR REPLACE TARGET App3_NullTarget using NullWriter()
INPUT FROM App3_SampleStream;
END APPLICATION App3;
DEPLOY APPLICATION App3;
START APPLICATION App3;

STOP APPLICATION App4;
UNDEPLOY APPLICATION App4;
DROP APPLICATION App4 CASCADE;
CREATE APPLICATION App4;
CREATE OR REPLACE SOURCE App4_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App4_SampleStream;
CREATE OR REPLACE TARGET App4_NullTarget using NullWriter()
INPUT FROM App4_SampleStream;
END APPLICATION App4;
DEPLOY APPLICATION App4;
START APPLICATION App4;

STOP APPLICATION App5;
UNDEPLOY APPLICATION App5;
DROP APPLICATION App5 CASCADE;
CREATE APPLICATION App5;
CREATE OR REPLACE SOURCE App5_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App5_SampleStream;
CREATE OR REPLACE TARGET App5_NullTarget using NullWriter()
INPUT FROM App5_SampleStream;
END APPLICATION App5;
DEPLOY APPLICATION App5;
START APPLICATION App5;

STOP APPLICATION App6;
UNDEPLOY APPLICATION App6;
DROP APPLICATION App6 CASCADE;
CREATE APPLICATION App6;
CREATE OR REPLACE SOURCE App6_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App6_SampleStream;
CREATE OR REPLACE TARGET App6_NullTarget using NullWriter()
INPUT FROM App6_SampleStream;
END APPLICATION App6;
DEPLOY APPLICATION App6;
START APPLICATION App6;

STOP APPLICATION App7;
UNDEPLOY APPLICATION App7;
DROP APPLICATION App7 CASCADE;
CREATE APPLICATION App7;
CREATE OR REPLACE SOURCE App7_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App7_SampleStream;
CREATE OR REPLACE TARGET App7_NullTarget using NullWriter()
INPUT FROM App7_SampleStream;
END APPLICATION App7;
DEPLOY APPLICATION App7;
START APPLICATION App7;

STOP APPLICATION App8;
UNDEPLOY APPLICATION App8;
DROP APPLICATION App8 CASCADE;
CREATE APPLICATION App8;
CREATE OR REPLACE SOURCE App8_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App8_SampleStream;
CREATE OR REPLACE TARGET App8_NullTarget using NullWriter()
INPUT FROM App8_SampleStream;
END APPLICATION App8;
DEPLOY APPLICATION App8;
START APPLICATION App8;


STOP APPLICATION App9;
UNDEPLOY APPLICATION App9;
DROP APPLICATION App9 CASCADE;
CREATE APPLICATION App9;
CREATE OR REPLACE SOURCE App9_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App9_SampleStream;
CREATE OR REPLACE TARGET App9_NullTarget using NullWriter()
INPUT FROM App9_SampleStream;
END APPLICATION App9;
DEPLOY APPLICATION App9;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:@EC@'
)
format using DSVFormatter (
)
input from @STREAM@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_DBSource USING Global.OracleReader (
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  Compression: false,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Password_encrypted: 'false',
  SupportPDB: false,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  FetchSize: 1000,
  CDDLAction: 'Process',
  ConnectionURL: '192.168.56.3:1521:orcl',
  DictionaryMode: 'OnlineCatalog',
  QueueSize: 2048,
  CommittedTransactions: true,
  SetConservativeRange: false,
  CDDLCapture: false,
  Username: 'fan',
  Tables: 'FAN.S_BLOB',
  TransactionBufferType: 'Disk',
  Password: '9S5GnbGmBQNDD5c/baD0Tw==',
  TransactionBufferSpilloverSize: '100MB',
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  DatabaseRole: 'Primary' )
OUTPUT TO @APPNAME@_stream;

CREATE OR REPLACE TARGET @APPNAME@_target USING Global.BigQueryWriter (
  ColumnDelimiter: '|',
  projectId: 'striim-support',
  BatchPolicy: 'eventCount:1, Interval:1',
  NullMarker: 'NULL',
  streamingUpload: 'false',
  ServiceAccountKey: '/Users/fzhang/fan/u01/app/product/striim/striim_latest/UploadedFiles/striim-support-286429beb74d.json',
  Encoding: 'UTF-8',
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30',
  AllowQuotedNewLines: 'false',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  Tables: 'FAN.S_BLOB,Fan.s_blob columnmap(A=A,B=B,C=C,D=C,E=@metadata(OperationName));',
  TransportOptions: 'connectionTimeout=300, readTimeout=120',
  adapterName: 'BigQueryWriter',
  Mode: 'APPENDONLY',
  StandardSQL: 'true',
  includeInsertId: 'true',
  QuoteCharacter: '\"' )
INPUT FROM @APPNAME@_stream;


END APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
--drop exceptionstore admin.Oracle12C_To_Oracle12CApp_ExceptionStore;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ use exceptionstore;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@',
    CommitPolicy: 'Interval:5'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;
create or replace cq @cq@
insert into @finalstream@
select exceptionType,action,appName,entityType,entityName,className,message,relatedActivity from @APPNAME@_ExceptionStore;

Create target @targetfile@ using filewriter (
filename:'@APPNAME@_file.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using jsonFormatter()
input from @finalstream@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop @APPNAME@_ExpStore;
undeploy application @APPNAME@_ExpStore;
drop application @APPNAME@_ExpStore cascade;
CREATE APPLICATION @APPNAME@_ExpStore;

CREATE TYPE @APPNAME@_ExpStore_CDCStreams_Type  (
  evtlist java.util.List  
 );

CREATE STREAM @APPNAME@_ExpStore_CDCStreams OF @APPNAME@_ExpStore_CDCStreams_Type;

CREATE CQ @APPNAME@_ReadFromExpStore 
INSERT INTO @APPNAME@_ExpStore_CDCStreams
select to_waevent(s.relatedObjects) as evtlist from admin.@APPNAME@_ExceptionStore [jumping 5 second] s;

CREATE STREAM @APPNAME@_ExpStore_CDCEventStream OF Global.WAEvent;

CREATE CQ @APPNAME@_ExpStore_GetCDCEvent 
INSERT INTO @APPNAME@_ExpStore_CDCEventStream
SELECT com.webaction.proc.events.WAEvent.makecopy(cdcevent) FROM @APPNAME@_ExpStore_CDCStreams a, iterator(a.evtlist) cdcevent;

CREATE CQ @APPNAME@_ExpStore_JoinDataCQ
INSERT INTO @APPNAME@_ExpStore_JoinedDataStream
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1])
        from @APPNAME@_ExpStore_CDCEventStream f;
        
CREATE OR REPLACE TARGET @APPNAME@_ExpStore_WriteToFileAsJSON USING FileWriter  ( 
  filename: 'expEvent_Oracle',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:1,interval:30',
  adapterName: 'FileWriter',
  directory: 'ExpStore_logs',
  rolloverpolicy: 'eventcount:6,interval:30s'
 ) 
FORMAT USING JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter',
  formatterName: 'JSONFormatter',
  jsonMemberDelimiter: '\n',
  jsonobjectdelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true'
 ) 
INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;
        
CREATE TARGET @APPNAME@_ExpStore_dbtarget USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1,Interval:1',
CommitPolicy:'Interval:1,Eventcount:1',
Tables:'@TargetTable@'
) INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;

END APPLICATION @APPNAME@_ExpStore;

deploy application @APPNAME@_ExpStore;
start @APPNAME@_ExpStore;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using AzureblobWriter(
    accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:7'
)
format using DSVFormatter (
)
input from @STREAM@;

end application @APPNAME@;

CREATE OR REPLACE APPLICATION @AppName@;
Create connectionProfile @CP@ TYPE Snowflake(ConnectionURL:'@SFurl@',username:'@SFUsername@',Password:'@SFPassword@',authenticationType:'Password');

Create Source @AppName@_source Using OracleReader(
  Username:'@username@',
  Password:'@password@',
  ConnectionURL:'@url@',
  Tables:'@tableName@',
  Fetchsize:1
)
Output To @AppName@_Stream;
CREATE OR REPLACE TARGET @AppName@_Target USING Global.SnowflakeWriter (
  streamingUpload: 'false',
  useConnectionProfile:'true',
  connectionProfileName: 'admin.@CP@',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  columnDelimiter: '|',
  tables: '@tableName@,SANJAYPRATAP.SAMPLESCHEMA.SAMPLE_PK',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:1,interval:5m',
  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',
  externalStageType: 'Local',
  adapterName: 'SnowflakeWriter',
  fileFormatOptions: 'null_if = \"\"' )
INPUT FROM @AppName@_Stream;
END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src1 USING Global.GCSReader ()
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream1;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer ()
FORMAT USING DSVFormatter (
members:'data')
INPUT FROM @APPNAME@_Stream1;

CREATE OR REPLACE SOURCE @APPNAME@_src2 USING Global.GCSReader ()
PARSE USING Global.JSONParser ()
OUTPUT TO @APPNAME@_Stream2;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_Stream2;

CREATE OR REPLACE SOURCE @APPNAME@_src3 USING GCSReader ()
PARSE USING AvroParser ()
OUTPUT TO @APPNAME@_Stream3;

CREATE CQ @APPNAME@_CQ3
INSERT INTO @APPNAME@_CQOut3
SELECT AvroToJson(data,false) FROM @APPNAME@_Stream3;

CREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_CQOut3;

CREATE OR REPLACE SOURCE @APPNAME@_src4 USING Global.GCSReader ()
PARSE USING Global.XMLParser ()
OUTPUT TO @APPNAME@_Stream4;

CREATE OR REPLACE TARGET @APPNAME@_trgt4 USING S3Writer ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_Stream4;

END APPLICATION @APPNAME@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:100,Interval:3',
  CommitPolicy: 'EventCount:100,Interval:3',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@1;

CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@1;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

Create Source dataGenSrc Using MSSqlReader
(
 Username:'qatest',
 Password:'w3b@ct10n',
 DatabaseName:'qatest',
 ConnectionURL:'localhost:1433',
 Tables:'@tableNames@', 
 ConnectionPoolSize:1,
 FetchSize:1,
 StartPosition:'EOF'
 )
 Output To LCRStream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;

END APPLICATION DataGenSampleApp;

deploy application DataGenSampleApp;
start  application DataGenSampleApp;

CREATE APPLICATION ExportApp2_API;

CREATE OR REPLACE SOURCE ExportApp2_API_FileSource USING FileReader ( 
  wildcard: 'posdata.csv', 
  positionByEOF: false, 
  directory: '@testdata_dir@' ) 
PARSE USING DSVParser ( 
 ) 
OUTPUT TO ExportApp2_API_SampleStream;

CREATE OR REPLACE TARGET ExportApp2_API_NullTarget USING NullWriter ( 
 ) 
INPUT FROM ExportApp2_API_SampleStream;

END APPLICATION ExportApp2_API;

--
-- Recovery Test 10 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> CW(p) -> CQ -> WS
--

STOP Recov10Tester.RecovTest10;
UNDEPLOY APPLICATION Recov10Tester.RecovTest10;
DROP APPLICATION Recov10Tester.RecovTest10 CASCADE;
CREATE APPLICATION RecovTest10 RECOVERY 1 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTest10Data.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  partKey String KEY,
  serialNumber int
);

CREATE STREAM DataStream OF CsvData PARTITION BY partKey;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    TO_INT(data[1])
FROM CsvStream;

CREATE JUMPING WINDOW DataStreamTwoItems
OVER DataStream KEEP 2 ROWS
PARTITION BY partKey;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    first(partKey),
    to_int(first(serialNumber))
FROM DataStreamTwoItems
GROUP BY partKey;

END APPLICATION RecovTest10;

STOP APPLICATION KafkaWPTester.KWApp;
STOP APPLICATION KafkaWPTester.KRApp;
UNDEPLOY APPLICATION KafkaWPTester.KWApp;
UNDEPLOY APPLICATION KafkaWPTester.KRApp;
DROP APPLICATION KafkaWPTester.KWApp CASCADE;
DROP APPLICATION KafkaWPTester.KRApp CASCADE;

CREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;
GRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;
CONNECT KafkaWPTester KafkaWPTester;


CREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE CSVSource USING FileReader (
	directory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',
    WildCard:'access_log_half',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;

CREATE TYPE AccessLogType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;

CREATE CQ AceeslogCQ
INSERT INTO TypedAccessLogStream
SELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]
FROM FileStream;

create Target DsvWriter using KafkaWriter VERSION '0.8.0' ( 
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V8dsvExistTopic',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream;


create Target JsonWriter using KafkaWriter VERSION '0.8.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V8jsonExistTopic',
	Mode:'Sync'
    )
 
FORMAT USING JSONFormatter ()
input from TypedAccessLogStream;


create Target AvroWriter using KafkaWriter VERSION '0.8.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V8avroExistTopic',
    Mode:'Sync'
    )
 
FORMAT USING AvroFormatter (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')
input from TypedAccessLogStream;


END APPLICATION KWApp;

DEPLOY APPLICATION KWApp on any in default;


-- Kafka Reader Apps

CREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;


CREATE TYPE AccessLogType2(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE Stream KafkaDSVReaderStream of AccessLogType2;
CREATE Stream KafkaJsonReaderStream of AccessLogType2;
CREATE Stream KafkaAvroReaderStream of AccessLogType2;


CREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '0.8.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V8dsvExistTopic',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;




CREATE CQ DSVtoKafkaDSVReaderStream
INSERT INTO KafkaDSVReaderStream
SELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]
FROM KafkaReaderStream1;

CREATE TARGET DSVDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaDSVReaderStream;

CREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '0.8.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V8jsonExistTopic',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;




CREATE CQ DSVtoKafkaJsonReaderStream
INSERT INTO KafkaJsonReaderStream
SELECT 
data.get('Col1').textValue(),
data.get('Col2').textValue(),
data.get('Col3').textValue(),
data.get('Col4').textValue(),
data.get('Col5').textValue(),
data.get('Col6').textValue(),
data.get('Col7').textValue(),
data.get('Col8').textValue(),
data.get('Col9').textValue(),
data.get('Col10').textValue()
FROM KafkaReaderStream2;

CREATE TARGET JSONDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8_RT_JSON')
FORMAT USING DSVFormatter()
INPUT FROM KafkaJsonReaderStream;

CREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '0.8.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V8avroExistTopic',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'
)
OUTPUT TO KafkaReaderStream3;



CREATE Stream KRTypedStream of AccessLogType;

CREATE CQ DSVtoKafkaAvroReaderStream
INSERT INTO KafkaAvroReaderStream
SELECT    
data.get("Col1").toString(), 
data.get("Col2").toString(), 
data.get("Col3").toString(), 
data.get("Col4").toString(),
data.get("Col5").toString(), 
data.get("Col6").toString(), 
data.get("Col7").toString(), 
data.get("Col8").toString(),
data.get("Col9").toString(), 
data.get("Col10").toString()
FROM KafkaReaderStream3;

CREATE TARGET AVRODump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8_RT_AVRO')
FORMAT USING DSVFormatter()
INPUT FROM KafkaAvroReaderStream;

END APPLICATION KRApp;
DEPLOY APPLICATION KRApp on any in default;

--
-- Kafka Stream Recovery Test 11
-- Bert Hashemi and Nicholas Keene, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS1
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS2
--

STOP Recov11Tester.KStreamRecovTest11;
UNDEPLOY APPLICATION Recov11Tester.KStreamRecovTest11;
DROP APPLICATION Recov11Tester.KStreamRecovTest11 CASCADE;
DROP USER KStreamRecov11Tester;
DROP NAMESPACE KStreamRecov11Tester CASCADE;
CREATE USER KStreamRecov11Tester IDENTIFIED BY KStreamRecov11Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov11Tester;
CONNECT KStreamRecov11Tester KStreamRecov11Tester;

CREATE APPLICATION KStreamRecovTest11 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions1 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions2 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions1
INSERT INTO Wactions1
SELECT
    *
FROM DataStream5Minutes;

CREATE CQ InsertWactions2
INSERT INTO Wactions2
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION KStreamRecovTest11;

STOP APPLICATION OneAgentMultiSourceCQTester.OneAgentWithMultiSourceCQApp;
UNDEPLOY APPLICATION OneAgentMultiSourceCQTester.OneAgentWithMultiSourceCQApp;
DROP APPLICATION OneAgentMultiSourceCQTester.OneAgentWithMultiSourceCQApp cascade;

create Application OneAgentWithMultiSourceCQApp;
CREATE FLOW AgentFlow;

create source XMLSource using FileReader (
  Directory:'@TEST-DATA-PATH@',
  WildCard:'books.xml',
  positionByEOF:false
)
parse using XMLParser (
  RootNode:'/catalog/book',
  columnlist:'book/@id,book/author,book/title,book/genre,book/price,book/publish_date,book/description'
)
OUTPUT TO XmlStream;

CREATE TYPE MyTypeXml(
id String KEY,
author String,
title String,
genre String,
price String,
publish_date String,
description String
);

CREATE STREAM TypedStreamXml of MyTypeXml;

CREATE CQ TypeConversionCQXml
INSERT INTO TypedStreamXml
SELECT
data[0],
data[1],
data[2],
data[3],
data[4],
data[5],
data[6]
from XmlStream;

create source DSVCSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'customerdetails-agent.csv',
  charset: 'UTF-8',
  positionByEOF:false
)
parse using DSVParser (
  header:'yes',
  minexpectedcolumns:'8'
)
OUTPUT TO DSVCsvStream;

CREATE TYPE MyTypeCsv(
PAN String,
FNAME String KEY,
LNAME String,
ADDRESS String,
CITY String,
STATE String,
ZIP String,
GENDER String
);

CREATE STREAM TypedStreamCsv of MyTypeCsv;

CREATE CQ TypeConversionCQCsv
INSERT INTO TypedStreamCsv
SELECT
data[0],
data[1],
data[2],
data[3],
data[4],
data[5],
data[6],
data[7]
from DSVCsvStream;

-- Read from File

create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'StoreNames.csv',
  columndelimiter:',',
  positionByEOF:false
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CSVStream;

CREATE TYPE MyType (
Store_Id String KEY,
Store_Name String
);

CREATE STREAM TypedStream of MyType;

CREATE CQ TypeConversionCQ
INSERT INTO TypedStream
SELECT data[0], data[1]
from CsvStream;

END FLOW AgentFlow;

CREATE FLOW ServerFlow;

CREATE WACTIONSTORE StoreInfo CONTEXT OF MyType
EVENT TYPES ( MyType )
@PERSIST-TYPE@

CREATE CQ StoreWaction
INSERT INTO StoreInfo
SELECT * FROM TypedStream
LINK SOURCE EVENT;

CREATE WACTIONSTORE StoreInfoXml CONTEXT OF MyTypeXml
EVENT TYPES ( MyTypeXml )
@PERSIST-TYPE@

CREATE CQ StoreWactionXml
INSERT INTO StoreInfoXml
SELECT * FROM TypedStreamXml
LINK SOURCE EVENT;

CREATE WACTIONSTORE StoreInfoCsv CONTEXT OF MyTypeCsv
EVENT TYPES ( MyTypeCsv )
@PERSIST-TYPE@

CREATE CQ StoreWactionCsv
INSERT INTO StoreInfoCsv
SELECT * FROM TypedStreamCsv
LINK SOURCE EVENT;


END FLOW ServerFlow;
end Application OneAgentWithMultiSourceCQApp;

DEPLOY APPLICATION OneAgentWithMultiSourceCQApp with AgentFlow in AGENTS, ServerFlow on any in default;
start OneAgentWithMultiSourceCQApp;

STOP APPLICATION @APPNAME@app1;
STOP APPLICATION @APPNAME@app2;
STOP APPLICATION @APPNAME@app3;
STOP APPLICATION @APPNAME@app4;
STOP APPLICATION @APPNAME@app5;
UNDEPLOY APPLICATION @APPNAME@app1;
UNDEPLOY APPLICATION @APPNAME@app2;
UNDEPLOY APPLICATION @APPNAME@app3;
UNDEPLOY APPLICATION @APPNAME@app4;
UNDEPLOY APPLICATION @APPNAME@app5;
DROP APPLICATION @APPNAME@app1 CASCADE;
DROP APPLICATION @APPNAME@app2 CASCADE;
DROP APPLICATION @APPNAME@app3 CASCADE;
DROP APPLICATION @APPNAME@app4 CASCADE;
DROP APPLICATION @APPNAME@app5 CASCADE;

CREATE APPLICATION @APPNAME@app1 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;

create flow @APPNAME@agentflowps;
CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',
acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');


create type @APPNAME@type1(
  id String,
  name String,
  city string
);

CREATE STREAM @APPNAME@sourcestream OF Global.waevent persist using @APPNAME@KafkaPropset;
CREATE STREAM @APPNAME@kps_typedStream OF @APPNAME@type1 partition by city persist using @APPNAME@KafkaPropset;


CREATE OR REPLACE SOURCE @APPNAME@s USING oracleReader  ( 
  Username:'qatest',
  Password:'qatest',
  ConnectionURL:'localhost:1521/xe',
  Tables:'QATEST.test01',
  FetchSize:1
 ) 
OUTPUT TO @APPNAME@rawstream;

create cq @APPNAME@cq1
INSERT INTO @APPNAME@sourcestream
SELECT * from @APPNAME@rawstream;

CREATE CQ @APPNAME@cq2
INSERT INTO @APPNAME@kps_typedStream
SELECT TO_STRING(data[0]),
TO_STRING(data[1]),
TO_STRING(data[2])FROM @APPNAME@rawstream;
end flow @APPNAME@agentflowps;
end application @APPNAME@app1;
--deploy application app1;
--deploy application app1 with agentflowps on AGENTS;
@DEPLOY@;

CREATE APPLICATION @APPNAME@app2 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE TARGET @APPNAME@app2_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS1'
) INPUT FROM @APPNAME@sourcestream;


end application @APPNAME@app2;
deploy application @APPNAME@app2;


CREATE APPLICATION @APPNAME@app3 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE TARGET @APPNAME@app3_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS2'
) INPUT FROM @APPNAME@sourcestream;

end application @APPNAME@app3;
--deploy application app3;


CREATE APPLICATION @APPNAME@app4 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE TARGET @APPNAME@app4_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS3'
) INPUT FROM @APPNAME@sourcestream;

end application @APPNAME@app4;
--deploy application app4;


CREATE APPLICATION @APPNAME@app5 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE TARGET @APPNAME@app5_target1 USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'snappy1',
KafkaConfig:'compression.type=snappy'
) 
FORMAT USING DSVFormatter ()
INPUT FROM @APPNAME@kps_typedStream;

CREATE TARGET @APPNAME@app5_target2 USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'gzip1',
KafkaConfig:'compression.type=gzip'
) 
FORMAT USING DSVFormatter ()
INPUT FROM @APPNAME@sourcestream;

CREATE TARGET @APPNAME@app5_target3 USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'lz41',
KafkaConfig:'compression.type=lz4'
) 
FORMAT USING DSVFormatter ()
INPUT FROM @APPNAME@sourcestream;

end application @APPNAME@app5;
--deploy application app5;

--This is a dummy file.
--Framework doesn't support to create an empty resource directory.
--Purpose of creating resource dir here is to store test logs in specific test related directory.

STOP APPLICATION orrs;
	UNDEPLOY APPLICATION orrs;
	DROP APPLICATION orrs CASCADE;
	CREATE APPLICATION orrs;
	Create Source OraSource Using OracleReader 
	(
	 Username:'user-name',	
	 Password:'password',
	 ConnectionURL: 'src_url',
	 Tables:'src_table',
	 FilterTransactionBoundaries:true,
	 FetchSize:'fetch-size'
	) Output To LCRStream;
	
	CREATE TARGET RSTarget USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: '@ACCESS-KEY-ID@',
	  --secretaccesskey: '@SECRET-ACCESS-KEY@',
	  S3IAMRole:'@IAMROLE@',
	  Tables: 'tgt_table',
	  uploadpolicy:'eventcount:300,interval:1m'
	) INPUT FROM LCRStream;
	
	END APPLICATION orrs;
	deploy application orrs;
	START application orrs;

stop application ADW2;
undeploy application ADW2;
drop application ADW2 cascade;
CREATE APPLICATION ADW2;

CREATE  SOURCE SqlServerInitialLoad2 USING DatabaseReader  
 (
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'@SOURCE-TABLES@',
 FetchSize:2000
) 
OUTPUT TO InitialLoadStream2;

CREATE TARGET AzureDWInitialLoad2 USING AzureSQLDWHWriter(
ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
)
INPUT FROM InitialLoadStream2;

END APPLICATION ADW2;
deploy application ADW2;
start application ADW2;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE @SOURCE@ USING MSSQLReader  ( 
  FilterTransactionBoundaries: true,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Username: '@SOURCE_USER@',
  DatabaseName: 'qatest',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@ USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;


END APPLICATION @APPNAME@;

deploy application @APPNAME@ on ANY in default;

start application @APPNAME@;

Stop bq;
Undeploy application bq;
alter application bq;
--CREATE CQ cq1
--INSERT INTO OpsStream
--SELECT  
--CASE WHEN (META(a,"OperationName").toString() == "INSERT")
--THEN putUserData(a, 'ops_name', 'I') 
--Else putUserData(a, 'ops_name', 'NOT_INSERT') 
--END
--FROM ss a;
CREATE OR REPLACE TARGET T USING BigQueryWriter  ( 
  serviceAccountKey: '/Users/saranyad/Product/IntegrationTests/TestData/google-gcs.json',
  projectId: 'bigquerywritertest',
  Tables: '@TABLE@',
  datalocation: 'US',
  nullmarker: 'null',
  columnDelimiter: '|',
  Mode: 'Merge',
  _h_throwExceptionOnTableNotFound:'false',
  IgnorableExceptioncode:'TABLE_NOT_FOUND',
  BatchPolicy: 'eventcount:100,interval:10'
 ) INPUT FROM ss;
alter application bq recompile;
deploy application bq;
Start bq;

DROP APPLICATION ns1.OPExample cascade;
DROP NAMESPACE ns1 cascade;
CREATE OR REPLACE NAMESPACE ns1;
USE ns1;
CREATE APPLICATION OPExample;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'PosDataPreview.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
)
OUTPUT TO CsvStream;
 
CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);

CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) 
QUERY (keytomap:'merchantId') 
OF MerchantHourlyAve;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
  TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
  DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
  TO_DOUBLE(data[7]) as amount,
  TO_INT(data[9]) as zip
FROM CsvStream;
 
CREATE CQ cq2
INSERT INTO SendToOPStream
SELECT makeList(dateTime) as dateTime,
  makeList(zip) as zip
FROM PosDataStream;
 
CREATE TYPE ReturnFromOPStream_Type ( time DateTime , val Integer );
CREATE STREAM ReturnFromOPStream OF ReturnFromOPStream_Type;

CREATE TARGET OPExampleTarget 
USING FileWriter (filename: 'OPExampleOut') 
FORMAT USING JSONFormatter() 
INPUT FROM ReturnFromOPStream;

CREATE OPEN PROCESSOR testOp USING Global.TupleConverter ( lastItemSeen: 0, ahead: 1 )
INSERT INTO ReturnFromOPStream FROM SendToOPStream ENRICH WITH HourlyAveLookup;
 
END APPLICATION OPExample;

--
-- Recovery Test 38 with two sources, two jumping time-count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5t3W/p -> CQ1 -> WS
--   S2 -> Jc6t4W/p -> CQ2 -> WS
--

STOP Recov38Tester.RecovTest38;
UNDEPLOY APPLICATION Recov38Tester.RecovTest38;
DROP APPLICATION Recov38Tester.RecovTest38 CASCADE;
CREATE APPLICATION RecovTest38 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Rows3Seconds
OVER DataStream1 KEEP 5 ROWS WITHIN 3 SECOND
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Rows4Seconds
OVER DataStream2 KEEP 6 ROWS WITHIN 4 SECOND
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataStream5Rows3Seconds
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Rows3Seconds p
GROUP BY p.merchantId;

CREATE CQ DataStream6Rows4Seconds
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Rows4Seconds p
GROUP BY p.merchantId;

END APPLICATION RecovTest38;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
 )
INPUT FROM @STREAM@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

 create flow myagentflow;

CREATE OR REPLACE SOURCE @APPNAME@DB_emp1 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO @APPNAME@Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE @APPNAME@DB_emp2 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO @APPNAME@Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE @APPNAME@DB_emp3 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO @APPNAME@Oracle_ChangeDataStream;

end flow myagentflow;

create flow myserverflow;

CREATE OR REPLACE TARGET @APPNAME@DB_etarget USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:100,Interval:10',
  CommitPolicy: 'EventCount:100,Interval:10',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  Tables: '',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM @APPNAME@Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from @APPNAME@Oracle_ChangeDataStream;

end flow myserverflow;

END APPLICATION @APPNAME@;

deploy application @APPNAME@ on ALL in default with myagentflow on all in Agents, myserverflow on all in  default;

start @APPNAME@;

--
-- Recovery Test 7
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov7Tester.RecovTest7;
UNDEPLOY APPLICATION Recov7Tester.RecovTest7;
DROP APPLICATION Recov7Tester.RecovTest7 CASCADE;
CREATE APPLICATION RecovTest7 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM wacStream OF WactionType;

CREATE CQ InsertWactions
INSERT INTO wacStream
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE WINDOW waWindow
OVER wacStream KEEP WITHIN 1 SECOND ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT *
FROM waWindow
LINK SOURCE EVENT;

END APPLICATION RecovTest7;

CREATE APPLICATION @APPNAME@;

CREATE TYPE @APPNAME@type1 (
 companyName java.lang.String,
 merchantId java.lang.String
 );

CREATE STREAM @APPNAME@TypedStream OF @APPNAME@type1;

CREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (
  wildcard: '',
  positionByEOF: false,
  directory: ''
  )
PARSE USING DSVParser (
header:'true'
)
OUTPUT TO @APPNAME@Stream;

CREATE OR REPLACE CQ @APPNAME@CQ
INSERT INTO @APPNAME@TypedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantId
FROM @APPNAME@Stream;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING XMLFormatter(
  rootelement:'JMSXMLIN',
  elementtuple:'companyName:merchantId:text=companyName'
)
INPUT FROM @APPNAME@TypedStream;

END APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
--drop exceptionstore admin.MySQL_To_MySQLApp_ExceptionStore;
drop application @APPNAME@ cascade;
create application @APPNAME@ use exceptionstore;

create source @SourceName@ using MySQLReader
  (ConnectionURL: '@SourceConnectionURL@',
   Username:'@UserName@',
   Password:'@Password@',
   Tables: '@SourceTableName@'
)
output to @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
   ConnectionURL:'@TargetConnectionURL@',
   Username:'@UserName@',
   Password:'@Password@',
   BatchPolicy:'EventCount:1,Interval:0',
   Tables: '@SourceTableName@,@TargetTableName@',
   CommitPolicy: 'Interval:5'
 ) INPUT FROM @SRCINPUTSTREAM@;

create or replace cq @cq@
insert into @finalstream@
select exceptionType,action,appName,entityType,entityName,className,message,relatedActivity from @APPNAME@_ExceptionStore;

Create target @targetfile@ using filewriter (
filename:'@APPNAME@_file.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using jsonFormatter()
input from @finalstream@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop @APPNAME@_ExpStore;
undeploy application @APPNAME@_ExpStore;
drop application @APPNAME@_ExpStore cascade;
CREATE APPLICATION @APPNAME@_ExpStore;

CREATE TYPE @APPNAME@_ExpStore_CDCStreams_Type  (
  evtlist java.util.List  
 );

CREATE STREAM @APPNAME@_ExpStore_CDCStreams OF @APPNAME@_ExpStore_CDCStreams_Type;

CREATE CQ @APPNAME@_ReadFromExpStore 
INSERT INTO @APPNAME@_ExpStore_CDCStreams
select to_waevent(s.relatedObjects) as evtlist from admin.@APPNAME@_ExceptionStore [jumping 5 second] s;

CREATE STREAM @APPNAME@_ExpStore_CDCEventStream OF Global.WAEvent;

CREATE CQ @APPNAME@_ExpStore_GetCDCEvent 
INSERT INTO @APPNAME@_ExpStore_CDCEventStream
SELECT com.webaction.proc.events.WAEvent.makecopy(cdcevent) FROM @APPNAME@_ExpStore_CDCStreams a, iterator(a.evtlist) cdcevent;

CREATE CQ @APPNAME@_ExpStore_JoinDataCQ
INSERT INTO @APPNAME@_ExpStore_JoinedDataStream
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1])
        from @APPNAME@_ExpStore_CDCEventStream f;
        
CREATE OR REPLACE TARGET @APPNAME@_ExpStore_WriteToFileAsJSON USING FileWriter  ( 
  filename: '@APPNAME@_file',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:1,interval:30',
  adapterName: 'FileWriter',
  directory: 'ExpStore_logs',
  rolloverpolicy: 'eventcount:6,interval:30s'
 ) 
FORMAT USING JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter',
  formatterName: 'JSONFormatter',
  jsonMemberDelimiter: '\n',
  jsonobjectdelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true'
 ) 
INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;
        
CREATE TARGET @APPNAME@_ExpStore_dbtarget USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1,Interval:1',
CommitPolicy:'Interval:1,Eventcount:1',
Tables:'@TargetTable@'
) INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;

END APPLICATION @APPNAME@_ExpStore;

deploy application @APPNAME@_ExpStore;
start @APPNAME@_ExpStore;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;


Create Source @SourceName@ Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'jdbc:oracle:thin:@//192.168.124.25:1522/orcl',
 Tables:'qatest.sourcetable',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;


CREATE TARGET @targetsys@ USING Global.SysOut (
  name: 'sysout' )
INPUT FROM @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
  BatchPolicy: 'EventCount:10000,Interval:30',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:10000,Interval:100',
  StatementCacheSize: '50',
  Password: 'w3b@ct10n',
  Username: 'qatest',
  IgnorableExceptionCode: '547,DUPLICATE_ROW_EXISTS,NO_OP_UPDATE,NO_OP_DELETE,NO_OP_PKUPDATE',
  DatabaseProviderType: 'SQLServer',
  PreserveSourceTransactionBoundary: 'false',
  Tables: 'dbo.sourceTable,dbo.targetTable',
  VendorConfiguration: 'enableIdentityInsert=true',
  adapterName: 'DatabaseWriter' )
INPUT FROM @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName1@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM1@;
Create Source @SourceName2@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM2@;
Create Source @SourceName3@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM3@;
Create Source @SourceName4@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM4@;
Create Source @SourceName5@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM5@;
Create Source @SourceName6@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM6@;
Create Source @SourceName7@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM7@;
Create Source @SourceName8@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM8@;
Create Source @SourceName9@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM9@;
Create Source @SourceName10@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM10@;

CREATE TARGET @targetName1@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM1@;
CREATE TARGET @targetName2@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM2@;
CREATE TARGET @targetName3@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM3@;
CREATE TARGET @targetName4@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM4@;
CREATE TARGET @targetName5@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM5@;
CREATE TARGET @targetName6@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM6@;
CREATE TARGET @targetName7@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM7@;
CREATE TARGET @targetName8@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM8@;
CREATE TARGET @targetName9@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM9@;
CREATE TARGET @targetName10@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM10@;


END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop application DualGen;
undeploy application DualGen;
drop application DualGen cascade;
CREATE APPLICATION DualGen;

CREATE OR REPLACE TYPE DualEvent (
    Dummy DateTime,
    PhoneNo java.lang.String
);

CREATE OR REPLACE STREAM DualEvents OF DualEvent;

CREATE OR REPLACE CQ GenDual 
INSERT INTO DualEvents
SELECT
  TO_DATEF('28-FEB-22',"dd-MMM-yy"),maskPhoneNumber('44 844 493 0787', "(\\\\d{0,4}\\\\s)(\\\\d{0,4}\\\\s)([0-9 ]+)", 1, 2)  as Dummy
FROM
   heartbeat(interval @INTERVAL@ second) h;


CREATE OR REPLACE TARGET DualSys USING SysOut  ( 
  name: 'heartbeat_out'
 ) 
INPUT FROM DualEvents;


CREATE TARGET DSVFormatterOut using FileWriter(
 filename:'HeartBeat_Output.log',
 flushpolicy:'EventCount:6',
 rolloverpolicy:'interval:97s')
FORMAT USING DSVFormatter ()
INPUT FROM DualEvents;

END APPLICATION DualGen;
deploy application DualGen;
start application DualGen;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING Global.DeltaLakeWriter (
  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.testaswin',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @STREAM@;

STOP cacheRefresher.cacheApp;
UNDEPLOY APPLICATION cacheRefresher.cacheApp;
DROP APPLICATION cacheRefresher.cacheApp cascade;

CREATE APPLICATION cacheApp;


CREATE TYPE bankData
(
bankID Integer KEY,
bankName String
);

CREATE CACHE cache1 USING CsvReader(
directory: '@TEST-DATA-PATH@',
wildcard: 'banks_cache.csv',
header: No,
columndelimiter: ',',
trimquote: false,
positionbyEOF: false
) QUERY (keytomap:'bankID', refreshinterval:'20 second') OF bankData;



END APPLICATION cacheApp;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src1 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: '@ORACLE-URL@',
  Tables: '@SOURCE-TABLES@',
  Username: '@ORACLE-USERNAME@',
  Password: '@ORACLE-PASSWORD@'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt1 USING SnowflakeWriter

(
  ConnectionURL:'@SNOWFLAKE-URL@',
  username:'@SNOWFLAKE-USERNAME@',
  appendOnly:'false',
  Tables:'@TARGET-TABLES@',
  uploadpolicy:'eventcount:3,interval:10s',
  externalStageType:'local'
)
INPUT FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt2 USING SnowflakeWriter

(
  ConnectionURL:'@SNOWFLAKE-URL@',
  username:'@SNOWFLAKE-USERNAME@',
  appendOnly:'false',
  Tables:'@TARGET-TABLES@',
  uploadpolicy:'eventcount:3,interval:10s',
  externalStageType:'local'
)
INPUT FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt3 USING SnowflakeWriter

(
  ConnectionURL:'@SNOWFLAKE-URL@',
  username:'@SNOWFLAKE-USERNAME@',
  appendOnly:'false',
  Tables:'@TARGET-TABLES@',
  uploadpolicy:'eventcount:3,interval:10s',
  externalStageType:'local'
)
INPUT FROM @APP_NAME@_Stream;


END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 5 second iNTERVAL;
create source @SOURCE@ using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'dynamicdirectory.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO @STREAM@;

Create Type CSVType_@APPNAME@ (
  id String,
  name String,
  department String,
  yoj String,
  moj String,
  doj int
);

Create Stream Typed@STREAM@ of CSVType_@APPNAME@;

CREATE CQ CsvToPosData@APPNAME@
INSERT INTO Typed@STREAM@
SELECT data[0],
       data[1],
       data[2],
       data[3],
       data[4],
       TO_INT(data[5])
FROM @STREAM@;

create Target @TARGET@ using ADLSGen2Writer(
    accountname:'',
	sastoken:'',
	filesystemname:'',
	filename:'',
	directory:'',
	uploadpolicy:'eventcount:5'

)format using DSVFormatter (
)
input from Typed@STREAM@;

end application @APPNAME@;

deploy application @APPNAME@;
start application @APPNAME@;

--
-- Kafka Stream Recovery Test 10 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same key
-- Bert Hashemi and Nicholas Keene WebAction, Inc.
--
-- S1 -> KS -> CQ -> CW(p) -> CQ -> WS
--

STOP KStreamRecov10Tester.KStreamRecovTest10;
UNDEPLOY APPLICATION KStreamRecov10Tester.KStreamRecovTest10;
DROP APPLICATION KStreamRecov10Tester.KStreamRecovTest10 CASCADE;
DROP USER KStreamRecov10Tester;
DROP NAMESPACE KStreamRecov10Tester CASCADE;
CREATE USER KStreamRecov10Tester IDENTIFIED BY KStreamRecov10Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov10Tester;
CONNECT KStreamRecov10Tester KStreamRecov10Tester;

CREATE APPLICATION KStreamRecovTest10 RECOVERY 1 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTest10Data.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  partKey String KEY,
  serialNumber int
);

CREATE STREAM DataStream OF CsvData PARTITION BY partKey;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    TO_INT(data[1])
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStreamTwoItems
OVER DataStream KEEP 2 ROWS
PARTITION BY partKey;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    first(partKey),
    to_int(first(serialNumber))
FROM DataStreamTwoItems
GROUP BY partKey;

END APPLICATION KStreamRecovTest10;

--
-- Recovery Test 11
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS1
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS2
--

STOP Recov11Tester.RecovTest11;
UNDEPLOY APPLICATION Recov11Tester.RecovTest11;
DROP APPLICATION Recov11Tester.RecovTest11 CASCADE;
CREATE APPLICATION RecovTest11 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions1 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions2 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions1
INSERT INTO Wactions1
SELECT
    *
FROM DataStream5Minutes;

CREATE CQ InsertWactions2
INSERT INTO Wactions2
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION RecovTest11;

CREATE TARGET @TARGET_NAME@ USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
) INPUT FROM @STREAM@;

create namespace ns1;
create namespace ns2;

use ns1;

create application group group1;

CREATE or REPLACE  APPLICATION testgroupapp1 RECOVERY 15 SECOND INTERVAL;
end application testgroupapp1;

CREATE or REPLACE  APPLICATION testgroupapp2 RECOVERY 15 SECOND INTERVAL;
end application testgroupapp2;

use ns2;
create application group group2;

CREATE or REPLACE  APPLICATION testgroupapp1 RECOVERY 15 SECOND INTERVAL;
end application testgroupapp1;

CREATE or REPLACE  APPLICATION testgroupapp2 RECOVERY 15 SECOND INTERVAL;
end application testgroupapp2;

alter application group group1 add ns1.testgroupapp1,testgroupapp1;

use ns1;
alter application group group2 add testgroupapp2,ns2.testgroupapp2;

use admin;

create user PosTester identified by PosTester;

drop role PosTester.enduser; drop role PosTester.useradmin;
drop role PosTester.dev; drop user PosTester cascade;
drop namespace PosTester;
create user PosTester identified by PosTester;
drop role PosTester.enduser; drop role PosTester.useradmin;
drop role PosTester.dev; drop user PosTester cascade;
drop namespace PosTester;

STOP APPLICATION TCPReader.TCPAPP;
UNDEPLOY APPLICATION TCPReader.TCPAPP;
DROP APPLICATION TCPReader.TCPAPP cascade;

CREATE APPLICATION TCPAPP;


CREATE SOURCE Tsource USING TCPReader (
@CHARSET@,
IpAddress:'@TCPREADERIPADDR@',
PortNo:'@TCPREADERPORT@'
)
PARSE USING @TSOURCEFORMATTERTYPE@ (
@TSOURCEFORMATTERMEMBERS@
)
OUTPUT TO TCPStream;


END APPLICATION TCPAPP;
deploy application TCPAPP in default;
start application TCPAPP;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@;

--
-- Crash Recovery Test 7 with Jumping window and partitioned on two node cluster with one agent
-- Bert Hashemi, WebAction, Inc.
--
-- S -> KafkaStream -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION KStreamN2S2CR7Tester.KStreamN2S2CRTest7;
UNDEPLOY APPLICATION KStreamN2S2CR7Tester.KStreamN2S2CRTest7;
DROP APPLICATION KStreamN2S2CR7Tester.KStreamN2S2CRTest7 CASCADE;
DROP USER KStreamN2S2CR7Tester;
DROP NAMESPACE KStreamN2S2CR7Tester CASCADE;
CREATE USER KStreamN2S2CR7Tester IDENTIFIED BY KStreamN2S2CR7Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR7Tester;
CONNECT KStreamN2S2CR6Tester KStreamN2S2CR7Tester;

CREATE APPLICATION KStreamN2S2CRTest7 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest7;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream of CsvDataTypeKStreamN2S2CRTest7 using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest7 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;


CREATE TYPE CsvDataTypeKStreamN2S2CRTest7 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE CQ TransferToKafka
INSERT INTO KafkaCsvStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest7;

CREATE FLOW DataProcessingKStreamN2S2CRTest7;

CREATE STREAM DataStream OF CsvDataTypeKStreamN2S2CRTest7 PARTITION BY merchantId;

CREATE CQ CsvToDataKStreamN2S2CRTest7
INSERT INTO DataStream
SELECT
    *
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest7 CONTEXT OF CsvDataTypeKStreamN2S2CRTest7
EVENT TYPES ( CsvDataTypeKStreamN2S2CRTest7 )
@PERSIST-TYPE@

CREATE CQ DataToWactionKStreamN2S2CRTest7
INSERT INTO WactionsKStreamN2S2CRTest7
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingKStreamN2S2CRTest7;

END APPLICATION KStreamN2S2CRTest7;

undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;
Create Source @APPNAME@_s Using DatabaseReader
(
 Username:'@UNAME@',
 Password:'@PASSWORD@',
 ConnectionURL:'@URL@',
 --Query: "SELECT * FROM QATEST.AUTHORIZATIONS",
 Tables:'QATEST.HDFS_IL_%',
 FetchSize:1,
 QuiesceOnILCompletion: true
)
Output To @APPNAME@_ss;


create Target @APPNAME@_t using HDFSWriter(
	hadoopurl:'hdfs://localhost:9000/',
	directory: '%@METADATA(TableName)%',
	filename:'%@METADATA(TableName)%',
    rolloverpolicy: 'filesize:500M',
    flushpolicy:'@FLUSHPOLICY@',
    hadoopConfigurationPath:'@CONF@'
	)
format using DSVFormatter (
)input from @APPNAME@_ss;

--create Target t2 using SysOut(name:Foo2) input from DataStream;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;

drop propertyset MysqlToMysqlPlatfm_App_KafkaPropset;
drop stream  MysqlToMysqlPlatfm_Stream CASCADE;


--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;
					
CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using MySQLReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNEC4TION_URL@',
  Tables: '$table1',
  adapterName: 'MySQLReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using MySQLReader( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@2',
  adapterName: 'MySQLReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'WACTION.MYSQLTOMYSQLPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using MySQLReader( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '$table2',
  adapterName: 'MySQLReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using MySQLReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'WACTION.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@4',
  adapterName: 'MySQLReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'WACTION.MYSQLTOMYSQLPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;

END APPLICATION @APPNAME@2;

create application sorted;

create source AALSortedSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'sorted_log',
  charset:'UTF-8',
  positionByEOF:false
) PARSE USING AALParser (
  columndelimiter:' ',
  IgnoreEmptyColumn:'Yes',
  columndelimittill:5
) OUTPUT TO AalSortedStream;

create Target AALSortedDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/sorted_log') input from AalSortedStream;

end application sorted;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[2]) = 'Null' ;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

STOP bq;
UNDEPLOY APPLICATION bq;
DROP APPLICATION bq CASCADE;

CREATE APPLICATION bq RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE DEV_22964_source USING MSSqlReader
(
	Username: 'qatest',
	Password: 'w3b@ct10n',
	ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
	Tables: 'QATEST.BitToBoolean',
	FetchTransactionMetadata: true, 
	FetchSize: '1'
)
OUTPUT TO SS;


CREATE or replace TARGET DEV_22964_target USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
    Tables:'QATEST.TABLE_TEST_1000001,qatest.% keycolumns(RONUM)',
    mode:'Appendonly',
    datalocation: 'US',
	nullmarker: 'NULL',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:100,Interval:10'	
) INPUT FROM ss;

END APPLICATION bq;
DEPLOY APPLICATION bq;
START APPLICATION bq;

--
-- Canon Test W60
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for a partitioned sliding attribute window
--
-- S -> SWa5p -> CQ -> WS
--


UNDEPLOY APPLICATION NameW60.W60;
DROP APPLICATION NameW60.W60 CASCADE;
CREATE APPLICATION W60 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW60;

CREATE SOURCE CsvSourceW60 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW60;

END FLOW DataAcquisitionW60;




CREATE FLOW DataProcessingW60;

CREATE TYPE DataTypeW60 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW60 OF DataTypeW60;

CREATE CQ CSVStreamW60_to_DataStreamW60
INSERT INTO DataStreamW60
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW60;

CREATE WINDOW SWa5pW60
OVER DataStreamW60
KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY word;

CREATE WACTIONSTORE WactionStoreW60 CONTEXT OF DataTypeW60
EVENT TYPES ( DataTypeW60 KEY(word) )
@PERSIST-TYPE@

CREATE CQ SWa5pW60_to_WactionStoreW60
INSERT INTO WactionStoreW60
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM SWa5pW60
GROUP BY word;

END FLOW DataProcessingW60;



END APPLICATION W60;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;;

Create Source MSSqlsource Using MSSqlReader
(
 Username:'@SQL-USERNAME',
 Password:'@SQL-PASSWORD',
 DatabaseName:'@DATABASE-NAME@',
 ConnectionURL: '@SQLSERVER-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'1'
) 
Output To str;

create target MssqlAzureTarget using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

CREATE  SOURCE @QuerySource@1 USING DatabaseReader  (
  ReplicationSlotName:'@slotname1@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@1;
create Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;
 CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@1;

CREATE  SOURCE @QuerySource@2 USING DatabaseReader  (
  ReplicationSlotName:'@slotname2@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@2;
create Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;
 CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@2;

CREATE  SOURCE @QuerySource@3 USING DatabaseReader  (
  ReplicationSlotName:'@slotname3@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@3;
create Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;
 CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@3;

CREATE  SOURCE @QuerySource@4 USING DatabaseReader  (
  ReplicationSlotName:'@slotname4@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@4;
create Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;
 CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@4;

CREATE  SOURCE @QuerySource@5 USING DatabaseReader  (
  ReplicationSlotName:'@slotname5@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@5;
create Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;
 CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@5;

CREATE  SOURCE @QuerySource@6 USING DatabaseReader  (
  ReplicationSlotName:'@slotname6@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@6;
create Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;
 CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@6;

CREATE  SOURCE @QuerySource@7 USING DatabaseReader  (
  ReplicationSlotName:'@slotname7@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@7;
create Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;
 CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@7;

CREATE  SOURCE @QuerySource@8 USING DatabaseReader  (
  ReplicationSlotName:'@slotname8@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@8;
create Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;
 CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@8;

CREATE  SOURCE @QuerySource@9 USING DatabaseReader  (
  ReplicationSlotName:'@slotname9@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@9;
create Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;
 CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@9;

CREATE  SOURCE @QuerySource@10 USING DatabaseReader  (
  ReplicationSlotName:'@slotname10@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@10;
create Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;
 CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@10;
end application @APPNAME@;

use PosTester;
DROP CACHE HourlyAveLookup;

CREATE TARGET @TARGET_NAME@ using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkuPDaTehAnDliNgmOdE:'DELETEANDINSERT',
tables: 'QATEST.ORCLALLDATATYPES,@TARGET_TABLE@ ColumnMap(Log_Id=LOG_ID,Description=Description,LogChar=LogChar,LogVarchar2=LogVarchar2,LogNumber=LogNumber,LogBinaryFloat=LogBinaryFloat,LogBinaryDouble=LogBinaryDouble,LogFloat=LogFloat,LogDate=LogDate,LogTimestamp=LogTimestamp,LogNchar=LogNchar,LogNvarchar2=LogNvarchar2,LogTimezone=LogTimezone,LogTimelocal=LogTimelocal,LogInterval=LogInterval,LogInterval2=LogInterval2,LogInteger=LogInteger)',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM @STREAM@;

stop application GGTrailReaderApp;
undeploy application GGTrailReaderApp;
drop application GGTrailReaderApp cascade;

create application GGTrailReaderApp recovery 5 second interval;

create source GGTrailSource using GGTrailReader (
tRaildIrectory:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario15',
tRAilfilepattern:'15*',
positionByEOF:false,
FilterTransactionBoundaries: true,
DefinitionFile:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario15/Scn15_beforeddl.def',
captureCDdl: true,
CDDLAction:'Process',
--CDDLAction:'ignore',
--CDDLAction:'quiesce',
--cddlAction:'Error',
TrailByTeOrder:'LittleEndian',
recoveryInterval: 5,
TABLES:'QATEST.INT2;QATEST.INT1'
)
OUTPUT TO GGTrailStream;

create Target t2 using SysOut(name:Foo2) input from GGTrailStream;

CREATE TARGET WriteCDCOracle1 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost/ORCL',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1,Interval:1',
CommitPolicy:'Eventcount:1,Interval:1',
Checkpointtable:'RGRN_CHKPOINT',
Tables:'QATEST.GGDDL5,QATEST.GGDDL5_TGT'
) INPUT FROM GGTrailStream;


end application GGTrailReaderApp;

deploy application GGTrailReaderApp;
start application GGTrailReaderApp;

CREATE OR REPLACE PROPERTYVARIABLE RetryPolicy='timeOut=00,retryInterval=1,maxRetries=3';
CREATE OR REPLACE PROPERTYVARIABLE KafkaBrokerAddress='localhost:9099';
CREATE OR REPLACE PROPERTYVARIABLE KafkaConfig='request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;';

STOP @WRITERAPPNAME@@RECOVSTATUS@;
UNDEPLOY APPLICATION @WRITERAPPNAME@@RECOVSTATUS@;
DROP APPLICATION @WRITERAPPNAME@@RECOVSTATUS@ CASCADE;

CREATE APPLICATION @WRITERAPPNAME@@RECOVSTATUS@ @Recovery@;
create flow AgentFlow;
CREATE SOURCE S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.oracle_kw_test%',
	FetchSize: '1',
	connectionRetryPolicy:'$RetryPolicy'
)
OUTPUT TO SS;
end flow AgentFlow;
create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;
create stream out_cq_select_SS_1 of global.waevent;
create stream out_cq_select_SS_2 of global.waevent;
create stream out_cq_select_SS_3 of global.waevent;
create stream out_cq_select_SS_4 of global.waevent;
create stream out_cq_select_SS_5 of global.waevent;
create stream out_cq_select_SS_6 of global.waevent;

CREATE OR REPLACE CQ cq_select_SS1 
INSERT INTO out_cq_select_SS_1
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST1';

CREATE OR REPLACE CQ cq_select_SS2 
INSERT INTO out_cq_select_SS_2
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST2';

CREATE OR REPLACE CQ cq_select_SS3 
INSERT INTO out_cq_select_SS_3
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST3';

CREATE OR REPLACE CQ cq_select_SS4 
INSERT INTO out_cq_select_SS_4
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST4';

CREATE OR REPLACE CQ cq_select_SS5 
INSERT INTO out_cq_select_SS_5
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST5';

CREATE OR REPLACE CQ cq_select_SS6 
INSERT INTO out_cq_select_SS_6
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST6';

create Target TARGET1 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_sync_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: '$KafkaConfig')
FORMAT USING dsvFormatter ()
input from out_cq_select_SS_1;

create Target TARGET2 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_sync_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: '$KafkaConfig')
FORMAT USING jsonFormatter ()
input from out_cq_select_SS_2;

create Target TARGET3 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_sync_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: '$KafkaConfig')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_sync_CQ.avsc')
input from out_cq_select_SS_3;

create Target TARGET4 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_Async_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(ROWID)',
Mode:'Async',
KafkaConfig: '$KafkaConfig')
FORMAT USING dsvFormatter ()
input from out_cq_select_SS_4;

create Target TARGET5 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_Async_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(ROWID)',
Mode:'Async',
KafkaConfig: '$KafkaConfig')
FORMAT USING jsonFormatter ()
input from out_cq_select_SS_5;

create Target TARGET6 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_Async_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(ROWID)',
Mode:'Async',
KafkaConfig: '$KafkaConfig')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_Async_CQ.avsc')
input from out_cq_select_SS_6;

create Target TARGET7 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_sync',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: '$KafkaConfig')
FORMAT USING dsvFormatter ()
input from ss;

create Target TARGET8 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_sync',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: '$KafkaConfig')
FORMAT USING jsonFormatter ()
input from ss;

create Target TARGET9 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_sync',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: '$KafkaConfig')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_sync.avsc')
input from ss;

create Target TARGET10 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_Async',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'Async',
KafkaConfig: '$KafkaConfig')
FORMAT USING dsvFormatter ()
input from ss;

create Target TARGET11 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_Async',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'Async',
KafkaConfig: '$KafkaConfig')
FORMAT USING jsonFormatter ()
input from ss;

create Target TARGET12 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_Async',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'Async',
KafkaConfig: '$KafkaConfig')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_Async.avsc')
input from ss;

end flow serverFlow;
end application @WRITERAPPNAME@@RECOVSTATUS@;
deploy application @WRITERAPPNAME@@RECOVSTATUS@;
start @WRITERAPPNAME@@RECOVSTATUS@;



stop application @READERAPPNAME@@RECOVSTATUS@;
undeploy application @READERAPPNAME@@RECOVSTATUS@;
drop application @READERAPPNAME@@RECOVSTATUS@ cascade;
CREATE APPLICATION @READERAPPNAME@@RECOVSTATUS@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @SOURCE@_DSV_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_sync_CQ',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;

CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
rolloverpolicy:'filesize:500M',
filename:'@READERAPPNAME@@RECOVSTATUS@_@SOURCE@_dsv_sync_CQ')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream1;

CREATE SOURCE @SOURCE@_JSON_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_sync_CQ',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;

CREATE SOURCE @SOURCE@_AVRO_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_sync_CQ',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_sync_CQ.avsc'
)
OUTPUT TO KafkaReaderStream3;

CREATE SOURCE @SOURCE@_DSV_Async_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_Async_CQ',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream4;

CREATE SOURCE @SOURCE@_JSON_Async_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_Async_CQ',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream5;

CREATE SOURCE @SOURCE@_AVRO_Async_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_Async_CQ',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_Async_CQ.avsc'
)
OUTPUT TO KafkaReaderStream6;

CREATE SOURCE @SOURCE@_DSV_sync USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_sync',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream7;

CREATE TARGET kafkaDumpDSV_rawstream USING FileWriter(
name:kafkaOuputDSV_rawstream,
rolloverpolicy:'filesize:500M',
filename:'@READERAPPNAME@@RECOVSTATUS@_@SOURCE@_dsv_sync')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream7;

CREATE SOURCE @SOURCE@_JSON_sync USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_sync',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream8;

CREATE SOURCE @SOURCE@_AVRO_sync USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_sync',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_sync.avsc'
)
OUTPUT TO KafkaReaderStream9;

CREATE SOURCE @SOURCE@_DSV_Async USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_Async',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream10;

CREATE SOURCE @SOURCE@_JSON_Async USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_Async',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream11;

CREATE SOURCE @SOURCE@_AVRO_Async USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_Async',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_Async.avsc'
)
OUTPUT TO KafkaReaderStream12;

end application @READERAPPNAME@@RECOVSTATUS@;
deploy application @READERAPPNAME@@RECOVSTATUS@;

STOP APPLICATION SystemTimeTester.SystemTimeWindows;
UNDEPLOY APPLICATION SystemTimeTester.SystemTimeWindows;
DROP APPLICATION SystemTimeTester.SystemTimeWindows cascade;


CREATE APPLICATION SystemTimeWindows;

CREATE TYPE RandomData(
bankNumber int KEY,
bankName String
);

CREATE  SOURCE ranDataSource USING StreamReader (
  OutputType: 'SystemTimeTester.RandomData',
  noLimit: 'false',
  isSeeded: 'true',
  maxRows: 0,
  iterations: 30,
  iterationDelay: 1000,
  StringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',
  NumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]R'
 )
OUTPUT TO CSVDataStream;

CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT data[0], data[1]
FROM CSVDataStream;

CREATE @WINDOWTYPE@ WINDOW tierone OVER RandomDataStream keep within 20 second;

CREATE STREAM onetwostream OF RandomData;

CREATE CQ onetwocq
INSERT INTO onetwostream
SELECT bankNumber,bankName
FROM tierone
where  bankNumber >= 300
order by bankName;

CREATE WACTIONSTORE MyDataActivity  CONTEXT OF RandomData
EVENT TYPES ( RandomData  )
@PERSIST-TYPE@

Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
SELECT bankNumber,bankName from @FROMSTREAM@
where  bankNumber >= 300
order by bankName
LINK SOURCE EVENT;

END APPLICATION SystemTimeWindows;
deploy application SystemTimeWindows;
start application SystemTimeWindows;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 10 second interval;
create source CS using FileReader (
	directory:'@DIR@',
	WildCard:'@WILDCARD@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target T using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:50'
)
format using DSVFormatter (
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

STOP ModifyBeforeDataTester.ModifyBeforeData;
UNDEPLOY APPLICATION ModifyBeforeDataTester.ModifyBeforeData;
DROP APPLICATION ModifyBeforeDataTester.ModifyBeforeData CASCADE;

CREATE APPLICATION ModifyBeforeData;

create source GGTrailSource using FileReader (
    directory:'@TEST-DATA-PATH@/OGG/oracle_alltypes_122',
    WildCard:'ld*',
    positionByEOF:false
) parse using GGTrailParser (
    FilterTransactionBoundaries: true,
    metadata:'@TEST-DATA-PATH@/OGG/oracle_alltypes_122/def_oracle_alltypes122.def',
    compression:false
)
OUTPUT TO SourceStream;

CREATE STREAM ModifiedBeforeStream OF Global.WAEvent;
CREATE STREAM ModifiedDataStream OF Global.WAEvent;

CREATE OR REPLACE CQ ModifierBeforeCQ
INSERT INTO ModifiedBeforeStream
SELECT * FROM SourceStream
where not(before is null)
modify(before[1] = data[1].toString().replaceAll(".", "B"));

CREATE OR REPLACE CQ ModifierDataCQ
INSERT INTO ModifiedDataStream
SELECT * FROM SourceStream
where not(before is null)
modify(data[1] = before[1].toString().replaceAll(".", "D"));

-- Generate the final filtered stream to write to the waction store.
CREATE OR REPLACE CQ CopyBeforeCQ
INSERT INTO FilteredBeforeStream
SELECT *
FROM (SELECT TO_STRING(before[1]) as tcolBefore
      FROM ModifiedBeforeStream) as src
where src.tcolBefore like 'BBBBBB%';

CREATE OR REPLACE CQ CopyDataCQ
INSERT INTO FilteredDataStream
SELECT *
FROM (SELECT TO_STRING(data[1]) as tcolData
      FROM ModifiedDataStream) as src
where src.tcolData like "DDDDDD%";

-- Need duplicate types due to a limitation of Waction Stores: two Waction Stores
-- should not share the same type for CONTEXT OF.
CREATE TYPE WactionType1 (
  colTest String KEY
);

CREATE TYPE WactionType2 (
  colTest String KEY
);

CREATE WACTIONSTORE WactionsBefore CONTEXT OF WactionType1
EVENT TYPES ( WactionType1 )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsData CONTEXT OF WactionType2
EVENT TYPES ( WactionType2 )
@PERSIST-TYPE@

CREATE CQ InsertWactions1
INSERT INTO WactionsBefore
SELECT tcolBefore as colTest
FROM FilteredBeforeStream;

CREATE CQ InsertWactions2
INSERT INTO WactionsData
SELECT tcolData as colTest
FROM FilteredDataStream;

END APPLICATION ModifyBeforeData;

STOP APPLICATION OneAgentCQTester.CSV;
UNDEPLOY APPLICATION OneAgentCQTester.CSV;
DROP APPLICATION OneAgentCQTester.CSV cascade;

create application CSV;

CREATE FLOW AgentFlow;

create source CSVSource using CSVReader
(
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'StoreNames.csv',
  columndelimiter:',',
  positionByEOF:false
)OUTPUT TO CsvStream;

CREATE TYPE MyType (
Store_Id String KEY,
Store_Name String
);

CREATE STREAM TypedStream of MyType;

CREATE CQ TypeConversionCQ
INSERT INTO TypedStream
SELECT data[0], data[1]
from CsvStream;

END FLOW AgentFlow;

CREATE FLOW ServerFlow;

CREATE WACTIONSTORE StoreInfo CONTEXT OF MyType
EVENT TYPES ( MyType )
@PERSIST-TYPE@

CREATE CQ StoreWaction
INSERT INTO StoreInfo
SELECT * FROM TypedStream
LINK SOURCE EVENT;

END FLOW ServerFlow;

end application CSV;
DEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;
START CSV;

-- The PosApp sample application demonstrates how a credit card
-- payment processor might use Striim to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.

stop test.PosApp;
undeploy application test.PosApp;
drop application test.PosApp cascade;

CREATE APPLICATION PosApp;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING FileReader (
  directory:'Samples/Customer/PosApp/appData',
  wildcard : '$admin.wildcard',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells Striim that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells Striim to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvToPosData

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP.
--
-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as
-- the types to be used in PosDataStream, which is created automatically.

-- The DATETIME field from the source is converted to both a dateTime
-- value, used as the event timestamp by the application, and (via the
-- function) an integer hourValue, which is used to look up
-- historical hourly averages from the HourlyAveLookup cache,
-- discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)
-- 17 (hourValue)
-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateOnly
--
-- The PosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: 'Samples/Customer/PosApp/appData',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startingTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;


-- WAction store MerchantActivity
--
-- The following group of statements create and populate the MerchantActivity
-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced
-- with merchant details from NameLookup cache and with latitude and longitude
-- values from the USAddressData cache.

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );


CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'Samples/Customer/PosApp/appData',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false
)
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: 'Samples/Customer/PosApp/appData',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;


-- CQ GenerateAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;



--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM AlertStream;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;


END APPLICATION PosApp;


CREATE DASHBOARD USING "Samples/Customer/PosApp/PosAppDashboard.json";

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

CREATE SOURCE @SourceName@ USING MySqlReader  ( 
TransactionSupport: false, 
  FetchTransactionMetadata: false, 
  Compression: false, 
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', 
  Password_encrypted: 'false', 
  ConnectionURL: '@ConnectionURL@', 
  Fetchsize: 0, 
  ConnectionPoolSize: 10, 
  Username: '@UN@', 
  cdcRoleName: 'STRIIM_READER', 
  Password: '@PWD@', 
  Tables: 'qatest.%', 
  FilterTransactionBoundaries: true, 
  SendBeforeImage: true, 
  AutoDisableTableCDC: false ) 
OUTPUT TO @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;


CREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'), 'OpTime',META(x, 'TimeStamp'))) FROM @SRCINPUTSTREAM@ x; ;

CREATE TARGET @targetName@ USING DatabaseWriter ( 
ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  ParallelThreads: '', 
  CheckPointTable: 'CHKPOINT', 
  Password_encrypted: 'false', 
  ConnectionURL: '@ConnectionURL@', 
  CommitPolicy: 'EventCount:1,Interval:60', 
  StatementCacheSize: '50', 
  DatabaseProviderType: 'Default', 
  Username: '@UN@', 
  Password: '@PWD@', 
  PreserveSourceTransactionBoundary: 'false', 
  BatchPolicy: 'EventCount:1,Interval:60', 
  Tables: 'qatest.%, dbo.% columnmap(opt_type=@USERDATA(OpType),opt_time=@USERDATA(OpTime));' ) 
INPUT FROM admin.sqlreader_cq_out;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using AzureblobWriter(
    accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:7'
)
format using DSVFormatter (
)
input from @STREAM@;

end flow @APPNAME@_serverflow;

end application @APPNAME@;

CONNECT ADMIN abc;

CREATE OR REPLACE TARGET @TARGET_NAME@ using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
		BatchPolicy: 'EventCount:1',
  		CommitPolicy: 'EventCount:1',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@2 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
  		BatchPolicy: 'Interval:10',
  		CommitPolicy: 'Interval:10',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@3 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
		BatchPolicy: 'eventCount:100000,Interval:20',
		CommitPolicy: 'eventCount:100000,Interval:20',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@4 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
  		BatchPolicy: 'EventCount:1',
		CommitPolicy: 'EventCount:1',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
) INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;;

create Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;

STOP APPLICATION @Appname@;
UNDEPLOY APPLICATION @Appname@;
DROP APPLICATION @Appname@ CASCADE;
unload OPEN PROCESSOR "OP_SCM_PATH";

CREATE APPLICATION @Appname@ RECOVERY 10 SECOND INTERVAL;

Create flow AgentFlow;

CREATE  SOURCE @Appname@s USING databaseReader  (
Username: 'qatest',
Password: 'qatest',
ConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',
Tables: 'QATEST.TGT_T%')
OUTPUT TO @Appname@Stream1;

CREATE SOURCE @Appname@sd USING databaseReader  (
Username: 'qatest',
Password: 'qatest',
ConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',
Tables: 'QATEST.TGT_T%')
OUTPUT TO @Appname@Stream2;

LOAD OPEN PROCESSOR "OP_SCM_PATH";

CREATE STREAM  @Appname@outStream OF Global.WAEvent;

CREATE OPEN PROCESSOR @Appname@op USING TUPLECONVERTER (
   magicprop:"ASIA"
)
INSERT INTO @Appname@outStream
FROM @Appname@Stream1;

CREATE OR REPLACE ROUTER @Appname@router1 INPUT FROM @Appname@outStream s CASE
WHEN meta(s,"TableName").toString()='QATEST.OPA_INPUTR1' THEN ROUTE TO @Appname@ss1,
WHEN meta(s,"TableName").toString()='QATEST.OPA_INPUTR2' THEN ROUTE TO @Appname@ss2,
ELSE ROUTE TO @Appname@ss_else;


CREATE OR REPLACE ROUTER @Appname@router2 INPUT FROM @Appname@Stream2 s CASE
WHEN meta(s,"TableName").toString()='QATEST.OPA_OUTPUTR1' THEN ROUTE TO @Appname@ss1,
WHEN meta(s,"TableName").toString()='QATEST.OPA_OUTPUTR2' THEN ROUTE TO @Appname@ss2,
ELSE ROUTE TO @Appname@ss_else;

end flow AgentFlow;

create flow nflow;

create Target @Appname@Target_1 using FileWriter
(
directory: 'testApr22',
filename:'%@metadata(TableName)%',
flushpolicy:'eventCount:1000,Interval:90s',
RollOverPolicy:'eventCount:1000,Interval:90s'
)
FORMAT USING dsvFormatter ()
input from @Appname@ss1;

create Target @Appname@Target_2 using FileWriter
(
directory: 'testApr22',
filename:'%@metadata(TableName)%',
flushpolicy:'eventCount:1000,Interval:90s',
RollOverPolicy:'eventCount:1000,Interval:90s'
)
FORMAT USING dsvFormatter ()
input from @Appname@ss2;

end flow nflow;

end application @Appname@;
deploy application @Appname@ with AgentFlow in agents, nflow in default;
start @Appname@;

stop application ManyToManyADLSGen1;
undeploy application ManyToManyADLSGen1;
drop application ManyToManyADLSGen1 cascade;

create application ManyToManyADLSGen1 Recovery 5 second interval;


create type ADLSGen1csv_type(
id String,
name String,
seq String
);

create type ADLSGen1Order_type(
id String,
Name String,
Company String
);



create source ADLSGen1CSVSource_multi using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'Canon1000_All.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO ADLSGen1CsvStream_user;

create source ADLSGen1CSVSource2 using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'portfolio.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO ADLSGen1CsvStream;

create source ADLSGen1CSVSource3 using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'dynamicdirectory.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO ADLSGen1CsvStream;


CREATE SOURCE ADLSGen1OraSource1 USING OracleReader
(
 Username:'miner',
 Password:'miner',
 ConnectionURL:'localhost:1521:xe',
 Tables:'QATEST.CUSTOMER1,QATEST.CUSTOMER2,QATEST.CUSTOMER3',
 OnlineCatalog:true,
 FetchSize:10000,
 QueueSize:2148,
 CommittedTransactions:false,
 Compression:true
)
OUTPUT TO ADLSGen1OrdersStream;

CREATE SOURCE ADLSGen1OraSource2 USING OracleReader
(
 Username:'miner',
 Password:'miner',
 ConnectionURL:'localhost:1521:xe',
 Tables:'QATEST.CUSTOMER4,QATEST.CUSTOMER5',
 OnlineCatalog:true,
 FetchSize:10000,
 QueueSize:2148,
 CommittedTransactions:false,
 Compression:true
)
OUTPUT TO ADLSGen1OrdersStream;


create Target ADLSGen1_tgt1 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'%@metadata(FileName)%',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
		rolloverpolicy:'interval:30s'
)
FORMAT USING JSONFormatter()
input from ADLSGen1CsvStream; 

create Target ADLSGen1_tgt2 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'%@metadata(TableName)%',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
		rolloverpolicy:'filesize:10M',
		compressiontype: 'true'
)
format using DSVFormatter (
)
input from ADLSGen1OrdersStream; 

create stream ADLSGen1UserdataStream of Global.WAEvent;

Create CQ ADLSGen1CQUser
insert into ADLSGen1UserdataStream
select 
putuserdata (data1,'Fileowner',data[1]) from ADLSGen1CsvStream_user data1;

create stream ADLSGen1CSVTypedStream1 of csv_type;
create stream ADLSGen1CSVTypedStream2 of csv_type;
create stream ADLSGen1CSVTypedStream3 of csv_type;

CREATE CQ ADLSGen1cq1
INSERT INTO ADLSGen1CSVTypedStream1
SELECT data[0],
data[1],
data[2]
FROM ADLSGen1UserdataStream
WHERE USERDATA(ADLSGen1UserdataStream,'Fileowner').toString() == 'Lorem';

CREATE CQ ADLSGen1cq2
INSERT INTO ADLSGen1CSVTypedStream2
SELECT data[0],
data[1],
data[2]
FROM ADLSGen1UserdataStream
WHERE USERDATA(ADLSGen1UserdataStream,'Fileowner').toString() == 'doloremque';

CREATE CQ ADLSGen1cq3
INSERT INTO ADLSGen1CSVTypedStream3
SELECT data[0],
data[1],
data[2]
FROM ADLSGen1UserdataStream
WHERE USERDATA(ADLSGen1UserdataStream,'Fileowner').toString() == 'accusantium';

create Target ADLSGen1_tgt3 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'1_%name%',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
	rolloverpolicy:'eventcount:74'
)
format using XMLFormatter (
  elementtuple: 'Eventname:name:id:seq:text=name',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from ADLSGen1CSVTypedStream1; 

create Target ADLSGen1_tgt4 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'2_%name%',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
    rolloverpolicy:'eventcount:4,interval:50s'
)
format using XMLFormatter (
  elementtuple: 'Eventname:name:id:seq:text=name',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from ADLSGen1CSVTypedStream2; 

create Target ADLSGen1_tgt5 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'3_%name%',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
    rolloverpolicy:'eventcount:12'
)
format using XMLFormatter (
  elementtuple: 'Eventname:name:id:seq:text=name',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from ADLSGen1CSVTypedStream3; 


create stream ADLSGen1OrderTypedStream1 of Order_type;
create stream ADLSGen1OrderTypedStream2 of Order_type;
create stream ADLSGen1OrderTypedStream3 of Order_type;

CREATE CQ ADLSGen1cq1_db
INSERT INTO ADLSGen1OrderTypedStream1
SELECT data[0],
data[1],
data[2]
FROM ADLSGen1OrdersStream
WHERE META(ADLSGen1OrdersStream,'TableName').toString() == 'QATEST.CUSTOMER1';

CREATE CQ ADLSGen1cq2_db
INSERT INTO ADLSGen1OrderTypedStream2
SELECT data[0],
data[1],
data[2]
FROM ADLSGen1OrdersStream
WHERE META(ADLSGen1OrdersStream,'TableName').toString() == 'QATEST.CUSTOMER2';

CREATE CQ ADLSGen1cq3_db
INSERT INTO ADLSGen1OrderTypedStream3
SELECT data[0],
data[1],
data[2]
FROM ADLSGen1OrdersStream
WHERE META(ADLSGen1OrdersStream,'TableName').toString() == 'QATEST.CUSTOMER3';


create Target ADLSGen1_tgt6 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'Customer1',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
	rolloverpolicy:'eventcount:10000'
)
format using AvroFormatter(
  formatAs: 'Default',
  handler: 'com.webaction.proc.AvroFormatter',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA@'
)
input from ADLSGen1OrderTypedStream1; 

create Target ADLSGen1_tgt7 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'Customer2',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
    rolloverpolicy:'eventcount:10000'
)
format using AvroFormatter(
  formatAs: 'Default',
  handler: 'com.webaction.proc.AvroFormatter',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA@'
)input from ADLSGen1OrderTypedStream2; 

create Target ADLSGen1_tgt8 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'Customer3',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
    rolloverpolicy:'eventcount:10000'
)
format using AvroFormatter(
  formatAs: 'Default',
  handler: 'com.webaction.proc.AvroFormatter',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA@'
)
input from ADLSGen1OrderTypedStream3; 

end application ManyToManyADLSGen1;

deploy application ManyToManyADLSGen1;
start application ManyToManyADLSGen1;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;


CREATE OR REPLACE SOURCE @APPNAME@CDC_Source1 USING OracleReader( 
  StartTimestamp: 'null',
  SupportPDB: false,
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  FetchSize: 1,
  DDLCaptureMode: 'All',
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.EMP',
  adapterName: 'OracleReader',
  Password: 'qatest',
  TransactionBufferType: 'Memory',
  DictionaryMode: 'OnlineCatalog',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '1MB',
  compression: true,
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@AppStream1;


CREATE OR REPLACE TARGET @APPNAME@sap_target USING DatabaseWriter( 
  DatabaseProviderType:'SAPHANA',
  ConnectionRetryPolicy: 'retryInterval=30,maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'SYSTEM',
  Password_encrypted: 'false',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:sap://10.77.21.116:39013/?databaseName=striim&currentSchema=QA',
  Tables: 'waction.crash_type,QA.CRASH_TYPES',
  adapterName: 'DatabaseWriter',
  --IgnorableExceptionCode: '',
  Password: 'Striim_SAP@123'
 ) 
INPUT FROM @APPNAME@AppStream1;


create or replace target @APPNAME@sys_tgt using sysout(
name:Foo2
)input from @APPNAME@AppStream1;

END APPLICATION @APPNAME@;

deploy application @APPNAME@;
start application @APPNAME@;

--
-- Recovery Test 22 with two sources, two sliding attribute windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sa5W -> CQ1 -> WS
-- S2 -> Sa6W -> CQ2 -> WS
--

STOP KStreamRecov22Tester.KStreamRecovTest22;
UNDEPLOY APPLICATION KStreamRecov22Tester.KStreamRecovTest22;
DROP APPLICATION KStreamRecov22Tester.KStreamRecovTest22 CASCADE;
DROP USER KStreamRecov22Tester;
DROP NAMESPACE KStreamRecov22Tester CASCADE;
CREATE USER KStreamRecov22Tester IDENTIFIED BY KStreamRecov22Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov22Tester;
CONNECT KStreamRecov22Tester KStreamRecov22Tester;

CREATE APPLICATION KStreamRecovTest22 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION KStreamRecovTest22;

use admin;
drop application ns1.testgroupapp1 cascade;
drop application ns1.testgroupapp2 cascade;
drop application ns2.testgroupapp1 cascade;
drop application ns2.testgroupapp2 cascade;
drop namespace ns1 CASCADE;
drop namespace ns2 CASCADE;

--
-- Crash Recovery Test 5 with Jumping window and partitioned on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION KStreamN2S2CR5Tester.KStreamN2S2CRTest5;
UNDEPLOY APPLICATION KStreamN2S2CR5Tester.KStreamN2S2CRTest5;
DROP APPLICATION KStreamN2S2CR5Tester.KStreamN2S2CRTest5 CASCADE;

DROP USER KStreamN2S2CR5Tester;
DROP NAMESPACE KStreamN2S2CR5Tester CASCADE;
CREATE USER KStreamN2S2CR5Tester IDENTIFIED BY KStreamN2S2CR5Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR5Tester;
CONNECT KStreamN2S2CR5Tester KStreamN2S2CR5Tester;

CREATE APPLICATION KStreamN2S2CRTest5 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest5;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest5 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest5;

CREATE FLOW DataProcessingKStreamN2S2CRTest5;

CREATE TYPE CsvDataTypeKStreamN2S2CRTest5 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvDataTypeKStreamN2S2CRTest5 PARTITION BY merchantId;

CREATE CQ CsvToDataKStreamN2S2CRTest5
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest5 CONTEXT OF CsvDataTypeKStreamN2S2CRTest5
EVENT TYPES ( CsvDataTypeKStreamN2S2CRTest5 )
@PERSIST-TYPE@

CREATE CQ DataToWactionKStreamN2S2CRTest5
INSERT INTO WactionsKStreamN2S2CRTest5
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingKStreamN2S2CRTest5;

END APPLICATION KStreamN2S2CRTest5;

CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'data.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO @AppName@_rawstream;

CREATE OR REPLACE STREAM @BuiltinFunc@_Stream OF Global.WAEVent;
CREATE OR REPLACE STREAM CombineStream OF Global.WAEVent;

CREATE OR REPLACE CQ cq1
INSERT INTO @BuiltinFunc@_Stream
SELECT
@BuiltinFunc@(s1, 'city',data[5])
FROM @AppName@_rawstream s1;

CREATE OR REPLACE CQ cq2
INSERT INTO CombineStream
Select *
FROM @BuiltinFunc@_Stream s4;

CREATE OR REPLACE CQ cq3
INSERT INTO CombineStream
select *
FROM @AppName@_rawstream s5;

CREATE OR REPLACE TARGET @AppName@_FileTarget USING Global.FileWriter ( 
  flushpolicy: 'EventCount:10000,Interval:30s', 
  adapterName: 'FileWriter', 
  directory: '@logs@',
  filename: '@BuiltinFunc@_Data', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING Global.JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter', 
  jsonMemberDelimiter: '\n', 
  EventsAsArrayOfJsonObjects: 'true', 
  formatterName: 'JSONFormatter', 
  jsonobjectdelimiter: '\n' ) 
INPUT FROM CombineStream;

End application @AppName@;
Deploy application @AppName@; 
Start application @AppName@;

STOP APPLICATION routerApp;
UNDEPLOY APPLICATION routerApp;
DROP APPLICATION routerApp CASCADE;


CREATE APPLICATION routerApp;

CREATE  SOURCE OraSource USING OracleReader  (
Username: 'qatest',
Password: 'qatest',
ConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',
Tables: 'QATEST.TGT_T%',
 FetchSize:'100'
)
OUTPUT TO MasterStream1;

CREATE OR REPLACE ROUTER tablerouter1 INPUT FROM MasterStream1 s CASE
WHEN meta(s,"TableName").toString()='QATEST.TGT_T1' THEN ROUTE TO ss1,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T2' THEN ROUTE TO ss2,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T3' THEN ROUTE TO ss3,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T4' THEN ROUTE TO ss4,
ELSE ROUTE TO ss_else;

create Target FileTarget_1 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'
)
FORMAT USING dsvFormatter ()
input from ss1;

create Target FileTarget_2 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'
)
FORMAT USING dsvFormatter ()
input from ss2;

create Target FileTarget_3 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from ss3;

create Target FileTarget_4 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from ss4;


create Target FileTarget_5 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from ss_else;


end application routerApp;
deploy application routerApp;
start routerApp;

stop tpcc;
undeploy application tpcc;
drop application tpcc cascade;
CREATE APPLICATION tpcc;

Create Source oracSource
 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'localhost:1521:orcl',
 Tables:'QATEST.TIMETEST',
 Fetchsize:1
)
Output To DataStream;

CREATE TARGET WriteCDCOracle USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:orcl',
  Username:'qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'QATEST.TIMETEST,QATEST.TIMETEST_TGT'
) INPUT FROM DataStream;

create Target t2 using SysOut(name:Foo2) input from DataStream;

END APPLICATION tpcc;
deploy application tpcc in default;
start tpcc;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING MSSqlReader
(
  Compression: false,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.test01',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
)
OUTPUT TO @APPNAME@_SS;


CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(id,name) COLUMNMAP(id=id,col1=col11,col2=col12)',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

--
-- Recovery Test 24 with two sources, two sliding time-count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5a9W  -> CQ1 -> WS
-- S2 -> Sc6a11W -> CQ2 -> WS
--

STOP Recov24Tester.RecovTest24;
UNDEPLOY APPLICATION Recov24Tester.RecovTest24;
DROP APPLICATION Recov24Tester.RecovTest24 CASCADE;
CREATE APPLICATION RecovTest24 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION RecovTest24;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;
CREATE  SOURCE @FirstSourceName@ USING DatabaseReader  ( 
  ConnectionURL: '@SourceConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  Tables: '@SourceTable@',
  ReplicationSlotName: 'null'
 ) OUTPUT TO @SRCFirstINPUTSTREAM@;

 CREATE  SOURCE @MiddleSourceName@ USING DatabaseReader  ( 
  ConnectionURL: '@SourceConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  Tables: '@SourceTable@',
  ReplicationSlotName: 'null'
 )
OUTPUT TO @SRCMiddleINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCMiddleINPUTSTREAM@;

CREATE  TARGET @FirsttargetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
 ) 
 INPUT FROM @SRCFirstINPUTSTREAM@;

 CREATE  TARGET @MiddletargetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
 )
INPUT FROM @SRCMiddleINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

--
-- Recovery Test 40 with two sources and two WactionStores. A variety of partitioned windows in between
-- assure that we are testing a complicated recovery scenario.
--
-- NOTE THIS APP IS INCONSISTENT AND NOT COMPATIBLE WITH THE CURRENT VERSION OF RECOVERY BECAUSE IT HAS COMBINING STREAMS
--
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> JWc2 -> JWc5 -> WS1
--   S2 -> JWc2 -> JWc7 -> WS2
--

STOP Recov40Tester.RecovTest40;
UNDEPLOY APPLICATION Recov40Tester.RecovTest40;
DROP APPLICATION Recov40Tester.RecovTest40 CASCADE;
CREATE APPLICATION RecovTest40 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStreamTop OF CsvData;

CREATE CQ Csv1ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ Csv2ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;








CREATE JUMPING WINDOW TopJWc2
OVER DataStreamTop KEEP 2 ROWS;







CREATE STREAM DataStreamLeft OF CsvData;
CREATE STREAM DataStreamRight OF CsvData;

CREATE CQ DataToLeft
INSERT INTO DataStreamLeft
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM TopJWc2 p;

CREATE CQ DataToRight
INSERT INTO DataStreamRight
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM TopJWc2 p;






CREATE JUMPING WINDOW LeftJWc5
OVER DataStreamLeft KEEP 5 ROWS;

CREATE JUMPING WINDOW RightJWc10
OVER DataStreamRight KEEP 10 ROWS;



CREATE WACTIONSTORE WactionsLeft CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsRight CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ ToWactionsLeft
INSERT INTO WactionsLeft
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM LeftJWc5 p;

CREATE CQ ToWactionsRight
INSERT INTO WactionsRight
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM RightJWc10 p;

END APPLICATION RecovTest40;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

STOP AdhocTester.ws_one;
UNDEPLOY APPLICATION AdhocTester.ws_one;
DROP APPLICATION AdhocTester.ws_one cascade;

CREATE APPLICATION ws_one;


CREATE SOURCE wsSource USING CSVReader
(
directory:'@TEST-DATA-PATH@',
header: Yes,
wildcard:'AdhocQueryData2.csv',
columndelimiter:',',
blocksize: 10240,
positionByEOF:false
) OUTPUT TO QaStream;

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE ZipLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'AdhocQueryData.csv',
  header: Yes,
  columndelimiter: '	',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

Create TYPE wsData(
	CompanyNum String,
	CompanyName String KEY,
	CompanyCode int,
	Zip String
);


CREATE STREAM wsStream OF wsData;

--Select data from QaStream and insert into wsStream

CREATE CQ csvTowsData
INSERT INTO wsStream
SELECT  data[0],
    data[1],
    TO_INT(data[3]),
    data[9]
 FROM QaStream;




CREATE WACTIONSTORE oneWS CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@


CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT * FROM wsStream
LINK SOURCE EVENT;

END APPLICATION ws_one;

STOP JSONRecoveryApp;

UNDEPLOY APPLICATION admin.JSONRecoveryApp;
DROP APPLICATION admin.JSONRecoveryApp cascade;

CREATE APPLICATION JSONRecoveryApp RECOVERY 1 SECOND INTERVAL;

CREATE TYPE Emptype (
firstName String,
lastName String );

CREATE STREAM EmpStream of Emptype;

create source CSVSource using FileReader (
	directory:'/Users/bhashemi/Product/IntegrationTests/TestData/jsonRecov',
	WildCard:'jsonRecov*.json',
	positionByEOF:false
) PARSE USING
JSONParser (
eventType:''
) OUTPUT TO EmpStream;

CREATE WACTIONSTORE jsonWactions CONTEXT OF Emptype
EVENT TYPES ( Emptype )
@PERSIST-TYPE@

CREATE CQ InsertjsonWactions
INSERT INTO jsonWactions
SELECT firstName, lastName
FROM EmpStream;

CREATE TARGET jsonRecovSYSOUT using SysOut(name:jsonrecov) INPUT FROM EmpStream;
END APPLICATION JSONRecoveryApp;

deploy application JSONRecoveryApp in default;
START JSONRecoveryApp;

CREATE OR REPLACE APPLICATION @AppName@;

Create Source @AppName@_source Using OracleReader(
  Username:'@username@',
  Password:'@password@',
  ConnectionURL:'@url@',
  Tables:'@srctableName@',
  Fetchsize:1
)
Output To @AppName@_Stream;


CREATE OR REPLACE TARGET @AppName@_DB_Target USING Global.DeltaLakeWriter (
connectionProfileName: 'admin.@DBCP@',
   useConnectionProfile: 'true',
  Tables: '@srctableName@,@trgtableName@',
  uploadPolicy: 'eventcount:100000,interval:60s'
)

INPUT FROM @AppName@_Stream;
END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

stop application APP_KAFKASOURCE_AGG;
undeploy application APP_KAFKASOURCE_AGG;
drop application APP_KAFKASOURCE_AGG cascade;

CREATE APPLICATION APP_KAFKASOURCE_AGG;

CREATE OR REPLACE TYPE STREAM_CQ_CALCULATELAG_Type (
 topic java.lang.String KEY,
 TotalTopicLag java.lang.Integer,
 lastdatatime org.joda.time.DateTime,
 status java.lang.String,
 is_green java.lang.Integer);

CREATE OR REPLACE TYPE STREAM_CQ_JOIN_KAFKA_SOURCES1_Type (
 Company java.lang.String KEY,
 TotalLast24hour java.lang.Integer,
 TotalLast1hour java.lang.Integer,
 TotalTopicLag java.lang.Integer,
 lastdatatime org.joda.time.DateTime,
 status java.lang.String,
 is_green java.lang.Integer,
 latitude java.lang.String,
 longitude java.lang.String,
 topic java.lang.String,
 city_name java.lang.String,
 city_id java.lang.Integer);

CREATE OR REPLACE TYPE STREAM_CQ_CALCULATE_HOURLYTOTALS_Type (
 topic java.lang.String KEY,
 TotalLast24hour java.lang.Integer,
 TotalLast1hour java.lang.Integer);

CREATE OR REPLACE CQ CQ_GET_LASTHOUR
INSERT INTO STREAM_CQ_GET_LASTHOUR
SELECT rawdatacount, topic,timerange from  ET_HOURLYTOTALS_KAFKADATA_FILE,JUMP_WND_1EVT_30SEC where timerange = DHOURS(DNOW())-1;

CREATE OR REPLACE CQ CQ_CALCULATE_HOURLY_TOTAL
INSERT INTO STREAM_CQ_CALCULATE_HOURLY_TOTAL
SELECT f.topic as topic, sum(f.rawdatacount) as TotalLast24hour, B.rawdatacount as TotalLast1hour FROM JUMP_WND_1EVT_1MIN h
   join SLIDE_WND_HOURLYTOTALS_KAFKADATA_FILE f on 1=1
   join STREAM_CQ_GET_LASTHOUR B on B.topic=f.topic
   Group by f.topic;

CREATE OR REPLACE EVENTTABLE ET_KAFKA_HOURLY_TOTAL USING STREAM (
  name: 'STREAM_CQ_CALCULATE_HOURLY_TOTAL' )
QUERY (
  keytomap: 'topic',
  persistPolicy: 'true' )
OF STREAM_CQ_CALCULATE_HOURLY_TOTAL_Type;

END APPLICATION APP_KAFKASOURCE_AGG;

--
-- Recovery Test 35 with two sources, two jumping count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5W/p -> CQ1 -> WS
--   S2 -> Jc6W/p -> CQ2 -> WS
--

STOP KStreamRecov35Tester.KStreamRecovTest35;
UNDEPLOY APPLICATION KStreamRecov35Tester.KStreamRecovTest35;
DROP APPLICATION KStreamRecov35Tester.KStreamRecovTest35 CASCADE;

DROP USER KStreamRecov35Tester;
DROP NAMESPACE KStreamRecov35Tester CASCADE;
CREATE USER KStreamRecov35Tester IDENTIFIED BY KStreamRecov35Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov35Tester;
CONNECT KStreamRecov35Tester KStreamRecov35Tester;

CREATE APPLICATION KStreamRecovTest35 RECOVERY 5 SECOND INTERVAL

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION KStreamRecovTest35;

-- detectFraudsterApp1.tql, detectFraudsterApp1_vis_settings.json, apiCallLogs.txt are needed to run this example.
-- gamelogs simulates api call logs for an online game. xferCur represents virtual currency transfer.
-- If this method is called by same user more than once in 1000 records its anomaly and hence waction.

IMPORT static com.webaction.runtime.converters.DateConverter.*;

UNDEPLOY APPLICATION admin.detectFraudsterApp1;
DROP APPLICATION admin.detectFraudsterApp1 CASCADE;
CREATE APPLICATION detectFraudsterApp1;


/* READ DATA FROM A CSV FILE. USE CSV-READER  */
CREATE source gameCSVDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@/persistence',
  header:No,
  wildcard:'apiCallLogs.txt',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO PCsvStream;

create jumping window apiwindow over PCsvStream keep 1000 rows;

CREATE TYPE apiCallData(
  date DateTime,
  apiName String,
  amount int,
  username String key
);

CREATE STREAM apiCallStream OF apiCallData;

/* FROM INPUT DATA READ ABOVE, CREATE EVENTS OF TYPE apiCallData
 * TO_INT(data[10]) > 100000 --> 3 WACTIONS
 * TO_INT(data[10]) > 50000 --> 8 WACTIONS
 * TO_INT(data[10]) > 25000 --> 11 WACTIONS
 * TO_INT(data[10]) > 20000 --> 14 WACTIONS
 * */

CREATE CQ gameCQ
INSERT INTO apiCallStream
SELECT
    TO_DATE(TO_LONG(data[3])),
    data[10],
    TO_INT(data[11]),
    data[12]
FROM apiwindow
where cast (data[10] as String)  =  'xferCur' and  TO_INT(data[11]) > 50000;

create TYPE apiCallContext(
  date DateTime,
  apiName String,
  amount int,
  username String key
);

/* PERSISTING TO DERBY  */
CREATE WACTIONSTORE fraudWactionsDerby
CONTEXT OF apiCallContext
EVENT TYPES (apiCallData )
PERSIST EVERY 2 second USING
(
JDBC_DRIVER:'@WASTORE-DRIVER@',
JDBC_URL:'@WASTORE-URL@;CREATE=true',
JDBC_USER:'@WASTORE-UNAME@',
JDBC_PASSWORD:'@WASTORE-PASSWORD@',
DDL_GENERATION:'drop-and-create-tables',
LOGGING_LEVEL:'SEVERE'
);


CREATE CQ populateFraudWactionsDerby
INSERT INTO fraudWactionsDerby
SELECT date, apiName, amount, username
FROM apiCallStream
LINK SOURCE EVENT;


/* PERSISTING TO MYSQL  */
/*
CREATE WACTIONSTORE fraudWactionsMySQL
CONTEXT OF apiCallContext
EVENT TYPES (apiCallData )
PERSIST EVERY 3 second USING
(
JDBC_DRIVER:'com.mysql.jdbc.Driver',
JDBC_URL:'jdbc:mysql://127.0.0.1:3306/test',
JDBC_USER:root,
JDBC_PASSWORD:'root',
DDL_GENERATION:'drop-and-create-tables',
LOGGING_LEVEL:'SEVERE'
);

CREATE CQ populateFraudWactionsMySQL
INSERT INTO fraudWactionsMySQL
SELECT date, apiName, amount, username
FROM apiCallStream
LINK SOURCE EVENT;
*/
/* PERSISTING TO MONGODB  */
/*
CREATE WACTIONSTORE fraudWactionsMongo
CONTEXT OF apiCallContext
EVENT TYPES (apiCallData )
PERSIST EVERY 5 second USING
(
NOSQL_PROPERTY:'localhost:27017',
TARGET_DATABASE:'org.eclipse.persistence.nosql.adapters.mongo.MongoPlatform',
DDL_GENERATION:'drop-and-create-tables',
DB_NAME:db
);


CREATE CQ populateFraudWactionsMongo
INSERT INTO fraudWactionsMongo
SELECT date, apiName, amount, username
FROM apiCallStream
LINK SOURCE EVENT;
*/



--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM PCsvStream;
--CREATE TARGET output2 USING SysOut(name : selectedinput) input FROM apiCallStream;


CREATE VISUALIZATION detectFraudsterApp1 "@TEST-DATA-PATH@/json/detectFraudsterApp1_vis_settings.json";

END APPLICATION detectFraudsterApp1;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@ @APP_PROPERTY@ USE EXCEPTIONSTORE;

CREATE OR REPLACE STREAM @APP_NAME@_DataStreamFromCQ OF Global.WAEvent;

Create Source @APP_NAME@_Source Using @SOURCE_ADAPTER@ (

) OUTPUT TO @APP_NAME@_DataStream;

CREATE OR REPLACE CQ @APP_NAME@_CQ INSERT INTO @APP_NAME@_DataStreamFromCQ SELECT * FROM @APP_NAME@_DataStream s where META(s,"TableName") is not null AND META(s,"TableName").toString().trim().isEmpty() == false;

CREATE TARGET @APP_NAME@_Target USING @TARGET_ADAPTER@ ( 

) INPUT FROM @APP_NAME@_DataStreamFromCQ;

CREATE OR REPLACE TARGET @APP_NAME@_SysOut_ReadFromSource USING Global.SysOut ( 
	name: '@APP_NAME@_SysOutWA_Source' 
) INPUT FROM @APP_NAME@_DataStream;

CREATE OR REPLACE TARGET @APP_NAME@_SysOut_WriteToTarget USING Global.SysOut ( 
	name: '@APP_NAME@_SysOutWA_Target' 
) INPUT FROM @APP_NAME@_DataStreamFromCQ;

END APPLICATION @APP_NAME@;

DEPLOY APPLICATION @APP_NAME@ IN DEFAULT;
START APPLICATION @APP_NAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 1 second interval;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@'
  --recordSelector: '@PROP8@'
)
OUTPUT TO @APPNAME@Stream;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;

/*
create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:1,Interval:5',
  CommitPolicy: 'EventCount:1,Interval:5',
  Tables: 'QATEST.@table@'
)
input from @APPNAME@Stream;*/
end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

undeploy application RedshiftColmap;
alter application RedshiftColmap;

CREATE OR REPLACE SOURCE OracleSource USING OracleReader  (
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: '@SOURCE_TABLES@',
  FetchSize: 1
 ) Output To LogminerStream;
     
CREATE OR REPLACE TARGET RedshiftTarget USING RedshiftWriter
	(
	  ConnectionURL: '@TARGET-URL@',
	  Username: '@TARGET-UNAME@',
	  Password: '@TARGET-PASSWORD@',
	  bucketname: '@BUCKETNAME@',
	  --accesskeyId: '@ACCESS-KEY-ID@',
	  --secretaccesskey: '@SECRET-ACCESS-KEY@',
	  S3IAMRole:'@IAMROLE@',
	  Tables: '@TARGET-TABLES@',
	  uploadpolicy:'eventcount:1,interval:5s',
	  Mode:'incremental'
	) INPUT FROM LogminerStream;
	
END APPLICATION RedshiftColmap;
ALTER APPLICATION RedshiftColmap RECOMPILE;
deploy application RedshiftColmap;
START application RedshiftColmap;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@ @APP_PROPERTY@ USE EXCEPTIONSTORE;

CREATE OR REPLACE STREAM @APP_NAME@_DataStream OF Global.WAEvent;

Create Source @APP_NAME@_Source Using @SOURCE_ADAPTER@ (

) OUTPUT TO @APP_NAME@_DataStream;

CREATE TARGET @APP_NAME@_Target USING @TARGET_ADAPTER@ ( 

) INPUT FROM @APP_NAME@_DataStream;

CREATE OR REPLACE TARGET @APP_NAME@_SysOut USING Global.SysOut ( 
	name: '@APP_NAME@_SysOutWA' 
) INPUT FROM @APP_NAME@_DataStream;

END APPLICATION @APP_NAME@;

DEPLOY APPLICATION @APP_NAME@ IN DEFAULT;
START APPLICATION @APP_NAME@;

STOP OneAgentTester.CSV;
UNDEPLOY APPLICATION OneAgentTester.CSV;
DROP APPLICATION OneAgentTester.CSV CASCADE;
CONNECT ADMIN abc;
DROP USER OneAgentTester;
USE ADMIN;
DROP NAMESPACE OneAgentTester CASCADE;
drop dg AGENTS;
drop dg LocalServer;

drop namespace hubspot cascade force;
create namespace hubspot;
use hubspot;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ RECOVERY 30 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;

CREATE FLOW @AppName@_SourceFlow;

CREATE SOURCE @srcName@ USING Global.HubSpotReader ( 
  PollingInterval: '1m', 
  StartPosition: '%=-1', 
  Tables: '@objectname@s__c', 
  ThreadPoolCount: '10', 
  ConnectionPoolSize: '20', 
  ClientSecret_encrypted: 'false', 
  RefreshToken: '', 
  RefreshToken_encrypted: 'false', 
  Mode: 'automated', 
  useConnectionProfile: false,
  AuthMode: 'PrivateAppToken', 
  PrivateAppToken: '@accesstoken@', 
  ClientSecret: '', 
  ClientId: '', 
  PrivateAppToken_encrypted: 'false', 
  MigrateSchema: true ) 
OUTPUT TO @outstreamname@;

END FLOW @AppName@_SourceFlow;

CREATE TARGET @tgtName@ USING Global.BigQueryWriter ( 
  projectId: '@projectId@',
  batchPolicy: 'eventcount:10000,interval:2', 
  streamingUpload: 'true', 
  Mode: 'MERGE', 
  CDDLAction : 'process',
  CDDLOptions: '{\"CreateTable\":{\"action\":\"IgnoreIfExists\",\"options\":[{\"CreateSchema\":{\"action\":\"IgnoreIfExists\"}}]}}', 
  ServiceAccountKey: '@keyFileName@', 
  Tables: '%,@tgtschema@.%' ) 
INPUT FROM @instreamname@;

End APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

Create Source dataGenSrc using MySQLReader
(
 Username:'root',
 Password:'w@ct10n',
 ConnectionURL: 'mysql://127.0.0.1:3306/waction',
 Tables:'@tableNames@'
 )
 Output To LCRStream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;

END APPLICATION DataGenSampleApp;

use PosTester;
alter application PosApp;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startTime DateTime,
  count int,
  totalAmount double,
  hourlyAve int,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);

end application PosApp;
alter application PosApp recompile;

--
-- Crash Recovery Test 6 with Jumping window and partitioned on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> KafkaStream -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION N2S2CR6Tester.N2S2CRTest6;
UNDEPLOY APPLICATION N2S2CR6Tester.N2S2CRTest6;
DROP APPLICATION N2S2CR6Tester.N2S2CRTest6 CASCADE;
CREATE APPLICATION N2S2CRTest6 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest6;

CREATE SOURCE CsvSourceN2S2CRTest6 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;




CREATE TYPE CsvDataTypeN2S2CRTest6 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream of CsvDataTypeN2S2CRTest6 using KafkaProps;

CREATE CQ TransferToKafka
INSERT INTO KafkaCsvStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;








END FLOW DataAcquisitionN2S2CRTest6;

CREATE FLOW DataProcessingN2S2CRTest6;

CREATE STREAM DataStream OF CsvDataTypeN2S2CRTest6 PARTITION BY merchantId;

CREATE CQ CsvToDataN2S2CRTest6
INSERT INTO DataStream
SELECT
    *
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsN2S2CRTest6 CONTEXT OF CsvDataTypeN2S2CRTest6
EVENT TYPES ( CsvDataTypeN2S2CRTest6 )
@PERSIST-TYPE@

CREATE CQ DataToWactionN2S2CRTest6
INSERT INTO WactionsN2S2CRTest6
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN2S2CRTest6;

END APPLICATION N2S2CRTest6;

stop application ManyToManyADLSGen2;
undeploy application ManyToManyADLSGen2;
drop application ManyToManyADLSGen2 cascade;

create application ManyToManyADLSGen2 Recovery 5 second interval;


create type csv_type_gen2(
id String,
name String,
seq String
);

create type Order_type_gen2(
id String,
Name String,
Company String
);



create source CSVSource_multi_gen2 using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'Canon1000_All.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO CsvStream_user_gen2;

create source CSVSource2_gen2 using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'portfolio.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO CsvStream_gen2;

create source CSVSource3_gen2 using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'DataCenterData.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO CsvStream_gen2;


CREATE SOURCE OraSource1_gen2 USING OracleReader
(
 Username:'miner',
 Password:'miner',
 ConnectionURL:'localhost:1521:xe',
 Tables:'QATEST.CUSTOMER1,QATEST.CUSTOMER2,QATEST.CUSTOMER3',
 OnlineCatalog:true,
 FetchSize:10000,
 QueueSize:2148,
 CommittedTransactions:false,
 Compression:true
)
OUTPUT TO OrdersStream_gen2;

CREATE SOURCE OraSource2_gen2 USING OracleReader
(
 Username:'miner',
 Password:'miner',
 ConnectionURL:'localhost:1521:xe',
 Tables:'QATEST.CUSTOMER4,QATEST.CUSTOMER5',
 OnlineCatalog:true,
 FetchSize:10000,
 QueueSize:2148,
 CommittedTransactions:false,
 Compression:true
)
OUTPUT TO OrdersStream_gen2;


create Target ADLSGen2_tgt1 using ADLSGen2Writer(
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatest',
        directory:'%@metadata(FileName)%',
        filename:'event_data.csv',
        uploadpolicy:'eventcount:5'
)
FORMAT USING JSONFormatter()
input from CsvStream_gen2; 

create Target ADLSGen2_tgt2 using ADLSGen2Writer(
          accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'%@metadata(TableName)%',
        filename:'table.csv',
        uploadpolicy:'filesize:10M',
        compressiontype: 'true'
)
format using DSVFormatter (
)
input from OrdersStream_gen2; 

create stream UserdataStream_gen2 of Global.WAEvent;

Create CQ CQUser_gen2
insert into UserdataStream_gen2
select 
putuserdata (data1,'Fileowner',data[1]) from CsvStream_user_gen2 data1;

create stream CSVTypedStream1_gen2 of csv_type_gen2;
create stream CSVTypedStream2_gen2 of csv_type_gen2;
create stream CSVTypedStream3_gen2 of csv_type_gen2;

CREATE CQ cq1_gen2
INSERT INTO CSVTypedStream1_gen2
SELECT data[0],
data[1],
data[2]
FROM UserdataStream_gen2
WHERE USERDATA(UserdataStream_gen2,'Fileowner').toString() == 'Lorem';

CREATE CQ cq2
INSERT INTO CSVTypedStream2_gen2
SELECT data[0],
data[1],
data[2]
FROM UserdataStream_gen2
WHERE USERDATA(UserdataStream_gen2,'Fileowner').toString() == 'doloremque';

CREATE CQ cq3
INSERT INTO CSVTypedStream3_gen2
SELECT data[0],
data[1],
data[2]
FROM UserdataStream_gen2
WHERE USERDATA(UserdataStream_gen2,'Fileowner').toString() == 'accusantium';

create Target ADLSGen2_tgt3 using ADLSGen2Writer(
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'1up_%name%',
        filename:'many_event_data.csv',
        uploadpolicy:'eventcount:74'
)
format using XMLFormatter (
  elementtuple: 'Eventname:name:id:seq:text=name',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from CSVTypedStream1_gen2; 

create Target ADLSGen2_tgt4 using ADLSGen2Writer(
         accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'2up_%name%',
        filename:'many_event_data.csv',
        uploadpolicy:'interval:10s'
)
format using XMLFormatter (
  elementtuple: 'Eventname:name:id:seq:text=name',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from CSVTypedStream2_gen2; 

create Target ADLSGen2_tgt5 using ADLSGen2Writer(
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'3up_%name%',
        filename:'many_event_data.csv',
        uploadpolicy:'eventcount:10,intervals:30s'
)
format using XMLFormatter (
  elementtuple: 'Eventname:name:id:seq:text=name',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from CSVTypedStream3_gen2; 


create stream OrderTypedStream1_gen2 of Order_type_gen2;
create stream OrderTypedStream2_gen2 of Order_type_gen2;
create stream OrderTypedStream3_gen2 of Order_type_gen2;

CREATE CQ cq1_gen2_db
INSERT INTO OrderTypedStream1_gen2
SELECT data[0],
data[1],
data[2]
FROM OrdersStream_gen2
WHERE META(OrdersStream_gen2,'TableName').toString() == 'QATEST.CUSTOMER1';

CREATE CQ cq2_gen2_db
INSERT INTO OrderTypedStream2_gen2
SELECT data[0],
data[1],
data[2]
FROM OrdersStream_gen2
WHERE META(OrdersStream_gen2,'TableName').toString() == 'QATEST.CUSTOMER2';

CREATE CQ cq3_gen2_db
INSERT INTO OrderTypedStream3_gen2
SELECT data[0],
data[1],
data[2]
FROM OrdersStream_gen2
WHERE META(OrdersStream_gen2,'TableName').toString() == 'QATEST.CUSTOMER3';


create Target ADLSGen2_tgt6 using ADLSGen2Writer(
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'Customer1',
        filename:'many_event_data.csv',
        uploadpolicy:'eventcount:10000'
)
format using AvroFormatter (
  formatAs: 'Default',
  handler: 'com.webaction.proc.AvroFormatter',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA@1.avsc'
)
input from OrderTypedStream1_gen2; 

create Target ADLSGen2_tgt7 using ADLSGen2Writer(
       accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'Customer2',
        filename:'many_event_data.csv',
        uploadpolicy:'eventcount:10000'
)
format using AvroFormatter (
  formatAs: 'Default',
  handler: 'com.webaction.proc.AvroFormatter',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA@2.avsc'
)
input from OrderTypedStream2_gen2; 

create Target ADLSGen2_tgt8 using ADLSGen2Writer(
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'Customer3',
        filename:'many_event_data.csv',
        uploadpolicy:'eventcount:10000'
)
format using AvroFormatter (
  formatAs: 'Default',
  handler: 'com.webaction.proc.AvroFormatter',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA@3.avsc'
)
input from OrderTypedStream3_gen2; 

end application ManyToManyADLSGen2;

deploy application ManyToManyADLSGen2;
start application ManyToManyADLSGen2;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@ recovery 5 SECOND Interval;
CREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: '@ORACLE-URL@',
  Tables: '@SOURCE-TABLES@',
  Username: '@ORACLE-USERNAME@',
  Password: '@ORACLE-PASSWORD@'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt USING SnowflakeWriter

(
  ConnectionURL:'@SNOWFLAKE-URL@',
  username:'@SNOWFLAKE-USERNAME@',
  appendOnly:'false',
  Tables:'@TARGET-TABLES@',
  uploadpolicy:'eventcount:3,interval:10s',
  externalStageType:'local'
)
INPUT FROM @APP_NAME@_Stream;


END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

stop application CDCTester.CDCTest;
undeploy application CDCTester.CDCTest;
drop application CDCTester.CDCTest cascade;

create application CDCTest;

Create Source Rac11g Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'QATEST.SAMPLETEST2',
 FetchSize:1,
 QueueSize:2048
)
Output To LCRStream;


end application CDCTest;
deploy application CDCTest;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ recovery 5 second Interval;

CREATE SOURCE @src1Name@ USING MySQLReader (
  Username:'@srcusername@',
  Password:'@srcpassword@',
  ConnectionURL:'@srcurl@',
  Tables:'@srcschema@.@srctable@',
  BidirectionalMarkerTable: '@srcschema@.@srcbidirectionaltable@'
)
OUTPUT TO @outstream1name@;

CREATE TARGET @tgt1Name@ USING DatabaseWriter (
  ConnectionURL:'@tgturl@',
  Username:'@tgtusername@',
  Password:'@tgtpassword@',
  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
  CheckPointTable: 'CHKPOINT',
  BidirectionalMarkerTable: '@tgtschema@.@tgtbidirectionaltable@'
)
INPUT FROM @instream1name@;

CREATE SOURCE @src2Name@ USING MSSQLReader (
  ConnectionURL:'@tgturl@',
  DatabaseName:'@databasename@',
  Username:'@tgtusername@',
  Password:'@tgtpassword@',
  Tables:'@tgtschema@.@tgttable@',
  BidirectionalMarkerTable: '@tgtschema@.@tgtbidirectionaltable@',
  TransactionSupport: true
)
OUTPUT TO @outstream2name@;

CREATE TARGET @tgt2Name@ USING DatabaseWriter (
  Username:'@srcusername@',
  Password:'@srcpassword@',
  ConnectionURL:'@srcurl@',
  Tables: '@tgtschema@.@tgttable@,@srcschema@.@srctable@',
  CheckPointTable: 'CHKPOINT',
  BidirectionalMarkerTable: '@srcschema@.@srcbidirectionaltable@'
)
INPUT FROM @instream2name@;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

--
-- Recovery Test 31 with two sources, two sliding count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5W/p -> CQ1 -> WS
-- S2 -> Sc6W/p -> CQ2 -> WS
--

STOP KStreamRecov31Tester.KStreamRecovTest31;
UNDEPLOY APPLICATION KStreamRecov31Tester.KStreamRecovTest31;
DROP APPLICATION KStreamRecov31Tester.KStreamRecovTest31 CASCADE;

DROP USER KStreamRecov31Tester;
DROP NAMESPACE KStreamRecov31Tester CASCADE;
CREATE USER KStreamRecov31Tester IDENTIFIED BY KStreamRecov31Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov31Tester;
CONNECT KStreamRecov31Tester KStreamRecov31Tester;

CREATE APPLICATION KStreamRecovTest31 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION KStreamRecovTest31;

create application XML;
create source CSVSource using FileReader (
	directory:'Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
	filename:'posdata_XML',
	rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'
)
format using XMLFormatter (
	rootelement:'document',
	elementtuple:'MerchantName:merchantid:text=merchantname'
)
input from TypedCSVStream;
end application XML;

Stop Oracle_Agent;
Undeploy application Oracle_Agent;
drop application Oracle_Agent cascade;

CREATE APPLICATION Oracle_Agent;

CREATE FLOW test_SourceFlow;

CREATE  SOURCE Oracle_Source USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.124.25:1521/orcl',
  Tables: 'QATEST.%',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO Oracle_ChangeDataStream ;

END FLOW test_SourceFlow;

CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Oracle_ChangeDataStream;

END APPLICATION Oracle_Agent;
DEPLOY APPLICATION Oracle_Agent ON ANY IN default WITH test_SourceFlow ON ANY IN AGENTS;
start Oracle_Agent;

stop application ADW1;
undeploy application ADW1;
drop application ADW1 cascade;
CREATE APPLICATION ADW1;

CREATE  SOURCE SqlServerInitialLoad1 USING DatabaseReader  
 (
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'@SOURCE-TABLES@',
 FetchSize:2000
) 
OUTPUT TO InitialLoadStream1;

CREATE TARGET AzureDWInitialLoad1 USING AzureSQLDWHWriter(
ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
)
INPUT FROM InitialLoadStream1;

END APPLICATION ADW1;
deploy application ADW1;
start application ADW1;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE APPLICATION @APPNAME@ WITH ENCRYPTION @Recovery@ USE EXCEPTIONSTORE;
CREATE SOURCE @APPNAME@_s USING FileReader(
  directory:'Samples/AppData',
  wildcard:'PO.JSON',
  positionByEOF:false
)
parse using JSONParser (
) OUTPUT TO @APPNAME@_ss1;

--Using JSONNew() to create empty json node (j1)
CREATE CQ @APPNAME@_cq1 INSERT INTO @APPNAME@_ss2 SELECT JSONNew() j1,* FROM @APPNAME@_ss1 ;
--Using JSONSet() to set value to the empty json node (j1) and naming it as (j2)
CREATE CQ @APPNAME@_cq2 INSERT INTO @APPNAME@_ss3 SELECT JSONSet(j1, "Mission","Test") j2,* FROM @APPNAME@_ss2 ;
--Using JSONFrom() to create new json node (j3) using string
CREATE CQ @APPNAME@_cq3 INSERT INTO @APPNAME@_ss4 SELECT JSONFrom('{"PONumber":1600,"Reference":"ABULL-20140421","Requestor":"Alexis Bull","User":"ABULL","CostCenter":"A50","ShippingInstructions":{"name":"Alexis Bull","Address":{"street":"200 Sporting Green","city":"South San Francisco","state":"CA","zipCode":99236,"country":"United States of America"},"Phone":[{"type":"Office","number":"909-555-7307"},{"type":"Mobile","number":"415-555-1234"}]},"Special Instructions":null,"AllowPartialShipment":false,"LineItems":[{"ItemNumber":1,"Part":{"Description":"One Magic Christmas","UnitPrice":19.95,"UPCCode":13131092899},"Quantity":9,"Barcodes":[{"type":"Home","number":"9-555-7307"},{"type":"Office","number":"9-555-1234"}]},{"ItemNumber":2,"Part":{"Description":"Lethal Weapon","UnitPrice":19.95,"UPCCode":85391628927},"Quantity":5,"Barcodes":[{"type":"Home","number":"10-555-7307"},{"type":"Office","number":"10-555-1234"}]}]}')j3,* FROM @APPNAME@_ss3 ;
--Using JSONSet() to set new element to already existing node (data) - naming it as j4
CREATE CQ @APPNAME@_cq4 INSERT INTO @APPNAME@_ss5 SELECT JSONSet(data, "FileName","Dummy") j4,* FROM @APPNAME@_ss4 ;
--Using JSONArrayAdd() to Add element to existing JsonArray - naming it as j5
CREATE CQ @APPNAME@_cq5 INSERT INTO @APPNAME@_ss6 SELECT JSONArrayAdd(data.get("ShippingInstructions").get("Phone"),"work") j5,* FROM @APPNAME@_ss5 ;
--Using JSONArrayInsert() to Add element to existing JsonArray by specifying Index - naming it as j6
CREATE CQ @APPNAME@_cq6 INSERT INTO @APPNAME@_ss7 SELECT JSONArrayInsert(data.get("ShippingInstructions").get("Phone"),1,"residence") j6,* FROM @APPNAME@_ss6 ;
--Using JSONRemove() to Remove element from existing Json node 'data' - naming it as j7
CREATE CQ @APPNAME@_cq7 INSERT INTO @APPNAME@_ss8 SELECT JSONRemove(data,"Requestor") j7,* FROM @APPNAME@_ss7 ;
--Using JSONSetAll() to Add collection of objects (from metadata) to existing Json node 'data' - naming it as j8
CREATE CQ @APPNAME@_cq8 INSERT INTO @APPNAME@_ss9 SELECT JSONSetAll(j4, metadata) j8,* FROM @APPNAME@_ss8;
--Using JSONSetAll() in empty json node j1 - naming it as j9
CREATE CQ @APPNAME@_cq9 INSERT INTO @APPNAME@_ss10 SELECT JSONSetAll(j1, metadata) j9,* FROM @APPNAME@_ss9;
--Using JSONSetAll() with simple json node j2 - naming it as j10
CREATE CQ @APPNAME@_cq10 INSERT INTO @APPNAME@_ss11 SELECT JSONSetAll(j2, metadata) j10,* FROM @APPNAME@_ss10;
--Using JSONSetAll() with json node which has the same field/values to verify overriding
CREATE CQ @APPNAME@_cq11 INSERT INTO @APPNAME@_ss12 SELECT JSONSetAll(j10, metadata) j11,* FROM @APPNAME@_ss11;
create target @APPNAME@_t using sysout (name:ss1) input from @APPNAME@_ss12;

CREATE Stream @APPNAME@_str1 (
     PODetails com.fasterxml.jackson.databind.JsonNode,
     Phone com.fasterxml.jackson.databind.JsonNode,
     LineItems com.fasterxml.jackson.databind.JsonNode,
     BarCodes com.fasterxml.jackson.databind.JsonNode,
     Addr com.fasterxml.jackson.databind.JsonNode
);

CREATE CQ @APPNAME@_cq12
INSERT into @APPNAME@_str1
    select
    j8,
    j8.get('ShippingInstructions').get('Phone'),
    j8.get('LineItems'),
    j8.get('LineItems').get('Barcodes'),
    j8.get('ShippingInstructions').get('Address')
from @APPNAME@_ss12;

CREATE Stream @APPNAME@_str2(
     PONumber Integer,
     Reference String,
     Usr String,
     CostCenter String,
     Name String,
     street String,
     city String,
     state String,
     zipCode Integer,
     country String,
     officephone String,
     mobilephone String,
     SplInstruction String,
     AllowPartialShipment boolean,
     PhoneType String,
     PhoneNo String,
     ItemNumber Integer,
     ItemDesc String,
     UnitProce Double,
     UPCCode Double,
     Quantity Integer,
     BarcodeType String,
     BarcodeNo String
);

CREATE CQ @APPNAME@_cq13
INSERT into @APPNAME@_str2
SELECT

    /* PO Details */
    JSONGetInteger(x.PODetails,"PONumber"),
    JSONGetString(x.PODetails,"Reference"),
    JSONGetString(x.PODetails,"User"),
    JSONGetString(x.PODetails,"CostCenter"),

    /* Shipping Details */
    JSONGetString(x.PODetails.get('ShippingInstructions'),"name"),
    JSONGetString(x.Addr,"street"),
    JSONGetString(x.Addr,"city"),
    JSONGetString(x.Addr,"state"),
    JSONGetInteger(x.Addr,"zipCode"),
    JSONGetString(x.Addr,"country"),

    /* Phone nos by array position*/
    JSONGetString(x.Phone.get(0),"number"),
    JSONGetString(x.Phone.get(1),"number"),

    /*Others*/
    JSONGetString(x.PODetails,"Special Instructions"),
    JSONGetboolean(x.PODetails,"AllowPartialShipment"),

    /* Phone nos using iterator*/
    JSONGetString(pho,"type"),
    JSONGetString(pho,"number"),

    /*Item Specific one */
    JSONGetInteger(Items,"ItemNumber"),
    JSONGetString(Items.get('Part'),"Description"),
    JSONGetDouble(Items.get('Part'),"UnitPrice"),
    JSONGetDouble(Items.get('Part'),"UPCCode"),
    JSONGetInteger(Items,"Quantity"),

    /*Barcode specific values */
    JSONGetString(Barcode,"type"),
    JSONGetString(Barcode,"number")

from @APPNAME@_str1 x, iterator(x.LineItems) Items, iterator(x.Phone) pho, iterator(Items.Barcodes) Barcode;

create Target @APPNAME@_JSONNew_T using FileWriter (
filename:'@APPNAME@_JSONNew_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j1)
input from @APPNAME@_ss2;

create Target @APPNAME@_JSONSet1_T using FileWriter (
filename:'@APPNAME@_JSONSet1_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j2)
input from @APPNAME@_ss3;

create Target @APPNAME@_JSONFrom_T using FileWriter (
filename:'@APPNAME@_JSONFrom_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j3)
input from @APPNAME@_ss4;

create Target @APPNAME@_JSONSet2_T using FileWriter (
filename:'@APPNAME@_JSONSet2_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j4)
input from @APPNAME@_ss5;

create Target @APPNAME@_JSONArrayAdd_T using FileWriter (
filename:'@APPNAME@_JSONArrayAdd_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j5)
input from @APPNAME@_ss6;

create Target @APPNAME@_JSONArrayInsert_T using FileWriter (
filename:'@APPNAME@_JSONArrayInsert_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j6)
input from @APPNAME@_ss7;

create Target @APPNAME@_JSONRemove_T using FileWriter (
filename:'@APPNAME@_JSONRemove_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j7)
input from @APPNAME@_ss8;

create Target @APPNAME@_JSONSetAll1_T using FileWriter (
filename:'@APPNAME@_JSONSetAll1_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j8)
input from @APPNAME@_ss9;

create Target @APPNAME@_JSONSetAll2_T using FileWriter (
filename:'@APPNAME@_JSONSetAll2_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j9)
input from @APPNAME@_ss10;

create Target @APPNAME@_JSONSetAll3_T using FileWriter (
filename:'@APPNAME@_JSONSetAll3_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j10)
input from @APPNAME@_ss11;

create Target @APPNAME@_JSONSetAll4_T using FileWriter (
filename:'@APPNAME@_JSONSetAll4_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j11)
input from @APPNAME@_ss12;

create Target @APPNAME@_PO_T using FileWriter (
filename:'@APPNAME@_PO_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000'
)
format using dsvFormatter()
input from @APPNAME@_str2;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

create or replace type @STREAM@details(
ID INT,
name String,
company String);

create or replace stream @STREAM@_TYPED of @STREAM@details PARTITION BY name;

Create or replace CQ @STREAM@detailsCQ
insert into @STREAM@_TYPED
select 
to_int(data[0]),data[1],data[2]
from @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING FileWriter  ( 
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
 ) Format using DSVFormatter()
INPUT FROM @STREAM@_TYPED;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.test01',
  connectionRetryPolicy:'retryInterval=1,maxRetries=3'
 ) 
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'public.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true	
) INPUT FROM @APPNAME@_ss;
--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;
Create Source OracleSource Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;


create target AzureTarget using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

create Target t2 using SysOut(name:Foo2) input from str;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE FLOW @APP_NAME@_SrcFlow;

CREATE OR REPLACE SOURCE @APP_NAME@_src USING FileReader (
directory:'',
WildCard:''
)
parse using DSVParser (
header:'no'
)
OUTPUT TO @APP_NAME@_Stream;
END FLOW @APP_NAME@_SrcFlow;

CREATE FLOW @APP_NAME@_TgtFlow;

CREATE OR REPLACE TYPE @APP_NAME@_Type  ( BUSINESS_NAME java.lang.String KEY,
MERCHANT_ID java.lang.String,
PRIMARY_ACCOUNT_NUMBER java.lang.String
 ) ;

CREATE OR REPLACE STREAM @APP_NAME@_Stream2 OF @APP_NAME@_Type;
CREATE OR REPLACE CQ @APP_NAME@_CQ
INSERT INTO @APP_NAME@_Stream2
SELECT data[0],data[1],data[2]
FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.FabricDataWarehouseWriter (
  Tables: '',
  ConnectionURL: '@CONN_URL@',
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  uploadpolicy: 'eventcount:1',
  AccountName: '@ACCOUNTNAME@')
INPUT FROM @APP_NAME@_Stream2;

END FLOW @APP_NAME@_TgtFlow;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@ WITH @APP_NAME@_SrcFlow IN agents,@APP_NAME@_TgtFlow IN default;
START APPLICATION @APP_NAME@;

STOP application AlterTester.DSV;
undeploy application AlterTester.DSV;
drop application AlterTester.DSV cascade;


create application DSV;

create flow myFlowDSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

end flow myFlowDSV;
end application DSV;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;

drop propertyset MysqlToMysqlPlatfm_App_KafkaPropset;
drop stream  MysqlToMysqlPlatfm_Stream CASCADE;


--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;
					
CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using MySQLReader(
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true',
  Tables: '$table1'
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using MySQLReader( 
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true',
  Tables: '@TABLENAME@2'
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'WACTION.MYSQLTOMYSQLPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

END APPLICATION @APPNAME@1;

CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using MySQLReader( 
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true',
  Tables: '$table2'
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using MySQLReader(
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true',
  Tables: '@TABLENAME@4',
  
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'WACTION.MYSQLTOMYSQLPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@;

END APPLICATION @APPNAME@2;

--
-- Kafka Stream with KryoParser Kafka Reader without Recovery Test 2 
-- Bert Hashemi and Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> KS -> WS
-- S -> K -> CQ -> WS

STOP KStreamKryoParserTester2WOR.KStreamKryoParserTest2WOR;
UNDEPLOY APPLICATION KStreamKryoParserTester2WOR.KStreamKryoParserTest2WOR;
DROP APPLICATION KStreamKryoParserTester2WOR.KStreamKryoParserTest2WOR CASCADE;
DROP USER KStreamKryoParserTester2WOR;
DROP NAMESPACE KStreamKryoParserTester2WOR CASCADE;
CREATE USER KStreamKryoParserTester2WOR IDENTIFIED BY KStreamKryoParserTester2WOR;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamKryoParserTester2WOR;
CONNECT KStreamKryoParserTester2WOR KStreamKryoParserTester2WOR;

-- CREATE APPLICATION KStreamKryoParserTest2WOR RECOVERY 5 SECOND INTERVAL;
CREATE APPLICATION KStreamKryoParserTest2WOR;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'1');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE or REPLACE TYPE KafkaCsvStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF KafkaCsvStreamType 
EVENT TYPES ( KafkaCsvStreamType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE or REPLACE TYPE KafkaStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

--CREATE STREAM KafkaStream OF KafkaStreamType;
CREATE STREAM KafkaStream OF Global.waevent;

CREATE SOURCE KafkaSource USING KafkaReader
(
        brokerAddress:'localhost:9092',
        Topic:'KStreamKryoParserTester2WOR_KafkaCsvStream',
        PartitionIDList:'0',
        startOffset:0
)
PARSE USING KryoParser ()
OUTPUT TO KafkaStream;

CREATE WACTIONSTORE KRWactions CONTEXT OF KafkaStreamType
EVENT TYPES ( KafkaStreamType )
@PERSIST-TYPE@

CREATE CQ KRInsertWactions
INSERT INTO KRWactions
SELECT TO_STRING(data[1]) as merchantId,
    TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
    TO_DOUBLE(data[7]) as amount,
    TO_STRING(data[10]) as city 
FROM KafkaStream;

END APPLICATION KStreamKryoParserTest2WOR;

Create Target @TARGET_NAME@ using HiveWriter
(
  ConnectionURL:'jdbc:hive2://dockerhost:10000/default',
  Username:'cloudera',
  Password:'cloudera',
  hadoopurl:'hdfs://dockerhost:9000',
  Mode:'initialload',
  mergepolicy:'eventcount:5,interval:1s',
  Tables:'QATEST.HIVE_EMP,default.hive_emp KEYCOLUMNS(id)',
  hadoopConfigurationPath:'/home/ubuntu/Product/IntegrationTests/TestData/hdfsconf/'
)
INPUT FROM @STREAM@;

stop application app2PS;
undeploy application app2PS;
drop application app2PS cascade;

create application app2PS;

create target File_TargerPS2 using FileWriter
(
directory : '',
filename : ''
)
format using DSVFormatter()
input from Recoveryss2;

end application app2PS;

deploy application app2PS;
start application app2PS;

CREATE OR REPLACE APPLICATION OtelMonitoringApp;

CREATE OR REPLACE TYPE MonitorBatchEvent_Type (
items java.util.ArrayList
);
CREATE OR REPLACE STREAM MonitorBatchStream OF MonitorBatchEvent_Type;

CREATE OR REPLACE TYPE MonitorEvent_Type (
item com.webaction.runtime.monitor.MonitorEvent
);
CREATE OR REPLACE STREAM MonitorEventStream OF MonitorEvent_Type;


CREATE OR REPLACE CQ MonitorBatchCQ
INSERT INTO MonitorBatchStream
select filterMonEventsForOtelWriter(ms,'INPUT','MEMORY_USED_PERCENT','DISK_FREE','CPU_PER_CORE_PCT','LAG_END2END', 'STATUS_CHANGE') from global.MonitoringSourceStream ms;

CREATE CQ MonitorEventCQ
INSERT INTO MonitorEventStream
SELECT ri FROM MonitorBatchStream m, ITERATOR(m.items) ri;

CREATE OR REPLACE Target OTelWriter1 USING StriimOTelWriter INPUT FROM MonitorEventStream;

END APPLICATION OtelMonitoringApp;
deploy application OtelMonitoringApp;
start OtelMonitoringApp;

CREATE APPLICATION @AppName@;

CREATE OR REPLACE SOURCE Filereader_Src USING FileReader  (
   WildCard: 'posdata100.csv',
  directory: '@SrcDir@',
  positionbyeof: false)
 PARSE USING DSVParser  (
 )
OUTPUT TO CsvStream ;

CREATE OR REPLACE SOURCE initialLoad_Src USING Global.DatabaseReader (
  QuiesceOnILCompletion: false,
  Tables: '@SrcTableName@',
  adapterName: 'DatabaseReader',
  Password: '@Password@',
  Username: '@UserName@',
  ConnectionURL: '@Srcurl@',
   FetchSize: 10000)
OUTPUT TO ILStream;

Create Type CSVType (
  companyid String,
  merchantId String
);

CREATE STREAM CommonTypedStream OF CSVType;


CREATE OR REPLACE  CQ CsvToPosData
INSERT INTO CommonTypedStream
SELECT
TO_STRING(data[0]).replaceAll("COMPANY ", ""),
data[1]
FROM CsvStream;

CREATE CQ cq1
INSERT INTO CommonTypedStream
SELECT data[0],data[1]
FROM ILStream;


CREATE OR REPLACE TARGET Postgres_Trg USING Global.DatabaseWriter (
  ConnectionURL: '@trgUrl@',
  Username: '@trgUsrName@',
  Tables: '@trgTable@',
  Password: '@trgPswd@',
  CommitPolicy: 'EventCount:10000,Interval:60',
  adapterName: 'DatabaseWriter' )
INPUT FROM CommonTypedStream;

CREATE TARGET filewriter_tgt USING Global.FileWriter (
 directory:'@trgDir@',
  filename: '@fileName@',
  flushpolicy: 'EventCount:10000,Interval:30s',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n' )
INPUT FROM CommonTypedStream;

CREATE OR REPLACE TARGET BigQuery_Target USING Global.BigQueryWriter (
  streamingUpload: 'false',
  projectId: '@projectID@',
  Tables: '@BQTableName@',
  optimizedMerge: 'false',
  ServiceAccountKey: '@ServiceAccountKey@',
  BatchPolicy: 'EventCount:1000000,Interval:90',
  Mode: 'APPENDONLY' )
INPUT from CommonTypedStream;

END APPLICATION @AppName@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING SnowflakeWriter (
  ConnectionURL: '@SNOWFLAKE-URL@',
  username: '@SNOWFLAKE-USERNAME@',
  password: '@SNOWFLAKE-PASSWORD@',
  appendOnly:'false',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  Tables: '@TARGET-TABLES@',
  uploadpolicy:'eventcount:10,interval:5s'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING SnowflakeWriter (
  ConnectionURL: '@SNOWFLAKE-URL@',
  username: '@SNOWFLAKE-USERNAME@',
  password: '@SNOWFLAKE-PASSWORD@',
  appendOnly:'false',
  BatchPolicy: 'Interval:10',
  CommitPolicy: 'Interval:10',
  Tables: '@TARGET-TABLES@',
  uploadpolicy:'eventcount:10,interval:5s'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING SnowflakeWriter (
  ConnectionURL: '@SNOWFLAKE-URL@',
  username: '@SNOWFLAKE-USERNAME@',
  password: '@SNOWFLAKE-PASSWORD@',
  appendOnly:'false',
  BatchPolicy: 'eventCount:100000,Interval:20',
  CommitPolicy: 'eventCount:100000,Interval:20',
  Tables: '@TARGET-TABLES@',
  uploadpolicy:'eventcount:10,interval:5s'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING SnowflakeWriter (
  ConnectionURL: '@SNOWFLAKE-URL@',
  username: '@SNOWFLAKE-USERNAME@',
  password: '@SNOWFLAKE-PASSWORD@',
  appendOnly:'false',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  Tables: '@TARGET-TABLES@',
  uploadpolicy:'eventcount:10,interval:5s'
 )
INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)
INPUT FROM @STREAM@;;

create Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR;

Create Source s1 Using IncrementalBatchReader (
 FetchSize: 1,
  Username: 'striim',
  Password: 'o4l1uMpwIDQ=',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.autotest01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.autotest01=id',
  startPosition: '%=0'
 )
OUTPUT TO data_stream1;

create source s2 using IncrementalBatchReader (
FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.autotest02',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.autotest02=id',
  startPosition: '%=0'
 )
OUTPUT TO data_stream2;

create source s3 using IncrementalBatchReader (
FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.autotest03',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.autotest03=id',
  startPosition: '%=0'
 )
OUTPUT TO data_stream3;

Create Type EventType (
ID int,
PIN int
);

CREATE STREAM insertData1  of EventType;
CREATE STREAM deleteData1 of EventType;
CREATE STREAM joinData1 of EventType;
CREATE STREAM joinData2 of EventType;
CREATE STREAM deleteData2 of EventType;
CREATE STREAM OutStream of EventType;

CREATE CQ cq1 INSERT INTO insertData1  SELECT TO_INT(data[0]),TO_INT(data[1]) FROM data_stream1;

CREATE CQ cq2 INSERT INTO deleteData1 SELECT TO_INT(data[0]),TO_INT(data[1]) FROM data_stream2;

CREATE CQ cq3 INSERT INTO joinData1 SELECT TO_INT(data[0]),TO_INT(data[1]) FROM data_stream3;

CREATE JUMPING WINDOW DataWin1 OVER deleteData1 KEEP 1 ROWS;

CREATE CQ TypedEventKeyCQ6 INSERT INTO deleteData2 SELECT * from DataWin1;

CREATE JUMPING WINDOW DataWin2 OVER joinData1 KEEP 1 ROWS;

CREATE CQ TypedEventKeyCQ5 INSERT INTO joinData2 SELECT * from DataWin2;

CREATE EVENTTABLE ETABLE1 using STREAM ( NAME: 'insertData1 ' )
DELETE using STREAM ( NAME: 'deleteData1')
QUERY (keytomap:"ID", persistPolicy: 'true') OF EventType;

CREATE CQ cq4 INSERT INTO OutStream SELECT B.ID,B.PIN FROM joinData2 A, ETABLE1 B where A.ID=B.ID;

CREATE TARGET EventTableFW USING FileWriter
(filename:'BasicIR_RT.log',
 rolloverpolicy: 'EventCount:1000000')
FORMAT USING DSVFormatter () INPUT FROM OutStream;

create target Target_Azure using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'STRIIM',
        password: 'W3b@ct10n',
        AccountName: 'striimqatestdonotdelete',
        accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables:'dbo.autotest01',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM OutStream;

END APPLICATION IR;
deploy application IR in default;
start IR;

stop Quiesce_CDC;
undeploy application Quiesce_CDC;
alter application Quiesce_CDC;
CREATE or replace FLOW Quiesce_CDC_flow;
Create or replace Source Quiesce_CDC_Oraclesrc Using oraclereader(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',
 Tables:'QATEST.QUIESCE_TABLE1',
 _h_fetchexactrowcount: 'true'
)
Output To Quiesce_CDC_OrcStrm;
END FLOW Quiesce_CDC_flow;
alter application Quiesce_CDC recompile;
DEPLOY APPLICATION Quiesce_CDC;
start application Quiesce_CDC;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu RECOVERY 5 SECOND INTERVAL;
Create Source oracSource
 Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;
CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
ConnectionRetryPolicy: 'retryInterval=40,maxRetries=7',
batchpolicy: 'EventCount:20,Interval:60')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

STOP APPLICATION EH;
UNDEPLOY APPLICATION EH;
DROP APPLICATION EH CASCADE;
CREATE APPLICATION EH @Recovery@;
CREATE SOURCE s USING FileReader (
	directory:'Product/IntegrationTests/TestData/',
    WildCard:'posdata.csv',
	positionByEOF:false
	)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO ss;

CREATE TYPE userType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String
);

CREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;

CREATE CQ cq1
INSERT INTO userDefinedTypedStream
SELECT data[0],data[1],data[2], data[3],data[4]
FROM ss;

create Target t1 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	--Partitionkey:'@metadata(RecordOffset)',
	ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
jsonMemberDelimiter: '\n',
jsonobjectdelimiter: '\n',
EventsAsArrayOfJsonObjects: 'true')
input from ss;

create Target t2 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	--Partitionkey:'Col1',
	ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
jsonMemberDelimiter: '\n',
jsonobjectdelimiter: '\n',
EventsAsArrayOfJsonObjects: 'true')
input from userDefinedTypedStream;

END APPLICATION EH;
DEPLOY APPLICATION EH;
start application EH;

STOP APPLICATION ER;
UNDEPLOY APPLICATION ER;
DROP APPLICATION ER CASCADE;
CREATE APPLICATION ER;
CREATE SOURCE ER_S1 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING jsonParser (
)OUTPUT TO ER_SS1;


CREATE SOURCE ER_S2 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING jsonParser (
)OUTPUT TO ER_SS2;

create Type CustType1 
(writerdata com.fasterxml.jackson.databind.JsonNode
--TopicName java.lang.String,
--PartitionID java.lang.String
);

Create Stream datastream1 of CustType1;

create Type CustType2
(Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String
--TopicName String,
--PartitionID String
);

Create Stream datastream2 of CustType2;

CREATE CQ CustCQ1
INSERT INTO datastream1
SELECT data.data
--metadata.get("TopicName").toString() AS TopicName,
--metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS1;

CREATE CQ CustCQ2
INSERT INTO datastream2
SELECT 
data.get('Col1').textValue(),
data.get('Col2').textValue(),
data.get('Col3').textValue(),
data.get('Col4').textValue(),
data.get('Col5').textValue()
--metadata.get("TopicName").toString() AS TopicName,
--metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS2;

create Target ER_t1 using FileWriter (
filename:'FT1_5L_JSON_RT',
directory:'FEATURE-DIR/logs/',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from datastream1;

create Target ER_t2 using FileWriter (
filename:'FT2_5L_JSON_RT',
directory:'FEATURE-DIR/logs/',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from datastream2;
end application ER;
deploy application ER;

create or replace type @STREAM@orderBill(
id int,
name String,
cost float,
TableName string,
operationName String
);

create or replace stream @STREAM@_TYPED of @STREAM@OrderBill;

Create or replace CQ @STREAM@orderbillCQ
insert into @STREAM@_TYPED
select 
to_int(data[0]),data[1],to_float(data[2]),
meta(@STREAM@,'TableName'),Meta(@STREAM@,'OperationName') from @STREAM@;


create or replace Target @TARGET_NAME@ using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @STREAM@_TYPED;

STOP Istreamer.ISAPP;
UNDEPLOY APPLICATION Istreamer.ISAPP;
DROP APPLICATION Istreamer.ISAPP CASCADE;

CREATE APPLICATION ISAPP;


CREATE source implicitSource USING FileReader (
      directory:'@TEST-DATA-PATH@',
      columndelimiter: ',',
      wildcard:'ISdata.csv',
      blocksize: 10240,
      positionByEOF:false
)
PARSE USING DSVParser (
      header:False,
      trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity double,
  size long,
  currentDate DateTime);

CREATE CACHE cache1 USING CsvReader(
  directory: '@TEST-DATA-PATH@',
  wildcard: 'ISdata.csv',
  header: false,
  columndelimiter: ',',
  trimquote: false
  ) QUERY (keytomap:'productID') OF Atm;


CREATE STREAM newStream OF Atm;


CREATE CQ newCQ
INSERT INTO newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]), TO_DATE(data[5]) FROM
CsvStream;

CREATE WINDOW win1
OVER newStream
KEEP 50 rows;


CREATE CQ newCQ2
INSERT INTO newStream2
SELECT productID as A , stateID AS B, productWeight AS C, quantity AS D, size AS E, currentDate AS F FROM
newStream;


CREATE CQ newCQ3
INSERT INTO newStream3 PARTITION BY A
SELECT A,B,C,D,E,F FROM newStream2
link source event;

CREATE CQ newCQ4
INSERT INTO newStream4
SELECT count(productID),currentDate FROM newStream ORDER BY currentDate
link source event;

CREATE CQ newCQ5
INSERT INTO newStream5
SELECT x.*, y.* from cache1 x, newStream y WHERE x.productweight > 6 ORDER BY x.currentDate, x.productID;


CREATE WACTIONSTORE WS1 CONTEXT OF Atm
EVENT TYPES(Atm );

CREATE CQ newCQ6
INSERT INTO WS1
SELECT * FROM newStream WHERE productID = '001';

CREATE CQ newCQ7
INSERT INTO newStream6
SELECT aa.productID FROM WS1 [push] aa, cache1 bb;

CREATE CQ newCQ8
INSERT INTO newStream7
SELECT Sum(X.size) FROM (Select size from win1 where productweight > 5) X;

CREATE CQ newCQ9
INSERT INTO newStream8
SELECT count(productID) FROM WS1 [push] ORDER BY productID;


END APPLICATION ISAPP;
deploy APPLICATION ISAPP;

STOP APPLICATION MysqltoBQ;
UNDEPLOY APPLICATION MysqltoBQ;
DROP APPLICATION MysqltoBQ CASCADE;
CREATE APPLICATION MysqltoBQ recovery 5 SECOND Interval;
CREATE OR REPLACE SOURCE MysqltoBQ_Source USING MySQLReader 
(
  Username:'root',
  Password:'w@ct10n',
  connectionURL:'jdbc:mysql://localhost:3306/waction',
  Tables:'waction.sourceTable',
  sendBeforeImage:'true',
  FilterTransactionBoundaries:'true',
  ExcludedTables:'waction.CHKPOINT',
  useSSL:true
) 
OUTPUT TO MysqltoBQ_Stream;

CREATE OR REPLACE TARGET MysqltoBQ_Target USING BigQueryWriter  (
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  projectId:'striimqa-214712',
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:5,Interval:120',
  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', 
  AllowQuotedNewLines: 'false', 
  adapterName: 'BigQueryWriter', 
  optimizedMerge: 'true', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', 
  StandardSQL: 'true', 
  QuoteCharacter: '\"', 
  Tables: 'waction.sourceTable,.oratobqtgt',
  Mode: 'MERGE',
  StandardSQL: 'true',
  QuoteCharacter: '\"'
  ) INPUT FROM MysqltoBQ_Stream;

CREATE OR REPLACE TARGET MysqltoBQ_SysOut USING Global.SysOut (name: 'wa') INPUT FROM MysqltoBQ_Stream;

END APPLICATION MysqltoBQ;
DEPLOY APPLICATION MysqltoBQ;
START MysqltoBQ;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.OJet (
  PrimaryDatabasePassword: '@PRIMARY_PASSWORD@',
  ConnectionURL: '@DOWNSTREAM_URL@',
  PrimaryDatabaseUsername: '@PRIMARY_USER@',
  Password: '@DOWNSTREAM_PASSWORD@',
  DownstreamCaptureMode: 'REAL_TIME',
  DownstreamCapture: true,
  PrimaryDatabaseConnectionURL: '@PRIMARY_URL@',
  Tables: '@SOURCE_TABLES@',
  Username: '@DOWNSTREAM_USER@'
  )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET_SYS@ USING Global.SysOut (
  name: 'Out' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter (
  ConnectionURL: '@TARGET_URL@',
  Username: '@TARGET_USER@',
  Password: '@TARGET_PASSWORD@',
  CheckPointTable: 'CHKPOINT',
  CommitPolicy: 'EventCount:1',
  Tables: '@TARGET_TABLES@',
  BatchPolicy: 'EventCount:1' )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

--
-- Recovery Test 6 with sliding window and partitioned feature
-- Nicholas Keene, Bert Hashemi WebAction, Inc.
--
-- S -> CQ -> SW(partitioned) -> CQ(no aggregate) -> WS
--

STOP Recov6Tester.RecovTest6;
UNDEPLOY APPLICATION Recov6Tester.RecovTest6;
DROP APPLICATION Recov6Tester.RecovTest6 CASCADE;
CREATE APPLICATION RecovTest6 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvData PARTITION BY merchantId;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

CREATE WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION RecovTest6;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@ recovery 5 SECOND Interval;
CREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;


CREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:30s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

STOP APPLICATION TQLwithinTqlTester.TQLwithinTqlApp;
UNDEPLOY APPLICATION TQLwithinTqlTester.TQLwithinTqlApp;
DROP APPLICATION TQLwithinTqlTester.TQLwithinTqlApp CASCADE;

CREATE APPLICATION TQLwithinTqlApp;

@@FEATURE-DIR@/tql/TQLwithinTQL2.tql;
@@FEATURE-DIR@/tql/TQLwithinTQL4.tql;

END APPLICATION TQLwithinTqlApp;

CREATE FLOW @STREAM@_SourceFlow;

CREATE SOURCE @SOURCE_NAME@ USING MSSqlReader (
 Username:'@READER-UNAME@',
 Password:'@READER-PASSWORD@',
 DatabaseName:'@DB-NAME@',
 ConnectionURL:'@CDC-READER-URL@',
 Tables:@WATABLES@,
 ConnectionPoolSize:2,
 Compression:false,
 StartPosition:'EOF'
) OUTPUT TO @STREAM@;

END FLOW @STREAM@_SourceFlow;

stop application MSSQLTransactionSupportCompression;
undeploy application MSSQLTransactionSupportCompression;
drop application MSSQLTransactionSupportCompression cascade;

CREATE APPLICATION MSSQLTransactionSupportCompression recovery 1 second interval;

Create Source ReadFromMSSQL7
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
FetchTransactionMetadata:'true',
FilterTransactionBoundaries: true,
Compression:'true',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportCompressionStream;


CREATE TARGET WriteToMySQL7 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC@,@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportCompressionStream;

CREATE TARGET MSSqlReaderOutput7 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportCompressionStream; 


CREATE OR REPLACE TARGET MSSQLFileOut7 USING FileWriter  ( 
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  directory:'@FEATURE-DIR@/logs/',
  filename: 'TransactionSupportMSSQLToMySQLCompressionOn.txt'
 ) 
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 ) 
INPUT FROM MSSQLTransactionSupportCompressionStream;

END APPLICATION MSSQLTransactionSupportCompression;
deploy application MSSQLTransactionSupportCompression;
start application MSSQLTransactionSupportCompression;

CREATE FLOW ServerFlow;

CREATE TARGET @TARGET_NAME@_sysout USING Global.SysOut (
  name: '@TARGET_NAME@_SysOut' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1'
 )
INPUT FROM @STREAM@;
END FLOW ServerFlow;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 SupportPDB: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP Mssqltobigquery;
UNDEPLOY APPLICATION Mssqltobigquery;
DROP APPLICATION Mssqltobigquery CASCADE;

CREATE APPLICATION Mssqltobigquery RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE Mssqltobigquery_source USING MSSqlReader
(
Username:'qatest',
Password:'w3b@ct10n',
DatabaseName:'qatest',
ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',
Tables:'qatest.srctb',
ConnectionPoolSize:1,
Compression:'true'
)
OUTPUT TO SS;


CREATE or replace TARGET Mssqltobigquery_Target USING BigQueryWriter (
ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  projectId:'striimqa-214712',
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:5,Interval:120',
  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', 
  AllowQuotedNewLines: 'false', 
  adapterName: 'BigQueryWriter', 
  optimizedMerge: 'true', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', 
  StandardSQL: 'true', 
  QuoteCharacter: '\"', 
  Tables: 'QATEST.srctb,.oratobqtgt',
  Mode: 'MERGE',
  StandardSQL: 'true',
  QuoteCharacter: '\"'
) INPUT FROM SS;

END APPLICATION Mssqltobigquery;
DEPLOY APPLICATION Mssqltobigquery;
START APPLICATION Mssqltobigquery;

--
-- Canon Test W10
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for an unpartitioned sliding count window
--
-- S -> SWc5u -> CQ -> WS
--


UNDEPLOY APPLICATION NameW10.W10;
DROP APPLICATION NameW10.W10 CASCADE;
CREATE APPLICATION W10 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW10;

CREATE SOURCE CsvSourceW10 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW10;

END FLOW DataAcquisitionW10;


CREATE FLOW DataProcessingW10;

CREATE TYPE DataTypeW10 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW10 OF DataTypeW10;

CREATE CQ CSVStreamW10_to_DataStreamW10
INSERT INTO DataStreamW10
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW10;

CREATE WINDOW SWc5uW10
OVER DataStreamW10
KEEP 5 ROWS;

CREATE WACTIONSTORE WactionStoreW10 CONTEXT OF DataTypeW10
EVENT TYPES ( DataTypeW10 KEY(word) )
@PERSIST-TYPE@

CREATE CQ SWc5uW10_to_WactionStoreW10
INSERT INTO WactionStoreW10
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM SWc5uW10;

END FLOW DataProcessingW10;



END APPLICATION W10;

--
-- Recovery Test 36 with two sources, two jumping attribute windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W/p -> CQ1 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS
--

STOP Recov36Tester.RecovTest36;
UNDEPLOY APPLICATION Recov36Tester.RecovTest36;
DROP APPLICATION Recov36Tester.RecovTest36 CASCADE;
CREATE APPLICATION RecovTest36 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION RecovTest36;

drop namespace stripe cascade force;
create namespace stripe;
use stripe;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ RECOVERY 30 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;

CREATE FLOW @AppName@_SourceFlow;

CREATE SOURCE @srcName@ USING StripeReader ( 
 PollingInterval: '5m', 
  AccountId: '', 
  ApiKey: '@srcurl@', 
  StartPosition: '%=-1', 
  ThreadPoolCount: '10', 
  ConnectionPoolSize: '20', 
  RefreshToken: '', 
  useConnectionProfile: false,
  Mode: 'Automated', 
  IncrementalLoadMarker: 'Created', 
  ApiKey_encrypted: 'false', 
  ConnectedAccount: 'false', 
  ClientSecret: '', 
  ClientId: '', 
  Tables: 'Charges', 
  AuthMode: 'ApiKey', 
  MigrateSchema: true ) 
  OUTPUT TO @outstreamname@;

END FLOW @AppName@_SourceFlow;

CREATE TARGET Tgtstripebigquerysanity USING Global.BigQueryWriter ( 
  projectId: '@projectId@',
  batchPolicy: 'eventcount:10000,interval:2', 
  streamingUpload: 'true', 
  Mode: 'MERGE', 
  CDDLOptions: '{\"CreateTable\":{\"action\":\"IgnoreIfExists\",\"options\":[{\"CreateSchema\":{\"action\":\"IgnoreIfExists\"}}]}}', 
  ServiceAccountKey: 'UploadedFiles/google-gcs-test.json', 
  Tables: '%,rishi.%' ) 
INPUT FROM @outstreamname@;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

-- Wactionstore has been moved to DSWaction.tql

CREATE APPLICATION MyPosApp;

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


CREATE TYPE PosData(
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);
CREATE STREAM PosDataStream OF PosData PARTITION BY merchantId;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue int,
  hourlyAve int
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;


CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       p.zip,
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;


CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE CACHE NameLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false
)
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;

END APPLICATION MyPosApp;
deploy application MyPosApp;
start application MyPosApp;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING AvroParser ()
OUTPUT TO @APPNAME@_Stream;

CREATE CQ @APPNAME@_CQ
INSERT INTO @APPNAME@_CQOut
SELECT AvroToJson(data,false) FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_CQOut;

END APPLICATION @APPNAME@;

CREATE TARGET @TARGET_NAME@ USING RedshiftWriter
	(
	  ConnectionURL: '@CONNECTION_URL@',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  accesskeyId: 'access_key',
	  secretaccesskey: 'secret_access',
	  Tables: 'tgt_table',
	  uploadpolicy:'eventcount:10,interval:1m'
	) INPUT FROM @STREAM@;

create application KinesisTest;
create source CSVSource using FileReader (
	directory:'/home/dz/src/product/Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  companyName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0], data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM'
)
format using DSVFormatter (
)
input from TypedCSVStream;
end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

STOP rest2.applicationApi;
UNDEPLOY APPLICATION rest2.applicationApi;
DROP APPLICATION rest2.applicationApi cascade;

CREATE APPLICATION applicationApi;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)

PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;


CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startingTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
@PERSIST-TYPE@


CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false,
  trimwhitespace:true
) 
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'zip') OF USAddressData;


CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;
        
END APPLICATION applicationApi;

CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'data.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO @AppName@_rawstream;

CREATE CQ @BuiltinFunc@CQ
INSERT INTO @BuiltinFunc@Stream
SELECT updateUserData(x, 'Last_Date', data[5], 'Country', data[10])
FROM @AppName@_rawstream x;

CREATE OR REPLACE CQ cq1
INSERT INTO clearUserData_Stream
SELECT
clearUserData(s1)
FROM @BuiltinFunc@Stream s1;

CREATE OR REPLACE TARGET @AppName@_FileTarget USING Global.FileWriter ( 
  flushpolicy: 'EventCount:10000,Interval:30s', 
  adapterName: 'FileWriter', 
  directory: '@logs@',
  filename: '@BuiltinFunc@_ClearData', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING Global.JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter', 
  jsonMemberDelimiter: '\n', 
  EventsAsArrayOfJsonObjects: 'true', 
  formatterName: 'JSONFormatter', 
  jsonobjectdelimiter: '\n' ) 
INPUT FROM clearUserData_Stream;

End application @AppName@;
Deploy application @AppName@; 
Start application @AppName@;

stop application RedshiftColmap;
undeploy application RedshiftColmap;
drop application RedshiftColmap CASCADE;
create application RedshiftColmap recovery 1 second interval;

CREATE OR REPLACE SOURCE OracleSource USING OracleReader  (
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: '@SOURCE_TABLES@',
  FetchSize: 1
 ) Output To LogminerStream;
 
--create Target t2 using SysOut(name:Foo2) input from LogminerStream; 
 
CREATE TARGET RedshiftTarget USING RedshiftWriter
	(
	  ConnectionURL: '@TARGET-URL@',
	  Username: '@TARGET-UNAME@',
	  Password: '@TARGET-PASSWORD@',
	  bucketname: '@BUCKETNAME@',
	  --accesskeyId: '@ACCESS-KEY-ID@',
	  --secretaccesskey: '@SECRET-ACCESS-KEY@',
	  S3IAMRole:'@IAMROLE@',
	  Tables: '@TARGET-TABLES@',
	  uploadpolicy:'eventcount:5,interval:10s',
	  Mode:'incremental'
	) INPUT FROM LogminerStream;
	
END APPLICATION RedshiftColmap;
deploy application RedshiftColmap;
START application RedshiftColmap;

Stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 1 second interval;
CREATE TYPE @APPNAME@_Test_1_type
( id java.lang.Integer KEY , 
status java.lang.String  
 );

CREATE OR REPLACE CACHE @APPNAME@_Test_1 USING DatabaseReader ( 
  --ConnectionURL: 'jdbc:sqlserver://@MSSQLIP@:@MSSQLPORT@',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433',
  DatabaseProviderType: 'Default',
  FetchSize: 1,
  Password:'@Password@',
  Query: 'Select id,status from [QATEST].[QATEST].[Test_1]',
  Username: '@Username@'
 ) 
QUERY ( 
  keytomap: 'id',
  skipinvalid: 'false'
 ) 
 OF @APPNAME@_Test_1_type;

CREATE TYPE @APPNAME@_Test_3_type
( id java.lang.Integer KEY , 
status java.lang.String  
 );

CREATE TYPE @APPNAME@_Test_2_type
( id java.lang.Integer KEY , 
status java.lang.String  
 );

CREATE OR REPLACE CACHE @APPNAME@_Test_3 USING DatabaseReader ( 
  --ConnectionURL: 'jdbc:sqlserver://@MSSQLIP@:@MSSQLPORT@',
    ConnectionURL: 'jdbc:sqlserver://localhost:1433',
  DatabaseProviderType: 'Default',
  FetchSize: 1,
  Password:'@Password@',
  Query: 'Select id,status from [QATEST].[QATEST].[Test_3]',
  Username: '@Username@'
 ) 
QUERY ( 
  keytomap: 'id',
  skipinvalid: 'false'
 ) 
 OF @APPNAME@_Test_3_type;
 
 CREATE OR REPLACE CACHE @APPNAME@_Test_2 USING DatabaseReader ( 
  --ConnectionURL: 'jdbc:sqlserver://@MSSQLIP@:@MSSQLPORT@',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433',
  DatabaseProviderType: 'Default',
  FetchSize: 1,
  Password:'@Password@',
  Query: 'Select id,status from [QATEST].[QATEST].[Test_2]',
  Username: '@Username@'
 ) 
QUERY ( 
  keytomap: 'id',
  skipinvalid: 'false',
  refreshinterval: '5 SECOND'
 ) 
 OF @APPNAME@_Test_2_type;
 

CREATE OR REPLACE SOURCE @APPNAME@_s1 USING MSSqlReader  ( 
  Compression: true,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.Test_Master',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
) 
OUTPUT TO @APPNAME@_ss1;

CREATE OR REPLACE SOURCE @APPNAME@_s2 USING MSSqlReader  ( 
  Compression: true,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qates%.Test_%',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
) 
OUTPUT TO @APPNAME@_ss2;

CREATE TYPE @APPNAME@_ExtractedFields_Type  ( 
id java.lang.Long , 
name java.lang.String , 
city java.lang.String , 
Test_1_id java.lang.Long , 
Test_3_id java.lang.Long ,
Test_2_id java.lang.Long 
 );

CREATE OR REPLACE STREAM @APPNAME@_ExtractedFields OF @APPNAME@_ExtractedFields_Type;


CREATE OR REPLACE CQ @APPNAME@_ExtractedFields_cq
INSERT INTO @APPNAME@_ExtractedFields
SELECT 
   b.data[0] as id,
   b.data[1] as name,
   b.data[2] as city,
   b.data[3] as Test_1_ID,
   b.data[4] as Test_3_ID,
   b.data[5] as Test_2_ID
FROM
@APPNAME@_ss1 b LEFT OUTER JOIN @APPNAME@_Test_2 s ON TO_INT(b.data[0]) = s.id
 LEFT OUTER JOIN 
@APPNAME@_Test_1 p ON TO_INT(b.data[3]) = p.id
 LEFT OUTER JOIN
@APPNAME@_Test_3 r ON TO_INT(b.data[4]) = r.id
WHERE (TO_STRING(META(b, "OperationName")) = "UPDATE" or TO_STRING(META(b, "OperationName")) = "INSERT")
limit 1;

CREATE  TARGET @APPNAME@_t1_dsv USING AzureEventHubWriter  ( 
  EventHubNamespace: 'EventHubWriterTest',
  EventHubName: 'test_01',
  E1P: 'true',
  SASPolicyName: 'RootManageSharedAccessKey',
  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
  PartitionKey: 'id',
  ConsumerGroup: 'test_01_cg',
  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
  --ParallelThreads: '2'
 ) 
FORMAT USING DSVFormatter  (  ) 
INPUT FROM @APPNAME@_ExtractedFields;

CREATE  TARGET @APPNAME@_t2_json USING AzureEventHubWriter  ( 
  EventHubNamespace: 'EventHubWriterTest',
  EventHubName: 'test_02',
  E1P: 'true',
  SASPolicyName: 'RootManageSharedAccessKey',
  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
  PartitionKey: 'id',
  ConsumerGroup: 'test_02_cg',
  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
  --ParallelThreads: '2'
 ) 
FORMAT USING JSONFormatter  (  ) 
INPUT FROM @APPNAME@_ExtractedFields;
CREATE  TARGET @APPNAME@_t3_avro USING AzureEventHubWriter  ( 
  EventHubNamespace: 'EventHubWriterTest',
  EventHubName: 'test_03',
  E1P: 'true',
  SASPolicyName: 'RootManageSharedAccessKey',
  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
  PartitionKey: 'id',
  ConsumerGroup: 'test_03_cg',
  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
  --ParallelThreads: '2'
 ) 
FORMAT USING AvroFormatter  (   schemaFileName: 'kafkaAvroTest_multipleReader.avsc'
 ) 
INPUT FROM @APPNAME@_ExtractedFields;

CREATE  TARGET @APPNAME@_t1_dsv_rawstream USING AzureEventHubWriter  ( 
  EventHubNamespace: 'EventHubWriterTest',
  EventHubName: 'test_04',
  E1P: 'true',
  SASPolicyName: 'RootManageSharedAccessKey',
  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
  PartitionKey: '@metadata(TableName)',
  ConsumerGroup: 'test_04_cg',
  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
  --ParallelThreads: '2'
 ) 
FORMAT USING DSVFormatter  (  ) 
INPUT FROM @APPNAME@_ss2;

CREATE  TARGET @APPNAME@_t2_json_rawstream USING AzureEventHubWriter  ( 
  EventHubNamespace: 'EventHubWriterTest',
  EventHubName: 'test_05',
  E1P: 'true',
  SASPolicyName: 'RootManageSharedAccessKey',
  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
  PartitionKey: '@metadata(TableName)',
  ConsumerGroup: 'test_05_cg',
  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
  --ParallelThreads: '2'
 ) 
FORMAT USING JSONFormatter  (  ) 
INPUT FROM @APPNAME@_ss2;

CREATE  TARGET @APPNAME@_t3_avro_rawstream USING AzureEventHubWriter  ( 
  EventHubNamespace: 'EventHubWriterTest',
  EventHubName: 'test_06',
  E1P: 'true',
  SASPolicyName: 'RootManageSharedAccessKey',
  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
  PartitionKey: '@metadata(TableName)',
  ConsumerGroup: 'test_06_cg',
  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
  --ParallelThreads: '2'
 ) 
FORMAT USING AvroFormatter  (   schemaFileName: 'kafkaAvroTest_multipleReader.avsc'
 ) 
INPUT FROM @APPNAME@_ss2;

END APPLICATION @APPNAME@;
Deploy application @APPNAME@;
start @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@ RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE @parquetsrc@ USING FileReader (
wildcard: '',
directory: '',
positionbyeof: false )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: ''
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ recovery 5 second interval;
--create application @APPNAME@;

--create flow agentflow;
CREATE OR REPLACE SOURCE @APPNAME@_Src USING SpannerBatchReader  (
  DatabaseProviderType: 'Default',
  pollingInterval: '5ms',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  ConnectionURL: 'jdbc:cloudspanner:/projects/bigquerywritertest/instances/testspanner/databases/spannertestdb?credentials=/Users/jenniffer/Downloads/abc.json',
  Tables: 'Recovery_Timestam%',
  --_h_mode:'InitialLoad',
--  VendorConfiguration:'_h_SpannerReadStaleness=MAX_STALENESS 20s',
  adapterName: 'SpannerBatchReader',
    StartPosition: '%=0',
  CheckColumn: '%=id'
 )
OUTPUT TO @APPNAME@_Output_Stream;
--end flow agentflow;

CREATE TARGET @APPNAME@_tgt USING SpannerWriter (
	Tables: 'spannersource,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	BatchPolicy: 'EventCount: 1; Interval: 1s',
	instanceId: 'qatest'
) INPUT FROM @APPNAME@_Output_Stream;

create Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_Output_Stream;

end application @APPNAME@;
deploy application @APPNAME@;
--deploy application @APPNAME@ with agentflow in agents;
start application @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_scnRange: 1000,
 _h_eoffDelay: 10,
 SupportPDB: false,
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

--
-- Recovery Test 65
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ1 -> JWc10 -> CQ(aggregate) -> WS1
-- S -> CQ2 -> JWc11-> CQ(aggregate) -> WS2
--

STOP Recov65Tester.RecovTest65;
UNDEPLOY APPLICATION Recov65Tester.RecovTest65;
DROP APPLICATION Recov65Tester.RecovTest65 CASCADE;
CREATE APPLICATION RecovTest65 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStreamSize10
OVER DataStream KEEP 10 ROWS;

CREATE JUMPING WINDOW DataStreamSize11
OVER DataStream KEEP 11 ROWS;

CREATE WACTIONSTORE Wactions1 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions2 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions1
INSERT INTO Wactions1
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    TO_DATE(FIRST(p.dateTime)),
    TO_DOUBLE(FIRST(p.amount)),
    FIRST(p.city)
FROM DataStreamSize10 p;

CREATE CQ InsertWactions2
INSERT INTO Wactions2
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    TO_DATE(FIRST(p.dateTime)),
    TO_DOUBLE(FIRST(p.amount)),
    FIRST(p.city)
FROM DataStreamSize11 p;

create Target t1 using logwriter(name:Foo1, filename: output1) input from DataStreamSize10;
create Target t2 using logwriter(name:Foo2, filename: output2) input from DataStreamSize11;

END APPLICATION RecovTest65;

stop Quiesce_CDC_BQ_TARGET;
undeploy application Quiesce_CDC_BQ_TARGET;
alter application Quiesce_CDC_BQ_TARGET;
Create or replace TARGET Quiesce_CDC_BigQueryTrg USING BigQueryWriter (
  serviceAccountKey: '@SERVICEACCOUNTKEY@',
  projectId:'@PROJECTID@',
  BatchPolicy:'Interval:10',
  _h_maxParallelStreamingRequests: '10',
  Tables:'QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE1'
)
INPUT FROM Quiesce_CDC_OrcStrm;
alter application Quiesce_CDC_BQ_TARGET recompile;
DEPLOY APPLICATION Quiesce_CDC_BQ_TARGET;
start application Quiesce_CDC_BQ_TARGET;

stop @appname@;
undeploy application @appname@;
DROP APPLICATION @appname@ CASCADE;
CREATE APPLICATION @appname@;

CREATE SOURCE @appname@_src USING databaseReader  (
  Username: '@@',
  Password: '@@',
  ConnectionURL: '@@',
  Tables: '@@',
  FetchSize: '100'
 )
OUTPUT TO @appname@_ss;

--CREATE JUMPING WINDOW @appname@_win OVER @appname@_ss KEEP @winsize@ ROWS;

CREATE TYPE @appname@_MapType
    (   
       id INTEGER,
        name STRING,
        city  STRING
    );
    
CREATE EXTERNAL CACHE @appname@_cach (
  AdapterName: 'DatabaseReader',
    ConnectionURL: '@url@',
    UserName: '@uname@',
    Password: '@pwd@',
   Table: '@tablename@',
  FetchSize: 100,
  Columns: 'id,name,city',
  trimquote: false,
  KeyToMap: '@key@'
 )
 OF @appname@_MapType;
 
CREATE TYPE @appname@_MapTypenew
    (   id_t            INTEGER,
        name_t           STRING,
        city_t            STRING,
        id_c            INTEGER,
        name_c            STRING,
        city_c            STRING
    );
    
CREATE STREAM @appname@_JoinedData OF @appname@_MapTypenew;

CREATE CQ @appname@_JoinDataCQ
INSERT INTO @appname@_JoinedData
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_STRING(f.data[2]),
        z.id,
        z.name,
        z.city
FROM @appname@_ss f, @appname@_cach z
where TO_INT(f.data[0]) = z.id
@Ex@;

CREATE TARGET @appname@_tgt USING DatabaseWriter
(
  ConnectionURL:'@@',
  Username:'@@',
  Password:'@@',
  BatchPolicy:'Eventcount:10000,Interval:1',
  CommitPolicy:'Interval:1,Eventcount:10000',
  Tables:'@@'
) 
INPUT FROM @appname@_JoinedData;

END APPLICATION @appname@;
deploy application @appname@;
start @appname@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
wildcard: '',
directory: '',
positionbyeof: false )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: 'ParquetFileSchema'
)
INPUT FROM @appname@CqOut;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using ParquetFormatter (
schemaFileName: 'ParquetS3Schema'
)
input from @appname@CqOut;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using ParquetFormatter (
schemaFileName: 'ParquetAzureSchema'
)
INPUT FROM @appname@CqOut;

CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using ParquetFormatter (
schemaFileName: 'ParquetGCSSchema'
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadpolicy:'EventCount:7'
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from @STREAM@;

end flow @APPNAME@_serverflow;

end application @APPNAME@;

use consoletest;
alter application noApp;

CREATE source CsvDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'posdata.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;


end application noApp;

alter application noApp recompile;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.OJet (
  PrimaryDatabasePassword: '@PRIMARY_PASSWORD@',
  ConnectionURL: '@DOWNSTREAM_URL@',
  PrimaryDatabaseUsername: '@PRIMARY_USER@',
  Password: '@DOWNSTREAM_PASSWORD@',
  DownstreamCaptureMode: 'REAL_TIME',
  DownstreamCapture: true,
  PrimaryDatabaseConnectionURL: '@PRIMARY_URL@',
  Tables: '@SOURCE_TABLES@',
  CDDLCapture: true,
  CDDLAction: 'Process',
  Username: '@DOWNSTREAM_USER@' )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET_SYS@ USING Global.SysOut (
  name: 'Out' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter (
  ConnectionURL: '@TARGET_URL@',
  Username: '@TARGET_USER@',
  Password: '@TARGET_PASSWORD@',
  CheckPointTable: 'CHKPOINT',
  CommitPolicy: 'EventCount:1',
  Tables: '@TARGET_TABLES@',
  BatchPolicy: 'EventCount:1' )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

STOP twoNodeCacheRefresh.AppTest;
UNDEPLOY APPLICATION twoNodeCacheRefresh.AppTest;
DROP APPLICATION twoNodeCacheRefresh.AppTest cascade;

CREATE APPLICATION AppTest;

CREATE FLOW THESOURCEFLOW;
----------------------------------------------------

CREATE TYPE GenType
(
  GenID Integer KEY,
  GenCode double,
  GenState String,
  GenDate DateTime,
  GenLong Long
);


CREATE SOURCE GenSource USING StreamReader(
	OutputType: 'twoNodeCacheRefresh.GenType',
	noLimit: 'false',
	maxRows: 100,
	iterationDelay: 5,
	StringSet: 'GenState[CA-FL]',
	NumberSet: 'GenID[0-0]Linc,GenCode[99-99]Linc,GenLong[4999999-4999999]G'
)OUTPUT TO rawGenStream;


END FLOW THESOURCEFLOW;
----------------------------------------------------

CREATE FLOW MAINPROCESSFLOW;

CREATE TYPE theGenType
(
  theGenID Integer KEY,
  theGenCode double,
  theGenState String,
  theGenDate DateTime,
  theGenLong Long
);

CREATE STREAM GenStream OF theGenType;

CREATE CQ GenCQ1
INSERT INTO GenStream
SELECT TO_INT(data[0]),
	   TO_DOUBLE(data[1]),
	   data[2],
	   TO_DATE(data[3]),
	   TO_LONG(data[4])
FROM rawGenStream;


CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'addy.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false
) QUERY (keytomap:'zip', refreshinterval:'20 second', skipinvalid:'true') OF USAddressData;


CREATE TYPE MergedData
(
  country String,
  zip String,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String,
  theGenID Integer,
  theGenCode double,
  theGenState String,
  theGenDate DateTime,
  theGenLong Long
);

CREATE STREAM SanDiegoDataStream OF MergedData;
CREATE STREAM MiamiDataStream OF MergedData;

-- join the data
CREATE CQ SanDiegoCQData
INSERT INTO SanDiegoDataStream
SELECT z.country, z.zip, z.city, z.state, z.stateCode,
       z.fullCity, z.someNum, z.pad, z.latVal,
       z.longVal, z.empty, z.empty2, gs.theGenID,
       gs.theGenCode, gs.theGenState, gs.theGenDate, gs.theGenLong
       FROM GenStream gs, ZipLookup z where gs.theGenState = 'CA' AND z.zip LIKE '921%';

CREATE CQ MiamiCQData
INSERT INTO MiamiDataStream
SELECT z.country, z.zip, z.city, z.state, z.stateCode,
       z.fullCity, z.someNum, z.pad, z.latVal,
       z.longVal, z.empty, z.empty2, gs.theGenID,
       gs.theGenCode, gs.theGenState, gs.theGenDate, gs.theGenLong
       FROM GenStream gs, ZipLookup z where gs.theGenState = 'FL' AND z.zip LIKE '331%';


CREATE TYPE MiamiData
(
    zip String KEY,
    fullCity String,
    theGenCode double,
    theGenState String
);

CREATE WACTIONSTORE MiamiStore
CONTEXT OF MiamiData
EVENT TYPES(MergedData )
PERSIST NONE USING ( );


CREATE CQ MiamiStoreCQ
INSERT INTO MiamiStore
SELECT zip, fullCity, theGenCode, theGenState
FROM MiamiDataStream
LINK SOURCE EVENT;


CREATE TYPE SanDiegoData
(
    zip String KEY,
    fullCity String,
    theGenCode double,
    theGenState String
);

CREATE WACTIONSTORE SanDiegoStore
CONTEXT OF SanDiegoData
EVENT TYPES(MergedData )
PERSIST NONE USING ( );


CREATE CQ SanDiegoStoreCQ
INSERT INTO SanDiegoStore
SELECT zip, fullCity, theGenCode, theGenState
FROM SanDiegoDataStream
LINK SOURCE EVENT;


END FLOW MAINPROCESSFLOW;
----------------------------------------------------

END APPLICATION AppTest;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSV1Source using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'RFC4180.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'no'
)
OUTPUT TO Csv1Stream;

create Target t using FileWriter(
  filename:'FileWriterStandardRFC4180',
  directory:'@FEATURE-DIR@/logs/',
  standard : 'RFC4180',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (
members:'data'
)
input from Csv1Stream;

end application DSV;

DROP APPLICATION ns1.OPExample cascade;
DROP NAMESPACE ns1 cascade;
CREATE OR REPLACE NAMESPACE ns1;
USE ns1;
CREATE APPLICATION OPExample;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'PosDataPreview.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
)
OUTPUT TO CsvStream;
 
CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);

CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) 
QUERY (keytomap:'merchantId') 
OF MerchantHourlyAve;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
  TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
  DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
  TO_DOUBLE(data[7]) as amount,
  TO_INT(data[9]) as zip
FROM CsvStream;
 
CREATE CQ cq2
INSERT INTO SendToOPStream
SELECT makeList(dateTime) as dateTime,
  makeList(zip) as zip
FROM PosDataStream;
 
CREATE TYPE ReturnFromOPStream_Type ( time DateTime , val Integer );
CREATE STREAM ReturnFromOPStream OF ReturnFromOPStream_Type;

CREATE TARGET OPExampleTarget 
USING FileWriter (filename: 'OPExampleOut') 
FORMAT USING JSONFormatter() 
INPUT FROM ReturnFromOPStream;
 
END APPLICATION OPExample;

--
-- Recovery Test 33 with two sources, two sliding time windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> St1W/p -> CQ1 -> WS
-- S2 -> St2W/p -> CQ2 -> WS
--

STOP Recov33Tester.RecovTest33;
UNDEPLOY APPLICATION Recov33Tester.RecovTest33;
DROP APPLICATION Recov33Tester.RecovTest33 CASCADE;
CREATE APPLICATION RecovTest33 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 1 SECOND
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 2 SECOND
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION RecovTest33;

CREATE STREAM @STREAM@_JSON OF Global.JsonNodeEvent;

CREATE TARGET @TARGET_NAME@ USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
) INPUT FROM @STREAM@_JSON;

CREATE APPLICATION @APPNAME@ @RECOVERY@;

CREATE FLOW @APPNAME@AgentFlow;
CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream;
END FLOW @APPNAME@AgentFlow;

CREATE FLOW @APPNAME@serverFlow;
CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING DSVFormatter (
members:'data')
INPUT FROM @APPNAME@_Stream;
END FLOW @APPNAME@serverFlow;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@ with @APPNAME@AgentFlow in Agents, @APPNAME@ServerFlow in default;
start application @APPNAME@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.JsonNodeEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (
  ProviderName: '',
  Provider: '',
  Ctx: '',
  QueueName: '',
  Topic:'',
  UserName: '',
  Password: '',
  EnableTransaction: '',
  transactionpolicy: ''
 )
PARSE USING JSONParser ()
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING JSONFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream@RANDOM@;

END APPLICATION @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

stop application @appname@Out;
undeploy application @appname@Out;
drop application @appname@Out cascade;

drop stream @appname@KafkaStream;
CREATE OR REPLACE PROPERTYSET @appname@KafkaPropset (zk.address:@keeper@, bootstrap.brokers:@broker@, partitions:'50');
CREATE STREAM @appname@KafkaStream OF Global.parquetevent PERSIST USING @appname@KafkaPropset;
 
CREATE APPLICATION @appname@;

CREATE OR REPLACE SOURCE @parquetsrc@ USING Global.FileReader ( 
  directory: '', 
  wildcard: '',
  positionbyeof: false ) 
PARSE USING Global.ParquetParser () 
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@KafkaStream
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;
    
END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE APPLICATION @appname@Out;

CREATE OR REPLACE TARGET @filetarget@ USING Global.FileWriter ( 
  directory: '', 
  filename: '' 
)
FORMAT USING Global.ParquetFormatter  ( 
  schemaFileName: 'parquetSchema' 
) 
INPUT FROM @appname@KafkaStream;

END APPLICATION @appname@Out;
deploy application @appname@Out on all in default;
start application @appname@Out;

stop application ThreeAgentTester.CSV;
undeploy application ThreeAgentTester.CSV;
drop application ThreeAgentTester.CSV cascade;

create application CSV;

CREATE FLOW AgentFlowOne;
create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStreamOne;
END FLOW AgentFlowOne;

CREATE FLOW AgentFlowTwo;
create source CSVSourceOne using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStreamTwo;
END FLOW AgentFlowTwo;

CREATE FLOW AgentFlowThree;
create source CSVSourceTwo using CSVReader (
  Directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStreamThree;
END FLOW AgentFlowThree;

CREATE FLOW ServerFlow;

CREATE TARGET FileDumpOne Using FileWriter (
directory:'@FEATURE-DIR@/logs/',
filename:'SourceDumpFromStreamOne_%yyyy-MM-dd_HH_mm_ss_SSS%.csv',
rolloverpolicy:'eventcount:39'

)
format using DSVFormatter (
)
input from CsvStreamOne;
CREATE TYPE UserDataType
(
  UserId String KEY,
  UserName String
);

CREATE STREAM UserDataStream OF UserDataType PARTITION BY UserId;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  data[0],
        data[1]
FROM CsvStreamOne;

CREATE TARGET FileDumpTwo Using FileWriter (
directory:'@FEATURE-DIR@/logs/',
filename:'CQDumpFromUserDataStream_%yyyy-MM-dd_HH_mm_ss_SSS%.csv',
rolloverpolicy:'eventcount:39'

)
format using DSVFormatter (
)
input from UserDataStream;

CREATE WACTIONSTORE UserActivityInfo
CONTEXT OF UserDataType
EVENT TYPES ( UserDataType )
@PERSIST-TYPE@

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction
INSERT INTO UserActivityInfo
SELECT * FROM UserDataStream
LINK SOURCE EVENT;
END FLOW ServerFlow;

END APPLICATION CSV;
DEPLOY APPLICATION CSV with AgentFlowOne on all in AGENTS, AgentFlowTwo on all in AGENTS,AgentFlowThree on all in AGENTS,ServerFlow on any in default;
START CSV;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ recovery 5 second Interval;

CREATE SOURCE @srcName@ USING SalesForceReader ( 
  customObjects: false, 
  autoAuthTokenRenewal: 'true',
  pollingInterval: '1 min', 
  sObjects: '@srcobject@', 
  useConnectionProfile: 'false',
  consumerSecret: '@srcconsumersecret@',
  consumerKey: '@srcconsumerkey@', 
  Username: '@srcusername@',
  Password: '@srcpassword@',
  mode: 'Automated',
  connectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  securityToken: '@srcsecuritytoken@',
  apiEndPoint: '@srcapiurl@',
  MigrateSchema: true, 
  threadPoolSize: 5
)

OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@ USING Global.DeltaLakeWriter (
  personalAccessToken:'@tgtpassword@',
  hostname:'@tgthostname@',
  stageLocation:'/',
  Mode:'MERGE',
  AuthenticationType: 'PersonalAccessToken',
  Tables:'@srcschema@,@tgtschema@.@tgttable@ COLUMNMAP()',
  adapterName:'DeltaLakeWriter',
  personalAccessToken_encrypted:'false',
  optimizedMerge:'false',
  uploadPolicy:'eventcount:1,interval:10s',
  connectionUrl:'@tgturl@',
  IgnorableExceptionCode:'TABLE_NOT_FOUND',
  externalStageType:'DBFSROOT'
)
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ recovery 1 second interval;

CREATE OR REPLACE SOURCE @SOURCENAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)

OUTPUT TO @STREAM@ ;

CREATE TARGET @TARGETNAME@ using DatabaseWriter
(
    ConnectionURL: '@TARGETURL',
    username: '@TARGETUSERNAME@',
    Password: '@TARGETPASSWORD@',
    Tables: '@TARGETTABLE@',
    BatchPolicy:'EventCount:1,Interval:1',
    CommitPolicy:'EventCount:1,Interval:1'
)
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

stop application app1PS;
undeploy application app1PS;
drop application app1PS cascade;

create application app1PS;

create target File_TargerPS using FileWriter
(
directory : '',
filename : ''
)
format using DSVFormatter()
input from KPSRss1;

end application app1PS;

deploy application app1PS;
start application app1PS;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE OR REPLACE TYPE @appname@CQOUT1_Type (
 companyName java.lang.String,
 merchantId java.lang.String,
 dateTime org.joda.time.DateTime,
 hourValue java.lang.String,
 amount java.lang.String,
 zip java.lang.String,
 FileName java.lang.String);

CREATE SOURCE @parquetsrc@ USING FILEReader (
    wildcard: '',
    directory: '',
    positionbyeof: false
 )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE OR REPLACE STREAM @appname@CQOUT1 OF @appname@CQOUT1_Type;
CREATE OR REPLACE CQ @appname@CQ_PQEvent
INSERT INTO @appname@CQOUT1
    Select
    data.get("companyName").toString(),
    data.get("merchantId").toString(),
    TO_DATE(data.get("dateTime").toString()),
    data.get("hourValue").toString(),
    data.get("amount").toString(),
    data.get("zip").toString(),
    metadata.get("FileName").toString()
    FROM @appname@Stream p;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using JSONFormatter ()
input from @appname@CQOUT1;

CREATE OR REPLACE TARGET @dbtarget@ USING DatabaseWriter (
  Tables: '',
  ConnectionURL:'',
  Username:'',
  Password:'',
  CommitPolicy: 'EventCount:10,Interval:0',
  BatchPolicy:'EventCount:10,Interval:0'
)
INPUT FROM @appname@CQOUT1;

create Target @jsontarget@ using FileWriter(
filename: '',
directory: '',
flushpolicy: 'EventCount:10,Interval:30s',
rolloverpolicy: 'EventCount:10,Interval:30s' )
format using JSONFormatter ()
INPUT FROM @appname@CQOUT1;

create Target @xmltarget@ using FileWriter(
filename: '',
directory: '',
flushpolicy: 'EventCount:10,Interval:30s',
rolloverpolicy: 'EventCount:10,Interval:30s' )
format using XMLFormatter (
    rootelement:'',
    elementtuple:'',
    charset:'UTF-8'
)
INPUT FROM @appname@CQOUT1;

create Target @dsvtarget@ using FileWriter(
filename: '',
directory: '',
flushpolicy: 'EventCount:10,Interval:30s',
rolloverpolicy: 'EventCount:10,Interval:30s' )
format using DSVFormatter ()
INPUT FROM @appname@CQOUT1;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;


CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using PostgreSQLReader
(
   adapterName: PostgreSQLReader,
   CDDLAction: Quiesce_Cascade,
   CDDLCapture: true,
   CDDLTrackingTable:'striim.ddlcapturetable',
   ConnectionURL: jdbc:postgresql://localhost:5432/qatest,
   FilterTransactionBoundaries: true,
   Password: w@ct10n,
   ReplicationSlotName:'test_slot',
   Tables: public.PGMultiDownstream_src,
   Username: sa,
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;

CREATE APPLICATION @APPNAME3@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName1@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME3@;
DEPLOY APPLICATION @APPNAME3@;
START APPLICATION @APPNAME3@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE OR REPLACE TYPE @appname@CQOUT1_Type (
 companyName java.lang.String,
 merchantId java.lang.String,
 dateTime org.joda.time.DateTime,
 hourValue java.lang.String,
 amount java.lang.String,
 zip java.lang.String,
 FileName java.lang.String);

CREATE SOURCE @parquetsrc@ USING S3Reader (
    bucketname:'',
    objectnameprefix:''
     )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE OR REPLACE STREAM @appname@CQOUT1 OF @appname@CQOUT1_Type;
CREATE OR REPLACE CQ @appname@CQ_PQEvent
INSERT INTO @appname@CQOUT1
    Select
    data.get("companyName").toString(),
    data.get("merchantId").toString(),
    TO_DATE(data.get("dateTime").toString()),
    data.get("hourValue").toString(),
    data.get("amount").toString(),
    data.get("zip").toString(),
    metadata.get("FileName").toString()
    FROM @appname@Stream p;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using AvroFormatter (
schemaFileName: 'AvroS3Schema'
)
input from @appname@CQOUT1;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using JSONFormatter ()
INPUT FROM @appname@CQOUT1;


CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using JSONFormatter (
)
INPUT FROM @appname@CQOUT1;

CREATE OR REPLACE TARGET @dbtarget@ USING DatabaseWriter (
  Tables: '',
  ConnectionURL:'',
  Username:'',
  Password:'',
  CommitPolicy: 'EventCount:1,Interval:0',
  BatchPolicy:'EventCount:1,Interval:0'
)
INPUT FROM @appname@CQOUT1;

CREATE TARGET @bqtarget@ USING BigQueryWriter (
  Tables: '',
  projectId:'',
  BatchPolicy: 'eventCount:1, Interval:1',
  ServiceAccountKey: '',
   )
INPUT FROM @appname@CQOUT1;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter
  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1'
 )

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ Recovery 5 second interval;
--create application @APPNAME@;

--CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaProps(zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11');
--CREATE STREAM @APPNAME@kperststream of Global.WAEvent PERSIST USING @APPNAME@KafkaProps;

create type @APPNAME@employee
(
id integer,
name String,
company String
);
CREATE STREAM @APPNAME@Hana_TypedStream of @APPNAME@employee;

CREATE OR REPLACE SOURCE @APPNAME@OnPrem_Oracle USING OracleReader  (
  Compression: false,
  StartTimestamp: 'null',
  SupportPDB: false,
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  FetchSize: 1,
  DDLCaptureMode: 'All',
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'localhost:1521:xe',
 Tables: 'QATEST.EMP%',
-- Tables: 'QATEST.OracToCql_alldatatypes',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  TransactionBufferType: 'Memory',
  DictionaryMode: 'OnlineCatalog',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '1MB'
 )
OUTPUT TO @APPNAME@kperststream ;

create stream @APPNAME@UserdataStream1 of Global.WAEvent;

Create CQ @APPNAME@CQUser
insert into @APPNAME@UserdataStream1
select putuserdata (data1,'OperationName',META(data1,'OperationName').toString()) from @APPNAME@kperststream data1;

Create CQ @APPNAME@CQUser_typed
insert into @APPNAME@Hana_TypedStream
select 
to_int(data[0]),
data[1],
data[2]
from @APPNAME@UserdataStream1 u 
where USERDATA(u,'OperationName').toString()=='INSERT' and meta(u,'TableName').toString()="QATEST.EMP3";


CREATE OR REPLACE TARGET @APPNAME@DBTarget USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'SYSTEM',
  Password_encrypted: 'false',
  --ParallelThreads: '4',
  BatchPolicy: 'EventCount:1000,Interval:60',
  CommitPolicy: 'EventCount:100,Interval:60',
  --ExcludedTables: 'QATEST.EMP2;QATEST.EMP3',
  ConnectionURL: 'jdbc:sap://10.77.21.116:39013/?databaseName=striim&currentSchema=QA',
  Tables: 'QA.EMP3',
  adapterName: 'DatabaseWriter',
  Password: 'XgsL2qpACEIHrXXh4SueCg=='
 ) 
INPUT FROM @APPNAME@Hana_TypedStream;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

CREATE FLOW @STREAM@_SourceFlow;

CREATE SOURCE @SOURCE_NAME@ USING MySQLReader (
 Username: '@READER-UNAME@',
Password: '@READER-PASSWORD@',
ConnectionURL: '@CDC-READER-URL@',
Tables: @WATABLES@,
sendBeforeImage:'true',
FilterTransactionBoundaries: 'true'
) OUTPUT TO @STREAM@;

END FLOW @STREAM@_SourceFlow;

stop application dev15823;
undeploy application dev15823;
drop application dev15823 cascade;
CREATE APPLICATION dev15823 RECOVERY 1 SECOND INTERVAL;

CREATE  SOURCE OracleSource USING OracleReader  (
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  ConnectionURL: '@URL@',
  Tables: '@source-table@',
  FetchSize: 1
 )
OUTPUT TO LogminerStream;

--Create or replace Target test using SysOut (name:test) input from MySQLTestStream;

CREATE OR REPLACE TARGET WriteCDCMySQL USING DatabaseWriter  (
  Username: '@USERNAME@',
  BatchPolicy: 'Eventcount:5,Interval:300',
  CommitPolicy: 'Eventcount:5,Interval:300',
  ConnectionURL: '@URL@',
  Tables: '@TABLES@',
  Checkpointtable: 'CHKPOINT',
  Password: '@PASSWORD@'
 )
INPUT FROM LogminerStream;

END APPLICATION dev15823;
deploy application dev15823;
start dev15823;

CREATE or replace TARGET @TARGET_NAME@ USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:600',
StandardSQL:true	
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @APPNAME@_src Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
   _h_ReturnDateTimeAs:'ZonedDateTime'
) Output To @APPNAME@_stream;

create Target @APPNAME@_tgt using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_stream;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@ recovery 1 second interval;

CREATE SOURCE @parquetsrc@ USING FileReader (
  directory: '',
  positionByEOF: false,
  WildCard: '' )
PARSE USING ParquetParser (
 )
OUTPUT TO @appname@Streams;

CREATE OR REPLACE TARGET @kafkatarget@ USING Global.KafkaWriter VERSION @KAFKAVERSION@(
     brokerAddress: '',
     Topic: '',
     KafkaConfigValueSeparator: '=',
     MessageKey: '',
     MessageHeader: '',
     KafkaConfigPropertySeparator: ';',
     Mode: 'Sync',
     KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000' )
format using AvroFormatter (
formatAs: 'Default',
  schemaregistryurl: 'http://localhost:8081/',
  SchemaRegistrySubjectName: '',
  formatterName: 'AvroFormatter',
  schemaregistryConfiguration: ''
)
INPUT FROM @appname@Streams;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[0]) = '1' ;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

--
-- Crash Recovery Test 4 on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS
-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS
--

STOP APPLICATION N4S4CR4Tester.N4S4CRTest4;
UNDEPLOY APPLICATION N4S4CR4Tester.N4S4CRTest4;
DROP APPLICATION N4S4CR4Tester.N4S4CRTest4 CASCADE;
CREATE APPLICATION N4S4CRTest4 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest4;

CREATE SOURCE CsvSourceN4S4CRTest4 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest4;

CREATE FLOW DataProcessingN4S4CRTest4;

CREATE TYPE CsvDataN4S4CRTest4 (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionDataN4S4CRTest4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream OF CsvDataN4S4CRTest4;

CREATE CQ CsvToDataN4S4CRTest4
INSERT INTO DataStream
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN4S4CRTest4 CONTEXT OF WactionDataN4S4CRTest4
EVENT TYPES ( CsvDataN4S4CRTest4 )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO WactionsN4S4CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO WactionsN4S4CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END FLOW DataProcessingN4S4CRTest4;

END APPLICATION N4S4CRTest4;

CREATE APPLICATION ValidateFiles;

CREATE OR REPLACE TYPE AggregatorOutput_Type  ( SUMsthingSUMtthing java.lang.Long );

CREATE OR REPLACE STREAM AggregatorOutput OF AggregatorOutput_Type;

CREATE OR REPLACE TARGET DiffFile USING FileWriter  (
  filename: 'Difference',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:10000,interval:30',
  adapterName: 'FileWriter',
  directory: '@FEATURE-DIR@/logs',
  rolloverpolicy: 'eventcount:10000,interval:30s'
 )
FORMAT USING DSVFormatter  (   nullvalue: 'NULL',
  standard: 'none',
  handler: 'com.webaction.proc.DSVFormatter',
  formatterName: 'DSVFormatter',
  usequotes: 'false',
  rowdelimiter: '\n',
  quotecharacter: '\"',
  header: 'false',
  columndelimiter: ','
 )
INPUT FROM AggregatorOutput;

CREATE OR REPLACE TYPE ewfew_Type  ( thing java.lang.Integer );

CREATE OR REPLACE TYPE newTargetST_Type  ( thing java.lang.Integer );

CREATE OR REPLACE STREAM newTargetST OF newTargetST_Type;

CREATE OR REPLACE JUMPING WINDOW AllTargetSums OVER newTargetST KEEP WITHIN 15 SECOND;

CREATE OR REPLACE SOURCE TargetSums USING FileReader  (
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '@FEATURE-DIR@/logs',
  skipbom: true,
  wildcard: 'TargetResults.00'
 )
 PARSE USING DSVParser  (
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: '-1',
  nocolumndelimiter: true,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: false,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 )
OUTPUT TO compareTstream ;

CREATE OR REPLACE CQ GetDemTargetValues
INSERT INTO newTargetST
SELECT TO_LONG(data[0]) as thing
FROM compareTstream;

CREATE OR REPLACE SOURCE SourceSums USING FileReader  (
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '@FEATURE-DIR@/logs',
  skipbom: true,
  wildcard: 'SourceResults.00'
 )
 PARSE USING DSVParser  (
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: '-1',
  nocolumndelimiter: true,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: false,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 )
OUTPUT TO compareStream ;

CREATE OR REPLACE TYPE newST_Type  ( thing java.lang.Integer );

CREATE OR REPLACE STREAM newST OF newST_Type;

CREATE OR REPLACE JUMPING WINDOW AllSourceSums OVER newST KEEP WITHIN 15 SECOND;

CREATE OR REPLACE CQ Aggregator
INSERT INTO AggregatorOutput
SELECT SUM(s.thing) - SUM(t.thing)
FROM AllSourceSums s, AllTargetSums t;

CREATE TARGET output1 USING SysOut(name : SrcItem) input FROM compareStream;
CREATE TARGET output2 USING SysOut(name : TrgItem) input FROM compareTstream;
CREATE TARGET output3 USING SysOut(name : AggregatorItem) input FROM AggregatorOutput;

CREATE OR REPLACE CQ GetdemValues
INSERT INTO newST
SELECT TO_LONG(data[0]) as thing
FROM compareStream;

END APPLICATION ValidateFiles;

STOP APPLICATION @APP_NAME@1;
STOP APPLICATION @APP_NAME@2;

UNDEPLOY APPLICATION @APP_NAME@1;
UNDEPLOY APPLICATION @APP_NAME@2;

DROP APPLICATION @APP_NAME@1 CASCADE;
DROP APPLICATION @APP_NAME@2 CASCADE;

CREATE APPLICATION @APP_NAME@1;

CREATE OR REPLACE SOURCE @APP_NAME@1_Source USING @SOURCE_ADAPTER@  ( 
  Tables:'@SRC_TABLE@'
) OUTPUT TO @APP_NAME@_In1;

CREATE OR REPLACE TARGET @APP_NAME@1_Target USING @TARGET_ADAPTER@  ( 
  Tables: '@SRC_TABLE@, @TGT_TABLE@1', 
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  CheckPointTable: '@CHK_TABLE@', 
) INPUT FROM @APP_NAME@_In1;

CREATE TARGET @APP_NAME@_SysOut1
USING SysOut(name:@APP_NAME@1Sys)
INPUT FROM @APP_NAME@_In1;

END APPLICATION @APP_NAME@1;

DEPLOY APPLICATION @APP_NAME@1;
START APPLICATION @APP_NAME@1;


CREATE APPLICATION @APP_NAME@2;

CREATE OR REPLACE SOURCE @APP_NAME@2_Source USING @SOURCE_ADAPTER@  ( 
  Tables:'@SRC_TABLE@'
) OUTPUT TO @APP_NAME@_In2;

CREATE OR REPLACE TARGET @APP_NAME@2_Target USING @TARGET_ADAPTER@  ( 
  Tables: '@SRC_TABLE@, @TGT_TABLE@2', 
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  CheckPointTable: '@CHK_TABLE@'
) INPUT FROM @APP_NAME@_In2;

CREATE TARGET @APP_NAME@_SysOut2
USING SysOut(name:@APP_NAME@2Sys)
INPUT FROM @APP_NAME@_In2;

END APPLICATION @APP_NAME@2;

DEPLOY APPLICATION @APP_NAME@2;
START APPLICATION @APP_NAME@2;

STOP APPLICATION HW ;
undeploy application HW ;
drop application HW cascade;

CREATE APPLICATION HW Recovery 5 second interval;

CREATE  SOURCE S USING OrReader  ( 
  Username: 'miner',
  Password: '@miner',
  ConnectionURL: '@conn-url@',
  Tables: '@src@',
  FetchSize: 1) 
OUTPUT TO hivestream;

Create Target T using HiveWriter (
  ConnectionURL:'@hive-url@',
  Username:'@uname@', 
            Password:'@pwd@',
        --hadoopurl:'hdfs://localhost:9000/',
        hadoopurl:'hdfs://dockerhost:9000/',
	        Mode:'incremental',
	        mergepolicy: 'eventcount:100,interval:1s',
            Tables:'@tgt-table@',
            hadoopConfigurationPath:'/Users/saranyad/Documents/hello/'
 )
INPUT FROM hivestream;


END APPLICATION HW;
deploy application HW on all in default;

Start application HW;

--
-- Recovery Test 1
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov1Tester.RecovTest1;
UNDEPLOY APPLICATION Recov1Tester.RecovTest1;
DROP APPLICATION Recov1Tester.RecovTest1 CASCADE;
CREATE APPLICATION RecovTest1 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END APPLICATION RecovTest1;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE TYPE @appname@CQOUT1_Type (
 companyName java.lang.String,
 merchantId java.lang.String,
 dateTime org.joda.time.DateTime,
 hourValue java.lang.String,
 amount java.lang.String,
 zip java.lang.String,
 FileName java.lang.String);

CREATE SOURCE @parquetsrc@ USING S3Reader (
    bucketname:'',
    objectnameprefix:'',
    foldername:''
     )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE OR REPLACE STREAM @appname@CQOUT1 OF @appname@CQOUT1_Type;
CREATE OR REPLACE CQ @appname@CQ_PQEvent
INSERT INTO @appname@CQOUT1
    Select
    data.get("companyName").toString(),
    data.get("merchantId").toString(),
    TO_DATE(data.get("dateTime").toString()),
    data.get("hourValue").toString(),
    data.get("amount").toString(),
    data.get("zip").toString(),
    metadata.get("FileName").toString()
    FROM @appname@Stream p;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using DSVFormatter ()
input from @appname@CQOUT1;

END APPLICATION @appname@;
deploy application @appname@ on @node@ in default;
start application @appname@;

stop application GCSWriterTest;
undeploy application GCSWriterTest;
drop application GCSWriterTest cascade;
create application GCSWriterTest recovery 1 second interval;
create source GCS_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

create target sys using sysout(name:'raw')input from CsvStream;

create Target GCSTarget using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    foldername:'@foldername@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@'    
)
format using AvroFormatter (
)
input from CsvStream;

end application GCSWriterTest;
deploy application GCSWriterTest on all in default;
start application GCSWriterTest;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
wildcard: '',
directory: '',
positionbyeof: false )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING AvroFormatter  (
schemaFileName: 'AvroFileSchema'
)
INPUT FROM @appname@CqOut;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using AvroFormatter (
schemaFileName: 'AvroS3Schema'
)
input from @appname@CqOut;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using AvroFormatter (
schemaFileName: 'AvroAzureSchema'
)
INPUT FROM @appname@CqOut;

CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using AvroFormatter (
schemaFileName: 'AvroGCSSchema'
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

create or replace type @STREAM@details(
C_CUSTKEY int,
C_MKTSEGMENT String,
C_NATIONKEY int,
C_NAME String,
C_ADDRESS String,
C_PHONE String,
C_ACCTBAL int,
C_COMMENT String
);

create or replace stream @STREAM@_TYPED of @STREAM@details;

Create or replace CQ @STREAM@detailsCQ
insert into @STREAM@_TYPED
select 
to_int(data[0]),data[1],to_int(data[2]),data[3],data[4],data[5],to_int(data[6]),data[7]
from @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 ) 
INPUT FROM @STREAM@_TYPED;

stop application GCSWriterTest;
undeploy application GCSWriterTest;
drop application GCSWriterTest cascade;
create application GCSWriterTest recovery 1 second interval;
create source GS Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

create Target T using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@',
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from CsvStream;

end application GCSWriterTest;
deploy application GCSWriterTest on all in default;
start application GCSWriterTest;

STOP APPLICATION DBWriterTester.DBWriterApp;
UNDEPLOY APPLICATION DBWriterTester.DBWriterApp;
DROP APPLICATION DBWriterTester.DBWriterApp CASCADE;

DROP USER DBWriterTester;
DROP NAMESPACE DBWriterTester CASCADE;
CREATE USER DBWriterTester IDENTIFIED BY DBWriterTester;
GRANT create,drop ON deploymentgroup Global.* To user DBWriterTester;
CONNECT DBWriterTester DBWriterTester;

CREATE APPLICATION DBWriterApp;
CREATE OR REPLACE SOURCE Source_DBReader USING OracleReader  (
-- StartTimestamp: '@CDC-STARTUPTIME@',
Username: 'miner',
Password: 'miner',
ConnectionURL: '//10.1.186.110:1521/orcl',
TABLES: 'qatest.alltype1;qatest.dbr_marker;',
FetchSize: '1',
committedtransactions: true,
CatalogMode: 'Offline',
BatchPolicy: 'eventCount:10'
)
OUTPUT TO DBReaderStrm;
CREATE OR REPLACE TARGET Target_DBWriter USING DatabaseWriter  (
Tables: 'QATEST.ALLTYPE1,QATEST.ALLTYPE1;QATEST.DBR_MARKER,QATEST.DBR_MARKER;',
Username: 'qatest',
PasSword: 'qatest',
ConnecTionURL: 'jdbc:oracle:thin:@//10.1.110.142:1521/orcl',
BatchPolicy: 'eventCount:10'
)
INPUT FROM DBReaderStrm;
END APPLICATION DBWriterApp;
deploy application DBWriterApp;
start application DBWriterApp;

Stop IR;
Undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;
CREATE OR REPLACE SOURCE Teradata_source1 USING IncrementalBatchReader  ( 
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.test01=id',
  startPosition: '%=0',
  PollingInterval: '20sec'
 )
OUTPUT TO data_stream1;

CREATE OR REPLACE SOURCE Teradata_source2 USING IncrementalBatchReader  ( 
  FetchSize: 10000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.test01=t1',
  startPosition: '%=0',
  PollingInterval: '20sec' )
OUTPUT TO data_stream2;

CREATE OR REPLACE SOURCE Teradata_source3 USING IncrementalBatchReader  ( 
  FetchSize: 10000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.test01=id',
  startPosition: '%=1',
  PollingInterval: '20sec'
 )
OUTPUT TO data_stream3;

CREATE OR REPLACE SOURCE Teradata_source4 USING IncrementalBatchReader  ( 
  FetchSize: 10000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.test01=t1',
  startPosition: '%=1',
  PollingInterval: '20sec' )
OUTPUT TO data_stream4;
CREATE OR REPLACE SOURCE Teradata_source5 USING IncrementalBatchReader  ( 
  FetchSize: 10000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.test01=id',
  startPosition: '%=1',
  PollingInterval: '20sec'
 )
OUTPUT TO data_stream5;

CREATE OR REPLACE SOURCE Teradata_source6 USING IncrementalBatchReader  ( 
  FetchSize: 10000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.test01=t1',
  startPosition: '%=0',
  PollingInterval: '20sec' )
OUTPUT TO data_stream5;


create target AzureSQLDWHTarget1 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test4 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream1;


create target AzureSQLDWHTarget2 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test5',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream2;

create target AzureSQLDWHTarget3 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test5',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream3;

create target AzureSQLDWHTarget4 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test5',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream4;

create target AzureSQLDWHTarget5 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test5',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream5;



create target AzureSQLDWHTarget6 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test5',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream5;


END APPLICATION IR;
deploy application IR on all in default;
start application IR;

stop application PCAPTester.PCAPTest;
undeploy application PCAPTester.PCAPTest;
drop application PCAPTester.PCAPTest cascade;

CREATE APPLICATION PCAPTest;


    CREATE OR REPLACE TYPE PCAPData (
      ts DateTime,
      srcIp String,
      srcPort String,
      dstIp String,
      dstPort String,
      connection String
    );
    
    CREATE OR REPLACE SOURCE PCAPSource USING PCAPReader ( 
      snaplen: '65536',
      wildcard: false,
      directory: '@TEST-DATA-PATH@',
      file: 'pcapTest.pcap',
      library: '/usr/lib/libpcap.dylib',
      live: false 
     )
    OUTPUT TO PCAPOut;
    
    CREATE OR REPLACE STREAM PCAPDataStream OF PCAPData;

    CREATE OR REPLACE CQ GetPCAPData
    INSERT INTO PCAPDataStream
    SELECT TO_DATE(ts), 
           srcAddress,
           TO_STRING(srcPort),
           dstAddress,
           TO_STRING(dstPort),
           transportType + '/' + srcAddress + ':' + srcPort + '/' + dstAddress + ':' + dstPort
    FROM PCAPOut;
    



END APPLICATION PCAPTest;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING MariaDbXpandReader
(
Username: '@READER-UNAME@',
Password: '@READER-PASSWORD@',
ConnectionURL: '@CDC-READER-URL@',
Tables: @WATABLES@,
sendBeforeImage:'true',
FilterTransactionBoundaries: 'true'
) 
OUTPUT TO @STREAM@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@ MAP (table: 'QATEST.SMFTEST6')
SELECT NUM_COL,CHAR_COL,VARCHAR2_COL,LONG_COL,DATE_COL,TIMESTAMP_COL where TO_INT(NUM_COL) > 1;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SourceName@ Using MSSqlReader
(
 Username:'qatest',
 Password:'w3b@ct10n',
 DatabaseName:'qatest',
 ConnectionURL:'localhost:1433',
 Tables:'qatest.source1',
 ConnectionPoolSize:1,
 StartPosition:'EOF'
 ) Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter ( 
CheckPointTable: 'CHKPOINT',
Username: 'qatest',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',
Tables: 'QATEST.SOURCE1,qatest.target1',
Password: 'qatest'
)INPUT FROM @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetsys@ USING SysOut (name: 'ora12_out') INPUT FROM @SRCINPUTSTREAM@;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

STOP APPLICATION BQ;
UNDEPLOY APPLICATION BQ;
DROP APPLICATION BQ CASCADE;
CREATE APPLICATION BQ RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE s USING FileReader (
	directory:'Product/IntegrationTests/TestData/',
    WildCard:'posdata5L.csv',
	positionByEOF:false
	)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO ss;

CREATE TYPE userType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String
);

CREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;

CREATE CQ cq1
INSERT INTO userDefinedTypedStream
SELECT data[0],data[1],data[2], data[3],data[4]
FROM ss;

CREATE or replace TARGET TABLE1 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE1@.TABLE1',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE2 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE2@.TABLE2',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE3 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE3@.TABLE3',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE4 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE4@.TABLE4',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE5 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE5@.TABLE5',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE6 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE6@.TABLE6',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE7 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE7@.TABLE7',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE8 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE8@.TABLE8',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE9 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE9@.TABLE9',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE10 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE10@.TABLE10',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

END APPLICATION BQ;
DEPLOY APPLICATION BQ;
start application BQ;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@;

create flow agentflow;
CREATE OR REPLACE SOURCE @APPNAME@_Src USING SpannerBatchReader  (
  DatabaseProviderType: 'Default',
  pollingInterval: '5ms',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  ConnectionURL: 'jdbc:cloudspanner:/projects/bigquerywritertest/instances/testspanner/databases/spannertestdb?credentials=/Users/jenniffer/Downloads/abc.json',
  Tables: 'Recovery_Timestam%',
  --_h_mode:'InitialLoad',
--  VendorConfiguration:'_h_SpannerReadStaleness=MAX_STALENESS 20s',
  adapterName: 'SpannerBatchReader',
    StartPosition: '%=0',
  CheckColumn: '%=id'
 )
OUTPUT TO @APPNAME@_Output_Stream;
end flow agentflow;

CREATE TARGET @APPNAME@_tgt USING SpannerWriter (
	Tables: 'spannersource,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	BatchPolicy: 'EventCount: 1; Interval: 1s',
	instanceId: 'qatest'
) INPUT FROM @APPNAME@_Output_Stream;

create Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_Output_Stream;

end application @APPNAME@;
deploy application @APPNAME@ with agentflow in agents;
start application @APPNAME@;

--
-- Crash Recovery Test 6 with Jumping window and partitioned on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> KafkaStream -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION KStreamN2S2CR6Tester.KStreamN2S2CRTest6;
UNDEPLOY APPLICATION KStreamN2S2CR6Tester.KStreamN2S2CRTest6;
DROP APPLICATION KStreamN2S2CR6Tester.KStreamN2S2CRTest6 CASCADE;
DROP USER KStreamN2S2CR6Tester;
DROP NAMESPACE KStreamN2S2CR6Tester CASCADE;
CREATE USER KStreamN2S2CR6Tester IDENTIFIED BY KStreamN2S2CR6Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR6Tester;
CONNECT KStreamN2S2CR6Tester KStreamN2S2CR6Tester;

CREATE APPLICATION KStreamN2S2CRTest6 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest6;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest6 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest6;

CREATE FLOW DataProcessingKStreamN2S2CRTest6;

CREATE TYPE CsvDataTypeKStreamN2S2CRTest6 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvDataTypeKStreamN2S2CRTest6 PARTITION BY merchantId;

CREATE CQ CsvToDataKStreamN2S2CRTest6
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest6 CONTEXT OF CsvDataTypeKStreamN2S2CRTest6
EVENT TYPES ( CsvDataTypeKStreamN2S2CRTest6 )
@PERSIST-TYPE@

CREATE CQ DataToWactionKStreamN2S2CRTest6
INSERT INTO WactionsKStreamN2S2CRTest6
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingKStreamN2S2CRTest6;

END APPLICATION KStreamN2S2CRTest6;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

CREATE TARGET @TARGET@ USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: 'access_key',
	  --secretaccesskey: 'secret_access',
	  Tables: 'QATEST.%,QATEST.%',
	   S3IAMRole:'@IAMROLE@',
	uploadpolicy:'EventCount:7'
	) INPUT FROM @STREAM@;

end application @APPNAME@;

--
-- Recovery Test 25 with two sources, two jumping count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5W -> CQ1 -> WS
--   S2 -> Jc6W -> CQ2 -> WS
--

STOP Recov25Tester.RecovTest25;
UNDEPLOY APPLICATION Recov25Tester.RecovTest25;
DROP APPLICATION Recov25Tester.RecovTest25 CASCADE;
CREATE APPLICATION RecovTest25 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION RecovTest25;

STOP APPLICATION EH;
UNDEPLOY APPLICATION EH;
DROP APPLICATION EH CASCADE;
CREATE APPLICATION EH recovery 5 second interval;
CREATE Source s USING PostgreSQLReader  ( 
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Tables: 'public.tablename1000%') 
OUTPUT TO ss ;

CREATE OR REPLACE TYPE jsontype( 
datae java.util.HashMap , 
TABLE_NAME java.lang.String , 
OPS_NAME java.lang.String , 
DB_TIMESTAMP java.lang.String );

CREATE STREAM cq_json_out OF jsontype PARTITION BY TABLE_NAME;

CREATE OR REPLACE CQ cq_json 
INSERT INTO cq_json_out
SELECT 
data(e),
META(e,"TableName").toString() as TABLE_NAME,
META(e, "OperationName").toString() as OPS_NAME,
META(e, "TimeStamp").toString() as DB_TIMESTAMP
FROM ss e;

CREATE CQ cq1
INSERT INTO TypedAccessLogStream1
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000101'; 

create Target t1 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_101',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_101',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream1;

CREATE CQ cq2
INSERT INTO TypedAccessLogStream2
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000102'; 

create Target t2 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_102',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_102',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream2;

CREATE CQ cq3
INSERT INTO TypedAccessLogStream3
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000103'; 

create Target t3 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_103',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_103',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream3;

CREATE CQ cq4
INSERT INTO TypedAccessLogStream4
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000104'; 

create Target t4 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_104',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_104',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream4;

CREATE CQ cq5
INSERT INTO TypedAccessLogStream5
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000105'; 

create Target t5 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_105',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_105',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream5;

CREATE CQ cq6
INSERT INTO TypedAccessLogStream6
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000106'; 

create Target t6 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_106',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_106',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream6;

CREATE CQ cq7
INSERT INTO TypedAccessLogStream7
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000107'; 

create Target t7 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_107',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_107',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream7;

CREATE CQ cq8
INSERT INTO TypedAccessLogStream8
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000108'; 

create Target t8 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_108',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_108',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream8;

CREATE CQ cq9
INSERT INTO TypedAccessLogStream9
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000109'; 

create Target t9 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_109',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_109',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream9;

-- CREATE CQ cq10
-- INSERT INTO TypedAccessLogStream10
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000110'; 
-- 
-- create Target t10 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_110',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_110',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream10;


-- CREATE CQ cq11
-- INSERT INTO TypedAccessLogStream11
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000111'; 
-- 
-- create Target t11 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_111',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_111',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream11;
-- 
-- CREATE CQ cq12
-- INSERT INTO TypedAccessLogStream12
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000112'; 
-- 
-- create Target t12 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_112',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_112',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream12;
-- 
-- CREATE CQ cq13
-- INSERT INTO TypedAccessLogStream13
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000113'; 
-- 
-- create Target t13 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_113',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_113',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream13;
-- 
-- CREATE CQ cq14
-- INSERT INTO TypedAccessLogStream14
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000114'; 
-- 
-- create Target t14 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_114',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_114',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream14;
-- 
-- CREATE CQ cq15
-- INSERT INTO TypedAccessLogStream15
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000115'; 
-- 
-- create Target t15 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_115',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_115',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream15;
-- 
-- CREATE CQ cq16
-- INSERT INTO TypedAccessLogStream16
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000116'; 
-- 
-- create Target t16 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_116',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_116',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream16;
-- 
-- CREATE CQ cq17
-- INSERT INTO TypedAccessLogStream17
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000117'; 
-- 
-- create Target t17 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_117',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_117',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream17;
-- 
-- CREATE CQ cq18
-- INSERT INTO TypedAccessLogStream18
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000118'; 
-- 
-- create Target t18 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_118',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_118',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream18;
-- 
-- CREATE CQ cq19
-- INSERT INTO TypedAccessLogStream19
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000119'; 
-- 
-- create Target t19 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_119',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_119',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream19;
-- 
-- CREATE CQ cq20
-- INSERT INTO TypedAccessLogStream20
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000120'; 
-- 
-- create Target t20 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_120',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_120',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream20;


END APPLICATION EH;
DEPLOY APPLICATION EH;
start application EH;

STOP APPLICATION TQLwithinTqlTester.TQLwithinTqlApp;
UNDEPLOY APPLICATION TQLwithinTqlTester.TQLwithinTqlApp;
DROP APPLICATION TQLwithinTqlTester.TQLwithinTqlApp CASCADE;

CREATE APPLICATION TQLwithinTqlApp;

@@FEATURE-DIR@/tql/TQLwithinTQL5.tql;

END APPLICATION TQLwithinTqlApp;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE @SourceName@ USING PostgreSQLReader  (
  ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: '@UserName@',
  Password_encrypted: false,
  ConnectionURL: '@SourceConnectionURL@',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: '@Password@',
  Tables: '@SourceTable@'
 ) 
OUTPUT TO @SRCINPUTSTREAM@;


CREATE OR REPLACE TARGET @targetsys@ USING SysOut (name: 'ora12_out') INPUT FROM @SRCINPUTSTREAM@;


CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@UserName@',
  Password_encrypted: 'false',
  BatchPolicy: 'EventCount:1,Interval:60',
  CommitPolicy: 'EventCount:1,Interval:60',
  ConnectionURL: '@TargetConnectionURL@',
  Tables: '@SourceTable@,@TargetTable@',

  adapterName: 'PostgreSQLReader',
  Password: '@Password@'
 )
INPUT FROM @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

use global;

-- undeploy application T10;
drop application T10 cascade;

create application T10
RECOVERY 5 SECOND INTERVAL;

CREATE FLOW AgentFlow;
create source T10Source using CSVReader (
  directory:'Samples/AppData',
  header:Yes,
  wildcard:'customerdetails-recovery.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO T10Stream;
END FLOW AgentFlow;

CREATE FLOW ServerFlow;
create Target t using CSVWriter(fileName:AgentOut) input from T10Stream;
END FLOW ServerFlow;

end application T10;

DEPLOY APPLICATION T10 with AgentFlow in AGENTS, ServerFlow in SERVERS;

STOP APPLICATION snow2pg;
UNDEPLOY APPLICATION snow2pg;
DROP APPLICATION snow2pg CASCADE;


CREATE OR REPLACE APPLICATION snow2pg;

CREATE OR REPLACE SOURCE snow_pg USING Global.ServiceNowReader (
  Mode: 'InitialLoad',
  ServiceNow.ConnectionTimeOut: 60,
  ServiceNow.MaxConnections: 20,
  ServiceNow.FetchSize: 10000,
  ThreadPoolCount: '10',
  ServiceNow.ConnectionRetries: 3,
  PollingInterval: '1',
  ClientSecret: '6Wa-cv`I7x',
  Password: '^Pre&$EMO%6O.e_{96h+$R?rJd,=[4Vt=K)Szh?6g<J9D3,3zs8R;hpZqh]-3?C&.u-@GvSakPXH1:2eygbBDI>ou-z#GjBw[u8x',
  ServiceNow.Tables: 'u_empl',
  UserName: 'snr',
  ClientID: 'ce4fd5af894a11103d2c5c3a8fe075e1',
  adapterName: 'ServiceNowReader',
  ServiceNow.BatchAPI: false,
  ServiceNow.ConnectionUrl: 'https://dev84954.service-now.com/' )
OUTPUT TO sn;

CREATE TARGET pg USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'w@ct10n',
  Tables: 'u_empl,u_empl ColumnMap(name=u_name,age=u_age,address=u_address,sys_id=sys_id)',
  ParallelThreads: '',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  CommitPolicy: 'EventCount:1000,Interval:60',
  StatementCacheSize: '50',
  Username: 'waction',
  DatabaseProviderType: 'Postgres',
  BatchPolicy: 'EventCount:1000,Interval:60',
  PreserveSourceTransactionBoundary: 'false' )
INPUT FROM sn;

END APPLICATION snow2pg;
deploy application snow2pg;
start snow2pg;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;
DROP EXCEPTIONSTORE @APP_NAME@_EXCEPTIONSTORE;

CREATE APPLICATION @APP_NAME@ WITH ENCRYPTION RECOVERY 2 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE SOURCE @APP_NAME@_Source USING @SOURCE_ADAPTER@(
)OUTPUT TO @APP_NAME@DataStream;


CREATE TARGET @APP_NAME@_Target USING @TARGET_ADAPTER@( 
) INPUT FROM @APP_NAME@DataStream;


CREATE OR REPLACE TARGET @APP_NAME@_SysOut USING Global.SysOut(
	name: 'waEvent'
) INPUT FROM @APP_NAME@DataStream;

END APPLICATION @APP_NAME@;

DEPLOY APPLICATION @APP_NAME@ in default;
START APPLICATION @APP_NAME@;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;
DROP EXCEPTIONSTORE @APP_NAME@_EXCEPTIONSTORE;

CREATE APPLICATION @APP_NAME@ @APP_PROPERTY@ USE EXCEPTIONSTORE;

CREATE OR REPLACE STREAM @APP_NAME@_DataStream OF Global.WAEvent;

Create Source @APP_NAME@_Source Using @SOURCE_ADAPTER@ (

) OUTPUT TO @APP_NAME@_DataStream;

CREATE TARGET @APP_NAME@_Target USING @TARGET_ADAPTER@( 

) INPUT FROM @APP_NAME@_DataStream;

CREATE OR REPLACE TARGET @APP_NAME@_SysOut USING Global.SysOut  ( 
	name: '@APP_NAME@_SysOutWA' 
) INPUT FROM @APP_NAME@_DataStream;

END APPLICATION @APP_NAME@;

DEPLOY APPLICATION @APP_NAME@ IN DEFAULT;
START APPLICATION @APP_NAME@;

stop application Postgres_To_PostgresApp;
undeploy application Postgres_To_PostgresApp;
drop application Postgres_To_PostgresApp cascade;

CREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;

CREATE FLOW agentflow;

CREATE OR REPLACE SOURCE Postgres_Src1 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_1',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename0'
 ) 
OUTPUT TO Change_Data_Stream ;

CREATE OR REPLACE SOURCE Postgres_Src2 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_2',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename1'
 ) 
OUTPUT TO Change_Data_Stream ;


CREATE OR REPLACE SOURCE Postgres_Src3 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_3',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename2'
 ) 
OUTPUT TO Change_Data_Stream ;

CREATE OR REPLACE SOURCE Postgres_Src4 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_4',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename3'
 ) 
OUTPUT TO Change_Data_Stream ;

end flow agentflow;

create flow serverflow;

CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt1 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target0;public.tablename1, public.target0;public.tablename2, public.target0;public.tablename3, public.target0;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt2 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target1;public.tablename1, public.target1;public.tablename2, public.target1;public.tablename3, public.target1;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt3 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target2;public.tablename1, public.target2;public.tablename2, public.target2;public.tablename3, public.target2;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt4 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target3;public.tablename1, public.target3;public.tablename2, public.target3;public.tablename3, public.target3;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

end flow serverflow;
end application Postgres_To_PostgresApp;
deploy application Postgres_To_PostgresApp with agentflow on any in agents, serverflow in default;
start Postgres_To_PostgresApp;







stop application Postgres_To_PostgresApp2;
undeploy application Postgres_To_PostgresApp2;
drop application Postgres_To_PostgresApp2 cascade;

CREATE APPLICATION Postgres_To_PostgresApp2 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW agentflow2;

CREATE OR REPLACE SOURCE Postgres_Src21 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_1',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename0'
 ) 
OUTPUT TO Change_Data_Stream2 ;

CREATE OR REPLACE SOURCE Postgres_Src22 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_2',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename1'
 ) 
OUTPUT TO Change_Data_Stream2 ;


CREATE OR REPLACE SOURCE Postgres_Src23 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_3',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename2'
 ) 
OUTPUT TO Change_Data_Stream2 ;

CREATE OR REPLACE SOURCE Postgres_Src24 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_4',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename3'
 ) 
OUTPUT TO Change_Data_Stream2 ;

end flow agentflow2;

create flow serverflow2;

CREATE OR REPLACE TARGET Postgres_Sys2 USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Change_Data_Stream2;

CREATE OR REPLACE TARGET PostgreSQL_Tgt21 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target0;public.tablename1, public.target0;public.tablename2, public.target0;public.tablename3, public.target0;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream2;

CREATE OR REPLACE TARGET PostgreSQL_Tgt22 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target1;public.tablename1, public.target1;public.tablename2, public.target1;public.tablename3, public.target1;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream2;

CREATE OR REPLACE TARGET PostgreSQL_Tgt23 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target2;public.tablename1, public.target2;public.tablename2, public.target2;public.tablename3, public.target2;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream2;

CREATE OR REPLACE TARGET PostgreSQL_Tgt24 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target3;public.tablename1, public.target3;public.tablename2, public.target3;public.tablename3, public.target3;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream2;

end flow serverflow2;

end application Postgres_To_PostgresApp2;
deploy application Postgres_To_PostgresApp2 with agentflow2 on any in agents, serverflow2 in default;
start Postgres_To_PostgresApp2;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@
 Using Ojet
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@'
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop @appName@;
undeploy application @appName@;
drop application @appName@ cascade;
CREATE APPLICATION @appName@ RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE @appName@_Source USING OracleReader
(
  FilterTransactionBoundaries:true,
  ConnectionURL:'@ConnectionURL@',
  Tables:'@OrcTable@',
  Password:'@Password@',
  fetchsize:'1',
  connectionRetryPolicy:'timeOut=30, retryInterval=30, maxRetries=3',
  Username:'@Username@'
)
OUTPUT TO @appName@_Stream;

CREATE OR REPLACE TARGET @appName@_Target1 USING Global.BigQueryWriter (
  ColumnDelimiter: '|',
  NullMarker: 'NULL',
  ConnectionRetryPolicy: 'retryInterval=30,\n maxRetries=3',
  streamingUpload: 'false',
  Mode: 'Merge',
  projectId: '@ProjectId@',
  Encoding: 'UTF-8',
  TransportOptions: 'connectionTimeout=300,\n readTimeout=120',
  Tables: '@OrcTable@,@BqTable@',
  AllowQuotedNewlines: 'false',
  CDDLAction: 'Process',
  adapterName: 'BigQueryWriter',
  serviceAccountKey: '@GCS-AuthPath@',
  optimizedMerge: 'true',
  StandardSQL: 'true',
  includeInsertId: 'true',
  QuoteCharacter: '\"',
  BatchPolicy: 'eventCount:100,Interval:10' )
INPUT FROM @appName@_Stream;

End application @appName@;

--
-- Recovery Test 8
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov8Tester.RecovTest8;
UNDEPLOY APPLICATION Recov8Tester.RecovTest8;
DROP APPLICATION Recov8Tester.RecovTest8 CASCADE;
CREATE APPLICATION RecovTest8 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END APPLICATION RecovTest8;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
stop application @APPNAME3@;
undeploy application @APPNAME3@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;
drop application @APPNAME3@ cascade;

CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using OracleReader
(
  Compression:true,
  StartTimestamp:'null',
  CommittedTransactions:true,
  FilterTransactionBoundaries:true,
  Password_encrypted:'false',
  SendBeforeImage:true,
  XstreamTimeOut:600,
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',
  Tables:'qatest.oraMultiDownstream_src',
  adapterName:'OracleReader',
  Password:'qatest',
  DictionaryMode:'OfflineCatalog',
  FilterTransactionState:true,
  connectionRetryPolicy:'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType:'LogMiner',
  FetchSize: 1,
  Username:'qatest',
  OutboundServerProcessName:'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic:true,
  CDDLAction:'Quiesce_Cascade',
  CDDLCapture:'true'
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;


CREATE APPLICATION @APPNAME3@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName1@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME3@;
DEPLOY APPLICATION @APPNAME3@;
START APPLICATION @APPNAME3@;

CREATE APPLICATION @APPNAME@ @RECOVERY@;

CREATE FLOW @APPNAME@AgentFlow;
CREATE OR REPLACE SOURCE @APPNAME@_src USING Global.GCSReader ()
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream;
END FLOW @APPNAME@AgentFlow;

CREATE FLOW @APPNAME@serverFlow;
CREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer ()
FORMAT USING DSVFormatter (
members:'data')
INPUT FROM @APPNAME@_Stream;
END FLOW @APPNAME@serverFlow;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ with @APPNAME@AgentFlow in Agents, @APPNAME@ServerFlow in default;
start application @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using MSSqlReader
(
 Username:'@UserName@',
 Password:'@Password@',
 DatabaseName:'qatest',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'@SourceTable@',
 ConnectionPoolSize:1,
 StartPosition:'EOF'
 ) Output To @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;


 CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@SourceTable@,@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ recovery 5 second interval;

CREATE OR REPLACE SOURCE @SOURCENAME@ USING IncrementalBatchReader  (
  FetchSize: 1000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: @startPosition@,
  PollingInterval: '20sec'
  )
  OUTPUT TO @STREAM@;

  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;

  CREATE TARGET @targetName@ USING DatabaseWriter(
    ConnectionURL:'@READER-URL@',
    Username:'@READER-UNAME@',
    Password:'@READER-PASSWORD@',
    BatchPolicy:'Eventcount:1000,Interval:1000',
    CommitPolicy:'Eventcount:1000,Interval:1000',
    Checkpointtable:'CHKPOINT',
    Tables:'@WATABLES@,@WATABLES@_target'
  ) INPUT FROM @STREAM@;

  END APPLICATION @APPNAME@;

  DEPLOY APPLICATION @APPNAME@;
  start application @APPNAME@;

CREATE OR REPLACE APPLICATION @appname@ @recovery@ AUTORESUME MAXRETRIES 2 RETRYINTERVAL 60;

CREATE OR REPLACE SOURCE @appname@src USING Global.FileReader (
  adapterName: 'FileReader',
  rolloverstyle: 'Default',
  blocksize: 64,
  networkfilesystem: true,
  wildcard: @wildcard@,
  compressiontype: 'gzip',
  includesubdirectories: false,
  directory: @inp_directory@,
  skipbom: false,
  positionbyeof: false )
PARSE USING Global.DSVParser (
  trimwhitespace: false,
  linenumber: '-1',
  columndelimittill: '-1',
  trimquote: true,
  ignoreemptycolumn: false,
  separator: ':',
  parserName: 'DSVParser',
  quoteset: '\"',
  handler: 'com.webaction.proc.DSVParser_1_0',
  charset: 'UTF-8',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  columndelimiter: '|',
  blockascompleterecord: false,
  rowdelimiter: '\n',
  nocolumndelimiter: false,
  headerlineno: 0,
  header: true )                  
OUTPUT TO @appname@STREAM;
                                                                                                         
CREATE OR REPLACE CQ @appname@CQ
INSERT INTO @appname@CQ_Out
SELECT replaceStringRegex(f,'^$','NULL')
FROM @appname@STREAM f;

CREATE OR REPLACE CQ @appname@CQ1
INSERT INTO @appname@CQ_Out1
SELECT
CASE when TO_STRING(data[0])!='NULL' THEN TO_STRING(data[0]) ELSE data[0] END AS CLIENT_ID,
CASE when TO_STRING(data[1])!='NULL' THEN TO_STRING(data[1]) ELSE data[1] END AS ACCOUNT_NAME,
CASE when TO_STRING(data[2])!='NULL' THEN TO_DATE(data[2], 'MM/dd/yyyy') ELSE data[2] END AS EFFECTIVE_DATE,
CASE when TO_STRING(data[3])!='NULL' THEN TO_DATE(data[3], 'MM/dd/yyyy') ELSE data[3] END AS EXPIRATION_DATE,
CASE when TO_STRING(data[4])!='NULL' THEN TO_STRING(data[4]) ELSE data[4] END AS XREF,
CASE when TO_STRING(data[5])!='NULL' THEN TO_STRING(data[5]) ELSE data[5] END AS XREF_TYPE,
CASE when TO_STRING(data[6])!='NULL' THEN TO_STRING(data[6]) ELSE data[6] END AS XREF_DESCRIPTION,
CASE when TO_STRING(data[7])!='NULL' THEN TO_LONG(data[7]) ELSE data[7] END AS AUD_REC_ID,
CASE when TO_STRING(data[8])!='NULL' THEN TO_LONG(data[8]) ELSE data[8] END AS X_CLIENT_ID,
CASE when TO_STRING(data[9])!='NULL' THEN TO_LONG(data[9]) ELSE data[9] END AS X_XREF_ID,
CASE when TO_STRING(data[10])!='NULL' THEN TO_DATE(data[10], 'MM/dd/yyyy HH:mm:ss') ELSE data[10] END AS AUDIT_DATE,
DNOW() AS UDP_DB2_INSERT_TIMESTAMP,
DNOW() AS UDP_DB2_UPDATE_TIMESTAMP
FROM @appname@CQ_Out f;

CREATE OR REPLACE TARGET @appname@tgt USING Global.FileWriter (
  rolloverpolicy: 'EventCount:150,Interval:120s',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  flushpolicy: 'EventCount:150',
  filename: @tar_filename@,
  directory: @tar_directory@ )
FORMAT USING Global.JSONFormatter  (
   members:'CLIENT_ID,ACCOUNT_NAME,EXPIRATION_DATE,XREF,XREF_TYPE,XREF_DESCRIPTION,AUD_REC_ID,X_CLIENT_ID,X_XREF_ID'

)
INPUT FROM @appname@CQ_Out1;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@;

CREATE SOURCE @SourceName@ USING OracleReader  (
ReaderType: 'LogMiner',
  Password_encrypted: 'false',
  SupportPDB: false,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  QueueSize: 2048,
  CommittedTransactions: true,
  Username: '@UserName@',
  TransactionBufferType: 'Memory',
  _h_ReturnDateTimeAs: 'ZonedDateTime',
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  OutboundServerProcessName: 'WebActionXStream',
  Password: '@Password@',
  DDLCaptureMode: 'All',
  Compression: false,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  FetchSize: 1,
  Tables: '@SourceTables@',
  DictionaryMode: 'OnlineCatalog',
  XstreamTimeOut: 600,
  TransactionBufferSpilloverSize: '1MB',
  StartTimestamp: 'null',
  FilterTransactionBoundaries: true,
  StartSCN: null,
  ConnectionURL: '@ConnectionURL@',
  SendBeforeImage: true )
OUTPUT TO @AppStream@  ;

CREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream SELECT data[2], to_string(data[2], "dd-MMM-yy hh.mm.ss") FROM @AppStream@ o ;

CREATE  TARGET @targetsys@ USING Global.SysOut  (
name: 'ora1_sys' )
INPUT FROM admin.ZDT_cq_stream;

create Target @TargetFile@ using FileWriter(
  filename:'toStringOut.log',
  directory:'@FilePath@',
  rolloverpolicy:'eventcount:1000'
)
format using DSVFormatter (

)
input from admin.ZDT_cq_stream;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP APPLICATION SystemTimeTester.SystemTimeWindows;
UNDEPLOY APPLICATION SystemTimeTester.SystemTimeWindows;
DROP APPLICATION SystemTimeTester.SystemTimeWindows cascade;


CREATE APPLICATION SystemTimeWindows;

CREATE TYPE RandomData(
bankNumber int KEY,
bankName String
);

CREATE  SOURCE ranDataSource USING StreamReader (
  OutputType: 'SystemTimeTester.RandomData',
  noLimit: 'false',
  isSeeded: 'true',
  maxRows: 0,
  iterations: 30,
  iterationDelay: 1000,
  StringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',
  NumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]R'
 )
OUTPUT TO CSVDataStream;

CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT data[0], data[1]
FROM CSVDataStream;

CREATE @WINDOWTYPE@ WINDOW tierone OVER RandomDataStream keep within 20 second;

CREATE STREAM onetwostream OF RandomData;

CREATE CQ onetwocq
INSERT INTO onetwostream
SELECT bankNumber,bankName
FROM tierone
where  bankName LIKE 'bofa'
order by bankName;

CREATE WACTIONSTORE MyDataActivity  CONTEXT OF RandomData
EVENT TYPES ( RandomData  )
@PERSIST-TYPE@

Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
SELECT bankNumber,bankName from @FROMSTREAM@
where  bankName LIKE '%fa%'
order by bankName
LINK SOURCE EVENT;

END APPLICATION SystemTimeWindows;
deploy application SystemTimeWindows;
start application SystemTimeWindows;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;
CREATE  SOURCE @SourceName@ USING MySqlReader  ( 
  Username: '@Username@',
  Password: '@Password@',
  DatabaseName: 'qatest',
  connectionRetryPolicy: @ConnectionRetryPolicy@,
  ConnectionURL: '@ConnectionURL@',
  Tables: '@SourceTables@',
  ConnectionPoolSize: 1,
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @SRCINPUTSTREAM@;
create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;;

Create Source OracleSource Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;


create target AzureTarget1 using AzureSQLDWHWriter (
		CoNNectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',  
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;


create target AzureTarget2 using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        UserName: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@', 
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;


create target AzureTarget3 using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        UserName: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@', 
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

create target AzureTarget4 using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        UserName: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@', 
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

--
-- Crash Recovery Test 2 on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS
--

STOP APPLICATION N4S4CR2Tester.N4S4CRTest2;
UNDEPLOY APPLICATION N4S4CR2Tester.N4S4CRTest2;
DROP APPLICATION N4S4CR2Tester.N4S4CRTest2 CASCADE;
CREATE APPLICATION N4S4CRTest2 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest2;

CREATE SOURCE CsvSourceN4S4CRTest2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest2;

CREATE FLOW DataProcessingN4S4CRTest2;

CREATE TYPE WactionTypeN4S4CRTest2 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionTypeN4S4CRTest2;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN4S4CRTest2 CONTEXT OF WactionTypeN4S4CRTest2
EVENT TYPES ( WactionTypeN4S4CRTest2 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN4S4CRTest2
INSERT INTO WactionsN4S4CRTest2
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN4S4CRTest2;

END APPLICATION N4S4CRTest2;

--
-- Kafka Stream Recovery Test 1
-- Bert Hashemi and Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> KS -> WS

STOP KStreamRecov1Tester.KStreamRecovTest1;
UNDEPLOY APPLICATION KStreamRecov1Tester.KStreamRecovTest1;
DROP APPLICATION KStreamRecov1Tester.KStreamRecovTest1 CASCADE;
DROP USER KStreamRecov1Tester;
DROP NAMESPACE KStreamRecov1Tester CASCADE;
CREATE USER KStreamRecov1Tester IDENTIFIED BY KStreamRecov1Tester;
-- GRANT 'Global:create,drop:deploymentgroup:*' TO USER KStreamRecov1Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov1Tester;
CONNECT KStreamRecov1Tester KStreamRecov1Tester;

CREATE APPLICATION KStreamRecovTest1 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE or REPLACE TYPE KafkaCsvStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF KafkaCsvStreamType 
EVENT TYPES ( KafkaCsvStreamType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

END APPLICATION KStreamRecovTest1;

stop PatternMatching.CSV;
undeploy application PatternMatching.CSV;
drop application PatternMatching.CSV cascade;

create application CSV RECOVERY 5 SECOND INTERVAL;

create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'ctest.csv',
  columndelimiter:',',
  positionByEOF:false
)
OUTPUT TO CsvStream;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  TO_INT(data[0]) as UserId,
	TO_INT(data[1]) as temp1,
        TO_DOUBLE(data[2]) as temp2,
	TO_STRING(data[3]) as temp3
FROM CsvStream;

-- scenario 1.1 check pattern alterations
CREATE CQ TypeConversionCsvCQ1
INSERT INTO TypedStream1
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A | B  | C
define A = UserDataStream(temp1 = 20), B= UserDataStream(temp2 = 30.40), C= UserDataStream(temp3 = 'Bret')
PARTITION BY UserId;

-- scenario 1.2 check pattern permutation with partition by
CREATE CQ TypeConversionCsvCQ2
INSERT INTO TypedStream2
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A & B & C
define A = UserDataStream(temp1 = 10), B= UserDataStream(temp2 = 20.30), C= UserDataStream(temp3 = 'zalak')
PARTITION BY UserId;

-- scenario 1.3 check pattern quantifire with partition by
CREATE CQ TypeConversionCsvCQ3
INSERT INTO TypedStream3
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A ? B ? C
define A = UserDataStream(temp1 = 10), B= UserDataStream(temp2 = 20.30), C= UserDataStream(temp3 = 'zalak')
PARTITION BY UserId;

-- scenario 1.4 check pattern quantifire with grouping and partition by
CREATE CQ TypeConversionCsvCQ4
INSERT INTO TypedStream4
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A ( B ? C)
define A = UserDataStream(temp1 = 10), B= UserDataStream(temp2 = 20.30), C= UserDataStream(temp3 = 'zalak')
PARTITION BY UserId;

-- scenario 1.5 check pattern overlapping and alteration with grouping and partition by
CREATE CQ TypeConversionCsvCQ5
INSERT INTO TypedStream5
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN (A # B) | C
define A = UserDataStream(temp1 = 30), B= UserDataStream(temp2 = 20.30), C= UserDataStream(temp3 = 'Bret')
PARTITION BY UserId;

-- scenario 1.6 check pattern quantifire with grouping and two partition by
CREATE CQ TypeConversionCsvCQ6
INSERT INTO TypedStream6
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A ? B ? C
define A = UserDataStream(temp1 = 10), B= UserDataStream(temp2 = 30.40), C= UserDataStream(temp3 = 'Bret')
PARTITION BY temp1,temp2;

-- scenario 1.7 check pattern quantifire(0 or 1),<=,>= with partition by
CREATE CQ TypeConversionCsvCQ7
INSERT INTO TypedStream7
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A * B * C
define A = UserDataStream(temp1 <= 10), B= UserDataStream(temp2 >= 30.40), C= UserDataStream(temp3 = 'zalak')
PARTITION BY UserId,temp3;

-- scenario 1.8 check pattern alteration and permutation using between values with partition by
CREATE CQ TypeConversionCsvCQ8
INSERT INTO TypedStream8
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A | B & C
define A = UserDataStream(temp1 between 10 and 40), B= UserDataStream(temp2 between 10.40 and 30.50), C= UserDataStream(temp3 != 'zalak')
PARTITION BY temp3;

-- scenario 1.9 check pattern or using <,>,!= values with partition by
CREATE CQ TypeConversionCsvCQ9
INSERT INTO TypedStream9
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A  B  C
define A = UserDataStream(temp1 > 20), B= UserDataStream(temp2 < 60), C= UserDataStream(temp3 != 'prajkta')
PARTITION BY UserId;

-- scenario 1.10 check pattern {m,n} using != values with partition by
CREATE CQ TypeConversionCsvCQ10
INSERT INTO TypedStream10
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A{0,2} | B{1,3} & C
define A = UserDataStream(temp1 != 20), B= UserDataStream(temp2 != 40.10), C= UserDataStream(temp3 != 'bert')
PARTITION BY temp1;


CREATE WACTIONSTORE UserActivityInfo1
CONTEXT OF TypedStream1_Type
EVENT TYPES ( TypedStream1_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo2
CONTEXT OF TypedStream2_Type
EVENT TYPES ( TypedStream2_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo3
CONTEXT OF TypedStream3_Type
EVENT TYPES ( TypedStream3_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo4
CONTEXT OF TypedStream4_Type
EVENT TYPES ( TypedStream4_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo5
CONTEXT OF TypedStream5_Type
EVENT TYPES ( TypedStream5_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo6
CONTEXT OF TypedStream6_Type
EVENT TYPES ( TypedStream6_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo7
CONTEXT OF TypedStream7_Type
EVENT TYPES ( TypedStream7_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo8
CONTEXT OF TypedStream8_Type
EVENT TYPES ( TypedStream8_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo9
CONTEXT OF TypedStream9_Type
EVENT TYPES ( TypedStream9_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo10
CONTEXT OF TypedStream10_Type
EVENT TYPES ( TypedStream10_Type )
@PERSIST-TYPE@

create Target t2 using SysOut(name:AgentTyped) input from TypedStream1;
create Target t3 using SysOut(name:AgentTyped1) input from TypedStream2;

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction1
INSERT INTO UserActivityInfo1
SELECT * FROM TypedStream1
LINK SOURCE EVENT;

CREATE CQ UserWaction2
INSERT INTO UserActivityInfo2
SELECT * FROM TypedStream2
LINK SOURCE EVENT;

CREATE CQ UserWaction3
INSERT INTO UserActivityInfo3
SELECT * FROM TypedStream3
LINK SOURCE EVENT;

CREATE CQ UserWaction4
INSERT INTO UserActivityInfo4
SELECT * FROM TypedStream4
LINK SOURCE EVENT;

CREATE CQ UserWaction5
INSERT INTO UserActivityInfo5
SELECT * FROM TypedStream5
LINK SOURCE EVENT;

CREATE CQ UserWaction6
INSERT INTO UserActivityInfo6
SELECT * FROM TypedStream6
LINK SOURCE EVENT;

CREATE CQ UserWaction7
INSERT INTO UserActivityInfo7
SELECT * FROM TypedStream7
LINK SOURCE EVENT;

CREATE CQ UserWaction8
INSERT INTO UserActivityInfo8
SELECT * FROM TypedStream8
LINK SOURCE EVENT;

CREATE CQ UserWaction9
INSERT INTO UserActivityInfo9
SELECT * FROM TypedStream9
LINK SOURCE EVENT;

CREATE CQ UserWaction10
INSERT INTO UserActivityInfo10
SELECT * FROM TypedStream10
LINK SOURCE EVENT;

end application CSV;
deploy application csv;
start csv;

STOP APPLICATION EnvvarTester.envVar;
UNDEPLOY APPLICATION EnvvarTester.envVar;
DROP APPLICATION EnvvarTester.envVar CASCADE;

CREATE APPLICATION envVar;


CREATE SOURCE AccessLogSource USING FileReader (
directory:'@TEST-DATA-PATH@/envVar',
wildcard:'$FILENAME',
blocksize: $BLOCKSIZE,
positionByEOF:false
)
PARSE USING DSVParser (
columndelimiter:' ',
ignoreemptycolumn:'Yes',
quoteset:'[]~"',
separator:'~'
)
OUTPUT TO RawAccessStream;


END APPLICATION envVar;
DEPLOY APPLICATION envVar on any in default;
START envVar;

STOP bq;
UNDEPLOY APPLICATION bq;
DROP APPLICATION bq CASCADE;

CREATE APPLICATION bq RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE BQ_source USING OracleReader
(
	Username:'qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
  OnlineCatalog:true,
  FetchSize:'1',
  Tables: 'QATEST.sourceTable'
)
OUTPUT TO SS;


CREATE or replace TARGET BQ_target USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
    Tables:'QATEST.sourceTable,qatest.% keycolumns(RONUM)',
    mode:'Appendonly',
    datalocation: 'US',
	nullmarker: 'NULL',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:100,Interval:10'	
) INPUT FROM ss;

CREATE OR REPLACE TARGET bq_SysOut USING Global.SysOut (name: 'wa') INPUT FROM ss;

END APPLICATION bq;
DEPLOY APPLICATION bq;
START APPLICATION bq;

--
-- Crash Recovery Test 3 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW -> CQ(aggregate) -> WS
--

STOP APPLICATION KStreamN2S2CR3Tester.KStreamN2S2CRTest3;
UNDEPLOY APPLICATION KStreamN2S2CR3Tester.KStreamN2S2CRTest3;
DROP APPLICATION KStreamN2S2CR3Tester.KStreamN2S2CRTest3 CASCADE;

DROP USER KStreamN2S2CR3Tester;
DROP NAMESPACE KStreamN2S2CR3Tester CASCADE;
CREATE USER KStreamN2S2CR3Tester IDENTIFIED BY KStreamN2S2CR3Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR3Tester;
CONNECT KStreamN2S2CR3Tester KStreamN2S2CR3Tester;

CREATE APPLICATION KStreamN2S2CRTest3 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest3;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest3 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest3;

CREATE FLOW DataProcessingKStreamN2S2CRTest3;

CREATE TYPE WactionTypeKStreamN2S2CRTest3 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount int,
  city String
);

CREATE STREAM DataStream OF WactionTypeKStreamN2S2CRTest3;

CREATE CQ CsvToDataKStreamN2S2CRTest3
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_INT(TO_DOUBLE(data[7])),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest3 CONTEXT OF WactionTypeKStreamN2S2CRTest3
EVENT TYPES ( WactionTypeKStreamN2S2CRTest3 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsKStreamN2S2CRTest3
INSERT INTO WactionsKStreamN2S2CRTest3
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

END FLOW DataProcessingKStreamN2S2CRTest3;

END APPLICATION KStreamN2S2CRTest3;

CREATE TARGET @TARGET_NAME@ USING SnowflakeWriter (
  ConnectionURL: '@SNOWFLAKE-URL@',
  username: '@SNOWFLAKE-USERNAME@',
  password: '@SNOWFLAKE-PASSWORD@',
  appendOnly:'false',
  Tables: '@TARGET-TABLES@'
 )
 INPUT FROM @STREAM@;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;
CREATE APPLICATION OracleToKudu RECOVERY 1 SECOND INTERVAL;
CREATE  SOURCE oracSource USING OracleReader  ( 
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.56.102:1521/orcl',
  Tables: 'QATEST.oracle_alldatatypes',
  OnlineCatalog: true,
  FetchSize: 1
 ) 
OUTPUT TO DataStream;
CREATE  TARGET WriteintoKudu USING KuduWriter  ( 
  kuduclientconfig: 'master.addresses->192.168.56.101:7051;socketreadtimeout->240;operationtimeout->1200',
  pkupdatehandlingmode: 'DELETEANDINSERT',
  tables: 'QATEST.ORACLE_ALLDATATYPES,KUDU_ALLDATATYPES',
  batchpolicy: 'EventCount:1,Interval:0'
 ) 
INPUT FROM DataStream;
END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ recovery 1 second interval;

CREATE SOURCE @SOURCENAME@ USING OracleReader
(
    Username: '@SRCUSERNAME@',
    Password: '@SRCPASSWORD@',
    ConnectionURL: '@SRCURL',
    Tables: '@SRCTABLE',
    FetchSize: '@FETCHSIZE@',
    CommittedTransactions: true
)

OUTPUT TO @STREAM@ ;

CREATE TARGET @TARGETNAME@ using DatabaseWriter
(
    ConnectionURL: '@TARGETURL',
    username: '@TARGETUSERNAME@',
    Password: '@TARGETPASSWORD@',
    Tables: '@TARGETTABLE@',
    BatchPolicy:'EventCount:1,Interval:1',
    CommitPolicy:'EventCount:1,Interval:1'
)
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

Create Source @SOURCE_NAME@ Using MSJet
(
 Username:'@READER-UNAME@',
 Password:'@READER-PASSWORD@',
 DatabaseName:'@DB_NAME@',
 ConnectionURL:'@CONN_URL@',
 Tables:@WATABLES@,
 compression:'@COMP@'
)
OUTPUT TO @STREAM@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;


CREATE OR REPLACE SOURCE @APPNAME@CDC_Source1 USING MySQLReader( 
  Compression: true,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',
  Tables: 'waction.crash_type',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root'
 ) 
OUTPUT TO @APPNAME@AppStream1;


CREATE OR REPLACE TARGET @APPNAME@sap_target USING DatabaseWriter( 
  DatabaseProviderType:'SAPHANA',
  ConnectionRetryPolicy: 'retryInterval=30,maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'SYSTEM',
  Password_encrypted: 'false',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:sap://10.77.21.116:39013/?databaseName=striim&currentSchema=QA',
  Tables: 'waction.crash_type,QA.CRASH_TYPES',
  adapterName: 'DatabaseWriter',
  --IgnorableExceptionCode: '',
  Password: 'Striim_SAP@123'
 ) 
INPUT FROM @APPNAME@AppStream1;


create or replace target @APPNAME@sys_tgt using sysout(
name:Foo2
)input from @APPNAME@AppStream1;

END APPLICATION @APPNAME@;

deploy application @APPNAME@;
start application @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ Recovery 5 second interval;

create stream @APPNAME@_UserdataStream of Global.WAEvent;

create type @APPNAME@_Order_type(
id int,
order_id int,
zipcode int,
category String,
tablename string
);

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src1 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.order_%'
)
OUTPUT TO @APPNAME@_OrdersStream;

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src2 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_2',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.second_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream2;

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src3 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_3',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.third_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream3;

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src4 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_4',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.fourth_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream4;


Create CQ @APPNAME@_CQUser
insert into @APPNAME@_UserdataStream
select 
putuserdata (data,'Fileowner','FIRST_ORDER') from @APPNAME@_OrdersStream data;


Create CQ @APPNAME@_CQUser2
insert into @APPNAME@_UserdataStream
select 
putuserdata (data2,'Fileowner','SECOND_ORDER') from @APPNAME@_OrdersStream2 data2;


Create CQ @APPNAME@_CQUser3
insert into @APPNAME@_UserdataStream
select 
putuserdata (data3,'Fileowner','THIRD_ORDER') from @APPNAME@_OrdersStream3 data3;


Create CQ @APPNAME@_CQUser4
insert into @APPNAME@_UserdataStream
select 
putuserdata (data4,'Fileowner','FOURTH_ORDER') from @APPNAME@_OrdersStream4 data4;

create stream @APPNAME@_OrderTypedStream1 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream2 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream3 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream4 of @APPNAME@_Order_type;

CREATE CQ @APPNAME@_fin_cq
INSERT INTO @APPNAME@_OrderTypedStream1
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FIRST_ORDER';

CREATE CQ @APPNAME@_fin_cq2
INSERT INTO @APPNAME@_OrderTypedStream2
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'SECOND_ORDER';

CREATE CQ @APPNAME@_fin_cq3
INSERT INTO @APPNAME@_OrderTypedStream3
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'THIRD_ORDER';

CREATE CQ @APPNAME@_fin_cq4
INSERT INTO @APPNAME@_OrderTypedStream4
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FOURTH_ORDER';


create Target @APPNAME@_ADLSGen1_tgt1 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'%category%/%tablename%',
        datalakestorename:'striimdlstest.azuredatalakestore.net',
        clientid:'94195e09-651c-431e-8556-59343c99cc05',
        authtokenendpoint:'https://login.microsoftonline.com/71bfeed5-1905-43da-a4a4-49d8490731da/oauth2/token',
        clientkey:'Vt7Reaamli1DXpqa3kY1+VTzQuEQrvchs5PJ3VNVmfM=',
  rolloverpolicy:'eventcount:8,interval:20s'
)
format using DSVFormatter (
    header:'true'
)
input from @APPNAME@_OrderTypedStream1; 

create Target @APPNAME@_ADLSGen1_tgt2 using ADLSGen1Writer(
        filename:'event_data.xml',
        directory:'%category%/%tablename%',
        datalakestorename:'striimdlstest.azuredatalakestore.net',
        clientid:'94195e09-651c-431e-8556-59343c99cc05',
        authtokenendpoint:'https://login.microsoftonline.com/71bfeed5-1905-43da-a4a4-49d8490731da/oauth2/token',
        clientkey:'Vt7Reaamli1DXpqa3kY1+VTzQuEQrvchs5PJ3VNVmfM=',
	rolloverpolicy:'eventcount:8,interval:20s'
)
format using XMLFormatter (
  elementtuple: 'Order_id:id:order_id:zipcode:category:text=tablename',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from @APPNAME@_OrderTypedStream2; 

create Target @APPNAME@_ADLSGen1_tgt3 using ADLSGen1Writer(
        filename:'event_data.avro',
        directory:'%category%/%tablename%',
        datalakestorename:'striimdlstest.azuredatalakestore.net',
        clientid:'94195e09-651c-431e-8556-59343c99cc05',
        authtokenendpoint:'https://login.microsoftonline.com/71bfeed5-1905-43da-a4a4-49d8490731da/oauth2/token',
        clientkey:'Vt7Reaamli1DXpqa3kY1+VTzQuEQrvchs5PJ3VNVmfM=',
  rolloverpolicy:'eventcount:8,interval:20s'
)
format using AvroFormatter (
  formatAs: 'Default',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA-FILE@'
)
input from @APPNAME@_OrderTypedStream3; 


create Target @APPNAME@_ADLSGen1_tgt4 using ADLSGen1Writer(
        filename:'event_data.json',
        directory:'%category%/%tablename%',
        datalakestorename:'striimdlstest.azuredatalakestore.net',
        clientid:'94195e09-651c-431e-8556-59343c99cc05',
        authtokenendpoint:'https://login.microsoftonline.com/71bfeed5-1905-43da-a4a4-49d8490731da/oauth2/token',
        clientkey:'Vt7Reaamli1DXpqa3kY1+VTzQuEQrvchs5PJ3VNVmfM=',
  rolloverpolicy:'eventcount:8,interval:20s'
)
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@_OrderTypedStream4;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

CREATE  SOURCE @QuerySource@1 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@1;
create Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;
 CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@1;

CREATE  SOURCE @QuerySource@2 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@2;
create Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;
 CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@2;

CREATE  SOURCE @QuerySource@3 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@3;
create Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;
 CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@3;

CREATE  SOURCE @QuerySource@4 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@4;
create Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;
 CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@4;

CREATE  SOURCE @QuerySource@5 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@5;
create Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;
 CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@5;

CREATE  SOURCE @QuerySource@6 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@6;
create Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;
 CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@6;

CREATE  SOURCE @QuerySource@7 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@7;
create Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;
 CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@7;

CREATE  SOURCE @QuerySource@8 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@8;
create Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;
 CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@8;

CREATE  SOURCE @QuerySource@9 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@9;
create Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;
 CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@9;

CREATE  SOURCE @QuerySource@10 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@10;
create Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;
 CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@10;
end application @APPNAME@;

STOP APPLICATION app1;
STOP APPLICATION app2;
STOP APPLICATION app3;
STOP APPLICATION app4;
STOP APPLICATION app5;
STOP APPLICATION app6;
STOP APPLICATION app7;
UNDEPLOY APPLICATION app1;
UNDEPLOY APPLICATION app2;
UNDEPLOY APPLICATION app3;
UNDEPLOY APPLICATION app4;
UNDEPLOY APPLICATION app5;
UNDEPLOY APPLICATION app6;
UNDEPLOY APPLICATION app7;
DROP APPLICATION app1 CASCADE;
DROP APPLICATION app2 CASCADE;
DROP APPLICATION app3 CASCADE;
DROP APPLICATION app4 CASCADE;
DROP APPLICATION app5 CASCADE;
DROP APPLICATION app6 CASCADE;
DROP APPLICATION app7 CASCADE;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',
acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

CREATE APPLICATION app1 RECOVERY 1 SECOND INTERVAL;

create flow sourceflow;

create type type1(
  id String,
  name String,
  city string
);

CREATE STREAM rawstream OF Global.waevent persist using KafkaPropset;
CREATE STREAM kps_typedStream OF type1 partition by city persist using KafkaPropset;
CREATE STREAM sourcestream of Global.waevent;

CREATE OR REPLACE SOURCE s USING oracleReader  (
  Username:'qatest',
  Password:'qatest',
  ConnectionURL:'localhost:1521/xe',
  Tables:'QATEST.test%',
  FetchSize:1
 )
OUTPUT TO rawstream;

end flow sourceflow;
create flow targetflow;
create cq cq1
INSERT INTO sourcestream
SELECT * from rawstream;

CREATE CQ cq2
INSERT INTO kps_typedStream
SELECT TO_STRING(data[0]),
TO_STRING(data[1]),
TO_STRING(data[2])FROM rawstream;
end flow targetflow;

end application app1;
-- deploy application app1;
-- deploy application app1 with sourceflow in AGENTS, targetflow on any in default;

CREATE APPLICATION app2 RECOVERY 1 SECOND INTERVAL;

CREATE TARGET app2_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS1'
) INPUT FROM rawstream;


end application app2;
deploy application app2;


CREATE APPLICATION app3 RECOVERY 1 SECOND INTERVAL;

CREATE TARGET app3_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test02,QATEST.KPS2'
) INPUT FROM rawstream;

end application app3;
deploy application app3;


CREATE APPLICATION app4 RECOVERY 1 SECOND INTERVAL;

CREATE TARGET app4_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test03,QATEST.KPS3'
) INPUT FROM rawstream;

end application app4;
deploy application app4;


CREATE APPLICATION app5 RECOVERY 1 SECOND INTERVAL;

CREATE TARGET app5_target1 USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'snappy',
KafkaConfig:'compression.type=snappy'
)
FORMAT USING DSVFormatter ()
INPUT FROM kps_typedStream;

CREATE TARGET app5_target2 USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'gzip',
KafkaConfig:'compression.type=gzip'
)
FORMAT USING DSVFormatter ()
INPUT FROM rawstream;

CREATE TARGET app5_target3 USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'lz4',
KafkaConfig:'compression.type=lz4'
)
FORMAT USING DSVFormatter ()
INPUT FROM rawstream;


end application app5;
deploy application app5;

CREATE APPLICATION app6 RECOVERY 1 SECOND INTERVAL;

CREATE TARGET app6_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test03,QATEST.KPS4'
) INPUT FROM rawstream;

end application app6;
deploy application app6;

CREATE APPLICATION app7 RECOVERY 1 SECOND INTERVAL;

CREATE TARGET app7_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS_NEW1;qatest.test02,QATEST.KPS_NEW2;qatest.test03,QATEST.KPS_NEW3;'
) INPUT FROM rawstream;

end application app7;
deploy application app7;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
CREATE SOURCE @APPNAME@_S USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.test01',
  connectionRetryPolicy:'retryInterval=1,maxRetries=3'
 ) 
OUTPUT TO @APPNAME@_SS;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'public.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

CREATE APPLICATION FileSource;

CREATE  TYPE FileStr2_Type  ( seq java.lang.Integer
 );

CREATE STREAM FileStr2 OF FileStr2_Type;

CREATE OR REPLACE JUMPING WINDOW FileDataAggregation OVER FileStr2 KEEP 1000000 ROWS;

CREATE OR REPLACE SOURCE FileSource USING FileReader  (
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '@TEST-DATA-PATH@/Validate-Striim',
  skipbom: true,
  wildcard: 'FiletoRead.txt'
 )
 PARSE USING DSVParser  (
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: '-1',
  nocolumndelimiter: true,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: false,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 )
OUTPUT TO FileStr1 ;

CREATE OR REPLACE CQ FileDataConvert
INSERT INTO FileStr2
SELECT TO_INT(data[0]) as seq
FROM FileStr1;

CREATE  TYPE FileStr3_Type  ( SUMFileDataAggregationseq java.lang.Long );

CREATE STREAM FileStr3 OF FileStr3_Type;

CREATE OR REPLACE TARGET FileWrite USING FileWriter  (
  filename: 'SourceResults',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:10000,interval:30',
  adapterName: 'FileWriter',
  directory: '@FEATURE-DIR@/logs',
  rolloverpolicy: 'eventcount:10000,interval:30s'
 )
FORMAT USING DSVFormatter  (   nullvalue: 'NULL',
  standard: 'none',
  handler: 'com.webaction.proc.DSVFormatter',
  formatterName: 'DSVFormatter',
  usequotes: 'false',
  rowdelimiter: '\n',
  quotecharacter: '\"',
  header: 'false',
  columndelimiter: ','
 )
INPUT FROM FileStr3;

CREATE OR REPLACE CQ SumAggregat
INSERT INTO FileStr3
SELECT SUM(FileDataAggregation.seq)
FROM FileDataAggregation;

END APPLICATION FileSource;

stop application AzureApp2;
undeploy application AzureApp2;
drop application AzureApp2 cascade;

create application AzureApp2
RECOVERY 5 second interval;
create source CSVSource2 using FileReader (
	directory:'@DIR@',
	WildCard:'@WILDCARD@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream2;

Create Type CSVType2 (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream2 of CSVType2;

CREATE CQ CsvToPosData2
INSERT INTO TypedCSVStream2
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream2;

create Target BlobT2 using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:100000'
)
format using DSVFormatter (
)
input from TypedCSVStream2;
end application AzureApp2;
deploy application AzureApp2 in default;
start application AzureApp2;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '@GRPPOLICY@',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'MultipleEvents',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
  filename: '%@metadata(FileName)%',
  directory: '@DIR@',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter  ()
INPUT FROM CCBStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

STOP UpdatableCacher.UpdatableCache;
UNDEPLOY APPLICATION UpdatableCacher.UpdatableCache;
DROP APPLICATION UpdatableCacher.UpdatableCache CASCADE;
CREATE APPLICATION UpdatableCacher.UpdatableCache;

CREATE TYPE MerchantHourlyAve(
  merchantId String KEY,
  hourlyAve Integer,
  theDate DateTime,
  dVal Double
);


CREATE source CsvDataSource USING FileReader (
      directory:'@TEST-DATA-PATH@',
      columndelimiter: ',',
      wildcard:'ucData.csv',
      blocksize: 10240,
      positionByEOF:false
)
PARSE USING DSVParser (
      header:No,
      trimquote:false
) OUTPUT TO CsvStream;


CREATE STREAM S1 OF MerchantHourlyAve;

CREATE CQ cq1
	insert into S1
		SELECT data[0],
				TO_INT(data[1]),
				TO_DATE(data[2]),
				TO_DOUBLE(data[3])
		FROM CsvStream;


CREATE EVENTTABLE ET1 using STREAM (
  NAME: 'S1'
) QUERY (keytomap:'dVal', persistPolicy: 'true' ) OF MerchantHourlyAve;


CREATE EVENTTABLE ET2 using STREAM (
  NAME: 'S1'
) QUERY (keytomap:'merchantId' ) OF MerchantHourlyAve;



END APPLICATION UpdatableCache;

--
-- Recovery Test 23 with two sources, two sliding time windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> St1W -> CQ1 -> WS
-- S2 -> St2W -> CQ2 -> WS
--

STOP KStreamRecov23Tester.KStreamRecovTest23;
UNDEPLOY APPLICATION KStreamRecov23Tester.KStreamRecovTest23;
DROP APPLICATION KStreamRecov23Tester.KStreamRecovTest23 CASCADE;
DROP USER KStreamRecov23Tester;
DROP NAMESPACE KStreamRecov23Tester CASCADE;
CREATE USER KStreamRecov23Tester IDENTIFIED BY KStreamRecov23Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov23Tester;
CONNECT KStreamRecov23Tester KStreamRecov23Tester;

CREATE APPLICATION KStreamRecovTest23 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 1 SECOND;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 2 SECOND;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION KStreamRecovTest23;

Stop @APPNAME@_App;
undeploy application @APPNAME@_App;
drop application @APPNAME@_App cascade;

CREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;
CREATE SOURCE @APPNAME@_Src USING Global.OracleReader(
  FetchSize:'1',
  Username:'@SOURCE_USER@',
  Password:'@SOURCE_PASSWORD@',
  ConnectionURL:'@SOURCE_URL@',
  Tables:'@SOURCE_TABLES@',
  ConnectionRetryPolicy:'@AUTO_CONNECTION_RETRY@'
)
OUTPUT TO @APPNAME@_Stream;

CREATE TARGET @APPNAME@_SYS USING SysOut (
  name: '@APPNAME@_SYS' )
INPUT FROM @APPNAME@_Stream;

End APPLICATION @APPNAME@_App;

stop application Postgres_To_PostgresApp;
undeploy application Postgres_To_PostgresApp;
drop application Postgres_To_PostgresApp cascade;

CREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE Postgres_src USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 ) 
OUTPUT TO Change_Data_Stream ;


CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET Postgres_tgt USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

end application Postgres_To_PostgresApp;
deploy application Postgres_To_PostgresApp;
start Postgres_To_PostgresApp;

stop application JMSWriter.JMS;
undeploy application JMSWriter.JMS;
drop application JMSWriter.JMS cascade;

create application JMS;
create source JMSCSVSource using FileReader (
        directory:'@TEST-DATA-PATH@',
        WildCard:'AdhocQueryData2.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target JmsTarget  using JMSWriter (
		Provider:'@JMSWRITERPROVIDER@',
		Ctx:'@JMSWRITERCONTEXT@',
		messagetype: @MESSAGETYPE@,
		UserName:'@JMSWRITERUSERNAME@',
		Password:'@JMSWRITERPASSWORD@',
		@DESTINATIONTYPE@)
format using @JMSTARGETFORMATTERTYPE@ (
@JMSTARGETFORMATTERMEMBERS@
)
input from TypedCSVStream;

end Application Jms;

create application access;

create source AALAccessSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'access_log',
  charset:'UTF-8',
  positionByEOF:false
) PARSE USING AALParser (
  columndelimiter:' ',
  IgnoreEmptyColumn:'Yes'
) OUTPUT TO AalAccessStream;

create Target AALAccessDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/logdata') input from AalAccessStream;

end application access;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@  RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @srcName@ USING OracleReader (
 Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@'
)
OUTPUT TO @outstreamname@;

CREATE  TARGET @tgtName@ USING Global.KinesisWriter ( 
  BatchPolicy: 'Size:900000,Interval:1', 
  streamName: '@streamname@', 
  accesskeyid: '@accesskeyid@', 
  Mode: 'Sync', 
  regionName: '@region@', 
  secretaccesskey: '@secretaccesskey@', 
  adapterName: 'KinesisWriter' ) 
FORMAT USING Global.DSVFormatter  ( 
  quotecharacter: '\"', 
  handler: 'com.webaction.proc.DSVFormatter', 
  columndelimiter: ',', 
  formatterName: 'DSVFormatter', 
  nullvalue: 'NULL', 
  usequotes: 'false', 
  rowdelimiter: '\n', 
  standard: 'none', 
  header: 'false' ) 
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
directory:'@FEATURE-DIR@/logs',
filename:'PosDataFS',
rolloverpolicy:'filesize:1M,sequence:00'
)
format using DSVFormatter (

)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizePosData_actual.log') input from TypedCSVStream;

end application DSV;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING ADLSReader ()
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE CQ @APPNAME@_CQ
INSERT INTO @APPNAME@_CQOut
SELECT
    data[0] as BusinessName,
    data[1] as MerchantId,
    data[2] as PosDataCode,
    data[3] as AccNumber,
    data[4] as DateTime,
    data[5] as ExpDate,
    data[6] as CurrencyCode,
    data[7] as AuthAmount,
    data[8] as TerminalId,
    data[9] as Zip,
    data[10] as City
FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_KafkaTarget USING Global.KafkaWriter VERSION @KAFKA_VERSION@(
  brokerAddress: '',
  Topic: '',
  Mode: 'Sync' )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_BlobTarget USING Global.AzureBlobWriter (
  containername: '',
  blobname: '',
  accountaccesskey: '',
  accountname: '',
  foldername: '' )
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_DWHTarget USING Global.BigQueryWriter (
  Tables: '',
  BatchPolicy: '',
  projectId: '',
  ServiceAccountKey: '' )
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_NoSqlTarget USING Global.MongoDBWriter (
  AuthDB: 'admin',
  ConnectionURL: '',
  Username: '',
  collections: '',
  Password: '' )
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_FileTarget USING Global.FileWriter (
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '',
  filename: '' )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_OLTPTarget USING Global.DatabaseWriter (
  ConnectionURL: '',
  Password: '',
  Username: '',
  Tables: '' )
INPUT FROM @APPNAME@_CQOut;

CREATE SOURCE @APPNAME@_KafkaSource USING KafkaReader VERSION @KAFKA_VERSION@ (
  brokerAddress: '',
  Topic: '',
  startOffset: '0' )
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@_Stream2;

CREATE OR REPLACE TARGET @APPNAME@_KafkaFileTarget USING Global.FileWriter (
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '',
  filename: '' )
FORMAT USING JSONFormatter(
members:'data')
INPUT FROM @APPNAME@_Stream2;

END APPLICATION @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;


create application @appname@ recovery 5 second interval;

CREATE OR REPLACE SOURCE @cobolsrc@ USING FileReader (
  wildcard: '',
  positionbyeof: false,
  directory: ''
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: 'ProcessRecordAsEvent',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'SingleEvent',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'Level01',
  copybookFileName: ''
   )
OUTPUT TO @appname@Stream;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
  filename: '',
  directory: '',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter  (
  members: 'data',
  EventsAsArrayOfJsonObjects: 'true'
 )
INPUT FROM @appname@Stream;

end application @appname@;
deploy application @appname@ on all in default;
start application @appname@;

create flow AgentFlow;

CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (
  StartPosition: '@startPosition@',
  ConnectionURL: '@sourceURL@',
  Username: '@userName@',
  Tables: '@tables@',
  CheckColumn: '@checkColum@',
  FetchSize: 1,
  Password: '@password@',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

end flow AgentFlow;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING MSSqlReader
(
  Compression: false,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.test01',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
)
OUTPUT TO @APPNAME@_SS;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(id,col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'NULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true	,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

stop @appname@;
undeploy application @appname@;
DROP APPLICATION @appname@ CASCADE;
CREATE APPLICATION @appname@;

CREATE SOURCE @appname@_src USING databaseReader  (
  Username: '@@',
  Password: '@@',
  ConnectionURL: '@@',
  Tables: '@@',
  FetchSize: '100'
 )
OUTPUT TO @appname@_ss;

CREATE JUMPING WINDOW @appname@_win OVER @appname@_ss KEEP @winsize@ ROWS;

CREATE TYPE @appname@_MapType
    (   
       id INTEGER,
        name STRING,
        city  STRING
    );
    
CREATE EXTERNAL CACHE @appname@_cach (
  AdapterName: 'DatabaseReader',
    ConnectionURL: '@url@',
    UserName: '@uname@',
    Password: '@pwd@',
   Table: '@tablename@',
  FetchSize: 100,
  Columns: 'id,name,city',
  trimquote: false,
  KeyToMap: '@key@'
 )
 OF @appname@_MapType;
 
CREATE TYPE @appname@_MapTypenew
    (   id_t            INTEGER,
        name_t           STRING,
        city_t            STRING,
        id_c            INTEGER,
        name_c            STRING,
        city_c            STRING
    );
    
CREATE STREAM @appname@_JoinedData OF @appname@_MapTypenew;

CREATE CQ @appname@_JoinDataCQ
INSERT INTO @appname@_JoinedData
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_STRING(f.data[2]),
        z.id,
        z.name,
        z.city
FROM @appname@_win f, @appname@_cach z
where TO_INT(f.data[0]) = z.id
@Ex@;

CREATE TARGET @appname@_tgt USING DatabaseWriter
(
  ConnectionURL:'@@',
  Username:'@@',
  Password:'@@',
  BatchPolicy:'Eventcount:10000,Interval:1',
  CommitPolicy:'Interval:1,Eventcount:10000',
  Tables:'@@'
) 
INPUT FROM @appname@_JoinedData;

END APPLICATION @appname@;
deploy application @appname@;
start @appname@;

CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @APPNAME@_Source USING Global.MSSqlReader (
  TransactionSupport: false,
  _h_returnNumericAs: 'Double',
  Tables: 'qatest.T27342_Source',
  FetchTransactionMetadata: false,
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
  Compression: false,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Password_encrypted: 'false',
  Password: 'w3b@ct10n',
  StartPosition: 'NOW',
  adapterName: 'MSSqlReader',
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'qatest',
  Username: 'qatest',
  FetchSize: 0,
  IntegratedSecurity: false,
  FilterTransactionBoundaries: true,
  ConnectionPoolSize: 2,
  SendBeforeImage: true,
  AutoDisableTableCDC: false )
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_Target USING Global.AzureSQLDWHWriter (
  AccountName: 'testaswin',
  Tables: 'qatest.T27342_Source,dbo.test',
  Password_encrypted: 'false',
  AccountAccessKey_encrypted: 'false',
  CDDLAction: 'Process',
  StorageAccessDriverType: 'WASBS',
  ConnectionURL: 'jdbc:sqlserver://testaswin.database.windows.net:1433;database=testaswin',
  Username: 'testaswin',
  columnDelimiter: '|',
  Mode: 'MERGE',
  AccountAccessKey: 'MmlfH35Vc2mcOScbY2wnOyXol6deT8gtGA4XW3C5EXwwdFQEukP37RfHGWeUgMhfKsIvDvCHF/v3GF6frXGdYg==',
  Password: 'W3b@ct10n2020',
  uploadpolicy: 'eventcount:1,interval:5m' )
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_sysout USING Global.SysOut (
  name: 'sysout_1' )
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

STOP APPLICATION tpcc;
UNDEPLOY APPLICATION tpcc;
DROP APPLICATION tpcc CASCADE;

CREATE APPLICATION tpcc RECOVERY 5 SECOND Interval;

CREATE SOURCE Ojet_Source USING Ojet
(
    Username: '@Username@',
    Password: '@Password@',
    ConnectionURL: '@ConnectionURL@',
    Tables: '@Tables@',
)

OUTPUT TO SourceStream ;

create Target Ojet_FileWriter using FileWriter(
  filename:'qatar.csv',
  directory:'',
  flushpolicy: 'EventCount:10000,Interval:60s',
  rolloverpolicy: 'EventCount:10000,Interval:60s'
)
format using DSVFormatter (

)
input from SourceStream;

create Target t2 using SysOut(name:Foo2) input from SourceStream;

END APPLICATION tpcc;

DEPLOY APPLICATION tpcc;
START APPLICATION tpcc;

CREATE OR REPLACE APPLICATION @AppFeature@;

CREATE OR REPLACE SOURCE initialLoad_Src USING Global.DatabaseReader (
  FetchSize: 100,
  QuiesceOnILCompletion: false,
  Tables: '@SrcTable@',
  adapterName: 'DatabaseReader',
  Password: '@SrcPswd@',
  Username: '@SrcUserName@',
  ConnectionURL: '@SrcUrl@'
   )
OUTPUT TO @AppName@_Stream;

CREATE OR REPLACE TARGET initialLoadPostgres_Trg USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:100,Interval:60',
  StatementCacheSize: '50',
  ConnectionURL: '@TrgUrl@',
  Username: '@TrgUserName@',
  BatchPolicy: 'EventCount:100,Interval:60',
  Tables: '@SrcTable@,@TrgTable@',
  Password: '@TrgPswd@',
  adapterName: 'DatabaseWriter' )
INPUT FROM @AppName@_Stream;

END APPLICATION @AppName@;

use PosTester;
DROP CQ CsvToPosData;

STOP APPLICATION oraddl;
UNDEPLOY APPLICATION oraddl;
DROP APPLICATION oraddl CASCADE;
CREATE APPLICATION oraddl recovery 5 second interval;
 
Create Source Ora Using OracleReader 
(
 Username:'@user-name@',
 Password:'@password@',
 ConnectionURL:'src_url',
 Tables:'QATEST.ORACLEDDL%',
 DictionaryMode:OfflineCatalog,
 DDLCaptureMode : 'All',
 FetchSize:1
) Output To LogminerStream;

Create Target tgt using DatabaseWriter 
(
 Username:'@username@',
 Password:'@password@',
 ConnectionURL:'TGT_URL',
 BatchPolicy:'EventCount:1,Interval:1',
 CommitPolicy:'EventCount:1,Interval:1',
 IgnorableExceptionCode: '1,2290,942',
 Tables :'QATEST.ORACLEDDL%,QATEST2.%'
) input from LogminerStream;

CREATE TARGET cdcDump USING LogWriter(
name:testOuput,
directory:'/Users/abinandan/product/IntegrationTests/target/test-classes/testNG/AllTargetWriters/OracleDDLDatabaseWriter/logs',
filename:'oraclecdc.log',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING JSONFormatter ()
INPUT FROM LogminerStream;
end application oraddl;
deploy application oraddl;
start application oraddl;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

stop application @APPNAME@2;
undeploy application @APPNAME@2;
drop application @APPNAME@2 cascade;

create application @APPNAME@ recovery 1 second interval;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@'
)
OUTPUT TO @APPNAME@Stream;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;



CREATE TYPE test_typeJo (
 GG String,
CORP String,
DIV String,
YR FLOAT,
WK INT,
SOURCEID String,
JENUM INT,
ACCTPRIME String,
ACCTSUB String,
AMTTYPE String,
FAC String,
DEPT String,
SECT String,
REF String,
AMT FLOAT,
UNITAMT INT,
POSTINGDATE String,
EFFECTIVEDATE String,
SOURCEDESC String,
SEQUENCENUMBER INT,
SYSTEMN String
);

Create stream cqAsJSONNodeStreamJo of test_typeJo;

CREATE CQ cqAsJSONNodeStreamJo
INSERT into JSONNodeStreamJo
    select 
    data.get('GG'),
data.get('CORP'),
data.get('DIV'),
TO_FLOAT(data.get('YR')),
TO_INT(data.get('WK')),
data.get('SOURCE-ID'),
TO_INT(data.get('JE-NUM')),
data.get('ACCT-PRIME'),
data.get('ACCT-SUB'),
data.get('AMT-TYPE'),
data.get('FAC'),
data.get('DEPT'),
data.get('SECT'),
data.get('REF'),
TO_FLOAT(data.get('AMT')),
TO_INT(data.get('UNIT-AMT')),
data.get('POSTING-DATE'),
data.get('EFFECTIVE-DATE'),
data.get('SOURCE-DESC'),
TO_INT(data.get('SEQUENCE-NUMBER')),
data.get('SYSTEM-N')
from @APPNAME@Stream js;

create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:100,Interval:5',
  CommitPolicy: 'EventCount:100,Interval:5',
  Tables: 'QATEST.@table@'
)
input from JSONNodeStreamJo;

end application @APPNAME@;

-----------------------------------------------
create application @APPNAME@2 recovery 1 second interval;

create source @APPNAME@2_SRC Using FileReader(
	directory:'@DIRECTORY2@',
	WildCard:'@FILENAME2@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP12@',
  dataFileFont: '@PROP22@',
  copybookSplit: '@PROP32@',
  dataFileOrganization: '@PROP42@',
  copybookDialect: '@PROP52@', 
  skipIndent:'@PROP62@',
  DatahandlingScheme:'@PROP72@'
 )
OUTPUT TO @APPNAME@2Stream;

create Target @APPNAME@2Target using FileWriter(
    filename :'@FILE2@',
    directory : '@FOLDER2@'
)
format using JsonFormatter (
)
input from @APPNAME@2Stream;


CREATE TYPE test_typeRe 
(
node_new com.fasterxml.jackson.databind.JsonNode,
node_name com.fasterxml.jackson.databind.JsonNode,
node_addr com.fasterxml.jackson.databind.JsonNode
);

Create stream cqAsJSONNodeStreamRe of test_typeRe;

CREATE CQ GetPOAsJsonNodesRe
INSERT into cqAsJSONNodeStreamRe
select 
data.get('ACCTS-RECORD'),
data.get('ACCTS-RECORD').get('NAME'),
data.get('ACCTS-RECORD').get('ADDRESS3')
from @APPNAME@2Stream js;

create type finaldtypeRe
(ACCOUNT_NO int,
FIRST_NAME String,
LAST_NAME String,
ADDRESS1 String,
ADDRESS2 String,
CITY String,
STATE String,
ZIP_CODE int);

CREATE STREAM getdataStreamPS OF finaldtypeRe;

CREATE CQ getdataRe
INSERT into getdataStreamPS
select JSONGetInteger(x.node_new,"ACCOUNT-NO"),
JSONGetString(x.node_name,"FIRST-NAME"),
JSONGetString(x.node_name,"LAST-NAME"),
JSONGetString(x.node_new,"ADDRESS1"),
JSONGetString(x.node_new,"ADDRESS2"),
JSONGetString(x.node_addr,"CITY"),
JSONGetString(x.node_addr,"STATE"),
JSONGetInteger(x.node_addr,"ZIP-CODE")
from cqAsJSONNodeStreamRe x;

create Target @APPNAME@2DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:1000,Interval:50',
  CommitPolicy: 'EventCount:1000,Interval:50',
  Tables: 'QATEST.@table2@'
)
input from getdataStreamPS;

end application @APPNAME@2;
deploy application @APPNAME@2 on all in default;
deploy application @APPNAME@ on all in default;

start application @APPNAME@2;
start application @APPNAME@;

use PosTester;
DROP Source CsvDataSource;

STOP application admin.app1;
undeploy application admin.app1;
drop application admin.app1 cascade;


CREATE APPLICATION app1;

CREATE SOURCE S1 USING Global.OracleReader (
  Tables: 'QATEST.TEST01',
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  OutboundServerProcessName: 'WebActionXStream',
  Password: 'qatest',
  Compression: false,
  ReaderType: 'LogMiner',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  FetchSize: 1,
  Password_encrypted: 'false',
  SupportPDB: false,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CDDLAction: 'Process',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  DictionaryMode: 'OnlineCatalog',
  QueueSize: 2048,
  CommittedTransactions: true,
  XstreamTimeOut: 600,
  CDDLCapture: false,
  TransactionBufferType: 'Disk',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '100MB',
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  DatabaseRole: 'Primary' )
OUTPUT TO buffer;

CREATE TARGET t USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'qatest',
  ParallelThreads: '',
  DatabaseProviderType: 'Oracle',
  CheckPointTable: 'CHKPOINT',
  Password_encrypted: 'false',
  CDDLAction: 'Process',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.TEST01,QATEST.TEST02',
  CommitPolicy: 'EventCount:1000,Interval:60',
  StatementCacheSize: '50',
  Username: 'qatest',
  BatchPolicy: 'EventCount:1000,Interval:60',
  PreserveSourceTransactionBoundary: 'false' )
INPUT FROM buffer;

END APPLICATION app1;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using googlepubsubwriter(
    ServiceAccountKey:'@SAS-KEY@',
ProjectId:'@PROJECTID@',
topic:'@topic@',
BatchPolicy:'@BATCHPOLICY@'
)
format using DSVFormatter (
)
input from @STREAM@;

end application @APPNAME@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream1@RANDOM@ OF Global.waevent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc1 USING JMSReader (
  ProviderName: '',
  UserName: '',
  transactionpolicy: '',
  Provider: '',
  Ctx: '',
  EnableTransaction: '',
  QueueName: '',
  Topic:'',
  Password: '' )
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@PersistStream1@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt1 USING Global.FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING Global.DSVFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream1@RANDOM@;

CREATE STREAM @APPNAME@PersistStream2@RANDOM@ OF Global.JSONNodeEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc2 USING JMSReader (
  ProviderName: '',
  UserName: '',
  transactionpolicy: '',
  Provider: '',
  Ctx: '',
  EnableTransaction: '',
  QueueName: '',
  Topic:'',
  Password: '' )
PARSE USING JSONParser ()
OUTPUT TO @APPNAME@PersistStream2@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt2 USING Global.FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING Global.JSONFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream2@RANDOM@;

CREATE STREAM @APPNAME@PersistStream3@RANDOM@ OF Global.waevent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc3 USING JMSReader (
  ProviderName: '',
  UserName: '',
  transactionpolicy: '',
  Provider: '',
  Ctx: '',
  EnableTransaction: '',
  QueueName: '',
  Topic:'',
  Password: '' )
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@PersistStream3@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt3 USING Global.FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: ''  )
FORMAT USING Global.JSONFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream3@RANDOM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

Stop application DEV20814.ValidateFile;
Undeploy application DEV20814.ValidateFile;
Drop application DEV20814.ValidateFile cascade;

CREATE APPLICATION ValidateFile;

CREATE STREAM AlertFileStream OF Global.AlertEvent;

CREATE SUBSCRIPTION FileAlert USING WebAlertAdapter( ) INPUT FROM AlertFileStream;

CREATE STREAM jsonFileStream OF Global.JsonNodeEvent;

CREATE  SOURCE readFromFile USING FileReader  (
  blocksize: 64,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  directory:'@TEST-DATA-PATH@',
  skipbom: true,
  wildcard: 'Postgres*'
 )
 PARSE USING JSONParser  (
 )
OUTPUT TO jsonFileStream ;

CREATE OPEN PROCESSOR alertFileOP USING AlertGenerator
(
   messagePrefix:'File: ',
   severity:'info',
   flag:'notify'
)
INSERT INTO AlertFileStream
FROM jsonFileStream;

END APPLICATION ValidateFile;

alter APPLICATION DBRTOCW;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes1',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource2 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes2',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource3 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes3',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget2 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget3 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:60',
  CommitPolicy: 'EventCount:1,Interval:60',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget4 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:120',
  CommitPolicy: 'EventCount:1,Interval:120',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget5 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:120',
  CommitPolicy: 'EventCount:1,Interval:120',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  ExcludedTables:'QATEST.ORACTOCQL_ALLDATATYPES',
  Password: '+hbb060plSWQwscvI105cg==',
  Password_encrypted: true
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET FWTarget USING FileWriter(
	name:CassandraOuput,
	filename:'OracToFw.log',
	flushpolicy : 'interval:120,eventcount:11',
	rolloverpolicy : 'interval:300s'
)
FORMAT USING DSVFormatter()
INPUT FROM Oracle_ChangeDataStream;
create  or replace Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;


END APPLICATION DBRTOCW;
ALTER APPLICATION DBRTOCW RECOMPILE;
deploy application DBRTOCW in default;
start DBRTOCW;

--
-- Crash Recovery Test 3 on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW -> CQ(aggregate) -> WS
--

STOP APPLICATION N4S4CR3Tester.N4S4CRTest3;
UNDEPLOY APPLICATION N4S4CR3Tester.N4S4CRTest3;
DROP APPLICATION N4S4CR3Tester.N4S4CRTest3 CASCADE;
CREATE APPLICATION N4S4CRTest3 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest3;

CREATE SOURCE CsvSourceN4S4CRTest3 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest3;

CREATE FLOW DataProcessingN4S4CRTest3;

CREATE TYPE WactionTypeN4S4CRTest3 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount int,
  city String
);

CREATE STREAM DataStream OF WactionTypeN4S4CRTest3;

CREATE CQ CsvToDataN4S4CRTest3
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_INT(TO_DOUBLE(data[7])),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN4S4CRTest3 CONTEXT OF WactionTypeN4S4CRTest3
EVENT TYPES ( WactionTypeN4S4CRTest3 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN4S4CRTest3
INSERT INTO WactionsN4S4CRTest3
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

END FLOW DataProcessingN4S4CRTest3;

END APPLICATION N4S4CRTest3;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@;

IMPORT static com.webaction.runtime.converters.DateConverter.*;

UNDEPLOY APPLICATION admin.SQLMXReaderApp;
DROP APPLICATION admin.SQLMXReaderApp cascade;

CREATE APPLICATION SQLMXReaderApp;
create source SQMXSource using HPNonStopSQLMXReader (
	portno:2020,
	ipaddress:'10.10.196.122',
	Name:'intg',
	AuditTrails:'parallel',
	AgentPortNo:8012,
	AgentIpAddress:'10.10.197.116', 
	Tables:'watest.wasch.sqlmxtest1;watest.wasch.sqlmxtest2') output to CDCStream,
	SQLMXMATStream MAP (table:'WATEST.WASCH.SQLMXTEST2');


CREATE TYPE SQLMXTEST2Data(
    C0 Integer,
    C1 String,
    C2 Short,
    OPR String,
    TABLENAME String,
    AUXNAME String
);

CREATE STREAM SQLMXTEST2Stream OF SQLMXTEST2Data;


CREATE JUMPING WINDOW SQLMXDataWindow
OVER SQLMXTEST2Stream KEEP 4 ROWS
PARTITION BY OPR;


CREATE CQ ToSQLMXData
INSERT INTO SQLMXTEST2Stream
SELECT TO_INT(data[0]),
	   data[1],
       TO_SHORT(data[2]),
       META(x,"OperationName").toString(),
       META(x, "TableName").toString(),
       META(x,"AuditTrailName").toString()
FROM CDCStream x
WHERE not(META(x,"OperationName").toString() = "BEGIN") AND not(META(x,"OperationName").toString() = "COMMIT") AND not(META(x, "TableName").toString() is null) 
AND META(x, "TableName").toString() = "WATEST.WASCH.SQLMXTEST1" AND META(x, "AuditTrailName").toString() = "MAT";


--CREATE TARGET SQLMXSYSOUT using SysOut(name:sqlmx) INPUT FROM CDCStream;
CREATE TARGET SQLMXMAT USING LogWriter(
  name:SQLMXReaderAppMAT,
filename:'@FEATURE-DIR@/logs/sqlmxmat.log'
--  filename:'mat.log'
) INPUT FROM SQLMXMATStream;

CREATE TARGET SQLMXAUX01 USING LogWriter(
  name:SQLMXReaderAppMAT1,
filename:'@FEATURE-DIR@/logs/sqlmxmat1.log'
--  filename:'aux1.log'
) INPUT FROM SQLMXTEST2Stream;


END APPLICATION SQLMXReaderApp;
deploy application SQLMXReaderApp in default;

stop application reconnect;
undeploy application reconnect;
drop application reconnect cascade;
CREATE APPLICATION reconnect recovery 1 second interval;

CREATE  SOURCE mssqlsource USING MssqlReader  ( 
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  ConnectionURL: '@URL@',
  Tables: '@TABLE@',
  FetchSize: 1
 ) 
OUTPUT TO sqlstream;

CREATE TARGET dbtarget USING DatabaseWriter(
  ConnectionURL:'@URL@',
  Username:'@USERNAME@',
  Password:'@PASSWORD@',
  ConnectionRetryPolicy: 'retryInterval=15s,maxRetries=2',
  BatchPolicy:'EventCount:5,Interval:30',
  CommitPolicy:'EventCount:5,Interval:30',
  Tables: '@TABLES@'
 ) INPUT FROM sqlstream;

 create Target tSysOut using Sysout(name:OrgData) input from sqlstream;
 end application reconnect;
 deploy application reconnect;
 start application reconnect;

stop application app2PS;
undeploy application app2PS;
drop application app2PS cascade;

create application app2PS;

create target File_TargerPS2 using FileWriter
(
directory : '',
filename : ''
)
format using DSVFormatter()
input from KPSRss2;

end application app2PS;

deploy application app2PS;
start application app2PS;

CREATE APPLICATION @APPNAME@ USE EXCEPTIONSTORE TTL : '7d' ;

CREATE SOURCE @APPNAME@_Source USING Global.OracleReader (
  Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@', )
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_Target1 USING Global.SnowflakeWriter (
  connectionUrl: '@tgturl@',
    tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
    password: '@tgtpassword@',
    username: '@tgtusername@',
    appendOnly: 'true',
    uploadPolicy: 'eventcount:1,interval:5m',
    externalStageType: 'Local',
    adapterName: 'SnowflakeWriter' )
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_Target2 USING Global.SnowflakeWriter (
  connectionUrl: '@tgturl@',
    tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
    password: '@tgtpassword@',
    username: '@tgtusername@',
    appendOnly: 'false',
    optimizedMerge: 'false',
    uploadPolicy: 'eventcount:1,interval:5m',
    externalStageType: 'Local',
    adapterName: 'SnowflakeWriter' )
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_Target3 USING Global.SnowflakeWriter (
  connectionUrl: '@tgturl@',
    tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
    password: '@tgtpassword@',
    username: '@tgtusername@',
    optimizedMerge: 'true',
    uploadPolicy: 'eventcount:1,interval:5m',
    externalStageType: 'Local',
    adapterName: 'SnowflakeWriter' )
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

create application KinesisTest RECOVERY 1 SECOND INTERVAL;
CREATE OR REPLACE SOURCE ora_reader USING OracleReader (
  Username: 'miner',
  Password: 'miner',
  ConnectionURL: '192.168.1.113:1521:ORCL',
  TABLES: 'QATEST.H_REGION;QATEST.H_NATION;QATEST.H_CUSTOMER',
  FetchSize: '1'
 )
OUTPUT TO DDLCDCStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	accesskeyid:'ACCESS_KEY',
	secretaccesskey:'SECRET_KEY'
)
format using JSONFormatter (
)
input from DDLCDCStream;

end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using DatabaseReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application @APPNAME@Apps2;
undeploy application @APPNAME@Apps2;
drop application @APPNAME@Apps2 cascade;


stop application @APPNAME@Apps3;
undeploy application @APPNAME@Apps3;
drop application @APPNAME@Apps3 cascade;



stop application @APPNAME@Apps4;
undeploy application @APPNAME@Apps4;
drop application @APPNAME@Apps4 cascade;



stop application @APPNAME@Apps1;
undeploy application @APPNAME@Apps1;
drop application @APPNAME@Apps1 cascade;

CREATE OR REPLACE PROPERTYSET @APPNAME@Apps_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9099', kafkaversion:'0.11');

CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream1 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;
CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream3 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;
CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream4 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;


--**********************Application 1*******************
-- with 2 source flow 
-- <sourceflow1>source1->PS1<sourceflow1>
-- <sourceflow2>Source2->Inmemory1<sourceflow2>
-- Inmemomry1->cq1->PS1

CREATE APPLICATION @APPNAME@Apps1 RECOVERY 5 SECOND INTERVAL;
create flow @APPNAME@serverflow;
CREATE OR REPLACE SOURCE @APPNAME@AppsDBSource1 USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@1',
  adapterName: 'OracleReader',
  Password: '@PASSWORD@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@USERNAME@',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@Apps_PS_Stream1;
end flow @APPNAME@serverflow;

create flow @APPNAME@serverflow2;
CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream2 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;
CREATE OR REPLACE SOURCE @APPNAME@AppsDBSource2 USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@2;@SOURCE_TABLE@3',
  adapterName: 'OracleReader',
  Password: '@PASSWORD@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@USERNAME@',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@Apps_PS_InmemoryStream1;
end flow @APPNAME@serverflow2;

CREATE CQ @APPNAME@cq_Inmemory1
INSERT INTO @APPNAME@Apps_PS_Stream2
SELECT *
FROM @APPNAME@Apps_PS_InmemoryStream1;

end application @APPNAME@Apps1;

-- ********************Application 2***********************
-- PS1->Target1

CREATE APPLICATION @APPNAME@Apps2 RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE TARGET @APPNAME@Apps2DBTarget1 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:5,Interval:0',
  CommitPolicy: 'EventCount:5,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@1,@TARGET_TABLE@1',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream1;
END APPLICATION @APPNAME@Apps2;

-- ********************Application 3**************************
--WITH 3 TARGET FLOW
-- 1. <targetflow1> cq->ps3->target2 <targetflow1>
-- 2. cq->ps4 <targetflow2>ps4->target3 <targetflow2>
-- 3. <targetflow3> ps2 -> target4 <targetflow3>

CREATE APPLICATION @APPNAME@Apps3 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @APPNAME@TARGETFLOW1;
CREATE CQ @APPNAME@cq_ps1
INSERT INTO @APPNAME@Apps_PS_Stream3
SELECT *
FROM @APPNAME@Apps_PS_Stream2
WHERE META(@APPNAME@Apps_PS_Stream2,'TableName').toString() == '@SOURCE_TABLE@2';
CREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget2 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@2,@TARGET_TABLE@2',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream3;
END FLOW @APPNAME@TARGETFLOW1;

CREATE CQ @APPNAME@cq_ps2
INSERT INTO @APPNAME@Apps_PS_Stream4
SELECT *
FROM @APPNAME@Apps_PS_Stream2
WHERE META(@APPNAME@Apps_PS_Stream2,'TableName').toString() == '@SOURCE_TABLE@3';

CREATE FLOW @APPNAME@TARGETFLOW2;
CREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget3 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:1000,Interval:0',
  CommitPolicy: 'EventCount:1000,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@3,@TARGET_TABLE@3',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream4;
END FLOW @APPNAME@TARGETFLOW2;

CREATE FLOW @APPNAME@TARGETFLOW3;
CREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget4 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:50,Interval:0',
  CommitPolicy: 'EventCount:50,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@2,@TARGET_TABLE@4;@SOURCE_TABLE@3,@TARGET_TABLE@4',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream2;
END FLOW @APPNAME@TARGETFLOW3;

END APPLICATION @APPNAME@Apps3;


--********************Application 4************************
-- PS2->cq->Inmemory2 <targetflow4> Inmemory2->target5<targetflow4>
-- PS3->Target6

CREATE APPLICATION @APPNAME@Apps4 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE STREAM @APPNAME@Apps_PS_InmemoryStream2 OF Global.waevent;

CREATE CQ @APPNAME@cq_Ps_Inmemory1
INSERT INTO @APPNAME@Apps_PS_InmemoryStream2
SELECT *
FROM @APPNAME@Apps_PS_Stream2
WHERE META(@APPNAME@Apps_PS_Stream2,'TableName').toString() == '@SOURCE_TABLE@3';

create flow @APPNAME@targetflow4;
CREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget5 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:10,Interval:0',
  CommitPolicy: 'EventCount:10,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@3,@TARGET_TABLE@5',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_InmemoryStream2;
end flow @APPNAME@targetflow4;

CREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget6 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@2,@TARGET_TABLE@6',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream3;

END APPLICATION @APPNAME@Apps4;

deploy application @APPNAME@Apps4 on ANY in Appsdg with @APPNAME@Targetflow4 in Targetdg;
start application @APPNAME@Apps4;

deploy application @APPNAME@Apps3 on ANY in Appsdg with @APPNAME@Targetflow3 in Targetdg,@APPNAME@Targetflow2 in Targetdg,@APPNAME@Targetflow1 in Targetdg;
start application @APPNAME@Apps3;

deploy application @APPNAME@Apps2 on ANY in default;
start application @APPNAME@Apps2;

deploy application @APPNAME@Apps1 in Appsdg with @APPNAME@serverflow in sourcedg, @APPNAME@serverflow2 in sourcedg2;
start application @APPNAME@Apps1;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using PostgreSQLReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

CREATE  SOURCE @QuerySource@1 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@1;
create Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;
 CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@1;

CREATE  SOURCE @QuerySource@2 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@2;
create Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;
 CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@2;

CREATE  SOURCE @QuerySource@3 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@3;
create Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;
 CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@3;

CREATE  SOURCE @QuerySource@4 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@4;
create Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;
 CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@4;

CREATE  SOURCE @QuerySource@5 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@5;
create Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;
 CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@5;

CREATE  SOURCE @QuerySource@6 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@6;
create Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;
 CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@6;

CREATE  SOURCE @QuerySource@7 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@7;
create Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;
 CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@7;

CREATE  SOURCE @QuerySource@8 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@8;
create Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;
 CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@8;

CREATE  SOURCE @QuerySource@9 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@9;
create Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;
 CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@9;

CREATE  SOURCE @QuerySource@10 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@10;
create Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;
 CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@10;
end application @APPNAME@;

create application HPTippingLog;
create source DHCPLogSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'hp*',
	charset:'UTF-8',
	positionByEOF:false 
) PARSE USING HPTippingPointLogParser (
) OUTPUT TO HPLogStream;
create Target HPDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/hp_log') input from HPLogStream;
end application HPTippingLog;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (
  ConnectionURL: '@CONN_URL@',
  Tables: '@TABLES@',
  Username: '@USERNAME@',
  Password: '@PASSWORD@'
) OUTPUT TO @APP_NAME@_Stream;


CREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.FabricDataWarehouseWriter (
  Tables: '@TABLES@',
  ConnectionURL: '@CONN_URL@',
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  uploadpolicy: 'eventcount:1',
  AccountName: '@ACCOUNTNAME@')
INPUT FROM @APP_NAME@_Stream;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ USE EXCEPTIONSTORE TTL : '7d' ;

CREATE SOURCE @APPNAME@_DBSource USING DatabaseReader (
  Tables: '"qatest"."dbo"."emp"',
  Username: 'qatest',
  Password: '5wZ8jZNAU1dzU0bPbxhATA==',
  FetchSize: 10000,
  Password_encrypted: 'false',
  QuiesceOnILCompletion: 'true',
  DatabaseProviderType: 'SQLSERVER',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;DatabaseName=qatest' )
OUTPUT TO @APPNAME@_OutputStream;

CREATE OR REPLACE TARGET @APPNAME@_Target USING Global.BigQueryWriter (
  ColumnDelimiter: '|',
  NullMarker: 'NULL',
  streamingUpload: 'false',
  projectId: 'striimqa-214712',
  Encoding: 'UTF-8',
  batchPolicy: 'eventcount:10,interval:60',
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30',
  AllowQuotedNewLines: 'false',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  TransportOptions: 'connectionTimeout=300, readTimeout=120',
  adapterName: 'BigQueryWriter',
  Mode: 'APPENDONLY',
  ServiceAccountKey: 'Platform/UploadedFiles/google-gcs.json',
  StandardSQL: 'true',
  includeInsertId: 'true',
  QuoteCharacter: '\"',
  Tables: '"qatest"."dbo"."%",DEV_30875.%' )
INPUT FROM @APPNAME@_OutputStream;

END APPLICATION @APPNAME@;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.56.101:1521/orcl',
  Tables: 'QATEST.oracle_200',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;


create stream xferredDataStream1 of Global.WAEvent;

CREATE CQ CQ1
insert into xferredDataStream1
select  putuserdata (data1,'ID', data[0]) from Oracle_ChangeDataStream data1;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:4,Interval:60',
  CommitPolicy: 'EventCount:4,Interval:60',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  IgnorableExceptionCode:'PRIMARY KEY',
  Tables: 'QATEST.oracle_200,test.cassandra_200 columnmap(field1=field1,field2=field2,field3=field3,field4=field4,field5=field5,field6=field6,field7=field7,field8=field8,field9=field9,field10=field10,field11=field11,field12=field12,field13=field13,field14=field14,field15=field15,field16=field16,field17=field17,field18=field18,field19=field19,field20=field20,field21=field21,field22=field22,field23=field23,field24=field24,field25=field25,field26=field26,field27=field27,field28=field28,field29=field29,field30=field30,field31=field31,field32=field32,field33=field33,field34=field34,field35=field35,field36=field36,field37=field37,field38=field38,field39=field39,field40=field40,field41=field41,field42=field42,field43=field43,field44=field44,field45=field45,field46=field46,field47=field47,field48=field48,field49=field49,field50=field50,field51=field51,field52=field52,field53=field53,field54=field54,field55=field55,field56=field56,field57=field57,field58=field58,field59=field59,field60=field60,field61=field61,field62=field62,field63=field63,field64=field64,field65=field65,field66=field66,field67=field67,field68=field68,field69=field69,field70=field70,field71=field71,field72=field72,field73=field73,field74=field74,field75=field75,field76=field76,field77=field77,field78=field78,field79=field79,field80=field80,field81=field81,field82=field82,field83=field83,field84=field84,field85=field85,field86=field86,field87=field87,field88=field88,field89=field89,field90=field90,field91=field91,field92=field92,field93=field93,field94=field94,field95=field95,field96=field96,field97=field97,field98=field98,field99=field99,field100=field100,field101=field101,field102=field102,field103=field103,field104=field104,field105=field105,field106=field106,field107=field107,field108=field108,field109=field109,field110=field110,field111=field111,field112=field112,field113=field113,field114=field114,field115=field115,field116=field116,field117=field117,field118=field118,field119=field119,field120=field120,field121=field121,field122=field122,field123=field123,field124=field124,field125=field125,field126=field126,field127=field127,field128=field128,field129=field129,field130=field130,field131=field131,field132=field132,field133=field133,field134=field134,field135=field135,field136=field136,field137=field137,field138=field138,field139=field139,field140=field140,field141=field141,field142=field142,field143=field143,field144=field144,field145=field145,field146=field146,field147=field147,field148=field148,field149=field149,field150=field150,field151=@METADATA(OperationName),field152=field1,field153=@METADATA(SQLRedoLength),field154=@METADATA(SEQUENCE),field155=@METADATA(SegmentName),field156=@METADATA(OperationType),field157=@METADATA(TxnUserID),field158=@METADATA(ThreadID),field159=@METADATA(TxnUserID),field160=@USERDATA(ID),field161=@USERDATA(ID),field162=@USERDATA(ID),field163=@USERDATA(ID),field164=field3,field165=field150,field166=field151)',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM xferredDataStream1;

create Target t2 using SysOut(name:Foo2) input from xferredDataStream1;

END APPLICATION DBRTOCW;

deploy application DBRTOCW in  default;

start application DBRTOCW;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @APPNAME@_src1 Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
   _h_ReturnDateTimeAs:'ZonedDateTime'
) Output To @APPNAME@_stream1;

Create Source @APPNAME@_src2 Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
   _h_ReturnDateTimeAs:'ZonedDateTime'
) Output To @APPNAME@_stream2;


create or replace stream @APPNAME@_combined_stream OF Global.WAEvent;

Create CQ @APPNAME@CQ1
insert into @APPNAME@_combined_stream
select
* from @APPNAME@_stream1;

Create CQ @APPNAME@CQ2
insert into @APPNAME@_combined_stream
select
* from @APPNAME@_stream2;



create Target @APPNAME@_tgt1 using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:100'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_combined_stream;

create Target @APPNAME@_tgt2 using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:100'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@',
	members: 'Table=@metadata(TableName),OpName=@metadata(OperationName)'
)
input from @APPNAME@_stream2;

create Target @APPNAME@_tgt3 using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:100'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_stream1;

create type @APPNAME@_type(
id int,
name String,
cost float,
TableName string,
OperationName String
);

create or replace stream @APPNAME@_typed_Stream of @APPNAME@_type;

Create CQ @APPNAME@_TypedCQ
insert into @APPNAME@_typed_Stream
select
to_int(data[0]),data[1],to_float(data[2]),
meta(@APPNAME@_stream2,'TableName'),
Meta(@APPNAME@_stream2,'OperationName') from @APPNAME@_stream2;


create Target @APPNAME@_tgt4 using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:100'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_typed_Stream;




end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

--
-- Kafka Stream Recovery Test 1
-- Bert Hashemi and Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> KS -> WS

STOP KStreamAvroRecov1Tester.KStreamAvroRecovTest1;
UNDEPLOY APPLICATION KStreamAvroRecov1Tester.KStreamAvroRecovTest1;
DROP APPLICATION KStreamAvroRecov1Tester.KStreamAvroRecovTest1 CASCADE;
DROP USER KStreamAvroRecov1Tester;
DROP NAMESPACE KStreamAvroRecov1Tester CASCADE;
CREATE USER KStreamAvroRecov1Tester IDENTIFIED BY KStreamAvroRecov1Tester;
-- GRANT 'Global:create,drop:deploymentgroup:*' TO USER KStreamAvroRecov1Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamAvroRecov1Tester;
CONNECT KStreamAvroRecov1Tester KStreamAvroRecov1Tester;

CREATE APPLICATION KStreamAvroRecovTest1 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250', dataformat:'avro');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE or REPLACE TYPE KafkaCsvStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF KafkaCsvStreamType 
EVENT TYPES ( KafkaCsvStreamType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

END APPLICATION KStreamAvroRecovTest1;

--
-- Recovery Multi Node Test 1 with uniqe objects to this App
-- Nicholas Keene, Bert Hashemi WebAction, Inc.
--
-- App with Single Source, CQ and Waction Store to be deployed on two or more node
-- Full app on node1 and one Source on node2 so source reads from node2 only.
--
-- S -> CQ -> WS
--


CREATE APPLICATION RecovTestMN01
RECOVERY 5 SECOND INTERVAL;



CREATE FLOW DataAcquisition;

CREATE SOURCE CsvSourceMN01 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamMN01;

END FLOW DataAcquisition;



CREATE FLOW DataProcessing;

CREATE TYPE WactionTypeMN01 (
  merchantId String,
  dateTime DateTime,
  amount double,
  city String,
  serverName String KEY
);

CREATE STREAM DataStreamMN01 OF WactionTypeMN01
PARTITION BY serverName;

CREATE CQ CsvToDataMN01
INSERT INTO DataStreamMN01
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10],
    data[11]
FROM CsvStreamMN01;

CREATE WINDOW DataWindowMN01
OVER DataStreamMN01 KEEP WITHIN 30 SECOND ON dateTime
PARTITION BY serverName;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionTypeMN01
EVENT TYPES ( WactionTypeMN01 )
PERSIST EVERY 1 second USING (
JDBC_DRIVER:'@WASTORE-DRIVER@',  JDBC_URL:'@WASTORE-URL@;CREATE=true',
JDBC_USER:'@WASTORE-UNAME@', JDBC_PASSWORD:'@WASTORE-PASSWORD@', pu_name:@WASTORE-TYPE@,
DDL_GENERATION:'create-or-extend-tables',  LOGGING_LEVEL:'SEVERE' );

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    *
FROM DataWindowMN01;

END FLOW DataProcessing;



END APPLICATION RecovTestMN01;


DEPLOY APPLICATION RecovTestMN01 WITH DataAcquisition ON ALL IN agents, DataProcessing ON ALL IN servers;

START RecovTestMN01;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade; 

create application @APPNAME@;

create or replace type @APPNAME@emp_type(
Sno integer,
Empname string,
Doj string,
Country string,
CompanyName string
);

CREATE OR REPLACE SOURCE @APPNAME@File_SOURCE1 using Filereader(
	directory:'@DIRECTORY@',
  wildcard:'File_empdata.csv',
  positionByEOF:false
)parse using dsvParser(
    header:'yes'
)
OUTPUT TO @APPNAME@File_Stream1,
OUTPUT TO @APPNAME@File_Stream1_automap MAP(filename:'File_empdata.csv');

CREATE OR REPLACE SOURCE @APPNAME@Init_Source1 USING DatabaseReader  ( 
  Username: '@SRC-USER@',
  Password_encrypted: false,
  ConnectionURL: '@SRC-URL@',
  Tables: 'QATEST.EMP_INIT',
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SRC-PASS@'
 ) 
OUTPUT TO 	@APPNAME@InitialLoad_Stream1;

CREATE OR REPLACE SOURCE @APPNAME@CDC_Source1 USING OracleReader  ( 
  StartTimestamp: 'null',
  SupportPDB: false,
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  FetchSize: 1,
  DDLCaptureMode: 'All',
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@SRC-URL@',
  Tables: 'QATEST.EMP_CDC',
  adapterName: 'OracleReader',
  Password: '@SRC-USER@',
  TransactionBufferType: 'Memory',
  DictionaryMode: 'OnlineCatalog',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SRC-USER@',
  TransactionBufferSpilloverSize: '1MB',
  compression: true,
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@CDC_Stream1 ;

create or replace stream @APPNAME@FileSource_cdc_init_TypedStream of @APPNAME@emp_type;
create or replace cq @APPNAME@file_typed_streamcq 
insert into @APPNAME@FileSource_cdc_init_TypedStream 
SELECT to_int(data[0]),
data[1],
data[2],
data[3],
data[4]
from @APPNAME@File_Stream1;

create or replace cq @APPNAME@cdc_typed_streamcq 
insert into @APPNAME@FileSource_cdc_init_TypedStream 
SELECT to_int(data[0]),
data[1],
data[2],
data[3],
data[4]
from @APPNAME@CDC_Stream1;

create or replace cq @APPNAME@init_typed_streamcq 
insert into @APPNAME@FileSource_cdc_init_TypedStream 
SELECT to_int(data[0]),
data[1],
data[2],
data[3],
data[4]
from @APPNAME@InitialLoad_Stream1;

CREATE OR REPLACE TARGET @APPNAME@sap_target1 USING DatabaseWriter  ( 
  DatabaseProviderType:'SAPHANA',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT-USER@',
  Password_encrypted: 'false',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TGT-URL@',
  Tables: 'QA.FILE_EMP',
  adapterName: 'DatabaseWriter',
  IgnorableExceptionCode: '301',
  Password: '@TGT-PASS@'
 ) 
INPUT FROM @APPNAME@File_Stream1_automap;

CREATE OR REPLACE TARGET @APPNAME@sap_target2 USING DatabaseWriter  ( 
  DatabaseProviderType:'SAPHANA',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT-USER@',
  Password_encrypted: 'false',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TGT-URL@',
  Tables: 'QATEST.EMP,QA.CDC_EMP',
  adapterName: 'DatabaseWriter',
  --IgnorableExceptionCode: '301',
  Password: '@TGT-PASS@'
 ) 
INPUT FROM @APPNAME@InitialLoad_Stream1;

CREATE OR REPLACE TARGET @APPNAME@sap_target3 USING DatabaseWriter  ( 
  DatabaseProviderType:'SAPHANA',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT-USER@',
  Password_encrypted: 'flase',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TGT-URL@',
  Tables: 'QATEST.EMP_INIT,QA.INITIALLOAD_EMP',
  adapterName: 'DatabaseWriter',
  --IgnorableExceptionCode: '301',
  Password: '@TGT-PASS@'
 ) 
INPUT FROM @APPNAME@CDC_Stream1;


CREATE OR REPLACE TARGET @APPNAME@sap_target4 USING DatabaseWriter  ( 
  DatabaseProviderType:'SAPHANA',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT-USER@',
  Password_encrypted: 'false',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TGT-URL@',
  Tables: 'QA.FILE_INIT_CDC_EMP',
  adapterName: 'DatabaseWriter',
  IgnorableExceptionCode: '301',
  Password: '@TGT-PASS@'
 ) 
INPUT FROM @APPNAME@FileSource_cdc_init_TypedStream;

create or replace target @APPNAME@sys_file_tgt using sysout(
name:'foo_file'
)input from @APPNAME@FileSource_Stream1;

create or replace target @APPNAME@sys_cdc_tgt using sysout(
name:'foo_cdc'
)input from @APPNAME@CDC_Stream1;

create or replace target @APPNAME@sys_init_tgt using sysout(
name:'foo_init'
)input from @APPNAME@InitialLoad_Stream1;

End Application @APPNAME@;


deploy application @APPNAME@;
start application @APPNAME@;

create application KinesisTest;
CREATE OR REPLACE SOURCE ora_reader USING OracleReader (
  Username: 'miner',
  Password: 'miner',
  ConnectionURL: '192.168.1.113:1521:ORCL',
  TABLES: 'QATEST.H_REGION;QATEST.H_NATION;QATEST.H_CUSTOMER',
  FetchSize: '1'
 )
OUTPUT TO DDLCDCStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	accesskeyid:'ACCESS_KEY',
	secretaccesskey:'SECRET_KEY'
)
format using JSONFormatter (
)
input from DDLCDCStream;

end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

--
-- Crash Recovery Test 5 with Jumping window and partitioned on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION N4S4CR5Tester.N4S4CRTest5;
UNDEPLOY APPLICATION N4S4CR5Tester.N4S4CRTest5;
DROP APPLICATION N4S4CR5Tester.N4S4CRTest5 CASCADE;
CREATE APPLICATION N4S4CRTest5 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest5;

CREATE SOURCE CsvSourceN4S4CRTest5 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest5;

CREATE FLOW DataProcessingN4S4CRTest5;

CREATE TYPE CsvDataN4S4CRTest5 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvDataN4S4CRTest5 PARTITION BY merchantId;

CREATE CQ CsvToDataN4S4CRTest5
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsN4S4CRTest5 CONTEXT OF CsvDataN4S4CRTest5
EVENT TYPES ( CsvDataN4S4CRTest5 )
@PERSIST-TYPE@

CREATE CQ DataToWactionN4S4CRTest5
INSERT INTO WactionsN4S4CRTest5
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN4S4CRTest5;

END APPLICATION N4S4CRTest5;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW ;

 

CREATE OR REPLACE SOURCE DBSource USING DatabaseReader  ( 
  Username: '@SOURCE_USER@',
  Password_encrypted: false,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Query: "SELECT * FROM qatest.MssqlToCql_alldatatypes",
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SOURCE_PASS@'
 ) 
OUTPUT TO Oracle_ChangeDataStream;


CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1000,Interval:0',
  CommitPolicy: 'EventCount:1000,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

CREATE OR REPLACE APPLICATION @AppName@ USE EXCEPTIONSTORE TTL : '7d';

Create Source @AppName@_source Using OracleReader(
  Username:'@username@',
  Password:'@password@',
  ConnectionURL:'@url@',
  Tables:'@srctableName@',
  Fetchsize:1
)
Output To @AppName@_Stream;
CREATE OR REPLACE TARGET @AppName@_Target USING Global.SnowflakeWriter (
  streamingUpload: 'false',
  useConnectionProfile:'true',
  connectionProfileName: '@CP@',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  columnDelimiter: '|',
  tables: '@srctableName@,@trgtableName@',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:1000,interval:60s',
  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',
  externalStageType: 'Local',
  adapterName: 'SnowflakeWriter',
  fileFormatOptions: 'null_if = \"\"' )
INPUT FROM @AppName@_Stream;
END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

--
-- Recovery Test 33 with two sources, two sliding time windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> St1W/p -> CQ1 -> WS
-- S2 -> St2W/p -> CQ2 -> WS
--

STOP KStreamRecov33Tester.KStreamRecovTest33;
UNDEPLOY APPLICATION KStreamRecov33Tester.KStreamRecovTest33;
DROP APPLICATION KStreamRecov33Tester.KStreamRecovTest33 CASCADE;

DROP USER KStreamRecov33Tester;
DROP NAMESPACE KStreamRecov33Tester CASCADE;
CREATE USER KStreamRecov33Tester IDENTIFIED BY KStreamRecov33Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov33Tester;
CONNECT KStreamRecov33Tester KStreamRecov33Tester;

CREATE APPLICATION KStreamRecovTest33 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 1 SECOND
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 2 SECOND
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION KStreamRecovTest33;

stop application GGTrailReaderApp;
undeploy application GGTrailReaderApp;
drop application GGTrailReaderApp cascade;

create application GGTrailReaderApp recovery 5 second interval;

create source GGTrailSource using GGTrailReader (
tRaildIrectory:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario1',
tRAilfilepattern:'n1*',
positionByEOF:false,
FilterTransactionBoundaries: true,
DefinitionFile:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario1/Scn1_beforeddl.def',
captureCDdl: true,
CDDLAction:'Process',
--CDDLAction:'Ignore',
TrailByTeOrder:'LittleEndian',
recoveryInterval: 5
)
OUTPUT TO GGTrailStream;

create Target t2 using SysOut(name:Foo2) input from GGTrailStream;

CREATE TARGET WriteCDCOracle1 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost/orcl',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1,Interval:1',
CommitPolicy:'Eventcount:1,Interval:1',
Checkpointtable:'RGRN_CHKPOINT',
Tables:'QATEST.GGDDL1,QATEST.GGDDL1_TGT'
) INPUT FROM GGTrailStream1;


end application GGTrailReaderApp;

deploy application GGTrailReaderApp;
start application GGTrailReaderApp;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;

Create Source @SourceName@ Using OracleReader

(
  Username:'c##qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',
  Tables:'CDB$ROOT."C##QATEST".ojet_src;ORCLPDB.QATEST.ojet_src',
  _h_useClassic:false,
  Fetchsize:1,
  Compression: true,
  SupportPDB:true,
  ReplicationSlotName:'null'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',
  Username:'c##qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'CDB$ROOT."C##QATEST".ojet_src,CDB$ROOT."C##QATEST".ojet_tgt'
) INPUT FROM @SRCINPUTSTREAM@;



create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

CREATE SOURCE @SOURCE_NAME@ USING Global.FileReader (
  positionbyeof: false )
PARSE USING Global.DSVParser (
 )
OUTPUT TO @STREAM@;

CREATE TARGET @SOURCE_NAME@_sysout USING Global.SysOut (
  name: '@SOURCE_NAME@_sysout' )
INPUT FROM @STREAM@;

CREATE APPLICATION  @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE  @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'JsonNodeEvent.json',
positionByEOF:false
)
PARSE USING Global.JSONParser (
 )  OUTPUT TO  @AppName@_rawstream;

CREATE CQ @BuiltinFunc@CQ
INSERT INTO  @BuiltinFunc@_Stream
SELECT @BuiltinFunc@(x, 'Sno', data.get("_id"), 'Name', data.get("firstname"))
FROM @AppName@_rawstream x;

CREATE OR REPLACE CQ cq1
INSERT INTO ClearUserData_Stream
SELECT
clearUserData(s1)
FROM @BuiltinFunc@_Stream s1;

CREATE OR REPLACE TARGET  @AppName@_FileTarget USING Global.FileWriter (
  flushpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
 directory: '@logs@',
  filename: '@BuiltinFunc@_JsonEventClearData',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  (
  handler: 'com.webaction.proc.JSONFormatter',
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  formatterName: 'JSONFormatter',
  jsonobjectdelimiter: '\n' )
INPUT FROM ClearUserData_Stream;

End application  @AppName@;
Deploy application  @AppName@;
Start application  @AppName@;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

create Target FileTarget using FileWriter(
    rolloverpolicy:'@UPLOAD-SIZE@',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using XMLFormatter (
charset:'@charset@',
rootelement:'@mem@'
)
input from CsvStream;

end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

--
-- Recovery Test 26 with two sources, two jumping attribute windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W -> CQ1 -> WS
-- S2 -> Ja6W -> CQ2 -> WS
--

STOP Recov26Tester.RecovTest26;
UNDEPLOY APPLICATION Recov26Tester.RecovTest26;
DROP APPLICATION Recov26Tester.RecovTest26 CASCADE;
CREATE APPLICATION RecovTest26 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION RecovTest26;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING CassandraWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @SourceName@ USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 ) 
OUTPUT TO @SRCINPUTSTREAM@ ;


CREATE OR REPLACE TARGET @targetsys@ USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM @SRCINPUTSTREAM@;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

--
-- Recovery Test 27 with two sources, two jumping time windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jt1W -> CQ1 -> WS
--   S2 -> Jt2W -> CQ2 -> WS
--

STOP KStreamRecov27Tester.KStreamRecovTest27;
UNDEPLOY APPLICATION KStreamRecov27Tester.KStreamRecovTest27;
DROP APPLICATION KStreamRecov27Tester.KStreamRecovTest27 CASCADE;
DROP USER KStreamRecov27Tester;
DROP NAMESPACE KStreamRecov27Tester CASCADE;
CREATE USER KStreamRecov27Tester IDENTIFIED BY KStreamRecov27Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov27Tester;
CONNECT KStreamRecov27Tester KStreamRecov27Tester;

CREATE APPLICATION KStreamRecovTest27 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream1Second
OVER DataStream1 KEEP WITHIN 1 SECOND;

CREATE JUMPING WINDOW DataStream2Second
OVER DataStream2 KEEP WITHIN 2 SECOND;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data1ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream1Second p;

CREATE CQ Data2ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream2Second p;

END APPLICATION KStreamRecovTest27;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE @SOURCE@ USING Ojet  (
  FilterTransactionBoundaries: true,
  ConnectionURL: '@OCI-URL@',
  Tables: '@SOURCE_TABLE@',
  Password: '@OJET-PASSWORD@',
  fetchsize: 1,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Username: '@OJET-UNAME@'
 )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@1 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'false',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@2 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'false',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'true',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@3 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'true',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

--
-- Recovery Test 50 is a simple Kafka test
-- Nicholas Keene WebAction, Inc.
--
-- S -> CQ -> KSu -> JWc10 -> WS
--

STOP Recov50Tester.RecovTest50;
UNDEPLOY APPLICATION Recov50Tester.RecovTest50;
DROP APPLICATION Recov50Tester.RecovTest50 CASCADE;
CREATE APPLICATION RecovTest50 RECOVERY 5 SECOND INTERVAL;

CREATE or REPLACE TYPE KafkaType(
  value java.lang.Long KEY  
);

CREATE SOURCE KafkaSource USING NumberSource ( 
  lowValue: '1',
  highValue: '1003',
  delayMillis: '10',
  delayNanos: '0',
  repeat: 'false'
 ) 
OUTPUT TO NumberSourceOut;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaStream OF KafkaType using KafkaProps;

CREATE OR REPLACE CQ KafkaStreamPopulate 
INSERT INTO KafkaStream
SELECT data[1]
FROM NumberSourceOut;

CREATE JUMPING WINDOW SizeTenWindow
OVER KafkaStream KEEP 10 ROWS;


CREATE WACTIONSTORE Wactions CONTEXT of KafkaType
@PERSIST-TYPE@

CREATE CQ WactionsPopulate
INSERT INTO Wactions
SELECT * FROM SizeTenWindow;

END APPLICATION RecovTest50;

--
-- Recovery Test 20 with two sources going to one wactionstore
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> CQ1 -> WS
-- S2 -> CQ2 -> WS
--

STOP KStreamRecov20Tester.KStreamRecovTest20;
UNDEPLOY APPLICATION KStreamRecov20Tester.KStreamRecovTest20;
DROP APPLICATION KStreamRecov20Tester.KStreamRecovTest20 CASCADE;
DROP USER KStreamRecov20Tester;
DROP NAMESPACE KStreamRecov20Tester CASCADE;
CREATE USER KStreamRecov20Tester IDENTIFIED BY KStreamRecov20Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov20Tester;
CONNECT KStreamRecov20Tester KStreamRecov20Tester;

CREATE APPLICATION KStreamRecovTest20 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions1
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream1;

CREATE CQ InsertWactions2
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream2;

END APPLICATION KStreamRecovTest20;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE application @APPNAME@ @Recovery@ AUTORESUME MAXRETRIES 2 RETRYINTERVAL 10;

create type @APPNAME@type1(
  companyName String,
  merchantId String,
  city string
);

create type @APPNAME@type2(
  c1 integer,
  c2 String,
  c3 string
);

create type @APPNAME@type3(
c1 integer
);

create type @APPNAME@type4(
c1 integer,
c2 integer
);

create stream @APPNAME@in_memory_typedStream of @APPNAME@type1 partition by city;
create stream @APPNAME@in_memory_typedStream_num of @APPNAME@type2;
create stream @APPNAME@in_memory_typedStream_num1 of @APPNAME@type2;
create stream @APPNAME@in_memory_typedStream_num2 of @APPNAME@type2;
create stream @APPNAME@in_memory_typedStream_num3 of @APPNAME@type2;
create stream @APPNAME@in_memory_typedStream_num4 of @APPNAME@type2;
create stream @APPNAME@in_memory_typedStream_num5 of @APPNAME@type2;
create stream @APPNAME@finalstream6 of @APPNAME@type4;

create source @APPNAME@s using FileReader (
        directory:'Product/IntegrationTests/TestData/',
        wildcard:'posdata5L.csv',
        positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:',',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  header: true,
  separator:'~'

)
OUTPUT TO @APPNAME@in_memory_rawStream;


create CQ @APPNAME@cq1
INSERT INTO @APPNAME@kps_waevent
SELECT *
FROM @APPNAME@in_memory_rawStream  ;

create CQ @APPNAME@cq2
INSERT INTO @APPNAME@in_memory_typedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", ""),
TO_STRING(data[1]),
TO_STRING(data[10])
FROM @APPNAME@kps_waevent ;

create CQ @APPNAME@cq3
INSERT INTO @APPNAME@in_memory_typedStream_num1
SELECT TO_INT(companyName) as c1, merchantId as c2,city as c3
FROM @APPNAME@in_memory_typedStream;
-- order by c3;

create CQ @APPNAME@cq4
INSERT INTO @APPNAME@in_memory_typedStream_num2
SELECT TO_INT(companyName) as c1, merchantId as c2,city as c3
FROM @APPNAME@in_memory_typedStream;

create CQ @APPNAME@cq5
INSERT INTO @APPNAME@in_memory_typedStream_num3
SELECT TO_INT(companyName) as c1, merchantId as c2,city as c3
FROM @APPNAME@in_memory_typedStream;

create CQ @APPNAME@cq6
INSERT INTO @APPNAME@in_memory_typedStream_num4
SELECT TO_INT(companyName) as c1, merchantId as c2,city as c3
FROM @APPNAME@in_memory_typedStream;

create CQ @APPNAME@cq7
INSERT INTO @APPNAME@in_memory_typedStream_num5
SELECT TO_INT(companyName) as c1, merchantId as c2,city as c3
FROM @APPNAME@in_memory_typedStream;

CREATE CQ @APPNAME@cq8
INSERT INTO @APPNAME@in_memory_typedStream_num6
SELECT TO_INT(companyName) as c1
FROM @APPNAME@in_memory_typedStream;


CREATE JUMPING WINDOW @APPNAME@DataStream1_100000Rows
OVER @APPNAME@in_memory_typedStream_num1 KEEP 100000 ROWS;


CREATE JUMPING WINDOW @APPNAME@DataStream2_100000Rows
OVER @APPNAME@in_memory_typedStream_num2 KEEP 100000 ROWS;


CREATE JUMPING WINDOW @APPNAME@DataStream3_100000Rows
OVER @APPNAME@in_memory_typedStream_num3 KEEP 100000 ROWS;


CREATE JUMPING WINDOW @APPNAME@DataStream4_100000Rows
OVER @APPNAME@in_memory_typedStream_num4 KEEP 100000 ROWS;


CREATE JUMPING WINDOW @APPNAME@DataStream5_100000Rows
OVER @APPNAME@in_memory_typedStream_num5 KEEP 100000 ROWS;

CREATE JUMPING WINDOW @APPNAME@DataStream6_100000Rows
OVER @APPNAME@in_memory_typedStream_num6 KEEP 100000 ROWS;

create CQ @APPNAME@cq9
INSERT INTO @APPNAME@finalstream1
SELECT c1 FROM @APPNAME@DataStream1_100000Rows sample by c1;

create CQ @APPNAME@cq10
INSERT INTO @APPNAME@finalstream2
SELECT c1 FROM @APPNAME@DataStream2_100000Rows sample by c1 selectivity 0.1;

create CQ @APPNAME@cq11
INSERT INTO @APPNAME@finalstream3
SELECT c1 FROM @APPNAME@DataStream3_100000Rows sample by c1 selectivity 0.25;


create CQ @APPNAME@cq12
INSERT INTO @APPNAME@finalstream4
SELECT c1 FROM @APPNAME@DataStream4_100000Rows sample by c1 selectivity 0.05;
--SELECT count(*) FROM @APPNAME@DataStream4Rows10000Seconds sample by c1 selectivity 0.05;

create CQ @APPNAME@cq13
INSERT INTO @APPNAME@finalstream5
SELECT c1 FROM @APPNAME@DataStream5_100000Rows sample by c1 selectivity 0.01;

create CQ @APPNAME@cq14
INSERT INTO @APPNAME@finalstream6
SELECT c1,c1 as c2 FROM @APPNAME@DataStream6_100000Rows sample by c1,c2 selectivity 0.01;

create target @APPNAME@target1 using filewriter (
filename:'FEATURE-DIR/logs/@APPNAME@target1.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using dsvFormatter()
input from @APPNAME@finalstream1;

create target @APPNAME@target2 using filewriter (
filename:'FEATURE-DIR/logs/@APPNAME@target2.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using dsvFormatter()
input from @APPNAME@finalstream2;

create target @APPNAME@target3 using filewriter (
filename:'FEATURE-DIR/logs/@APPNAME@target3.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using dsvFormatter()
input from @APPNAME@finalstream3;

create target @APPNAME@target4 using filewriter (
filename:'FEATURE-DIR/logs/@APPNAME@target4.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using dsvFormatter()
input from @APPNAME@finalstream4;

create target @APPNAME@target5 using filewriter (
filename:'FEATURE-DIR/logs/@APPNAME@target5.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using dsvFormatter()
input from @APPNAME@finalstream5;

CREATE WACTIONSTORE @APPNAME@Wactions1 CONTEXT OF @APPNAME@type3
EVENT TYPES ( @APPNAME@type2 )
USING ( storageProvider:'elasticsearch' );

CREATE WACTIONSTORE @APPNAME@Wactions2 CONTEXT OF @APPNAME@type3
EVENT TYPES ( @APPNAME@type2 )
USING ( storageProvider:'elasticsearch' );

CREATE WACTIONSTORE @APPNAME@Wactions3 CONTEXT OF @APPNAME@type3
EVENT TYPES ( @APPNAME@type2 )
USING ( storageProvider:'elasticsearch' );

CREATE WACTIONSTORE @APPNAME@Wactions4 CONTEXT OF @APPNAME@type4
EVENT TYPES ( @APPNAME@type4 )
USING ( storageProvider:'elasticsearch' );

CREATE WACTIONSTORE @APPNAME@Wactions5 CONTEXT OF @APPNAME@type4
EVENT TYPES ( @APPNAME@type4 )
USING ( storageProvider:'elasticsearch' );

--sampling twice: one in finalstream1 and another in select query.
CREATE CQ @APPNAME@cq15
INSERT INTO @APPNAME@Wactions1
SELECT FIRST(p.c1) FROM @APPNAME@finalstream1 p GROUP BY p.c1 sample by p.c1 ;

--sampling once: results will be same as target2 and target1.
CREATE CQ @APPNAME@cq16
INSERT INTO @APPNAME@Wactions2
SELECT * from @APPNAME@finalstream1 order by c1 desc limit 10000 ;

--sampling twice: one in finalstream1 and another in select query.
CREATE CQ @APPNAME@cq17
INSERT INTO @APPNAME@Wactions3
SELECT * from @APPNAME@finalstream1 order by c1 sample by c1;

--sampling using 2 fields, 2800 for single field and 332 for 2 field
CREATE CQ @APPNAME@cq18
INSERT INTO @APPNAME@Wactions4
SELECT c1,c1 from @APPNAME@finalstream6 order by c1 sample by c1;

--same as Wactions4 - here selectivity alone varies, so output is 8
CREATE CQ @APPNAME@cq19
INSERT INTO @APPNAME@Wactions5
SELECT c1,c1 from @APPNAME@finalstream6 order by c1 sample by c1 selectivity 0.0001;

end application @APPNAME@;
deploy application @APPNAME@;
--start @APPNAME@;

stop application BigqueryBulkLoadMonMetrics_cdc;
undeploy application BigqueryBulkLoadMonMetrics_cdc;
drop application BigqueryBulkLoadMonMetrics_cdc cascade;

CREATE APPLICATION BigqueryBulkLoadMonMetrics_cdc;

CREATE FLOW BigqueryBulkLoadMonMetrics_cdc_SourceFlow;

CREATE SOURCE BigqueryBulkLoadMonMetrics_cdc_DBSource USING oracleReader ( 
  Username: 'qatest', 
  FetchSize: 10000, 
  Password_encrypted: 'false', 
  Password: 'JVaLv3ZpgQDY8R2ZxS38xg==', 
  Tables: 'QATEST.EMPLOYEE', 
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe' ) 
OUTPUT TO BigqueryBulkLoadMonMetrics_cdc_OutputStream;

END FLOW BigqueryBulkLoadMonMetrics_cdc_SourceFlow;

CREATE OR REPLACE TARGET BigqueryBulkLoadMonMetrics_cdc_BigQueryTarget1 USING BigQueryWriter ( 
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  streamingUpload: 'false', 
  projectId: 'striimqa-214712', 
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:100000, Interval:90', 
   AllowQuotedNewLines: 'false', 
  optimizedMerge: 'false', 
  TransportOptions: 'connectionTimeout=300, readTimeout=120', 
  adapterName: 'BigQueryWriter', 
  Mode: 'MERGE', 
  StandardSQL: 'true', 
  includeInsertId: 'true', 
  QuoteCharacter: '\"', 
  ServiceAccountKey: '/Users/jenniffer/Product2/IntegrationTests/TestData/google-gcs.json' ) 
INPUT FROM BigqueryBulkLoadMonMetrics_cdc_OutputStream;

END APPLICATION BigqueryBulkLoadMonMetrics_cdc;

deploy application BigqueryBulkLoadMonMetrics_cdc;
start application BigqueryBulkLoadMonMetrics_cdc;

CREATE APPLICATION SourceFraudApp;

CREATE SOURCE FraudCsvDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:no,
  wildcard:'fraudPosData.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO FraudCsvStream;

CREATE TARGET FraudSourceDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceFraudAppData') input from FraudCsvStream;

CREATE SOURCE FraudZipLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  header: no,
  columndelimiter: '	',
  positionByEOF:false
) OUTPUT TO FraudCacheSource1;

CREATE TARGET FraudCacheDump1 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceFraudCacheData1') input from FraudCacheSource1;

CREATE SOURCE FraudNameLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'MerchantNames.csv',
  header: no,
  columndelimiter: ',',
  positionByEOF:false
) OUTPUT TO FraudCacheSource2;

CREATE TARGET FraudCacheDump2 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceFraudCacheData2') input from FraudCacheSource2;

CREATE SOURCE FraudCustomerLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'customerdetails.csv',
  header: no,
  columndelimiter: ',',
  positionByEOF:false
) OUTPUT TO FraudCacheSource3;

CREATE TARGET FraudCacheDump3 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceFraudCacheData3') input from FraudCacheSource3;


END APPLICATION SourceFraudApp;

undeploy application GCSWriterTest;
alter application GCSWriterTest;

CREATE OR REPLACE SOURCE OracleSource USING OracleReader  (
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: '@SOURCE_TABLES@',
  FetchSize: 1
 ) Output To OracleStream;

 create or replace Target OracleGCSTarget using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@',
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from OracleStream;
end application GCSWriterTest;
alter application GCSWriterTest recompile;
deploy application GCSWriterTest;
start application GCSWriterTest;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;


create Target DBRTOCW_t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;

deploy application DBRTOCW on ANY in default;

start application DBRTOCW;

CREATE APPLICATION  @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE  @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'JsonNodeEvent.json',
positionByEOF:false
)
PARSE USING Global.JSONParser (
 )  OUTPUT TO  @AppName@_rawstream;


CREATE CQ @BuiltinFunc@CQ
INSERT INTO  @BuiltinFunc@_Stream
SELECT @BuiltinFunc@(x, 'Sno', data.get("_id"), 'Name', data.get("firstname"))
FROM @AppName@_rawstream x;

CREATE OR REPLACE TARGET  @AppName@_FileTarget USING Global.FileWriter (
  flushpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
 directory: '@logs@',
  filename: '@BuiltinFunc@_JsonNodeEventData',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  (
  handler: 'com.webaction.proc.JSONFormatter',
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  formatterName: 'JSONFormatter',
  jsonobjectdelimiter: '\n' )
INPUT FROM @BuiltinFunc@_Stream;

End application  @AppName@;
Deploy application  @AppName@;
Start application  @AppName@;

--
-- Recovery Test 4
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS
-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS
--

STOP Recov4Tester.RecovTest4;
UNDEPLOY APPLICATION Recov4Tester.RecovTest4;
DROP APPLICATION Recov4Tester.RecovTest4 CASCADE;
CREATE APPLICATION RecovTest4 RECOVERY 50 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream OF CsvData;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION RecovTest4;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.test01'
 ) 
OUTPUT TO @APPNAME@_SS;


CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'public.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12)',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ recovery 5 second Interval;
CREATE SOURCE @srcName@ USING PostgreSQLReader  

(
  ReplicationSlotName:'@srcreplicationslot@',
  FilterTransactionBoundaries:'true',
  Username:'@srcusername@',
  Password_encrypted:false,
  ConnectionURL: '@srcurl@',
  adapterName:'PostgreSQLReader',
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  Password:'@srcpassword@',
  Tables:'@srcschema@.@srctable@'
) 
OUTPUT TO @outstreamname@ ;

CREATE OR REPLACE TARGET @tgtName@ USING DatabaseWriter  

(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'@tgtusername@',
  BatchPolicy:'EventCount:1,Interval:0',
  CommitPolicy:'EventCount:1,Interval:0',
  ConnectionURL:'@tgturl@',
  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
  Password:'@tgtpassword@'
) 
INPUT FROM @instreamname@;
End APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

--
-- Recovery Test 28 with two sources, two jumping time-count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5t9W  -> CQ1 -> WS
--   S2 -> Jc6t11W -> CQ2 -> WS
--

STOP Recov28Tester.RecovTest28;
UNDEPLOY APPLICATION Recov28Tester.RecovTest28;
DROP APPLICATION Recov28Tester.RecovTest28 CASCADE;
CREATE APPLICATION RecovTest28 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION RecovTest28;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE OR REPLACE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;

CREATE OR REPLACE SOURCE @SOURCE@ USING SalesForceReader (
  autoAuthTokenRenewal: 'true',
  Username: '@userName@',
  securityToken: '@securityToken@',
  sObjects: '@srcObjectName@',
  pollingInterval: '1 min',
  Password_encrypted: 'false',
  securityToken_encrypted: 'false',
  customObjects: 'False',
  consumerKey: '@consumerKey@',
  startTimestamp: '',
  apiEndPoint: 'https://ap2.salesforce.com',
  mode: 'Incremental',
  consumerSecret: '@consumerSecert@',
  consumerSecret_encrypted: 'false',
  Password: '@Password@' )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@ USING Global.BigQueryWriter (
  ColumnDelimiter: '|',
  NullMarker: 'NULL',
  ConnectionRetryPolicy: 'retryInterval=30,\n maxRetries=3',
  streamingUpload: 'false',
  Mode: 'Merge',
  projectId: '@ProjectId@',
  Encoding: 'UTF-8',
  TransportOptions: 'connectionTimeout=300,\n readTimeout=120',
  Tables: '@srcObjectName@,@TargetTableName@ columnmap(ID=ID,checkbool__c=checkbool__c,dt__c=dt__c,percnt__c=percnt__c,phn__c=phn__c,txtlong__c=txtlong__c,url1__c=url1__c);',
  AllowQuotedNewlines: 'false',
  CDDLAction: 'Process',
  adapterName: 'BigQueryWriter',
  serviceAccountKey: '@GCS-AuthPath@',
  optimizedMerge: 'true',
  StandardSQL: 'true',
  includeInsertId: 'true',
  QuoteCharacter: '\"',
  BatchPolicy: 'eventCount:1000,Interval:10' )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

stop application MSSQLTransactionSupportMSSQLToMySQL1;
undeploy application MSSQLTransactionSupportMSSQLToMySQL1;
drop application MSSQLTransactionSupportMSSQLToMySQL1 cascade;

CREATE APPLICATION MSSQLTransactionSupportMSSQLToMySQL1 recovery 1 second interval;

Create Source ReadFromMSSQL6
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
FetchTransactionMetadata:'true',
FilterTransactionBoundaries: true,
Compression:'false',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportMSSQLToMySQL1Stream;


CREATE TARGET WriteToMySQL6 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC@,@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportMSSQLToMySQL1Stream;

CREATE TARGET MSSqlReaderOutput6 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportMSSQLToMySQL1Stream; 


CREATE OR REPLACE TARGET MSSQLFileOut6 USING FileWriter  ( 
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  directory:'@FEATURE-DIR@/logs/',
  filename: 'TransactionSupportMSSQLToMySQL.txt'
 ) 
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 ) 
INPUT FROM MSSQLTransactionSupportMSSQLToMySQL1Stream;

END APPLICATION MSSQLTransactionSupportMSSQLToMySQL1;
deploy application MSSQLTransactionSupportMSSQLToMySQL1;
start application MSSQLTransactionSupportMSSQLToMySQL1;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE APPLICATION @APPNAME@ WITH ENCRYPTION @Recovery@ USE EXCEPTIONSTORE;
create flow @APPNAME@_SourceFlow;
CREATE SOURCE @APPNAME@_s USING FileReader(
  directory:'Samples/AppData',
  wildcard:'PO.JSON',
  positionByEOF:false
)
parse using JSONParser (
) OUTPUT TO @APPNAME@_ss1;
end flow @APPNAME@_SourceFlow;
create target @APPNAME@_t using sysout (name:ss1) input from @APPNAME@_ss1;
end application @APPNAME@;
deploy application @APPNAME@ @DP@;
start @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@ Using Ojet
(
Username:'@OJET-UNAME@',
Password:'@OJET-PASSWORD@',
ConnectionURL:'@OCI-URL@',
Tables:'@SourceTable@',
)
Output To @SRCINPUTSTREAM@;
CREATE OR REPLACE STREAM Rstream1 OF Global.WAEvent;
CREATE OR REPLACE OPEN PROCESSOR Open_Processor1 USING PartialRecordPolicy
(
Username: 'qatest_ojet',
Password: 'qatest_ojet',
ConnectionURL: 'jdbc:oracle:oci:@//localhost:1525/orcl',
Tables: 'QATEST.Ojetsrc(ROWIDCOL)'
)
INSERT INTO Rstream1
FROM @SRCINPUTSTREAM@;
CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
Username:'@UN@',
Password:'@PWD@',
BatchPolicy:'EventCount:1,Interval:1',
Tables: '@Tablemapping@'
)INPUT FROM Rstream1;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING S3Reader (
    bucketname:'',
    objectnameprefix:'',
     )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING AvroFormatter  (
schemaFileName: 'AvroFileSchema'
)
INPUT FROM @appname@CqOut;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using AvroFormatter (
schemaFileName: 'AvroS3Schema'
)
input from @appname@CqOut;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using AvroFormatter (
schemaFileName: 'AvroAzureSchema'
)
INPUT FROM @appname@CqOut;

CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using AvroFormatter (
schemaFileName: 'AvroGCSSchema'
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ recovery 5 second Interval;
create source @srcName@ USING MySQLReader
(
  Username:'@srcusername@',
  Password:'@srcpassword@',
  ConnectionURL:'@srcurl@',
  Tables:'@srcschema@.@srctable@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries:'true'
) 
OUTPUT TO @outstreamname@;

CREATE OR REPLACE TARGET @tgtName@ USING DatabaseWriter
(
  CheckPointTable:'CHKPOINT',
  Username:'@tgtusername@',
  Password:'@tgtpassword@',
  BatchPolicy:'EventCount:1,Interval:0',
  CommitPolicy:'EventCount:1,Interval:0',
  ConnectionURL:'@tgturl@',
  Tables:'@srcschema@.@srctable@,@tgtschema@.@tgttable@'
) 
INPUT FROM @instreamname@;


End APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

CREATE Source dataGenSrc Using PostgreSQLReader  ( 
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: true,
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://127.0.0.1:5432/webaction?stringtype=unspecified',
  Tables:'@tableNames@',
  adapterName: 'PostgreSQLReader',
  Password: 'xFzvJYZf1b8=',
  Password_encrypted: 'true'
 ) 
Output To LCRStream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;

END APPLICATION DataGenSampleApp;

Stop Teradata_LogWriter;
Undeploy application Teradata_LogWriter;
drop application Teradata_LogWriter cascade;

CREATE APPLICATION Teradata_LogWriter WITH ENCRYPTION recovery 5 second interval;

CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.TDSOURCE',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.TEST01=ID;',
  PollingInterval: '5sec',
  ReturnDateTimeAs: 'String',
  startPosition:'striim.test01=0'
  )
  OUTPUT TO data_stream;

  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

CREATE TARGET BinaryDump USING LogWriter(
  name: 'TeraData',
  filename:'TeraData.log'
)INPUT FROM data_stream;

END APPLICATION Teradata_LogWriter;

deploy application Teradata_LogWriter in default;

start application Teradata_LogWriter;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

CREATE SOURCE @APPNAME@_Source USING MSSqlReader
(
  Compression: false,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.test01',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
)
OUTPUT TO @APPNAME@_stream;

CREATE TARGET @APPNAME@_Target USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	BatchPolicy: 'EventCount: 1; Interval: 1s',
	instanceId: 'qatest'
) INPUT FROM @APPNAME@_stream;

create Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_stream;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( 
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: '@CDC_URL@',
  Tables: '@Source1Tables@',
  FetchSize: 1) 
OUTPUT TO @APPNAME@cdcStream;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@1 ( 
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  FetchSize: 20,
  DatabaseProviderType: 'Default',
  Table: '@Source3Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')  
OF @APPNAME@cachetype;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@2 ( 
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  FetchSize: 10,
  DatabaseProviderType: 'Default',
  Table: '@Source2Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')  
OF @APPNAME@cachetype;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING DataBaseReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

END APPLICATION @APPNAME@;

stop application APP_KAFKASOURCE_AGG;
undeploy application APP_KAFKASOURCE_AGG;
alter application APP_KAFKASOURCE_AGG;

CREATE OR REPLACE CQ CQ_CALCULATE_HOURLY_TOTAL
INSERT INTO STREAM_CQ_CALCULATE_HOURLY_TOTAL
SELECT f.topic as topic, sum(f.rawdatacount) as TotalLast24hour, B.rawdatacount as TotalLast1hour FROM JUMP_WND_1EVT_1MIN h
       join SLIDE_WND_HOURLYTOTALS_KAFKADATA_FILE f on 1=1
       join (SELECT rawdatacount, topic,timerange from ET_HOURLYTOTALS_KAFKADATA_FILE,JUMP_WND_1EVT_30SEC where timerange = DHOURS(DNOW())-1) B on B.topic=f.topic
       Group by f.topic;

alter application APP_KAFKASOURCE_AGG recompile;

CREATE  SOURCE @SOURCE_NAME@ USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@1;

CREATE  SOURCE @SOURCE_NAME@2 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@2;

CREATE  SOURCE @SOURCE_NAME@3 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@3;

CREATE  SOURCE @SOURCE_NAME@4 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@4;

CREATE  SOURCE @SOURCE_NAME@5 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@5;

CREATE  SOURCE @SOURCE_NAME@6 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@6;

CREATE  SOURCE @SOURCE_NAME@7 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@7;

CREATE  SOURCE @SOURCE_NAME@8 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@8;

CREATE  SOURCE @SOURCE_NAME@9 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@9;

CREATE  SOURCE @SOURCE_NAME@10 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@10;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@ Using Ojet
(
Username:'@OJET-UNAME@',
Password:'@OJET-PASSWORD@',
ConnectionURL:'@OCI-URL@',
Tables:'@SourceTable@',
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
Username:'@UN@',
Password:'@PWD@',
BatchPolicy:'EventCount:1,Interval:1',
Tables: '@Tablemapping@'
)INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP APPLICATION KafkaWPTester.KWApp;
STOP APPLICATION KafkaWPTester.KRApp;
UNDEPLOY APPLICATION KafkaWPTester.KWApp;
UNDEPLOY APPLICATION KafkaWPTester.KRApp;
DROP APPLICATION KafkaWPTester.KWApp CASCADE;
DROP APPLICATION KafkaWPTester.KRApp CASCADE;

CREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;
GRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;
CONNECT KafkaWPTester KafkaWPTester;


CREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE CSVSource USING FileReader (
	directory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',
    WildCard:'access_log_half',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;

CREATE TYPE AccessLogType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;

CREATE CQ AceeslogCQ
INSERT INTO TypedAccessLogStream
SELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]
FROM FileStream;

create Target DsvWriter using KafkaWriter VERSION '2.1.0' ( 
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V2dsvExistTopic',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream;


create Target JsonWriter using KafkaWriter VERSION '2.1.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V2jsonExistTopic',
	Mode:'Sync'
    )
 
FORMAT USING JSONFormatter ()
input from TypedAccessLogStream;


create Target AvroWriter using KafkaWriter VERSION '2.1.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V2avroExistTopic',
    Mode:'Sync'
    )
 
FORMAT USING AvroFormatter (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')
input from TypedAccessLogStream;


END APPLICATION KWApp;

DEPLOY APPLICATION KWApp on any in default;


-- Kafka Reader Apps

CREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;


CREATE TYPE AccessLogType2(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE Stream KafkaDSVReaderStream of AccessLogType2;
CREATE Stream KafkaJsonReaderStream of AccessLogType2;
CREATE Stream KafkaAvroReaderStream of AccessLogType2;


CREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V2dsvExistTopic',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;




CREATE CQ DSVtoKafkaDSVReaderStream
INSERT INTO KafkaDSVReaderStream
SELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]
FROM KafkaReaderStream1;

CREATE TARGET DSVDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V2_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaDSVReaderStream;

CREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V2jsonExistTopic',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;




CREATE CQ DSVtoKafkaJsonReaderStream
INSERT INTO KafkaJsonReaderStream
SELECT 
data.get('Col1').textValue(),
data.get('Col2').textValue(),
data.get('Col3').textValue(),
data.get('Col4').textValue(),
data.get('Col5').textValue(),
data.get('Col6').textValue(),
data.get('Col7').textValue(),
data.get('Col8').textValue(),
data.get('Col9').textValue(),
data.get('Col10').textValue()
FROM KafkaReaderStream2;

CREATE TARGET JSONDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V2_RT_JSON')
FORMAT USING DSVFormatter()
INPUT FROM KafkaJsonReaderStream;

CREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V2avroExistTopic',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'
)
OUTPUT TO KafkaReaderStream3;



CREATE Stream KRTypedStream of AccessLogType;

CREATE CQ DSVtoKafkaAvroReaderStream
INSERT INTO KafkaAvroReaderStream
SELECT    
data.get("Col1").toString(), 
data.get("Col2").toString(), 
data.get("Col3").toString(), 
data.get("Col4").toString(),
data.get("Col5").toString(), 
data.get("Col6").toString(), 
data.get("Col7").toString(), 
data.get("Col8").toString(),
data.get("Col9").toString(), 
data.get("Col10").toString()
FROM KafkaReaderStream3;

CREATE TARGET AVRODump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V2_RT_AVRO')
FORMAT USING DSVFormatter()
INPUT FROM KafkaAvroReaderStream;

END APPLICATION KRApp;
DEPLOY APPLICATION KRApp on any in default;

--
-- Recovery Test 41 with two sources, two jumping count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5W/p -> CQ1 -> WS
--   S2 -> Jc6W/p -> CQ2 -> WS
--

STOP KStreamRecov41Tester.KStreamRecovTest41;
UNDEPLOY APPLICATION KStreamRecov41Tester.KStreamRecovTest41;
DROP APPLICATION KStreamRecov41Tester.KStreamRecovTest41 CASCADE;
DROP USER KStreamRecov41Tester;
DROP NAMESPACE KStreamRecov41Tester CASCADE;
CREATE USER KStreamRecov41Tester IDENTIFIED BY KStreamRecov41Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov41Tester;
CONNECT KStreamRecov41Tester KStreamRecov41Tester;

CREATE APPLICATION KStreamRecovTest41 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION KStreamRecovTest41;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.OJet (
  PrimaryDatabasePassword: '@PRIMARY_PASSWORD@',
  ConnectionURL: '@DOWNSTREAM_URL@',
  PrimaryDatabaseUsername: '@PRIMARY_USER@',
  Password: '@DOWNSTREAM_PASSWORD@',
  DownstreamCaptureMode: 'REAL_TIME',
  DownstreamCapture: true,
  PrimaryDatabaseConnectionURL: '@PRIMARY_URL@',
  Tables: '@SOURCE_TABLES@',
  CDDLCapture: true,
  Username: '@DOWNSTREAM_USER@' )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET_SYS@ USING Global.SysOut (
  name: 'Out' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter (
  ConnectionURL: '@TARGET_URL@',
  Username: '@TARGET_USER@',
  Password: '@TARGET_PASSWORD@',
  CheckPointTable: 'CHKPOINT',
  CommitPolicy: 'EventCount:1',
  Tables: '@TARGET_TABLES@',
  BatchPolicy: 'EventCount:1' )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

--
-- Crash Recovery Test 3 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW -> CQ(aggregate) -> WS
--

STOP APPLICATION N2S2CR3Tester.N2S2CRTest3;
UNDEPLOY APPLICATION N2S2CR3Tester.N2S2CRTest3;
DROP APPLICATION N2S2CR3Tester.N2S2CRTest3 CASCADE;
CREATE APPLICATION N2S2CRTest3 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest3;

CREATE SOURCE CsvSourceN2S2CRTest3 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN2S2CRTest3;

CREATE FLOW DataProcessingN2S2CRTest3;

CREATE TYPE WactionTypeN2S2CRTest3 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount int,
  city String
);

CREATE STREAM DataStream OF WactionTypeN2S2CRTest3;

CREATE CQ CsvToDataN2S2CRTest3
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_INT(TO_DOUBLE(data[7])),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN2S2CRTest3 CONTEXT OF WactionTypeN2S2CRTest3
EVENT TYPES ( WactionTypeN2S2CRTest3 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN2S2CRTest3
INSERT INTO WactionsN2S2CRTest3
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

END FLOW DataProcessingN2S2CRTest3;

END APPLICATION N2S2CRTest3;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application MSSQLTransactionSupportFTMFTBTrue;
undeploy application MSSQLTransactionSupportFTMFTBTrue;
drop application MSSQLTransactionSupportFTMFTBTrue cascade;

CREATE APPLICATION MSSQLTransactionSupportFTMFTBTrue recovery 1 second interval;

Create Source ReadFromMSSQL2
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
FetchTransactionMetadata:'true',
FilterTransactionBoundaries: true,
Compression:'false',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportFTMFTBTrueStream;


CREATE TARGET WriteToMSSQL2 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC@,@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportFTMFTBTrueStream;

CREATE TARGET MSSqlReaderOutput2 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportFTMFTBTrueStream; 


CREATE OR REPLACE TARGET MSSQLFileOut2 USING FileWriter  ( 
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  directory:'@FEATURE-DIR@/logs/',
  filename: 'TransactionSupportFTMFTBTrue.txt'
 ) 
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 ) 
INPUT FROM MSSQLTransactionSupportFTMFTBTrueStream;

END APPLICATION MSSQLTransactionSupportFTMFTBTrue;
deploy application MSSQLTransactionSupportFTMFTBTrue;
start application MSSQLTransactionSupportFTMFTBTrue;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using MSSqlReader
(
 Username:'@UserName@',
 Password:'@Password@',
 DatabaseName:'@DatabaseName@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'@SourceTable@',
 ConnectionPoolSize:1,
 StartPosition:'EOF'
 ) Output To @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;

create Target @targetFile@ using FileWriter(
  filename:'TestOut.log',
  directory:'@FileDirectoryPath@',
  rolloverpolicy:'eventcount:1000'
)
format using DSVFormatter (

)
input from @SRCINPUTSTREAM@;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;

CREATE OR REPLACE APPLICATION @AppName@;

CREATE SOURCE @AppName@_MssqlSource USING MSSqlReader
(
   Username:'@userName@',
  Tables: '@tableName@', 
  Password:'@password@',
 DatabaseName:'qatest',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
 AutoDisableTableCDC:false,
 connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5'
)
OUTPUT TO @AppName@_Stream;

CREATE OR REPLACE CQ @AppName@_Cq
INSERT INTO InputData
select userdata, metadata,
CASE WHEN meta(h,'OperationName').toString() = 'DELETE' THEN data(h) ELSE before(h) END AS before , 
CASE WHEN meta(h,'OperationName').toString() = 'DELETE' THEN NULL ELSE data(h) END AS data,
CASE WHEN meta(h,'OperationName').toString() = 'UPDATE' THEN data(h) ELSE before(h) END AS before ,
CASE WHEN meta(h,'OperationName').toString() = 'INSERT' THEN data(h) ELSE before(h) END AS before
from  @AppName@_Stream  h where meta(h,'TableName').tostring() = 'qatest.MetaDataTable';;


CREATE OR REPLACE TARGET @AppName@_FileWriterTarget USING Global.FileWriter ( 
  flushpolicy: 'EventCount:10000,Interval:30s', 
  adapterName: 'FileWriter', 
  directory: '@logsDir@',
  filename: '@FileName@', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING Global.JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter', 
  jsonMemberDelimiter: '\n', 
  EventsAsArrayOfJsonObjects: 'true', 
  formatterName: 'JSONFormatter', 
  jsonobjectdelimiter: '\n' ) 
INPUT FROM InputData;

END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;


CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using OracleReader
(
  Compression:true,
  StartTimestamp:'null',
  FetchSize:1,
  CommittedTransactions:true,
  QueueSize:2048,
  FilterTransactionBoundaries:true,
  Password_encrypted:'false',
  SendBeforeImage:true,
  XstreamTimeOut:600,
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE',
  adapterName:'OracleReader',
  Password:'qatest',
  DictionaryMode:'OfflineCatalog',
  FilterTransactionState:true,
  connectionRetryPolicy:'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType:'LogMiner',
  Username:'qatest',
  OutboundServerProcessName:'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic:true,
  CDDLAction:'Quiesce_Cascade',
  CDDLCapture:'true'
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING BigqueryWriter  
(
  serviceAccountKey:'/Users/hariharasudhan/Downloads/google-gcs.json',
  projectId:'striimqa-214712',
  datalocation:'US',
  Tables:'public.dbr_pg234567890123456789source1,public.dbr_pg234567890123456789Target1;public.dbr_pg234567890123456789source2,public.dbr_pg234567890123456789Target2',
  BatchPolicy:"eventCount:1,Interval:90",
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;

drop application dropDeployApp cascade;
create application dropDeployApp;
deploy application admin.dropDeployApp on any in default;
drop application dropDeployApp;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create or replace propertyvariable ms_pass='@SOURCE_PASS@';
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE @SOURCE@ USING MSSQLReader  ( 
  FilterTransactionBoundaries: true,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Password: '$ms_pass',
  Password_encrypted: 'true',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Username: '@SOURCE_USER@',
  DatabaseName: 'qatest',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@ USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;


END APPLICATION @APPNAME@;

deploy application @APPNAME@ on ANY in default;

start application @APPNAME@;

stop application @APPNAME@Apps2;
undeploy application @APPNAME@Apps2;
drop application @APPNAME@Apps2 cascade;


stop application @APPNAME@Apps3;
undeploy application @APPNAME@Apps3;
drop application @APPNAME@Apps3 cascade;



stop application @APPNAME@Apps4;
undeploy application @APPNAME@Apps4;
drop application @APPNAME@Apps4 cascade;



stop application @APPNAME@Apps1;
undeploy application @APPNAME@Apps1;
drop application @APPNAME@Apps1 cascade;

CREATE OR REPLACE PROPERTYSET @APPNAME@Apps_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9099', kafkaversion:'0.11');

CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream1 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;
CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream2 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;
CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream3 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;
CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream4 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;


--**********************Application 1*******************
-- with 2 agent flow 
-- <agentflow1>source1->PS1<agentflow1>
-- <agentflow2>Source2->Inmemory1->cq1->PS2<agentflow2>

CREATE APPLICATION @APPNAME@Apps1 RECOVERY 5 SECOND INTERVAL;
create flow @APPNAME@agentflow;
CREATE OR REPLACE SOURCE @APPNAME@AppsDBSource1 USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@1',
  adapterName: 'OracleReader',
  Password: '@PASSWORD@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@USERNAME@',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@Apps_PS_Stream1;
end flow @APPNAME@agentflow;

create flow @APPNAME@agentflow2;
CREATE OR REPLACE SOURCE @APPNAME@AppsDBSource2 USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@2',
  adapterName: 'OracleReader',
  Password: '@PASSWORD@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@USERNAME@',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@Apps_PS_InmemoryStream1;

CREATE CQ @APPNAME@cq_Inmemory1
INSERT INTO @APPNAME@Apps_PS_Stream2
SELECT *
FROM @APPNAME@Apps_PS_InmemoryStream1;
end flow @APPNAME@agentflow2;

end application @APPNAME@Apps1;

-- ********************Application 2***********************
-- with 2 agent flow 
-- <agentflow3>source3->inmemory2<agentflow3>inmemory->cq2->ps3
-- <agentflow4>Source2->Inmemory3<agentflow4><serverflow1>inmemory->cq3->ps4<serverflow1>

CREATE APPLICATION @APPNAME@Apps2 RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@agentflow3;
CREATE OR REPLACE SOURCE @APPNAME@AppsDBSource3 USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@3',
  adapterName: 'OracleReader',
  Password: '@PASSWORD@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@USERNAME@',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@Apps_PS_InmemoryStream2;

CREATE CQ @APPNAME@cq_Inmemory2
INSERT INTO @APPNAME@Apps_PS_Stream3
SELECT *
FROM @APPNAME@Apps_PS_InmemoryStream2;

end flow @APPNAME@agentflow3;

create flow @APPNAME@agentflow4;
CREATE OR REPLACE SOURCE @APPNAME@AppsDBSource4 USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@4',
  adapterName: 'OracleReader',
  Password: '@PASSWORD@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@USERNAME@',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@Apps_PS_InmemoryStream3;

end flow @APPNAME@agentflow4;

create  flow @APPNAME@serverflow;
CREATE CQ @APPNAME@cq_Inmemory3
INSERT INTO @APPNAME@Apps_PS_Stream4
SELECT *
FROM @APPNAME@Apps_PS_InmemoryStream3;
end flow @APPNAME@serverflow;
END APPLICATION @APPNAME@Apps2;

-- ********************Application 3**************************
--WITH 3 TARGET FLOW
-- 1. <targetflow1> ps1->target1 <targetflow1>
-- 2. <targetflow2>ps2->target2 <targetflow2>
-- 3. <targetflow3> ps1,ps2->cq4 -> target3 <targetflow3>

CREATE APPLICATION @APPNAME@Apps3 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @APPNAME@TARGETFLOW1;
CREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget1 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@1,@TARGET_TABLE@1',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream1;
END FLOW @APPNAME@TARGETFLOW1;

CREATE FLOW @APPNAME@TARGETFLOW2;
CREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget2 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@2,@TARGET_TABLE@2',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream2;
END FLOW @APPNAME@TARGETFLOW2;

CREATE OR REPLACE STREAM @APPNAME@Apps_PS_InmemoryStream4 OF Global.waevent;

CREATE CQ @APPNAME@cq_ps1
INSERT INTO @APPNAME@Apps_PS_InmemoryStream4
SELECT *
FROM @APPNAME@Apps_PS_Stream1;

CREATE CQ @APPNAME@cq_ps2
INSERT INTO @APPNAME@Apps_PS_InmemoryStream4
SELECT *
FROM @APPNAME@Apps_PS_Stream2;

CREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget2 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@1,@TARGET_TABLE@5;@SOURCE_TABLE@2,@TARGET_TABLE@5',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_InmemoryStream4;

END APPLICATION @APPNAME@Apps3;


--********************Application 4************************
--WITH 3 TARGET FLOW
-- 1. <targetflow1> ps3->target4 <targetflow1>
-- 2. <targetflow2>ps4->target5 <targetflow2>
-- 3. <targetflow3> ps3,ps4->cq5 -> target6 <targetflow3>

CREATE APPLICATION @APPNAME@Apps4 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @APPNAME@TARGETFLOW3;
CREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget4 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@3,@TARGET_TABLE@3',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream3;
END FLOW @APPNAME@TARGETFLOW3;

CREATE FLOW @APPNAME@TARGETFLOW4;
CREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget5 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@4,@TARGET_TABLE@4',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream4;
END FLOW @APPNAME@TARGETFLOW4;

CREATE OR REPLACE STREAM @APPNAME@Apps_PS_InmemoryStream5 OF Global.waevent;

CREATE CQ @APPNAME@cq_ps3
INSERT INTO @APPNAME@Apps_PS_InmemoryStream5
SELECT *
FROM @APPNAME@Apps_PS_Stream3;

CREATE CQ @APPNAME@cq_ps4
INSERT INTO @APPNAME@Apps_PS_InmemoryStream5
SELECT *
FROM @APPNAME@Apps_PS_Stream4;


CREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget6 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@3,@TARGET_TABLE@6;@SOURCE_TABLE@4,@TARGET_TABLE@6',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_InmemoryStream5;

END APPLICATION @APPNAME@Apps4;

deploy application @APPNAME@Apps4 on ANY in Appsdg with @APPNAME@Targetflow3 in Targetdg1,@APPNAME@Targetflow4 in Targetdg;
start application @APPNAME@Apps4;

deploy application @APPNAME@Apps3 on ANY in Appsdg with @APPNAME@Targetflow2 in Targetdg1,@APPNAME@Targetflow1 in Targetdg;
start application @APPNAME@Apps3;

deploy application @APPNAME@Apps2 in Appsdg with @APPNAME@agentflow3 in agents, @APPNAME@agentflow4 in agents2, @APPNAME@serverflow in default;
start application @APPNAME@Apps2;

deploy application @APPNAME@Apps1 in Appsdg with @APPNAME@agentflow in agents, @APPNAME@agentflow2 in agents2;
start application @APPNAME@Apps1;

Stop application appname;
undeploy application appname;
drop application appname cascade;

create application appname use exceptionstore;

  


CREATE SOURCE OracleSource USING OracleReader  ( 
Username: 'qatest', 
  Tables: 'QATEST.TEST01', 
  FetchSize: 100, 
  Password_encrypted: 'false', 
  ConnectionURL: 'localhost:1521:xe', 
  DictionaryMode: 'OnlineCatalog', 
  queuesize: 25000, 
  Password: 'qatest'
  ) 
OUTPUT TO SS_oracle;

CREATE TYPE CDCtestnMapType
    (   id INTEGER,
        name STRING,
        cost DOUBLE
    );


CREATE EXTERNAL CACHE c_ext_oracle (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
    DatabaseProviderType: 'Default',
  FetchSize: 1,
  Password: 'qatest',
  Columns: 'id,name,cost',
  Table: 'QATEST.SRCCACHE12',
  trimquote: false,
  Username: 'qatest',
  keytomap: 'id',
  AdapterName:'DatabaseReader',
  connectionRetryPolicy: 'timeOut=5, retryInterval=5, maxRetries=10'
 )
OF CDCtestnMapType;
 

CREATE EXTERNAL CACHE c_oracle  (
 ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  DatabaseProviderType: 'Default',
  FetchSize: 1,
  Password: 'qatest',
  Columns: 'id,name,cost',
  Table: 'QATEST.SRCCACHE12',
  trimquote: false,
  Username: 'qatest',
  keytomap: 'id',
  AdapterName:'DatabaseReader',
  connectionRetryPolicy: 'timeOut=5, retryInterval=5, maxRetries=10' 
 )
 OF  CDCtestnMapType;
 
 CREATE TYPE CDCtestnMapTypenew
    (   id_t INTEGER,
        name_t STRING,
        cost_t DOUBLE,
        id_c INTEGER,
        name_c STRING,
        cost_c DOUBLE
    );
    
create stream JoinedDataStreamOracle of CDCtestnMapTypenew;
create stream JoinedDataStreamOraclenull of CDCtestnMapTypenew;

    
CREATE CQ JoinDataCQOracle1
INSERT INTO JoinedDataStreamOracle
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_DOUBLE(f.data[2]),
        z.id,
        z.name,
        z.cost
FROM SS_oracle f inner join c_ext_oracle z
on TO_INT(f.data[0]) = z.id;

CREATE CQ JoinDataCQOracle2 
INSERT INTO JoinedDataStreamOraclenull
SELECT  TO_INT(s.data[0]),
        TO_STRING(s.data[1]),
        TO_DOUBLE(s.data[2]),
        z.id,
        z.name,
        z.cost
FROM c_oracle f left outer join c_ext_oracle z
on z.id=f.id RIGHT OUTER JOIN SS_oracle s
on  TO_INT(s.data[0])=z.id
WHERE (TO_STRING(META(s, "OperationName")) = "INSERT");



CREATE TARGET Target_DBWriter USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1000,Interval:60',
CommitPolicy:'Eventcount:10,Interval:60',
Tables:'QATEST.test03'
) INPUT FROM JoinedDataStreamOracle;

CREATE TARGET Target_DBWriter2 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1000,Interval:60',
CommitPolicy:'Eventcount:10,Interval:60',
Tables:'QATEST.test04'
) INPUT FROM JoinedDataStreamOraclenull;

end application appname;
deploy application appname;
start appname;

create Application netflow;
create source NetflowSource using UDPReader (
	IpAddress:'127.0.0.1',
	PortNo:'3546'
)
parse using NetflowParser (
)
OUTPUT TO NetflowStream;

create type NetflowV5_Type (
vers int,
sys_uptime long,
unix_sec long,
unix_nsec long,
flow_sequence long,
engine_type string,
engine_id string,

src_ip string,
dst_ip string,
next_hop  string,
input_idx int,
output_idx  int,

flow_pkt_cnt long,
l3_cnt long,
first_sys_uptime long,
last_sys_uptime long,
src_port int,

dst_port int,
unused1 string,
tcp_flg string,
protocol_type string,
tos string,

src_as int,
dst_as int,
src_mask string,
dst_mask string,
unused2 int

);

CREATE STREAM NetflowV5Stream of NetflowV5_Type;

CREATE CQ NetflowCQ
INSERT INTO NetflowV5Stream
SELECT META(x,'version'),META(x,'sys_uptime'),META(x,'unix_sec'),META(x,'unix_nsec'),META(x,'flow_sequence'),META(x,'engine_type').toString(),META(x,'engine_id').toString(),
       VALUE(x,'src_ip'), VALUE(x,'dst_ip'), VALUE(x,'next_hop'), VALUE(x,'input_idx'), VALUE(x,'output_idx'),
       VALUE(x,'flow_pkt_cnt'), VALUE(x,'l3_cnt'), VALUE(x,'first_sys_uptime'), VALUE(x,'last_sys_uptime'), VALUE(x,'src_port'),
       VALUE(x,'dst_port'), VALUE(x,'unused1').toString(), VALUE(x,'tcp_flg').toString(), VALUE(x,'protocol_type').toString(), VALUE(x,'tos').toString(),
       VALUE(x,'src_as'), VALUE(x,'dst_as'), VALUE(x,'src_mask').toString(), VALUE(x,'dst_mask').toString(), VALUE(x,'unused2')
FROM NetflowStream x;

create Target dump1 using LogWriter(name:'NetflowV5',fileName:'@FEATURE-DIR@/logs/netflow.out') input from NetflowV5Stream;
end Application netflow;

stop application HDFSDSV;
undeploy application HDFSDSV;
drop application HDFSDSV cascade;

create application HDFSDSV;
create source HDFSCSVSource using HDFSReader (
	hadoopurl:'@HDFSREADERHADOOPURL@/home/hadoop/input/',
    WildCard:'posdata.csv',
	charset:'UTF-8',
    positionByEOF:false
)
parse using @HDFSCSVSOURCEFORMATTERTYPE@ (
	@HDFSCSVSOURCEFORMATTERMEMBERS@
)
OUTPUT TO HDFSCsvStream;
create Target HDFSDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/posdata') input from HDFSCsvStream;
end application HDFSDSV;

--
-- Crash Recovery Test 1 on Four node all server cluster 
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP N4S4CR1Tester.N4S4CRTest1;
UNDEPLOY APPLICATION N4S4CR1Tester.N4S4CRTest1;
DROP APPLICATION N4S4CR1Tester.N4S4CRTest1 CASCADE;
CREATE APPLICATION N4S4CRTest1 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest1;

CREATE SOURCE CsvSourceN4S4CRTest1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest1;

CREATE FLOW DataProcessingN4S4CRTest1;

CREATE TYPE WactionTypeN4S4CRTest1 (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE WactionsN4S4CRTest1 CONTEXT OF WactionTypeN4S4CRTest1
EVENT TYPES ( WactionTypeN4S4CRTest1 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN4S4CRTest1
INSERT INTO WactionsN4S4CRTest1
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END FLOW DataProcessingN4S4CRTest1;

END APPLICATION N4S4CRTest1;

-- The PosAppAgent sample application demonstrates how a credit card
-- payment processor might use Striim to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.


CREATE APPLICATION PosAppAgent;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosAppAgent application.

-- source CsvAgentDataSource

CREATE FLOW AgentFlow;

CREATE source CsvAgentDataSource USING FileReader (
  directory: 'C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvAgentStream;

END FLOW AgentFlow;

-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells Striim that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells Striim to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvAgentStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvAgentToPosDataCq

CREATE FLOW ProcessFlow;

CREATE CQ CsvAgentToPosDataCq
INSERT INTO PosDataAgentStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvAgentStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP.
--
-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as
-- the types to be used in PosDataAgentStream, which is created automatically.

-- The DATETIME field from the source is converted to both a dateTime
-- value, used as the event timestamp by the application, and (via the
-- function) an integer hourValue, which is used to look up
-- historical hourly averages from the HourlyAgentAveLookup cache,
-- discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)
-- 17 (hourValue)
-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateAgentOnly
--
-- The AgentPosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAgentAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAgentAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW AgentPosData5Minutes
OVER PosDataAgentStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantAgentHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAgentAveLookup using FileReader (
  directory: 'C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'merchantId') OF MerchantAgentHourlyAve;

CREATE TYPE MerchantTxRateAgent(
  merchantId String KEY,
  zip String,
  startTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateAgentOnlyStream OF MerchantTxRateAgent PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateAgentOnly
INSERT INTO MerchantTxRateAgentOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM AgentPosData5Minutes p, HourlyAgentAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAgentAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateAgentWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE STREAM MerchantTxRateAgentWithStatusStream OF MerchantTxRateAgent;

CREATE CQ GenerateMerchantTxRateAgentWithStatus
INSERT INTO MerchantTxRateAgentWithStatusStream
SELECT merchantId,
       zip,
       startTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateAgentOnlyStream;


-- WAction store MerchantActivityAgent
--
-- The following group of statements create and populate the MerchantActivityAgent
-- WAction store. Data from the MerchantTxRateAgentWithStatusStream is enhanced
-- with merchant details from NameLookupAgent cache and with latitude and longitude
-- values from the USAddressDataAgent cache.

CREATE TYPE MerchantActivityAgentContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivityAgent CONTEXT OF MerchantActivityAgentContext
EVENT TYPES ( MerchantTxRateAgent )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );


CREATE TYPE MerchantAgentNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressDataAgent(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookupAgent using FileReader (
  directory:'C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false,
  trimwhitespace:true
)
QUERY(keytomap:'merchantId') OF MerchantAgentNameData;

CREATE CACHE ZipLookupAgent using FileReader (
  directory: 'C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'zip') OF USAddressDataAgent;


CREATE CQ GenerateWactionAgentContext
INSERT INTO MerchantActivityAgent
SELECT  m.merchantId,
        m.startTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateAgentWithStatusStream m, NameLookupAgent n, ZipLookupAgent z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

-- CQ GenerateAgentAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertAgentStream OF Global.AlertEvent;

CREATE CQ GenerateAgentAlerts
INSERT INTO AlertAgentStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateAgentWithStatusStream m, NameLookupAgent n
WHERE m.merchantId = n.merchantId;

CREATE SUBSCRIPTION AgentAlertSub USING WebAlertAdapter( ) INPUT FROM AlertAgentStream;

END FLOW ProcessFlow;

END APPLICATION PosAppAgent;

DEPLOY APPLICATION PosAppAgent with AgentFlow in AGENTS, ProcessFlow in default;

-- CREATE DASHBOARD USING "C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData/PosAppDashboard.json";

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  _h_BindEmptyStringasNull : 'true'
 )
INPUT FROM @STREAM@;

CREATE APPLICATION TestAPP
CREATE TYPE TestType(
    id java.lang.Long KEY,
    name java.lang.String
);

CREATE STREAM TestStream OF TestType;




CREATE CQ TestCQ
INSERT INTO TestStream
SELECT
    h.value,
    TO_STRING(DNOW())
FROM heartbeat(interval 1 second) h;

END APPLICATION TestAPP;

--
-- Canon Test W72
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for sliding count window
--
-- S -> SWc5p -> CQ1(aggregate) -> WS
-- S -> SWc2p -> CQ1(aggregate) -> WS
--


UNDEPLOY APPLICATION NameW72.W72;
DROP APPLICATION NameW72.W72 CASCADE;
CREATE APPLICATION W72 RECOVERY 5 SECOND INTERVAL;




CREATE FLOW DataAcquisitionW72;


CREATE SOURCE CsvSourceW72 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW72;


END FLOW DataAcquisitionW72;




CREATE FLOW DataProcessingW72;

CREATE TYPE DataTypeW72 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW72 OF DataTypeW72
PARTITION BY word;

CREATE CQ CSVStreamW72_to_DataStreamW72
INSERT INTO DataStreamW72
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW72;

CREATE JUMPING WINDOW JWc5pW72
OVER DataStreamW72
KEEP 5 ROWS
PARTITION BY word;

CREATE JUMPING WINDOW JWc2pW72
OVER DataStreamW72
KEEP 2 ROWS
PARTITION BY word;

CREATE WACTIONSTORE WactionStoreW72 CONTEXT OF DataTypeW72
EVENT TYPES ( DataTypeW72 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWc5pW72_to_WactionStoreW72
INSERT INTO WactionStoreW72
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc5pW72 p;

CREATE CQ JWc2pW72_to_WactionStoreW72
INSERT INTO WactionStoreW72
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc2pW72;

END FLOW DataProcessingW72;



END APPLICATION W72;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;


CREATE SOURCE @APPNAME@_Source USING MySqlReader
(
  Compression: false,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'mysql://localhost:3306',
  DatabaseName: 'waction',
  Tables: 'waction.test01',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root'
)
OUTPUT TO @APPNAME@_Stream;


CREATE TARGET @APPNAME@_Target USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	BatchPolicy: 'EventCount: 1; Interval: 1s',
	instanceId: 'qatest'
) INPUT FROM @APPNAME@_stream;

create Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_stream;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 1 second interval;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@'
 )
OUTPUT TO @APPNAME@Stream;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;


CREATE TYPE test_typeRecov 
(
node_new com.fasterxml.jackson.databind.JsonNode,
node_name com.fasterxml.jackson.databind.JsonNode,
node_addr com.fasterxml.jackson.databind.JsonNode
);

Create stream cqAsJSONNodeStreamRecov of test_typeRecov;

CREATE CQ GetPOAsJsonNodesRecov
INSERT into cqAsJSONNodeStreamRecov
select 
data.get('ACCTS-RECORD'),
data.get('ACCTS-RECORD').get('NAME'),
data.get('ACCTS-RECORD').get('ADDRESS3')
from @APPNAME@Stream js;

create type finaldtypeRecov
(ACCOUNT_NO int,
FIRST_NAME String,
LAST_NAME String,
ADDRESS1 String,
ADDRESS2 String,
CITY String,
STATE String,
ZIP_CODE int);

CREATE STREAM getdataStreamPSRecov OF finaldtypeRecov;

CREATE CQ getdataRecov
INSERT into getdataStreamPSRecov
select JSONGetInteger(x.node_new,"ACCOUNT-NO"),
JSONGetString(x.node_name,"FIRST-NAME"),
JSONGetString(x.node_name,"LAST-NAME"),
JSONGetString(x.node_new,"ADDRESS1"),
JSONGetString(x.node_new,"ADDRESS2"),
JSONGetString(x.node_addr,"CITY"),
JSONGetString(x.node_addr,"STATE"),
JSONGetInteger(x.node_addr,"ZIP-CODE")
from cqAsJSONNodeStreamRecov x;

create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:1000,Interval:50',
  CommitPolicy: 'EventCount:1000,Interval:50',
  Tables: 'QATEST.@table@'
)
input from getdataStreamPSRecov;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

STOP APPLICATION EH;
UNDEPLOY APPLICATION EH;
DROP APPLICATION EH CASCADE;
CREATE APPLICATION EH @Recovery@;
CREATE SOURCE s USING FileReader (
	directory:'Product/IntegrationTests/TestData/',
    WildCard:'posdata.csv',
	positionByEOF:false
	)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO ss;

CREATE TYPE userType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String
);

CREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;

CREATE CQ cq1
INSERT INTO userDefinedTypedStream
SELECT data[0],data[1],data[2], data[3],data[4]
FROM ss;

create Target t1 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	Partitionkey:'@metadata(RecordOffset)',
	--ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using xmlFormatter(
rootelement:'data')
input from ss;

create Target t2 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	Partitionkey:'Col1',
	--ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using xmlFormatter(
    rootelement:'document',
	elementtuple:'Col1:Col2:text=Col1'
)
input from userDefinedTypedStream;

END APPLICATION EH;
DEPLOY APPLICATION EH;
start application EH;

UNDEPLOY APPLICATION TcpDsvAgentTester.TcpDsvWithAgent;
DROP APPLICATION TcpDsvAgentTester.TcpDsvWithAgent cascade;

create Application TcpDsvWithAgent;


create source TcpDsvAgent using TCPReader
(
  IpAddress:'127.0.0.1',
  PortNo:'3549',
  charset: 'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO TcpDsvAgentStream;


CREATE TYPE UserDataType
(
  UserId String KEY,
  UserName String,
  CompanyName String,
  UserZip int,
  CompanyZip int
);

CREATE STREAM UserDataStream OF UserDataType PARTITION BY UserId;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  data[0],
        data[1],
        data[2],
        TO_INT(data[3]),
        TO_INT(data[4])
FROM TcpDsvAgentStream;


CREATE WACTIONSTORE UserActivityInfo
CONTEXT OF UserDataType
EVENT TYPES ( UserDataType )
PERSIST EVERY 6 second USING (
JDBC_DRIVER:'@WASTORE-DRIVER@',
JDBC_URL:'@WASTORE-URL@',
JDBC_USER:'@WASTORE-UNAME@',
JDBC_PASSWORD:'@WASTORE-PASSWORD@',
DDL_GENERATION:'drop-and-create-tables',
LOGGING_LEVEL:'SEVERE',
CONTEXT_TABLE:'USERTABLE',
EVENT_TABLE:'USEREVENTS'
);


--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction
INSERT INTO UserActivityInfo
SELECT * FROM UserDataStream
LINK SOURCE EVENT;



end Application TcpDsvWithAgent;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
  directory: '',
  positionByEOF: false,
  WildCard: '' )
PARSE USING ParquetParser (
 )
OUTPUT TO @appname@Streams;

CREATE STREAM @appname@Stream3 OF Global.ParquetEvent;

CREATE STREAM @appname@Stream4 OF Global.ParquetEvent;

CREATE OR REPLACE CQ @appname@CQOrder3
INSERT INTO @appname@Stream3
SELECT
PUTUSERDATA(s,'schemaName',s.data.getSchema().getName())
FROM @appname@Streams s;

CREATE OR REPLACE CQ @appname@CQOrder4
INSERT INTO @appname@Stream4
SELECT
PUTUSERDATA(s2, 'customFilename', Userdata(s2, 'schemaName').toString().concat(".test"))
FROM @appname@Stream3 s2;

CREATE OR REPLACE TARGET @filetarget@ USING Global.FileWriter (
  flushpolicy: 'EventCount:10000,Interval:30s',
  directory: '',
  filename: '',
  rolloverpolicy: 'eventcount:10' )
FORMAT USING ParquetFormatter  (
schemafilename:''
)
INPUT FROM @appname@Stream4;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '@GRPPOLICY@',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'MultipleEvents',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE TARGET MongoTarget USING MongoDBWriter (
  Username: '',
  AuthDB: 'admin',
  ConnectionURL: '',
  batchpolicy: 'EventCount:10, Interval:30',
  Password: '',
  collections: '' )
INPUT FROM CCBStream;

create Target BlobTarget using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,Interval:30s'
)
format using JSONFormatter (
)
INPUT FROM CCBStream;


CREATE OR REPLACE TARGET GCSTarget USING GCSWriter (
    bucketname:'',
    objectname:'',
    projectId:'',
    uploadPolicy:'EventCount:10,Interval:30s'
)
format using JSONFormatter ()
INPUT FROM CCBStream;

create Target KafkaTarget using KafkaWriter VERSION @KAFKAVERSION@ (
brokerAddress:'',
Topic:'',
Mode: 'Sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;linger.ms=30000'
)
FORMAT USING JSONFormatter (
members:'data')
input from CCBStream;

CREATE TARGET S3Target USING Global.S3Writer (
  bucketname: '',
  objectname: '',
  uploadpolicy: 'EventCount:10,Interval:30s' )
FORMAT USING Global.JSONFormatter  ()
INPUT FROM CCBStream;

create source KafkaSource using KafkaReader VERSION @KAFKAVERSION@(
brokerAddress:'',
	Topic:''
)
parse using JSONParser ()
output to KafkaStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:10,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING JSONFormatter  ()
INPUT FROM KafkaStream;


end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

END APPLICATION @APPNAME@;

STOP APPLICATION KafkaWPTester.KWApp;
STOP APPLICATION KafkaWPTester.KRApp;
UNDEPLOY APPLICATION KafkaWPTester.KWApp;
UNDEPLOY APPLICATION KafkaWPTester.KRApp;
DROP APPLICATION KafkaWPTester.KWApp CASCADE;
DROP APPLICATION KafkaWPTester.KRApp CASCADE;

CREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;
GRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;
CONNECT KafkaWPTester KafkaWPTester;


CREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE CSVSource USING FileReader (
	directory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',
    WildCard:'access_log_half',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;

CREATE TYPE AccessLogType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;

CREATE CQ AceeslogCQ
INSERT INTO TypedAccessLogStream
SELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]
FROM FileStream;

create Target DsvWriter using KafkaWriter VERSION '0.9.0' ( 
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V9dsvExistTopic',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream;


create Target JsonWriter using KafkaWriter VERSION '0.9.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V9jsonExistTopic',
	Mode:'Sync'
    )
 
FORMAT USING JSONFormatter ()
input from TypedAccessLogStream;


create Target AvroWriter using KafkaWriter VERSION '0.9.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V9avroExistTopic',
    Mode:'Sync'
    )
 
FORMAT USING AvroFormatter (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')
input from TypedAccessLogStream;


END APPLICATION KWApp;

DEPLOY APPLICATION KWApp on any in default;


-- Kafka Reader Apps

CREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;


CREATE TYPE AccessLogType2(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE Stream KafkaDSVReaderStream of AccessLogType2;
CREATE Stream KafkaJsonReaderStream of AccessLogType2;
CREATE Stream KafkaAvroReaderStream of AccessLogType2;


CREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '0.9.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V9dsvExistTopic',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;




CREATE CQ DSVtoKafkaDSVReaderStream
INSERT INTO KafkaDSVReaderStream
SELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]
FROM KafkaReaderStream1;

CREATE TARGET DSVDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V9_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaDSVReaderStream;

CREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '0.9.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V9jsonExistTopic',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;




CREATE CQ DSVtoKafkaJsonReaderStream
INSERT INTO KafkaJsonReaderStream
SELECT 
data.get('Col1').textValue(),
data.get('Col2').textValue(),
data.get('Col3').textValue(),
data.get('Col4').textValue(),
data.get('Col5').textValue(),
data.get('Col6').textValue(),
data.get('Col7').textValue(),
data.get('Col8').textValue(),
data.get('Col9').textValue(),
data.get('Col10').textValue()
FROM KafkaReaderStream2;

CREATE TARGET JSONDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V9_RT_JSON')
FORMAT USING DSVFormatter()
INPUT FROM KafkaJsonReaderStream;

CREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '0.9.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V9avroExistTopic',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'
)
OUTPUT TO KafkaReaderStream3;



CREATE Stream KRTypedStream of AccessLogType;

CREATE CQ DSVtoKafkaAvroReaderStream
INSERT INTO KafkaAvroReaderStream
SELECT    
data.get("Col1").toString(), 
data.get("Col2").toString(), 
data.get("Col3").toString(), 
data.get("Col4").toString(),
data.get("Col5").toString(), 
data.get("Col6").toString(), 
data.get("Col7").toString(), 
data.get("Col8").toString(),
data.get("Col9").toString(), 
data.get("Col10").toString()
FROM KafkaReaderStream3;

CREATE TARGET AVRODump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V9_RT_AVRO')
FORMAT USING DSVFormatter()
INPUT FROM KafkaAvroReaderStream;

END APPLICATION KRApp;
DEPLOY APPLICATION KRApp on any in default;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCENAME1@ USING IncrementalBatchReader  (
  FetchSize: 10,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: '@startPosition@',
  PollingInterval: '20sec'
  )
  OUTPUT TO @STREAM@;

  CREATE OR REPLACE SOURCE @SOURCENAME2@ USING IncrementalBatchReader  (
    FetchSize: 10,
    Username: 'striim',
    Password: 'striim',
    ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
    Tables: 'striim.test01',
    adapterName: 'IncrementalBatchReader',
    CheckColumn: @checkColumn1@,
    startPosition: '@startPosition1@',
    PollingInterval: '20sec'
    )
    OUTPUT TO @STREAM@;

  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;

  CREATE TARGET @targetName@ USING DatabaseWriter(
    ConnectionURL:'@READER-URL@',
    Username:'@READER-UNAME@',
    Password:'@READER-PASSWORD@',
    BatchPolicy:'Eventcount:1,Interval:1',
    CommitPolicy:'Eventcount:1,Interval:1',
    Checkpointtable:'CHKPOINT',
    Tables:'@WATABLES@,@WATABLES@_target'
  ) INPUT FROM @STREAM@;


  END APPLICATION @APPNAME@;

  DEPLOY APPLICATION @APPNAME@;
  start application @APPNAME@;

drop application ConsoleApplication cascade;
create application ConsoleApplication;

-- The PosApp sample application demonstrates how a credit card
-- payment processor might use Striim to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.
STOP APPLICATION PosAppKafka.PosAppKafka;
UNDEPLOY APPLICATION PosAppKafka.PosAppKafka;
DROP APPLICATION PosAppKafka.PosAppKafka CASCADE;

CREATE APPLICATION PosAppKafka;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells Striim that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells Striim to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvToPosData

create type posdatatype(
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip string
);

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092');

create stream PosDataStream of posdatatype persist using KafkaProps;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT TO_STRING(data[1]),
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       TO_STRING(data[9])
FROM CsvStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP.
--
-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as
-- the types to be used in PosDataStream, which is created automatically.

-- The DATETIME field from the source is converted to both a dateTime
-- value, used as the event timestamp by the application, and (via the
-- function) an integer hourValue, which is used to look up
-- historical hourly averages from the HourlyAveLookup cache,
-- discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)
-- 17 (hourValue)
-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateOnly
--
-- The PosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startingTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;


-- WAction store MerchantActivity
--
-- The following group of statements create and populate the MerchantActivity
-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced
-- with merchant details from NameLookup cache and with latitude and longitude
-- values from the USAddressData cache.

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
@PERSIST-TYPE@

CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false
)
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;


CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

-- CQ GenerateAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;



--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM AlertStream;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;

END APPLICATION PosAppKafka;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallretaildata2M.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
storeId String,
nameId String,
city String,
state String

);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],
data[1],
data[2],
data[3]

FROM CsvStream;

create Target t using FileWriter(
filename:'EventNCDefault',
directory:'@FEATURE-DIR@/logs/',
sequence:'00',
--filelimit: '5',
rolloverpolicy:'eventcount:-100'
)
format using DSVFormatter (

)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetNegativeECDefault_actual.log') input from TypedCSVStream;
end application DSV;
DEPLOY APPLICATION DSV on any in default;
START DSV;
deploy application DSV;

STOP APPLICATION HW ;
undeploy application HW ;
drop application HW cascade;

CREATE APPLICATION HW Recovery 5 second interval;

CREATE  SOURCE S USING OracleReader  ( 
  Username: 'miner',
  Password: '@miner',
  ConnectionURL: '@conn-url@',
  Tables: '@src@',
  FetchSize: 1) 
OUTPUT TO hivestream;

Create Target T using HiveWriter (
  ConnectionURL:'@hive-url@',
  Username:'@uname@', 
            Password:'@pwd@',
            hadoopurl:'hdfs://dockerhost:9000/',
	        Mode:'incremental',
	        mergepolicy: 'eventcount:5,interval:1s',
            Tables:'@tgt-table@',
            hadoopConfigurationPath:'/Users/saranyad/Documents/hello/'
 )
INPUT FROM hivestream;


END APPLICATION HW;
deploy application HW on all in default;

Start application HW;

Stop Oracle_IRLogWriter;
Undeploy application Oracle_IRLogWriter;
drop application Oracle_IRLogWriter cascade;
CREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;
create flow AgentFlow;
CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( 
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.119/DBS_PORT=1025',
  Tables: 'striim.upgrade01',
  CheckColumn:'striim.upgrade01=t1',
  startPosition:'striim.upgrade01=2018-09-20 06:43:59',
  ReturnDateTimeAs:'string'
  )
OUTPUT TO data_stream1;
end flow AgentFlow;

create flow serverFlow;
create target AzureSQLDWHTarget using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test26,dbo.test26',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream1;

end flow serverFlow;
END APPLICATION Oracle_IRLogWriter;
deploy application Oracle_IRLogWriter with AgentFlow in Agents, ServerFlow in default;
start application Oracle_IRLogWriter;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

Create Source dataGenSrc USING MariaDBReader  ( 
  Username:'qatest',
  Password:'w3b@ct10n',
  ConnectionURL:'jdbc:mariadb://10.77.21.53:3306/qatest',
  Tables: '@tableNames@',
  ClusterSupport: 'Galera'
 ) 
 Output To LCRStream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;

END APPLICATION DataGenSampleApp;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[2]) = 'Null' ;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW ;

CREATE STREAM Oracle_ChangeDataStream of Global.WAEvent;

CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: false,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.56.101:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream1;


CREATE CQ CQfilter
INSERT INTO Oracle_ChangeDataStream
select putuserdata (data1,'IntToInt', data[0]) from Oracle_ChangeDataStream1 data1
where (META(data1, 'OperationName').toString() =='INSERT' or META(data1, 'OperationName').toString() =='UPDATE');

CREATE STREAM Oracle_DataStream of Global.WAEvent;

CREATE CQ CQfilter1
INSERT INTO Oracle_DataStream
select * from Oracle_ChangeDataStream c where to_int(USERDATA(c, 'IntToInt'))<4;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:60',
  CommitPolicy: 'EventCount:1,Interval:60',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  Tables: 'QATEST.OracToCql_alldatatypes,test.oractocq_alldatatypes columnmap(IntToInt=IntToInt)',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_DataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d';

CREATE OR REPLACE SOURCE @SOURCE@1 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
 )
OUTPUT TO @STREAM@1;

CREATE OR REPLACE SOURCE @SOURCE@2 USING PostgreSQLReader  (
    ReplicationSlotName:'test_slot',
    FilterTransactionBoundaries:'true',
    Username:'@SOURCE_USER@',
    Password_encrypted:false,
    ConnectionURL:'@CONNECTION_URL@',
    adapterName:'PostgreSQLReader',
    ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
    Password:'@SOURCE_PASS@',
    Tables:'@SOURCE_TABLE@',
    ExcludedTables:'public.chkpoint',
 )
OUTPUT TO @STREAM@1;

CREATE OR REPLACE SOURCE @SOURCE@3 USING MySQLReader (
    Username: '@READER-UNAME@',
    Password: '@READER-PASSWORD@',
    ConnectionURL: '@CDC-READER-URL@',
    Tables: @WATABLES@,
    sendBeforeImage:'true',
    FilterTransactionBoundaries: 'true',
    ExcludedTables: 'waction.CHKPOINT'
)
OUTPUT TO @STREAM@1;

 -- CREATE OR REPLACE SOURCE @SOURCE@4 USING MariaDbXpandReader
 -- (
 -- Username: '@READER-UNAME@',
 -- Password: '@READER-PASSWORD@',
 -- ConnectionURL: '@CDC-READER-URL@',
 -- Tables: @WATABLES@,
 -- sendBeforeImage:'true',
 -- FilterTransactionBoundaries: 'true',
 -- ExcludedTables: 'test.CHKPOINT'
 -- )
 -- OUTPUT TO @STREAM@1;

CREATE OR REPLACE TARGET @TARGET@1 USING SnowflakeWriter (
  ConnectionURL: '@SNOWFLAKE-URL@',
  username: '@SNOWFLAKE-USERNAME@',
  password: '@SNOWFLAKE-PASSWORD@',
  appendOnly:'false',
  uploadPolicy: 'eventcount:1,interval:5m',
  Tables: '@TARGET-TABLES@'
 )
INPUT FROM @STREAM@1;

CREATE OR REPLACE TARGET @TARGET@2 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@WATABLES@',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true
)
INPUT FROM @STREAM@1;



END APPLICATION @APPNAME@;

CREATE APPLICATION @AppName@;

CREATE OR REPLACE SOURCE initialLoad_Src USING Global.DatabaseReader (
  QuiesceOnILCompletion: false,
  Tables: '@SrcTableName@',
  adapterName: 'DatabaseReader',
  Password: '@Password@',
  Username: '@UserName@',
  ConnectionURL: '@Srcurl@',
   FetchSize: 10000)
OUTPUT TO CommonRawStream;

Create Source OrcReader_Src Using OracleReader(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@Srcurl@',
 Tables:'@SrcTableName@',
 Fetchsize:100)
Output To CommonRawStream;

CREATE OR REPLACE TARGET Postgres_Trg USING Global.DatabaseWriter (
  ConnectionURL: '@trgUrl@',
  Username: '@trgUsrName@',
  Tables: '@SrcTableName@,@trgTable@',
  Password: '@trgPswd@',
  CommitPolicy: 'EventCount:10000,Interval:60',
  adapterName: 'DatabaseWriter' )
INPUT FROM CommonRawStream;

CREATE TARGET filewriter_tgt USING Global.FileWriter (
 directory:'@trgDir@',
  filename: '@fileName@',
  flushpolicy: 'EventCount:10000,Interval:30s',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n' )
INPUT FROM CommonRawStream;

CREATE OR REPLACE TARGET BigQuery_Target USING Global.BigQueryWriter (
  streamingUpload: 'false',
  projectId: '@projectID@',
  Tables: '@SrcTableName@,@BQTableName@',
  optimizedMerge: 'false',
  ServiceAccountKey: '@ServiceAccountKey@',
  BatchPolicy: 'EventCount:1000000,Interval:90',
  Mode: 'APPENDONLY' )
INPUT from CommonRawStream;

END APPLICATION @AppName@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING HL7v2Parser (
  EnableMessageValidation: false,
  MLLPDelimited: false
  )
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING XMLFormatter (
    rootelement:"document",
    charset: "UTF-8"
    )
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

Stop Oracle_IRLogWriter;
Undeploy application Oracle_IRLogWriter;
drop application Oracle_IRLogWriter cascade;

CREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;

CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( 
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.TDSOURCE',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.TEST01=ID;',
  PollingInterval: '5sec',
  ReturnDateTimeAs: 'String',
  startPosition:'striim.test01=0'
  )
  OUTPUT TO data_stream;

  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

CREATE TARGET BinaryDump USING LogWriter(
  name: 'TeraData',
  filename:'TeraData.log',
  flushpolicy:'EventCount:100,Interval:30s'
)INPUT FROM data_stream;

END APPLICATION Oracle_IRLogWriter;

deploy application Oracle_IRLogWriter in default;

start application Oracle_IRLogWriter;

STOP APPLICATION @appname@routerApp;
UNDEPLOY APPLICATION @appname@routerApp;
DROP APPLICATION @appname@routerApp CASCADE;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',
  acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

CREATE APPLICATION @appname@routerApp;

CREATE  SOURCE @appname@OraSource USING OracleReader  (
Username: 'qatest',
Password: 'qatest',
ConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',
Tables: 'QATEST.TGT_T%',
 FetchSize:'100'
)
OUTPUT TO @appname@MasterStream1;

-- CREATE STREAM @appname@ss1 OF Global.waevent persist using Global.DefaultKafkaProperties;
-- CREATE STREAM @appname@ss2 OF Global.waevent persist using Global.DefaultKafkaProperties;
-- CREATE STREAM @appname@ss3 OF Global.waevent persist using Global.DefaultKafkaProperties;

CREATE STREAM @appname@ss1 OF Global.waevent PERSIST USING KafkaPropset;
CREATE STREAM @appname@ss2 OF Global.waevent PERSIST USING KafkaPropset;
CREATE STREAM @appname@ss3 OF Global.waevent PERSIST USING KafkaPropset;

CREATE OR REPLACE ROUTER @appname@tablerouter1 INPUT FROM @appname@MasterStream1 s CASE
WHEN meta(s,"TableName").toString()='QATEST.TGT_T1' THEN ROUTE TO @appname@ss1,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T2' THEN ROUTE TO @appname@ss2,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T3' THEN ROUTE TO @appname@ss3,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T4' THEN ROUTE TO @appname@ss4,
ELSE ROUTE TO @appname@ss_else;

create Target @appname@FileTarget_1 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'
)
FORMAT USING dsvFormatter ()
input from @appname@ss1;

create Target @appname@FileTarget_2 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'
)
FORMAT USING dsvFormatter ()
input from @appname@ss2;

create Target @appname@FileTarget_3 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from @appname@ss3;

create Target @appname@FileTarget_4 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from @appname@ss4;


create Target @appname@FileTarget_5 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from @appname@ss_else;


end application @appname@routerApp;
deploy application @appname@routerApp;
start @appname@routerApp;

use PosTester;
alter application PosApp;

CREATE source CsvDataSource USING CSVReader (
  directory:'Samples/Customer/PosApp/appData',
  header:Yes,
  wildcard:'posdata.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

end application PosApp;

alter application PosApp recompile;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (
  positionByEOF: false,
  wildcard: '',
  directory: '' )
PARSE USING XMLParserV2 (
  rootnode: 'document/MerchantName' )
OUTPUT TO @APPNAME@Stream;

CREATE CQ @APPNAME@CQ
INSERT INTO @APPNAME@CQStream
SELECT
data.attributeValue("merchantid") as merchantid,
data.getText() as MerchantName
FROM @APPNAME@Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@CQStream;

END APPLICATION @APPNAME@;

--use global;
CREATE OR REPLACE PROPERTYSET LDAP1 ( PROVIDER_URL:"@LDAP_URL@", SECURITY_AUTHENTICATION:@LDAP_AUTH@, SECURITY_PRINCIPAL: "@LDAP_PRINCIPAL@" , SECURITY_CREDENTIALS:@LDAP_CRED@, USER_BASE_DN:"@LDAP_DN@", User_userId:@LDAP_USERID@ );

CREATE APPLICATION @AppName@;
CREATE FLOW Il_Agent_flow;
CREATE OR REPLACE SOURCE FileReader_Src USING FileReader  (
   WildCard: 'posdata100.csv',
  directory: '@dir@',
  positionbyeof: false)
 PARSE USING DSVParser  (
 )
OUTPUT TO CsvStream ;
END FLOW Il_Agent_flow;

Create Type CSVType (
  companyid String,
  merchantId String
);

CREATE STREAM TypedCSVStream OF CSVType;

CREATE OR REPLACE  CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT
TO_STRING(data[0]).replaceAll("COMPANY ", ""),
data[1]
FROM CsvStream;

CREATE OR REPLACE TARGET initialLoadPostgres_Trg USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:100,Interval:60',
  StatementCacheSize: '50',
  ConnectionURL: '@TrgUrl@',
  Username: '@TrgUserName@',
  BatchPolicy: 'EventCount:100,Interval:60',
  Tables: '@TrgTable@',
  Password: '@TrgPswd@',
  adapterName: 'DatabaseWriter' )
INPUT FROM TypedCSVStream;
END APPLICATION @AppName@;

create Target @TARGET_NAME@ using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'eventCount:1000',
    ServiceAccountKey:'@file-path@'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @STREAM@;

--
-- Canon Test W02
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for an unpartitioned jumping count window
--
-- S -> JWc101uW02 -> CQ -> WS
--


UNDEPLOY APPLICATION NameW02.W02;
DROP APPLICATION NameW02.W02 CASCADE;
CREATE APPLICATION W02 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW02;


CREATE SOURCE CsvSourceW02 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW02;


END FLOW DataAcquisitionW02;



CREATE FLOW DataProcessingW02;

CREATE TYPE DataTypeW02 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW02 OF DataTypeW02;

CREATE CQ CSVStreamW02_to_DataStreamW02
INSERT INTO DataStreamW02
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW02;

CREATE JUMPING WINDOW JWc101uW02
OVER DataStreamW02
KEEP 101 ROWS;

CREATE WACTIONSTORE WactionStoreW02 CONTEXT OF DataTypeW02
EVENT TYPES ( DataTypeW02 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWc101uW02_to_WactionStoreW02
INSERT INTO WactionStoreW02
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc101uW02;

END FLOW DataProcessingW02;



END APPLICATION W02;

--
-- Canon Test W40
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for an unpartitioned jumping attribute window
--
-- S -> JWa5u -> CQ -> WS
--


UNDEPLOY APPLICATION NameW40.W40;
DROP APPLICATION NameW40.W40 CASCADE;
CREATE APPLICATION W40 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW40;


CREATE SOURCE CsvSourceW40 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW40;


END FLOW DataAcquisitionW40;




CREATE FLOW DataProcessingW40;

CREATE TYPE DataTypeW40 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW40 OF DataTypeW40;

CREATE CQ CSVStreamW40_to_DataStreamW40
INSERT INTO DataStreamW40
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW40;

CREATE JUMPING WINDOW JWa5uW40
OVER DataStreamW40
KEEP WITHIN 5 SECOND ON dateTime;

CREATE WACTIONSTORE WactionStoreW40 CONTEXT OF DataTypeW40
EVENT TYPES ( DataTypeW40 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWa5uW40_to_WactionStoreW40
INSERT INTO WactionStoreW40
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWa5uW40;

END FLOW DataProcessingW40;



END APPLICATION W40;

Create Source @SOURCE_NAME@ Using MSSqlReader
(
 Username:'@READER-UNAME@',
 Password:'@READER-PASSWORD@',
 DatabaseName:'qatest',
 ConnectionURL:'@CDC-READER-URL@',
 Tables:@WATABLES@,
 ConnectionPoolSize:2,
 Compression:false,
 StartPosition:'EOF'
)
Output To @STREAM@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '@GRPPOLICY@',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'MultipleEvents',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

create Target KafkaTarget using KafkaWriter VERSION '2.1.0' (
brokerAddress:'',
Topic:'',
Mode: 'Sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;linger.ms=30000'
)
FORMAT USING JSONFormatter (
members:'data')
input from CCBStream;

create source KafkaSource using KafkaReader VERSION '2.1.0'(
brokerAddress:'',
	Topic:''
)
parse using JSONParser ()
output to KafkaStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING JSONFormatter ()
INPUT FROM KafkaStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

STOP liveStream;
UNDEPLOY APPLICATION liveStream;
DROP APPLICATION liveStream CASCADE;

CREATE APPLICATION liveStream;

CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity int,
  size int);

--SOURCE DESCRIPTION
---------------------------------------------
--OutputType: name of the corresponding TYPE
--noLimit: produce infinite data
--maxRows: (if noLimit = false) produces specified rows
--iterations (optional): 1 iteration = populating Type attributes once.
--iterationDelay (ms)(optional): delay between iterations (0 if none)

-- ** either maxRows or iterations must be 0 **

--StringSet: columns of type String. column values within brackets (seperate values by dash), seperate columns by comma
--NumberSet: columns of type Int, Double, Long. supply a range between brackets, followed by G (Gaussian) or R (Random) distribution.
---------------------------------------------

CREATE SOURCE liveSource using StreamReader(
  OutputType: 'eventLister.Atm',
  noLimit: 'false',
  maxRows: 20,
  iterations: 0,
  iterationDelay: 100,
  StringSet: 'productID[001-002-003-004],stateID[AS-CA-WA-NY]',
  NumberSet: 'productWeight[3-3]R,quantity[20-20]R,size[250-250]R'
  )OUTPUT TO CsvStream;

CREATE STREAM newStream OF Atm;


CREATE CQ newCQ
INSERT INTO newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_INT(data[3]), TO_INT(data[4]) FROM
CsvStream;

CREATE WACTIONSTORE streamActivity CONTEXT OF Atm
EVENT TYPES ( Atm )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ newCQ2
INSERT INTO streamActivity
SELECT * FROM newStream
link source event;

END APPLICATION liveStream;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

create Target FileTarget using FileWriter(
    rolloverpolicy:'@UPLOAD-SIZE@',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JSONFormatter (
charset:'@charset@',
members:'@mem@'
)
input from CsvStream;

end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @srcName@ USING OracleReader (
 Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@'
)
OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@ USING DatabaseWriter (
  CheckPointTable: 'CHKPOINT', 
  ReplicationSlotName:'test_slot',
  Username:'@tgtusername@',
  Password:'@tgtpassword@',
  ConnectionURL:'@tgturl@',
  adapterName:'PostgreSQLReader',
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@'
)
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP APPLICATION DatabaseWriterTester.DatabaseReaderApp;
UNDEPLOY APPLICATION DatabaseWriterTester.DatabaseReaderApp;
DROP APPLICATION DatabaseWriterTester.DatabaseReaderApp CASCADE;
STOP APPLICATION DatabaseWriterTester.DatabaseWriterApp;
UNDEPLOY APPLICATION DatabaseWriterTester.DatabaseWriterApp;
DROP APPLICATION DatabaseWriterTester.DatabaseWriterApp CASCADE;

CREATE APPLICATION DatabaseWriterApp;

CREATE SOURCE CSVSource USING FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'banks.csv',
positionByEOF:false
)
PARSE USING DSVParser (
header:'yes'
)
OUTPUT TO Orders;

CREATE TYPE OrdersData(
  id String Key,
  name String);

CREATE STREAM OrdersDataStream OF OrdersData;

CREATE CQ CsvToOrdersData
INSERT INTO OrdersDataStream (id, name)
SELECT data[0], data[1]
FROM Orders;

CREATE TARGET WriteJpaOracle USING DatabaseWriter(
DriverName:'oracle.jdbc.OracleDriver',
ConnectionURL:'@READER-URL@',
        Username:'@READER-UNAME@',
        Password:'@READER-PASSWORD@',
Tables:'QATEST.DB_WRITER_TEST',
eventType:'DatabaseWriterTester.OrdersData'
) INPUT FROM OrdersDataStream;

END APPLICATION DatabaseWriterApp;
DEPLOY APPLICATION DatabaseWriterApp;
START APPLICATION DatabaseWriterApp;

CREATE APPLICATION DatabaseReaderApp;

create source DatabaseReaderSource USING DatabaseReader
(
ConnectionURL:'@READER-URL@',
        Username:'@READER-UNAME@',
        Password:'@READER-PASSWORD@',
Tables:'QATEST.DB_WRITER_TEST',
   FetchSize: 1

)
OUTPUT TO DatabaseReaderTestStream;

create Target t using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/Databasewritertarget_RT') input from DatabaseReaderTestStream;


END APPLICATION DatabaseReaderApp;
DEPLOY APPLICATION DatabaseReaderApp;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src1 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream1;

CREATE OR REPLACE CQ @APP_NAME@_CQ1
INSERT INTO @APP_NAME@_Stream6
SELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream1 d;

CREATE OR REPLACE SOURCE @APP_NAME@_src2 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream2;

CREATE OR REPLACE CQ @APP_NAME@_CQ2
INSERT INTO @APP_NAME@_Stream6
SELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream2 d;

CREATE OR REPLACE SOURCE @APP_NAME@_src3 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream3;

CREATE OR REPLACE CQ @APP_NAME@_CQ3
INSERT INTO @APP_NAME@_Stream6
SELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream3 d;

CREATE OR REPLACE SOURCE @APP_NAME@_src4 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream4;

CREATE OR REPLACE CQ @APP_NAME@_CQ4
INSERT INTO @APP_NAME@_Stream6
SELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream4 d;

CREATE OR REPLACE SOURCE @APP_NAME@_src5 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream5;

CREATE OR REPLACE CQ @APP_NAME@_CQ5
INSERT INTO @APP_NAME@_Stream6
SELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream5 d;


CREATE OR REPLACE TARGET @APP_NAME@_tgt1 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream6;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

--
-- Recovery Test 27 with two sources, two jumping time windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jt1W -> CQ1 -> WS
--   S2 -> Jt2W -> CQ2 -> WS
--

STOP Recov27Tester.RecovTest27;
UNDEPLOY APPLICATION Recov27Tester.RecovTest27;
DROP APPLICATION Recov27Tester.RecovTest27 CASCADE;
CREATE APPLICATION RecovTest27 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream1Second
OVER DataStream1 KEEP WITHIN 1 SECOND;

CREATE JUMPING WINDOW DataStream2Second
OVER DataStream2 KEEP WITHIN 2 SECOND;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data1ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream1Second p;

CREATE CQ Data2ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream2Second p;

END APPLICATION RecovTest27;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING DSVFormatter (
members:'data')
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
wildcard: '',
directory: '',
positionbyeof: false )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: ''
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

create Source waSrcName
        Using HPNonStopSQLMXReader
  (AgentPortNo:'7012',
   AgentIPaddress:'$NSKADDR',
   PortNo:'5013',
   -- ipaddress: '$WAADDR',
   Name:'S00',
   ReturnDateTimeAs: 'String',
   Tables:'sncat.snsch.DBWriterTest1')
Output To waAppName_Stream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from waAppName_Stream;

END APPLICATION DataGenSampleApp;

UNDEPLOY APPLICATION FileToKWriterUpgrade;

DROP APPLICATION FileToKWriterUpgrade cascade;

CREATE APPLICATION FileToKWriterUpgrade;

CREATE TYPE Type1
(
 City String,
 Col2 String,
 Col3 String,
 Col4 String
);

-- Create a stream of type Type1

CREATE STREAM TypedStream OF Type1;

-- Create a source using FileReader

CREATE SOURCE KafkaCSVSource USING FILEREADER
(
 directory:'@DIRECTORY@',
 WildCard:'city*.dsv',
 positionByEOF:false,
 charset:'UTF-8'
)
PARSE USING DSVPARSER
(
 columndelimiter:',',
 ignoreemptycolumn:'Yes'
)
OUTPUT TO FileStream;

-- Read from raw stream to typed stream using CQ

CREATE CQ RawStreamCQ
INSERT INTO TypedStream
SELECT 
 data[0],
 data[1],
 data[2],
 data[3] 
FROM FileStream;

-- Load the KafkaWriter from TypedStream

CREATE TARGET KWriter USING KAFKAWRITER VERSION '0.9.0'
(
 brokerAddress:'localhost:9092',
 Topic:'@TOPIC@',
 KafkaMessageFormatVersion:v2
)
FORMAT USING DSVFORMATTER()
INPUT FROM TypedStream;


CREATE SOURCE KReader USING KAFKAREADER VERSION '0.9.0'
(
 brokerAddress:'localhost:9092',
 Topic:'@TOPIC@',
 charset : 'UTF-8',
 KafkaConfig:'retry.backoff.ms=5000',
 startOffset:0
)
PARSE USING DSVParser (
)

OUTPUT TO KafkaReaderStream;

CREATE TARGET LogKafkaReaderStream USING LOGWRITER
(
 name:KafkaLOuput,
 filename:'@LOGFILENAME@',
 flushpolicy : 'flushcount:1',
 rolloverpolicy : 'EventCount:10000,Interval:30s'
)
INPUT FROM KafkaReaderStream;


END APPLICATION FileToKWriterUpgrade;

DEPLOY APPLICATION FileToKWriterUpgrade;

START APPLICATION FileToKWriterUpgrade;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@;
CREATE SOURCE @srcName@ USING Global.OracleReader ( 
  Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@') 
OUTPUT TO @instreamname@;

CREATE OR REPLACE TARGET @tgtName@ USING Global.ServiceNowWriter ( 
  MaxConnections: 20, 
  ClientSecret: '@clientsecret@', 
  ApplicationErrorCountThreshold: 0, 
  BatchPolicy: 'eventCount:10000, Interval:60', 
  ConnectionUrl: '@tgturl@', 
  Password: '@tgtpassword@', 
  BatchAPI: false,  
  ConnectionTimeOut: 60, 
  Mode: 'APPENDONLY', 
  ClientID: '@clientid@', 
  Tables: '@srcschema@.@srctable@,@tgttable@ COLUMNMAP()', 
  ConnectionRetries: 3, 
  useConnectionProfile: false, 
  UserName: '@tgtusername@', 
  adapterName: 'ServiceNowWriter' ) 
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP APPLICATION App1;
UNDEPLOY APPLICATION App1;
DROP APPLICATION App1 CASCADE;
CREATE APPLICATION App1;
CREATE FLOW AgentFlow;
CREATE OR REPLACE SOURCE App1_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App1_SampleStream;
END FLOW AgentFlow;
CREATE FLOW ServerFlow;
CREATE OR REPLACE TARGET App1_NullTarget using NullWriter()
INPUT FROM App1_SampleStream;
END FLOW ServerFlow;
END APPLICATION App1;
deploy application App1 on any in ServerDG1 with AgentFlow on any in Agents, ServerFlow on any in ServerDG1;
START APPLICATION App1;

STOP APPLICATION App2;
UNDEPLOY APPLICATION App2;
DROP APPLICATION App2 CASCADE;
CREATE APPLICATION App2;
CREATE FLOW AgentFlow2;
CREATE OR REPLACE SOURCE App2_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App2_SampleStream;
END FLOW AgentFlow2;
CREATE FLOW ServerFlow2;
CREATE OR REPLACE TARGET App2_NullTarget using NullWriter()
INPUT FROM App2_SampleStream;
END FLOW ServerFlow2;
END APPLICATION App2;
deploy application App2 on any in ServerDG1 with AgentFlow2 on any in Agents, ServerFlow2 on any in ServerDG1;
START APPLICATION App2;

STOP APPLICATION App3;
UNDEPLOY APPLICATION App3;
DROP APPLICATION App3 CASCADE;
CREATE APPLICATION App3;
CREATE OR REPLACE SOURCE App3_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App3_SampleStream;
CREATE OR REPLACE TARGET App3_NullTarget using NullWriter()
INPUT FROM App3_SampleStream;
END APPLICATION App3;
DEPLOY APPLICATION App3;
START APPLICATION App3;

STOP APPLICATION App4;
UNDEPLOY APPLICATION App4;
DROP APPLICATION App4 CASCADE;
CREATE APPLICATION App4;
CREATE OR REPLACE SOURCE App4_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App4_SampleStream;
CREATE OR REPLACE TARGET App4_NullTarget using NullWriter()
INPUT FROM App4_SampleStream;
END APPLICATION App4;
DEPLOY APPLICATION App4 ON ONE IN ServerDG1;
START APPLICATION App4;

STOP APPLICATION App5;
UNDEPLOY APPLICATION App5;
DROP APPLICATION App5 CASCADE;
CREATE APPLICATION App5;
CREATE OR REPLACE SOURCE App5_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App5_SampleStream;
CREATE OR REPLACE TARGET App5_NullTarget using NullWriter()
INPUT FROM App5_SampleStream;
END APPLICATION App5;
DEPLOY APPLICATION App5 ON ONE IN ServerDG1;
START APPLICATION App5;

STOP APPLICATION App6;
UNDEPLOY APPLICATION App6;
DROP APPLICATION App6 CASCADE;
CREATE APPLICATION App6;
CREATE OR REPLACE SOURCE App6_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App6_SampleStream;
CREATE OR REPLACE TARGET App6_NullTarget using NullWriter()
INPUT FROM App6_SampleStream;
END APPLICATION App6;
DEPLOY APPLICATION App6 ON ONE IN ServerDG1;
START APPLICATION App6;

STOP APPLICATION App7;
UNDEPLOY APPLICATION App7;
DROP APPLICATION App7 CASCADE;
CREATE APPLICATION App7;
CREATE OR REPLACE SOURCE App7_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App7_SampleStream;
CREATE OR REPLACE TARGET App7_NullTarget using NullWriter()
INPUT FROM App7_SampleStream;
END APPLICATION App7;
DEPLOY APPLICATION App7 ON ONE IN ServerDG1;
START APPLICATION App7;

STOP APPLICATION App8;
UNDEPLOY APPLICATION App8;
DROP APPLICATION App8 CASCADE;
CREATE APPLICATION App8;
CREATE OR REPLACE SOURCE App8_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App8_SampleStream;
CREATE OR REPLACE TARGET App8_NullTarget using NullWriter()
INPUT FROM App8_SampleStream;
END APPLICATION App8;
DEPLOY APPLICATION App8 ON ONE IN ServerDG1;
START APPLICATION App8;


STOP APPLICATION App9;
UNDEPLOY APPLICATION App9;
DROP APPLICATION App9 CASCADE;
CREATE APPLICATION App9;
CREATE OR REPLACE SOURCE App9_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App9_SampleStream;
CREATE OR REPLACE TARGET App9_NullTarget using NullWriter()
INPUT FROM App9_SampleStream;
END APPLICATION App9;
DEPLOY APPLICATION App9;
START APPLICATION App9;

STOP APPLICATION App10;
UNDEPLOY APPLICATION App10;
DROP APPLICATION App10 CASCADE;
CREATE APPLICATION App10;
CREATE OR REPLACE SOURCE App10_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App10_SampleStream;
CREATE OR REPLACE TARGET App10_NullTarget using NullWriter()
INPUT FROM App10_SampleStream;
END APPLICATION App10;
DEPLOY APPLICATION App10;
START APPLICATION App10;

STOP APPLICATION App11;
UNDEPLOY APPLICATION App11;
DROP APPLICATION App11 CASCADE;
CREATE APPLICATION App11;
CREATE OR REPLACE SOURCE App11_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App11_SampleStream;
CREATE OR REPLACE TARGET App11_NullTarget using NullWriter()
INPUT FROM App11_SampleStream;
END APPLICATION App11;
DEPLOY APPLICATION App11;
START APPLICATION App11;

STOP APPLICATION App12;
UNDEPLOY APPLICATION App12;
DROP APPLICATION App12 CASCADE;
CREATE APPLICATION App12;
CREATE OR REPLACE SOURCE App12_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App12_SampleStream;
CREATE OR REPLACE TARGET App12_NullTarget using NullWriter()
INPUT FROM App12_SampleStream;
END APPLICATION App12;
DEPLOY APPLICATION App12;
START APPLICATION App12;

STOP APPLICATION App13;
UNDEPLOY APPLICATION App13;
DROP APPLICATION App13 CASCADE;
CREATE APPLICATION App13;
CREATE FLOW AgentFlow13;
CREATE OR REPLACE SOURCE App13_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App13_SampleStream;
END FLOW AgentFlow13;
CREATE FLOW ServerFlow13;
CREATE OR REPLACE TARGET App13_NullTarget using NullWriter()
INPUT FROM App13_SampleStream;
END FLOW ServerFlow13;
END APPLICATION App13;
deploy application App13 on any in ServerDG1 with AgentFlow13 on any in Agents, ServerFlow13 on any in ServerDG1;
START APPLICATION App13;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application APP_KAFKA_DATASOURCES;
undeploy application APP_KAFKA_DATASOURCES;
drop application APP_KAFKA_DATASOURCES cascade;

CREATE APPLICATION APP_KAFKA_DATASOURCES;

CREATE OR REPLACE SOURCE SRC_FR_KAFKA_HOURLYTOTALS USING Global.FileReader (
  adapterName: 'FileReader',
  rolloverstyle: 'Default',
  blocksize: 64,
  skipbom: true,
  wildcard: 'kafka_hourly_total*.txt',
  directory: '@confDir@',
  includesubdirectories: false,
  positionbyeof: false ) 
PARSE USING Global.DSVParser (
  trimwhitespace: false,
  linenumber: '-1',
  columndelimittill: '-1',
  trimquote: true,
  ignoreemptycolumn: false,
  parserName: 'DSVParser',
  quoteset: '\"',
  handler: 'com.webaction.proc.DSVParser_1_0',
  charset: 'UTF-8',
  columndelimiter: ':',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  separator: ',',
  header: false,
  blockascompleterecord: false,
  rowdelimiter: '\n',
  nocolumndelimiter: false,
  headerlineno: 0 )
OUTPUT TO STREAM_SRC_FR_KAFKA_HOURLYTOTALS;

CREATE OR REPLACE TYPE SRC_FILECACHE_KAFKA_HOURLYTOTALS_Type (
 topic java.lang.String KEY,
 timerange java.lang.Integer,
 rawdatacount java.lang.Integer);

CREATE OR REPLACE CQ CQ_FR_KAFKA_HOURLYTOTALS_COLMAP
INSERT INTO STREAM_CQ_FR_KAFKA_HOURLYTOTALS_COLMAP
SELECT to_string(data[0]) as topic, to_int(data[1]) as timerange, to_int(data[2]) as rawdatacount, to_string(to_string(data[0])+ '-' + to_string(data[1])) as TopicTimeRange FROM STREAM_SRC_FR_KAFKA_HOURLYTOTALS s;

CREATE OR REPLACE EVENTTABLE ET_HOURLYTOTALS_KAFKADATA_FILE USING STREAM (
  name: 'STREAM_CQ_FR_KAFKA_HOURLYTOTALS_COLMAP' )
QUERY (
  keytomap: 'TopicTimeRange',
  replicas: 'all',
  persistPolicy: 'true' )
OF STREAM_CQ_FR_KAFKA_HOURLYTOTALS_COLMAP_Type;

CREATE WINDOW SLIDE_WND_HOURLYTOTALS_KAFKADATA_FILE OVER STREAM_CQ_FR_KAFKA_HOURLYTOTALS_COLMAP
KEEP WITHIN 1 DAY
PARTITION BY timerange;

END APPLICATION APP_KAFKA_DATASOURCES;

CREATE APPLICATION @APPNAME@;

CREATE TYPE @APPNAME@_completeRecord(
completedata com.fasterxml.jackson.databind.JsonNode);
CREATE stream @APPNAME@_CompleteRecordInJSONStream of @APPNAME@_completeRecord;

CREATE SOURCE @APPNAME@_src USING KafkaReader VERSION @KAFKA_VERSION@ ()
PARSE USING AvroParser (
schemaFileName: 'avroSchema'
)
OUTPUT TO @APPNAME@_Stream2;

CREATE CQ @APPNAME@_CQ1
 INSERT INTO @APPNAME@_CompleteRecordInJSONStream
 SELECT
 AvroToJson(y.data)
 from @APPNAME@_Stream2 y;

CREATE CQ @APPNAME@_GetNativeRecordInJSONCQ
INSERT INTO @APPNAME@_NativeRecordStream
SELECT
 completedata.get("Facility").toString() as Facility,
 completedata.get("MsgTime").toString() as MsgTime,
 completedata.get("PatientID").toString() as PatientID,
 completedata.get("OrderIdentifier").toString() as OrderIdentifier,
 completedata.get("OrderText").toString() as OrderText
FROM @APPNAME@_CompleteRecordInJSONStream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING Global.FileWriter ()
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@_NativeRecordStream;

END APPLICATION @APPNAME@;

CREATE OR REPLACE APPLICATION @AppName@;

CREATE OR REPLACE SOURCE CP_Oracle_source USING OracleReader (
  ConnectionURL: '',
  Tables: '',
  Username: '',
  Password: '',
  Fetchsize: 1 )
OUTPUT TO CP_EndToEnd_SF_Adapter_Stream;

CREATE OR REPLACE TARGET CP_SF_Target USING Global.SnowflakeWriter (
  connectionProfileName: '',
  streamingUpload: 'false',
  StreamingConfiguration: 'MaxParallelRequests=5, MaxRequestSizeInMB=5, MaxRecordsPerRequest=10000',
  useConnectionProfile: 'true',
  externalStageConnectionProfileName: '',
  uploadPolicy: 'eventcount:10000,interval:5m',
  Tables: 'QATEST.Test_CP,SANJAYPRATAP.SAMPLESCHEMA.SAMPLE_PK')
INPUT FROM CP_EndToEnd_SF_Adapter_Stream;

END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

STOP TestAlertsSMS.TestAlertsSmsApp;
UNDEPLOY APPLICATION TestAlertsSMS.TestAlertsSmsApp;
DROP APPLICATION TestAlertsSMS.TestAlertsSmsApp CASCADE;

CREATE APPLICATION TestAlertsSmsApp;

CREATE source rawSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:No,
  wildcard:'alerts_csv.txt',
  coldelimiter:' ',
  positionByEOF:false
) OUTPUT TO rawStream;

CREATE STREAM MyAlertStream OF Global.AlertEvent;

CREATE CQ GenerateMyAlerts
INSERT INTO MyAlertStream (name, keyVal, severity, flag, message)
SELECT "Testing Alerts", trimStr(data[0]), trimStr(data[1]), trimStr(data[2]), trimStr(data[3])
FROM rawStream s;

CREATE TARGET output2 USING SysOut(name : alertsrecevied) input FROM MyAlertStream;

CREATE SUBSCRIPTION smsAlertSubscription4 USING ClickatellSMSAdapter
(
clickatelluserName:"@Alerts_smsuser@",
CLICKaTELLpasSWORD:"@Alerts_smspassword@",
clickatellapiID:"@Alerts_smsid@",
userIds:"@Alerts_smsuserid@",
threadCount:"@Alerts_smsthreadcount@",
senderNumber:"@Alerts_smssenderno@",
phoneNumberList:"@Alerts_smsphonelist@" ) INPUT FROM MyAlertStream;

END APPLICATION TestAlertsSmsApp;
DEPLOY APPLICATION TestAlertsSMS.TestAlertsSmsApp;
START TestAlertsSMS.TestAlertsSmsApp;

--spool on to '/Users/jeyaselvan/Product/spoolfile.log';
select * from Oracle12C_To_Oracle12CApp_ExceptionStore;
--spool off;

stop CSVToHBase;
undeploy application CSVToHBase;
drop application CSVToHBase cascade;

CREATE APPLICATION CSVToHBase;

CREATE OR REPLACE SOURCE CSVPoller USING FileReader ( 
	directory:'/Users/ravipathak/webactionrepo/Product',
	WildCard:'smallpos.csv',
	positionByEOF:false
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

CREATE OR REPLACE TYPE CSVStream_Type  ( BUSINESS_NAME java.lang.String KEY, 
MERCHANT_ID java.lang.String, 
PRIMARY_ACCOUNT_NUMBER java.lang.String  
 ) ;

CREATE OR REPLACE STREAM CSVTypeStream OF CSVStream_Type;

CREATE OR REPLACE CQ CQ1 
INSERT INTO CSVTypeStream
SELECT data[0],data[1],data[2]
FROM CsvStream;

CREATE OR REPLACE TARGET Target1 USING SysOut ( 
  name: "dstream"
 ) 
INPUT FROM CsvStream;

CREATE OR REPLACE TARGET Target2 using HBaseWriter(
 HBaseConfigurationPath:"/Users/ravipathak/soft/hbase-1.1.5/conf/hbase-site.xml",
  Tables: "maprtest.maprdata",
  --FamilyNames: "maprdata",
  BatchPolicy: "eventCount:1")
INPUT FROM CSVTypeStream;

END APPLICATION CSVToHBase;

deploy application CSVToHBase;
start CSVToHBase;

create Target @TARGET_NAME@ using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (

)
input from @STREAM@;

CREATE APPLICATION @APPNAME@ RECOVERY 5 second interval;
CREATE SOURCE @APPNAME@_src USING OracleReader ()
OUTPUT TO @APPNAME@_Stream;

CREATE TARGET @APPNAME@_trgt USING AzureBlobWriter()
format using DSVFormatter ()
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE FLOW @appname@AgentFlow;
    CREATE SOURCE @parquetsrc@ USING FileReader (
    wildcard: '',
    directory: '',
    positionbyeof: false )
    PARSE USING ParquetParser (
    )
    OUTPUT TO @appname@Stream;
END FLOW @appname@AgentFlow;

CREATE FLOW @appname@serverFlow;
    CREATE CQ @appname@CQ
    INSERT INTO @appname@CqOut
        SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;
    
    CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
    filename: '',
    directory: '',
    flushpolicy: 'EventCount:1,Interval:30s',
    rolloverpolicy: 'EventCount:10000,Interval:30s' )
    FORMAT USING ParquetFormatter  (
    schemaFileName: ''
    )
    INPUT FROM @appname@CqOut;
END FLOW @appname@serverFlow;

END APPLICATION @appname@;
DEPLOY APPLICATION @appname@ with @appname@AgentFlow in Agents, @appname@ServerFlow in default;
start application @appname@;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes1',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource2 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes2',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource3 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes3',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget2 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget3 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:10,Interval:60',
  CommitPolicy: 'EventCount:10,Interval:60',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget4 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:11,Interval:120',
  CommitPolicy: 'EventCount:11,Interval:120',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget5 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:3,Interval:120',
  CommitPolicy: 'EventCount:3,Interval:120',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  ExcludedTables:'QATEST.ORACTOCQL_ALLDATATYPES',
  Password: '+hbb060plSWQwscvI105cg==',
  Password_encrypted: true
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE TARGET FWTarget USING FileWriter(
	name:CassandraOuput,
	filename:'OracToFw.log',
	flushpolicy : 'interval:120,eventcount:3',
	rolloverpolicy : 'interval:300s'
)
FORMAT USING DSVFormatter()
INPUT FROM Oracle_ChangeDataStream;
create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;


END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

STOP APPLICATION SystemTimeTester.SystemTimeWindows;
UNDEPLOY APPLICATION SystemTimeTester.SystemTimeWindows;
DROP APPLICATION SystemTimeTester.SystemTimeWindows cascade;

CREATE APPLICATION SystemTimeWindows;

CREATE TYPE RandomData(
bankNumber int KEY,
bankName String
);


CREATE SOURCE ranDataSource using StreamReader(
OutputType: 'SystemTimeTester.RandomData',
noLimit: 'false',
isSeeded: 'true',
maxRows: 0,
iterations: 30,
iterationDelay: 1000,
StringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',
NumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]R'
)OUTPUT TO CSVDataStream;

CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT (data[0]), data[1]
FROM CSVDataStream;

CREATE @WINDOWTYPE@ WINDOW tierone OVER RandomDataStream keep within 20 second;

CREATE STREAM onetwostream OF RandomData;

CREATE CQ onetwocq
INSERT INTO onetwostream
SELECT bankNumber,bankName
FROM tierone
where bankName ='bofa'
order by bankNumber;

CREATE @WINDOWTYPE@ WINDOW tiertwo OVER onetwostream keep within 40 second;

CREATE STREAM twothreestream OF RandomData;

CREATE CQ twothreecq
INSERT INTO twothreestream
SELECT bankNumber,bankName
FROM tierTwo
where bankName ='bofa'
order by bankNumber;

CREATE @WINDOWTYPE@ WINDOW tierthree OVER twothreestream keep within 1 minute;

CREATE WACTIONSTORE MyDataActivity
CONTEXT OF RandomData
EVENT TYPES(RandomData )
@PERSIST-TYPE@

Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
SELECT bankNumber,bankName from @FROMSTREAM@
where bankName ='bofa'
order by bankNumber
LINK SOURCE EVENT;


END APPLICATION SystemTimeWindows;
deploy application SystemTimeWindows;
start application SystemTimeWindows;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;;

Create Source Ojetsrc Using Ojet
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;


create target AzureTarget1 using AzureSQLDWHWriter (
		CoNNectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',  
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;


create target AzureTarget2 using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        UserName: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@', 
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;


create target AzureTarget3 using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        UserName: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@', 
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

create target AzureTarget4 using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        UserName: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@', 
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

use PosTester;
DROP STREAM MerchantTxRateOnlyStream;

STOP APPLICATION @appName@;
UNDEPLOY APPLICATION @appName@;
DROP APPLICATION @appName@ CASCADE;

CREATE APPLICATION @appName@;

CREATE OR REPLACE TYPE @EventType@ (
 CC_Number java.lang.String,
 Amount java.lang.String,
 TXN_Type java.lang.String,
 TXN_Timestamp java.lang.String);

CREATE SOURCE @sourceComp@ USING Global.FileReader (
  rolloverstyle: 'Default',
  blocksize: 64,
  wildcard: '@sourceFileName@',
  skipbom: true,
  directory: '@sourceFileDir@',
  includesubdirectories: false,
  positionbyeof: false )
PARSE USING Global.DSVParser (
  trimwhitespace: true,
  commentcharacter: '',
  linenumber: '-1',
  columndelimiter: ',',
  trimquote: true,
  columndelimittill: '-1',
  eventtype: '@EventType@',
  ignoreemptycolumn: false,
  separator: ':',
  quoteset: '\"',
  charset: 'UTF-8',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  blockascompleterecord: false,
  rowdelimiter: '\n',
  nocolumndelimiter: false,
  headerlineno: 0,
  header: false )
OUTPUT TO @sourceOutStream@;

CREATE OR REPLACE STREAM @sourceOutStream@ OF @EventType@;

CREATE CQ @match_patternCQ@
INSERT INTO @patternOutStream@
SELECT
LIST(A,B) as events,
COUNT(B) as count
FROM @sourceOutStream@ t
MATCH_PATTERN T A+ (W|B)
DEFINE
	A = t(TXN_Type = 'AUTH/HOLD'),
	B = t(TXN_Type = 'CHARGE'),
	T = TIMER(interval 3 minute),
	W = WAIT(T)
PARTITION BY t.CC_Number;;

CREATE CQ @cqForTarget1@
INSERT INTO @target1InStream@
SELECT events FROM @patternOutStream@ a
WHERE a.count > 0;;

CREATE CQ @cqForTarget2@
INSERT INTO @target2InStream@
SELECT events FROM @patternOutStream@ a
WHERE a.count = 0;;

CREATE TARGET @target1@ USING Global.FileWriter (
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  rolloverpolicy: 'EventCount:250000',
  directory: '@targetFilesDir@',
  filename: '@targetFile1@' )
FORMAT USING Global.JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n' )
INPUT FROM @target1InStream@;

CREATE TARGET @target2@ USING Global.FileWriter (
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  directory: '@targetFilesDir@',
  filename: '@targetFile2@',
  rolloverpolicy: 'EventCount:250000' )
FORMAT USING Global.JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n' )
INPUT FROM @target2InStream@;

END APPLICATION @appName@;

create application CSVToXML;
create source CSVSource using FileReader (
	directory:'Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
	filename:'posdata_XML',
	rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'
)
format using XMLFormatter (
	rootelement:'document',
	elementtuple:'MerchantName:merchantid:text=merchantname'
)
input from TypedCSVStream;
end application CSVToXML;

deploy application CSVToXML;
start application CSVToXML;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

--create application @APPNAME@ Recovery 5 Second Interval;
create application @APPNAME@;

create or replace type @APPNAME@emp_type(
Sno integer,
Empname string,
Doj string,
Country string,
CompanyName string
);

CREATE OR REPLACE SOURCE @APPNAME@File_Source1 using Filereader(
	directory:'@DIRECTORY@',
  wildcard:'File_empdata.csv',
  positionByEOF:false
)parse using dsvParser(
    header:'yes'
)
OUTPUT TO @APPNAME@FileSource_Stream1,
OUTPUT TO @APPNAME@FileSource_Stream1_automap MAP(filename:'File_empdata.csv');

CREATE OR REPLACE SOURCE @APPNAME@Init_Source1 USING DatabaseReader  (
  Username: 'qatest',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.EMP_INIT',
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: 'qatest'
 )
OUTPUT TO 	@APPNAME@InitialLoad_Stream1;

CREATE OR REPLACE SOURCE @APPNAME@CDC_Source1 USING OracleReader  (
  StartTimestamp: 'null',
  SupportPDB: false,
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  FetchSize: 1,
  DDLCaptureMode: 'All',
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.EMP',
  adapterName: 'OracleReader',
  Password: 'qatest',
  TransactionBufferType: 'Memory',
  DictionaryMode: 'OnlineCatalog',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: null,
  ReaderType: 'LogMiner',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '1MB',
  compression: true,
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO @APPNAME@CDC_Stream1 ;

create or replace stream @APPNAME@FileSource_cdc_init_TypedStream of @APPNAME@emp_type;
create or replace cq @APPNAME@file_typed_streamcq
insert into @APPNAME@FileSource_cdc_init_TypedStream
SELECT to_int(data[0]),
data[1],
data[2],
data[3],
data[4]
from @APPNAME@FileSource_Stream1;

create or replace cq @APPNAME@cdc_typed_streamcq
insert into @APPNAME@FileSource_cdc_init_TypedStream
SELECT to_int(data[0]),
data[1],
data[2],
data[3],
data[4]
from @APPNAME@CDC_Stream1;

create or replace cq @APPNAME@init_typed_streamcq
insert into @APPNAME@FileSource_cdc_init_TypedStream
SELECT to_int(data[0]),
data[1],
data[2],
data[3],
data[4]
from @APPNAME@InitialLoad_Stream1;

CREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target1 USING CassandraCosmosDBWriter  (
  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --CheckPointTable: '',
  FlushPolicy: 'EventCount:1000,Interval:60',
  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',
  AccountKey:'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  --ConsistencyLevel:'fh',
  --port:'10505',
  Tables: 'test.file_emp',
  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',
  adapterName: 'CassandraCosmosDBWriter'
 )
INPUT FROM @APPNAME@FileSource_Stream1_automap;

CREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target2 USING CassandraCosmosDBWriter  (
  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --CheckPointTable: '',
  FlushPolicy: 'EventCount:1000,Interval:60',
  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',
  AccountKey: 'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  --ConsistencyLevel:'fh',
  --port:'10505',
  Tables: 'QATEST.EMP,test.cdc_emp',
  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',
  adapterName: 'CassandraCosmosDBWriter'
 )
INPUT FROM @APPNAME@CDC_Stream1;

CREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target3 USING CassandraCosmosDBWriter  (
  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --CheckPointTable: '',
  FlushPolicy: 'EventCount:1000,Interval:60',
  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',
  AccountKey: 'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  --ConsistencyLevel:'fh',
  --port:'10505',
  Tables: 'QATEST.EMP_INIT,test.initialload_emp',
  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',
  adapterName: 'CassandraCosmosDBWriter'
 )
INPUT FROM @APPNAME@InitialLoad_Stream1;


CREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target4 USING CassandraCosmosDBWriter  (
  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --CheckPointTable: '',
  FlushPolicy: 'EventCount:1000,Interval:60',
  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',
  AccountKey: 'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  --ConsistencyLevel:'fh',
  --port:'10505',
  Tables: 'test.file_cdc_emp',
  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',
  adapterName: 'CassandraCosmosDBWriter'
 )
INPUT FROM @APPNAME@FileSource_cdc_init_TypedStream;

create or replace target @APPNAME@sys_file_tgt using sysout(
name:'foo_file'
)input from @APPNAME@FileSource_Stream1;

create or replace target @APPNAME@sys_cdc_tgt using sysout(
name:'foo_cdc'
)input from @APPNAME@CDC_Stream1;

create or replace target @APPNAME@sys_init_tgt using sysout(
name:'foo_init'
)input from @APPNAME@InitialLoad_Stream1;

End Application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]);

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

stop OracleReaderToDBWriter;
Undeploy application OracleReaderToDBWriter;
alter application OracleReaderToDBWriter;
CREATE OR REPLACE SOURCE Oraclesrc USING OracleReader  ( 
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Tables: 'QATEST.Orcalesrc',
  FetchSize: '3'
 ) 
OUTPUT TO OrcStrm;
alter application OracleReaderToDBWriter recompile;
DEPLOY APPLICATION OracleReaderToDBWriter with Hz_Agent_flow on any in AGENTS;
start application OracleReaderToDBWriter;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE CQ @CQName@ INSERT INTO @CQOUTPUTSTREAM@ select @FUNCTION@ from @SRCINPUTSTREAM@ s ;

create Target @targetsys@ using SysOut(name:Foo2) input from @CQOUTPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:1',
  Tables: '@TargetTableMapping@'
 ) 
INPUT FROM @CQOUTPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ Recovery 5 second interval;

create stream @APPNAME@_UserdataStream of Global.WAEvent;

create type @APPNAME@_Order_type(
id int,
order_id int,
zipcode int,
category String,
tablename string
);

CREATE OR REPLACE SOURCE @APPNAME@S1 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.order_%'
)
OUTPUT TO @APPNAME@_OrdersStream;

CREATE OR REPLACE SOURCE @APPNAME@S2 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_2',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.second_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream2;

CREATE OR REPLACE SOURCE @APPNAME@S3 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_3',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.third_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream3;

CREATE OR REPLACE SOURCE @APPNAME@S4 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_4',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.fourth_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream4;


Create CQ @APPNAME@_CQUser
insert into @APPNAME@_UserdataStream
select 
putuserdata (data,'Fileowner','FIRST_ORDER') from @APPNAME@_OrdersStream data;


Create CQ @APPNAME@_CQUser2
insert into @APPNAME@_UserdataStream
select 
putuserdata (data2,'Fileowner','SECOND_ORDER') from @APPNAME@_OrdersStream2 data2;


Create CQ @APPNAME@_CQUser3
insert into @APPNAME@_UserdataStream
select 
putuserdata (data3,'Fileowner','THIRD_ORDER') from @APPNAME@_OrdersStream3 data3;


Create CQ @APPNAME@_CQUser4
insert into @APPNAME@_UserdataStream
select 
putuserdata (data4,'Fileowner','FOURTH_ORDER') from @APPNAME@_OrdersStream4 data4;

create stream @APPNAME@_OrderTypedStream1 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream2 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream3 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream4 of @APPNAME@_Order_type;

CREATE CQ @APPNAME@_fin_cq
INSERT INTO @APPNAME@_OrderTypedStream1
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FIRST_ORDER';

CREATE CQ @APPNAME@_fin_cq2
INSERT INTO @APPNAME@_OrderTypedStream2
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'SECOND_ORDER';

CREATE CQ @APPNAME@_fin_cq3
INSERT INTO @APPNAME@_OrderTypedStream3
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'THIRD_ORDER';

CREATE CQ @APPNAME@_fin_cq4
INSERT INTO @APPNAME@_OrderTypedStream4
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FOURTH_ORDER';


create Target @APPNAME@T1 using ADLSGen2Writer(
        filename:'event_data.csv',
        directory:'%category%/%tablename%',
       	accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        uploadpolicy:'eventcount:8,interval:20s'
)
format using DSVFormatter (
    header:'true'
)
input from @APPNAME@_OrderTypedStream1; 

create Target @APPNAME@T2 using ADLSGen2Writer(
        filename:'event_data.xml',
        directory:'%category%/%tablename%',
		accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
   		uploadpolicy:'eventcount:8,interval:20s'
)
format using XMLFormatter (
  elementtuple: 'Order_id:id:order_id:zipcode:category:text=tablename',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from @APPNAME@_OrderTypedStream2; 

create Target @APPNAME@T3 using ADLSGen2Writer(
        filename:'event_data.avro',
        directory:'%category%/%tablename%',
		accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
  		uploadpolicy:'eventcount:8,interval:20s'
)
format using AvroFormatter (
  formatAs: 'Default',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA-FILE@'
)
input from @APPNAME@_OrderTypedStream3; 


create Target @APPNAME@T4 using ADLSGen2Writer(
        filename:'event_data.json',
        directory:'%category%/%tablename%',
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
  		uploadpolicy:'eventcount:8,interval:20s'
)
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@_OrderTypedStream4;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

CREATE OR REPLACE PROPERTYVARIABLE RetryPolicy='timeOut=10,retryInterval=10,maxRetries=30';
CREATE OR REPLACE PROPERTYVARIABLE KafkaBrokerAddress='localhost:9099';
--CREATE OR REPLACE PROPERTYVARIABLE KafkaConfig='batch.size=1048576;linger.ms=30000';

STOP @WRITERAPPNAME@;
UNDEPLOY APPLICATION @WRITERAPPNAME@;
DROP APPLICATION @WRITERAPPNAME@ CASCADE;

CREATE APPLICATION @WRITERAPPNAME@ Recovery 5 second interval;
create flow @WRITERAPPNAME@AgentFlow;
CREATE SOURCE @WRITERAPPNAME@S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.oracle_kw_quiesce_simple_test%',
	FetchSize: '1',
	connectionRetryPolicy:'$RetryPolicy',
	DictionaryMode: onlineCatalog
)
OUTPUT TO @WRITERAPPNAME@SS;
end flow @WRITERAPPNAME@AgentFlow;
create flow @WRITERAPPNAME@serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;
create stream @WRITERAPPNAME@out_cq_select_SS_1 of global.waevent;

CREATE OR REPLACE CQ @WRITERAPPNAME@cq_select_SS1 
INSERT INTO @WRITERAPPNAME@out_cq_select_SS_1
select *
from @WRITERAPPNAME@ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_QUIESCE_SIMPLE_TEST';


create Target @WRITERAPPNAME@TARGET1 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@WRITERAPPNAME@_dsv_sync_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(COMMIT_TIMESTAMP)',
Mode:'sync',
KafkaConfig: 'batch.size=1048576;linger.ms=300000;')
FORMAT USING dsvFormatter ()
input from @WRITERAPPNAME@ss;

create Target @WRITERAPPNAME@TARGET2 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@WRITERAPPNAME@_json_sync_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(COMMIT_TIMESTAMP)',
Mode:'sync',
KafkaConfig: 'batch.size=1048576;linger.ms=300000;')
FORMAT USING jsonFormatter ()
input from @WRITERAPPNAME@out_cq_select_SS_1;

create Target @WRITERAPPNAME@TARGET3 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@WRITERAPPNAME@_avro_sync_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(COMMIT_TIMESTAMP)',
Mode:'sync',
KafkaConfig: 'batch.size=1048576;linger.ms=300000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTestQuiesce_sync_CQ.avsc')
input from @WRITERAPPNAME@out_cq_select_SS_1;

end flow @WRITERAPPNAME@serverFlow;
end application @WRITERAPPNAME@;
@DEPLOYAPP@;
start @WRITERAPPNAME@;


stop application @READERAPPNAME@;
undeploy application @READERAPPNAME@;
drop application @READERAPPNAME@ cascade;
CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @READERAPPNAME@_DSV_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@WRITERAPPNAME@_dsv_sync_CQ',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO @READERAPPNAME@KafkaReaderStream1;

CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
rolloverpolicy:'filesize:500M',
filename:'@READERAPPNAME@_dsv_sync_CQ')
FORMAT USING DSVFormatter()
INPUT FROM @READERAPPNAME@KafkaReaderStream1;

CREATE SOURCE @READERAPPNAME@_JSON_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@WRITERAPPNAME@_json_sync_CQ',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO @READERAPPNAME@KafkaReaderStream2;

CREATE SOURCE @READERAPPNAME@_AVRO_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@WRITERAPPNAME@_avro_sync_CQ',
        startOffset:0          
)
PARSE USING AvroParser (
schemaFileName:'KafkaAvroTestQuiesce_sync_CQ.avsc')
OUTPUT TO @READERAPPNAME@KafkaReaderStream3;


end application @READERAPPNAME@;
deploy application @READERAPPNAME@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.XMLNodeEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (
  ProviderName: '',
  UserName: '',
  transactionpolicy: '',
  Provider: '',
  Ctx: '',
  EnableTransaction: '',
  QueueName: '',
  Topic:'',
  Password: '' )
PARSE USING XMLParserV2 (
  rootnode:'/JMSXMLIN'
  )
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

CREATE CQ @APPNAME@CQ
INSERT INTO @APPNAME@XMLStream
SELECT
  data.element("companyName").attributeValue("merchantId") as merchantId,
  data.element("companyName").getText() as companyName
FROM @APPNAME@PersistStream@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING Global.FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@XMLStream;

END APPLICATION @APPNAME@;

--
-- Canon Test W30
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for an unpartitioned jumping count window
--
-- S -> JWc5u -> CQ -> WS
--


UNDEPLOY APPLICATION NameW30.W30;
DROP APPLICATION NameW30.W30 CASCADE;
CREATE APPLICATION W30 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW30;


CREATE SOURCE CsvSourceW30 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW30;


END FLOW DataAcquisitionW30;



CREATE FLOW DataProcessingW30;

CREATE TYPE DataTypeW30 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW30 OF DataTypeW30;

CREATE CQ CSVStreamW30_to_DataStreamW30
INSERT INTO DataStreamW30
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW30;

CREATE JUMPING WINDOW JWc5uW30
OVER DataStreamW30
KEEP 5 ROWS;

CREATE WACTIONSTORE WactionStoreW30 CONTEXT OF DataTypeW30
EVENT TYPES ( DataTypeW30 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWc5uW30_to_WactionStoreW30
INSERT INTO WactionStoreW30
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc5uW30;

END FLOW DataProcessingW30;



END APPLICATION W30;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;;

Create Source Ojetsrc1 Using Ojet
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source Ojetsrc2 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source Ojetsrc3 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source Ojetsrc4 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source Ojetsrc5 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

create target AzureTarget using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',  
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;
END APPLICATION ADW;
deploy application ADW;
start application ADW;

STOP OuterJoinTester.OJApp;
UNDEPLOY APPLICATION OuterJoinTester.OJApp;
DROP APPLICATION OuterJoinTester.OJApp cascade;

CREATE APPLICATION OJApp;


CREATE TYPE bankData
(
ucID Integer,
ucLong long,
ucDate DateTime,
ucDouble Double KEY
);


CREATE STREAM wsStream OF bankData;

CREATE CACHE cache1 USING CsvReader(
directory: '@TEST-DATA-PATH@',
wildcard: 'ucData.csv',
header: No,
columndelimiter: ',',
trimquote: false
) QUERY (keytomap:'ucDouble') OF bankData;



END APPLICATION OJApp;

-- The PosApp sample application demonstrates how a credit card
-- payment processor might use Striim to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.


STOP admin.PosApp;
UNDEPLOY APPLICATION admin.PosApp;
Drop Application admin.PosApp cascade;
CREATE APPLICATION PosApp;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING FileReader (
  directory:'/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/appData',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells Striim that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells Striim to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvToPosData

CREATE CQ CsvToPosData
INSERT INTO PosDataStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP.
--
-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as
-- the types to be used in PosDataStream, which is created automatically.

-- The DATETIME field from the source is converted to both a dateTime
-- value, used as the event timestamp by the application, and (via the
-- function) an integer hourValue, which is used to look up
-- historical hourly averages from the HourlyAveLookup cache,
-- discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)
-- 17 (hourValue)
-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateOnly
--
-- The PosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: '/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/appData',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;


-- WAction store MerchantActivity
--
-- The following group of statements create and populate the MerchantActivity
-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced
-- with merchant details from NameLookup cache and with latitude and longitude
-- values from the USAddressData cache.

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
PERSIST EVERY 10 second USING ( JDBC_DRIVER:'org.apache.derby.jdbc.ClientDriver',  JDBC_URL:'jdbc:derby://192.168.1.5:1527/wactionrepos', JDBC_USER:'waction', JDBC_PASSWORD:'w@ct10n', pu_name:derby, DDL_GENERATION:'create-or-extend-tables',  LOGGING_LEVEL:'SEVERE' );


CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/appData',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false,
  trimwhitespace:true
) 
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: '/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/appData',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'zip') OF USAddressData;


CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

-- CQ GenerateAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;

END APPLICATION PosApp;

CREATE DASHBOARD USING "/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/PosAppDashboard.json";

CREATE APPLICATION KafkaReader;

CREATE OR REPLACE TYPE KafkaSourceStr2_Type  ( seq java.lang.Integer
 );

CREATE OR REPLACE STREAM KafkaSourceStr2 OF KafkaSourceStr2_Type;

CREATE  JUMPING WINDOW GetTargData OVER KafkaSourceStr2 KEEP 1000000 ROWS;

CREATE OR REPLACE SOURCE KafkaSource USING KafkaReader VERSION '0.11.0' (
  KafkaConfigPropertySeparator: ';',
  startOffset: 0,
  adapterName: 'KafkaReader',
  Topic: 'kafkaTopic7',
  AutoMapPartition: true,
  brokerAddress: 'localhost:9092',
  KafkaConfigValueSeparator: '=',
  KafkaConfig: 'max.partition.fetch.bytes=10485760;fetch.min.bytes=1048576;fetch.max.wait.ms=1000;receive.buffer.bytes=2000000;poll.timeout.ms=10000;request.timeout.ms=60001;session.timeout.ms=60000'
 )
 PARSE USING DSVParser  (
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: 0,
  nocolumndelimiter: false,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: false,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 )
OUTPUT TO KafkaSourceStr1 ;

CREATE OR REPLACE CQ GetKafkaDataQuery
INSERT INTO KafkaSourceStr2
SELECT TO_INT(data[1]) as seq
FROM KafkaSourceStr1;

CREATE  TYPE KafkaSourceStr3_Type  ( SUMKafkaSourceStr2seq java.lang.Long
 );

CREATE STREAM KafkaSourceStr3 OF KafkaSourceStr3_Type;

CREATE OR REPLACE CQ GetTheSum
INSERT INTO KafkaSourceStr3
SELECT SUM(GetTargData .seq)
FROM GetTargData;

CREATE OR REPLACE TARGET KafkaFile USING FileWriter  (
  filename: 'TargetResults',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:10000,interval:30',
  adapterName: 'FileWriter',
  directory: '@FEATURE-DIR@/logs',
  rolloverpolicy: 'eventcount:10000,interval:30s'
 )
FORMAT USING DSVFormatter  (   nullvalue: 'NULL',
  standard: 'none',
  handler: 'com.webaction.proc.DSVFormatter',
  formatterName: 'DSVFormatter',
  usequotes: 'false',
  rowdelimiter: '\n',
  quotecharacter: '\"',
  header: 'false',
  columndelimiter: ','
 )
INPUT FROM KafkaSourceStr3;

END APPLICATION KafkaReader;

drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

CREATE TYPE @APPNAME@type1 (
 companyName java.lang.String,
 merchantId java.lang.String,
 city java.lang.String);

CREATE STREAM @APPNAME@TypedStream OF @APPNAME@type1 PARTITION BY city;

CREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (
  wildcard: '',
  positionByEOF: false,
  directory: ''
  )
PARSE USING DSVParser (
header:'true'
)
OUTPUT TO @APPNAME@Stream;

CREATE OR REPLACE CQ @APPNAME@CQ
INSERT INTO @APPNAME@TypedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantID,
TO_STRING(data[10]) as city
FROM @APPNAME@Stream;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@TypedStream;

END APPLICATION @APPNAME@;

STOP APPLICATION orrs;
UNDEPLOY APPLICATION orrs;
DROP APPLICATION orrs CASCADE;
CREATE APPLICATION orrs recovery 5 second interval;
Create Source OraSource Using OracleReader 
	(
	 Username:'user-name',	
	 Password:'password',
	 ConnectionURL: 'src_url',
	 Tables:'src_table',
	 FilterTransactionBoundaries:true,
	 FetchSize:'fetch-size'
	) Output To LCRStream;
	
	CREATE TARGET RSTarget USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: '@ACCESS-KEY-ID@',
	  --secretaccesskey: '@SECRET-ACCESS-KEY@',
	  S3IAMRole:'@IAMROLE@',
	  Tables: 'tgt_table',
	  uploadpolicy:'eventcount:300,interval:1m'
	) INPUT FROM LCRStream;
END APPLICATION orrs;
DEPLOY APPLICATION orrs;
START APPLICATION orrs;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@;
CREATE SOURCE  @srcName@  USING Global.OJet ( 
  Username:'@srcusername@',
  Password:'@srcpassword@', 
  Tables:'@srcschema@.@srctable@',
  ConnectionURL:'@srcurl@',
  Tables:'@srcschema@.@srctable@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries:'true'
) 
OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@ USING DatabaseWriter (
  CheckPointTable: 'CHKPOINT', 
  ReplicationSlotName:'test_slot',
  Username:'@tgtusername@',
  Password:'@tgtpassword@',
  ConnectionURL:'@tgturl@',
  adapterName:'PostgreSQLReader',
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@'
)
INPUT FROM @outstreamname@;

End APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'MINER.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@(
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:100,Interval:120',
  CommitPolicy: 'EventCount:100,Interval:120',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;


create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;

deploy application DBRTOCW on ANY in default;

start application DBRTOCW;

stop application MSSQLTransactionSupportAutoDisableCdcTrue;
undeploy application MSSQLTransactionSupportAutoDisableCdcTrue;
drop application MSSQLTransactionSupportAutoDisableCdcTrue cascade;

CREATE APPLICATION MSSQLTransactionSupportAutoDisableCdcTrue recovery 1 second interval;

Create Source ReadFromMSSQL3
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
AutoDisableTableCDC:'true',
FetchTransactionMetadata:'true',
FilterTransactionBoundaries: false,
Compression:'false',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportAutoDisableCdcTrueStream;


CREATE TARGET WriteToMSSQL3 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC,@@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportAutoDisableCdcTrueStream;

CREATE TARGET MSSqlReaderOutput3 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportAutoDisableCdcTrueStream; 


CREATE OR REPLACE TARGET MSSQLFileOut3 USING FileWriter  ( 
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  directory:'@FEATURE-DIR@/logs/',
  filename: 'TransactionSupportAutoDisableTableCdcTrue.txt'
 ) 
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 ) 
INPUT FROM MSSQLTransactionSupportAutoDisableCdcTrueStream;

END APPLICATION MSSQLTransactionSupportAutoDisableCdcTrue;
deploy application MSSQLTransactionSupportAutoDisableCdcTrue;
start application MSSQLTransactionSupportAutoDisableCdcTrue;

Stop @APPNAME@_App;
undeploy application @APPNAME@_App;
drop application @APPNAME@_App cascade;

CREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;
CREATE SOURCE @APPNAME@_Source USING Global.Ojet

(
  Username:'@SOURCE_USER@',
  Password:'@SOURCE_PASSWORD@',
  ConnectionURL:'@SOURCE_URL@',
  Tables:'@SOURCE_TABLES@'
)
OUTPUT TO @APPNAME@_Stream;

CREATE TARGET @APPNAME@_SYS USING SysOut (
  name: '@APPNAME@_SYS' )
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_CDB_Target USING DatabaseWriter

(
  Username:'@TARGET_CDB_USER@',
  ConnectionURL:'@TARGET_CDB_URL@',
  Tables:'@TARGET_CDB_TABLES@',
  Password:'@TARGET_CDB_PASSWORD@',
  BatchPolicy:'EventCount:1,Interval:1',
  CommitPolicy:'EventCount:1,Interval:1'
)
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_PDB_Target USING DatabaseWriter

(
  Username:'@TARGET_PDB_USER@',
  ConnectionURL:'@TARGET_PDB_URL@',
  Tables:'@TARGET_PDB_TABLES@',
  Password:'@TARGET_PDB_PASSWORD@',
  BatchPolicy:'EventCount:1,Interval:1',
  CommitPolicy:'EventCount:1,Interval:1'
)
INPUT FROM @APPNAME@_Stream;

End APPLICATION @APPNAME@_App;

STOP TestAlertsEmail.TestAlertsEmailApp;
UNDEPLOY APPLICATION TestAlertsEmail.TestAlertsEmailApp;
DROP APPLICATION TestAlertsEmail.TestAlertsEmailApp CASCADE;

CREATE APPLICATION TestAlertsEmailApp;

CREATE source rawSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:No,
  wildcard:'alerts_csv.txt',
  coldelimiter:' ',
  positionByEOF:false
) OUTPUT TO rawStream;

CREATE STREAM MyAlertStream OF Global.AlertEvent;
CREATE CQ GenerateMyAlerts
INSERT INTO MyAlertStream (name, keyVal, severity, flag, message)
SELECT "Testing Alerts", data[0], data[1], data[2], data[3]
FROM rawStream s;
CREATE TARGET output2 USING SysOut(name : alertsrecevied) input FROM MyAlertStream;

CREATE SUBSCRIPTION alertSubscription USING EmailAdapter
(SMTPUSER:'zalakalerts@gmail.com',
SMTPPASSWORD:'paloalto',
smtpurl:'smtp.gmail.com',
starttls_enable:'true',
smtp_auth:'true',
subject:"Test Email Alerts With Security Enabled",
emailList:"siddhika@striim.com,s.henry@striim.com,saranya@striim.com,invalidmailid.@striim.com",
userIds:'admin',
threadCount:"5",
senderEmail:"doga@striim.com"
)
INPUT FROM MyAlertStream;

END APPLICATION TestAlertsEmailApp;
DEPLOY APPLICATION TestAlertsEmail.TestAlertsEmailApp;
START TestAlertsEmail.TestAlertsEmailApp;

INPUT FROM @STREAM@;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@DataSrc USING OracleReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
)
OUTPUT TO @APPNAME@DataStream;

CREATE OR REPLACE TARGET @APPNAME@DataTrgt USING MongoDBWriter (
  ConnectionURL: '',
  Username: '',
  Password: '',
  collections: ''
  AuthDB: '',
  batchpolicy: 'EventCount:1000, Interval:30',
 )
INPUT FROM @APPNAME@DataStream;

CREATE OR REPLACE SOURCE @APPNAME@_src USING MongoDBReader (
  ConnectionURL: '',
  Username: '',
  password: '',
  authDB: '',
  collections: '',
  mode: 'Incremental'
  )
OUTPUT TO @APPNAME@stream;

CREATE CQ @APPNAME@CQ
INSERT INTO @APPNAME@CQSTREAM
SELECT data.get("NUM_COL").toString() AS NUM_COL,
  data.get("CHAR_COL").toString() AS CHAR_COL,
  data.get("VARCHAR2_COL").toString() AS VARCHAR2_COL,
  data.get("FLOAT_COL").toString() AS FLOAT_COL,
  data.get("BINARY_FLOAT_COL").toString() AS BINARY_FLOAT_COL,
  data.get("BINARY_DOUBLE_COL").toString() AS BINARY_DOUBLE_COL,
  data.get("DATE_COL").toString() AS DATE_COL,
  data.get("TIMESTAMP_COL").toString() AS TIMESTAMP_COL
FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING AvroFormatter (
schemaFileName: '@SCHEMAFILE@'
)
INPUT FROM @APPNAME@CQSTREAM;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING DSVFormatter ()
INPUT FROM @APPNAME@CQSTREAM;

END APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @SOURCE@ USING MSSQLReader
 (
   Username: '@LOGMINER-UNAME@',
   Password: '@LOGMINER-PASSWORD@',
   ConnectionURL: '@LOGMINER-URL@',
   DatabaseName:'qatest',
   Tables: '@SOURCE_TABLE@',
    Compression:false,
    AutoDisableTableCDC:false,FetchTransactionMetadata:true,
    StartPosition:'EOF'
 )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@1 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'false',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@2 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'false',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'true',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@3 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'true',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 1 second interval;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@',
  recordSelector: 'OH:MOH-SEG-ID=OH,OH2:OH2-SEG-ID=OH2,OHU:OHU-SEG-ID=OHU,OR1:OR1-SEG-ID=OR1,OR2:OR2-SEG-ID=OR2,OR3:OR3-SEG-ID=OR3,OR3:OR3-SEG-ID=OR3,OHM:OHM-SEG-ID=OR1,OD:OD-SEG-ID=OD,ODU:ODU-SEG-ID=ODU,OD1:OD1-SEG-ID=OD1,ODM:ODM-SEG-ID=ODM,OT:OT-SEG-ID=OT'
)
OUTPUT TO @APPNAME@Stream;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;

/*
create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:1,Interval:5',
  CommitPolicy: 'EventCount:1,Interval:5',
  Tables: 'QATEST.@table@'
)
input from @APPNAME@Stream;*/
end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

stop application MSSQLTransactionSupportFTBTrue;
undeploy application MSSQLTransactionSupportFTBTrue;
drop application MSSQLTransactionSupportFTBTrue cascade;

CREATE APPLICATION MSSQLTransactionSupportFTBTrue recovery 1 second interval;

Create Source ReadFromMSSQL1
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
FetchTransactionMetadata:'false',
FilterTransactionBoundaries: true,
Compression:'false',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportFTBTrueStream;


CREATE TARGET WriteToMSSQL1 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC@,@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportFTBTrueStream;

CREATE TARGET MSSqlReaderOutput1 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportFTBTrueStream; 


CREATE OR REPLACE TARGET MSSQLFileOut1 USING FileWriter  ( 
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  directory:'@FEATURE-DIR@/logs/',
  filename: 'TransactionSupportFTBTrue.txt'
 ) 
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 ) 
INPUT FROM MSSQLTransactionSupportFTBTrueStream;

END APPLICATION MSSQLTransactionSupportFTBTrue;
deploy application MSSQLTransactionSupportFTBTrue;
start application MSSQLTransactionSupportFTBTrue;

stop APPLICATION OrcToDWH;
undeploy APPLICATION OrcToDWH;
DROP APPLICATION OrcToDWH CASCADE;
CREATE APPLICATION OrcToDWH recovery 5 second interval;
Create Source OracleSource Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;


create target AzureTarget using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
        _h_TransportOptions:'connectionTimeout=30s, readTimeout=12s',
) INPUT FROM str;

END APPLICATION OrcToDWH;
deploy APPLICATION OrcToDWH;
start APPLICATION OrcToDWH;

CREATE OR REPLACE APPLICATION @AppName@;

Create Source @AppName@_source Using OracleReader(
  Username:'@username@',
  Password:'@password@',
  ConnectionURL:'@url@',
  Tables:'@srctableName@',
  Fetchsize:1
)
Output To @AppName@_Stream;
CREATE OR REPLACE TARGET @AppName@_SF_Target USING Global.SnowflakeWriter (
  streamingUpload: 'false',
  useConnectionProfile:'true',
  connectionProfileName: 'admin.@SFCP@',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  columnDelimiter: '|',
  tables: '@srctableName@,@trgtableName@',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:1,interval:5m',
  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',
  externalStageType: 'Local',
  adapterName: 'SnowflakeWriter',
  fileFormatOptions: 'null_if = \"\"' )
INPUT FROM @AppName@_Stream;


CREATE OR REPLACE TARGET @AppName@_DB_Target USING Global.DeltaLakeWriter (
connectionProfileName: 'admin.@DBCP@',
useConnectionProfile:'true',
  Tables: '@srctableName@,@DBtrgtableName@',
  uploadPolicy: 'eventcount:100000,interval:60s'
)

INPUT FROM @AppName@_Stream;
END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[0]) = '1' ;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

Create Source dataGenSrc Using MSSqlReader
(
 Username:'qatest',
 Password:'w3b@ct10n',
 DatabaseName:'qatest',
 ConnectionURL:'localhost:1433',
 Tables:'@tableNames@', 
 ConnectionPoolSize:1,
 StartPosition:'EOF'
 )
 Output To LCRStream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;

END APPLICATION DataGenSampleApp;

CREATE CQ @CQ_NAME@
INSERT INTO @EMB_STREAM@
@SELECT_QUERY@
FROM @STREAM@ e;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

cCREATE TARGET @TARGET@ USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: 'access_key',
	  --secretaccesskey: 'secret_access',
	  Tables: 'QATEST.%,QATEST.%',
	   S3IAMRole:'@IAMROLE@',
	uploadpolicy:'EventCount:7'
	) INPUT FROM @STREAM@;
	
end flow @APPNAME@_serverflow;

end application @APPNAME@;

stop application SalesForceReaderTest;
undeploy application SalesForceReaderTest;
drop application SalesForceReaderTest cascade;

CREATE APPLICATION SalesForceReaderTest recovery 5 second interval;

CREATE OR REPLACE SOURCE SFPoller USING SalesForceReader 
(
  sObjects:'newobj__c',
  --sObject:'Campaign',
  pollingInterval:'1 min',
  autoAuthTokenRenewal:'true',
  Username:'siddhika@webaction.com',
  Password:'webaction@1234',
  securityToken:'qhNbKKmafFpanz8Y2oiM89UhR',
  consumerKey:'3MVG9ZL0ppGP5UrBayz85eLnPg69gWLaE8pA3uzwcFCZ9s.J0mgE7AKvPCEhTaop4uYRbBaDnGXHjnLmngG6P',
  consumerSecret:'2500119200751301808',
  apiEndPoint:'https://ap2.salesforce.com',
  mode:'InitialLoad',
  startTimestamp:null
)
OUTPUT TO DataStream;

create target tout using sysout(name : 'out')input from DataStream;

/*
CREATE TARGET dbtar USING DatabaseWriter( 
	BatchPolicy:'EventCount:100,Interval:10',
	CommitPolicy:'EventCount:100,Interval:10',
	ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
	Username:'qatest',
	Password:'qatest',
  Tables:'Position__c,QATEST.POS columnmap(Id=Id,numnum=numnum__c)'
  -- Tables: 'newobj__c,QATEST.SFTABLE1 columnmap(Id=Id,CHECKBOOL=checkbool__c,CURR=curr__c,DT=dt__c,TIME1=time1__c,DtTime=DtTime__c,EMAILID=emailid__c,NUM=num__c,PERCNT=percnt__c,PHN=phn__c,TXTLONG=txtlong__c,URL1=url1__c,TXT=txt__c)'
   ---,loc__latitude=loc__latitude__s,loc__longitude=loc__longitude__s)' 
) INPUT FROM DataStream;

*/
CREATE OR REPLACE TARGET Target2 using FileWriter
(
  filename:'Obj123.json',
  directory:'/Users/siddhika/Product/',
  rolloverpolicy:'EventCount:1'
)
FORMAT USING JSONFormatter ()
INPUT FROM DataStream;

END APPLICATION SalesForceReaderTest;
DEPLOY APPLICATION SalesForceReaderTest on any in default;
START SalesForceReaderTest;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

CREATE SOURCE @SourceName@ USING MSSqlReader  ( 
TransactionSupport: false, 
  FetchTransactionMetadata: false, 
  autodisabletablecdc: true,
  Compression: false, 
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', 
  Password_encrypted: 'false', 
  ConnectionURL: '@ConnectionURL@', 
  Fetchsize: 0, 
  ConnectionPoolSize: 10, 
  StartPosition: 'EOF', 
  DatabaseName: 'qatest', 
  Username: '@UN@', 
  cdcRoleName: 'STRIIM_READER', 
  Password: '@PWD@', 
  Tables: '@sourcetable@', 
  FilterTransactionBoundaries: true, 
  SendBeforeImage: true, 
  ExcludedTables: 'qatest.CHKPOINT',
  AutoDisableTableCDC: false ) 
OUTPUT TO @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;


CREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'))) FROM @SRCINPUTSTREAM@ x; ;

CREATE TARGET @targetName@ USING DatabaseWriter  ( 
ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  ParallelThreads: '', 
  CheckPointTable: 'CHKPOINT', 
  Password_encrypted: 'false', 
  ConnectionURL: '@ConnectionURL@', 
  CommitPolicy: 'EventCount:1,Interval:60', 
  StatementCacheSize: '50', 
  DatabaseProviderType: 'Default', 
  Username: '@UN@', 
  Password: '@PWD@', 
  PreserveSourceTransactionBoundary: 'false', 
  BatchPolicy: 'EventCount:1,Interval:60', 
  Tables: '@TablemappingwithColmap@' ) 
INPUT FROM admin.sqlreader_cq_out;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE MongoDB_Source USING Global.MongoDBReader(
collections: '@table@',
  ConnectionURL: '@connectionUrl@',
  mode: 'Incremental',
  Username: '@userName@',
  Password: '@Password@' )
Output To mongoDBRaw_Stream;

CREATE CQ JSON2StriimType
INSERT INTO jsonNodeEventStream
SELECT data.get("_id").toString() AS ID,
  data.get("firstname").toString() AS FirstName,
  data.get("lastname").toString() AS LastName,
  data.get("age").toString() AS Age
FROM mongoDBRaw_Stream;

CREATE TARGET MongoJson_FileDSV_Target USING Global.FileWriter ( 
  flushpolicy: 'EventCount:10000,Interval:30s',  
  directory: '@logs@',
  filename: '@BuiltinFunc@_Data', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING Global.DSVFormatter  ( 
   ) 
INPUT FROM jsonNodeEventStream;

End Application @AppName@;
Deploy application @AppName@;
Start Application @AppName@;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;

CREATE FLOW OracleTokudu1;
	Create Source oracSource1
	 Using OracleReader
	(
	 Username:'@LOGMINER-UNAME@',
 	Password:'@LOGMINER-PASSWORD@',
 	ConnectionURL:'@LOGMINER-URL@',
 	Tables:'@SOURCE_TABLES@',
 	OnlineCatalog:true,
 	FetchSize:1
	) Output To DataStream1;

	CREATE TARGET WriteintoKudu1 using KuduWriter (
	kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
	pkupdatehandlingmode:'@MODE@',
	tables: '@TARGET_TABLES@',
	batchpolicy: 'EventCount:1,Interval:0')
	INPUT FROM DataStream1;
END FLOW OracleTokudu1;

CREATE FLOW OracleTokudu2;
	Create Source oracSource2
	 Using OracleReader
	(
	 Username:'@LOGMINER-UNAME@',
 	Password:'@LOGMINER-PASSWORD@',
 	ConnectionURL:'@LOGMINER-URL@',
 	Tables:'@SOURCE_TABLES@',
 	OnlineCatalog:true,
 	FetchSize:1
	) Output To DataStream2;

	CREATE TARGET WriteintoKudu2 using KuduWriter (
	kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
	pkupdatehandlingmode:'@MODE@',
	tables: '@TARGET_TABLES@',
	batchpolicy: 'EventCount:10,Interval:10')
	INPUT FROM DataStream2;
END FLOW OracleTokudu2;

CREATE FLOW OracleTokudu3;
	Create Source oracSource3
	 Using OracleReader
	(
	 Username:'@LOGMINER-UNAME@',
 	Password:'@LOGMINER-PASSWORD@',
 	ConnectionURL:'@LOGMINER-URL@',
 	Tables:'@SOURCE_TABLES@',
 	OnlineCatalog:true,
 	FetchSize:1
	) Output To DataStream3;

	CREATE TARGET WriteintoKudu3 using KuduWriter (
	kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
	pkupdatehandlingmode:'@MODE@',
	tables: '@TARGET_TABLES@',
	batchpolicy: 'EventCount:5,Interval:120')
	INPUT FROM DataStream3;
END FLOW OracleTokudu3;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

stop application reconnect;
undeploy application reconnect;
drop application reconnect cascade;
CREATE APPLICATION reconnect recovery 1 second interval;

CREATE  SOURCE mssqlsource USING MssqlReader  ( 
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  ConnectionURL: '@URL@',
  Tables: '@TABLE@',
  FetchSize: 1
 ) 
OUTPUT TO sqlstream;

CREATE TARGET dbtarget USING CassandraWriter(
  ConnectionURL:'@URL@',
  Username:'@USERNAME@',
  Password:'@PASSWORD@',
  ConnectionRetryPolicy: 'retryInterval=15s,maxRetries=2',
  BatchPolicy:'EventCount:5,Interval:30',
  CommitPolicy:'EventCount:5,Interval:30',
  Tables: '@TABLES@'
 ) INPUT FROM sqlstream;

 create Target tSysOut using Sysout(name:OrgData) input from sqlstream;
 end application reconnect;
 deploy application reconnect;
 start application reconnect;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:'@eof@',
	charset:'@charset@'
)
parse using DSVParser (
	header:'@header@'
)
OUTPUT TO CsvStream;

create Target FileTarget using FileWriter(
    rolloverpolicy:'eventcount:100',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using DSVFormatter (
)
input from CsvStream;

end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

CREATE OR REPLACE PROPERTYVARIABLE Partitionkey1='@metadata(RecordOffset)';
CREATE OR REPLACE PROPERTYVARIABLE Partitionkey2='Col1';
CREATE OR REPLACE PROPERTYVARIABLE OperationTimeout='500000';
CREATE OR REPLACE PROPERTYVARIABLE ConnectionRetryPolicy='Retries:5,RetryBackOff:1m';

STOP APPLICATION EH;
UNDEPLOY APPLICATION EH;
DROP APPLICATION EH CASCADE;
CREATE APPLICATION EH @Recovery@;
CREATE SOURCE s USING FileReader (
	directory:'Product/IntegrationTests/TestData/',
    WildCard:'posdata.csv',
	positionByEOF:false
	)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO ss;

CREATE TYPE userType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String
);

CREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;

CREATE CQ cq1
INSERT INTO userDefinedTypedStream
SELECT data[0],data[1],data[2], data[3],data[4]
FROM ss;

create Target t1 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	Partitionkey:'$Partitionkey1',
	--ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'$OperationTimeout',
	ConnectionRetryPolicy:'$ConnectionRetryPolicy'
)
format using AvroFormatter (
	schemaFileName:'kafkaAvroTest1.avsc')
input from ss;

create Target t2 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	--ParallelThreads:'2',
	Partitionkey:'$Partitionkey2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'$ConnectionRetryPolicy'
)
format using AvroFormatter (
	schemaFileName:'kafkaAvroTest2.avsc')
input from userDefinedTypedStream;

END APPLICATION EH;
DEPLOY APPLICATION EH;
start application EH;


STOP APPLICATION ER;
UNDEPLOY APPLICATION ER;
DROP APPLICATION ER CASCADE;
CREATE APPLICATION ER;
CREATE SOURCE ER_S1 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING avroParser (
schemaFileName:'kafkaAvroTest1.avsc'
)OUTPUT TO ER_SS1;
CREATE SOURCE ER_S2 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING avroParser (
schemaFileName:'kafkaAvroTest2.avsc'
)OUTPUT TO ER_SS2;

create Type CustType 
(writerdata com.fasterxml.jackson.databind.JsonNode
--TopicName java.lang.String,
--PartitionID java.lang.String
);

Create Stream datastream1 of CustType;
Create Stream datastream2 of CustType;

CREATE CQ CustCQ1
INSERT INTO datastream1
SELECT AvroToJson(s1.data)
--metadata.get("TopicName").toString() AS TopicName,
--metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS1 s1;

CREATE CQ CustCQ2
INSERT INTO datastream2
SELECT AvroToJson(s2.data)
--metadata.get("TopicName").toString() AS TopicName,
--metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS2 s2;

create Target ER_t1 using FileWriter (
filename:'FT1_5L_AVRO_RT',
directory:'FEATURE-DIR/logs/',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from datastream1;

create Target ER_t2 using FileWriter (
filename:'FT2_5L_AVRO_RT',
directory:'FEATURE-DIR/logs/',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from datastream2;
end application ER;
deploy application ER;

--
-- Crash Recovery Test 2 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS
--

STOP APPLICATION KStreamN2S2CR2Tester.KStreamN2S2CRTest2;
UNDEPLOY APPLICATION KStreamN2S2CR2Tester.KStreamN2S2CRTest2;
DROP APPLICATION KStreamN2S2CR2Tester.KStreamN2S2CRTest2 CASCADE;

DROP USER KStreamN2S2CR2Tester;
DROP NAMESPACE KStreamN2S2CR2Tester CASCADE;
CREATE USER KStreamN2S2CR2Tester IDENTIFIED BY KStreamN2S2CR2Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR2Tester;
CONNECT KStreamN2S2CR6Tester KStreamN2S2CR2Tester;

CREATE APPLICATION KStreamN2S2CRTest2 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest2;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest2;

CREATE FLOW DataProcessingKStreamN2S2CRTest2;

CREATE TYPE WactionTypeKStreamN2S2CRTest2 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionTypeKStreamN2S2CRTest2;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest2 CONTEXT OF WactionTypeKStreamN2S2CRTest2
EVENT TYPES ( WactionTypeKStreamN2S2CRTest2 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsKStreamN2S2CRTest2
INSERT INTO WactionsKStreamN2S2CRTest2
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingKStreamN2S2CRTest2;

END APPLICATION KStreamN2S2CRTest2;

STOP OuterJoinTester.OJApp;
UNDEPLOY APPLICATION OuterJoinTester.OJApp;
DROP APPLICATION OuterJoinTester.OJApp cascade;

CREATE APPLICATION OJApp;



CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;



CREATE source wsSource2 USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'bankCards.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO stream2;



CREATE TYPE bankData
(
bankID Integer KEY,
bankName String
);

CREATE TYPE cardData
(
cardID Integer KEY,
cardName String
);


CREATE STREAM wsStream OF bankData;
CREATE STREAM wsStream2 OF cardData;

CREATE JUMPING WINDOW win1 OVER wsStream KEEP 20 rows;


CREATE JUMPING WINDOW win2 OVER stream2 KEEP 4 rows;

CREATE WACTIONSTORE oneWS CONTEXT OF bankData
EVENT TYPES(bankData )
@PERSIST-TYPE@

CREATE WACTIONSTORE twoWS CONTEXT OF cardData
EVENT TYPES(cardData )
@PERSIST-TYPE@


--Select data from QaStream and insert into wsStream

CREATE CQ csvTobankData
INSERT INTO oneWS
SELECT TO_INT(data[0]), data[1] FROM QaStream;

CREATE CQ csvTobankData2
INSERT INTO twoWS
SELECT TO_INT(data[0]), data[1] FROM stream2;


END APPLICATION OJApp;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@_stream;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'SingleEvent',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
  filename: '%@metadata(FileName)%',
  directory: '@DIR@',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter  ()
INPUT FROM CCBStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;
CREATE APPLICATION DSV;

Create type ScanResultType (
  timestamp1 String,
  rssi String
);

Create Stream ScanResultStream of ScanResultType;

CREATE  SOURCE CSVSource USING FileReader (
  directory: '@TEST-DATA-PATH@',
  WildCard: 'sample.json',
  positionByEOF: false
 )
 PARSE USING JSONParser (
  eventType: 'RollOverTester.ScanResultType',
  fieldName: 'scanresult'
 )
OUTPUT TO ScanResultStream;

CREATE TARGET KafkaSYSOUT USING FileWriter (
  filename: 'EventType',
  flushinterval: '0',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy: 'eventcount:1,sequence:00'
 )
format using JSONFormatter (
  members:'timestamp1,rssi'
)
INPUT FROM ScanResultStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/EventType_actual.log') input from ScanResultStream;

END APPLICATION DSV;

create target @TARGET_NAME@ using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
) INPUT FROM @STREAM@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE OR REPLACE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;

CREATE OR REPLACE SOURCE @SOURCE@ USING SalesForceReader (
  autoAuthTokenRenewal: 'true',
  Username: '@userName@',
  securityToken: '@securityToken@',
  sObjects: '@SourceObj@',
  pollingInterval: '1 min',
  Password_encrypted: 'false',
  securityToken_encrypted: 'false',
  customObjects: 'False',
  consumerKey: '@consumerKey@',
  startTimestamp: '',
  apiEndPoint: 'https://ap2.salesforce.com',
  mode: 'Incremental',
  consumerSecret: '@consumerSecert@',
  consumerSecret_encrypted: 'false',
  Password: '@Password@' )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@ USING SnowflakeWriter (
  connectionUrl: '@tgtConnectionUrl@',
  optimizedMerge: 'true',
  password: '@TgtPassword@',
  username: '@TgtUserName@',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10,interval:1m',
  tables: '@SourceObj@,@TargetTableName@ columnmap(ID=ID,checkbool__c=checkbool__c,dt__c=dt__c,percnt__c=percnt__c,phn__c=phn__c,txtlong__c=txtlong__c,url1__c=url1__c)'
 )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

STOP application JSONFormatterTester.DSV;
undeploy application JSONFormatterTester.DSV;
drop application JSONFormatterTester.DSV cascade;

create application DSV;

create source CSVSmallPosDataSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvSmallPosDataStream;

create source CSVPosDataSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'posdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvPosDataStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVPosDataStream of CSVType;
Create Stream TypedCSVSmallPosDataStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVPosDataStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvPosDataStream;

CREATE CQ CsvToSmallPosData
INSERT INTO TypedCSVSmallPosDataStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvSmallPosDataStream;



/**
* 3.4.5.b FileWriter JsonFileSize 333MB
**/
create Target JSONFormatterFileSize using FileWriter(
  filename:'JsonTargetFS',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'FileSizeRollingPolicy,filesize:333M,sequence:00'
)
format using JSONFormatter (
  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)
input from TypedCSVPosDataStream;

/**
* 3.5.1.b FileWriter Json EventCount 2000
**/
create Target JSONFormatterEventCount using FileWriter(
  filename:'JsonTargetEC',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'eventcount:2000,sequence:00'
)
format using JSONFormatter (
  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)
input from TypedCSVSmallPosDataStream;


/**
*  FileWriter Json events as array of json objects :true.
**/

create Target JSONFormatterEventCountT using FileWriter(
  filename:'JsonFormatterPropT',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'eventcount:500000,sequence:00'
)
format using JSONFormatter (
  members:'merchantname,merchantid,zip',
  jsonObjectDelimiter:'|',
  jsonMemberDelimiter:'~',
  EventsAsArrayOfJsonObjects : true
)
input from TypedCSVSmallPosDataStream;

/**
*  FileWriter Json events as array of json objects :false.
**/

create Target JSONFormatterEventCountF using FileWriter(
  filename:'JsonFormatterPropF',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'eventcount:500000,sequence:00'
)
format using JSONFormatter (
  members:'merchantname,merchantid,zip',
  jsonObjectDelimiter:'|',
  jsonMemberDelimiter:'~',
  EventsAsArrayOfJsonObjects : false
)
input from TypedCSVSmallPosDataStream;


create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizeBig_actual.log') input from TypedCSVPosDataStream;

end application DSV;

Stop application PosAppToFW;
UNDEPLOY APPLICATION PosAppToFW;
DROP APPLICATION PosAppToFW CASCADE;

CREATE APPLICATION PosAppToFW RECOVERY 1 MINUTE INTERVAL AUTORESUME MAXRETRIES 2 RETRYINTERVAL 60;

CREATE source CsvPosDataSource USING FileReader (
  WildCard: '',
  directory: '',
positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE OR REPLACE CQ CsvPosAppCq 
INSERT INTO FromCsvPosAppCq 
SELECT TO_STRING(data[1]) as merchantId,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;


CREATE TARGET FileWriterTarget USING Global.FileWriter ( 
 
  flushpolicy: 'EventCount:1000,Interval:30s', 
  directory: '',
  filename: '' ) 
FORMAT USING Global.DSVFormatter  ( 
  quotecharacter: '\"', 
  columndelimiter: ',', 
  usequotes: 'false', 
  rowdelimiter: '\n', 
  standard: 'none', 
  header: 'false' ) 
INPUT FROM FromCsvPosAppCq;

end application PosAppToFW;
deploy application PosAppToFW;
start application PosAppToFW;

--
-- Recovery Test 1
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov1Tester.RecovTest1;
UNDEPLOY APPLICATION Recov1Tester.RecovTest1;
DROP APPLICATION Recov1Tester.RecovTest1 CASCADE;
CREATE APPLICATION RecovTest1 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END APPLICATION RecovTest1;

CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'data.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO @AppName@_rawstream;

CREATE OR REPLACE STREAM @BuiltinFunc@_Stream OF Global.WAEVent;
CREATE OR REPLACE STREAM CombineStream OF Global.WAEVent;

CREATE OR REPLACE CQ cq1
INSERT INTO @BuiltinFunc@_Stream
SELECT
@BuiltinFunc@(s1, 'city',data[5])
FROM @AppName@_rawstream s1;

CREATE OR REPLACE CQ cq2
INSERT INTO CombineStream
Select *
FROM @BuiltinFunc@_Stream s4;

CREATE OR REPLACE CQ cq3
INSERT INTO CombineStream
select *
FROM @AppName@_rawstream s5;

CREATE OR REPLACE TARGET @AppName@_FileTarget USING Global.FileWriter ( 
  flushpolicy: 'EventCount:10000,Interval:30s', 
  adapterName: 'FileWriter', 
  directory: '@logs@',
  filename: '@BuiltinFunc@_Data', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING Global.JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter', 
  jsonMemberDelimiter: '\n', 
  EventsAsArrayOfJsonObjects: 'true', 
  formatterName: 'JSONFormatter', 
  jsonobjectdelimiter: '\n' ) 
INPUT FROM CombineStream;

End application @AppName@;
Deploy application @AppName@; 
Start application @AppName@;

--
-- Recovery Test 5 with Jumping window and partitioned
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP KStreamRecov5Tester.KStreamRecovTest5;
UNDEPLOY APPLICATION KStreamRecov5Tester.KStreamRecovTest5;
DROP APPLICATION KStreamRecov5Tester.KStreamRecovTest5 CASCADE;
DROP USER KStreamRecov5Tester;
DROP NAMESPACE KStreamRecov5Tester CASCADE;
CREATE USER KStreamRecov5Tester IDENTIFIED BY KStreamRecov5Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov5Tester;
CONNECT KStreamRecov5Tester KStreamRecov5Tester;

CREATE APPLICATION KStreamRecovTest5 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvData PARTITION BY merchantId;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION KStreamRecovTest5;

stop IR;
undeploy application IR;
drop application IR cascade;

CREATE APPLICATION IR;

CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.autotest01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.autotest01=id',
 startPosition: 'striim.autotest01=2',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream;

  create type AutoType(
  ID int,
  col1 int,
  col2 string,
  t1 DateTime,
  t2 DateTime
);

CREATE STREAM CDCdata_stream OF AutoType;

CREATE CQ Lookup
INSERT INTO CDCdata_stream
select data[0],data[1],data[2],data[3],data[4] from data_stream;

CREATE  TARGET AzureSQLDWHTarget1 USING AzureSQLDWHWriter  ( 
  ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
  username: 'striim',
  password: 'W3b@ct10n',
  storageaccount: 'striimqatestdonotdelete',
  accesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
  Tables: 'STRIIM.AUTOTEST01',
  uploadpolicy: 'eventcount:1,interval:10s'
 ) 
INPUT FROM CDCdata_stream;

CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM CDCdata_stream;


END APPLICATION IR;
deploy application IR;
start IR;

STOP UpdatableCacher.UpdatableCache;
UNDEPLOY APPLICATION UpdatableCacher.UpdatableCache;
DROP APPLICATION UpdatableCacher.UpdatableCache CASCADE;
CREATE APPLICATION UpdatableCacher.UpdatableCache;

CREATE TYPE MerchantHourlyAve(
  merchantId String KEY,
  hourlyAve Integer,
  theDate DateTime,
  price Double
);


CREATE source CsvDataSource USING FileReader (
      directory:'@TEST-DATA-PATH@',
      columndelimiter: ',',
      wildcard:'ucData.csv',
      blocksize: 10240,
      positionByEOF:false
)
PARSE USING DSVParser (
      header:No,
      trimquote:false
) OUTPUT TO CsvStream;


CREATE STREAM S1 OF MerchantHourlyAve;

CREATE CQ cq1
	insert into S1
		SELECT data[0],
				TO_INT(data[1]),
				TO_DATE(data[2]),
				TO_DOUBLE(data[3])
		FROM CsvStream;


CREATE EVENTTABLE ET1 using STREAM (
  NAME: 'S1'
) QUERY (keytomap:'price', persistPolicy: 'true' ) OF MerchantHourlyAve;


CREATE EVENTTABLE ET2 using STREAM (
  NAME: 'S1'
) QUERY (keytomap:'merchantId' ) OF MerchantHourlyAve;
--) QUERY (keytomap:'merchantId', uniquekey:'price' ) OF MerchantHourlyAve;



END APPLICATION UpdatableCache;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

--CREATE APPLICATION @APPNAME@;
create application @APPNAME@ Recovery 5 second Interval;

--create or replace flow @APPNAME@_agentflow;

CREATE OR REPLACE SOURCE @APPNAME@_source USING MariaDBReader 
(
Username: '@READER-UNAME@',
Password: '@READER-PASSWORD@',
ConnectionURL: '@CDC-READER-URL@',
Tables: @WATABLES@,
sendBeforeImage:'true',
FilterTransactionBoundaries: 'true'
) 
OUTPUT TO @APPNAME@_stream ;

--end flow @APPNAME@_@APPNAME@_agentflow;

CREATE OR REPLACE TARGET @APPNAME@_target USING CassandraCosmosDBWriter  (
  AccountEndpoint: 'cassandracosmostest.cassandra.cosmos.azure.com',
  AccountKey: 'pqDZvVgbdSCg7VzIzD77dAhPG2odGRZPLhAQA1qnZbAKoIDk6RuQX5r2phbRQFnR1l54qxOcvBXNdz8DeijYIg==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  Tables: 'QATEST.Source1,test.target1',
  adapterName: 'CassandraCosmosDBWriter'
 )
 INPUT FROM @APPNAME@_stream;

 END APPLICATION @APPNAME@;

deploy application @APPNAME@;
 --deploy application @APPNAME@ with agentflow in agents;
 start application @APPNAME@;

STOP BuiltInTester.AnomalyBoundTest;
UNDEPLOY APPLICATION BuiltInTester.AnomalyBoundTest;
DROP APPLICATION BuiltInTester.AnomalyBoundTest CASCADE;

CREATE APPLICATION AnomalyBoundTest;

CREATE OR REPLACE TYPE s1_Type  ( mString java.lang.String , 
mDouble java.lang.Double , 
mInt java.lang.Integer , 
mShort java.lang.Short , 
mLong java.lang.Long , 
mFloat java.lang.Float  
 );

CREATE OR REPLACE STREAM s1 OF s1_Type;

CREATE  WINDOW anomalyWin OVER s1 KEEP 4 ROWS;

CREATE OR REPLACE TYPE S3_Type  ( val java.lang.Integer , 
upperInt java.lang.Double , 
lowerInt java.lang.Double , 
upperDouble java.lang.Double , 
lowerDouble java.lang.Double , 
upperFloat java.lang.Double , 
lowerFloat java.lang.Double , 
upperLong java.lang.Double , 
lowerLong java.lang.Double , 
upperShort java.lang.Double , 
lowerShort java.lang.Double  
 ) ;

CREATE WACTIONSTORE anomalyWS  CONTEXT OF S3_Type
EVENT TYPES(S3_Type )
@PERSIST-TYPE@


CREATE OR REPLACE STREAM S3 OF S3_Type;

CREATE OR REPLACE CQ wsCQ 
INSERT INTO anomalyWS
SELECT * FROM S3 s;
;

CREATE OR REPLACE CQ anomalyCQ 
INSERT INTO S3
SELECT last(mInt) as val, 
round_double(distributionUpperBound(mInt), 2) as upperInt,  round_double(distributionLowerBound(mInt), 2) as lowerInt, round_double(distributionUpperBound(mDouble), 2) as upperDouble,  round_double(distributionLowerBound(mDouble), 2) as lowerDouble, round_double(distributionUpperBound(mFloat), 2) as upperFloat, round_double(distributionLowerBound(mFloat), 2) as lowerFloat, round_double(distributionUpperBound(mLong), 2) as upperLong, round_double(distributionLowerBound(mLong), 2) as lowerLong, round_double(distributionUpperBound(mShort), 2) as upperShort, round_double(distributionLowerBound(mShort), 2) as lowerShort 
FROM anomalyWin a
;

CREATE OR REPLACE TYPE hpstream3_Type  ( hpTime org.joda.time.DateTime , 
mDouble java.lang.Double , 
mInt java.lang.Integer , 
mShort java.lang.Short , 
mLong java.lang.Long  
 );

CREATE OR REPLACE SOURCE housePowerCSV USING FileReader  ( 
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '@TEST-DATA-PATH@',
  skipbom: true,
  wildcard: 'anomalyBound.csv'
 ) 
 PARSE USING DSVParser  ( 
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: '-1',
  nocolumndelimiter: false,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: true,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 ) 
OUTPUT TO housePowerStream ;

CREATE OR REPLACE CQ HPparseCQ 
INSERT INTO s1
SELECT to_string(data[0]) as mString, to_double(data[0]) as mDouble,  to_int(data[0]) as mInt, to_short(data[0]) as mShort, to_long(data[0]) as mLong, to_float(data[0]) as mFloat 
FROM housePowerStream s;

CREATE OR REPLACE TYPE hpstream_Type  ( hpTime org.joda.time.DateTime , 
mDouble java.lang.Double , 
mInt java.lang.Integer , 
mShort java.lang.Short , 
mString java.lang.String  
 );

CREATE OR REPLACE TYPE hqType  ( hpTime org.joda.time.DateTime , 
m1 java.lang.Double , 
m2 java.lang.Double , 
m3 java.lang.Double , 
m4 java.lang.Double  
 );

CREATE OR REPLACE TYPE s2_Type  ( val java.lang.Integer , 
upper java.lang.Double , 
lower java.lang.Double , 
isAnomaly java.lang.Boolean  
 );

CREATE OR REPLACE TYPE hpStream2_Type  ( hpTime org.joda.time.DateTime , 
m1 java.lang.Double , 
m2 java.lang.Double , 
m3 java.lang.Double , 
m4 java.lang.Double  
 );

END APPLICATION AnomalyBoundTest;

CREATE FLOW @STREAM@_SourceFlow;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING PostgreSQLReader  ( 
 ReplicationSlotName: 'slotname',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 ) 
OUTPUT TO @STREAM@ ;

END FLOW @STREAM@_SourceFlow;

-- The PosApp sample application demonstrates how a credit card
-- payment processor might use Striim to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.

stop admin.PosApp;
undeploy application admin.PosApp;
drop application admin.PosApp cascade;

CREATE APPLICATION PosApp;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING FileReader (
  directory:'Samples/Customer/PosApp/appData',
  wildcard:'$wildcard',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells Striim that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells Striim to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvToPosData

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP.
--
-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as
-- the types to be used in PosDataStream, which is created automatically.

-- The DATETIME field from the source is converted to both a dateTime
-- value, used as the event timestamp by the application, and (via the
-- function) an integer hourValue, which is used to look up
-- historical hourly averages from the HourlyAveLookup cache,
-- discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)
-- 17 (hourValue)
-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateOnly
--
-- The PosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: 'Samples/Customer/PosApp/appData',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startingTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;


-- WAction store MerchantActivity
--
-- The following group of statements create and populate the MerchantActivity
-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced
-- with merchant details from NameLookup cache and with latitude and longitude
-- values from the USAddressData cache.

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );


CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'Samples/Customer/PosApp/appData',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false
)
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: 'Samples/Customer/PosApp/appData',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;


-- CQ GenerateAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;



--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM AlertStream;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;


END APPLICATION PosApp;


CREATE DASHBOARD USING "Samples/Customer/PosApp/PosAppDashboard.json";

STOP APPLICATION OneAgentTester.CSV;
UNDEPLOY APPLICATION OneAgentTester.CSV;
DROP APPLICATION OneAgentTester.CSV cascade;

DROP USER OneAgentTester;
DROP NAMESPACE OneAgentTester CASCADE;
CREATE USER OneAgentTester IDENTIFIED BY OneAgentTester;
GRANT create,drop ON deploymentgroup Global.* To user OneAgentTester;
CONNECT OneAgentTester OneAgentTester;

CREATE APPLICATION CSV;

CREATE FLOW AgentFlow;
create source CSVSource using FileReader
(
directory:'@TEST-DATA-PATH@',
wildcard:'StoreNames.csv',
positionByEOF:false
)
parse using DSVParser
(
header:'yes',
columndelimiter:','
)
OUTPUT TO CsvStream;
END FLOW AgentFlow;

CREATE FLOW ServerFlow;
CREATE TARGET myout using LogWriter(name: OneAgentSource, filename:'@FEATURE-DIR@/logs/log.txt') input from CsvStream;
END FLOW ServerFlow;

END APPLICATION CSV;
DEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;
START CSV;

CREATE APPLICATION @APPNAME@;

CREATE TYPE @APPNAME@type1 (
 companyName java.lang.String,
 merchantId java.lang.String,
 city java.lang.String);

CREATE STREAM @APPNAME@TypedStream OF @APPNAME@type1 PARTITION BY city;

CREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (
  wildcard: '',
  positionByEOF: false,
  directory: ''
  )
PARSE USING DSVParser (
header:'true'
)
OUTPUT TO @APPNAME@Stream;

CREATE OR REPLACE CQ @APPNAME@CQ
INSERT INTO @APPNAME@TypedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantID,
TO_STRING(data[10]) as city
FROM @APPNAME@Stream;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@TypedStream;

END APPLICATION @APPNAME@;

--
-- Recovery Test 3
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> JW -> CQ(aggregate) -> WS
--

STOP KStreamRecov3Tester.KStreamRecovTest3;
UNDEPLOY APPLICATION KStreamRecov3Tester.KStreamRecovTest3;
DROP APPLICATION KStreamRecov3Tester.KStreamRecovTest3 CASCADE;

DROP USER KStreamRecov3Tester;
DROP NAMESPACE KStreamRecov3Tester CASCADE;
CREATE USER KStreamRecov3Tester IDENTIFIED BY KStreamRecov3Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov3Tester;
CONNECT KStreamRecov3Tester KStreamRecov3Tester;

CREATE APPLICATION KStreamRecovTest3 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount int,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_INT(TO_DOUBLE(data[7])),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

END APPLICATION KStreamRecovTest3;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING MySqlReader
(
  Compression: false,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'mysql://localhost:3306',
  DatabaseName: 'waction',
  Tables: 'waction.test01',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root',
  connectionRetryPolicy:'retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'waction.test01,@DATASET@.% KEYCOLUMNS(id,col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;

--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;

 CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01;striim.test02;striim.test03',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream ;

  CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

create target AzureSQLDWHTarget using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream;


CREATE TARGET log USING LOGWRITER
(
 name:log,
 filename:'LOGFILENAME'
)
INPUT FROM data_stream;


END APPLICATION IR;

deploy application IR;
start IR;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@ recovery 1 second interval;

CREATE SOURCE @parquetsrc@ USING FileReader (
  directory: '',
  positionByEOF: false,
  WildCard: '' )
PARSE USING ParquetParser ()
OUTPUT TO @appname@Streams;

CREATE OR REPLACE CQ @appname@CQOrder3
INSERT INTO @appname@Stream3
SELECT
PUTUSERDATA(s,'schemaName',s.data.getSchema().getName())
FROM @appname@Streams s;

CREATE TARGET @adlstarget@ USING Global.ADLSGen2Writer (
    accountname:'',
  	sastoken:'',
  	filesystemname:'',
  	filename:'',
  	directory:'',
  	uploadpolicy:'eventcount:10' )
format using ParquetFormatter (
schemaFileName: 'ParquetSchema'
)
INPUT FROM @appname@Stream3;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;
Create Source Ojetsrc Using Ojet
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;


create target AzureTarget using AzureSQLDWHWriter (
ConnectionURL: '@SQLDW-URL@',
username: '@SQLDW-USERNAME@',
password: '@SQLDW-PASSWORD@',
AccountName: '@STORAGEACCOUNT@',
AccountAccessKey: '@ACCESSKEY@',
Tables: '@TARGET-TABLES@',
uploadpolicy:'@EVENT-COUNT@',
Mode:'Merge'
) INPUT FROM str;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 10 second interval;
CREATE SOURCE OracleSource USING OracleReader
(
    Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
 Tables:'@TABLES@',
    FetchSize: 1
)
OUTPUT TO CsvStream;

Create Type CSVType (
  tablename String,
  data java.util.HashMap  
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT TO_LOWER(META(s, "TableName").toString()) as tablename,
       DATA(s) as data
FROM CsvStream s;

create Target BlobT using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:5,interval:5s'
)
format using AvroFormatter (
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;

 CREATE OR REPLACE SOURCE Teradata_source_App2 USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01;striim.test02;striim.test03',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',
 startPosition: 'striim.test01=1;striim.test02=-1;%=0',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream1 ;

  CREATE OR REPLACE TARGET sys2 USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream1;

create target AzureSQLDWHTarget_app2 using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream1;


CREATE TARGET log USING LOGWRITER
(
 name:log,
 filename:'LOGFILENAME'
)
INPUT FROM data_stream1;


END APPLICATION IR;

deploy application IR;
start IR;

stop @APPNAME@;
undeploy application @APPNAME@;
--drop exceptionstore admin.MSSQLServer_To_MSSQLServerApp_ExceptionStore;
drop application @APPNAME@ cascade;
create application @APPNAME@ use exceptionstore;


Create Source @SourceName@ Using MSSqlReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'@SourceTable@',
 ConnectionPoolSize:1,
 StartPosition:'EOF'
 ) Output To @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;


 CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@SourceTable@,@TargetTable@',
    CommitPolicy: 'Interval:5'

) INPUT FROM @SRCINPUTSTREAM@;

create or replace cq @cq@
insert into @finalstream@
select exceptionType,action,appName,entityType,entityName,className,message,relatedActivity from @APPNAME@_ExceptionStore;

Create target @targetfile@ using filewriter (
filename:'@APPNAME@_file.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using jsonFormatter()
input from @finalstream@;


end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop @APPNAME@_ExpStore;
undeploy application @APPNAME@_ExpStore;
drop application @APPNAME@_ExpStore cascade;
CREATE APPLICATION @APPNAME@_ExpStore;

CREATE TYPE @APPNAME@_ExpStore_CDCStreams_Type  (
  evtlist java.util.List
 );

CREATE STREAM @APPNAME@_ExpStore_CDCStreams OF @APPNAME@_ExpStore_CDCStreams_Type;

CREATE CQ @APPNAME@_ReadFromExpStore
INSERT INTO @APPNAME@_ExpStore_CDCStreams
select to_waevent(s.relatedObjects) as evtlist from admin.@APPNAME@_ExceptionStore [jumping 5 second] s;

CREATE STREAM @APPNAME@_ExpStore_CDCEventStream OF Global.WAEvent;

CREATE CQ @APPNAME@_ExpStore_GetCDCEvent
INSERT INTO @APPNAME@_ExpStore_CDCEventStream
SELECT com.webaction.proc.events.WAEvent.makecopy(cdcevent) FROM @APPNAME@_ExpStore_CDCStreams a, iterator(a.evtlist) cdcevent;

CREATE CQ @APPNAME@_ExpStore_JoinDataCQ
INSERT INTO @APPNAME@_ExpStore_JoinedDataStream
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1])
        from @APPNAME@_ExpStore_CDCEventStream f;

CREATE OR REPLACE TARGET @APPNAME@_ExpStore_WriteToFileAsJSON USING FileWriter  (
  filename: 'expEvent_MSSQL',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:1,interval:30',
  adapterName: 'FileWriter',
  directory: 'ExpStore_logs',
  rolloverpolicy: 'eventcount:6,interval:30s'
 )
FORMAT USING JSONFormatter  (
  handler: 'com.webaction.proc.JSONFormatter',
  formatterName: 'JSONFormatter',
  jsonMemberDelimiter: '\n',
  jsonobjectdelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true'
 )
INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;

CREATE TARGET @APPNAME@_ExpStore_dbtarget USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1,Interval:1',
CommitPolicy:'Interval:1,Eventcount:1',
Tables:'@TargetTable@'
) INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;

END APPLICATION @APPNAME@_ExpStore;

deploy application @APPNAME@_ExpStore;
start @APPNAME@_ExpStore;

stop application app2;
undeploy application app2;
alter application app2;
CREATE or replace TARGET app2_targetNew USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS1_Alter'
) INPUT FROM sourcestream;
alter application app2 recompile;
deploy application app2;

stop application app3;
undeploy application app3;
alter application app3;

CREATE or replace TARGET app3_targetNew USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test02,QATEST.KPS2_Alter'
) INPUT FROM sourcestream;
alter application app3 recompile;
deploy application app3;


stop application app4;
undeploy application app4;
alter application app4;
CREATE TARGET app4_targetNew USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test03,QATEST.KPS3_Alter'
) INPUT FROM sourcestream;
alter application app4 recompile;
deploy application app4;


stop application app5;
undeploy application app5;
alter application app5;
CREATE or replace TARGET app5_target1_New USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'snappy_Alter',
KafkaConfig:'compression.type=snappy'
) 
FORMAT USING DSVFormatter ()
INPUT FROM kps_typedStream;

CREATE or replace TARGET app5_target2_New USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'gzip_Alter',
KafkaConfig:'compression.type=gzip'
) 
FORMAT USING DSVFormatter ()
INPUT FROM sourcestream;

CREATE or replace TARGET app5_target3_New USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'lz4_Alter',
KafkaConfig:'compression.type=lz4'
) 
FORMAT USING DSVFormatter ()
INPUT FROM sourcestream;

alter application app5 recompile;
deploy application app5;

--
-- Recovery Test 22 with two sources, two sliding attribute windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sa5W -> CQ1 -> WS
-- S2 -> Sa6W -> CQ2 -> WS
--

STOP Recov22Tester.RecovTest22;
UNDEPLOY APPLICATION Recov22Tester.RecovTest22;
DROP APPLICATION Recov22Tester.RecovTest22 CASCADE;
CREATE APPLICATION RecovTest22 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION RecovTest22;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;
CREATE OR REPLACE SOURCE @SourceName@ Using OJet
(
  Username: '@Username@',
  Password: '@Password@',
  ConnectionURL: '@ConnectionURL@',
  connectionRetryPolicy: @ConnectionRetryPolicy@,
  Tables: '@SourceTables@',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  Password_encrypted: 'false',
  CommittedTransactions: true,
  adapterName: 'OJet',
)OUTPUT TO @SRCINPUTSTREAM@;
create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

create application CSVToJSON;
create source CSVSource using FileReader (
	directory:'Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
	filename:'posdata_JSON',
	rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'
)
format using JSONFormatter (
	members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)

input from TypedCSVStream;
end application CSVToJSON;

deploy application CSVToJSON;
start application CSVToJSON;

--
-- Recovery Test 7
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov7Tester.RecovTest7;
UNDEPLOY APPLICATION Recov7Tester.RecovTest7;
DROP APPLICATION Recov7Tester.RecovTest7 CASCADE;
CREATE APPLICATION RecovTest7 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM wacStream OF WactionType;

CREATE CQ InsertWacStream
INSERT INTO wacStream
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE WINDOW waWindow
OVER wacStream KEEP WITHIN 1 SECOND ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT *
FROM waWindow
LINK SOURCE EVENT;

END APPLICATION RecovTest7;

STOP AdhocTester.ws_one;
UNDEPLOY APPLICATION AdhocTester.ws_one;
DROP APPLICATION AdhocTester.ws_one cascade;

CREATE APPLICATION ws_one;

CREATE SOURCE wsSource USING CSVReader  ( 
  blocksize: 10240,
  charset: 'UTF-8',
  positionByEOF: false,
  columndelimiter: ',',
  directory: '@TEST-DATA-PATH@',
  eofdelay: 100,
  wildcard: 'sampleByData.csv',
  expected_column_count: 0,
  rowdelimiter: '\n',
  header: true,
  adapterName: 'CSVReader',
  quoteset: '\"',
  trimquote: true,
  skipbom: true
 ) 
OUTPUT TO QaStream ;

CREATE TYPE sampleS_Type  ( vID java.lang.Integer KEY, 
vInt java.lang.Integer , 
vDouble java.lang.Double , 
vLong java.lang.Long , 
vShort java.lang.Short , 
vFloat java.lang.Float  
 ) ;

CREATE STREAM sampleS OF sampleS_Type;

CREATE CQ csvTowsData 
INSERT INTO sampleS
SELECT  to_int(data[0]) as vID,
to_int(data[1]) as vInt,
to_double(data[1]) as vDouble,
to_long(data[1]) as vLong,
to_short(data[1]) as vShort,
to_float(data[1]) as vFloat
FROM QaStream
;

CREATE WACTIONSTORE oneWS  CONTEXT OF sampleS_Type
EVENT TYPES(sampleS_Type )
@PERSIST-TYPE@

CREATE CQ wsToWaction 
INSERT INTO oneWS
SELECT * FROM sampleS s
LINK SOURCE EVENT;

END APPLICATION ws_one;
DEPLOY APPLICATION ws_one on any in default;
START ws_one;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;
CREATE  SOURCE @SourceName@ USING MSSqlReader  ( 
  Username: '@Username@',
  Password: '@Password@',
  DatabaseName: 'qatest',
  connectionRetryPolicy: @ConnectionRetryPolicy@,
  ConnectionURL: '@ConnectionURL@',
  Tables: '@SourceTables@',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @SRCINPUTSTREAM@;
create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

STOP BuiltInTester.test_todate_builtinfunc;
UNDEPLOY APPLICATION BuiltInTester.test_todate_builtinfunc;
DROP APPLICATION BuiltInTester.test_todate_builtinfunc CASCADE;

CREATE APPLICATION test_todate_builtinfunc;

CREATE OR REPLACE TYPE date_cq_Type (
 gapdate org.joda.time.DateTime,
 randomdate org.joda.time.DateTime);

CREATE SOURCE date_file USING FileReader (
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  directory: '@TEST-DATA-PATH@',
  skipbom: true,
  wildcard: 'anomalyBound.csv'
 )
 PARSE USING DSVParser  (
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: '-1',
  nocolumndelimiter: false,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: true,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 )
OUTPUT TO date_file_stream;

CREATE OR REPLACE WACTIONSTORE datef_ws CONTEXT OF date_cq_Type EVENT TYPES ( date_cq_Type ) USING ( storageProvider: 'elasticsearch' );

CREATE OR REPLACE CQ date_cq INSERT INTO datef_ws SELECT TO_DATEF("1983-04-01", 'yyyy-MM-dd') as gapdate, TO_DATEF("2021-02-18", 'yyyy-MM-dd') as randomdate FROM date_file_stream d;

END APPLICATION test_todate_builtinfunc;

STOP APPLICATION OneAgentEncryptionTester.CSV;
UNDEPLOY APPLICATION OneAgentEncryptionTester.CSV;
DROP APPLICATION OneAgentEncryptionTester.CSV cascade;

create application CSV WITH ENCRYPTION;

CREATE FLOW AgentFlow;
create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails-agent.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStream1;


CREATE TYPE MyTypeCsv(
PAN String,
FNAME String KEY,
LNAME String,
ADDRESS String,
CITY String,
STATE String,
ZIP String,
GENDER String
);

CREATE STREAM TypedStreamCsv of MyTypeCsv;

CREATE CQ TypeConversionCQCsv
INSERT INTO TypedStreamCsv
SELECT
data[0],
data[1],
data[2],
data[3],
data[4],
data[5],
data[6],
data[7]
from CsvStream1;

END FLOW AgentFlow;

CREATE FLOW ServerFlow;

CREATE WACTIONSTORE StoreInfoCsv CONTEXT OF MyTypeCsv
EVENT TYPES ( MyTypeCsv )
@PERSIST-TYPE@

CREATE CQ StoreWactionCsv
INSERT INTO StoreInfoCsv
SELECT * FROM TypedStreamCsv
LINK SOURCE EVENT;


END FLOW ServerFlow;

end application CSV;

DEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;
START CSV;

stop application RedshiftColmap;
undeploy application RedshiftColmap;
drop application RedshiftColmap CASCADE;
create application RedshiftColmap recovery 1 second interval;
CREATE OR REPLACE SOURCE OracleSource USING OracleReader  (
  Username: 'miner',
  Password: 'miner',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.redshift_emp',
  FetchSize: 1
 )
OUTPUT TO LogminerStream;
CREATE  TARGET RedshiftTarget USING RedshiftWriter  (
  ConnectionURL: '@URL@',
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  bucketname: 'striim-s3',
  --accesskeyId: 'AKIAJFHRZ7VIH2GHON5A',
  --secretaccesskey: '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',
  S3IAMRole:'@IAMROLE@',
  Tables: 'QATEST.redshift_emp,qatest.EMPLOYEE COLUMNMAP(E_DOJ = DOJ, E_NAME = NAME,E_ID = ID)',
  uploadpolicy: 'eventcount:3,interval:5s',
  QuoteCharacter: '@QUOTECHARACTER@',
  Mode: 'incremental',
  ColumnDelimiter: '|'
 )
INPUT FROM LogminerStream;
END APPLICATION RedshiftColmap;
deploy application RedshiftColmap;
START application RedshiftColmap;

use admin;
drop namespace @namespace@ cascade;
create namespace @namespace@;
use @namespace@;
CREATE APPLICATION @appName@;
create flow @flowName@;
CREATE SOURCE @appName@_s1 USING Global.FileReader (
  wildcard: 'posdata100.csv',
  blocksize: 64,
  directory: '@TestDataDir@',
  positionByEOF:false)
PARSE USING Global.DSVParser ()
OUTPUT TO @appName@_st1;
end flow srcFlow;
create target @appName@_t1 using NullWriter() input from @appName@_st1;
END APPLICATION @appName@;

create source @SOURCE_NAME@
USING MariaDbXpandReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@ConnectionURL@',
 Tables:'@Tables@',
 FetchSize:10000,
 QueueSize:2048
)Output To @STREAM@;

stop PatternMatching1.CSV;
undeploy application PatternMatching1.CSV;
drop application PatternMatching1.CSV cascade;

create application CSV;

create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'posdata.csv',
  columndelimiter:',',
  positionByEOF:false
)
OUTPUT TO CsvStream;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  TO_STRING(data[0]) as temp1,
        TO_STRING(data[1]) as temp2,
        TO_DOUBLE(data[2]) as temp3
FROM CsvStream;

-- scenario 1.1 check pattern using PosApp data with alteration and quantifier(0 or 1)
CREATE CQ TypeConversionPosAppCQ1
INSERT INTO TypedStream1
SELECT A.temp1 as typeduserid,
       B.temp2 as typedtemp1,
       C.temp3 as typedtemp2
from UserDataStream
MATCH_PATTERN A | B ? C
define A = UserDataStream(temp1 = 'COMPANY 4'),
       B = UserDataStream(temp2 = 'OFp6pKTMg26n1iiFY00M9uSqh9ZfMxMBRf1'),
       C = UserDataStream(temp3 = 520236368216865619l)
PARTITION BY temp1;

-- scenario 1.2 check pattern using PosApp data with permutation and quantifier(0 or 1)
CREATE CQ TypeConversionPosAppCQ2
INSERT INTO TypedStream2
SELECT A.temp1 as typeduserid,
       B.temp2 as typedtemp1,
       C.temp3 as typedtemp2
from UserDataStream
MATCH_PATTERN A & B ? C
define A = UserDataStream(temp1 = 'COMPANY 100'),
       B = UserDataStream(temp2 = 'IYuqAbAQ07NS3lZO74VGPldfAUAGKwzR2k3'),
       C = UserDataStream(temp3 = 6388500771470313223l)
PARTITION BY temp3;

CREATE WACTIONSTORE UserActivityInfoPosApp1
CONTEXT OF TypedStream1_Type
EVENT TYPES ( TypedStream1_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoPosApp2
CONTEXT OF TypedStream2_Type
EVENT TYPES ( TypedStream2_Type )
@PERSIST-TYPE@

create Target t2 using SysOut(name:AgentTyped) input from TypedStream1;
create Target t3 using SysOut(name:AgentTyped1) input from TypedStream2;

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction1
INSERT INTO UserActivityInfoPosApp1
SELECT * FROM TypedStream1
LINK SOURCE EVENT;

CREATE CQ UserWaction2
INSERT INTO UserActivityInfoPosApp2
SELECT * FROM TypedStream2
LINK SOURCE EVENT;

end application CSV;
deploy application csv;
start csv;

STOP APPLICATION @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;
CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9099', kafkaversion:'0.11');

CREATE FLOW @APPNAME@_AgentFlow;

CREATE STREAM @APPNAME@_PS_Stream1 OF Global.waevent persist using  @APPNAME@_KafkaPropset;
CREATE STREAM @APPNAME@_PS_Stream2 OF Global.waevent persist using  @APPNAME@_KafkaPropset;
CREATE STREAM @APPNAME@_PS_Stream3 OF Global.waevent persist using  @APPNAME@_KafkaPropset;
CREATE STREAM @APPNAME@_PS_Stream4 OF Global.waevent persist using  @APPNAME@_KafkaPropset;


create source @APPNAME@_source using FileReader (
directory:'/Users/jenniffer/Product2/IntegrationTests/TestData/OGG/Recovery',
WildCard:'lg*.gz',
positionByEOF:false,
compressiontype:'gzip',
recoveryInterval: 5
) parse using GGTrailParser (
metadata:'@META-FILE@'
)
OUTPUT TO @APPNAME@_Stream;

END FLOW @APPNAME@_AgentFlow;

CREATE CQ @APPNAME@_CQ1
INSERT INTO @APPNAME@_PS_Stream1
SELECT *
FROM @APPNAME@_Stream sm
WHERE META(sm, 'TableName').toString() = 'MINER.CUSTOMER';

CREATE CQ @APPNAME@_CQ2
INSERT INTO @APPNAME@_PS_Stream2
SELECT *
FROM @APPNAME@_Stream sm
WHERE META(sm, 'TableName').toString() = 'MINER.CUSTOMER';

CREATE CQ @APPNAME@_CQ3
INSERT INTO @APPNAME@_PS_Stream3
SELECT *
FROM @APPNAME@_Stream sm
WHERE META(sm, 'TableName').toString() = 'MINER.CUSTOMER';

CREATE CQ @APPNAME@_CQ4
INSERT INTO @APPNAME@_PS_Stream4
SELECT *
FROM @APPNAME@_Stream sm
WHERE META(sm, 'TableName').toString() = 'MINER.CUSTOMER';


CREATE FLOW @APPNAME@_ServerFlow;

CREATE TARGET @APPNAME@_target1 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'MINER.CUSTOMER,QATEST.CUSTOMER_TARGET_AG4'
) INPUT FROM @APPNAME@_PS_Stream1;

CREATE TARGET @APPNAME@_target2 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'MINER.CUSTOMER,QATEST.CUSTOMER_TARGET_AG4'
) INPUT FROM @APPNAME@_PS_Stream2;

CREATE TARGET @APPNAME@_target3 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'MINER.CUSTOMER,QATEST.CUSTOMER_TARGET_AG4'
) INPUT FROM @APPNAME@_PS_Stream3;

CREATE TARGET @APPNAME@_target4 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'MINER.CUSTOMER,QATEST.CUSTOMER_TARGET_AG4'
) INPUT FROM @APPNAME@_PS_Stream4;

END FLOW @APPNAME@_ServerFlow;

end application @APPNAME@;

deploy application @APPNAME@ with @APPNAME@_AgentFlow in AGENTS, @APPNAME@_ServerFlow on any in default;

start application @APPNAME@;

undeploy application dev15823;
alter application dev15823;

CREATE TYPE ModifyNotNull (
  x int,
  y string
);

CREATE STREAM FormattedStream OF ModifyNotNull;

CREATE  CQ InsertWactions
INSERT INTO FormattedStream
SELECT
    TO_INT(data[0]),
   	"notnull"
FROM LogminerStream;

Create or replace Target test using SysOut (name:test) input from FormattedStream;

CREATE OR REPLACE TARGET WriteCDCMySQL USING DatabaseWriter  ( 
  Username: '@USERNAME@',
  BatchPolicy: 'Eventcount:5,Interval:5',
  CommitPolicy: 'Eventcount:5,Interval:5',
  ConnectionURL: '@URL@',
  Tables: '@TABLES@',
  Checkpointtable: 'CHKPOINT',
  --IgnorableExceptionCode:'1062',
  Password: '@PASSWORD@'
 ) 
INPUT FROM FormattedStream;

END APPLICATION dev15823;
alter application dev15823 recompile;
deploy application dev15823;
start dev15823;

Stop Oracle_IRLogWriter;
Undeploy application Oracle_IRLogWriter;
drop application Oracle_IRLogWriter cascade;

CREATE APPLICATION Oracle_IRLogWriter WITH ENCRYPTION recovery 5 second interval;

CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( 
  FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.TDSOURCE',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.TEST01=ID;',
  PollingInterval: '5sec',
  ReturnDateTimeAs: 'String',
  startPosition:'striim.test01=0'
  )
  OUTPUT TO data_stream;

  CREATE OR REPLACE TARGET Oracle_IRSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

CREATE TARGET BinaryDump USING LogWriter(
  name: 'TeraData',
  filename:'TeraData.log'
)INPUT FROM data_stream;

END APPLICATION Oracle_IRLogWriter;

deploy application Oracle_IRLogWriter in default;

start application Oracle_IRLogWriter;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ Recovery 5 second interval;

create stream @APPNAME@_UserdataStream of Global.WAEvent;

create type @APPNAME@_Order_type(
id int,
order_id int,
zipcode int,
category String,
tablename string
);

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src1 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.order_%'
)
OUTPUT TO @APPNAME@_OrdersStream;

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src2 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_2',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.second_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream2;

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src3 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_3',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.third_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream3;

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src4 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_4',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.fourth_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream4;


Create CQ @APPNAME@_CQUser
insert into @APPNAME@_UserdataStream
select 
putuserdata (data,'Fileowner','FIRST_ORDER') from @APPNAME@_OrdersStream data;


Create CQ @APPNAME@_CQUser2
insert into @APPNAME@_UserdataStream
select 
putuserdata (data2,'Fileowner','SECOND_ORDER') from @APPNAME@_OrdersStream2 data2;


Create CQ @APPNAME@_CQUser3
insert into @APPNAME@_UserdataStream
select 
putuserdata (data3,'Fileowner','THIRD_ORDER') from @APPNAME@_OrdersStream3 data3;


Create CQ @APPNAME@_CQUser4
insert into @APPNAME@_UserdataStream
select 
putuserdata (data4,'Fileowner','FOURTH_ORDER') from @APPNAME@_OrdersStream4 data4;

create stream @APPNAME@_OrderTypedStream1 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream2 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream3 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream4 of @APPNAME@_Order_type;

CREATE CQ @APPNAME@_fin_cq
INSERT INTO @APPNAME@_OrderTypedStream1
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FIRST_ORDER';

CREATE CQ @APPNAME@_fin_cq2
INSERT INTO @APPNAME@_OrderTypedStream2
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'SECOND_ORDER';

CREATE CQ @APPNAME@_fin_cq3
INSERT INTO @APPNAME@_OrderTypedStream3
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'THIRD_ORDER';

CREATE CQ @APPNAME@_fin_cq4
INSERT INTO @APPNAME@_OrderTypedStream4
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FOURTH_ORDER';


create Target @APPNAME@_ADLSGen2_tgt1 using ADLSGen2Writer(
        filename:'event_data.csv',
        directory:'',
       	accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        uploadpolicy:'eventcount:8,interval:20s'
)
format using DSVFormatter (
    header:'true'
)
input from @APPNAME@_OrderTypedStream1; 

create Target @APPNAME@_ADLSGen2_tgt2 using ADLSGen2Writer(
        filename:'event_data.xml',
        directory:'',
		accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
   		uploadpolicy:'eventcount:8,interval:20s'
)
format using XMLFormatter (
  elementtuple: 'Order_id:id:order_id:zipcode:category:text=tablename',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from @APPNAME@_OrderTypedStream2; 

create Target @APPNAME@_ADLSGen2_tgt3 using ADLSGen2Writer(
        filename:'event_data.avro',
        directory:'',
		accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
  		uploadpolicy:'eventcount:8,interval:20s'
)
format using AvroFormatter (
  formatAs: 'Default',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA-FILE@'
)
input from @APPNAME@_OrderTypedStream3; 


create Target @APPNAME@_ADLSGen2_tgt4 using ADLSGen2Writer(
        filename:'event_data.json',
        directory:'',
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
  		uploadpolicy:'eventcount:8,interval:20s'
)
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@_OrderTypedStream4;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING MySqlReader
(
  Compression: false,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'mysql://localhost:3306',
  DatabaseName: 'waction',
  Tables: 'waction.test01',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root'
)
OUTPUT TO @APPNAME@_SS;


CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'waction.test01,@DATASET@.% KEYCOLUMNS(id,name) COLUMNMAP(id=id,col1=col11,col2=col12)',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

--
-- Recovery Test 21 with two sources, two sliding count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5W -> CQ1 -> WS
-- S2 -> Sc6W -> CQ2 -> WS
--

STOP Recov21Tester.RecovTest21;
UNDEPLOY APPLICATION Recov21Tester.RecovTest21;
DROP APPLICATION Recov21Tester.RecovTest21 CASCADE;
CREATE APPLICATION RecovTest21 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION RecovTest21;

STOP bq;
UNDEPLOY APPLICATION bq;
DROP APPLICATION bq CASCADE;

CREATE APPLICATION bq RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:orcl',
	Tables: 'QATEST.TABLE_TEST_1000001',
	FetchSize: '1'
)
OUTPUT TO SS;


CREATE or replace TARGET T USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
    Tables:'QATEST.TABLE_TEST_1000001,qatest.% keycolumns(RONUM)',
    mode:'Appendonly',
    datalocation: 'US',
	nullmarker: 'defaultNULL',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:100,Interval:10'	
) INPUT FROM ss;

END APPLICATION bq;
DEPLOY APPLICATION bq;
START APPLICATION bq;

CREATE OR REPLACE SOURCE KafkaReaderSource USING Global.KafkaReader VERSION '2.1.0' 
( 
AutoMapPartition: true, 
brokerAddress: 'localhost:9092',       
KafkaConfigPropertySeparator: ',',
Topic: 'test011', 
startOffset: 0,
KafkaConfigValueSeparator: ':', 
KafkaConfig: 'request.timeout.ms:6001,session.timeout.ms:6000,security.protocol:SASL_SSL,sasl.mechanism:SCRAM-SHA-512,sasl.jaas.config:org.apache.kafka.common.security.scram.ScramLoginModule required username=kafkauser password=\"Oppenheimer\";,value.deserializer:com.striim.avro.deserializer.LengthDelimitedAvroRecordDeserializer' 
) 
PARSE USING Global.DSVParser () 
OUTPUT TO KafkaDevReadSourceStream;

STOP AdhocTester.ws_one;
UNDEPLOY APPLICATION AdhocTester.ws_one;
DROP APPLICATION AdhocTester.ws_one cascade;

CREATE APPLICATION ws_one;

CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO QaStream;

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE ZipLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'AdhocQueryData.csv',
  header: Yes,
  columndelimiter: '	',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

Create TYPE wsData(
	CompanyNum String,
	CompanyName String KEY,
	CompanyCode int,
	Zip String
);


CREATE STREAM wsStream OF wsData;

--Select data from QaStream and insert into wsStream

CREATE CQ csvTowsData
INSERT INTO wsStream
SELECT  data[0],
    data[1],
    TO_INT(data[3]),
    data[9]
 FROM QaStream;




CREATE WACTIONSTORE oneWS CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@


CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT * FROM wsStream
LINK SOURCE EVENT;

END APPLICATION ws_one;

STOP APPLICATION bq;
UNDEPLOY APPLICATION bq;
DROP APPLICATION bq CASCADE;
CREATE APPLICATION bq;

CREATE SOURCE s USING FileReader
(
  directory:'/Users/sujith_syk/MyTasks/Analyse_BQtableScan_on_partitioned_table_merge_query',
  WildCard:'testdata1.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO FileStream;

CREATE TYPE cdctype(
    STORE_ID  String,
    NAME  String,
    CITY  String,
    STATE  String,
    ZIP  String,
    CUSTOMER_ACCOUNT_NUMBER  String,
    ORDER_ID  String,
    SKU  String,
    ORDER_AMOUNT  String,
    DATETIME  String
);

CREATE STREAM cdctypestream OF cdctype;

CREATE CQ cdcstreamcq
INSERT INTO cdctypestream
SELECT TO_STRING(p.data[0]),
       TO_STRING(p.data[1]),
       TO_STRING(p.data[2]),
       TO_STRING(p.data[3]),
       TO_STRING(p.data[4]),
       TO_STRING(p.data[5]),
       TO_STRING(p.data[6]),
       TO_STRING(p.data[7]),
       TO_STRING(p.data[8]),
       TO_STRING(p.data[9])
FROM FileStream p;


CREATE TARGET t1 USING BigQueryWriter
(
  ServiceAccountKey:'/Users/sujith_syk/Documents/striimdev-creds.json',
  projectId:'striimdev',
  Tables:'qatest.FILETOBQHEAPTEST1',
  datalocation:'US',
  nullmarker:'NOTNULL',
  columnDelimiter:'|',
  BatchPolicy:'eventCount:100, Interval:180',
  Mode: 'MERGE'
) INPUT FROM cdctypestream;

CREATE TARGET t2 USING BigQueryWriter
(
  ServiceAccountKey:'/Users/sujith_syk/Documents/striimdev-creds.json',
  projectId:'striimdev',
  Tables:'qatest.FILETOBQHEAPTEST2',
  datalocation:'US',
  nullmarker:'NOTNULL',
  columnDelimiter:'|',
  BatchPolicy:'eventCount:100, Interval:180',
  Mode: 'MERGE'
) INPUT FROM cdctypestream;

CREATE TARGET t3 USING BigQueryWriter
(
  ServiceAccountKey:'/Users/sujith_syk/Documents/striimdev-creds.json',
  projectId:'striimdev',
  Tables:'qatest.FILETOBQHEAPTEST3',
  datalocation:'US',
  nullmarker:'NOTNULL',
  columnDelimiter:'|',
  BatchPolicy:'eventCount:100, Interval:180',
  Mode: 'MERGE'
) INPUT FROM cdctypestream;

END APPLICATION bq;
DEPLOY APPLICATION bq;
START APPLICATION bq;

STOP APPLICATION LoadMarketsAppTester.LoadMarketsApp;
UNDEPLOY APPLICATION LoadMarketsAppTester.LoadMarketsApp;
DROP APPLICATION LoadMarketsAppTester.LoadMarketsApp CASCADE;

CREATE APPLICATION LoadMarketsApp;

CREATE OR REPLACE SOURCE MarketSource USING FileReader (
 directory:'@TEST-DATA-PATH@',
  wildcard: 'marketlocations.json',
  blocksize: '64',
  charset: 'UTF-8',
  positionbyeof: 'false',
  rolloverpolicy: 'DefaultFileComparator',
  skipbom: 'true'
 )
 PARSE USING JSONParser (
  fieldName: 'locations'
 )
OUTPUT TO RawMarketLocationsStream;


CREATE OR REPLACE CQ ConvertRawShapes
INSERT INTO MarketStream
SELECT e.data.get("marketid").textValue() as id,
       e.data.get("marketname").textValue() as name,
       e.data.withArray("geopoints") as points,
       DNOW() as lastUpdated
FROM RawMarketLocationsStream e;

CREATE OR REPLACE CQ SplitLatLon
INSERT INTO PontStream
SELECT
  s.id as id,
  s.name as name,
  lat as lat ,
  lon as lon,
  s.lastUpdated as lastUpdated
FROM MarketStream s, iterator(s.points, (lat String, lon String)) a;

CREATE OR REPLACE WINDOW MarketWindow OVER MarketStream KEEP WITHIN 1 HOUR ON lastUpdated;


-- added by nambi ,Reason - for filtering out "lastUpdated", which has current time, is to have static expected results

CREATE OR REPLACE CQ SplitLatLon2
INSERT INTO PontStream2
SELECT
  s.id as id,
  s.name as name,
  lat as lat ,
  lon as lon
FROM MarketStream s, iterator(s.points, (lat String, lon String)) a;

CREATE TYPE wsData
(
  id String,
  name String,
  lat String,
  lon String
);


CREATE WACTIONSTORE oneWS
CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@

CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT
  id,
  name,
  lat ,
  lon
FROM PontStream2
LINK SOURCE EVENT;

END APPLICATION LoadMarketsApp;

stop @appname@;
undeploy application @appname@;
DROP APPLICATION @appname@ CASCADE;
CREATE APPLICATION @appname@;

CREATE SOURCE @appname@_src USING databaseReader  (
  Username: '@@',
  Password: '@@',
  ConnectionURL: '@@',
  Tables: '@@',
  FetchSize: '100'
 )
OUTPUT TO @appname@_ss;

CREATE JUMPING WINDOW @appname@_win OVER @appname@_ss KEEP @winsize@ ROWS;

CREATE TYPE @appname@_MapType
    (   
       id INTEGER,
        name STRING,
        city  STRING
    );
    
CREATE EXTERNAL CACHE @appname@_cach (
  AdapterName: 'DatabaseReader',
    ConnectionURL: '@url@',
    UserName: '@uname@',
    Password: '@pwd@',
   Table: '@tablename@',
  FetchSize: 100,
  skipinvalid : @valid@,
  Columns: 'id,name,city',
  trimquote: false,
  KeyToMap: '@key@'
 )
 OF @appname@_MapType;
 
CREATE TYPE @appname@_MapTypenew
    (   id_t            INTEGER,
        name_t           STRING,
        city_t            STRING,
        id_c            INTEGER,
        name_c            STRING,
        city_c            STRING
    );
    
CREATE STREAM @appname@_JoinedData OF @appname@_MapTypenew;

CREATE CQ @appname@_JoinDataCQ
INSERT INTO @appname@_JoinedData
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_STRING(f.data[2]),
        z.id,
        z.name,
        z.city
FROM @appname@_win f, @appname@_cach z
where TO_INT(f.data[0]) = z.id
@Ex@;

CREATE TARGET @appname@_tgt USING DatabaseWriter
(
  ConnectionURL:'@@',
  Username:'@@',
  Password:'@@',
  BatchPolicy:'Eventcount:10000,Interval:1',
  CommitPolicy:'Interval:1,Eventcount:10000',
  Tables:'@@'
) 
INPUT FROM @appname@_JoinedData;

END APPLICATION @appname@;
deploy application @appname@;
start @appname@;

stop application GCSWriterTest;
undeploy application GCSWriterTest;
drop application GCSWriterTest cascade;
create application GCSWriterTest recovery 1 second interval;
create source GCS_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'true'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;
create Target GCSTarget using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@',
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
--members:'data'
)
input from TypedCSVStream;

end application GCSWriterTest;
deploy application GCSWriterTest on all in default;
start application GCSWriterTest;

--
-- Recovery Test 32 with two sources, two sliding attribute windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sa5W/p -> CQ1 -> WS
-- S2 -> Sa6W/p -> CQ2 -> WS
--

STOP KStreamRecov32Tester.KStreamRecovTest32;
UNDEPLOY APPLICATION KStreamRecov32Tester.KStreamRecovTest32;
DROP APPLICATION KStreamRecov32Tester.KStreamRecovTest32 CASCADE;

DROP USER KStreamRecov32Tester;
DROP NAMESPACE KStreamRecov32Tester CASCADE;
CREATE USER KStreamRecov32Tester IDENTIFIED BY KStreamRecov32Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov32Tester;
CONNECT KStreamRecov32Tester KStreamRecov32Tester;

CREATE APPLICATION KStreamRecovTest32 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION KStreamRecovTest32;

--
-- Recovery Test 3
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> JW -> CQ(aggregate) -> WS
--

STOP Recov3Tester.RecovTest3;
UNDEPLOY APPLICATION Recov3Tester.RecovTest3;
DROP APPLICATION Recov3Tester.RecovTest3 CASCADE;
CREATE APPLICATION RecovTest3 RECOVERY 1 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount int,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_INT(TO_DOUBLE(data[7])),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

END APPLICATION RecovTest3;

stop ORAToBigquery;
undeploy application ORAToBigquery;
drop application ORAToBigquery cascade;

CREATE APPLICATION ORAToBigquery RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE Rac11g USING OracleReader ( 
  SupportPDB: false,
  SendBeforeImage: true,
  ReaderType: 'LogMiner',
  CommittedTransactions: false,
  FetchSize: 1,
  Password: 'manager',
  DDLTracking: false,
  StartTimestamp: 'null',
  OutboundServerProcessName: 'WebActionXStream',
  OnlineCatalog: true,
  ConnectionURL: '192.168.33.10:1521/XE',
  SkipOpenTransactions: false,
  Compression: false,
  QueueSize: 40000,
  RedoLogfiles: 'null',
  Tables: 'SYSTEM.GGAUTHORIZATIONS',
  Username: 'system',
  FilterTransactionBoundaries: true,
  adapterName: 'OracleReader',
  XstreamTimeOut: 600,
  connectionRetryPolicy: 'timeOut=30, retryInterval=60, maxRetries=3'
 ) 
OUTPUT TO DataStream;

CREATE OR REPLACE TARGET Target1 USING SysOut ( 
  name: "dstream"
 ) 
INPUT FROM DataStream;

CREATE OR REPLACE TARGET Target2 using BigqueryWriter(
  BQServiceAccountConfigurationPath:"/Users/ravipathak/Downloads/big-querytest-1963ae421e90.json",
  projectId:"big-querytest",
  Tables: "SYSTEM.GGAUTHORIZATIONS,testing1.ggauthorisation",
  parallelismCount: 2,
  BatchPolicy: "eventCount:100000,Interval:0")
INPUT FROM DataStream;

END APPLICATION ORAToBigquery;

deploy application ORAToBigquery;
start ORAToBigquery;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.DatabaseReader (
  FetchSize: 1,
  ConnectionURL: '@ORACLE-URL@',
  Tables: '@SOURCE-TABLES@',
  Username: '@ORACLE-USERNAME@',
  Password: '@ORACLE-PASSWORD@'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt USING SnowflakeWriter

(
  ConnectionURL:'@SNOWFLAKE-URL@',
  username:'@SNOWFLAKE-USERNAME@',
  appendOnly:'false',
  Tables:'@TARGET-TABLES@',
  uploadpolicy:'eventcount:3,interval:10s',
  externalStageType:'local'
)
INPUT FROM @APP_NAME@_Stream;


END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

create Application UdpDsv;
create source UdpDsvCSVSource using UDPReader (
	IpAddress:'127.0.0.1',
	PortNo:'3546',
	charset: 'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO UdpDsvCsvStream;
create Target UdpDsvDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/userLookup') input from UdpDsvCsvStream;
end Application UdpDsv;

CREATE SOURCE @SOURCE_NAME@ USING Global.incREmEnTALBatchrEADer (
  StartPosition: '@startPosition@',
  ConnectionURL: '@sourceURL@',
  Username: '@userName@',
  Tables: '@tables@',
  CheckColumn: '@checkColum@',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  Password: '@password@',
  Password_encrypted: 'false',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[0]) != '2';

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSV1Source using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO Csv1Stream;

Create Type CSV1Type (
  merchantId String,
  merchantName String
);

Create Stream TypedCSV1Stream of CSV1Type;

CREATE CQ CsvToMerchantNames
INSERT INTO TypedCSV1Stream
SELECT data[0],
       data[1]
FROM Csv1Stream;

create Target t using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'@FEATURE-DIR@/logs/',
  compressiontype : 'garbage',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (

)
input from TypedCSV1Stream;

end application DSV;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@ (a String, b String ,c String,g String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]) where TO_String(data[0]) > '1' ;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

STOP APPLICATION BooleanExpressionsTester.BooleanExpressionsApp;
UNDEPLOY APPLICATION BooleanExpressionsTester.BooleanExpressionsApp;
DROP APPLICATION BooleanExpressionsTester.BooleanExpressionsApp CASCADE;

CREATE APPLICATION BooleanExpressionsApp;

CREATE source wsSource USING FileReader (
directory:'@TEST-DATA-PATH@',
wildcard:'bool.csv',
blocksize: 10240,
positionByEOF:false
)
PARSE USING DSVParser (
header:No,
trimquote:false
) OUTPUT TO QaStream;

CREATE TYPE wsData
(
bankbool boolean,
bankID integer key
);


CREATE WACTIONSTORE oneWS
CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@



CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT To_boolean(data[0]),to_int(data[1]) FROM QaStream
LINK SOURCE EVENT;


END APPLICATION BooleanExpressionsApp;

--
-- Recovery Test 14 with one source, a pattern matching CQ, and one WactionStore
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> PM-CQ -> WS
--

STOP Recov14Tester.RecovTest14;
UNDEPLOY APPLICATION Recov14Tester.RecovTest14;
DROP APPLICATION Recov14Tester.RecovTest14 CASCADE;
CREATE APPLICATION RecovTest14 RECOVERY 10 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  companyName String,
  merchantId String,
  dataCode int KEY,
  expDate String
);

CREATE TYPE WactionData (
companyName1 String KEY,
companyName2 String,
companyName3 String,
dataCode1 int,
dataCode2 int,
dataCode3 int
);

CREATE STREAM DataStream OF CsvData;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_INT(data[3]),
    data[5]
FROM CsvStream;





CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@


CREATE CQ PatternMatchCQ
INSERT INTO Wactions
SELECT A.companyName as companyName1,
        B.companyName as companyName2,
        C.companyName as companyName3,
        A.dataCode as dataCode1,
        B.dataCode as dataCode2,
        C.dataCode as dataCode3
from DataStream
MATCH_PATTERN A E*  B D* C
DEFINE
A=DataStream(dataCode = 0),
E=DataStream(datacode != 1),
B=DataStream(dataCode = 1),
D= Datastream(datacode != 2),
C=DataStream(dataCode = 2);






CREATE WINDOW RestartFromSpecificLocation
OVER CsvStream KEEP 8 ROWS;





END APPLICATION RecovTest14;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

create stream @stream1@ of Global.WAEvent;

CREATE TYPE @type@(
  id String,
  name String  
);

CREATE STREAM @typeStream@ OF @type@;

CREATE CQ @CQName1@
INSERT INTO @typeStream@
SELECT TO_STRING(p.data[0]), 
       TO_STRING(p.data[1])
FROM @SRCINPUTSTREAM@ p;

create Target @targetsys1@ using SysOut(name:TypeOut) input from @typeStream@;

create cq @CQName2@ 
insert into @stream1@
select convertTypedeventToWAevent(c, 'admin.@type@')
from @typeStream@ c;

create stream @CQOUTPUTSTREAM@ of Global.WAEvent;

CREATE OR REPLACE CQ @CQName@ INSERT INTO @CQOUTPUTSTREAM@ select @FUNCTION@ from @stream1@  s ;

create Target @targetsys2@ using SysOut(name:EventOut) input from @CQOUTPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:1',
  Tables: '@TargetTableMapping@'
 ) 
INPUT FROM @CQOUTPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

CREATE APPLICATION IR recovery 5 second interval;

CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
 
  FetchSize: 5000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=t1',
 startPosition: '%=-1',
  PollingInterval: '20sec'
  )
  OUTPUT TO data_stream;
create target AzureSQLDWHTarget using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10000,interval:300s'
) INPUT FROM data_stream;
  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

END APPLICATION IR;
deploy application IR;
start IR;

CREATE OR REPLACE PROPERTYVARIABLE RetryPolicy='timeOut=00,retryInterval=1,maxRetries=3';
CREATE OR REPLACE PROPERTYVARIABLE KafkaBrokerAddress='localhost:9099';

STOP @WRITERAPPNAME@;
UNDEPLOY APPLICATION @WRITERAPPNAME@;
DROP APPLICATION @WRITERAPPNAME@ CASCADE;

CREATE APPLICATION @WRITERAPPNAME@ @Recovery@;
create flow AgentFlow;
CREATE SOURCE S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.oracle_kw_test%',
	FetchSize: '1',
	connectionRetryPolicy:'$RetryPolicy'
)
OUTPUT TO SS;
end flow AgentFlow;
create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;
create stream out_cq_select_SS_1 of global.waevent;
create stream out_cq_select_SS_2 of global.waevent;
create stream out_cq_select_SS_3 of global.waevent;
create stream out_cq_select_SS_4 of global.waevent;
create stream out_cq_select_SS_5 of global.waevent;
create stream out_cq_select_SS_6 of global.waevent;

CREATE OR REPLACE CQ cq_select_SS1 
INSERT INTO out_cq_select_SS_1
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST1';

CREATE OR REPLACE CQ cq_select_SS2 
INSERT INTO out_cq_select_SS_2
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST2';

CREATE OR REPLACE CQ cq_select_SS3 
INSERT INTO out_cq_select_SS_3
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST3';

CREATE OR REPLACE CQ cq_select_SS4 
INSERT INTO out_cq_select_SS_4
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST4';

CREATE OR REPLACE CQ cq_select_SS5 
INSERT INTO out_cq_select_SS_5
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST5';

CREATE OR REPLACE CQ cq_select_SS6 
INSERT INTO out_cq_select_SS_6
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST6';

create Target TARGET1 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_sync_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from out_cq_select_SS_1;

create Target TARGET2 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_sync_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from out_cq_select_SS_2;

create Target TARGET3 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_sync_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_sync_CQ.avsc')
input from out_cq_select_SS_3;

create Target TARGET4 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_Async_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(ROWID)',
Mode:'Async',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from out_cq_select_SS_4;

create Target TARGET5 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_Async_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(ROWID)',
Mode:'Async',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from out_cq_select_SS_5;

create Target TARGET6 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_Async_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(ROWID)',
Mode:'Async',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_Async_CQ.avsc')
input from out_cq_select_SS_6;

create Target TARGET7 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_sync',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from ss;

create Target TARGET8 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_sync',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from ss;

create Target TARGET9 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_sync',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_sync.avsc')
input from ss;

create Target TARGET10 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_Async',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'Async',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from ss;

create Target TARGET11 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_Async',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'Async',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from ss;

create Target TARGET12 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_Async',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'Async',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_Async.avsc')
input from ss;

end flow serverFlow;
end application @WRITERAPPNAME@;
deploy application @WRITERAPPNAME@;
start @WRITERAPPNAME@;



stop application @READERAPPNAME@;
undeploy application @READERAPPNAME@;
drop application @READERAPPNAME@ cascade;
CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @SOURCE@_DSV_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_sync_CQ',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;

CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
rolloverpolicy:'filesize:500M',
filename:'@READERAPPNAME@_@SOURCE@_dsv_sync_CQ')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream1;

CREATE SOURCE @SOURCE@_JSON_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_sync_CQ',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;

CREATE SOURCE @SOURCE@_AVRO_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_sync_CQ',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_sync_CQ.avsc'
)
OUTPUT TO KafkaReaderStream3;

CREATE SOURCE @SOURCE@_DSV_Async_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_Async_CQ',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream4;

CREATE SOURCE @SOURCE@_JSON_Async_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_Async_CQ',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream5;

CREATE SOURCE @SOURCE@_AVRO_Async_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_Async_CQ',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_Async_CQ.avsc'
)
OUTPUT TO KafkaReaderStream6;

CREATE SOURCE @SOURCE@_DSV_sync USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_sync',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream7;

CREATE TARGET kafkaDumpDSV_rawstream USING FileWriter(
name:kafkaOuputDSV_rawstream,
rolloverpolicy:'filesize:500M',
filename:'@READERAPPNAME@_@SOURCE@_dsv_sync')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream7;

CREATE SOURCE @SOURCE@_JSON_sync USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_sync',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream8;

CREATE SOURCE @SOURCE@_AVRO_sync USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_sync',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_sync.avsc'
)
OUTPUT TO KafkaReaderStream9;

CREATE SOURCE @SOURCE@_DSV_Async USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_Async',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream10;

CREATE SOURCE @SOURCE@_JSON_Async USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_Async',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream11;

CREATE SOURCE @SOURCE@_AVRO_Async USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_Async',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_Async.avsc'
)
OUTPUT TO KafkaReaderStream12;

end application @READERAPPNAME@;
deploy application @READERAPPNAME@;

stop @appName@;
undeploy application @appName@;
drop application @appName@ cascade;

CREATE OR REPLACE APPLICATION @appName@;

-x-

CREATE SOURCE @appName@_Source USING Global.MysqlReader (
  Username: '@UserName@',
  ConnectionURL: '@ConnectionURL@',
  Password: '@Password@',
  Tables: '@SourceTable@' )
OUTPUT TO @appName@_st1 ;

-x-

CREATE TYPE @appName@_cache_output_Type (
 product_id java.lang.Integer,
 product_name java.lang.String,
 discount java.lang.Integer);

CREATE TYPE @appName@_cache_type (
 p_id java.lang.Integer KEY,
 category java.lang.String,
 quantity java.lang.Integer,
 discount java.lang.Integer);

CREATE OR REPLACE STREAM @appName@_cache_output_st OF @appName@_cache_output_Type;

CREATE OR REPLACE CACHE @appName@_cache_comp USING DatabaseReader (
  Query: 'select * from @LookUpTableForCache@',
  username: '@UserName@',
  FetchSize: 1,
  ConnectionURL: '@ConnectionURL@',
  password: '@Password@' )
QUERY (
  keytomap: 'p_id'
  )
OF  @appName@_cache_type;

CREATE OR REPLACE CQ @appName@_cache_output_cq
INSERT INTO @appName@_cache_output_st
SELECT
      p.data[0] as product_id,
      p.data[1] as product_name,
      pcache.discount as discount
   FROM
   @appName@_st1 p INNER JOIN @appName@_cache_comp pcache ON TO_INT(p.data[0]) = pcache.p_id;

CREATE OR REPLACE TARGET @appName@_cacheTarget USING Global.DatabaseWriter (
  CommitPolicy: 'EventCount:10,Interval:10',
  BatchPolicy: 'EventCount:10,Interval:10',
  Username: '@UserName@',
  ConnectionURL: '@ConnectionURL@',
  Password: '@Password@',
  Tables: '@cache_target_table@' )
INPUT FROM @appName@_cache_output_st;

-x-

CREATE TYPE @appName@_external_cache_type (
 p_id java.lang.Integer KEY,
 category java.lang.String,
 quantity java.lang.Integer,
 discount java.lang.Integer);

CREATE EXTERNAL CACHE @appName@_external_cache_comp (
  Columns: 'p_id,category,quantity,discount',
  KeyToMap: 'p_id',
  AdapterName: 'DatabaseReader',
  Table: '@LookUpTableForExternalCache@',
  Username: '@UserName@',
  ConnectionURL: '@ConnectionURL@',
  Password: '@Password@' )
OF @appName@_external_cache_type;

CREATE CQ @appName@_external_cache_output_cq
INSERT INTO @appName@_external_cache_output_st
SELECT p.data[0] as product_id,p.data[1] as product_name,ecp.quantity as quantity
  FROM @appName@_st1  p inner join @appName@_external_cache_comp ecp
  on TO_INT(p.data[0]) = ecp.p_id;

CREATE OR REPLACE TARGET @appName@_externalCacheTarget USING Global.DatabaseWriter (
  CommitPolicy: 'EventCount:10,Interval:10',
  BatchPolicy: 'EventCount:10,Interval:10',
  Username: '@UserName@',
  ConnectionURL: '@ConnectionURL@',
  Password: '@Password@',
  Tables: '@externalCache_target_table@' )
INPUT FROM @appName@_external_cache_output_st;

-x-

END APPLICATION @appName@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.WAEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader ()
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING Global.FileWriter ()
FORMAT USING Global.JSONFormatter  (
  members: 'data'
)
INPUT FROM @APPNAME@PersistStream@RANDOM@;

END APPLICATION @APPNAME@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @srcName@ USING OracleReader (
  Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@'
)
OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@ USING Global.SpannerWriter ( 
   Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
  InstanceID: '@instanceId@', 
  ServiceAccountKey: '@keyFileName@', 
  CheckpointTable: 'CHKPOINT', 
  ProjectId: '@projectId@', 
  PrivateServiceConnectEndpoint: '@privatelinkname@', 
  BatchPolicy: 'EventCount: 1, Interval: 60s') 
INPUT FROM @instreamname@;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

Stop IR;
Undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;
create flow AgentFlow;
CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.119/DBS_PORT=1025',
  Tables: 'striim.upgrade01',
  CheckColumn:'striim.upgrade01=t1',
  startPosition:'striim.upgrade01=2018-09-20 06:43:59',
  ReturnDateTimeAs:'string'
  )
OUTPUT TO data_stream1;
end flow AgentFlow;

create flow serverFlow;
create target AzureSQLDWHTarget using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test26,dbo.test26',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream1;

end flow serverFlow;
END APPLICATION IR;
deploy application IR with AgentFlow in Agents, ServerFlow in default;
start application IR;

STOP APPLICATION ORACLETOBIGQUERY;
UNDEPLOY APPLICATION ORACLETOBIGQUERY;
DROP APPLICATION ORACLETOBIGQUERY CASCADE;

--create application 
CREATE APPLICATION OracleToBigquery RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE OracleSource USING OracleReader (
 ConnectionURL: '192.168.123.30:1521:ORCL',
 Tables: 'QATEST.E1PLOADTEST',
 Username: 'qatest',
 Password: 'qatest',
 FetchSize:100,
 OnlineCatalog:true,
 QueueSize:2000,
 CommittedTransactions:true,
 Compression:false
) OUTPUT TO CDCStream;

CREATE OR REPLACE TARGET bqtables using BigqueryWriter(
 BQServiceAccountConfigurationPath:"/Users/ravipathak/Downloads/bqtest-e287bcb47998.json",
 projectId:"bqtest-158706",
 Tables: "QATEST.E1PLOADTEST,issues.DEV11070",
 BatchPolicy: "eventCount:100,Interval:1")
INPUT FROM CDCStream;

CREATE OR REPLACE TARGET T1 using SysOut(
name : "some text"
)
INPUT FROM CDCStream;

END APPLICATION OracleToBigquery;

DEPLOY APPLICATION OracleToBigquery;
START APPLICATION OracleToBigquery;

CREATE APPLICATION DsPosApp;

CREATE OR REPLACE CQ CQPosApp
INSERT INTO PosStrm
select * from DS.MerchantActivity
LINK SOURCE EVENT;

-- WACTIONSTORE PosWaction is being loaded from MerchantActivity from DSWaction.tql

CREATE WACTIONSTORE PosWaction CONTEXT OF DS.MerchantActivityContext
EVENT TYPES ( DS.MerchantTxRate )
@PERSIST-TYPE@


CREATE CQ CQPosWaction
INSERT INTO PosWaction
select * from DS.PosStrm
LINK SOURCE EVENT;


END APPLICATION DsPosApp;
DEPLOY APPLICATION DsPosApp;
START APPLICATION DsPosApp;

create Target @TARGET_NAME@ using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (

)
input from @STREAM@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

create source @SourceName1@ USING IncrementalBatchReader
(
  FetchSize: 10,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: '@startPosition@',
  PollingInterval: '20sec'
)
OUTPUT TO @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:@targetsys@) input from @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
  ConnectionURL:'@READER-URL@',
  Username:'@READER-UNAME@',
  Password:'@READER-PASSWORD@',
  BatchPolicy:'Eventcount:1,Interval:1',
  CommitPolicy:'Eventcount:1,Interval:1',
  Checkpointtable:'RGRN_CHKPOINT',
  Tables:'@WATABLES@,@WATABLES@_target'
) INPUT FROM @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;

DEPLOY APPLICATION @APPNAME@;
start application @APPNAME@;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
drop application @APPNAME1@ cascade;

CREATE APPLICATION @APPNAME1@;

create source @SourceName2@ USING IncrementalBatchReader
(
FetchSize: 10,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn1@,
  startPosition: '@startPosition1@',
  PollingInterval: '20sec'
)
OUTPUT TO @SRCINPUTSTREAM1@;

create Target @targetsys1@ using SysOut(name:@targetsys1@) input from @SRCINPUTSTREAM1@;

CREATE TARGET @targetName1@ USING DatabaseWriter(
  ConnectionURL:'@READER-URL@',
  Username:'@READER-UNAME@',
  Password:'@READER-PASSWORD@',
  BatchPolicy:'Eventcount:1,Interval:1',
  CommitPolicy:'Eventcount:1,Interval:1',
  Checkpointtable:'RGRN_CHKPOINT',
  Tables:'@WATABLES_target'
) INPUT FROM @SRCINPUTSTREAM1@;

END APPLICATION @APPNAME1@;

DEPLOY APPLICATION @APPNAME1@;
start application @APPNAME1@;

stop application JMSWriter.JMS;
undeploy application JMSWriter.JMS;
drop application JMSWriter.JMS cascade;

create application JMS;
create source JMSCSVSource using FileReader (
        directory:'@TEST-DATA-PATH@',
        WildCard:'AdhocQueryData2.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target JmsTarget  using JMSWriter (
		Provider:'@JMSWRITERPROVIDER@',
		Ctx:'@JMSWRITERCONTEXT@',
		messagetype: @MESSAGETYPE@,
		UserName:'@JMSWRITERUSERNAME@',
		Password:'@JMSWRITERPASSWORD@',
		@DESTINATIONTYPE@)
format using @JMSTARGETFORMATTERTYPE@ (
@JMSTARGETFORMATTERMEMBERS@
)
input from TypedCSVStream;

--SECOND TARGET--

Create Type SecondType (
  zip String,
  city String
);

Create Stream SecondStream of SecondType;

CREATE CQ SecondCQ
INSERT INTO SecondStream
SELECT data[9],data[10]
FROM CsvStream;

create Target SecondTarget using JMSWriter (
		Provider:'@JMSWRITERPROVIDER@',
		Ctx:'@JMSWRITERCONTEXT@',
		messagetype: @MESSAGETYPE@,
		UserName:'@JMSWRITERUSERNAME@',
		Password:'@JMSWRITERPASSWORD@',
		@DESTINATIONTYPE@)
format using @SECONDTARGETFORMATTERTYPE@ (
@SECONDTARGETFORMATTERMEMBERS@
)
input from SecondStream;

end Application Jms;

--
-- Canon Test W32
-- Nicholas Keene, WebAction, Inc.
--
-- Test having one source leading to two data paths with unpartitioned
-- jumping count windows leading to one waction store.
--
-- S -> JWc5u -> CQ5 -> WS
-- S -> JWc2u -> CQ2 -> WS
--


UNDEPLOY APPLICATION NameW32.W32;
DROP APPLICATION NameW32.W32 CASCADE;
CREATE APPLICATION W32 RECOVERY 5 SECOND INTERVAL;




CREATE FLOW DataAcquisitionW32;


CREATE SOURCE CsvSourceW32 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW32;


END FLOW DataAcquisitionW32;




CREATE FLOW DataProcessingW32;

CREATE TYPE DataTypeW32 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW32 OF DataTypeW32;

CREATE CQ CSVStreamW32_to_DataStreamW32
INSERT INTO DataStreamW32
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW32;

CREATE JUMPING WINDOW JWc5uW32
OVER DataStreamW32
KEEP 5 ROWS;

CREATE JUMPING WINDOW JWc2uW32
OVER DataStreamW32
KEEP 2 ROWS;

CREATE WACTIONSTORE WactionStoreW32 CONTEXT OF DataTypeW32
EVENT TYPES ( DataTypeW32 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWc5uW32_to_WactionStoreW32
INSERT INTO WactionStoreW32
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc5uW32;

CREATE CQ JWc2uW32_to_WactionStoreW32
INSERT INTO WactionStoreW32
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc2uW32;

END FLOW DataProcessingW32;



END APPLICATION W32;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using AzureblobWriter(
    accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:7'
)
format using DSVFormatter (
)
input from @STREAM@;

end flow @APPNAME@_serverflow;

end application @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@ USING Ojet
(
 Username:'@OJET-UNAME@',
Password:'@OJET-PASSWORD@',
ConnectionURL:'@OCI-URL@',
Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectURL@',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (
  StartPosition: '@startPosition@',
  ConnectionURL: '@sourceURL@',
  Username: '@userName@',
  Tables: '@tables@',
  CheckColumn: '@checkColum@',
  FetchSize: 1,
  Password: '@password@',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

 create type @SOURCE_NAME@_AutoType(
  ID int,
  name string,
  country string
);

CREATE STREAM @STREAM@_stream OF @SOURCE_NAME@_AutoType;

CREATE CQ Lookup
INSERT INTO @STREAM@_stream
select data[0],data[1],data[2] from @STREAM@;

--
-- Canon Test W20
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for an unpartitioned sliding attribute window
--
-- S -> SWa5u -> CQ -> WS
--


UNDEPLOY APPLICATION NameW20.W20;
DROP APPLICATION NameW20.W20 CASCADE;
CREATE APPLICATION W20 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW20;

CREATE SOURCE CsvSourceW20 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW20;

END FLOW DataAcquisitionW20;



CREATE FLOW DataProcessingW20;

CREATE TYPE DataTypeW20 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW20 OF DataTypeW20;

CREATE CQ CSVStreamW20_to_DataStreamW20
INSERT INTO DataStreamW20
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW20;

CREATE WINDOW SWa5uW20
OVER DataStreamW20
KEEP WITHIN 5 SECOND ON dateTime;

CREATE WACTIONSTORE WactionStoreW20 CONTEXT OF DataTypeW20
EVENT TYPES ( DataTypeW20 KEY(word) )
@PERSIST-TYPE@

CREATE CQ SWa5uW20_to_WactionStoreW20
INSERT INTO WactionStoreW20
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM SWa5uW20;

END FLOW DataProcessingW20;



END APPLICATION W20;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	accesskeyid:'ACCESS_KEY',
	secretaccesskey:'SECRET_KEY'
)
format using JSONFormatter (
)
input from DDLCDCStream;

CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (
  StartPosition: '@startPosition@',
  ConnectionURL: '@sourceURL@',
  Username: '@userName@',
  Tables: '@tables@',
  CheckColumn: '@checkColum@',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  Password: '@password@',
  Password_encrypted: 'false',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

create or replace type @STREAM@details(
C_CUSTKEY int,
C_MKTSEGMENT String,
C_NATIONKEY int,
C_NAME String,
C_ADDRESS String,
C_PHONE String,
C_ACCTBAL int,
C_COMMENT String
);

create or replace stream @STREAM@_TYPED of @STREAM@details;

Create or replace CQ @STREAM@detailsCQ
insert into @STREAM@_TYPED
select 
to_int(data[0]),data[1],to_int(data[2]),data[3],data[4],data[5],to_int(data[6]),data[7]
from @STREAM@;

CREATE WINDOW @STREAM@_DBRWindow
OVER @STREAM@_TYPED
KEEP 1000 ROWS;

create or replace stream @STREAM@_TYPED2 of @STREAM@details;

Create or replace CQ @STREAM@detailsCQ2
insert into @STREAM@_TYPED2
select 
to_int(C_CUSTKEY),C_MKTSEGMENT,to_int(C_NATIONKEY),C_NAME,C_ADDRESS,C_PHONE,to_int(C_ACCTBAL),C_COMMENT
from @STREAM@_DBRWindow;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING FileWriter  ( 
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
 ) Format using DSVFormatter()
INPUT FROM @STREAM@_DBRWindow;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 5 second interval;
create source CSVSource using FileReader (
	directory:'@DIR@',
	WildCard:'@WILDCARD@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target BlobT using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:50'
)
format using DSVFormatter (
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( 
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: '@CDC_URL@',
  Tables: '@Source1Tables@',
  FetchSize: 1) 
OUTPUT TO @APPNAME@cdcStream;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

create source @SourceName1@ USING IncrementalBatchReader
(
  FetchSize: 10,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: '@startPosition@',
  PollingInterval: '20sec'
)
OUTPUT TO @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:@targetsys@) input from @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
  ConnectionURL:'@READER-URL@',
  Username:'@READER-UNAME@',
  Password:'@READER-PASSWORD@',
  BatchPolicy:'Eventcount:1,Interval:1',
  CommitPolicy:'Eventcount:1,Interval:1',
  Checkpointtable:'RGRN_CHKPOINT',
  Tables:'@WATABLES@,@WATABLES@_target'
) INPUT FROM @SRCINPUTSTREAM@;

create source @SourceName2@ USING IncrementalBatchReader
(
FetchSize: 10,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn1@,
  startPosition: '@startPosition1@',
  PollingInterval: '20sec'
)
OUTPUT TO @SRCINPUTSTREAM1@;

create Target @targetsys1@ using SysOut(name:@targetsys1@) input from @SRCINPUTSTREAM1@;

CREATE TARGET @targetName1@ USING DatabaseWriter(
  ConnectionURL:'@READER-URL@',
  Username:'@READER-UNAME@',
  Password:'@READER-PASSWORD@',
  BatchPolicy:'Eventcount:1,Interval:1',
  CommitPolicy:'Eventcount:1,Interval:1',
  Checkpointtable:'RGRN_CHKPOINT',
  Tables:'@WATABLES_target'
) INPUT FROM @SRCINPUTSTREAM1@;

END APPLICATION @APPNAME@;

DEPLOY APPLICATION @APPNAME@;
start application @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;

Create Source @SourceName@ Using Ojet

(
  Username:'c##qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',
  Tables:'CDB$ROOT."C##QATEST".ojet_src;ORCLPDB.QATEST.ojet_src',
  _h_useClassic:false,
  Fetchsize:1,
  Compression: true,
  SupportPDB:true,
  ReplicationSlotName:'null'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',
  Username:'c##qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'CDB$ROOT."C##QATEST".ojet_src,CDB$ROOT."C##QATEST".ojet_tgt'
) INPUT FROM @SRCINPUTSTREAM@;

CREATE TARGET @targetName1@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orclpdb',
  Username:'qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'ORCLPDB.QATEST.ojet_src,ORCLPDB.QATEST.ojet_tgt'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

use PosTester;
alter application PosApp;

CREATE CACHE HourlyAveLookup using CSVReader (
  directory: 'Samples/Customer/PosApp/appData',
  wildcard: 'hourlyData.txt',
  header: Yes,
  columndelimiter: ',',
  trimquote:false
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

end application PosApp;

alter application PosApp recompile;

CREATE OR REPLACE  EMBEDDINGGENERATOR @EMB_NAME@ USING @MODEL@ (
'modelProvider': '@MODEL@',
'modelName': '@MODEL_NAME@',
'project': '@PROJECT@',
'location': '@LOCATION@',
'publisher': '@PUBLISHER@',
'serviceAccountKey': '@SERVICE_ACCOUNT_KEY@'
);

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@  RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @srcName@ USING OracleReader (
 Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@'
)
OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@ USING AzureEventHubWriter (
  SASKey:'@saasKey@',
  EventHubNamespace:'@eventhubNamespace@',
  ConsumerGroup:'@consumerGrp@',
  SASPolicyName:'RootManageSharedAccessKey',
  E1P:'true',
  OperationTimeout:'1m',
  ConnectionRetryPolicy:'Retries:0,RetryBackOff:1m',
  EventHubName:'@eventhub@',
  BatchPolicy:'Size:1000000,Interval:1m'
)
FORMAT USING JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  members: 'data',
  jsonobjectdelimiter: '\n' )
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

CREATE TARGET @TARGET_NAME@ USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest',
	batchPolicy: 'EventCount: 10000,Interval:600s'
) INPUT FROM @STREAM@;

stop ADW;
undeploy application ADW;
drop application ADW cascade;

CREATE APPLICATION ADW recovery 5 second interval;
create flow agentflow;
CREATE OR REPLACE SOURCE CSVPoller USING FileReader (
directory:'@DIRECTORY@',
WildCard:'posdata5L.csv',
positionByEOF:false
)
parse using DSVParser (
header:'no'
)
OUTPUT TO CsvStream;
end flow agentflow;

create flow targetflow;
CREATE OR REPLACE TYPE CSVStream_Type  ( BUSINESS_NAME java.lang.String KEY,
MERCHANT_ID java.lang.String,
PRIMARY_ACCOUNT_NUMBER java.lang.String
 ) ;

CREATE OR REPLACE STREAM CSVTypeStream OF CSVStream_Type;
CREATE OR REPLACE CQ CQ1
INSERT INTO CSVTypeStream
SELECT data[0],data[1],data[2]
FROM CsvStream;

create target WriteToAzureSQLWH using AzureSQLDWHWriter (
       ConnectionURL: '@CONNECTION-URL@',
       username: '@USERNAME@',
       password: '@PASSWORD@',
	   AccountName: '@STORAGE-ACCOUNT@',
       AccountAccessKey: '@ACCESS-KEY@',
      Tables: '@TARGET-TABLE@',
      uploadpolicy:'eventcount:20000,interval:1m'
) INPUT FROM CSVTypeStream;
end flow targetflow;
END APPLICATION ADW;
deploy application ADW with agentflow in agents,targetflow in default;
start ADW;

stop APPLICATION @AppName@;
Undeploy APPLICATION @AppName@;
drop APPLICATION @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @AgentFlow@;

CREATE OR REPLACE SOURCE @SourceName@ USING Global.MSJet (
  Tables: 'dbo.compsrc',
  username: 'qatest',
  DatabaseName: 'qatest',
  FetchTransactionMetadata: true,
  filterTransactionBoundaries: true,
  compression: false,
  ConnectionURL: '@ConnectionURL@',
  Mode: '@mode@',
  CommittedTransactions: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
  SendBeforeImage: true,
  password: 'w3b@ct10n' )
OUTPUT TO @StreamName@;
END FLOW @AgentFlow@;

CREATE TARGET @SysTarget@ USING Global.SysOut (
  name: 'MS_CDC_SYSOUT' )
INPUT FROM @StreamName@;

CREATE FLOW @ServerFlow@;

CREATE TARGET @TargetName@ USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ParallelThreads: '',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: '@ConnectionURL@',
  StatementCacheSize: '50',
  DatabaseProviderType: 'Default',
  Username: 'qatest',
  Tables: 'dbo.compsrc,dbo.comptar',
  Password: 'w3b@ct10n',
  PreserveSourceTransactionBoundary: 'false',
  BatchPolicy: 'EventCount:1,Interval:60' )
INPUT FROM @StreamName@;

END FLOW @ServerFlow@;

END APPLICATION @AppName@;
DEPLOY APPLICATION @AppName@ with @AgentFlow@ in AGENTS ,@ServerFlow@ on any in default;
START APPLICATION @AppName@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName1@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM1@;
Create Source @SourceName2@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM2@;
Create Source @SourceName3@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM3@;
Create Source @SourceName4@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM4@;
Create Source @SourceName5@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM5@;
Create Source @SourceName6@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM6@;
Create Source @SourceName7@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM7@;
Create Source @SourceName8@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM8@;
Create Source @SourceName9@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM9@;
Create Source @SourceName10@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM10@;

CREATE TARGET @targetName1@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM1@;
CREATE TARGET @targetName2@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM2@;
CREATE TARGET @targetName3@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM3@;
CREATE TARGET @targetName4@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM4@;
CREATE TARGET @targetName5@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM5@;
CREATE TARGET @targetName6@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM6@;
CREATE TARGET @targetName7@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM7@;
CREATE TARGET @targetName8@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM8@;
CREATE TARGET @targetName9@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM9@;
CREATE TARGET @targetName10@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM10@;


END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

Undeploy application bq;
alter application bq;
CREATE OR REPLACE SOURCE S USING OracleReader  ( 
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.TABLE_TEST_1000100',
  DictionaryMode: offlineCatalog,
  FetchSize: '1'
 ) 
OUTPUT TO SS;
alter application bq recompile;
deploy application bq;

CREATE OR REPLACE APPLICATION @AppName@;

Create Source @AppName@_source Using OracleReader(
  Username:'@username@',
  Password:'@password@',
  ConnectionURL:'@url@',
  Tables:'@srctableName@',
  Fetchsize:1
)
Output To @AppName@_Stream;

CREATE OR REPLACE TARGET @AppName@_DB_Target USING Global.DeltaLakeWriter (
connectionProfileName: 'admin.@DBCP@',
   useConnectionProfile: 'true',
  Tables: '@srctableName@,@trgtableName@',
  uploadPolicy: 'eventcount:100000,interval:60s'
)

INPUT FROM @AppName@_Stream;

CREATE OR REPLACE TARGET @AppName@_DB_Target2 USING Global.DeltaLakeWriter (
connectionProfileName: 'admin.@DBCP@',
   useConnectionProfile: 'true',
  Tables: '@srctableName@,@trgtableName@',
  uploadPolicy: 'eventcount:100000,interval:60s'
)

INPUT FROM @AppName@_Stream;

END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@
 Using DatabaseReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectURL@',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP APPLICATION EH;
UNDEPLOY APPLICATION EH;
DROP APPLICATION EH CASCADE;
CREATE APPLICATION EH @Recovery@;
CREATE SOURCE s USING FileReader (
	directory:'Product/IntegrationTests/TestData/',
    WildCard:'posdata.csv',
	positionByEOF:false
	)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO ss;

CREATE TYPE userType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String
);

CREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;

CREATE CQ cq1
INSERT INTO userDefinedTypedStream
SELECT data[0],data[1],data[2], data[3],data[4]
FROM ss;

create Target t1 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	Partitionkey:'@metadata(RecordOffset)',
	ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using dsvFormatter()
input from ss;

create Target t2 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	Partitionkey:'Col1',
	ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using dsvFormatter()
input from userDefinedTypedStream;

END APPLICATION EH;
DEPLOY APPLICATION EH;
start application EH;

STOP APPLICATION ER;
UNDEPLOY APPLICATION ER;
DROP APPLICATION ER CASCADE;
CREATE APPLICATION ER;
CREATE SOURCE ER_S1 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:0	
	)
PARSE USING DSVParser (
)OUTPUT TO ER_SS1;
CREATE SOURCE ER_S2 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:0	
	)
PARSE USING DSVParser (
)OUTPUT TO ER_SS2;

CREATE TYPE CustType1(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
TopicName String,
PartitionID String
);

CREATE TYPE CustType2(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
TopicName String,
PartitionID String
);

CREATE Stream DSVReaderStream1 of CustType1;
CREATE Stream DSVReaderStream2 of CustType2;

CREATE CQ CustType_CQ1
INSERT INTO DSVReaderStream1
SELECT data[5],data[6],data[7],data[8],data[9],data[10],
metadata.get("TopicName").toString() AS TopicName,
metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS1;

CREATE CQ CustType_CQ2
INSERT INTO DSVReaderStream2
SELECT data[0],data[1],data[2],data[3],data[4],
metadata.get("TopicName").toString() AS TopicName,
metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS2;

create Target ER_t1 using FileWriter (
filename:'FT1_5L_DSV_RT',
directory:'FEATURE-DIR/logs/',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from DSVReaderStream1;

create Target ER_t2 using FileWriter (
filename:'FT2_5L_DSV_RT',
directory:'FEATURE-DIR/logs/',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from DSVReaderStream2;
end application ER;
deploy application ER;

--
-- Crash Recovery Test 3 on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW -> CQ(aggregate) -> WS
--

STOP APPLICATION N4S4CR3Tester.N4S4CRTest3;
UNDEPLOY APPLICATION N4S4CR3Tester.N4S4CRTest3;
DROP APPLICATION N4S4CR3Tester.N4S4CRTest3 CASCADE;
CREATE APPLICATION N4S4CRTest3 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest3;

CREATE SOURCE CsvSourceN4S4CRTest3 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest3;

CREATE FLOW DataProcessingN4S4CRTest3;

CREATE TYPE WactionTypeN4S4CRTest3 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount int,
  city String
);

CREATE STREAM DataStream OF WactionTypeN4S4CRTest3;

CREATE CQ CsvToDataN4S4CRTest3
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_INT(TO_DOUBLE(data[7])),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN4S4CRTest3 CONTEXT OF WactionTypeN4S4CRTest3
EVENT TYPES ( WactionTypeN4S4CRTest3 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN4S4CRTest3
INSERT INTO WactionsN4S4CRTest3
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

END FLOW DataProcessingN4S4CRTest3;

END APPLICATION N4S4CRTest3;

stop ROLLUPMON_CDC;
undeploy application ROLLUPMON_CDC;
alter application ROLLUPMON_CDC;
CREATE or replace FLOW ROLLUPMON_CDC_flow;
Create or replace Source ROLLUPMON_CDC_Oraclesrc Using oraclereader(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',
 Tables:'QATEST.ROLLUPMON_TABLE1;QATEST.ROLLUPMON_TABLE2;QATEST.ROLLUPMON_TABLE3;QATEST.ROLLUPMON_TABLE4;QATEST.ROLLUPMON_TABLE5',
 Fetchsize:1000,
 connectionRetryPolicy:'maxRetries=4',
 TransactionBufferSpilloverSize:'200MB',
 _h_fetchexactrowcount: 'true'
)
Output To ROLLUPMON_CDC_OrcStrm;
END FLOW ROLLUPMON_CDC_flow;
alter application ROLLUPMON_CDC recompile;
DEPLOY APPLICATION ROLLUPMON_CDC;
start application ROLLUPMON_CDC;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;


CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using MysqlReader
(
   adapterName: MysqlReader,
   CDDLAction: Process,
   CDDLCapture: false,
   Compression: false,
   ConnectionURL: jdbc:mysql://localhost:3306/waction,
   FilterTransactionBoundaries: true,
   Password: ReaderPassword,
   SendBeforeImage: true,
   Tables: srcTable,
   Username: ReaderUsername
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;

Create Source @SourceName@ Using OracleReader

(
  Username:'c##qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',
  Tables:'CDB$ROOT."C##QATEST".ojet_src;ORCLPDB.QATEST.ojet_src',
  _h_useClassic:false,
  Fetchsize:1,
  Compression: true,
  SupportPDB:true,
  ReplicationSlotName:'null'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',
  Username:'c##qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'CDB$ROOT."C##QATEST".ojet_src,CDB$ROOT."C##QATEST".ojet_tgt'
) INPUT FROM @SRCINPUTSTREAM@;

CREATE TARGET @targetName1@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orclpdb',
  Username:'qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'ORCLPDB.QATEST.ojet_src,ORCLPDB.QATEST.ojet_tgt'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP APPLICATION DBRTOCW;
UNDEPLOY APPLICATION DBRTOCW;
DROP APPLICATION DBRTOCW CASCADE;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  --QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 ) OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) INPUT FROM Oracle_ChangeDataStream;


CREATE TARGET t2 USING SysOut(name:Foo2) INPUT FROM Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;

DEPLOY APPLICATION DBRTOCW on ANY in default;

START APPLICATION DBRTOCW;

Stop @APPNAME@_App;
undeploy application @APPNAME@_App;
drop application @APPNAME@_App cascade;

CREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;
CREATE SOURCE @APPNAME@_Src USING Global.OracleReader(
  FetchSize:'1',
  Username:'@SOURCE_USER@',
  Password:'@SOURCE_PASSWORD@',
  ConnectionURL:'@SOURCE_URL@',
  Tables:'@SOURCE_TABLES@',
  _h_useClassic:'true',
  ConnectionRetryPolicy:'@AUTO_CONNECTION_RETRY@'
)
OUTPUT TO @APPNAME@_Stream;

CREATE TARGET @APPNAME@_SYS USING SysOut (
  name: '@APPNAME@_SYS' )
INPUT FROM @APPNAME@_Stream;

End APPLICATION @APPNAME@_App;

STOP @Appname@;
UNDEPLOY APPLICATION @Appname@;
DROP APPLICATION @Appname@ CASCADE;

CREATE APPLICATION @Appname@ @Recovery@ use exceptionstore;

CREATE SOURCE @Appname@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:orcl',
	Tables: 'QATEST.TABLE_TEST_%',
	FetchSize: '1'
)
OUTPUT TO @Appname@_SS;


CREATE or replace TARGET @Appname@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:10',
StandardSQL:true	
) INPUT FROM @Appname@_ss;

END APPLICATION @Appname@;
DEPLOY APPLICATION @Appname@;
START APPLICATION @Appname@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY;

Create Source @APPNAME@_src Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1000,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
   _h_ReturnDateTimeAs:'ZonedDateTime'
) Output To @APPNAME@_stream;

create Target @APPNAME@_tgt using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_stream;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

STOP IS2noder.IS2Node;
UNDEPLOY APPLICATION IS2noder.IS2Node;
DROP APPLICATION IS2noder.IS2Node CASCADE;

CREATE APPLICATION IS2Node;

CREATE FLOW ISFLOW1;
----------------------------------------------------
CREATE source implicitSOurce USING FileReader (
      directory:'@TEST-DATA-PATH@',
      columndelimiter: ',',
      wildcard:'ISdata.csv',
      blocksize: 10240,
      positionByEOF:false
)
PARSE USING DSVParser (
      header:False,
      trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity double,
  size long,
  currentDate DateTime);

END FLOW ISFLOW1;
----------------------------------------------------

CREATE FLOW ISFLOW2;

CREATE CACHE cache1 USING CsvReader(
  directory: '@TEST-DATA-PATH@',
  wildcard: 'ISdata.csv',
  header: false,
  columndelimiter: ',',
  trimquote: false
  ) QUERY (keytomap:'productID') OF Atm;


CREATE STREAM newStream OF Atm;


CREATE CQ newCQ
INSERT INTO newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]), TO_DATE(data[5]) FROM
CsvStream;

CREATE WINDOW win1
OVER newStream
KEEP 50 rows;


CREATE CQ newCQ2
INSERT INTO newStream2
SELECT productID as A , stateID AS B, productWeight AS C, quantity AS D, size AS E, currentDate AS F FROM
newStream;


CREATE CQ newCQ3
INSERT INTO newStream3 PARTITION BY A
SELECT A,B,C,D,E,F FROM newStream2 order by C,D
link source event;

CREATE CQ newCQ4
INSERT INTO newStream4
SELECT count(productID),currentDate FROM newStream ORDER BY currentDate
link source event;

CREATE CQ newCQ5
INSERT INTO newStream5
SELECT x.*, y.* from cache1 x, newStream y WHERE x.productweight > 6 ORDER BY x.currentDate;


CREATE WACTIONSTORE WS1 CONTEXT OF Atm
EVENT TYPES(Atm );

CREATE CQ newCQ6
INSERT INTO WS1
SELECT * FROM newStream WHERE productID = '001';

CREATE CQ newCQ7
INSERT INTO newStream6
SELECT aa.productID FROM WS1 [push] aa, cache1 bb;


CREATE CQ newCQ8
INSERT INTO newStream7
SELECT Sum(X.size) FROM (Select size from win1 where productweight > 5) X;

END FLOW ISFLOW2;
----------------------------------------------------

END APPLICATION IS2Node;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
	filename:'JsonTargetAdd',
	directory:'@FEATURE-DIR@/logs/',
	sequence:'00',
	rolloverpolicy:'FileSizeRollingPolicy,filesize:33 + 44M'
)
format using JSONFormatter (
	members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)

input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizeAdd_actual.log') input from TypedCSVStream;
end application DSV;

create application JuniperSaLog;
create source JuniperSaLogSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'juniper-SA*',
	charset:'UTF-8',
	positionByEOF:false
) PARSE USING JuniperSA2000LogParser (
	columndelimitTill:5
) OUTPUT TO JuniperSaLogStream;
create Target JuniperSaDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/junipersa_log',charset:'UTF-8') input from JuniperSaLogStream;
end application JuniperSaLog;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;


CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',
					acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

CREATE OR REPLACE STREAM @APPNAME@_stream OF Global.waevent persist using @APPNAME@_KafkaPropset;

CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @APPNAME@_stream2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNEC4TION_URL@',
  Tables: '$table1',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING SnowflakeWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'QATEST.ORCTOSFPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING SnowflakeWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo) input from @STREAM@;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @APPNAME@_stream3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '$table2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@4',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING SnowflakeWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'QATEST.ORCTOSFPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING SnowflakeWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@3;

END APPLICATION @APPNAME@2;

STOP APPLICATION @AppName@_App1;
UNDEPLOY APPLICATION @AppName@_App1;
DROP APPLICATION @AppName@_App1 CASCADE;
CREATE APPLICATION @AppName@_App1 recovery 1 second interval;


CREATE SOURCE @AppName@_FileSource USING FileReader (
  directory:'@TestdataDir@',
    WildCard:'banks*',
  positionByEOF:false
  )
PARSE USING DSVParser (
  header:no
)OUTPUT TO FileStream;


CREATE OR REPLACE ROUTER filerouter1 INPUT FROM FileStream s CASE
WHEN meta(s,"FileName").toString()='banks1.csv' THEN ROUTE TO stream1,
WHEN meta(s,"FileName").toString()='banks2.csv' THEN ROUTE TO stream2,
WHEN meta(s,"FileName").toString()='banks3.csv' THEN ROUTE TO stream3,
WHEN meta(s,"FileName").toString()='banks4.csv' THEN ROUTE TO stream4,
ELSE ROUTE TO ss_else;

CREATE TYPE banks1(
  id int,
  name String ,
Filename String
);

Create stream cdctypestream1 of banks1;

CREATE CQ cdcstreamcq1
INSERT INTO cdctypestream1
SELECT TO_INT(p.data[0]), 
       TO_STRING(p.data[1]), TO_STRING(META(p,'FileName'))
FROM stream1 p;


CREATE OR REPLACE TARGET @AppName@_DataBaset1 USING DatabaseWriter  ( 
ConnectionURL:'@url@',
Username:'@userName@',
Password:'@password@',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.NEWBANKS1') 
INPUT FROM cdctypestream1;

End application @AppName@_App1;
Deploy application @AppName@_App1;
start application @AppName@_App1;

stop application @AppName@_App2;
undeploy application @AppName@_App2;
drop application @AppName@_App2 CASCADE;
create application @AppName@_App2 recovery 1 second interval;

CREATE OR REPLACE TARGET @AppName@_DataBaset2 USING DatabaseWriter  ( 
ConnectionURL:'@url@',
Username:'@userName@',
Password:'@password@',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.BANKS1') 
INPUT FROM cdctypestream1;

end application @AppName@_App2;
deploy application @AppName@_App2;
start application @AppName@_App2;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING DatabaseReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE CQ @APPNAME@CQ
INSERT INTO @APPNAME@CQStream
SELECT putUserData(x, 'city',data[6])
FROM @APPNAME@stream x;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@CQStream;

END APPLICATION @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 5 Second interval;
create source @APPNAME@_Source using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'dynamicdirectory.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO @APPNAME@_Stream;

Create Type @APPNAME@_type (
  id String,
  name String,
  department String,
  yoj String,
  moj String,
  doj int
);

Create Stream @APPNAME@_typedstream of @APPNAME@_type;

CREATE CQ @APPNAME@_cq
INSERT INTO @APPNAME@_typedstream
SELECT data[0],
       data[1],
       data[2],
       data[3],
       data[4],
       TO_INT(data[5])
FROM @APPNAME@_stream;

create Target @APPNAME@_target using ADLSGen1Writer(
        filename:'',
        directory:'',
        datalakestorename:'',
        clientid:'',
        authtokenendpoint:'',
        clientkey:'',
		rolloverpolicy:'eventcount:5'
)
format using DSVFormatter (
)
input from @APPNAME@_typedstream; 

end application @APPNAME@;

deploy application @APPNAME@;
start application @APPNAME@;

create application FileXML;
create source XMLSource using FileReader (
	Directory:'@TEST-DATA-PATH@',
	WildCard:'books.xml',
	positionByEOF:false
)
parse using XMLParser (
	RootNode:'/catalog/book'
)
OUTPUT TO XmlStream;

-- Below Sysout is added to test DEV-23437.  Not directly validated in the test except the App should not crash with sysout target
CREATE TARGET XMLEventSYSout USING sysout  (
name: 'XMLEventSYSoutOut' )
INPUT FROM XmlStream;

create Target XMLDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/xmldata') input from XmlStream;
end application FileXML;

Stop Oracle_Oracle_IRLogWriterLogWriter;
Undeploy application Oracle_Oracle_IRLogWriterLogWriter;
drop application Oracle_Oracle_IRLogWriterLogWriter cascade;

CREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;

CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( 
 
  FetchSize: 5000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=t1',
 startPosition: '%=-1',
  PollingInterval: '20sec'
  )
  OUTPUT TO data_stream;
create target AzureSQLDWHTarget using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:0,interval:0s'
) INPUT FROM data_stream;
  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

END APPLICATION Oracle_IRLogWriter;
deploy application Oracle_IRLogWriter;
start Oracle_IRLogWriter;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING PostgreSQLReader  (
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 )
OUTPUT TO @STREAM@ ;

stop application MSSQLTransactionSupportMultiReaderWriter;
undeploy application MSSQLTransactionSupportMultiReaderWriter;
drop application MSSQLTransactionSupportMultiReaderWriter cascade;

CREATE APPLICATION MSSQLTransactionSupportMultiReaderWriter recovery 1 second interval;

Create Source ReadFromMSSQL4
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
--AutoDisableTableCDC:'true',
FetchTransactionMetadata:'true',
FilterTransactionBoundaries: false,
Compression:'false',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportMultiReaderWriterStream1;

Create Source ReadFromMSSQL5
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
--AutoDisableTableCDC:'true',
FetchTransactionMetadata:'true',
FilterTransactionBoundaries: false,
Compression:'true',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportMultiReaderWriterStream2;

CREATE TARGET WriteToMSSQL4 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC,@@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportMultiReaderWriterStream1;

CREATE TARGET WriteToMSSQL5 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC,@@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportMultiReaderWriterStream2;

/*CREATE TARGET MSSqlReaderOutput4 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportStream; 


/*CREATE OR REPLACE TARGET MSSQLFileOut4 USING FileWriter  ( 
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  directory:'@FEATURE-DIR@/logs/',
  filename: 'TransactionSupportAutoDisableTableCdcTrue.txt'
 ) 
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 ) 
INPUT FROM MSSQLTransactionSupportMultiReaderWriterStream;
*/
END APPLICATION MSSQLTransactionSupportMultiReaderWriter;
deploy application MSSQLTransactionSupportMultiReaderWriter;
start application MSSQLTransactionSupportMultiReaderWriter;

undeploy application reconnect;
alter application reconnect;

create or replace TARGET dbtarget USING DatabaseWriter  (
  ConnectionURL:'@URL@',
  Username:'@USERNAME@',
  Password:'@PASSWORD@',
  ConnectionRetryPolicy: 'retryInterval=20s, maxRetries=3',
  BatchPolicy:'EventCount:5,Interval:30',
  CommitPolicy:'EventCount:5,Interval:30',
  Tables: '@TABLES@'
 )
INPUT FROM sqlstream;

alter application reconnect recompile;
deploy application reconnect;
start application reconnect;

use PosTester;
alter application PosApp;

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
@PERSIST-TYPE@

end application PosApp;

alter application PosApp recompile;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using DatabaseReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
alter APPLICATION @APPNAME@;
CREATE OR REPLACE SOURCE @APPNAME@src USING Global.FileReader 
(
  adapterName:'FileReader',
  rolloverstyle:'Default',
  blocksize:64,
  networkfilesystem:true,
  wildcard:'client_xref*',
  compressiontype:'gzip',
  includesubdirectories:false,
  directory:'@APPNAME@',
  skipbom:false,
  positionbyeof:false
)
PARSE USING Global.DSVParser 
(
  trimwhitespace:false,
  linenumber: '-1',
  columndelimittill: '-1',
  trimquote: true,
  ignoreemptycolumn: false,
  separator: ':',
  parserName: 'DSVParser',
  quoteset: '\"',
  handler:'com.webaction.proc.DSVParser_1_0',
  charset:'UTF-8',
  ignoremultiplerecordbegin:'true',
  ignorerowdelimiterinquote:false,
  columndelimiter:'|',
  blockascompleterecord:false,
  rowdelimiter:'\n',
  nocolumndelimiter:false,
  headerlineno:0,
  header:'true'
)                  
OUTPUT TO @APPNAME@STREAM;
ALTER APPLICATION @APPNAME@ RECOMPILE;
deploy application @APPNAME@;
start @APPNAME@;

UNDEPLOY APPLICATION admin.BasicAppNoFlow;
DROP APPLICATION admin.BasicAppNoFlow cascade;

CREATE APPLICATION BasicAppNoFlow;

CREATE SOURCE CsvDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt',
  header: Yes,
  columndelimiter: ',',
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE MerchantHourlyAve(
  merchantId	String,
  hourValue int,
  hourlyAve int
);

CREATE STREAM MerchantHourlyStream OF MerchantHourlyAve PARTITION BY merchantId;

CREATE CQ CsvToPosData
INSERT INTO MerchantHourlyStream
SELECT data[1], TO_INT(data[2]),
       TO_INT(data[3])
FROM CsvStream;


END APPLICATION BasicAppNoFlow;

--
-- Recovery Test 26 with two sources, two jumping attribute windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W -> CQ1 -> WS
-- S2 -> Ja6W -> CQ2 -> WS
--

STOP KStreamRecov26Tester.KStreamRecovTest26;
UNDEPLOY APPLICATION KStreamRecov26Tester.KStreamRecovTest26;
DROP APPLICATION KStreamRecov26Tester.KStreamRecovTest26 CASCADE;
DROP USER KStreamRecov26Tester;
DROP NAMESPACE KStreamRecov26Tester CASCADE;
CREATE USER KStreamRecov26Tester IDENTIFIED BY KStreamRecov26Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov26Tester;
CONNECT KStreamRecov26Tester KStreamRecov26Tester;

CREATE APPLICATION KStreamRecovTest26 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION KStreamRecovTest26;

--
-- Recovery Test 37 with two sources, two jumping time windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jt1W/p -> CQ1 -> WS
--   S2 -> Jt2W/p -> CQ2 -> WS
--

STOP Recov37Tester.RecovTest37;
UNDEPLOY APPLICATION Recov37Tester.RecovTest37;
DROP APPLICATION Recov37Tester.RecovTest37 CASCADE;
CREATE APPLICATION RecovTest37 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 1 SECOND
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 2 SECOND
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION RecovTest37;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_source USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 ) 
OUTPUT TO @APPNAME@_Stream ;

CREATE TARGET @APPNAME@_Target USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	BatchPolicy: 'EventCount: 1; Interval: 1s',
	instanceId: 'qatest'
) INPUT FROM @APPNAME@_stream;

create Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_stream;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

create application KinesisTest;
CREATE OR REPLACE SOURCE OS USING OracleReader (
  Username: 'miner',
  Password: 'miner',
  ConnectionURL: '192.168.1.113:1521:ORCL',
  TABLES: 'QATEST.H_REGION;QATEST.H_NATION;QATEST.H_CUSTOMER',
  FetchSize: '1'
 )
OUTPUT TO DDLCDCStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	accesskeyid:'ACCESS_KEY',
	secretaccesskey:'SECRET_KEY'
)
format using JSONFormatter (
)
input from DDLCDCStream;

end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL ;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes1',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource2 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes2',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource3 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes3',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'TEST.user_chkpoint',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: 'QATEST.OracToCql_alldatatypes1,test.oractocq_alldatatypes columnmap(NumericToBigint=NumericToBigint);QATEST.OracToCql_alldatatypes2,test.oractocq_alldatatypes columnmap(NumericToBigint=NumericToBigint);QATEST.OracToCql_alldatatypes3,test.oractocq_alldatatypes columnmap(NumericToBigint=NumericToBigint)',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;


END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@;
CREATE  SOURCE @SourceName@ USING MSSqlReader  ( 
  Username: '@UserName@',
  Password: '@Password@',
  DatabaseName: 'qatest',
  ConnectionURL: '@SourceConnectionURL@',
  Tables: 'qatest.@SourceTable@',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @SRCINPUTSTREAM@;
create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;
CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:1',
  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
 ) 
INPUT FROM @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

CREATE APPLICATION @WRITERAPPNAME@ RECOVERY 1 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

CREATE SOURCE @SOURCE@ USING Ojet
(
FetchSize:1,
Username:'@SOURCE_USER@',
Password:'@pass@',
--Password:'$SRC_PASSWORD',
ConnectionURL:'@CONNECTION_URL@',
Tables:'@SOURCE_TABLE@',
)
OUTPUT TO @STREAM1@;


end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

CREATE OR REPLACE TYPE @TYPE@( 
datae java.util.HashMap , 
TABLE_NAME java.lang.String , 
OPS_NAME java.lang.String , 
DB_TIMESTAMP java.lang.String  ,
COMMITSCN java.lang.String ,
SCN java.lang.String ,
REC_INS_TIME java.lang.String );

CREATE CQ @CQ1@
INSERT INTO @STREAM2@
SELECT  
CASE WHEN (META(c,"OperationName").toString() == "DELETE")
THEN putUserData(c, 'isDelete', 'true') 
ELSE
putUserData(c,'isDelete', 'false')
END
FROM @STREAM1@ c;

CREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;

CREATE OR REPLACE CQ @CQ2@ 
INSERT INTO @STREAM3@
SELECT 
data(e),
META(e,"TableName").toString() as TABLE_NAME,
META(e, "OperationName").toString() as OPS_NAME,
META(e, "TimeStamp").toString() as DB_TIMESTAMP,
META(e,"COMMITSCN").toString() as COMMITSCN ,
META(e,"SCN").toString() as  SCN ,
DNOW().toString() as REC_INS_TIME
FROM @STREAM1@ e;

create Target @TARGET1@ using KafkaWriter VERSION @kafakaversion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC1',
ParallelThreads:'',
PartitionKey:'@metadata(TableName)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from @STREAM1@;


create Target @TARGET2@ using KafkaWriter VERSION @kafakaversion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC2',
ParallelThreads:'2',
PartitionKey:'TABLE_NAME',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from @STREAM3@;

create Target @TARGET3@ using KafkaWriter VERSION @kafakaversion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC3',
ParallelThreads:'',
PartitionKey:'@userdata(isDelete)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest.avsc')
input from @STREAM2@;

end application @WRITERAPPNAME@;

CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafakaversion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC1',
        startOffset:0       
)
PARSE USING DSVParser ()
OUTPUT TO KafkaReaderStream1;

CREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafakaversion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC2',
        startOffset:0          
)
PARSE USING JSONParser ()
OUTPUT TO KafkaReaderStream2;

CREATE TARGET kafkaDumpJSON USING FileWriter(
filename:'@READERAPPNAME@_RT_JSON')
FORMAT USING JSONFormatter()
INPUT FROM KafkaReaderStream2;

CREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafakaversion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC3',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest.avsc'
)
OUTPUT TO KafkaReaderStream3;

CREATE TARGET kafkaDumpAVRO USING FileWriter(
filename:'@READERAPPNAME@_RT_AVRO')
FORMAT USING AvroFormatter(
    schemaFileName:'@avro_schema_file@'
)
INPUT FROM KafkaReaderStream3;

end flow @APPNAME@_serverflow;
end application @READERAPPNAME@;

STOP APPLICATION KafkaWPTester.KWApp;
STOP APPLICATION KafkaWPTester.KRApp;
UNDEPLOY APPLICATION KafkaWPTester.KWApp;
UNDEPLOY APPLICATION KafkaWPTester.KRApp;
DROP APPLICATION KafkaWPTester.KWApp CASCADE;
DROP APPLICATION KafkaWPTester.KRApp CASCADE;

CREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;
GRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;
CONNECT KafkaWPTester KafkaWPTester;


CREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE CSVSource USING FileReader (
	directory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',
    WildCard:'access_log_half',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;

CREATE TYPE AccessLogType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;

CREATE CQ AceeslogCQ
INSERT INTO TypedAccessLogStream
SELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]
FROM FileStream;

create Target DsvWriter using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V10dsvExistTopic',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream;


create Target JsonWriter using KafkaWriter VERSION '0.10.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V10jsonExistTopic',
	Mode:'Sync'
    )
 
FORMAT USING JSONFormatter ()
input from TypedAccessLogStream;


create Target AvroWriter using KafkaWriter VERSION '0.10.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V10avroExistTopic',
    Mode:'Sync'
    )
 
FORMAT USING AvroFormatter (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')
input from TypedAccessLogStream;


END APPLICATION KWApp;

DEPLOY APPLICATION KWApp on any in default;


-- Kafka Reader Apps

CREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;


CREATE TYPE AccessLogType2(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE Stream KafkaDSVReaderStream of AccessLogType2;
CREATE Stream KafkaJsonReaderStream of AccessLogType2;
CREATE Stream KafkaAvroReaderStream of AccessLogType2;


CREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V10dsvExistTopic',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;




CREATE CQ DSVtoKafkaDSVReaderStream
INSERT INTO KafkaDSVReaderStream
SELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]
FROM KafkaReaderStream1;

CREATE TARGET DSVDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V10_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaDSVReaderStream;

CREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V10jsonExistTopic',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;




CREATE CQ DSVtoKafkaJsonReaderStream
INSERT INTO KafkaJsonReaderStream
SELECT 
data.get('Col1').textValue(),
data.get('Col2').textValue(),
data.get('Col3').textValue(),
data.get('Col4').textValue(),
data.get('Col5').textValue(),
data.get('Col6').textValue(),
data.get('Col7').textValue(),
data.get('Col8').textValue(),
data.get('Col9').textValue(),
data.get('Col10').textValue()
FROM KafkaReaderStream2;

CREATE TARGET JSONDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V10_RT_JSON')
FORMAT USING DSVFormatter()
INPUT FROM KafkaJsonReaderStream;

CREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V10avroExistTopic',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'
)
OUTPUT TO KafkaReaderStream3;



CREATE Stream KRTypedStream of AccessLogType;

CREATE CQ DSVtoKafkaAvroReaderStream
INSERT INTO KafkaAvroReaderStream
SELECT    
data.get("Col1").toString(), 
data.get("Col2").toString(), 
data.get("Col3").toString(), 
data.get("Col4").toString(),
data.get("Col5").toString(), 
data.get("Col6").toString(), 
data.get("Col7").toString(), 
data.get("Col8").toString(),
data.get("Col9").toString(), 
data.get("Col10").toString()
FROM KafkaReaderStream3;

CREATE TARGET AVRODump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V10_RT_AVRO')
FORMAT USING DSVFormatter()
INPUT FROM KafkaAvroReaderStream;

END APPLICATION KRApp;
DEPLOY APPLICATION KRApp on any in default;

stop httpjsonapp;
undeploy application httpjsonapp;
drop APPLICATION httpjsonapp cascade;
create application httpjsonapp;
create source HTTPSource using HTTPReader (
        IpAddress:'127.0.0.1',
        PortNo:'10001'
) OUTPUT TO HttpDataStream;

create Target HttpDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/httpjsondata') input from HttpDataStream;
end application httpjsonapp;

-----------------------------------
stop application SourceAgentApp;
undeploy application SourceAgentApp;

stop application TargetServerApp1;
undeploy application TargetServerApp1;

stop application TargetServerApp2;
undeploy application TargetServerApp2;

stop application TargetServerApp3;
undeploy application TargetServerApp3;

drop application SourceAgentApp cascade;
drop application TargetServerApp1 cascade;
drop application TargetServerApp2 cascade;
drop application TargetServerApp3 cascade;


CREATE APPLICATION SourceAgentApp;

create flow flow1;
create source CSVSource using FileReader (
directory: '@TEST-DATA-PATH@/tmp',
WildCard:'mybanks*',
positionByEOF: true,
charset:'UTF-8'
) parse using DSVParser (header:'no')
OUTPUT TO CsvStream;
end flow flow1;

--CREATE TARGET T USING Sysout(name:'sysout1') INPUT FROM CsvStream;

END APPLICATION SourceAgentApp;

DEPLOY APPLICATION SourceAgentApp with flow1 in AGENTS;

-- Fisrt app consuming from app running in agent
CREATE APPLICATION TargetServerApp1;
create flow flow2;

CREATE TARGET T2 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp1_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')
FORMAT USING JSONFormatter ()
INPUT FROM CsvStream;
end flow flow2;

END APPLICATION TargetServerApp1;
deploy application TargetServerApp1 with flow2 in default;


-- another app consuming from app running in agent

CREATE APPLICATION TargetServerApp2;
create flow flow3;

CREATE TARGET T3 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp2_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')
FORMAT USING JSONFormatter ()
INPUT FROM CsvStream;
end flow flow3;

END APPLICATION TargetServerApp2;
deploy application TargetServerApp2 with flow3 in default;

-- another app consuming from app running in agent. This app will be deployed and started after other apps started.

CREATE APPLICATION TargetServerApp3;
create flow flow4;

CREATE TARGET T4 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp3_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')
 FORMAT USING JSONFormatter ()
 INPUT FROM CsvStream;
end flow flow4;

END APPLICATION TargetServerApp3;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
wildcard: '',
directory: '',
positionbyeof: false )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: ''
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 1 second interval;

create flow agentflow;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@'
  --recordSelector: 'ARDPLKBX-RECORD:ARDPLKBX-RECORD-TYPE=ARDPLKBX-RECORD'
  --recordSelector: 'OH:MOH-SEG-ID=OH, OH2:OH2-SEG-ID=OH2, OHU:MOH-SEG-ID=OHU, OR1:OR1-SEG-ID=OR1, OR2:OR2-SEG-ID=OR2, OR3:OR3-SEG-ID=OR3, OR4:OR4-SEG-ID=OR4, OHM:OHM-SEG-ID=OHM, OD:OD-SEG-ID=OD, ODU:ODU-SEG-ID=ODU, OD1:OD1-SEG-ID=OD1, ODM:ODM-SEG-ID=ODM, OT:OT-SEG-ID=OT'
)
OUTPUT TO @APPNAME@Stream;

end flow agentflow;

create flow serverflow;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;


CREATE TYPE test_typeRe 
(
node_new com.fasterxml.jackson.databind.JsonNode,
node_name com.fasterxml.jackson.databind.JsonNode,
node_addr com.fasterxml.jackson.databind.JsonNode
);

Create stream cqAsJSONNodeStreamRe of test_typeRe;

CREATE CQ GetPOAsJsonNodesRe
INSERT into cqAsJSONNodeStreamRe
select 
data.get('ACCTS-RECORD'),
data.get('ACCTS-RECORD').get('NAME'),
data.get('ACCTS-RECORD').get('ADDRESS3')
from @APPNAME@Stream js;

create type finaldtypeRe
(ACCOUNT_NO int,
FIRST_NAME String,
LAST_NAME String,
ADDRESS1 String,
ADDRESS2 String,
CITY String,
STATE String,
ZIP_CODE int);

CREATE STREAM getdataStreamPS OF finaldtypeRe;

CREATE CQ getdataRe
INSERT into getdataStreamPS
select JSONGetInteger(x.node_new,"ACCOUNT-NO"),
JSONGetString(x.node_name,"FIRST-NAME"),
JSONGetString(x.node_name,"LAST-NAME"),
JSONGetString(x.node_new,"ADDRESS1"),
JSONGetString(x.node_new,"ADDRESS2"),
JSONGetString(x.node_addr,"CITY"),
JSONGetString(x.node_addr,"STATE"),
JSONGetInteger(x.node_addr,"ZIP-CODE")
from cqAsJSONNodeStreamRe x;

create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:1000,Interval:50',
  CommitPolicy: 'EventCount:1000,Interval:50',
  Tables: 'QATEST.@table@'
)
input from getdataStreamPS;
end flow serverflow;

end application @APPNAME@;
deploy application @APPNAME@ with agentflow on any in agents,serverflow in default; 
start application @APPNAME@;

stop application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
	directory:'@TEST-DATA-PATH@',
        WildCard:'banks.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'no'
)
OUTPUT TO CsvStream;

CREATE TARGET DBDump USING FileWriter(
filename:'@FEATURE-DIR@/logs/JsonRaw_RT.log', rolloverpolicy:'EventCount:10000,Interval:30s' )Format using JSONFormatter()
INPUT FROM CsvStream;
end application DSV;

Stop @APPNAME@_App;
undeploy application @APPNAME@_App;
drop application @APPNAME@_App cascade;

CREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;
CREATE SOURCE @APPNAME@_Source USING Global.OracleReader

(
  FetchSize:'1',
  Username:'@SOURCE_USER@',
  Password:'@SOURCE_PASSWORD@',
  ConnectionURL:'@SOURCE_URL@',
  Tables:'@SOURCE_TABLES@'
)
OUTPUT TO @APPNAME@_Stream;

CREATE TARGET @APPNAME@_SYS USING SysOut (
  name: '@APPNAME@_SYS' )
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_CDB_Target USING DatabaseWriter

(
  Username:'@TARGET_CDB_USER@',
  ConnectionURL:'@TARGET_CDB_URL@',
  Tables:'@TARGET_CDB_TABLES@',
  Password:'@TARGET_CDB_PASSWORD@',
  BatchPolicy:'EventCount:1,Interval:1',
  CommitPolicy:'EventCount:1,Interval:1'
)
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_PDB_Target USING DatabaseWriter

(
  Username:'@TARGET_PDB_USER@',
  ConnectionURL:'@TARGET_PDB_URL@',
  Tables:'@TARGET_PDB_TABLES@',
  Password:'@TARGET_PDB_PASSWORD@',
  BatchPolicy:'EventCount:1,Interval:1',
  CommitPolicy:'EventCount:1,Interval:1'
)
INPUT FROM @APPNAME@_Stream;

End APPLICATION @APPNAME@_App;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ recovery 5 second Interval;

CREATE source @srcName@ USING Global.OracleReader ( 
 Username:'@srcusername@',
  Password:'@srcpassword@',
  ConnectionURL:'@srcurl@',
  Tables:'@srcschema@.@srctable@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries:'true'
  ) 
OUTPUT TO @outstreamname@;


CREATE OR REPLACE TARGET @tgtName@ USING Global.SalesforceWriter ( 
  autoAuthTokenRenewal: 'true', 
  sObjects: '@srcschema@.@srctable@,@tgtobject@ COLUMNMAP(num__c=a,Name=b)', 
  useConnectionProfile: 'false', 
  consumerSecret: '@tgtconsumersecret@', 
  JWTKeystorePath: '', 
  BatchPolicy:'EventCount:1,Interval:10s',
  CommitPolicy:'EventCount:1,Interval:10s', 
  Mode: 'APPENDONLY', 
  consumerKey: '@tgtconsumerkey@', 
  apiEndPoint: '@tgtapiurl@', 
  InMemory: 'true', 
  FieldDelimeter: 'COMMA', 
  ApplicationErrorCountThreshold: '0', 
  useBulkApi: 'true', 
  OAuthAuthorizationFlows: 'PASSWORD', 
  JWTCertificateName: '', 
  hardDelete: 'false', 
  Username: '@tgtusername@', 
  useQuotes: 'false', 
  adapterName: 'SalesforceWriter', 
  Password: '@tgtpassword@', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  securityToken: '@tgtsecuritytoken@', 
  JWTKeystorePassword: '' ) 
INPUT FROM @instreamname@;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'smallposdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
	filename:'TargetPosDataXmlFSDefault',
	directory:'@FEATURE-DIR@/logs/',
    sequence:'00',
	rolloverpolicy:'FileSizeRollingPolicy'
)
format using XMLFormatter (
	rootelement:'document',
	elementtuple:'MerchantName:zip:text=merchantname'
)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlDFS_actual.log') input from TypedCSVStream;

end application DSV;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING MySqlReader
(
  Compression: false,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'mysql://localhost:3306',
  DatabaseName: 'waction',
  Tables: 'waction.test01',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root',
  connectionRetryPolicy:'retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'waction.test01,@DATASET@.% KEYCOLUMNS(col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;

--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @SOURCE@ USING Ojet  ( 
  FilterTransactionBoundaries: true,
  ConnectionURL: '@OCI-URL@',
  Tables: '@SOURCE_TABLE@',
  Password: '@OJET-PASSWORD@',
  fetchsize: 1,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Username: '@OJET-UNAME@'
 ) 
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@1 USING BigQueryWriter ( 
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  streamingUpload: 'false', 
  BatchPolicy: 'eventCount:1000000, Interval:90', 
  Tables: 'sample.tab', 
  ServiceAccountKey: '', 
  projectId: 'striimqa-214712', 
  Encoding: 'UTF-8', 
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30', 
  AllowQuotedNewLines: 'false', 
  CDDLAction: 'Process', 
  optimizedMerge: 'false', 
  TransportOptions: 'connectionTimeout=300, readTimeout=120', 
  Mode: 'MERGE', 
  StandardSQL: 'true', 
  includeInsertId: 'true', 
  QuoteCharacter: '\"' ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@2 USING BigQueryWriter ( 
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  streamingUpload: 'false', 
  BatchPolicy: 'eventCount:1000000, Interval:90', 
  Tables: 'sample.tab', 
  ServiceAccountKey: '', 
  projectId: 'striimqa-214712', 
  Encoding: 'UTF-8', 
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30', 
  AllowQuotedNewLines: 'false', 
  CDDLAction: 'Process', 
  optimizedMerge: 'true', 
  TransportOptions: 'connectionTimeout=300, readTimeout=120', 
  Mode: 'MERGE', 
  StandardSQL: 'true', 
  includeInsertId: 'true', 
  QuoteCharacter: '\"' ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@3 USING BigQueryWriter ( 
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  streamingUpload: 'false', 
  BatchPolicy: 'eventCount:1000000, Interval:90', 
  Tables: 'sample.tab', 
  ServiceAccountKey: '', 
  projectId: 'striimqa-214712', 
  Encoding: 'UTF-8', 
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30', 
  AllowQuotedNewLines: 'false', 
  CDDLAction: 'Process', 
  optimizedMerge: 'false', 
  TransportOptions: 'connectionTimeout=300, readTimeout=120', 
  Mode: 'APPENDONLY', 
  StandardSQL: 'true', 
  includeInsertId: 'true', 
  QuoteCharacter: '\"' ) 
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

STOP APPLICATION @APPNAME@_app;
UNDEPLOY APPLICATION @APPNAME@_app;
DROP APPLICATION @APPNAME@_app CASCADE;
-- DROP EXCEPTIONSTORE @APPNAME@_exceptionstore;

CREATE APPLICATION @APPNAME@_app RECOVERY 120 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @APPNAME@_Source USING @SOURCE_ADAPTER@  (
  Username:'qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: '',
  ) OUTPUT TO @APPNAME@_Stream  ;

CREATE OR REPLACE TARGET @APPNAME@_Target USING BigQueryWriter  (
  Tables                        : '',
  projectId                    : '',
  ServiceAccountKey            : '',
  Mode                         : 'APPENDONLY',
  BatchPolicy                  : 'EventCount:1, Interval:60',
  ) INPUT FROM @APPNAME@_Stream;

-- CREATE OR REPLACE TARGET @APPNAME@_SysOut USING Global.SysOut (name: 'wa') INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@_app;
DEPLOY APPLICATION @APPNAME@_app;
START APPLICATION @APPNAME@_app;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@;

CREATE SOURCE @SOURCE_NAME@2 USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@2;

STOP APPLICATION orrs;
UNDEPLOY APPLICATION orrs;
DROP APPLICATION orrs CASCADE;
CREATE APPLICATION orrs;
Create Source oraSource Using DatabaseReader
(
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'QATEST.ORACLETOREDSHIFTIL1;QATEST.ORACLETOREDSHIFTIL2',
 FilterTransactionBoundaries:true,
 FetchSize:1000
) Output To LCRStream;

CREATE TARGET RSTarget USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: 'access_key',
	  --secretaccesskey: 'secret_access',
	  S3IAMRole:'@IAMROLE@',
	  Tables: 'QATEST.ORACLETOREDSHIFT%,QATEST.ORACLETOREDSHIFT%',
	  uploadpolicy:'eventcount:1000,interval:1m'
	) INPUT FROM LCRStream;
	
END APPLICATION orrs;
DEPLOY APPLICATION orrs;
START APPLICATION orrs;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: '@ORACLE-URL@',
  Tables: '@SOURCE-TABLES@',
  Username: '@ORACLE-USERNAME@',
  Password: '@ORACLE-PASSWORD@'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt USING SnowflakeWriter

(
  ConnectionURL:'@SNOWFLAKE-URL@',
  username:'@SNOWFLAKE-USERNAME@',
  appendOnly:'false',
  Tables:'@TARGET-TABLES@',
  uploadpolicy:'eventcount:3,interval:10s',
  externalStageType:'local'
)
INPUT FROM @APP_NAME@_Stream;


END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE FLOW @APP_NAME@SrcFlow;

CREATE OR REPLACE SOURCE @APP_NAME@_src USING FileReader (
directory:'@DIRECTORY@',
WildCard:'posdata5L.csv',
positionByEOF:false
)
parse using DSVParser (
header:'no'
)
OUTPUT TO @APP_NAME@_Stream;
END FLOW @APP_NAME@SrcFlow;

CREATE FLOW @APP_NAME@TgtFlow;

CREATE OR REPLACE TYPE @APP_NAME@_Type  ( BUSINESS_NAME java.lang.String KEY,
MERCHANT_ID java.lang.String,
PRIMARY_ACCOUNT_NUMBER java.lang.String
 ) ;

CREATE OR REPLACE STREAM @APP_NAME@_Stream2 OF @APP_NAME@_Type;
CREATE OR REPLACE CQ @APP_NAME@_CQ
INSERT INTO @APP_NAME@_Stream2
SELECT data[0],data[1],data[2]
FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:50000,interval:50s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream2;

END FLOW @APP_NAME@TgtFlow;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@ WITH @APP_NAME@SrcFlow IN agents,@APP_NAME@TgtFlow IN default;
START APPLICATION @APP_NAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

--CREATE APPLICATION @APPNAME@;
create application @APPNAME@ Recovery 5 second Interval;

--create or replace flow @APPNAME@_agentflow;

CREATE OR REPLACE SOURCE @APPNAME@_source USING OracleReader  (
  Compression: false,
  StartTimestamp: 'null',
  SupportPDB: false,
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  FetchSize: 1,
  DDLCaptureMode: 'All',
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'localhost:1521:xe',
 Tables: 'QATEST.Source1',
-- Tables: 'QATEST.OracToCql_alldatatypes',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  TransactionBufferType: 'Memory',
  DictionaryMode: 'OnlineCatalog',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '1MB'
 )
OUTPUT TO @APPNAME@_stream ;

--end flow @APPNAME@_@APPNAME@_agentflow;

CREATE OR REPLACE TARGET @APPNAME@_target USING CassandraCosmosDBWriter  (
  AccountEndpoint: 'cassandracosmostest.cassandra.cosmos.azure.com',
  AccountKey: 'pqDZvVgbdSCg7VzIzD77dAhPG2odGRZPLhAQA1qnZbAKoIDk6RuQX5r2phbRQFnR1l54qxOcvBXNdz8DeijYIg==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  Tables: 'QATEST.Source1,test.target1',
  adapterName: 'CassandraCosmosDBWriter'
 )
 INPUT FROM @APPNAME@_stream;

 END APPLICATION @APPNAME@;

deploy application @APPNAME@;
 --deploy application @APPNAME@ with agentflow in agents;
 start application @APPNAME@;

--
-- Recovery Test 34 with two sources, two sliding time-count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5a9W/p  -> CQ1 -> WS
-- S2 -> Sc6a11W/p -> CQ2 -> WS
--

STOP KStreamRecov3Tester.KStreamRecovTest34;
UNDEPLOY APPLICATION KStreamRecov3Tester.KStreamRecovTest34;
DROP APPLICATION KStreamRecov34Tester.KStreamRecovTest34 CASCADE;

DROP USER KStreamRecov34Tester;
DROP NAMESPACE KStreamRecov34Tester CASCADE;
CREATE USER KStreamRecov34Tester IDENTIFIED BY KStreamRecov34Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov34Tester;
CONNECT KStreamRecov34Tester KStreamRecov34Tester;

CREATE APPLICATION KStreamRecovTest34 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION KStreamRecovTest34;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'TargetPosDataXmlTI',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:merchantid:text=merchantname'
)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlTI_actual.log') input from TypedCSVStream;

end application DSV;

stop OracleReaderToDBWriter;
undeploy application OracleReaderToDBWriter;
drop application OracleReaderToDBWriter cascade;

CREATE APPLICATION  OracleReaderToDBWriter RECOVERY 1 MINUTE INTERVAL AUTORESUME MAXRETRIES 2 RETRYINTERVAL 60;

CREATE FLOW Hz_Agent_flow;

Create Source Oraclesrc
 Using OracleReader
(
 Username:'',
 Password:'',
 ConnectionURL:'',
 Tables:'',
 Fetchsize:1
)
Output To OrcStrm;

END FLOW Hz_Agent_flow;


CREATE TARGET OracleTrg USING DatabaseWriter(
ConnectionURL:'',
  Username:'',
  Password:'',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: ''
) 
INPUT FROM OrcStrm;

END APPLICATION OracleReaderToDBWriter;
DEPLOY APPLICATION OracleReaderToDBWriter with Hz_Agent_flow on any in AGENTS;
start application OracleReaderToDBWriter;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING MySqlReader
(
  Compression: false,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'mysql://localhost:3306',
  DatabaseName: 'waction',
  Tables: 'waction.test01',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root',
  connectionRetryPolicy:'retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'waction.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;

--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

stop application JMSWriter.JMS;
undeploy application JMSWriter.JMS;
drop application JMSWriter.JMS cascade;

create application JMS;
create source JMSCSVSource using FileReader (
        directory:'@TEST-DATA-PATH@',
        WildCard:'AdhocQueryData2.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target JmsTarget  using JMSWriter (
		Provider:'@PROVIDER@',
		Ctx:'@CONTEXT@',
		messagetype: @MESSAGETYPE@,
		UserName:'@USERNAME@',
		Password:'@PASSWORD@',
		Queuename:'dynamicQueues/Test.bar')
format using dsvformatter (
)
input from TypedCSVStream;

end Application Jms;
deploy application jms;

start jms;

stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@;

CREATE SOURCE @SourceName@ USING MSSQLReader  ( 
ReaderType: 'LogMiner', 
  Password_encrypted: 'false', 
  DatabaseName: 'qatest',
  SupportPDB: false, 
  QuiesceMarkerTable: 'QUIESCEMARKER', 
  QueueSize: 2048, 
  CommittedTransactions: true, 
  Username: '@UserName@', 
  TransactionBufferType: 'Memory', 
  TransactionBufferDiskLocation: '.striim/LargeBuffer', 
  OutboundServerProcessName: 'WebActionXStream', 
  Password: '@Password@', 
  DDLCaptureMode: 'All', 
  Compression: false, 
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', 
  FetchSize: 1, 
  Tables: '@SourceTables@', 
  DictionaryMode: 'OnlineCatalog', 
  XstreamTimeOut: 600, 
  TransactionBufferSpilloverSize: '1MB', 
  StartTimestamp: 'null', 
  FilterTransactionBoundaries: true, 
  StartSCN: 'null', 
  ConnectionURL: '@ConnectionURL@', 
  SendBeforeImage: true ) 
OUTPUT TO @AppStream@  ;

CREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream SELECT data[2], to_string(to_date(data[2]), "dd-MMM-yy hh.mm.ss") FROM @AppStream@ o ;

CREATE  TARGET @targetsys@ USING Global.SysOut  ( 
name: 'ora1_sys' ) 
INPUT FROM admin.ZDT_cq_stream;

create Target @TargetFile@ using FileWriter(
  filename:'toStringOut.log',
  directory:'@FilePath@',
  rolloverpolicy:'eventcount:1000'
)
format using DSVFormatter (

)
input from admin.ZDT_cq_stream;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop Postgres_SQLDBWHEventTableApp;
undeploy application Postgres_SQLDBWHEventTableApp;
drop application Postgres_SQLDBWHEventTableApp cascade;
CREATE APPLICATION Postgres_SQLDBWHEventTableApp;

CREATE OR REPLACE SOURCE Postgres_Src1 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgres_src1',
  ExcludedTables: ''
 ) 
OUTPUT TO data_stream1;

CREATE OR REPLACE SOURCE Postgres_Src2 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgres_src2',
  ExcludedTables: ''
 ) 
OUTPUT TO data_stream2;

CREATE OR REPLACE SOURCE Postgres_Src3 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgres_src2',
  ExcludedTables: ''
 ) 
OUTPUT TO data_stream3;

Create Type EventType (
ID int,
NAME string,
COMPANY string
);

CREATE STREAM insertData1  of EventType;
CREATE STREAM deleteData1 of EventType;
CREATE STREAM joinData1 of EventType;
CREATE STREAM joinData2 of EventType;
CREATE STREAM deleteData2 of EventType;
CREATE STREAM OutStream of EventType;

CREATE CQ cq1 INSERT INTO insertData1  SELECT TO_INT(data[0]),data[1],data[2] FROM data_stream1;

CREATE CQ cq2 INSERT INTO deleteData1 SELECT TO_INT(data[0]),data[1],data[2] FROM data_stream2;

CREATE CQ cq3 INSERT INTO joinData1 SELECT TO_INT(data[0]),data[1],data[2] FROM data_stream3;

CREATE JUMPING WINDOW DataWin1 OVER deleteData1 KEEP 1 ROWS;

CREATE CQ TypedEventKeyCQ6 INSERT INTO deleteData2 SELECT * from DataWin1;

CREATE JUMPING WINDOW DataWin2 OVER joinData1 KEEP 1 ROWS;

CREATE CQ TypedEventKeyCQ5 INSERT INTO joinData2 SELECT * from DataWin2;

CREATE EVENTTABLE ETABLE1 using STREAM ( NAME: 'insertData1 ' )
--DELETE using STREAM ( NAME: 'deleteData1')
QUERY (keytomap:"ID", persistPolicy: 'true') OF EventType;

CREATE CQ cq4 INSERT INTO OutStream SELECT B.ID,B.NAME,B.COMPANY FROM joinData2 A, ETABLE1 B where A.ID=B.ID;

CREATE TARGET EventTableFW USING FileWriter
(filename:'BasicPostgres_SQLDBWHEventTableApp_RT.log',
 rolloverpolicy: 'EventCount:1000000')
FORMAT USING DSVFormatter () INPUT FROM OutStream;

create target Target_Azure using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'STRIIM',
        password: 'W3b@ct10n',
        AccountName: 'striimqatestdonotdelete',
        accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables:'dbo.autotest01',
        uploadpolicy:'eventcount:0,interval:0s'
) INPUT FROM OutStream;

END APPLICATION Postgres_SQLDBWHEventTableApp;
deploy application Postgres_SQLDBWHEventTableApp in default;
start Postgres_SQLDBWHEventTableApp;

Stop Oracle_LogWriter;
Undeploy application Oracle_LogWriter;
drop application Oracle_LogWriter cascade;

CREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;

CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  (

  FetchSize: 1000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=t1',
 startPosition: '%=-1',
  PollingInterval: '2sec',
  ConnectionPoolSize: 5,
  ThreadPoolSize: 5
  )
  OUTPUT TO data_stream;
create target AzureSQLDWHTarget using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:1000,interval:300s'
) INPUT FROM data_stream;
  CREATE OR REPLACE TARGET TeraSys USING SysOut  (
  name: 'ora12_out'
 ) INPUT FROM data_stream;

END APPLICATION Oracle_IRLogWriter;
deploy application Oracle_IRLogWriter;
start Oracle_IRLogWriter;

/* *******************************************************************
 Ibasis App for monitoring network with pings to different locations 
****************************************************************** */

use global;
drop namespace PingMonitorBatchApp cascade;
create namespace PingMonitorBatchApp;
use PingMonitorBatchApp;

CREATE APPLICATION PingMonitorBatchApp;

create	flow PingMonitorBatchAppFlow;	

CREATE TYPE PingDateEntry(
    MaxTestDateTime  DateTime KEY
 );


CREATE TYPE PINGEVENT(
        SERVICE_NAME  String,
        TEST_DATETIME DateTime,
	SOURCE_VERIFIER_ALIAS String,
	TARGET_VERIFIER_ALIAS  String,
        DELAY_AVG float,
	PACKET_LOST float,
        JITTER_AVG float,
        SEQ_NO     long
);


CREATE CACHE PINGEVENTCACHE  using DatabaseReader (
   ConnectionURL:'jdbc:oracle:thin:@server1204v.ivanet.net:1960:prtld',
   Username:'xtract',
   Password:'xtract123',
   Query:'select SERVICE_NAME,TEST_DATETIME,SOURCE_VERIFIER_ALIAS,TARGET_VERIFIER_ALIAS,NVL(DELAY_AVG,0),NVL(PACKET_LOST,0),NVL(JITTER_AVG,0),SEQ_NO  from ir34_ping_site where TEST_DATETIME > (select NVL(to_date(\'1970-01-01\', \'YYYY-MM-DD\') + (MAX(MaxTestDateTime)/ 86400000), sysdate -10/24) from IR34SiteLastDate)order by TEST_DATETIME ASC'
 ) QUERY (keytomap:'SOURCE_VERIFIER_ALIAS', refreshinterval: '60 second', publishonrefresh: 'true') OF PINGEVENT;


create Target trace0 using CSVWriter (filename:'logs/PINGEVENTCACHE.csv') input from PINGEVENTCACHE;

CREATE WACTIONSTORE IR34ElasticSearchWS  
CONTEXT OF   PingDateEntry 
EVENT TYPES ( PingDateEntry  )
PERSIST IMMEDIATE
USING ( storageProvider: "elasticsearch" );


CREATE WACTIONSTORE IR34SiteLastDateWS CONTEXT OF PingDateEntry
 		EVENT TYPES ( PingDateEntry )
 	    PERSIST EVERY 50 second USING (
 	    JDBC_DRIVER:'oracle.jdbc.driver.OracleDriver',
 	    JDBC_URL:'jdbc:oracle:thin:@server1204v.ivanet.net:1960:prtld',
 	    JDBC_USER:'xtract',
 	    JDBC_PASSWORD:'xtract123',
 	    DDL_GENERATION:'create-or-extend-tables',
            CONTEXT_TABLE:'IR34SiteLastDate'
 	   );	
	   

CREATE CQ populateMaxDate
 INSERT INTO IR34SiteLastDateWS
 select TO_DATE(MAX(TO_LONG(TEST_DATETIME)))
 from  PINGEVENTCACHE
 , heartbeat(interval '20' second, 1 , 1) hb;


 
/* create Target trace1 using CSVWriter (filename:'logs/IR34SiteLastDateWS.csv') input from IR34SiteLastDateWS; */


CREATE TYPE DELAYTHRESHTYPE(
        SITE_KEY  STRING,
        SOURCE_SITE STRING,
        TARGET_SITE STRING,
        DELAY_THRESH FLOAT,
        PROFILE_ID   INT
);


CREATE CACHE DELAYTHRESHCACHE  USING DATABASEREADER (
   ConnectionURL:'jdbc:oracle:thin:@timstsnap05.ivanet.net:1521:timsrdg2',
   Username:'network_cfg',
   Password:'network_cfg',
   QUERY: 'select src.site_abbreviation || \'-\' || tgt.site_abbreviation as SITE_KEY, src.site_abbreviation as SOURCE_SITE, tgt.site_abbreviation as TARGET_SITE,D.DELAY as DELAY_THRESH, d.PROFILE_ID from site_attributes src, geography gsrc, threshold_delay d, site_attributes tgt, geography gtgt where  Src.GEO_ABBREVIATION = Gsrc.GEO_ABBREVIATION  and src.GEO_ABBREVIATION = D.SOURCE_VERIFIER_GEO  and tgt.GEO_ABBREVIATION = D.TARGET_VERIFIER_GEO  and tgt.GEO_ABBREVIATION = Gtgt.GEO_ABBREVIATION  and tgt.PROFILE_ID = D.PROFILE_ID  and d.profile_id = 1'
 ) QUERY (KEYTOMAP:'SITE_KEY', REFRESHINTERVAL: '60 SECOND', publishonrefresh: 'true') OF DELAYTHRESHTYPE;


CREATE TARGET TRACE2 USING CSVWRITER (FILENAME:'logs/DELAYTHRESHCACHE.CSV') INPUT FROM DELAYTHRESHCACHE;


CREATE TYPE ALERTENTRY(
	ALERTKEY STRING KEY,
 	TOLOC STRING,
 	FROMLOC STRING,	
        STATUS STRING,
	INTSTATUS INT,
	ACTUAL_DELAY INT,
	DELAY_THRESHOLD INT,
        TESTTIME DATETIME
 );

CREATE STREAM PingStreamRaw OF PINGEVENT;

CREATE CQ PROCESSSortCQ
 INSERT INTO PingStreamRaw
 select  SERVICE_NAME  ,
        TEST_DATETIME ,
        SOURCE_VERIFIER_ALIAS ,
        TARGET_VERIFIER_ALIAS  ,
        DELAY_AVG ,
        PACKET_LOST ,
        JITTER_AVG ,
        SEQ_NO     
from PINGEVENTCACHE,  heartbeat(interval '30' second) hb;

CREATE SORTER PingSorterStream OVER
PingStreamRaw ON TEST_DATETIME OUTPUT TO SortedPingStream
WITHIN 70 second  
OUTPUT ERRORS TO fooErrorStream;


 CREATE STREAM ALERTENTRYSTREAM OF ALERTENTRY;

 CREATE CQ PROCESSCQ1
 INSERT INTO ALERTENTRYSTREAM
 SELECT DW.SOURCE_VERIFIER_ALIAS + " - " + DW.TARGET_VERIFIER_ALIAS,
        DW.TARGET_VERIFIER_ALIAS, 
        DW.SOURCE_VERIFIER_ALIAS,         
        CASE WHEN DW.DELAY_AVG > DT.DELAY_THRESH THEN "WARNING" 
             ELSE "NORMAL" 
        END,
        CASE WHEN DW.DELAY_AVG > DT.DELAY_THRESH  THEN 2
             ELSE 1
        END,
        DW.DELAY_AVG,
        DT.DELAY_THRESH,
        DW.TEST_DATETIME
 FROM SortedPingStream DW  , DELAYTHRESHCACHE DT 
 WHERE DT.SOURCE_SITE = DW.SOURCE_VERIFIER_ALIAS
   AND DT.TARGET_SITE = DW.TARGET_VERIFIER_ALIAS;

CREATE TARGET TRACE4 USING CSVWRITER (FILENAME:'logs/ALERTENTRIES.CSV') INPUT FROM ALERTENTRYSTREAM;

CREATE WINDOW DELAYWARNWINDOW OVER  ALERTENTRYSTREAM KEEP WITHIN 20 minute ON TESTTIME;

CREATE TYPE ALERTAGGENTRY(
        ALERTKEY STRING KEY,
        ALERT_STATUS STRING,
        NUMBER_OF_VIOLATIONS INT,
        DELAY_THRESHOLD INT,
        FIRSTTIME DATETIME
 );



CREATE STREAM ALERTAGGSTREAM OF ALERTAGGENTRY;


CREATE CQ PROCAGGCQ1 
 	INSERT INTO ALERTAGGSTREAM
		SELECT  ALERTKEY,
                        CASE WHEN COUNT(*) > 5 THEN 'CRITICAL'
                             WHEN COUNT(*) >= 3 AND COUNT(*) <= 5 THEN 'MAJOR'
                             WHEN COUNT(*) >=1 AND COUNT(*) < 3 THEN 'MINOR'
                        ELSE 'NORMAL'
                        END ,
                        COUNT(*),
                        DELAY_THRESHOLD,
                        FIRST(TESTTIME)
		FROM DELAYWARNWINDOW   
                WHERE STATUS = 'WARNING'
		GROUP BY ALERTKEY, STATUS,DELAY_THRESHOLD;


-- new waction stores

CREATE WACTIONSTORE PING_DETAILS
  CONTEXT OF ALERTENTRY
  EVENT TYPES (ALERTENTRY )
  PERSIST EVERY 60 second USING (
  JDBC_DRIVER:'oracle.jdbc.driver.OracleDriver',
  JDBC_URL:'jdbc:oracle:thin:@timstsnap05.ivanet.net:1521:timsrdg2',
  JDBC_USER:'network_cfg', JDBC_PASSWORD:'network_cfg',
  DDL_GENERATION:'create-or-extend-tables',
--  DDL_GENERATION:'drop-and-create-tables',
  CONTEXT_TABLE:'PING_DETAILS'
  );

CREATE CQ STOREDETAILSCQ
       INSERT INTO PING_DETAILS
              SELECT * FROM DELAYWARNWINDOW
              WHERE STATUS <> 'NORMAL';

CREATE WACTIONSTORE PingStore CONTEXT OF ALERTAGGENTRY
        EVENT TYPES( ALERTAGGENTRY )
        PERSIST NONE USING ( ) ;

CREATE CQ LoadPingStoreCQ
                INSERT INTO PingStore
                SELECT *
                FROM ALERTAGGSTREAM s;

-- end of stores 



CREATE STREAM AlertStream OF Global.AlertEvent;


CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT 'FROM TO SITES',      
       ALERTKEY, 
CASE
WHEN ALERT_STATUS != 'NORMAL' THEN 'warning'
ELSE 'info' END,
CASE
WHEN  (DNOW()  > DADD(FIRSTTIME, DMINS(20)) OR ALERT_STATUS = 'NORMAL')   THEN 'cancel'
ELSE 'raise' END,
CASE
WHEN ALERT_STATUS  != 'NORMAL' THEN ALERT_STATUS + ' ALERT FOR ' + ALERTKEY + ' has delay of over threshold ' + TO_STRING(DELAY_THRESHOLD) + ' has ' + TO_STRING(NUMBER_OF_VIOLATIONS) + '  violations over limit over the hour ' + TO_STRING(FIRSTTIME,'MM/dd/yyyy HH:mm:ss')  ELSE ''
END
    FROM ALERTAGGSTREAM 
WHERE NUMBER_OF_VIOLATIONS > 1;
--    FROM RESULTSSTORE; 


CREATE TARGET TRACE3 USING CSVWRITER (FILENAME:'logs/ALERTS.CSV') INPUT FROM AlertStream;

CREATE SUBSCRIPTION PingAppEmailAlert
USING EmailAdapter (
smtpurl:'smtp.nyc.ibasis.net:25',
smtp_auth:'false',
starttls_enable:'false',
subject:"IR34 Alert from Webaction", emailList:"pgopal@ibasis.net,ed@webaction.com",senderEmail:"webactionsupport@ibasis.net"
)
INPUT FROM AlertStream;

CREATE SUBSCRIPTION PosAppWebAlert
USING WebAlertAdapter( )
INPUT FROM AlertStream;


END flow PingMonitorBatchAppFlow;	

/*	
CREATE TYPE AlarmThreshold(
	profile int,
	threshholdValue int
);

CREATE CACHE AlarmThresholdCache using DatabaseReader (
   ConnectionURL:'jdbc:oracle:thin:@10.1.110.128:1521:orcl',
   Username:'scott',
   Password:'tiger',
   Query:'select * from scott.ALARM_THRESHOLD'
 ) QUERY (keytomap:'profile', refreshinterval: '60000000') OF AlarmThreshold; 
 

CREATE TYPE pingEvent(
	toLoc String,
	fromLoc String,	
        delay float,
        jitter float,
	packloss float,
        dateTime DateTime
);

CREATE STREAM dataStream OF pingEvent;

 Create Target trace1 Using Sysout (name:'rawping') input From LCRStream;

 
 CREATE CQ renderEvents
 INSERT INTO dataStream
 SELECT
 	x.TOLOCATION, x.FROMLOCACTION,  TO_FLOAT(x.DELAYAMT), TO_FLOAT(x.JITTERAMT), TO_FLOAT(x.PACKLOSSAMT), DNOW()
 FROM LCRStream1 x;
 
CREATE  jumping WINDOW  dataWindow OVER dataStream KEEP WITHIN 30 second; 
 
 CREATE TYPE AlertEntry(
	alertKey String KEY,
 	toLoc String,
 	fromLoc String,	
    status String,
	intStatus int,
    dateTime DateTime
 );

 CREATE STREAM AlertEntryStream OF AlertEntry;

 CREATE CQ processCQ1
 INSERT INTO AlertEntryStream
 select dw.toLoc + dw.fromLoc,
        dw.toLoc, 
        dw.fromLoc,         
        CASE WHEN dw.delay > c.threshholdValue THEN "WARNING" 
             ELSE "NORMAL" 
        END,
        CASE WHEN dw.delay > c.threshholdValue THEN 2
             ELSE 1
        END,
        dw.dateTime
 from dataWindow dw, AlarmThresholdCache c;
	  	 
 Create Target trace2 Using Sysout (name:'alert') input From AlertEntryStream;
*/ 
 
END APPLICATION PingMonitorBatchApp;
 -- create dashboard using "ClientApps/Overstock/ProdIndexDash.json";

create application DhcpLog;
create source DHCPLogSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'Dhcp*',
	charset:'UTF-8',
	positionByEOF:false
) PARSE USING DHCPLogParser (
	rowdelimiter:'\r\n',
	LineNumber:33
)
OUTPUT TO DHCPLogStream;
create Target DHCPDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/dhcp_log') input from DHCPLogStream;
end application DhcpLog;

STOP WStester.WSusingApp;
UNDEPLOY APPLICATION WStester.WSusingApp;
DROP APPLICATION WStester.WSusingApp CASCADE;

CREATE APPLICATION WSusingApp;

CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity int,
  size int);

CREATE SOURCE liveSource using StreamReader(
  OutputType: 'WStester.Atm',
  noLimit: 'false',
  maxRows: 20,
  iterations: 0,
  iterationDelay: 100,
  StringSet: 'productID[001-002-003-004],stateID[AS-CA-WA-NY]',
  NumberSet: 'productWeight[3-3]R,quantity[20-20]R,size[250-250]R'
  )OUTPUT TO CsvStream;


CREATE STREAM newStream OF Atm;

CREATE CQ newCQ
INSERT INTO newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_INT(data[3]), TO_INT(data[4]) FROM
CsvStream;

create target myOut8 using SysOut(name : testOut8) input from newStream;

CREATE WACTIONSTORE streamActivityMEM CONTEXT OF Atm
EVENT TYPES ( Atm )
USING MEMORY;

CREATE WACTIONSTORE streamActivityES CONTEXT OF Atm
EVENT TYPES ( Atm );

CREATE WACTIONSTORE streamActivityES2 CONTEXT OF Atm
EVENT TYPES ( Atm )
USING ( storageProvider: 'elasticsearch' );

CREATE WACTIONSTORE streamActivityDerby CONTEXT OF Atm
EVENT TYPES ( Atm )
USING (
storageProvider:'jdbc',
persistence_interval:'10 sec',
JDBC_DRIVER:'@WASTORE-DRIVER@',
JDBC_URL:'@WASTORE-URL@',
JDBC_USER:'@WASTORE-UNAME@',
JDBC_PASSWORD:'@WASTORE-PASSWORD@',
DDL_GENERATION:'drop-and-create-tables',
LOGGING_LEVEL:'SEVERE');

CREATE CQ newCQ2
INSERT INTO streamActivityMEM
SELECT * FROM newStream
link source event;

CREATE CQ newCQ3
INSERT INTO streamActivityES
SELECT * FROM newStream
link source event;

CREATE CQ newCQ4
INSERT INTO streamActivityES2
SELECT * FROM newStream
link source event;

CREATE CQ newCQ5
INSERT INTO streamActivityDerby
SELECT * FROM newStream
link source event;

END APPLICATION WSusingApp;

--
-- Recovery Test 32 with two sources, two sliding attribute windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sa5W/p -> CQ1 -> WS
-- S2 -> Sa6W/p -> CQ2 -> WS
--

STOP Recov32Tester.RecovTest32;
UNDEPLOY APPLICATION Recov32Tester.RecovTest32;
DROP APPLICATION Recov32Tester.RecovTest32 CASCADE;
CREATE APPLICATION RecovTest32 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION RecovTest32;

--
-- Recovery Test 8
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov8Tester.RecovTest8;
UNDEPLOY APPLICATION Recov8Tester.RecovTest8;
DROP APPLICATION Recov8Tester.RecovTest8 CASCADE;
CREATE APPLICATION RecovTest8 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END APPLICATION RecovTest8;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@ RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE @parquetsrc@ USING FileReader (
wildcard: '',
directory: '',
positionbyeof: false )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: ''
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCENAME@ USING IncrementalBatchReader  (
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: @startPosition@,
  PollingInterval: '120sec'
  )
  OUTPUT TO @STREAM@;

  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;

  CREATE TARGET @targetName@ USING DatabaseWriter(
    ConnectionURL:'@READER-URL@',
    Username:'@READER-UNAME@',
    Password:'@READER-PASSWORD@',
    BatchPolicy:'Eventcount:1,Interval:1',
    CommitPolicy:'Eventcount:1,Interval:1',
    Checkpointtable:'CHKPOINT',
    Tables:'@WATABLES@,@WATABLES@_target'
  ) INPUT FROM @STREAM@;

  END APPLICATION @APPNAME@;

  DEPLOY APPLICATION @APPNAME@;
  start application @APPNAME@;

--
-- Recovery Test 35 with two sources, two jumping count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5W/p -> CQ1 -> WS
--   S2 -> Jc6W/p -> CQ2 -> WS
--

STOP Recov35Tester.RecovTest35;
UNDEPLOY APPLICATION Recov35Tester.RecovTest35;
DROP APPLICATION Recov35Tester.RecovTest35 CASCADE;
CREATE APPLICATION RecovTest35 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION RecovTest35;

stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@;

CREATE SOURCE @SourceName@ USING MySQLReader  ( 
ReaderType: 'LogMiner', 
  Password_encrypted: 'false', 
  DatabaseName: 'qatest',
  SupportPDB: false, 
  QuiesceMarkerTable: 'QUIESCEMARKER', 
  QueueSize: 2048, 
  CommittedTransactions: true, 
  Username: '@UserName@', 
  TransactionBufferType: 'Memory', 
  TransactionBufferDiskLocation: '.striim/LargeBuffer', 
  OutboundServerProcessName: 'WebActionXStream', 
  Password: '@Password@', 
  DDLCaptureMode: 'All', 
  Compression: false, 
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', 
  FetchSize: 1, 
  Tables: '@SourceTables@', 
  DictionaryMode: 'OnlineCatalog', 
  XstreamTimeOut: 600, 
  TransactionBufferSpilloverSize: '1MB', 
  FilterTransactionBoundaries: true, 
  StartSCN: 'null', 
  ConnectionURL: '@ConnectionURL@', 
  SendBeforeImage: true ) 
OUTPUT TO @AppStream@  ;

CREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream SELECT data[2], to_string(to_date(data[2]), "dd-MMM-yy hh.mm.ss") FROM @AppStream@ o ;

CREATE  TARGET @targetsys@ USING Global.SysOut  ( 
name: 'ora1_sys' ) 
INPUT FROM admin.ZDT_cq_stream;

create Target @TargetFile@ using FileWriter(
  filename:'toStringOut.log',
  directory:'@FilePath@',
  rolloverpolicy:'eventcount:1000'
)
format using DSVFormatter (

)
input from admin.ZDT_cq_stream;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

alter application oraddl;
Create or replace Source Ora Using OracleReader 
(
 Username:'@user-name@',
 Password:'@password@',
 ConnectionURL:'src_url',
 Tables:'QATEST.ORACLEDDL%',
 DictionaryMode:OfflineCatalog,
 DDLCaptureMode : 'All',
 FetchSize:2
) Output To LogminerStream;
ALTER APPLICATION oraddl RECOMPILE;
deploy application oraddl;
START application oraddl;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 10 second interval;
create source CSVSource using FileReader (
	directory:'@DIR@',
	WildCard:'@WILDCARD@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  curr String,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       data[6],
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target BlobT using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:30,interval:5s'
)
format using AvroFormatter (
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW;
CREATE OR REPLACE PROPERTYSET KafkaProps(zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.9');
CREATE STREAM Oracle_ChangeDataStream1 of Global.WAEvent PERSIST USING KafkaProps;
CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  QuiesceMarkeRTable:'QATEST.QUIESCEMARKER',
  Password_encrypted: false,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//127.0.0.1:1521/xe',
  Tables: 'QATEST.OracToCql_alldatatypes',
  adapterName: 'OracleReader',
  Password: 'miner',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'miner',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream1;
CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'test.chkpoint',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:60',
  CommitPolicy: 'EventCount:1,Interval:60',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  Tables: 'QATEST.OracToCql_alldatatypes,test.oractocq_alldatatypes',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream1;
create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream1;
END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

stop application OneAgentWithMultiTester.AgentWithMultiReader;
undeploy application OneAgentWithMultiTester.AgentWithMultiReader;
drop application OneAgentWithMultiTester.AgentWithMultiReader cascade;

create application AgentWithMultiReader;


CREATE FLOW AgentFlow;

create source XMLSource using FileReader (
  Directory:'@TEST-DATA-PATH@',
  WildCard:'books.xml',
  positionByEOF:false
)
parse using XMLParser (
  RootNode:'/catalog/book'
)
OUTPUT TO XmlStream;

create source DSVCSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'customerdetails-agent.csv',
  charset: 'UTF-8',
  positionByEOF:false
)
parse using DSVParser (
  header:'no'
)
OUTPUT TO DSVCsvStream;

-- Read from File

create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'StoreNames.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CSVStream;

END FLOW AgentFlow;

--  Rest of the Stream and CQs are executed in Server flow

CREATE FLOW ServerFlow;

CREATE TARGET myout using LogWriter(name: XMLSource, filename:'@FEATURE-DIR@/logs/logXML.txt') input from XmlStream;
CREATE TARGET myout1 using LogWriter(name: DSVSource, filename:'@FEATURE-DIR@/logs/logDSV.txt', charset:'UTF-8') input from DSVCsvStream;
CREATE TARGET myout2 using LogWriter(name: CSVSource, filename:'@FEATURE-DIR@/logs/logCSV.txt') input from CSVStream;


END FLOW ServerFlow;

end application AgentWithMultiReader;
DEPLOY APPLICATION AgentWithMultiReader with AgentFlow in AGENTS, ServerFlow on any in default;

start AgentWithMultiReader;

--To be used with NonCdcAdapterCommons.startExceptionStoreApp()
CREATE APPLICATION @APPNAME@;

CREATE CQ @APPNAME@CQ INSERT INTO @APPNAME@CQOut
select x.exceptionType,
x.action,
x.appName,
x.entityType,
x.entityName,
x.className,
x.message,
x.relatedActivity,
x.relatedObjects,
x.relatedEntity,
x.exceptionCode
from @EXCEPTIONAPPNAME@_ExceptionStore x;

CREATE TARGET @APPNAME@Trgt USING FileWriter ()
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@CQOut;

END APPLICATION @APPNAME@;

--
-- Recovery Test 41 with two sources, two jumping count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5W/p -> CQ1 -> WS
--   S2 -> Jc6W/p -> CQ2 -> WS
--

STOP Recov41Tester.RecovTest41;
UNDEPLOY APPLICATION Recov41Tester.RecovTest41;
DROP APPLICATION Recov41Tester.RecovTest41 CASCADE;
CREATE APPLICATION RecovTest41 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION RecovTest41;

STOP APPLICATION testApp;
UNDEPLOY APPLICATION testApp;
DROP APPLICATION testApp CASCADE;
CREATE APPLICATION testApp recovery 5 SECOND Interval;


  CREATE OR REPLACE SOURCE testApp_Source Using PostgreSQLReader( 
  
  ReplicationSlotName:'test_slot',
  FilterTransactionBoundaries:'true',
  Username:'waction',
  Password_encrypted:false,
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  adapterName:'PostgreSQLReader',
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  Password:'w@ct10n',
  Tables:'public.sourceTable',
  ExcludedTables:'public.chkpoint'
 ) OUTPUT TO PGtoBQ_Stream;


CREATE OR REPLACE TARGET testApp_Target USING BigQueryWriter  (
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  projectId:'striimqa-214712',
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:5,Interval:120',
  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', 
  AllowQuotedNewLines: 'false', 
  adapterName: 'BigQueryWriter', 
  optimizedMerge: 'true', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', 
  StandardSQL: 'true', 
  QuoteCharacter: '\"', 
  Tables: 'public.sourceTable,BQAllpl.oratobqtgt',
  Mode: 'MERGE',
  StandardSQL: 'true',
  QuoteCharacter: '\"'
  ) INPUT FROM PGtoBQ_Stream;

CREATE OR REPLACE TARGET testApp_SysOut USING Global.SysOut (name: 'wa') INPUT FROM PGtoBQ_Stream;

END APPLICATION testApp;
DEPLOY APPLICATION testApp;
START testApp;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING DatabaseReader  (
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  Tables: @SOURCE_TABLE@,
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true'
 )
OUTPUT TO @STREAM@;

CREATE ROUTER @SOURCE_NAME@_ROUTER INPUT FROM @STREAM@ cs CASE
WHEN meta(cs,"OperationName").toString()='SELECT' THEN ROUTE TO @STREAM@1, ELSE ROUTE TO @STREAM@2;

STOP APPLICATION @Appname@;
UNDEPLOY APPLICATION @Appname@;
DROP APPLICATION @Appname@ CASCADE;

CREATE APPLICATION @Appname@ RECOVERY 10 SECOND INTERVAL;

CREATE  SOURCE @Appname@source USING MySQLReader
(
Username: '',
Password: '',
ConnectionURL: '',
Tables: 'waction.TABLE_TEST_%',
FetchSize:1
)
OUTPUT TO @Appname@MasterStream;

CREATE OR REPLACE ROUTER @Appname@Rs1 INPUT FROM @Appname@MasterStream s CASE
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_1' THEN ROUTE TO @Appname@Typed1,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_2' THEN ROUTE TO @Appname@Typed2,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_3' THEN ROUTE TO @Appname@Typed3,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_4' THEN ROUTE TO @Appname@Typed4,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_5' THEN ROUTE TO @Appname@Typed5,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_6' THEN ROUTE TO @Appname@Typed6,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_7' THEN ROUTE TO @Appname@Typed7,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_8' THEN ROUTE TO @Appname@Typed8,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_9' THEN ROUTE TO @Appname@Typed9,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_10' THEN ROUTE TO @Appname@Typed10,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_11' THEN ROUTE TO @Appname@Typed11,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_12' THEN ROUTE TO @Appname@Typed12,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_13' THEN ROUTE TO @Appname@Typed13,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_14' THEN ROUTE TO @Appname@Typed14,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_15' THEN ROUTE TO @Appname@Typed15,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_16' THEN ROUTE TO @Appname@Typed16,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_17' THEN ROUTE TO @Appname@Typed17,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_18' THEN ROUTE TO @Appname@Typed18,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_19' THEN ROUTE TO @Appname@Typed19,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_20' THEN ROUTE TO @Appname@Typed20,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_21' THEN ROUTE TO @Appname@Typed21,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_22' THEN ROUTE TO @Appname@Typed22,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_23' THEN ROUTE TO @Appname@Typed23,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_24' THEN ROUTE TO @Appname@Typed24,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_25' THEN ROUTE TO @Appname@Typed25,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_26' THEN ROUTE TO @Appname@Typed26,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_27' THEN ROUTE TO @Appname@Typed27,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_28' THEN ROUTE TO @Appname@Typed28,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_29' THEN ROUTE TO @Appname@Typed29,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_30' THEN ROUTE TO @Appname@Typed30,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_31' THEN ROUTE TO @Appname@Typed31,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_32' THEN ROUTE TO @Appname@Typed32,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_33' THEN ROUTE TO @Appname@Typed33,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_34' THEN ROUTE TO @Appname@Typed34,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_35' THEN ROUTE TO @Appname@Typed35,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_36' THEN ROUTE TO @Appname@Typed36,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_37' THEN ROUTE TO @Appname@Typed37,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_38' THEN ROUTE TO @Appname@Typed38,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_39' THEN ROUTE TO @Appname@Typed39,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_40' THEN ROUTE TO @Appname@Typed40,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_41' THEN ROUTE TO @Appname@Typed41,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_42' THEN ROUTE TO @Appname@Typed42,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_43' THEN ROUTE TO @Appname@Typed43,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_44' THEN ROUTE TO @Appname@Typed44,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_45' THEN ROUTE TO @Appname@Typed45,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_46' THEN ROUTE TO @Appname@Typed46,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_47' THEN ROUTE TO @Appname@Typed47,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_48' THEN ROUTE TO @Appname@Typed48,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_49' THEN ROUTE TO @Appname@Typed49,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_50' THEN ROUTE TO @Appname@Typed50,
/*WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_51' THEN ROUTE TO @Appname@Typed51,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_52' THEN ROUTE TO @Appname@Typed52,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_53' THEN ROUTE TO @Appname@Typed53,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_54' THEN ROUTE TO @Appname@Typed54,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_55' THEN ROUTE TO @Appname@Typed55,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_56' THEN ROUTE TO @Appname@Typed56,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_57' THEN ROUTE TO @Appname@Typed57,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_58' THEN ROUTE TO @Appname@Typed58,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_59' THEN ROUTE TO @Appname@Typed59,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_60' THEN ROUTE TO @Appname@Typed60,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_61' THEN ROUTE TO @Appname@Typed61,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_62' THEN ROUTE TO @Appname@Typed62,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_63' THEN ROUTE TO @Appname@Typed63,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_64' THEN ROUTE TO @Appname@Typed64,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_65' THEN ROUTE TO @Appname@Typed65,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_66' THEN ROUTE TO @Appname@Typed66,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_67' THEN ROUTE TO @Appname@Typed67,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_68' THEN ROUTE TO @Appname@Typed68,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_69' THEN ROUTE TO @Appname@Typed69,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_70' THEN ROUTE TO @Appname@Typed70,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_71' THEN ROUTE TO @Appname@Typed71,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_72' THEN ROUTE TO @Appname@Typed72,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_73' THEN ROUTE TO @Appname@Typed73,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_74' THEN ROUTE TO @Appname@Typed74,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_75' THEN ROUTE TO @Appname@Typed75,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_76' THEN ROUTE TO @Appname@Typed76,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_77' THEN ROUTE TO @Appname@Typed77,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_78' THEN ROUTE TO @Appname@Typed78,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_79' THEN ROUTE TO @Appname@Typed79,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_80' THEN ROUTE TO @Appname@Typed80,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_81' THEN ROUTE TO @Appname@Typed81,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_82' THEN ROUTE TO @Appname@Typed82,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_83' THEN ROUTE TO @Appname@Typed83,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_84' THEN ROUTE TO @Appname@Typed84,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_85' THEN ROUTE TO @Appname@Typed85,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_86' THEN ROUTE TO @Appname@Typed86,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_87' THEN ROUTE TO @Appname@Typed87,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_88' THEN ROUTE TO @Appname@Typed88,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_89' THEN ROUTE TO @Appname@Typed89,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_90' THEN ROUTE TO @Appname@Typed90,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_91' THEN ROUTE TO @Appname@Typed91,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_92' THEN ROUTE TO @Appname@Typed92,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_93' THEN ROUTE TO @Appname@Typed93,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_94' THEN ROUTE TO @Appname@Typed94,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_95' THEN ROUTE TO @Appname@Typed95,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_96' THEN ROUTE TO @Appname@Typed96,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_97' THEN ROUTE TO @Appname@Typed97,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_98' THEN ROUTE TO @Appname@Typed98,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_99' THEN ROUTE TO @Appname@Typed99,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_1000100' THEN ROUTE TO @Appname@Typed100,*/
ELSE ROUTE TO @Appname@TypedElse;



CREATE OR REPLACE cq @Appname@cq1
INSERT INTO combinedstream
select s FROM @Appname@Typed1 s;

CREATE OR REPLACE cq @Appname@cq2
INSERT INTO combinedstream
select s FROM @Appname@Typed2 s;

CREATE OR REPLACE cq @Appname@cq3
INSERT INTO combinedstream
select s FROM @Appname@Typed3 s;

CREATE OR REPLACE cq @Appname@cq4
INSERT INTO combinedstream
select s FROM @Appname@Typed4 s;

CREATE OR REPLACE cq @Appname@cq5
INSERT INTO combinedstream
select s FROM @Appname@Typed5 s;

CREATE OR REPLACE cq @Appname@cq6
INSERT INTO combinedstream
select s FROM @Appname@Typed6 s;

CREATE OR REPLACE cq @Appname@cq7
INSERT INTO combinedstream
select s FROM @Appname@Typed7 s;

CREATE OR REPLACE cq @Appname@cq8
INSERT INTO combinedstream
select s FROM @Appname@Typed8 s;

CREATE OR REPLACE cq @Appname@cq9
INSERT INTO combinedstream
select s FROM @Appname@Typed9 s;

CREATE OR REPLACE cq @Appname@cq10
INSERT INTO combinedstream
select s FROM @Appname@Typed10 s;


CREATE OR REPLACE cq @Appname@cq11
INSERT INTO combinedstream
select s FROM @Appname@Typed11 s;

CREATE OR REPLACE cq @Appname@cq12
INSERT INTO combinedstream
select s FROM @Appname@Typed12 s;

CREATE OR REPLACE cq @Appname@cq13
INSERT INTO combinedstream
select s FROM @Appname@Typed13 s;

CREATE OR REPLACE cq @Appname@cq14
INSERT INTO combinedstream
select s FROM @Appname@Typed14 s;

CREATE OR REPLACE cq @Appname@cq15
INSERT INTO combinedstream
select s FROM @Appname@Typed15 s;

CREATE OR REPLACE cq @Appname@cq16
INSERT INTO combinedstream
select s FROM @Appname@Typed16 s;

CREATE OR REPLACE cq @Appname@cq117
INSERT INTO combinedstream
select s FROM @Appname@Typed17 s;

CREATE OR REPLACE cq @Appname@cq18
INSERT INTO combinedstream
select s FROM @Appname@Typed18 s;

CREATE OR REPLACE cq @Appname@cq19
INSERT INTO combinedstream
select s FROM @Appname@Typed19 s;

CREATE OR REPLACE cq @Appname@cq20
INSERT INTO combinedstream
select s FROM @Appname@Typed20 s;

CREATE OR REPLACE cq @Appname@cq21
INSERT INTO combinedstream
select s FROM @Appname@Typed21 s;

CREATE OR REPLACE cq @Appname@cq22
INSERT INTO combinedstream
select s FROM @Appname@Typed22 s;

CREATE OR REPLACE cq @Appname@cq23
INSERT INTO combinedstream
select s FROM @Appname@Typed23 s;

CREATE OR REPLACE cq @Appname@cq24
INSERT INTO combinedstream
select s FROM @Appname@Typed24 s;

CREATE OR REPLACE cq @Appname@cq25
INSERT INTO combinedstream
select s FROM @Appname@Typed25 s;

CREATE OR REPLACE cq @Appname@cq26
INSERT INTO combinedstream
select s FROM @Appname@Typed26 s;

CREATE OR REPLACE cq @Appname@cq27
INSERT INTO combinedstream
select s FROM @Appname@Typed27 s;

CREATE OR REPLACE cq @Appname@cq28
INSERT INTO combinedstream
select s FROM @Appname@Typed28 s;

CREATE OR REPLACE cq @Appname@cq29
INSERT INTO combinedstream
select s FROM @Appname@Typed29 s;

CREATE OR REPLACE cq @Appname@cq30
INSERT INTO combinedstream
select s FROM @Appname@Typed30 s;

CREATE OR REPLACE cq @Appname@cq31
INSERT INTO combinedstream
select s FROM @Appname@Typed31 s;

CREATE OR REPLACE cq @Appname@cq32
INSERT INTO combinedstream
select s FROM @Appname@Typed32 s;

CREATE OR REPLACE cq @Appname@cq33
INSERT INTO combinedstream
select s FROM @Appname@Typed33 s;

CREATE OR REPLACE cq @Appname@cq34
INSERT INTO combinedstream
select s FROM @Appname@Typed34 s;

CREATE OR REPLACE cq @Appname@cq35
INSERT INTO combinedstream
select s FROM @Appname@Typed35 s;

CREATE OR REPLACE cq @Appname@cq36
INSERT INTO combinedstream
select s FROM @Appname@Typed36 s;

CREATE OR REPLACE cq @Appname@cq37
INSERT INTO combinedstream
select s FROM @Appname@Typed37 s;

CREATE OR REPLACE cq @Appname@cq38
INSERT INTO combinedstream
select s FROM @Appname@Typed38 s;

CREATE OR REPLACE cq @Appname@cq39
INSERT INTO combinedstream
select s FROM @Appname@Typed39 s;

CREATE OR REPLACE cq @Appname@cq40
INSERT INTO combinedstream
select s FROM @Appname@Typed40 s;

CREATE OR REPLACE cq @Appname@cq41
INSERT INTO combinedstream
select s FROM @Appname@Typed41 s;

CREATE OR REPLACE cq @Appname@cq42
INSERT INTO combinedstream
select s FROM @Appname@Typed42 s;

CREATE OR REPLACE cq @Appname@cq43
INSERT INTO combinedstream
select s FROM @Appname@Typed43 s;

CREATE OR REPLACE cq @Appname@cq44
INSERT INTO combinedstream
select s FROM @Appname@Typed44 s;

CREATE OR REPLACE cq @Appname@cq45
INSERT INTO combinedstream
select s FROM @Appname@Typed45 s;

CREATE OR REPLACE cq @Appname@cq46
INSERT INTO combinedstream
select s FROM @Appname@Typed46 s;

CREATE OR REPLACE cq @Appname@cq47
INSERT INTO combinedstream
select s FROM @Appname@Typed47 s;

CREATE OR REPLACE cq @Appname@cq48
INSERT INTO combinedstream
select s FROM @Appname@Typed48 s;

CREATE OR REPLACE cq @Appname@cq49
INSERT INTO combinedstream
select s FROM @Appname@Typed49 s;

CREATE OR REPLACE cq @Appname@cq50
INSERT INTO combinedstream
select s FROM @Appname@Typed50 s;
/*
CREATE OR REPLACE cq @Appname@cq51
INSERT INTO combinedstream
select s FROM @Appname@Typed51 s;

CREATE OR REPLACE cq @Appname@cq52
INSERT INTO combinedstream
select s FROM @Appname@Typed52 s;

CREATE OR REPLACE cq @Appname@cq53
INSERT INTO combinedstream
select s FROM @Appname@Typed53 s;

CREATE OR REPLACE cq @Appname@cq54
INSERT INTO combinedstream
select s FROM @Appname@Typed54 s;

CREATE OR REPLACE cq @Appname@cq55
INSERT INTO combinedstream
select s FROM @Appname@Typed55 s;

CREATE OR REPLACE cq @Appname@cq56
INSERT INTO combinedstream
select s FROM @Appname@Typed56 s;

CREATE OR REPLACE cq @Appname@cq57
INSERT INTO combinedstream
select s FROM @Appname@Typed57 s;

CREATE OR REPLACE cq @Appname@cq58
INSERT INTO combinedstream
select s FROM @Appname@Typed58 s;

CREATE OR REPLACE cq @Appname@cq59
INSERT INTO combinedstream
select s FROM @Appname@Typed59 s;

CREATE OR REPLACE cq @Appname@cq60
INSERT INTO combinedstream
select s FROM @Appname@Typed60 s;

CREATE OR REPLACE cq @Appname@cq61
INSERT INTO combinedstream
select s FROM @Appname@Typed61 s;

CREATE OR REPLACE cq @Appname@cq62
INSERT INTO combinedstream
select s FROM @Appname@Typed62 s;

CREATE OR REPLACE cq @Appname@cq63
INSERT INTO combinedstream
select s FROM @Appname@Typed63 s;

CREATE OR REPLACE cq @Appname@cq64
INSERT INTO combinedstream
select s FROM @Appname@Typed64 s;

CREATE OR REPLACE cq @Appname@cq65
INSERT INTO combinedstream
select s FROM @Appname@Typed65 s;

CREATE OR REPLACE cq @Appname@cq66
INSERT INTO combinedstream
select s FROM @Appname@Typed66 s;

CREATE OR REPLACE cq @Appname@cq67
INSERT INTO combinedstream
select s FROM @Appname@Typed67 s;

CREATE OR REPLACE cq @Appname@cq68
INSERT INTO combinedstream
select s FROM @Appname@Typed68 s;

CREATE OR REPLACE cq @Appname@cq69
INSERT INTO combinedstream
select s FROM @Appname@Typed69 s;

CREATE OR REPLACE cq @Appname@cq70
INSERT INTO combinedstream
select s FROM @Appname@Typed70 s;

CREATE OR REPLACE cq @Appname@cq71
INSERT INTO combinedstream
select s FROM @Appname@Typed71 s;

CREATE OR REPLACE cq @Appname@cq72
INSERT INTO combinedstream
select s FROM @Appname@Typed72 s;

CREATE OR REPLACE cq @Appname@cq73
INSERT INTO combinedstream
select s FROM @Appname@Typed73 s;

CREATE OR REPLACE cq @Appname@cq74
INSERT INTO combinedstream
select s FROM @Appname@Typed74 s;

CREATE OR REPLACE cq @Appname@cq75
INSERT INTO combinedstream
select s FROM @Appname@Typed75 s;

CREATE OR REPLACE cq @Appname@cq76
INSERT INTO combinedstream
select s FROM @Appname@Typed76 s;

CREATE OR REPLACE cq @Appname@cq77
INSERT INTO combinedstream
select s FROM @Appname@Typed77 s;

CREATE OR REPLACE cq @Appname@cq78
INSERT INTO combinedstream
select s FROM @Appname@Typed78 s;

CREATE OR REPLACE cq @Appname@cq79
INSERT INTO combinedstream
select s FROM @Appname@Typed79 s;

CREATE OR REPLACE cq @Appname@cq80
INSERT INTO combinedstream
select s FROM @Appname@Typed80 s;

CREATE OR REPLACE cq @Appname@cq81
INSERT INTO combinedstream
select s FROM @Appname@Typed81 s;

CREATE OR REPLACE cq @Appname@cq82
INSERT INTO combinedstream
select s FROM @Appname@Typed82 s;

CREATE OR REPLACE cq @Appname@cq83
INSERT INTO combinedstream
select s FROM @Appname@Typed83 s;

CREATE OR REPLACE cq @Appname@cq84
INSERT INTO combinedstream
select s FROM @Appname@Typed84 s;

CREATE OR REPLACE cq @Appname@cq85
INSERT INTO combinedstream
select s FROM @Appname@Typed85 s;

CREATE OR REPLACE cq @Appname@cq86
INSERT INTO combinedstream
select s FROM @Appname@Typed86 s;

CREATE OR REPLACE cq @Appname@cq87
INSERT INTO combinedstream
select s FROM @Appname@Typed87 s;

CREATE OR REPLACE cq @Appname@cq88
INSERT INTO combinedstream
select s FROM @Appname@Typed88 s;

CREATE OR REPLACE cq @Appname@cq89
INSERT INTO combinedstream
select s FROM @Appname@Typed89 s;

CREATE OR REPLACE cq @Appname@cq90
INSERT INTO combinedstream
select s FROM @Appname@Typed90 s;

CREATE OR REPLACE cq @Appname@cq91
INSERT INTO combinedstream
select s FROM @Appname@Typed91 s;

CREATE OR REPLACE cq @Appname@cq92
INSERT INTO combinedstream
select s FROM @Appname@Typed92 s;

CREATE OR REPLACE cq @Appname@cq93
INSERT INTO combinedstream
select s FROM @Appname@Typed93 s;

CREATE OR REPLACE cq @Appname@cq94
INSERT INTO combinedstream
select s FROM @Appname@Typed94 s;

CREATE OR REPLACE cq @Appname@cq95
INSERT INTO combinedstream
select s FROM @Appname@Typed95 s;

CREATE OR REPLACE cq @Appname@cq96
INSERT INTO combinedstream
select s FROM @Appname@Typed96 s;

CREATE OR REPLACE cq @Appname@cq97
INSERT INTO combinedstream
select s FROM @Appname@Typed97 s;

CREATE OR REPLACE cq @Appname@cq98
INSERT INTO combinedstream
select s FROM @Appname@Typed98 s;

CREATE OR REPLACE cq @Appname@cq99
INSERT INTO combinedstream
select s FROM @Appname@Typed99 s;

CREATE OR REPLACE cq @Appname@cq100
INSERT INTO combinedstream
select s FROM @Appname@Typed100 s;

CREATE OR REPLACE cq @Appname@cq101
INSERT INTO combinedstream
select s FROM @Appname@TypedElse s;*/

CREATE OR REPLACE ROUTER Rs2 INPUT FROM combinedstream s CASE
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_1' THEN ROUTE TO New@Appname@Typed1,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_2' THEN ROUTE TO New@Appname@Typed2,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_3' THEN ROUTE TO New@Appname@Typed3,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_4' THEN ROUTE TO New@Appname@Typed4,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_5' THEN ROUTE TO New@Appname@Typed5,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_6' THEN ROUTE TO New@Appname@Typed6,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_7' THEN ROUTE TO New@Appname@Typed7,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_8' THEN ROUTE TO New@Appname@Typed8,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_9' THEN ROUTE TO New@Appname@Typed9,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_10' THEN ROUTE TO New@Appname@Typed10,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_11' THEN ROUTE TO New@Appname@Typed11,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_12' THEN ROUTE TO New@Appname@Typed12,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_13' THEN ROUTE TO New@Appname@Typed13,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_14' THEN ROUTE TO New@Appname@Typed14,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_15' THEN ROUTE TO New@Appname@Typed15,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_16' THEN ROUTE TO New@Appname@Typed16,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_17' THEN ROUTE TO New@Appname@Typed17,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_18' THEN ROUTE TO New@Appname@Typed18,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_19' THEN ROUTE TO New@Appname@Typed19,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_20' THEN ROUTE TO New@Appname@Typed20,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_21' THEN ROUTE TO New@Appname@Typed21,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_22' THEN ROUTE TO New@Appname@Typed22,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_23' THEN ROUTE TO New@Appname@Typed23,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_24' THEN ROUTE TO New@Appname@Typed24,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_25' THEN ROUTE TO New@Appname@Typed25,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_26' THEN ROUTE TO New@Appname@Typed26,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_27' THEN ROUTE TO New@Appname@Typed27,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_28' THEN ROUTE TO New@Appname@Typed28,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_29' THEN ROUTE TO New@Appname@Typed29,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_30' THEN ROUTE TO New@Appname@Typed30,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_31' THEN ROUTE TO New@Appname@Typed31,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_32' THEN ROUTE TO New@Appname@Typed32,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_33' THEN ROUTE TO New@Appname@Typed33,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_34' THEN ROUTE TO New@Appname@Typed34,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_35' THEN ROUTE TO New@Appname@Typed35,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_36' THEN ROUTE TO New@Appname@Typed36,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_37' THEN ROUTE TO New@Appname@Typed37,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_38' THEN ROUTE TO New@Appname@Typed38,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_39' THEN ROUTE TO New@Appname@Typed39,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_40' THEN ROUTE TO New@Appname@Typed40,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_41' THEN ROUTE TO New@Appname@Typed41,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_42' THEN ROUTE TO New@Appname@Typed42,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_43' THEN ROUTE TO New@Appname@Typed43,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_44' THEN ROUTE TO New@Appname@Typed44,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_45' THEN ROUTE TO New@Appname@Typed45,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_46' THEN ROUTE TO New@Appname@Typed46,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_47' THEN ROUTE TO New@Appname@Typed47,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_48' THEN ROUTE TO New@Appname@Typed48,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_49' THEN ROUTE TO New@Appname@Typed49,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_50' THEN ROUTE TO New@Appname@Typed50,
/*WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_51' THEN ROUTE TO New@Appname@Typed51,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_52' THEN ROUTE TO New@Appname@Typed52,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_53' THEN ROUTE TO New@Appname@Typed53,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_54' THEN ROUTE TO New@Appname@Typed54,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_55' THEN ROUTE TO New@Appname@Typed55,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_56' THEN ROUTE TO New@Appname@Typed56,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_57' THEN ROUTE TO New@Appname@Typed57,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_58' THEN ROUTE TO New@Appname@Typed58,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_59' THEN ROUTE TO New@Appname@Typed59,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_60' THEN ROUTE TO New@Appname@Typed60,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_61' THEN ROUTE TO New@Appname@Typed61,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_62' THEN ROUTE TO New@Appname@Typed62,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_63' THEN ROUTE TO New@Appname@Typed63,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_64' THEN ROUTE TO New@Appname@Typed64,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_65' THEN ROUTE TO New@Appname@Typed65,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_66' THEN ROUTE TO New@Appname@Typed66,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_67' THEN ROUTE TO New@Appname@Typed67,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_68' THEN ROUTE TO New@Appname@Typed68,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_69' THEN ROUTE TO New@Appname@Typed69,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_70' THEN ROUTE TO New@Appname@Typed70,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_71' THEN ROUTE TO New@Appname@Typed71,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_72' THEN ROUTE TO New@Appname@Typed72,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_73' THEN ROUTE TO New@Appname@Typed73,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_74' THEN ROUTE TO New@Appname@Typed74,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_75' THEN ROUTE TO New@Appname@Typed75,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_76' THEN ROUTE TO New@Appname@Typed76,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_77' THEN ROUTE TO New@Appname@Typed77,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_78' THEN ROUTE TO New@Appname@Typed78,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_79' THEN ROUTE TO New@Appname@Typed79,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_80' THEN ROUTE TO New@Appname@Typed80,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_81' THEN ROUTE TO New@Appname@Typed81,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_82' THEN ROUTE TO New@Appname@Typed82,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_83' THEN ROUTE TO New@Appname@Typed83,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_84' THEN ROUTE TO New@Appname@Typed84,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_85' THEN ROUTE TO New@Appname@Typed85,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_86' THEN ROUTE TO New@Appname@Typed86,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_87' THEN ROUTE TO New@Appname@Typed87,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_88' THEN ROUTE TO New@Appname@Typed88,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_89' THEN ROUTE TO New@Appname@Typed89,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_90' THEN ROUTE TO New@Appname@Typed90,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_91' THEN ROUTE TO New@Appname@Typed91,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_92' THEN ROUTE TO New@Appname@Typed92,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_93' THEN ROUTE TO New@Appname@Typed93,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_94' THEN ROUTE TO New@Appname@Typed94,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_95' THEN ROUTE TO New@Appname@Typed95,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_96' THEN ROUTE TO New@Appname@Typed96,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_97' THEN ROUTE TO New@Appname@Typed97,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_98' THEN ROUTE TO New@Appname@Typed98,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_99' THEN ROUTE TO New@Appname@Typed99,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_1000100' THEN ROUTE TO New@Appname@Typed100,*/
ELSE ROUTE TO New@Appname@TypedElse;

CREATE OR REPLACE cq New@Appname@cq1
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed1 s;

CREATE OR REPLACE cq New@Appname@cq2
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed2 s;

CREATE OR REPLACE cq New@Appname@cq3
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed3 s;

CREATE OR REPLACE cq New@Appname@cq4
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed4 s;

CREATE OR REPLACE cq New@Appname@cq5
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed5 s;

CREATE OR REPLACE cq New@Appname@cq6
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed6 s;

CREATE OR REPLACE cq New@Appname@cq7
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed7 s;

CREATE OR REPLACE cq New@Appname@cq8
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed8 s;

CREATE OR REPLACE cq New@Appname@cq9
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed9 s;

CREATE OR REPLACE cq New@Appname@cq10
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed10 s;


CREATE OR REPLACE cq New@Appname@cq11
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed11 s;

CREATE OR REPLACE cq New@Appname@cq12
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed12 s;

CREATE OR REPLACE cq New@Appname@cq13
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed13 s;

CREATE OR REPLACE cq New@Appname@cq14
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed14 s;

CREATE OR REPLACE cq New@Appname@cq15
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed15 s;

CREATE OR REPLACE cq New@Appname@cq16
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed16 s;

CREATE OR REPLACE cq New@Appname@cq117
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed17 s;

CREATE OR REPLACE cq New@Appname@cq18
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed18 s;

CREATE OR REPLACE cq New@Appname@cq19
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed19 s;

CREATE OR REPLACE cq New@Appname@cq20
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed20 s;

CREATE OR REPLACE cq New@Appname@cq21
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed21 s;

CREATE OR REPLACE cq New@Appname@cq22
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed22 s;

CREATE OR REPLACE cq New@Appname@cq23
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed23 s;

CREATE OR REPLACE cq New@Appname@cq24
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed24 s;

CREATE OR REPLACE cq New@Appname@cq25
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed25 s;

CREATE OR REPLACE cq New@Appname@cq26
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed26 s;

CREATE OR REPLACE cq New@Appname@cq27
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed27 s;

CREATE OR REPLACE cq New@Appname@cq28
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed28 s;

CREATE OR REPLACE cq New@Appname@cq29
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed29 s;

CREATE OR REPLACE cq New@Appname@cq30
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed30 s;

CREATE OR REPLACE cq New@Appname@cq31
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed31 s;

CREATE OR REPLACE cq New@Appname@cq32
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed32 s;

CREATE OR REPLACE cq New@Appname@cq33
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed33 s;

CREATE OR REPLACE cq New@Appname@cq34
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed34 s;

CREATE OR REPLACE cq New@Appname@cq35
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed35 s;

CREATE OR REPLACE cq New@Appname@cq36
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed36 s;

CREATE OR REPLACE cq New@Appname@cq37
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed37 s;

CREATE OR REPLACE cq New@Appname@cq38
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed38 s;

CREATE OR REPLACE cq New@Appname@cq39
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed39 s;

CREATE OR REPLACE cq New@Appname@cq40
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed40 s;

CREATE OR REPLACE cq New@Appname@cq41
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed41 s;

CREATE OR REPLACE cq New@Appname@cq42
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed42 s;

CREATE OR REPLACE cq New@Appname@cq43
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed43 s;

CREATE OR REPLACE cq New@Appname@cq44
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed44 s;

CREATE OR REPLACE cq New@Appname@cq45
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed45 s;

CREATE OR REPLACE cq New@Appname@cq46
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed46 s;

CREATE OR REPLACE cq New@Appname@cq47
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed47 s;

CREATE OR REPLACE cq New@Appname@cq48
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed48 s;

CREATE OR REPLACE cq New@Appname@cq49
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed49 s;

CREATE OR REPLACE cq New@Appname@cq50
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed50 s;
/*
CREATE OR REPLACE cq New@Appname@cq51
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed51 s;

CREATE OR REPLACE cq New@Appname@cq52
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed52 s;

CREATE OR REPLACE cq New@Appname@cq53
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed53 s;

CREATE OR REPLACE cq New@Appname@cq54
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed54 s;

CREATE OR REPLACE cq New@Appname@cq55
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed55 s;

CREATE OR REPLACE cq New@Appname@cq56
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed56 s;

CREATE OR REPLACE cq New@Appname@cq57
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed57 s;

CREATE OR REPLACE cq New@Appname@cq58
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed58 s;

CREATE OR REPLACE cq New@Appname@cq59
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed59 s;

CREATE OR REPLACE cq New@Appname@cq60
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed60 s;

CREATE OR REPLACE cq New@Appname@cq61
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed61 s;

CREATE OR REPLACE cq New@Appname@cq62
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed62 s;

CREATE OR REPLACE cq New@Appname@cq63
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed63 s;

CREATE OR REPLACE cq New@Appname@cq64
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed64 s;

CREATE OR REPLACE cq New@Appname@cq65
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed65 s;

CREATE OR REPLACE cq New@Appname@cq66
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed66 s;

CREATE OR REPLACE cq New@Appname@cq67
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed67 s;

CREATE OR REPLACE cq New@Appname@cq68
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed68 s;

CREATE OR REPLACE cq New@Appname@cq69
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed69 s;

CREATE OR REPLACE cq New@Appname@cq70
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed70 s;

CREATE OR REPLACE cq New@Appname@cq71
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed71 s;

CREATE OR REPLACE cq New@Appname@cq72
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed72 s;

CREATE OR REPLACE cq New@Appname@cq73
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed73 s;

CREATE OR REPLACE cq New@Appname@cq74
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed74 s;

CREATE OR REPLACE cq New@Appname@cq75
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed75 s;

CREATE OR REPLACE cq New@Appname@cq76
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed76 s;

CREATE OR REPLACE cq New@Appname@cq77
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed77 s;

CREATE OR REPLACE cq New@Appname@cq78
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed78 s;

CREATE OR REPLACE cq New@Appname@cq79
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed79 s;

CREATE OR REPLACE cq New@Appname@cq80
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed80 s;

CREATE OR REPLACE cq New@Appname@cq81
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed81 s;

CREATE OR REPLACE cq New@Appname@cq82
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed82 s;

CREATE OR REPLACE cq New@Appname@cq83
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed83 s;

CREATE OR REPLACE cq New@Appname@cq84
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed84 s;

CREATE OR REPLACE cq New@Appname@cq85
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed85 s;

CREATE OR REPLACE cq New@Appname@cq86
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed86 s;

CREATE OR REPLACE cq New@Appname@cq87
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed87 s;

CREATE OR REPLACE cq New@Appname@cq88
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed88 s;

CREATE OR REPLACE cq New@Appname@cq89
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed89 s;

CREATE OR REPLACE cq New@Appname@cq90
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed90 s;

CREATE OR REPLACE cq New@Appname@cq91
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed91 s;

CREATE OR REPLACE cq New@Appname@cq92
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed92 s;

CREATE OR REPLACE cq New@Appname@cq93
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed93 s;

CREATE OR REPLACE cq New@Appname@cq94
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed94 s;

CREATE OR REPLACE cq New@Appname@cq95
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed95 s;

CREATE OR REPLACE cq New@Appname@cq96
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed96 s;

CREATE OR REPLACE cq New@Appname@cq97
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed97 s;

CREATE OR REPLACE cq New@Appname@cq98
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed98 s;

CREATE OR REPLACE cq New@Appname@cq99
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed99 s;

CREATE OR REPLACE cq New@Appname@cq100
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed100 s;

CREATE OR REPLACE cq New@Appname@cq101
INSERT INTO Newcombinedstream
select s FROM New@Appname@TypedElse s;*/

create target systar using sysout(name:'out')INPUT FROM Newcombinedstream;

CREATE OR REPLACE TARGET @Appname@Target USING DatabaseWriter (
ConnectionURL:'jdbc:mysql://162.222.179.3:3306/waction',
Username:'root',
Password:'w@ct10n',
Tables:'waction.TABLE_TEST_%;waction.RESULTTABLE',
IgnorableExceptioncode : 'NO_OP_DELETE,NO_OP_UPDATE,NO_OP_PKUPDATE',
CommitPolicy:'Eventcount:1000,Interval:3600',
BatchPolicy:'eventCount:1000,Interval:3600'
)
INPUT FROM Newcombinedstream;

end application @Appname@;
deploy application @Appname@;
start application @Appname@;

--
-- Recovery Test 21 with two sources, two sliding count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5W -> CQ1 -> WS
-- S2 -> Sc6W -> CQ2 -> WS
--

STOP KStreamRecov21Tester.KStreamRecovTest21;
UNDEPLOY APPLICATION KStreamRecov21Tester.KStreamRecovTest21;
DROP APPLICATION KStreamRecov21Tester.KStreamRecovTest21 CASCADE;
DROP USER KStreamRecov21Tester;
DROP NAMESPACE KStreamRecov21Tester CASCADE;
CREATE USER KStreamRecov21Tester IDENTIFIED BY KStreamRecov21Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov21Tester;
CONNECT KStreamRecov21Tester KStreamRecov21Tester;

CREATE APPLICATION KStreamRecovTest21 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION KStreamRecovTest21;

stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@;

CREATE SOURCE @SourceName@ USING PostgreSQLReader  ( 
ReaderType: 'LogMiner', 
  Password_encrypted: 'false', 
  SupportPDB: false, 
  ReplicationSlotName: 'test_slot',
  QuiesceMarkerTable: 'QUIESCEMARKER', 
  QueueSize: 2048, 
  CommittedTransactions: true, 
  Username: '@UserName@', 
  TransactionBufferType: 'Memory', 
  TransactionBufferDiskLocation: '.striim/LargeBuffer', 
  OutboundServerProcessName: 'WebActionXStream', 
  Password: '@Password@', 
  DDLCaptureMode: 'All', 
  Compression: false, 
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', 
  FetchSize: 1, 
  Tables: '@SourceTables@', 
  DictionaryMode: 'OnlineCatalog', 
  XstreamTimeOut: 600, 
  TransactionBufferSpilloverSize: '1MB', 
  FilterTransactionBoundaries: true, 
  ConnectionURL: '@ConnectionURL@', 
  SendBeforeImage: true ) 
OUTPUT TO @AppStream@  ;

CREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream SELECT data[2], to_string(to_date(data[2]), "dd-MMM-yy hh.mm.ss") FROM @AppStream@ o ;

CREATE  TARGET @targetsys@ USING Global.SysOut  ( 
name: 'ora1_sys' ) 
INPUT FROM admin.ZDT_cq_stream;

create Target @TargetFile@ using FileWriter(
  filename:'toStringOut.log',
  directory:'@FilePath@',
  rolloverpolicy:'eventcount:1000'
)
format using DSVFormatter (

)
input from admin.ZDT_cq_stream;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

--
-- Canon Test W50
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for a partitioned sliding count window
--
-- S -> SWc5p -> CQ -> WS
--


UNDEPLOY APPLICATION NameW50.W50;
DROP APPLICATION NameW50.W50 CASCADE;
CREATE APPLICATION W50 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW50;

CREATE SOURCE CsvSourceW50 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW50;

END FLOW DataAcquisitionW50;


CREATE FLOW DataProcessingW50;

CREATE TYPE DataTypeW50 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW50 OF DataTypeW50;

CREATE CQ CSVStreamW50_to_DataStreamW50
INSERT INTO DataStreamW50
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW50;

CREATE WINDOW SWc5pW50
OVER DataStreamW50
KEEP 5 ROWS
PARTITION BY word;

CREATE WACTIONSTORE WactionStoreW50 CONTEXT OF DataTypeW50
EVENT TYPES ( DataTypeW50 KEY(word) )
@PERSIST-TYPE@

CREATE CQ SWc5pW50_to_WactionStoreW50
INSERT INTO WactionStoreW50
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM SWc5pW50
GROUP BY word;

END FLOW DataProcessingW50;



END APPLICATION W50;

UNDEPLOY APPLICATION RollOverTester.DSV;
DROP APPLICATION RollOverTester.DSV CASCADE;
CREATE APPLICATION DSV;


create source JsonSource using FileReader (
        directory:'@TEST-DATA-PATH@',
        WildCard:'TestInput.txt',
        positionByEOF:false,
        charset:'UTF-8'
)
 PARSE USING JSONParser (
  eventType: ''
 )
OUTPUT TO activeguard_rawstream;


CREATE OR REPLACE TYPE activeguard_parsed (
	-- Enriched, or not in AGI left navigation
	hash_key String KEY,
	client_id Long,
	event_type_id Long,
	type_code String,
	rule_id Long,
	rule_name String,
	rule_action String,
	sce_id  Long,
	message String,
	ref_list_match String,
	event_date DateTime,
	log_count Long,

	-- As seen in AGI
	-- frequently used, but not duplicated
	received_date DateTime,
	event_sub_source String,

	-- database
	sol_action String,
	comment String,
	comment_text String,
	database_name String,
	database_user_name String,
	entry_id String,
	obj_name String,
	os_user_name String,
	owner String,
	priv_used String,
	return_code String,
	session_id String,

	-- geo
	destination_country_code String,
	source_country_code String,

	-- network
	data_size String,
	destination String,
	destination_object String,
	destination_port String,
	sol_domain String,
	event_manager String,
	event_path String,
	event_source String,
	msg_no String,
	protocol_id String,
	service String,
	sol_source String,
	source_mac_address String,
	source_object String,
	source_port String,
	status_code String,
	target_path String,

	-- other
	command String,

	-- security
	affected_file String,
	object_action String,
	object_action_result String,
	object_id String,
	object_name String,
	object_path String,
	process_name String,

	-- user
	caller_user_name String,
	logon_type String,
	target_user_name String,
	user_agent String,
	user_name String,
	workstation String,

	-- web
	web_file String,
	web_method String,
	web_page String,
	web_protocol String
);


CREATE OR REPLACE STREAM activeguard_stream OF activeguard_parsed;

CREATE CQ Parseactiveguard_stream
INSERT INTO activeguard_stream
SELECT

	-- Enriched, or not in AGI left navigation
	data.get('hash_key').textValue() as hash_key,
	data.get('client_id').longValue() as client_id,
	data.get('event_type_id').longValue() as event_type_id,
	data.get('type_code').textValue() as type_code,
	data.get('rule_id').longValue() as rule_id,
	data.get('rule_name').textValue() as rule_name,
	data.get('rule_action').textValue() as rule_action,
	data.get('sce_id').longValue() as sce_id,
	data.get('message').textValue() as message,
	data.get('ref_list_match').textValue() as ref_list_match,
	TO_DATE(data.get('event_date').textValue()) as event_date,
	data.get('log_count').longValue() as log_count,

	-- As seen in AGI
	-- frequently used, but not duplicated
	TO_DATE(data.get('received_date').textValue()) as received_date,
	data.get('event_sub_source').textValue() as event_sub_source,

	-- database
	data.get('sol_action').textValue() as sol_action,
	data.get('comment').textValue() as comment,
	data.get('comment_text').textValue() as comment_text,
	data.get('database_name').textValue() as database_name,
	data.get('database_user_name').textValue() as database_user_name,
	data.get('entry_id').textValue() as entry_id,
	data.get('obj_name').textValue() as obj_name,
	data.get('os_user_name').textValue() as os_user_name,
	data.get('owner').textValue() as owner,
	data.get('priv_used').textValue() as priv_used,
	data.get('return_code').textValue() as return_code,
	data.get('session_id').textValue() as session_id,

	-- geo
	data.get('destination_country_code').textValue() as destination_country_code,
	data.get('source_country_code').textValue() as source_country_code,

	-- network
	data.get('data_size').textValue() as data_size,
	data.get('destination').textValue() as destination,
	data.get('destination_object').textValue() as destination_object,
	data.get('destination_port').textValue() as destination_port,
	data.get('sol_domain').textValue() as sol_domain,
	data.get('event_manager').textValue() as event_manager,
	data.get('event_path').textValue() as event_path,
	data.get('event_source').textValue() as event_source,
	data.get('msg_no').textValue() as msg_no,
	data.get('protocol_id').textValue() as protocol_id,
	data.get('service').textValue() as service,
	data.get('sol_source').textValue() as sol_source,
	data.get('source_mac_address').textValue() as source_mac_address,
	data.get('source_object').textValue() as source_object,
	data.get('source_port').textValue() as source_port,
	data.get('status_code').textValue() as status_code,
	data.get('target_path').textValue() as target_path,

	-- other
	data.get('command').textValue() as command,

	-- security
	data.get('affected_file').textValue() as affected_file,
	data.get('object_action').textValue() as object_action,
	data.get('object_action_result').textValue() as object_action_result,
	data.get('object_id').textValue() as object_id,
	data.get('object_name').textValue() as object_name,
	data.get('object_path').textValue() as object_path,
	data.get('process_name').textValue() as process_name,

	-- user
	data.get('caller_user_name').textValue() as caller_user_name,
	data.get('logon_type').textValue() as logon_type,
	data.get('target_user_name').textValue() as target_user_name,
	data.get('user_agent').textValue() as user_agent,
	data.get('user_name').textValue() as user_name,
	data.get('workstation').textValue() as workstation,

	-- web
	data.get('web_file').textValue() as web_file,
	data.get('web_method').textValue() as web_method,
	data.get('web_page').textValue() as web_page,
	data.get('web_protocol').textValue() as web_protocol

FROM activeguard_rawstream;



CREATE TARGET alertf USING FileWriter (
  filename: 'JParser',
  flushinterval: '0',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy: 'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00'
 )
 FORMAT USING JSONFormatter (
		members:'hash_key,client_id,event_type_id,type_code,rule_id,rule_name,rule_action,sce_id,message,ref_list_match,event_date,log_count,received_date,event_sub_source,sol_action,comment,comment_text,database_name,database_user_name,entry_id,obj_name,os_user_name,owner,priv_used,return_code,session_id,destination_country_code,source_country_code,data_size,destination,destination_object,destination_port,sol_domain,event_manager,event_path,event_source,msg_no,protocol_id,service,sol_source,source_mac_address,source_object,source_port,status_code,target_path,command,affected_file,object_action,object_action_result,object_id,object_name,object_path,process_name,caller_user_name,logon_type,target_user_name,user_agent,user_name,workstation,web_file,web_method,web_page,web_protocol',
 		rowdelimiter:'\n'
 )
INPUT FROM activeguard_stream;

create Target z using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/Target_JParser_actual.log') input from activeguard_stream;


END APPLICATION DSV;

use RetailTester;
DROP FLOW RetailSourceFlow cascade;

STOP APPLICATION HW ;
undeploy application HW ;
drop application HW cascade;

CREATE APPLICATION HW Recovery 5 second interval;

CREATE  SOURCE S USING OracleReader  ( 
  Username: 'miner',
  Password: '@miner',
  ConnectionURL: '@conn-url@',
  Tables: '@src@',
  FetchSize: 1) 
OUTPUT TO hivestream;

Create Target T using ClouderaHiveWriter (
  ConnectionURL:'@hive-url@',
  Username:'@uname@', 
            Password:'@pwd@',
            hadoopurl:'hdfs://dockerhost:9000/',
	        Mode:'incremental',
	        mergepolicy: 'eventcount:5,interval:1s',
            Tables:'@tgt-table@',
            hadoopConfigurationPath:'/Users/saranyad/Documents/hello/'
 )
INPUT FROM hivestream;


END APPLICATION HW;
deploy application HW on all in default;

Start application HW;

--
-- Recovery Test 1
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--


UNDEPLOY APPLICATION NameT00.T00;
DROP APPLICATION NameT00.T00 CASCADE;
CREATE APPLICATION T00 RECOVERY 5 SECOND INTERVAL;




CREATE FLOW DataAcquisitionT00;


CREATE SOURCE CsvSourceT00 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO OutputStreamT00;


END FLOW DataAcquisitionT00;




CREATE FLOW DataProcessingT00;


Create Target OutputTargetT00
Using Sysout (name: 'OutputTargetT00')
Input From OutputStreamT00;


END FLOW DataProcessingT00;



END APPLICATION T00;

stop application GCSWriterTest;
undeploy application GCSWriterTest;
drop application GCSWriterTest cascade;
create application GCSWriterTest recovery 1 second interval;
create source GCS_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

create Target GCSTarget using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@',
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from CsvStream;

end application GCSWriterTest;
deploy application GCSWriterTest on all in default;
start application GCSWriterTest;

CREATE OR REPLACE TARGET  @TARGET_NAME@ USING CassandraCosmosDBWriter  (
  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --CheckPointTable: '',
  FlushPolicy: 'EventCount:1,Interval:60',
  AccountEndpoint: 'cassandracosmostest.cassandra.cosmos.azure.com',
  AccountKey: 'pqDZvVgbdSCg7VzIzD77dAhPG2odGRZPLhAQA1qnZbAKoIDk6RuQX5r2phbRQFnR1l54qxOcvBXNdz8DeijYIg==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  --ConsistencyLevel:'fh',
  --port:'10505',
  Tables: 'QATEST.Source1,test.target1',
  --Tables: 'QATEST.OracToCql_alldatatypes,test.tgt_data',
  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',
  adapterName: 'CassandraCosmosDBWriter'
 )
 INPUT FROM @STREAM@;

STOP banker.bankApp;
UNDEPLOY APPLICATION banker.bankApp;
DROP APPLICATION banker.bankApp cascade;

CREATE APPLICATION bankApp;


CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;

CREATE TYPE cacheType( merchantId String, 
			hourValue int, 
			hourlyAve int);

CREATE TYPE wsData
(
bankID Integer KEY,
bankName String
);

CREATE CACHE adhcache using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'banks.csv',
  header: Yes,
  columndelimiter: ',',
  trimquote:false
) QUERY (keytomap:'bankID') OF wsData;


CREATE STREAM wsStream OF wsData;

--Select data from QaStream and insert into wsStream

CREATE CQ csvTowsData
INSERT INTO wsStream
SELECT TO_INT(data[0]),data[1] FROM QaStream;

--create jumping window over data in wsStream

CREATE WACTIONSTORE oneWS
CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@

--get data from wsStream and place into wactionStore oneWS
CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT * FROM wsStream
LINK SOURCE EVENT;


END APPLICATION bankApp;

STOP APPLICATION snow2FW;
UNDEPLOY APPLICATION snow2FW;
DROP APPLICATION snow2FW CASCADE;
CREATE OR REPLACE APPLICATION snow2FW;

CREATE OR REPLACE SOURCE snow_fw USING Global.ServiceNowReader (
  Mode: 'InitialLoad',
  ServiceNow.ConnectionTimeOut: 60,
  ServiceNow.MaxConnections: 20,
  ServiceNow.FetchSize: 10000,
  ThreadPoolCount: '10',
  ServiceNow.ConnectionRetries: 3,
  PollingInterval: '1',
  ClientSecret: '6Wa-cv`I7x',
  Password: '^Pre&$EMO%6O.e_{96h+$R?rJd,=[4Vt=K)Szh?6g<J9D3,3zs8R;hpZqh]-3?C&.u-@GvSakPXH1:2eygbBDI>ou-z#GjBw[u8x',
  ServiceNow.Tables: 'u_empl',
  UserName: 'snr',
  ClientID: 'ce4fd5af894a11103d2c5c3a8fe075e1',
  adapterName: 'ServiceNowReader',
  ServiceNow.BatchAPI: true,
  ServiceNow.ConnectionUrl: 'https://dev84954.service-now.com/' )
OUTPUT TO sn;


CREATE TARGET ft USING Global.FileWriter (
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000,Interval:30s',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  DataEncryptionKeyPassphrase_encrypted: 'true',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  filename: 'dt' )
FORMAT USING Global.JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n' )
INPUT FROM sn;

CREATE TARGET pg_target USING Global.DatabaseWriter (

  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
    Password: 'w@ct10n',
    Tables: 'u_empl,u_empl ColumnMap(name=u_name,age=u_age,address=u_address,sys_id=sys_id)',
    ParallelThreads: '',
    CheckPointTable: 'CHKPOINT',
    CDDLAction: 'Process',
    ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
    CommitPolicy: 'EventCount:1000,Interval:60',
    StatementCacheSize: '50',
    Username: 'waction',
    DatabaseProviderType: 'Postgres',
    BatchPolicy: 'EventCount:1000,Interval:60',
    PreserveSourceTransactionBoundary: 'false' )
  INPUT FROM sn;

END APPLICATION snow2FW;
deploy application snow2FW;
start snow2FW;

--
-- Recovery Test 23 with two sources, two sliding time windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> St1W -> CQ1 -> WS
-- S2 -> St2W -> CQ2 -> WS
--

STOP Recov23Tester.RecovTest23;
UNDEPLOY APPLICATION Recov23Tester.RecovTest23;
DROP APPLICATION Recov23Tester.RecovTest23 CASCADE;
CREATE APPLICATION RecovTest23 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 1 SECOND;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 2 SECOND;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION RecovTest23;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;


CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',
					acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

CREATE OR REPLACE STREAM @APPNAME@_stream OF Global.waevent persist using @APPNAME@_KafkaPropset;

CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @APPNAME@_stream2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNEC4TION_URL@',
  Tables: '$table1',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'QATEST.ORCTOMSSQLPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo) input from @STREAM@;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @APPNAME@_stream3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '$table2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@4',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'QATEST.ORCTOMSSQLPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@3;

END APPLICATION @APPNAME@2;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;
Create Source oracSource
 Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;

CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

import com.webaction.proc.events.StringArrayEvent;
import com.webaction.proc.events.WAEvent;

STOP FunctionTester.BuiltInFunctionApp;
UNDEPLOY APPLICATION FunctionTester.BuiltInFunctionApp;
DROP APPLICATION FunctionTester.BuiltInFunctionApp cascade;

CREATE APPLICATION BuiltInFunctionApp;


CREATE source rawSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'csvdata.txt',
  positionByEOF:false
)
PARSE USING DSVParser
(
   header:False,
   columndelimiter:','
) OUTPUT TO rawStream;


CREATE TYPE rawDataType(
  level String,
  floatnumber float,
  doublenumber double,
  intvalue  int,
  ourEvent  WAEvent
);


CREATE STREAM testStream OF rawDataType ;


-- Create a WAEvent out of the first to items, using makeWAEvent makeWAEvent(data[0], data[1])
CREATE CQ testCQ
  INSERT INTO testStream
  SELECT data[0], ROUND_FLOAT(data[1],4), ROUND_DOUBLE(data[2],0), TO_INT(data[3]), makeWAEvent(data[0], data[1])
FROM rawStream;

CREATE OR REPLACE Target BuiltFuncTarget using LogWriter(name:BuiltFunc,filename:'@FEATURE-DIR@/logs/BuiltInFuncResults') input from testStream;


CREATE TYPE waEventDataType
(
  level String,
  floatnumber float
);

CREATE STREAM waEventStream OF waEventDataType;

-- Parse the WAEvent created by the CQ testCQ and store the values
CREATE CQ otherCQ
  INSERT INTO waEventStream
  SELECT TO_STRING(VALUE(ourEvent,0)), ROUND_FLOAT(VALUE(ourEvent,1),4)
FROM testStream;


CREATE OR REPLACE Target BuiltFuncWAEventTarget using LogWriter(name:BuiltFunc,filename:'@FEATURE-DIR@/logs/BuiltInFuncResultsWAEvent') input from waEventStream;



CREATE TYPE stringArrayEventDataType
(
  level String,
  floatnumber float,
  doublenumber double,
  intvalue  int,
  stringArrayEvent StringArrayEvent
);

CREATE STREAM stringArrayEventStream OF stringArrayEventDataType;

-- Create a StringArrayEvent from the raw stream
-- We can't use the target below to verify, as it's generating a timestamp field, based on system generated time
CREATE CQ stringArrayEventCQ
  INSERT INTO stringArrayEventStream
  SELECT data[0], ROUND_FLOAT(data[1],4), ROUND_DOUBLE(data[2],0), TO_INT(data[3]), makeStringArrayEvent(data[0], data[2], data[3])
FROM rawStream;


CREATE OR REPLACE Target BuiltFuncSimpleEventTarget using LogWriter(name:BuiltFunc,filename:'@FEATURE-DIR@/logs/BuiltInFuncResultsStringArrayEvent') input from stringArrayEventStream;


END APPLICATION BuiltInFunctionApp;

use RetailTester;
alter application RetailApp;

CREATE FLOW RetailSourceFlow;

-- RetailDataSource is the primary data source for this application.
--
-- ParseOrderData discards the fields not needed by this application and puts the
-- data into the appropriate Java types.
--
-- ParseOrderData outputs to RetailOrders stream, the start point of the
-- RetailProductFlow and RetailStoreFlow flows.

CREATE SOURCE RetailDataSource USING CSVReader (
  directory:'Samples/Customer/RetailApp/appData',
  header:Yes,
  wildcard:'retaildata2M.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO Orders;

-- A stream's type must be declared before the stream, and a CQ's
-- output stream must be defined before the CQ. Hence type-stream-CQ
-- sequences like the following are very common.

-- output for ParseOrderData
CREATE TYPE OrderType(
  storeId      String,
  orderId      String,
  sku          String,
  orderAmount  double,
  dateTime     DateTime,
  hourValue    int,
  state        String,
  city         String,
  zip          String
);
CREATE STREAM RetailOrders Of OrderType;

CREATE CQ ParseOrderData
INSERT INTO RetailOrders
SELECT  data[0],
        data[6],
        data[7],
        TO_DOUBLE(SRIGHT(data[8],1)),
        TO_DATE(data[9],'yyyyMMddHHmmss'),
        DHOURS(TO_DATE(data[9],'yyyyMMddHHmmss')),
        data[3],
        data[2],
        data[4]
FROM Orders;

END FLOW RetailSourceFlow;

end application RetailApp;

alter application RetailApp recompile;

stop APPLICATION RollOverTester.DSV;

UNDEPLOY APPLICATION RollOverTester.DSV;
DROP APPLICATION RollOverTester.DSV CASCADE;
CREATE APPLICATION DSV;

create source JsonSource using FileReader (
        directory:'@TEST-DATA-PATH@',
        WildCard:'TextInput.txt',
        positionByEOF:false,
        charset:'UTF-8'
)
 PARSE USING JSONParser ( 
  eventType: ''
 ) 
OUTPUT TO activeguard_rawstream;


CREATE OR REPLACE TYPE activeguard_parsed (
	-- Enriched, or not in AGI left navigation
	hash_key String KEY,
	client_id Long,
	event_type_id Long,
	type_code String,
	rule_id Long,
	rule_name String,
	rule_action String,
	sce_id  Long,
	message String,
	ref_list_match String,
	event_date DateTime,
	log_count Long,
	
	-- As seen in AGI
	-- frequently used, but not duplicated
	received_date DateTime,
	event_sub_source String,
					
	-- database
	sol_action String,
	comment String,
	comment_text String,
	database_name String,
	database_user_name String,
	entry_id String,
	obj_name String,
	os_user_name String,
	owner String,
	priv_used String,
	return_code String,
	session_id String,
	
	-- geo
	destination_country_code String,
	source_country_code String,
	
	-- network
	data_size String,
	destination String,
	destination_object String,
	destination_port String,
	sol_domain String,
	event_manager String,
	event_path String,
	event_source String,
	msg_no String,
	protocol_id String,
	service String,
	sol_source String,
	source_mac_address String,
	source_object String,
	source_port String,
	status_code String,
	target_path String,
	
	-- other
	command String,
	
	-- security
	affected_file String,
	object_action String,
	object_action_result String,
	object_id String,
	object_name String,
	object_path String,
	process_name String,
	
	-- user
	caller_user_name String,
	logon_type String,
	target_user_name String,
	user_agent String,
	user_name String,
	workstation String,
	
	-- web
	web_file String,
	web_method String,
	web_page String,
	web_protocol String
);


CREATE OR REPLACE STREAM activeguard_stream OF activeguard_parsed;

CREATE CQ Parseactiveguard_stream 
INSERT INTO activeguard_stream
SELECT

	-- Enriched, or not in AGI left navigation
	data.get('hash_key').textValue() as hash_key,
	data.get('client_id').longValue() as client_id,
	data.get('event_type_id').longValue() as event_type_id,
	data.get('type_code').textValue() as type_code,
	data.get('rule_id').longValue() as rule_id,
	data.get('rule_name').textValue() as rule_name,
	data.get('rule_action').textValue() as rule_action,
	data.get('sce_id').longValue() as sce_id,
	data.get('message').textValue() as message,
	data.get('ref_list_match').textValue() as ref_list_match,
	TO_DATE(data.get('event_date').textValue()) as event_date,
	data.get('log_count').longValue() as log_count,
	
	-- As seen in AGI
	-- frequently used, but not duplicated
	TO_DATE(data.get('received_date').textValue()) as received_date,
	data.get('event_sub_source').textValue() as event_sub_source,
					
	-- database
	data.get('sol_action').textValue() as sol_action,
	data.get('comment').textValue() as comment,
	data.get('comment_text').textValue() as comment_text,
	data.get('database_name').textValue() as database_name,
	data.get('database_user_name').textValue() as database_user_name,
	data.get('entry_id').textValue() as entry_id,
	data.get('obj_name').textValue() as obj_name,
	data.get('os_user_name').textValue() as os_user_name,
	data.get('owner').textValue() as owner,
	data.get('priv_used').textValue() as priv_used,
	data.get('return_code').textValue() as return_code,
	data.get('session_id').textValue() as session_id,
	
	-- geo
	data.get('destination_country_code').textValue() as destination_country_code,
	data.get('source_country_code').textValue() as source_country_code,
	
	-- network
	data.get('data_size').textValue() as data_size,
	data.get('destination').textValue() as destination,
	data.get('destination_object').textValue() as destination_object,
	data.get('destination_port').textValue() as destination_port,
	data.get('sol_domain').textValue() as sol_domain,
	data.get('event_manager').textValue() as event_manager,
	data.get('event_path').textValue() as event_path,
	data.get('event_source').textValue() as event_source,
	data.get('msg_no').textValue() as msg_no,
	data.get('protocol_id').textValue() as protocol_id,
	data.get('service').textValue() as service,
	data.get('sol_source').textValue() as sol_source,
	data.get('source_mac_address').textValue() as source_mac_address,
	data.get('source_object').textValue() as source_object,
	data.get('source_port').textValue() as source_port,
	data.get('status_code').textValue() as status_code,
	data.get('target_path').textValue() as target_path,
	
	-- other
	data.get('command').textValue() as command,
	
	-- security
	data.get('affected_file').textValue() as affected_file,
	data.get('object_action').textValue() as object_action,
	data.get('object_action_result').textValue() as object_action_result,
	data.get('object_id').textValue() as object_id,
	data.get('object_name').textValue() as object_name,
	data.get('object_path').textValue() as object_path,
	data.get('process_name').textValue() as process_name,
	
	-- user
	data.get('caller_user_name').textValue() as caller_user_name,
	data.get('logon_type').textValue() as logon_type,
	data.get('target_user_name').textValue() as target_user_name,
	data.get('user_agent').textValue() as user_agent,
	data.get('user_name').textValue() as user_name,
	data.get('workstation').textValue() as workstation,
	
	-- web
	data.get('web_file').textValue() as web_file,
	data.get('web_method').textValue() as web_method,
	data.get('web_page').textValue() as web_page,
	data.get('web_protocol').textValue() as web_protocol
	
FROM activeguard_rawstream;


CREATE TARGET alertf USING FileWriter ( directory: '@FEATURE-DIR@/logs', filename: 'TestOutput.txt', rolloverpolicy:'EventCount:10000,Interval:30s' )
 FORMAT USING JSONFormatter (
		members:'hash_key,client_id,event_type_id,type_code,rule_id,rule_name,rule_action,sce_id,message,ref_list_match,event_date,log_count,received_date,event_sub_source,sol_action,comment,comment_text,database_name,database_user_name,entry_id,obj_name,os_user_name,owner,priv_used,return_code,session_id,destination_country_code,source_country_code,data_size,destination,destination_object,destination_port,sol_domain,event_manager,event_path,event_source,msg_no,protocol_id,service,sol_source,source_mac_address,source_object,source_port,status_code,target_path,command,affected_file,object_action,object_action_result,object_id,object_name,object_path,process_name,caller_user_name,logon_type,target_user_name,user_agent,user_name,workstation,web_file,web_method,web_page,web_protocol',
 		rowdelimiter:'\n'
 )
--format using dsvformatter()
 INPUT FROM activeguard_stream;

 
END APPLICATION DSV;

--
-- Recovery Test 37 with two sources, two jumping time windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jt1W/p -> CQ1 -> WS
--   S2 -> Jt2W/p -> CQ2 -> WS
--

STOP KStreamRecov37Tester.KStreamRecovTest37;
UNDEPLOY APPLICATION KStreamRecov37Tester.KStreamRecovTest37;
DROP APPLICATION KStreamRecov37Tester.KStreamRecovTest37 CASCADE;

DROP USER KStreamRecov37Tester;
DROP NAMESPACE KStreamRecov37Tester CASCADE;
CREATE USER KStreamRecov37Tester IDENTIFIED BY KStreamRecov37Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov37Tester;
CONNECT KStreamRecov37Tester KStreamRecov37Tester;

CREATE APPLICATION KStreamRecovTest37 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 1 SECOND
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 2 SECOND
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION KStreamRecovTest37;

--
-- Recovery Test 31 with two sources, two sliding count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5W/p -> CQ1 -> WS
-- S2 -> Sc6W/p -> CQ2 -> WS
--

STOP Recov31Tester.RecovTest31;
UNDEPLOY APPLICATION Recov31Tester.RecovTest31;
DROP APPLICATION Recov31Tester.RecovTest31 CASCADE;
CREATE APPLICATION RecovTest31 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION RecovTest31;

stop ORAToBigquery;
undeploy application ORAToBigquery;
drop application ORAToBigquery cascade;
CREATE APPLICATION ORAToBigquery RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE Rac11g USING OracleReader ( 
  SupportPDB: false,
  SendBeforeImage: true,
  ReaderType: 'LogMiner',
  CommittedTransactions: false,
  FetchSize: 1,
  Password: 'manager',
  DDLTracking: false,
  StartTimestamp: 'null',
  OutboundServerProcessName: 'WebActionXStream',
  OnlineCatalog: true,
  ConnectionURL: '192.168.33.10:1521/XE',
  SkipOpenTransactions: false,
  Compression: false,
  QueueSize: 40000,
  RedoLogfiles: 'null',
  Tables: 'SYSTEM.GGAUTHORIZATIONS',
  Username: 'system',
  FilterTransactionBoundaries: true,
  adapterName: 'OracleReader',
  XstreamTimeOut: 600,
  connectionRetryPolicy: 'timeOut=30, retryInterval=60, maxRetries=3'
 ) 
OUTPUT TO DataStream;
CREATE OR REPLACE TARGET Target1 USING SysOut ( 
  name: "dstream"
 ) 
INPUT FROM DataStream;
CREATE OR REPLACE TARGET Target2 USING BigqueryWriter  ( 
  BQServiceAccountConfigurationPath: '/Users/ravipathak/Downloads/abc.json',
  projectId: 'big-querytest',
  Tables: 'SYSTEM.GGAUTHORIZATIONS,testing1.ggauthorisation',
  parallelismCount: 2,
  BatchPolicy: 'eventCount:100000,Interval:0'
 ) 
INPUT FROM DataStream;
END APPLICATION ORAToBigquery;
deploy application ORAToBigquery;
start ORAToBigquery;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;

Create Source @SourceName@ Using OracleReader

(
  Username:'c##qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',
  Tables:'CDB$ROOT."C##QATEST".ojet_src;ORCLPDB.QATEST.ojet_src',
  _h_useClassic:false,
  Fetchsize:1,
  Compression: true,
  SupportPDB:true,
  ReplicationSlotName:'null'
)
Output To @SRCINPUTSTREAM@;

CREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'))) FROM @SRCINPUTSTREAM@ x; ;


CREATE TARGET @targetName@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',
  Username:'c##qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'CDB$ROOT."C##QATEST".ojet_src,CDB$ROOT."C##QATEST".ojet_tgt'
) INPUT FROM admin.sqlreader_cq_out;



create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

CREATE STREAM PosDataStream OF PosData;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;


CREATE TARGET DumpFileStream USING LogWriter(
name:testOuput1,
filename:'@FEATURE-DIR@/logs/TQLwithinTQL-console-out-RT.log'
) INPUT FROM PosDataStream;

CREATE APPLICATION tungstenAppNoComments;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)

PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;


CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startingTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
@PERSIST-TYPE@


CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false,
  trimwhitespace:true
) 
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'zip') OF USAddressData;


CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;
        
END APPLICATION tungstenAppNoComments;

create application KinesisTest RECOVERY 1 SECOND INTERVAL;
CREATE OR REPLACE SOURCE ora_reader USING OracleReader (
  Username: 'miner',
  Password: 'miner',
  ConnectionURL: '192.168.1.113:1521:ORCL',
  TABLES: 'QATEST.H_REGION;QATEST.H_NATION;QATEST.H_CUSTOMER',
  FetchSize: '1'
 )
OUTPUT TO DDLCDCStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	accesskeyid:'ACCESS_KEY',
	secretaccesskey:'SECRET_KEY'
)
format using JSONFormatter (
)
input from DDLCDCStream;

create or replace Target t2 using KinesisWriter (
  regionName:'TARGET_REGION',
  streamName:'TARGET_STREAM',
  accesskeyid:'ACCESS_KEY',
  secretaccesskey:'SECRET_KEY'
)
format using JSONFormatter (
)
input from DDLCDCStream;

end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  IgnorableExceptionCode: '1,TABLE_NOT_FOUND'
 )
INPUT FROM @STREAM@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.e1ptest%',
	FetchSize: '1'
)
OUTPUT TO @APPNAME@_SS;


CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.e1ptest%,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12)',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'Interval:2',
StandardSQL:true,
optimizedMerge:true		
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

--
-- Crash Recovery Test 5 with Jumping window and partitioned on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION N2S2CR5Tester.N2S2CRTest5;
UNDEPLOY APPLICATION N2S2CR5Tester.N2S2CRTest5;
DROP APPLICATION N2S2CR5Tester.N2S2CRTest5 CASCADE;
CREATE APPLICATION N2S2CRTest5 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest5;

CREATE SOURCE CsvSourceN2S2CRTest5 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN2S2CRTest5;

CREATE FLOW DataProcessingN2S2CRTest5;

CREATE TYPE CsvDataTypeN2S2CRTest5 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvDataTypeN2S2CRTest5 PARTITION BY merchantId;

CREATE CQ CsvToDataN2S2CRTest5
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsN2S2CRTest5 CONTEXT OF CsvDataTypeN2S2CRTest5
EVENT TYPES ( CsvDataTypeN2S2CRTest5 )
@PERSIST-TYPE@

CREATE CQ DataToWactionN2S2CRTest5
INSERT INTO WactionsN2S2CRTest5
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN2S2CRTest5;

END APPLICATION N2S2CRTest5;

STOP cacheCase;
UNDEPLOY APPLICATION cacheCase;
DROP APPLICATION cacheCase cascade;

CREATE APPLICATION cacheCase;


CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity double,
  size long,
  currentDate DateTime);

CREATE CACHE cAcHe1 USING CsvReader(
  directory: '@TEST-DATA-PATH@',
  wildcard: 'ISdata.csv',
  header: false,
  columndelimiter: ',',
  trimquote: false
  ) QUERY (keytomap:'PRODUCTID') OF Atm;


CREATE WACTIONSTORE WS1 CONTEXT OF Atm
EVENT TYPES
(Atm );


CREATE CQ cq1
INSERT INTO ws1
SELECT * FROM CACHE1;

END APPLICATION cacheCase;
DEPLOY APPLICATION cacheCase;
START cacheCase;