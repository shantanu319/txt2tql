stop application MySQLAllDataTypes;
undeploy application MySQLAllDataTypes;
drop application MySQLAllDataTypes CASCADE;
create application MySQLAllDataTypes;

CREATE OR REPLACE SOURCE MySQLSource USING MySQLReader  (
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Database:'@DATABASE@',
  Tables: '@SOURCE_TABLES@'
 ) Output To MySQLStream;
 
--create Target t2 using SysOut(name:Foo2) input from MySQLStream; 
CREATE TARGET RedshiftTarget USING RedshiftWriter
	(
	  ConnectionURL: '@TARGET-URL@',
	  Username: '@TARGET-UNAME@',
	  Password: '@TARGET-PASSWORD@',
	  bucketname: '@BUCKETNAME@',
	  accesskeyId: '@ACCESS-KEY-ID@',
	  secretaccesskey: '@SECRET-ACCESS-KEY@',
	  Tables: '@TARGET-TABLES@',
	  uploadpolicy:'eventcount:1,interval:20s',
	  Mode:'incremental'
	) INPUT FROM MySQLStream;
	
END APPLICATION MySQLAllDataTypes;
deploy application MySQLAllDataTypes;
START application MySQLAllDataTypes;

DROP APPLICATION @APP_NAME@1 FORCE;
DROP APPLICATION @APP_NAME@2 FORCE;
DROP APPLICATION @APP_NAME@3 FORCE;


CREATE APPLICATION @APP_NAME@1 WITH ENCRYPTION USE EXCEPTIONSTORE TTL : '7d';
CREATE OR REPLACE PROPERTYSET @APP_NAME@1_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',
acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');
CREATE STREAM @APP_NAME@_In1 OF Global.waevent persist using @APP_NAME@1_KafkaPropset;

CREATE OR REPLACE SOURCE @APP_NAME@1_Source USING @SOURCE_ADAPTER@  ( 
  Tables:'@SRC_TABLE@'
) OUTPUT TO @APP_NAME@_In1;

CREATE OR REPLACE TARGET @APP_NAME@1_Target USING @TARGET_ADAPTER@  ( 
  Tables: '@SRC_TABLE@, @TGT_TABLE@1', 
  CheckPointTable: '@CHK_TABLE@', 
) INPUT FROM @APP_NAME@_In1;

CREATE TARGET @APP_NAME@1_SysOut
USING SysOut(name:@APP_NAME@1Sys)
INPUT FROM @APP_NAME@_In1;

CREATE TARGET @APP_NAME@1_FileWriter USING filewriter
(filename:'@APP_NAME@_FW1.log'
)
format using dsvFormatter()
INPUT FROM @APP_NAME@_In1;

END APPLICATION @APP_NAME@1;

DEPLOY APPLICATION @APP_NAME@1;
START APPLICATION @APP_NAME@1;

CREATE APPLICATION @APP_NAME@2;

CREATE OR REPLACE SOURCE @APP_NAME@2_Source USING @SOURCE_ADAPTER@  ( 
  Tables:'@SRC_TABLE@'
) OUTPUT TO @APP_NAME@_In2;

CREATE OR REPLACE TARGET @APP_NAME@2_Target USING @TARGET_ADAPTER@  ( 
  Tables: '@SRC_TABLE@, @TGT_TABLE@2', 
  CheckPointTable: '@CHK_TABLE@'
) INPUT FROM @APP_NAME@_In2;

CREATE TARGET @APP_NAME@2_SysOut
USING SysOut(name:@APP_NAME@2Sys)
INPUT FROM @APP_NAME@_In2;

CREATE TARGET @APP_NAME@2_FileWriter USING filewriter
(filename:'@APP_NAME@_FW2.log'
)
format using dsvFormatter()
INPUT FROM @APP_NAME@_In2;

END APPLICATION @APP_NAME@2;

DEPLOY APPLICATION @APP_NAME@2;
START APPLICATION @APP_NAME@2;

CREATE APPLICATION @APP_NAME@3;
CREATE FLOW @APP_NAME@_SOURCEFLOW;
CREATE OR REPLACE SOURCE @APP_NAME@3_Source USING @SOURCE_ADAPTER@  ( 
  Tables:'@SRC_TABLE@'
) OUTPUT TO @APP_NAME@_In3;
END FLOW @APP_NAME@_SOURCEFLOW;

CREATE FLOW @APP_NAME@_TARGETFLOW;
CREATE OR REPLACE TARGET @APP_NAME@3_Target USING @TARGET_ADAPTER@  ( 
  Tables: '@SRC_TABLE@, @TGT_TABLE@3', 
  CheckPointTable: '@CHK_TABLE@', 
) INPUT FROM @APP_NAME@_In3;

CREATE TARGET @APP_NAME@3_SysOut
USING SysOut(name:@APP_NAME@3Sys)
INPUT FROM @APP_NAME@_In3;

CREATE TARGET @APP_NAME@3_FileWriter USING filewriter
(filename:'@APP_NAME@_FW3.log'
)
format using dsvFormatter()
INPUT FROM @APP_NAME@_In3;
END FLOW @APP_NAME@_TARGETFLOW;

END APPLICATION @APP_NAME@3;

DEPLOY APPLICATION @APP_NAME@3;
START APPLICATION @APP_NAME@3;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@;

--
-- Recovery Test 1
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--


UNDEPLOY APPLICATION NameM00.M00;
DROP APPLICATION NameM00.M00 CASCADE;
CREATE APPLICATION M00 RECOVERY 5 SECOND INTERVAL;




CREATE FLOW DataAcquisitionM00;


CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;


END FLOW DataAcquisitionM00;




CREATE FLOW DataProcessingM00;


CREATE TYPE WactionTypeM00 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE WACTIONSTORE WactionsM00 CONTEXT OF WactionTypeM00
EVENT TYPES ( WactionTypeM00 KEY(word) )
@PERSIST-TYPE@

CREATE CQ InsertWactionsM00
INSERT INTO WactionsM00
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStream;


END FLOW DataProcessingM00;



END APPLICATION M00;

STOP APPLICATION EH;
UNDEPLOY APPLICATION EH;
DROP APPLICATION EH CASCADE;
CREATE APPLICATION EH @Recovery@;
CREATE SOURCE s USING FileReader (
	directory:'Product/IntegrationTests/TestData/',
    WildCard:'posdata.csv',
	positionByEOF:false
	)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO ss;

CREATE TYPE userType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String
);

CREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;

CREATE CQ cq1
INSERT INTO userDefinedTypedStream
SELECT data[0],data[1],data[2], data[3],data[4]
FROM ss;

create Target t1 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	Partitionkey:'@metadata(RecordOffset)',
	ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using dsvFormatter()
input from ss;

create Target t2 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	Partitionkey:'Col1',
	ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using dsvFormatter()
input from userDefinedTypedStream;

END APPLICATION EH;
DEPLOY APPLICATION EH;
start application EH;

STOP APPLICATION ER;
UNDEPLOY APPLICATION ER;
DROP APPLICATION ER CASCADE;
CREATE APPLICATION ER;
CREATE SOURCE ER_S1 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:0	
	)
PARSE USING DSVParser (
)OUTPUT TO ER_SS1;
CREATE SOURCE ER_S2 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:0	
	)
PARSE USING DSVParser (
)OUTPUT TO ER_SS2;

CREATE TYPE CustType1(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
TopicName String,
PartitionID String
);

CREATE TYPE CustType2(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
TopicName String,
PartitionID String
);

CREATE Stream DSVReaderStream1 of CustType1;
CREATE Stream DSVReaderStream2 of CustType2;

CREATE CQ CustType_CQ1
INSERT INTO DSVReaderStream1
SELECT data[5],data[6],data[7],data[8],data[9],data[10],
metadata.get("TopicName").toString() AS TopicName,
metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS1;

CREATE CQ CustType_CQ2
INSERT INTO DSVReaderStream2
SELECT data[0],data[1],data[2],data[3],data[4],
metadata.get("TopicName").toString() AS TopicName,
metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS2;

create Target ER_t1 using FileWriter (
filename:'FT1_5L_DSV_RT',
directory:'FEATURE-DIR/logs/',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from DSVReaderStream1;

create Target ER_t2 using FileWriter (
filename:'FT2_5L_DSV_RT',
directory:'FEATURE-DIR/logs/',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from DSVReaderStream2;
end application ER;
deploy application ER;

STOP APPLICATION @WRITERAPPNAME@;
UNDEPLOY APPLICATION @WRITERAPPNAME@;
DROP APPLICATION @WRITERAPPNAME@ CASCADE;

CREATE APPLICATION @WRITERAPPNAME@ RECOVERY 1 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

CREATE SOURCE @SOURCE@ USING OracleReader
(
FetchSize:1,
Username:'@SOURCE_USER@',
Password:'85d7qFnwTW8=',
ConnectionURL:'@CONNECTION_URL@',
Tables:'@SOURCE_TABLE@',
password_encrypted: 'true'
)
OUTPUT TO @STREAM1@;


end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

CREATE OR REPLACE TYPE @TYPE@( 
datae java.util.HashMap , 
TABLE_NAME java.lang.String , 
OPS_NAME java.lang.String , 
DB_TIMESTAMP java.lang.String  ,
COMMITSCN java.lang.String ,
SCN java.lang.String ,
REC_INS_TIME java.lang.String );

CREATE CQ @CQ1@
INSERT INTO @STREAM2@
SELECT  
CASE WHEN (META(c,"OperationName").toString() == "DELETE")
THEN putUserData(c, 'isDelete', 'true') 
ELSE
putUserData(c,'isDelete', 'false')
END
FROM @STREAM1@ c;

CREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;

CREATE OR REPLACE CQ @CQ2@ 
INSERT INTO @STREAM3@
SELECT 
data(e),
META(e,"TableName").toString() as TABLE_NAME,
META(e, "OperationName").toString() as OPS_NAME,
META(e, "TimeStamp").toString() as DB_TIMESTAMP,
META(e,"COMMITSCN").toString() as COMMITSCN ,
META(e,"SCN").toString() as  SCN ,
DNOW().toString() as REC_INS_TIME
FROM @STREAM1@ e;

create Target @TARGET1@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC1',
ParallelThreads:'',
PartitionKey:'@metadata(TableName)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from @STREAM1@;


create Target @TARGET2@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC2',
ParallelThreads:'2',
PartitionKey:'TABLE_NAME',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from @STREAM3@;

create Target @TARGET3@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC3',
ParallelThreads:'',
PartitionKey:'@userdata(isDelete)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest.avsc')
input from @STREAM2@;

end application @WRITERAPPNAME@;
deploy application @WRITERAPPNAME@;
start @WRITERAPPNAME@;
stop application @READERAPPNAME@;
undeploy application @READERAPPNAME@;
drop application @READERAPPNAME@ cascade;
CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC1',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;


CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
filename:'@READERAPPNAME@_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream1;

CREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC2',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;


CREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC3',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest.avsc'
)
OUTPUT TO KafkaReaderStream3;

end flow @APPNAME@_serverflow;

end application @READERAPPNAME@;
deploy application @READERAPPNAME@;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;
DROP EXCEPTIONSTORE @APP_NAME@_EXCEPTIONSTORE;

CREATE APPLICATION @APP_NAME@ WITH ENCRYPTION RECOVERY 2 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE SOURCE @APP_NAME@_Source USING @SOURCE_ADAPTER@(
)OUTPUT TO @APP_NAME@DataStream;


CREATE TARGET @APP_NAME@_Target USING @TARGET_ADAPTER@( 
) INPUT FROM @APP_NAME@DataStream;


CREATE OR REPLACE TARGET @APP_NAME@_SysOut USING Global.SysOut(
	name: 'waEvent'
) INPUT FROM @APP_NAME@DataStream;

END APPLICATION @APP_NAME@;

DEPLOY APPLICATION @APP_NAME@ in default;
START APPLICATION @APP_NAME@;

--
-- Recovery Test 65
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ1 -> JWc10 -> CQ(aggregate) -> WS1
-- S -> CQ2 -> JWc11-> CQ(aggregate) -> WS2
--

STOP Recov65Tester.RecovTest65;
UNDEPLOY APPLICATION Recov65Tester.RecovTest65;
DROP APPLICATION Recov65Tester.RecovTest65 CASCADE;
CREATE APPLICATION RecovTest65 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStreamSize10
OVER DataStream KEEP 10 ROWS;

CREATE JUMPING WINDOW DataStreamSize11
OVER DataStream KEEP 11 ROWS;

CREATE WACTIONSTORE Wactions1 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions2 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions1
INSERT INTO Wactions1
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    TO_DATE(FIRST(p.dateTime)),
    TO_DOUBLE(FIRST(p.amount)),
    FIRST(p.city)
FROM DataStreamSize10 p;

CREATE CQ InsertWactions2
INSERT INTO Wactions2
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    TO_DATE(FIRST(p.dateTime)),
    TO_DOUBLE(FIRST(p.amount)),
    FIRST(p.city)
FROM DataStreamSize11 p;

create Target t1 using logwriter(name:Foo1, filename: output1) input from DataStreamSize10;
create Target t2 using logwriter(name:Foo2, filename: output2) input from DataStreamSize11;

END APPLICATION RecovTest65;

stop ORAToBigquery;
undeploy application ORAToBigquery;
drop application ORAToBigquery cascade;
CREATE APPLICATION ORAToBigquery RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE Rac11g USING OracleReader ( 
  SupportPDB: false,
  SendBeforeImage: true,
  ReaderType: 'LogMiner',
  CommittedTransactions: false,
  FetchSize: 1,
  Password: 'manager',
  DDLTracking: false,
  StartTimestamp: 'null',
  OutboundServerProcessName: 'WebActionXStream',
  OnlineCatalog: true,
  ConnectionURL: '192.168.33.10:1521/XE',
  SkipOpenTransactions: false,
  Compression: false,
  QueueSize: 40000,
  RedoLogfiles: 'null',
  Tables: 'SYSTEM.GGAUTHORIZATIONS',
  Username: 'system',
  FilterTransactionBoundaries: true,
  adapterName: 'OracleReader',
  XstreamTimeOut: 600,
  connectionRetryPolicy: 'timeOut=30, retryInterval=60, maxRetries=3'
 ) 
OUTPUT TO DataStream;
CREATE OR REPLACE TARGET Target1 USING SysOut ( 
  name: "dstream"
 ) 
INPUT FROM DataStream;
CREATE OR REPLACE TARGET Target2 USING BigqueryWriter  ( 
  BQServiceAccountConfigurationPath: '/Users/ravipathak/Downloads/abc.json',
  projectId: 'big-querytest',
  Tables: 'SYSTEM.GGAUTHORIZATIONS,testing1.ggauthorisation',
  parallelismCount: 2,
  BatchPolicy: 'eventCount:100000,Interval:0'
 ) 
INPUT FROM DataStream;
END APPLICATION ORAToBigquery;
deploy application ORAToBigquery;
start ORAToBigquery;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	accesskeyid:'ACCESS_KEY',
	secretaccesskey:'SECRET_KEY'
)
format using JSONFormatter (
)
input from DDLCDCStream;

stop application logminer;
undeploy application logminer;
drop application logminer cascade;

create application logminer;

Create Source Rac11g Using OracleReader
(
 --StartTimestamp:'15-JAN-2015 13:00:40',
 Username:'miner',
 Password:'miner',
 ConnectionURL:'10.1.110.128:1521:orcl',
 --Tables:'SCOTT.SIMPLETEST',
 Tables:'QATEST.SAMPLETEST2',
 OnlineCatalog:true,
 FetchSize:1,
 QueueSize:2148,
 CommittedTransactions:false,
 Compression:false,
 FilterTransactionState:false
)
Output To LCRStream;

create target myout using sysout(name: logminer) input from LCRStream;
create Target y using logwriter(name:GitCommitInfo,filename:logminer) input from LCRStream;

end application logminer;
deploy application logminer;
start application logminer;

--
-- Recovery Test 1
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov1Tester.RecovTest1;
UNDEPLOY APPLICATION Recov1Tester.RecovTest1;
DROP APPLICATION Recov1Tester.RecovTest1 CASCADE;
CREATE APPLICATION RecovTest1 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END APPLICATION RecovTest1;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 _h_returnDateTimeAs: 'ZonedDateTime',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING AvroParser ()
OUTPUT TO @APPNAME@_Stream;

CREATE CQ @APPNAME@_CQ
INSERT INTO @APPNAME@_CQOut
SELECT AvroToJson(data,false) FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_CQOut;

END APPLICATION @APPNAME@;

stop application RedshiftColmap;
undeploy application RedshiftColmap;
drop application RedshiftColmap CASCADE;
create application RedshiftColmap recovery 1 second interval;

CREATE OR REPLACE SOURCE OracleSource USING OracleReader  (
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: '@SOURCE_TABLES@',
  FetchSize: 1
 ) Output To LogminerStream;
 
--create Target t2 using SysOut(name:Foo2) input from LogminerStream; 
 
CREATE TARGET RedshiftTarget USING RedshiftWriter
	(
	  ConnectionURL: '@TARGET-URL@',
	  Username: '@TARGET-UNAME@',
	  Password: '@TARGET-PASSWORD@',
	  bucketname: '@BUCKETNAME@',
	  --accesskeyId: '@ACCESS-KEY-ID@',
	  --secretaccesskey: '@SECRET-ACCESS-KEY@',
	  S3IAMRole:'@IAMROLE@',
	  Tables: '@TARGET-TABLES@',
	  uploadpolicy:'eventcount:5,interval:10s',
	  Mode:'incremental'
	) INPUT FROM LogminerStream;
	
END APPLICATION RedshiftColmap;
deploy application RedshiftColmap;
START application RedshiftColmap;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@ recovery 5 SECOND Interval;
CREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;


CREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:30s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

--
-- Recovery Test 21 with two sources, two sliding count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5W -> CQ1 -> WS
-- S2 -> Sc6W -> CQ2 -> WS
--

STOP KStreamRecov21Tester.KStreamRecovTest21;
UNDEPLOY APPLICATION KStreamRecov21Tester.KStreamRecovTest21;
DROP APPLICATION KStreamRecov21Tester.KStreamRecovTest21 CASCADE;
DROP USER KStreamRecov21Tester;
DROP NAMESPACE KStreamRecov21Tester CASCADE;
CREATE USER KStreamRecov21Tester IDENTIFIED BY KStreamRecov21Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov21Tester;
CONNECT KStreamRecov21Tester KStreamRecov21Tester;

CREATE APPLICATION KStreamRecovTest21 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION KStreamRecovTest21;

stop application app1PS;
undeploy application app1PS;
drop application app1PS cascade;

create application app1PS;

create target File_TargerPS using FileWriter
(
directory : '',
filename : ''
)
format using DSVFormatter()
input from Recoveryss1;

end application app1PS;

deploy application app1PS;
start application app1PS;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@;

CREATE OR REPLACE TYPE @APPNAME@_Type (
 HEADER java.lang.String,
 DETAILNode com.fasterxml.jackson.databind.JsonNode
 );

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '@GRPPOLICY@',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'MultipleEvents',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE OR REPLACE CQ @APPNAME@_CQ
INSERT INTO @APPNAME@_CQOut
SELECT data.get('HEADER-RECORD').get('DSD-BATCH').textvalue() as HEADER,
data.get('DETAIL-RECORD') as DETAILNode FROM CCBStream c;

CREATE OR REPLACE CQ @APPNAME@_CQ1
INSERT INTO @APPNAME@_CQOut1
SELECT HEADER,
DETAILNode.get('REC-TYPE').textvalue() as DRECTYPE,
DETAILNode.get('AP-VENDOR').textvalue() as DAPVENDOR,
DETAILNode.get('FACILITY').textvalue() as DFACILITY,
DETAILNode.get('INVOICE-NUM').textvalue() as DINVOICENUM,
DETAILNode.get('DIV').textvalue() as DDIV,
DETAILNode.get('BILLING-COST').doubleValue() as DBILLINGCOST,
DETAILNode.get('BILLING-RETAIL').doubleValue() as DBILLINGRETAIL,
DETAILNode.get('TAX-AMOUNT').doubleValue() as DTAXAMOUNT,
DETAILNode.get('CASH-DISCOUNT').doubleValue() as DCASHDISCOUNT
FROM @APPNAME@_CQOut c, iterator(c.DETAILNode) DETAILNode;;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
  filename: '%@metadata(FileName)%',
  directory: '@DIR@',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter  ()
INPUT FROM @APPNAME@_CQOut1;

CREATE OR REPLACE TARGET OracleTarget USING DatabaseWriter (
  ConnectionURL: '', 
  Password: '', 
  Username: '',
  Tables: '',  
  CommitPolicy: 'EventCount:10,Interval:10', 
  BatchPolicy: 'EventCount:10,Interval:10'
  )
INPUT FROM @APPNAME@_CQOut1;

CREATE TARGET BigQueryTarget USING BigQueryWriter (
  Tables: '',
  projectId:'',
  BatchPolicy: 'eventCount:1, Interval:1',
  ServiceAccountKey: '',
)
INPUT FROM @APPNAME@_CQOut1;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING MySqlReader
(
  Compression: false,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'mysql://localhost:3306',
  DatabaseName: 'waction',
  Tables: 'waction.test01',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root',
  connectionRetryPolicy:'retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'waction.test01,@DATASET@.% KEYCOLUMNS(col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;

--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

STOP APPLICATION oracletokudu;
UNDEPLOY APPLICATION oracletokudu;
DROP APPLICATION oracletokudu CASCADE;
CREATE APPLICATION oracletokudu;
Create Type CSVType (
	companyname String,
  merchantName String
);

Create Stream TypedFileStream of CSVType;

create source CSVSource using FileReader (
	directory:'/Users/jenniffer/Product2/IntegrationTests/TestData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	
)
OUTPUT TO FileStream;

CREATE CQ CsvToPosData
INSERT INTO TypedFileStream
SELECT TO_STRING(data[0]),TO_STRING(data[1])
FROM FileStream;

CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:10000,Interval:20s')
INPUT FROM TypedFileStream;
DEPLOY APPLICATION oracletokudu;
START APPLICATION oracletokudu;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;


CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using OracleReader
(
  Compression:true,
  StartTimestamp:'null',
  CommittedTransactions:true,
  FilterTransactionBoundaries:true,
  Password_encrypted:'false',
  SendBeforeImage:true,
  XstreamTimeOut:600,
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE',
  adapterName:'OracleReader',
  Password:'qatest',
  DictionaryMode:'OfflineCatalog',
  FilterTransactionState:true,
  connectionRetryPolicy:'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType:'LogMiner',
  FetchSize: 1,
  Username:'qatest',
  OutboundServerProcessName:'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic:true,
  CDDLAction:'Quiesce_Cascade',
  CDDLCapture:'true'
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;

STOP APPLICATION OneAgentTester.CSV;
UNDEPLOY APPLICATION OneAgentTester.CSV;
DROP APPLICATION OneAgentTester.CSV cascade;

DROP USER OneAgentTester;
DROP NAMESPACE OneAgentTester CASCADE;
CREATE USER OneAgentTester IDENTIFIED BY OneAgentTester;
GRANT create,drop ON deploymentgroup Global.* To user OneAgentTester;
CONNECT OneAgentTester OneAgentTester;

CREATE APPLICATION CSV;

CREATE FLOW AgentFlow;
create source CSVSource using FileReader
(
directory:'@TEST-DATA-PATH@',
wildcard:'StoreNames.csv',
positionByEOF:false
)
parse using DSVParser
(
header:'yes',
columndelimiter:','
)
OUTPUT TO CsvStream;
END FLOW AgentFlow;

CREATE FLOW ServerFlow;
CREATE TARGET myout using LogWriter(name: OneAgentSource, filename:'@FEATURE-DIR@/logs/log.txt') input from CsvStream;
END FLOW ServerFlow;

END APPLICATION CSV;
DEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;
START CSV;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
directory:'@FEATURE-DIR@/logs/',
filename:'PosData',
rolloverpolicy:'EventCount:5000000,Interval:60s'
)
format using DSVFormatter (

)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizePosDataDefault_actual.log') input from TypedCSVStream;

end application DSV;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@ MAP (table: '@SOURCE_SCHEMA@.@SOURCE_TABLE@1')
SELECT NUM_COL,CHAR_COL,VARCHAR2_COL,LONG_COL,DATE_COL,TIMESTAMP_COL where TO_INT(NUM_COL) > 1;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ Recovery 5 second interval;

create stream @APPNAME@_UserdataStream of Global.WAEvent;

create type @APPNAME@_Order_type(
id int,
order_id int,
zipcode int,
category String,
tablename string
);

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src1 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.order_%'
)
OUTPUT TO @APPNAME@_OrdersStream;

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src2 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_2',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.second_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream2;

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src3 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_3',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.third_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream3;

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src4 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_4',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.fourth_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream4;


Create CQ @APPNAME@_CQUser
insert into @APPNAME@_UserdataStream
select 
putuserdata (data,'Fileowner','FIRST_ORDER') from @APPNAME@_OrdersStream data;


Create CQ @APPNAME@_CQUser2
insert into @APPNAME@_UserdataStream
select 
putuserdata (data2,'Fileowner','SECOND_ORDER') from @APPNAME@_OrdersStream2 data2;


Create CQ @APPNAME@_CQUser3
insert into @APPNAME@_UserdataStream
select 
putuserdata (data3,'Fileowner','THIRD_ORDER') from @APPNAME@_OrdersStream3 data3;


Create CQ @APPNAME@_CQUser4
insert into @APPNAME@_UserdataStream
select 
putuserdata (data4,'Fileowner','FOURTH_ORDER') from @APPNAME@_OrdersStream4 data4;

create stream @APPNAME@_OrderTypedStream1 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream2 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream3 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream4 of @APPNAME@_Order_type;

CREATE CQ @APPNAME@_fin_cq
INSERT INTO @APPNAME@_OrderTypedStream1
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FIRST_ORDER';

CREATE CQ @APPNAME@_fin_cq2
INSERT INTO @APPNAME@_OrderTypedStream2
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'SECOND_ORDER';

CREATE CQ @APPNAME@_fin_cq3
INSERT INTO @APPNAME@_OrderTypedStream3
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'THIRD_ORDER';

CREATE CQ @APPNAME@_fin_cq4
INSERT INTO @APPNAME@_OrderTypedStream4
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FOURTH_ORDER';


create Target @APPNAME@_ADLSGen1_tgt1 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'%category%/%tablename%',
        datalakestorename:'striimdlstest.azuredatalakestore.net',
        clientid:'94195e09-651c-431e-8556-59343c99cc05',
        authtokenendpoint:'https://login.microsoftonline.com/71bfeed5-1905-43da-a4a4-49d8490731da/oauth2/token',
        clientkey:'Vt7Reaamli1DXpqa3kY1+VTzQuEQrvchs5PJ3VNVmfM=',
  rolloverpolicy:'eventcount:8,interval:20s'
)
format using DSVFormatter (
    header:'true'
)
input from @APPNAME@_OrderTypedStream1; 

create Target @APPNAME@_ADLSGen1_tgt2 using ADLSGen1Writer(
        filename:'event_data.xml',
        directory:'%category%/%tablename%',
        datalakestorename:'striimdlstest.azuredatalakestore.net',
        clientid:'94195e09-651c-431e-8556-59343c99cc05',
        authtokenendpoint:'https://login.microsoftonline.com/71bfeed5-1905-43da-a4a4-49d8490731da/oauth2/token',
        clientkey:'Vt7Reaamli1DXpqa3kY1+VTzQuEQrvchs5PJ3VNVmfM=',
	rolloverpolicy:'eventcount:8,interval:20s'
)
format using XMLFormatter (
  elementtuple: 'Order_id:id:order_id:zipcode:category:text=tablename',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from @APPNAME@_OrderTypedStream2; 

create Target @APPNAME@_ADLSGen1_tgt3 using ADLSGen1Writer(
        filename:'event_data.avro',
        directory:'%category%/%tablename%',
        datalakestorename:'striimdlstest.azuredatalakestore.net',
        clientid:'94195e09-651c-431e-8556-59343c99cc05',
        authtokenendpoint:'https://login.microsoftonline.com/71bfeed5-1905-43da-a4a4-49d8490731da/oauth2/token',
        clientkey:'Vt7Reaamli1DXpqa3kY1+VTzQuEQrvchs5PJ3VNVmfM=',
  rolloverpolicy:'eventcount:8,interval:20s'
)
format using AvroFormatter (
  formatAs: 'Default',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA-FILE@'
)
input from @APPNAME@_OrderTypedStream3; 


create Target @APPNAME@_ADLSGen1_tgt4 using ADLSGen1Writer(
        filename:'event_data.json',
        directory:'%category%/%tablename%',
        datalakestorename:'striimdlstest.azuredatalakestore.net',
        clientid:'94195e09-651c-431e-8556-59343c99cc05',
        authtokenendpoint:'https://login.microsoftonline.com/71bfeed5-1905-43da-a4a4-49d8490731da/oauth2/token',
        clientkey:'Vt7Reaamli1DXpqa3kY1+VTzQuEQrvchs5PJ3VNVmfM=',
  rolloverpolicy:'eventcount:8,interval:20s'
)
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@_OrderTypedStream4;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

STOP APPLICATION OneAgentEncryptionTester.CSV;
UNDEPLOY APPLICATION OneAgentEncryptionTester.CSV;
DROP APPLICATION OneAgentEncryptionTester.CSV cascade;

create application CSV WITH ENCRYPTION;

CREATE FLOW AgentFlow;
create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails-agent.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStream1;


CREATE TYPE MyTypeCsv(
PAN String,
FNAME String KEY,
LNAME String,
ADDRESS String,
CITY String,
STATE String,
ZIP String,
GENDER String
);

CREATE STREAM TypedStreamCsv of MyTypeCsv;

CREATE CQ TypeConversionCQCsv
INSERT INTO TypedStreamCsv
SELECT
data[0],
data[1],
data[2],
data[3],
data[4],
data[5],
data[6],
data[7]
from CsvStream1;

END FLOW AgentFlow;

CREATE FLOW ServerFlow;

CREATE WACTIONSTORE StoreInfoCsv CONTEXT OF MyTypeCsv
EVENT TYPES ( MyTypeCsv )
@PERSIST-TYPE@

CREATE CQ StoreWactionCsv
INSERT INTO StoreInfoCsv
SELECT * FROM TypedStreamCsv
LINK SOURCE EVENT;


END FLOW ServerFlow;

end application CSV;

DEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;
START CSV;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'JsonTargetTI',
  directory:'@FEATURE-DIR@/logs/',
  sequence:'00',
  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:10-89s'
)
format using JSONFormatter (
  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)

input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetJsonTIAddition_actual.log') input from TypedCSVStream;

end application DSV;

Stop Teradata_LogWriter;
Undeploy application Teradata_LogWriter;
drop application Teradata_LogWriter cascade;

CREATE APPLICATION Teradata_LogWriter recovery 5 second interval;

CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.TDSOURCE',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.TEST01=ID;',
  PollingInterval: '5sec',
  ReturnDateTimeAs: 'String',
  startPosition:'striim.test01=0'
  )
  OUTPUT TO data_stream;

  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

CREATE TARGET BinaryDump USING LogWriter(
  name: 'TeraData',
  filename:'TeraData.log',
  flushpolicy:'EventCount:100,Interval:30s'
)INPUT FROM data_stream;

END APPLICATION Teradata_LogWriter;

deploy application Teradata_LogWriter in default;

start application Teradata_LogWriter;

stop Oracle_IRLogWriter;
undeploy application Oracle_IRLogWriter;
drop application Oracle_IRLogWriter cascade;
CREATE APPLICATION Oracle_IRLogWriter;

Create Source s1 Using IncrementalBatchReader (
 FetchSize: 1,
  Username: 'striim',
  Password: 'o4l1uMpwIDQ=',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.autotest01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.autotest01=id',
  startPosition: '%=0'
 )
OUTPUT TO data_stream1;

create source s2 using IncrementalBatchReader (
FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.autotest02',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.autotest02=id',
  startPosition: '%=0'
 )
OUTPUT TO data_stream2;

create source s3 using IncrementalBatchReader (
FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.autotest03',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.autotest03=id',
  startPosition: '%=0'
 )
OUTPUT TO data_stream3;

Create Type EventType (
ID int,
NAME string,
COMPANY string,
COUNTRY string
);

CREATE STREAM insertData1  of EventType;
CREATE STREAM deleteData1 of EventType;
CREATE STREAM joinData1 of EventType;
CREATE STREAM joinData2 of EventType;
CREATE STREAM deleteData2 of EventType;
CREATE STREAM OutStream of EventType;

CREATE CQ cq1 INSERT INTO insertData1  SELECT TO_INT(data[0]),data[1],data[2],data[3] FROM data_stream1;

CREATE CQ cq2 INSERT INTO deleteData1 SELECT TO_INT(data[0]),data[1],data[2],data[3] FROM data_stream2;

CREATE CQ cq3 INSERT INTO joinData1 SELECT TO_INT(data[0]),data[1],data[2],data[3] FROM data_stream3;

CREATE JUMPING WINDOW DataWin1 OVER deleteData1 KEEP 1 ROWS;

CREATE CQ TypedEventKeyCQ6 INSERT INTO deleteData2 SELECT * from DataWin1;

CREATE JUMPING WINDOW DataWin2 OVER joinData1 KEEP 1 ROWS;

CREATE CQ TypedEventKeyCQ5 INSERT INTO joinData2 SELECT * from DataWin2;

CREATE EVENTTABLE ETABLE1 using STREAM ( NAME: 'insertData1 ' )
--DELETE using STREAM ( NAME: 'deleteData1')
QUERY (keytomap:"ID", persistPolicy: 'true') OF EventType;

CREATE CQ cq4 INSERT INTO OutStream SELECT B.ID,B.NAME,B.COMPANY,B.COUNTRY FROM joinData2 A, ETABLE1 B where A.ID=B.ID;

CREATE TARGET EventTableFW USING FileWriter
(filename:'BasicOracle_IRLogWriter_RT.log',
 rolloverpolicy: 'EventCount:1000000')
FORMAT USING DSVFormatter () INPUT FROM OutStream;

create target Target_Azure using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'STRIIM',
        password: 'W3b@ct10n',
        AccountName: 'striimqatestdonotdelete',
        accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables:'dbo.autotest01',
        uploadpolicy:'eventcount:0,interval:0s'
) INPUT FROM OutStream;

END APPLICATION Oracle_IRLogWriter;
deploy application Oracle_IRLogWriter in default;
start Oracle_IRLogWriter;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
wildcard: '',
directory: '',
positionbyeof: false )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: 'ParquetFileSchema'
)
INPUT FROM @appname@CqOut;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using ParquetFormatter (
schemaFileName: 'ParquetS3Schema'
)
input from @appname@CqOut;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using ParquetFormatter (
schemaFileName: 'ParquetAzureSchema'
)
INPUT FROM @appname@CqOut;

CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using ParquetFormatter (
schemaFileName: 'ParquetGCSSchema'
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

STOP APPLICATION MysqltoBQ;
UNDEPLOY APPLICATION MysqltoBQ;
DROP APPLICATION MysqltoBQ CASCADE;
CREATE APPLICATION MysqltoBQ recovery 5 SECOND Interval;
CREATE OR REPLACE SOURCE MysqltoBQ_Source USING MySQLReader 
(
  Username:'root',
  Password:'w@ct10n',
  connectionURL:'jdbc:mysql://localhost:3306/waction',
  Tables:'waction.sourceTable',
  sendBeforeImage:'true',
  FilterTransactionBoundaries:'true',
  ExcludedTables:'waction.CHKPOINT',
  useSSL:true
) 
OUTPUT TO MysqltoBQ_Stream;

CREATE OR REPLACE TARGET MysqltoBQ_Target USING BigQueryWriter  (
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  projectId:'striimqa-214712',
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:5,Interval:120',
  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', 
  AllowQuotedNewLines: 'false', 
  adapterName: 'BigQueryWriter', 
  optimizedMerge: 'true', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', 
  StandardSQL: 'true', 
  QuoteCharacter: '\"', 
  Tables: 'waction.sourceTable,.oratobqtgt',
  Mode: 'MERGE',
  StandardSQL: 'true',
  QuoteCharacter: '\"'
  ) INPUT FROM MysqltoBQ_Stream;

CREATE OR REPLACE TARGET MysqltoBQ_SysOut USING Global.SysOut (name: 'wa') INPUT FROM MysqltoBQ_Stream;

END APPLICATION MysqltoBQ;
DEPLOY APPLICATION MysqltoBQ;
START MysqltoBQ;

--
-- Recovery Test 10 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> CW(p) -> CQ -> WS
--

STOP Recov10Tester.RecovTest10;
UNDEPLOY APPLICATION Recov10Tester.RecovTest10;
DROP APPLICATION Recov10Tester.RecovTest10 CASCADE;
CREATE APPLICATION RecovTest10 RECOVERY 1 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTest10Data.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  partKey String KEY,
  serialNumber int
);

CREATE STREAM DataStream OF CsvData PARTITION BY partKey;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    TO_INT(data[1])
FROM CsvStream;

CREATE JUMPING WINDOW DataStreamTwoItems
OVER DataStream KEEP 2 ROWS
PARTITION BY partKey;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    first(partKey),
    to_int(first(serialNumber))
FROM DataStreamTwoItems
GROUP BY partKey;

END APPLICATION RecovTest10;

STOP @Appname@;
UNDEPLOY APPLICATION @Appname@;
DROP APPLICATION @Appname@ CASCADE;

CREATE APPLICATION @Appname@ @Recovery@ use exceptionstore;

CREATE SOURCE @Appname@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:orcl',
	Tables: 'QATEST.TABLE_TEST_%',
	FetchSize: '1'
)
OUTPUT TO @Appname@_SS;


CREATE or replace TARGET @Appname@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:10',
StandardSQL:true	
) INPUT FROM @Appname@_ss;

END APPLICATION @Appname@;
DEPLOY APPLICATION @Appname@;
START APPLICATION @Appname@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@
 Using DatabaseReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectURL@',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

Stop IR;
Undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;
create flow AgentFlow;
CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.119/DBS_PORT=1025',
  Tables: 'striim.upgrade01',
  CheckColumn:'striim.upgrade01=t1',
  startPosition:'striim.upgrade01=2018-09-20 06:43:59',
  ReturnDateTimeAs:'string'
  )
OUTPUT TO data_stream1;
end flow AgentFlow;

create flow serverFlow;
create target AzureSQLDWHTarget using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test26,dbo.test26',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream1;

end flow serverFlow;
END APPLICATION IR;
deploy application IR with AgentFlow in Agents, ServerFlow in default;
start application IR;

Create Source @SOURCE_NAME@ Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: true,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SCHEMANAME@.@TABLENAME@_copy;@SCHEMANAME@.@TABLENAME@_copy',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  SetConservativeRange: true
) Output To @STREAM@;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@;

CREATE SOURCE @SOURCE_NAME@2 USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@2;

STOP Jumping1Tester.Jumping1;
UNDEPLOY APPLICATION Jumping1Tester.Jumping1;
DROP APPLICATION Jumping1Tester.Jumping1 CASCADE;
CREATE APPLICATION Jumping1;

create source CsvSource1 using FileReader
(
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'WindowsTest.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
)
 parse using DSVParser
(
	header:'yes',
	columndelimiter:','
)
OUTPUT TO CsvStream1;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);
CREATE TYPE CsvData1 (
  zip double
);

CREATE TYPE WactionData1 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData2 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData3 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData5 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData6 (
  zip double
);
CREATE TYPE WactionData7 (
  zip double
);
CREATE TYPE WactionData8 (
  zip double
);

CREATE STREAM DataStream1 OF CsvData;

CREATE STREAM DataStream2 OF CsvData
PARTITION BY companyName;

CREATE STREAM DataStream3 OF CsvData
PARTITION BY city;

CREATE STREAM DataStream4 OF CsvData;
CREATE STREAM DataStream5 OF CsvData
PARTITION BY city;

CREATE STREAM DataStream6 OF CsvData1;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData3
INSERT INTO DataStream3
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData4
INSERT INTO DataStream4
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData5
INSERT INTO DataStream5
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData6
INSERT INTO DataStream6
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

CREATE CQ CsvToData7
INSERT INTO DataStream7
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

CREATE CQ CsvToData8
INSERT INTO DataStream8
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

-- Count based jumping window
CREATE JUMPING WINDOW DataStreamCount
OVER DataStream1 KEEP 5 ROWS;

-- Time based jumping window
CREATE JUMPING WINDOW DataStreamTime OVER DataStream2 KEEP
within 40 second
PARTITION BY companyName;

-- Attribute based jumping window
CREATE JUMPING WINDOW DataStreamAtrribute
OVER DataStream3 KEEP
range 5 minute
ON dateTime
PARTITION BY city;

-- Count + time based jumping window
CREATE JUMPING WINDOW DataStreamCountTime
OVER DataStream4 KEEP
5 rows
within 8 minute;

-- Attribute + time based jumping window
CREATE JUMPING WINDOW DataStreamAttributeTime
OVER DataStream5 KEEP
range 300 second
ON dateTime
within 5 minute
PARTITION BY city;

-- Attribute + time based jumping window using COUNT
CREATE JUMPING WINDOW DataStreamAttributeTime1
OVER DataStream5 KEEP
range 300 second
ON dateTime
within 5 minute;

CREATE WACTIONSTORE Wactions1 CONTEXT OF WactionData1
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions2 CONTEXT OF WactionData2
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions3 CONTEXT OF WactionData3
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions4 CONTEXT OF WactionData4
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions5 CONTEXT OF WactionData5
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions6 CONTEXT OF WactionData6
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions7 CONTEXT OF WactionData7
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions8 CONTEXT OF WactionData8
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE CQ Data1ToWaction
INSERT INTO Wactions1
SELECT
	p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamCount p;

CREATE CQ Data2ToWaction
INSERT INTO Wactions2
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamTime p
group by companyName;

CREATE CQ Data3ToWaction
INSERT INTO Wactions3
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamAtrribute p;

CREATE CQ Data4ToWaction
INSERT INTO Wactions4
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamCountTime p;

CREATE CQ Data5ToWaction
INSERT INTO Wactions5
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamAttributeTime p
group by city;

CREATE CQ Data6ToWaction
INSERT INTO Wactions6
SELECT
    count(*)
FROM DataStreamCount p;

CREATE CQ Data7ToWaction
INSERT INTO Wactions7
SELECT
    count(*)
FROM DataStreamCountTime p;

CREATE CQ Data8ToWaction
INSERT INTO Wactions8
SELECT
    count(*)
FROM DataStreamAttributeTime1 p;


END APPLICATION Jumping1;

STOP APPLICATION snow2FW;
UNDEPLOY APPLICATION snow2FW;
DROP APPLICATION snow2FW CASCADE;
CREATE OR REPLACE APPLICATION snow2FW;

CREATE OR REPLACE SOURCE snow_fw USING Global.ServiceNowReader (
  Mode: 'InitialLoad',
  ServiceNow.ConnectionTimeOut: 60,
  ServiceNow.MaxConnections: 20,
  ServiceNow.FetchSize: 10000,
  ThreadPoolCount: '10',
  ServiceNow.ConnectionRetries: 3,
  PollingInterval: '1',
  ClientSecret: '6Wa-cv`I7x',
  Password: '^Pre&$EMO%6O.e_{96h+$R?rJd,=[4Vt=K)Szh?6g<J9D3,3zs8R;hpZqh]-3?C&.u-@GvSakPXH1:2eygbBDI>ou-z#GjBw[u8x',
  ServiceNow.Tables: 'u_empl',
  UserName: 'snr',
  ClientID: 'ce4fd5af894a11103d2c5c3a8fe075e1',
  adapterName: 'ServiceNowReader',
  ServiceNow.BatchAPI: true,
  ServiceNow.ConnectionUrl: 'https://dev84954.service-now.com/' )
OUTPUT TO sn;


CREATE TARGET ft USING Global.FileWriter (
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000,Interval:30s',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  DataEncryptionKeyPassphrase_encrypted: 'true',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  filename: 'dt' )
FORMAT USING Global.JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n' )
INPUT FROM sn;

CREATE TARGET pg_target USING Global.DatabaseWriter (

  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
    Password: 'w@ct10n',
    Tables: 'u_empl,u_empl ColumnMap(name=u_name,age=u_age,address=u_address,sys_id=sys_id)',
    ParallelThreads: '',
    CheckPointTable: 'CHKPOINT',
    CDDLAction: 'Process',
    ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
    CommitPolicy: 'EventCount:1000,Interval:60',
    StatementCacheSize: '50',
    Username: 'waction',
    DatabaseProviderType: 'Postgres',
    BatchPolicy: 'EventCount:1000,Interval:60',
    PreserveSourceTransactionBoundary: 'false' )
  INPUT FROM sn;

END APPLICATION snow2FW;
deploy application snow2FW;
start snow2FW;

--
-- Crash Recovery Test 7 with Jumping window and partitioned on two node cluster with one agent
-- Bert Hashemi, WebAction, Inc.
--
-- S -> KafkaStream -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION KStreamN2S2CR7Tester.KStreamN2S2CRTest7;
UNDEPLOY APPLICATION KStreamN2S2CR7Tester.KStreamN2S2CRTest7;
DROP APPLICATION KStreamN2S2CR7Tester.KStreamN2S2CRTest7 CASCADE;
DROP USER KStreamN2S2CR7Tester;
DROP NAMESPACE KStreamN2S2CR7Tester CASCADE;
CREATE USER KStreamN2S2CR7Tester IDENTIFIED BY KStreamN2S2CR7Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR7Tester;
CONNECT KStreamN2S2CR6Tester KStreamN2S2CR7Tester;

CREATE APPLICATION KStreamN2S2CRTest7 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest7;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream of CsvDataTypeKStreamN2S2CRTest7 using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest7 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;


CREATE TYPE CsvDataTypeKStreamN2S2CRTest7 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE CQ TransferToKafka
INSERT INTO KafkaCsvStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest7;

CREATE FLOW DataProcessingKStreamN2S2CRTest7;

CREATE STREAM DataStream OF CsvDataTypeKStreamN2S2CRTest7 PARTITION BY merchantId;

CREATE CQ CsvToDataKStreamN2S2CRTest7
INSERT INTO DataStream
SELECT
    *
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest7 CONTEXT OF CsvDataTypeKStreamN2S2CRTest7
EVENT TYPES ( CsvDataTypeKStreamN2S2CRTest7 )
@PERSIST-TYPE@

CREATE CQ DataToWactionKStreamN2S2CRTest7
INSERT INTO WactionsKStreamN2S2CRTest7
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingKStreamN2S2CRTest7;

END APPLICATION KStreamN2S2CRTest7;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_scnRange: 1000,
 _h_eoffDelay: 10,
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP JSONRecoveryApp;

UNDEPLOY APPLICATION admin.JSONRecoveryApp;
DROP APPLICATION admin.JSONRecoveryApp cascade;

CREATE APPLICATION JSONRecoveryApp RECOVERY 1 SECOND INTERVAL;

CREATE TYPE Emptype (
firstName String,
lastName String );

CREATE STREAM EmpStream of Emptype;

create source CSVSource using FileReader (
	directory:'/Users/bhashemi/Product/IntegrationTests/TestData/jsonRecov',
	WildCard:'jsonRecov*.json',
	positionByEOF:false
) PARSE USING
JSONParser (
eventType:''
) OUTPUT TO EmpStream;

CREATE WACTIONSTORE jsonWactions CONTEXT OF Emptype
EVENT TYPES ( Emptype )
@PERSIST-TYPE@

CREATE CQ InsertjsonWactions
INSERT INTO jsonWactions
SELECT firstName, lastName
FROM EmpStream;

CREATE TARGET jsonRecovSYSOUT using SysOut(name:jsonrecov) INPUT FROM EmpStream;
END APPLICATION JSONRecoveryApp;

deploy application JSONRecoveryApp in default;
START JSONRecoveryApp;

stop tpcc;
undeploy application tpcc;
drop application tpcc cascade;
CREATE APPLICATION tpcc;

Create Source oracSource
 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'localhost:1521:orcl',
 Tables:'QATEST.TIMETEST',
 Fetchsize:1
)
Output To DataStream;

CREATE TARGET WriteCDCOracle USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:orcl',
  Username:'qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'QATEST.TIMETEST,QATEST.TIMETEST_TGT'
) INPUT FROM DataStream;

create Target t2 using SysOut(name:Foo2) input from DataStream;

END APPLICATION tpcc;
deploy application tpcc in default;
start tpcc;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using MSSqlReader
(
 Username:'@UserName@',
 Password:'@Password@',
 DatabaseName:'qatest',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'@SourceTable@',
 ConnectionPoolSize:1,
 StartPosition:'EOF'
 ) Output To @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;


 CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@SourceTable@,@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application GCSWriterTest;
undeploy application GCSWriterTest;
drop application GCSWriterTest cascade;
create application GCSWriterTest recovery 1 second interval;
create source GS Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

create Target T using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@',
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from CsvStream;

end application GCSWriterTest;
deploy application GCSWriterTest on all in default;
start application GCSWriterTest;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE @SOURCE@ USING Ojet  (
  FilterTransactionBoundaries: true,
  ConnectionURL: '@OCI-URL@',
  Tables: '@SOURCE_TABLE@',
  Password: '@OJET-PASSWORD@',
  fetchsize: 1,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Username: '@OJET-UNAME@'
 )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@1 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'false',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@2 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'false',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'true',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@3 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'true',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.GGTrailReader (
  Tables:'@TABLES@',
  CDDLCapture: false,
  TrailDirectory: '@TRAIL_FILE_DIR@',
  TrailFilePattern: '@WILDCARD@',
  Compression: false,
  SupportColumnCharset: false,
  CDDLAction: 'Process',
  FilterTransactionBoundaries: true,
  adapterName: 'GGTrailReader',
  TrailByteOrder: '@ENDIAN@' )
OUTPUT TO @STREAM@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 USING Global.GGTrailReader (
  Tables:'@TABLES@',
  CDDLCapture: false,
  TrailDirectory: '@TRAIL_FILE_DIR@',
  TrailFilePattern: '@WILDCARD@',
  Compression: false,
  SupportColumnCharset: false,
  CDDLAction: 'Process',
  FilterTransactionBoundaries: true,
  adapterName: 'GGTrailReader',
  TrailByteOrder: '@ENDIAN@' )
OUTPUT TO @STREAM@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING MSSqlReader
(
  Compression: false,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.test01',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'NULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true	,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;
--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

STOP APPLICATION BQ;
UNDEPLOY APPLICATION BQ;
DROP APPLICATION BQ CASCADE;
CREATE APPLICATION BQ RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE s USING FileReader (
	directory:'Product/IntegrationTests/TestData/',
    WildCard:'posdata5L.csv',
	positionByEOF:false
	)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO ss;

CREATE TYPE userType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String
);

CREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;

CREATE CQ cq1
INSERT INTO userDefinedTypedStream
SELECT data[0],data[1],data[2], data[3],data[4]
FROM ss;

CREATE or replace TARGET TABLE1 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE1@.TABLE1',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE2 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE2@.TABLE2',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE3 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE3@.TABLE3',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE4 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE4@.TABLE4',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE5 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE5@.TABLE5',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE6 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE6@.TABLE6',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE7 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE7@.TABLE7',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE8 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE8@.TABLE8',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE9 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE9@.TABLE9',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE10 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE10@.TABLE10',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

END APPLICATION BQ;
DEPLOY APPLICATION BQ;
start application BQ;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ recovery 5 second interval;
--create application @APPNAME@;

--create flow agentflow;
CREATE OR REPLACE SOURCE @APPNAME@_Src USING SpannerBatchReader  (
  DatabaseProviderType: 'Default',
  pollingInterval: '5ms',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  ConnectionURL: 'jdbc:cloudspanner:/projects/bigquerywritertest/instances/testspanner/databases/spannertestdb?credentials=/Users/jenniffer/Downloads/abc.json',
  Tables: 'Recovery_Timestam%',
  --_h_mode:'InitialLoad',
--  VendorConfiguration:'_h_SpannerReadStaleness=MAX_STALENESS 20s',
  adapterName: 'SpannerBatchReader',
    StartPosition: '%=0',
  CheckColumn: '%=id'
 )
OUTPUT TO @APPNAME@_Output_Stream;
--end flow agentflow;

CREATE TARGET @APPNAME@_tgt USING SpannerWriter (
	Tables: 'spannersource,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	BatchPolicy: 'EventCount: 1; Interval: 1s',
	instanceId: 'qatest'
) INPUT FROM @APPNAME@_Output_Stream;

create Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_Output_Stream;

end application @APPNAME@;
deploy application @APPNAME@;
--deploy application @APPNAME@ with agentflow in agents;
start application @APPNAME@;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@ @APP_PROPERTY@ USE EXCEPTIONSTORE;

CREATE OR REPLACE STREAM @APP_NAME@_DataStreamFromCQ OF Global.WAEvent;

Create Source @APP_NAME@_Source Using @SOURCE_ADAPTER@ (

) OUTPUT TO @APP_NAME@_DataStream;

CREATE OR REPLACE CQ @APP_NAME@_CQ INSERT INTO @APP_NAME@_DataStreamFromCQ SELECT * FROM @APP_NAME@_DataStream s where META(s,"TableName") is not null AND META(s,"TableName").toString().trim().isEmpty() == false;

CREATE TARGET @APP_NAME@_Target USING @TARGET_ADAPTER@ ( 

) INPUT FROM @APP_NAME@_DataStreamFromCQ;

CREATE OR REPLACE TARGET @APP_NAME@_SysOut_ReadFromSource USING Global.SysOut ( 
	name: '@APP_NAME@_SysOutWA_Source' 
) INPUT FROM @APP_NAME@_DataStream;

CREATE OR REPLACE TARGET @APP_NAME@_SysOut_WriteToTarget USING Global.SysOut ( 
	name: '@APP_NAME@_SysOutWA_Target' 
) INPUT FROM @APP_NAME@_DataStreamFromCQ;

END APPLICATION @APP_NAME@;

DEPLOY APPLICATION @APP_NAME@ IN DEFAULT;
START APPLICATION @APP_NAME@;

IMPORT static com.webaction.runtime.converters.DateConverter.*;

UNDEPLOY APPLICATION admin.SQLMXReaderApp;
DROP APPLICATION admin.SQLMXReaderApp cascade;

CREATE APPLICATION SQLMXReaderApp;
create source SQMXSource using HPNonStopSQLMXReader (
	portno:2020,
	ipaddress:'10.10.196.122',
	Name:'intg',
	AuditTrails:'parallel',
	AgentPortNo:8012,
	AgentIpAddress:'10.10.197.116', 
	Tables:'watest.wasch.sqlmxtest1;watest.wasch.sqlmxtest2') output to CDCStream,
	SQLMXMATStream MAP (table:'WATEST.WASCH.SQLMXTEST2');


CREATE TYPE SQLMXTEST2Data(
    C0 Integer,
    C1 String,
    C2 Short,
    OPR String,
    TABLENAME String,
    AUXNAME String
);

CREATE STREAM SQLMXTEST2Stream OF SQLMXTEST2Data;


CREATE JUMPING WINDOW SQLMXDataWindow
OVER SQLMXTEST2Stream KEEP 4 ROWS
PARTITION BY OPR;


CREATE CQ ToSQLMXData
INSERT INTO SQLMXTEST2Stream
SELECT TO_INT(data[0]),
	   data[1],
       TO_SHORT(data[2]),
       META(x,"OperationName").toString(),
       META(x, "TableName").toString(),
       META(x,"AuditTrailName").toString()
FROM CDCStream x
WHERE not(META(x,"OperationName").toString() = "BEGIN") AND not(META(x,"OperationName").toString() = "COMMIT") AND not(META(x, "TableName").toString() is null) 
AND META(x, "TableName").toString() = "WATEST.WASCH.SQLMXTEST1" AND META(x, "AuditTrailName").toString() = "MAT";


--CREATE TARGET SQLMXSYSOUT using SysOut(name:sqlmx) INPUT FROM CDCStream;
CREATE TARGET SQLMXMAT USING LogWriter(
  name:SQLMXReaderAppMAT,
filename:'@FEATURE-DIR@/logs/sqlmxmat.log'
--  filename:'mat.log'
) INPUT FROM SQLMXMATStream;

CREATE TARGET SQLMXAUX01 USING LogWriter(
  name:SQLMXReaderAppMAT1,
filename:'@FEATURE-DIR@/logs/sqlmxmat1.log'
--  filename:'aux1.log'
) INPUT FROM SQLMXTEST2Stream;


END APPLICATION SQLMXReaderApp;
deploy application SQLMXReaderApp in default;

STOP application XmlFormatterTester.DSV;
undeploy application XmlFormatterTester.DSV;
drop application XmlFormatterTester.DSV cascade;

create application DSV;
create source CSVSmallPosDataSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvSmallPosDataStream;

create source CSVPosDataSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'posdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvPosDataStream;


Create Type CSVPosDataType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVSmallPosDataStream of CSVPosDataType;
Create Stream TypedCSVPosDataStream of CSVPosDataType;


CREATE CQ CsvToSmallPosData
INSERT INTO TypedCSVSmallPosDataStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvSmallPosDataStream;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVPosDataStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvPosDataStream;

/**
*  3.2.1.b FileWriter XML TimeInterval
**/
create Target XmlFormatterTimeInterval using FileWriter(
  filename:'TargetPosDataXmlTIOpt',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:merchantid:text=merchantname',
  charset:'UTF-8'
)
input from TypedCSVSmallPosDataStream;

/**
* 3.2.1.c FileWriter XMLFileSize 101MB
**/
create Target XmlFormatterFileSize101 using FileWriter(
  filename:'TargetPosDataXmlFS',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'FileSizeRollingPolicy,filesize:101M,sequence:00'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:merchantid:text=merchantname',
  charset:'UTF-8'
)
input from TypedCSVPosDataStream;

/**
* 3.2.1.d FileWriter XMLDefaultFS 10 MB
**/
create Target XmlFormatterDefault using FileWriter(
  filename:'TargetPosDataXmlFSDefault',
  directory:'@FEATURE-DIR@/logs/',
    sequence:'00',
  rolloverpolicy:'FileSizeRollingPolicy'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:zip:text=merchantname',
  charset:'UTF-8'
)
input from TypedCSVSmallPosDataStream;


/**
* 3.2.1.e FileWriter XML EventCount 2000
**/
create Target XmlFormatterEventCount2000 using FileWriter(
  filename:'TargetPosDataXmlEC',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'eventcount:2000,sequence:00'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:zip:text=merchantname',
  charset:'UTF-8'
)
input from TypedCSVSmallPosDataStream;

create Target LogWriterSmallPosData using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlDFS_actual.log') input from TypedCSVSmallPosDataStream;

end application DSV;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;


CREATE OR REPLACE SOURCE @APPNAME@CDC_Source1 USING OracleReader( 
  StartTimestamp: 'null',
  SupportPDB: false,
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  FetchSize: 1,
  DDLCaptureMode: 'All',
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.EMP',
  adapterName: 'OracleReader',
  Password: 'qatest',
  TransactionBufferType: 'Memory',
  DictionaryMode: 'OnlineCatalog',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '1MB',
  compression: true,
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@AppStream1;


CREATE OR REPLACE TARGET @APPNAME@sap_target USING DatabaseWriter( 
  DatabaseProviderType:'SAPHANA',
  ConnectionRetryPolicy: 'retryInterval=30,maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'SYSTEM',
  Password_encrypted: 'false',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:sap://10.77.21.116:39013/?databaseName=striim&currentSchema=QA',
  Tables: 'waction.crash_type,QA.CRASH_TYPES',
  adapterName: 'DatabaseWriter',
  --IgnorableExceptionCode: '',
  Password: 'Striim_SAP@123'
 ) 
INPUT FROM @APPNAME@AppStream1;


create or replace target @APPNAME@sys_tgt using sysout(
name:Foo2
)input from @APPNAME@AppStream1;

END APPLICATION @APPNAME@;

deploy application @APPNAME@;
start application @APPNAME@;

stop application GCSWriterTest;
undeploy application GCSWriterTest;
drop application GCSWriterTest cascade;
create application GCSWriterTest recovery 1 second interval;
create source GCS_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

create target sys using sysout(name:'raw')input from CsvStream;

create Target GCSTarget using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    foldername:'@foldername@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@'    
)
format using AvroFormatter (
)
input from CsvStream;

end application GCSWriterTest;
deploy application GCSWriterTest on all in default;
start application GCSWriterTest;

CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (
  StartPosition: '@startPosition@',
  ConnectionURL: '@sourceURL@',
  Username: '@userName@',
  Tables: '@tables@',
  CheckColumn: '@checkColum@',
  FetchSize: 1,
  Password: '@password@',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

 create type @SOURCE_NAME@_AutoType(
  ID int,
  name string,
  country string
);

CREATE STREAM @STREAM@_stream OF @SOURCE_NAME@_AutoType;

CREATE CQ Lookup
INSERT INTO @STREAM@_stream
select data[0],data[1],data[2] from @STREAM@;

--
-- Recovery Test 38 with two sources, two jumping time-count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5t3W/p -> CQ1 -> WS
--   S2 -> Jc6t4W/p -> CQ2 -> WS
--

STOP Recov38Tester.RecovTest38;
UNDEPLOY APPLICATION Recov38Tester.RecovTest38;
DROP APPLICATION Recov38Tester.RecovTest38 CASCADE;
CREATE APPLICATION RecovTest38 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Rows3Seconds
OVER DataStream1 KEEP 5 ROWS WITHIN 3 SECOND
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Rows4Seconds
OVER DataStream2 KEEP 6 ROWS WITHIN 4 SECOND
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataStream5Rows3Seconds
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Rows3Seconds p
GROUP BY p.merchantId;

CREATE CQ DataStream6Rows4Seconds
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Rows4Seconds p
GROUP BY p.merchantId;

END APPLICATION RecovTest38;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;


CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',
					acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

CREATE OR REPLACE STREAM @APPNAME@_stream OF Global.waevent persist using @APPNAME@_KafkaPropset;

CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @APPNAME@_stream2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNEC4TION_URL@',
  Tables: '$table1',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'QATEST.ORCTOMSSQLPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo) input from @STREAM@;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @APPNAME@_stream3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '$table2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@4',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'QATEST.ORCTOMSSQLPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@3;

END APPLICATION @APPNAME@2;

create application sorted;

create source AALSortedSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'sorted_log',
  charset:'UTF-8',
  positionByEOF:false
) PARSE USING AALParser (
  columndelimiter:' ',
  IgnoreEmptyColumn:'Yes',
  columndelimittill:5
) OUTPUT TO AalSortedStream;

create Target AALSortedDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/sorted_log') input from AalSortedStream;

end application sorted;

stop APPLICATION @AppName@;
Undeploy APPLICATION @AppName@;
drop APPLICATION @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @AgentFlow@1;
CREATE OR REPLACE SOURCE @SourceName@1 USING Global.MSJet (
  Tables: 'dbo.compsrc',
  username: 'qatest',
  DatabaseName: 'qatest',
  FetchTransactionMetadata: true,
  filterTransactionBoundaries: true,
  compression: false,
  ConnectionURL: '@ConnectionURL@1',
  Mode: '@mode@',
  CommittedTransactions: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
  SendBeforeImage: true,
  password: 'w3b@ct10n' )
OUTPUT TO @StreamName@;
END FLOW @AgentFlow@1;

CREATE FLOW @AgentFlow@2;
CREATE OR REPLACE SOURCE @SourceName@2 USING Global.MSJet (
  Tables: 'dbo.compsrc',
  username: 'qatest',
  DatabaseName: 'qatest',
  FetchTransactionMetadata: true,
  filterTransactionBoundaries: true,
  compression: false,
  ConnectionURL: '@ConnectionURL@2',
  CaptureType: '@captureType@',
  CommittedTransactions: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
  SendBeforeImage: true,
  password: 'w3b@ct10n' )
OUTPUT TO @StreamName@;
END FLOW @AgentFlow@2;

CREATE TARGET @SysTarget@ USING Global.SysOut (
  name: 'MS_CDC_SYSOUT' )
INPUT FROM @StreamName@;

CREATE FLOW @ServerFlow@1;

CREATE TARGET @TargetName@1 USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ParallelThreads: '',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: '@ConnectionURL@',
  StatementCacheSize: '50',
  DatabaseProviderType: 'Default',
  Username: 'qatest',
  Tables: 'dbo.compsrc,dbo.comptar',
  Password: 'w3b@ct10n',
  PreserveSourceTransactionBoundary: 'false',
  BatchPolicy: 'EventCount:1,Interval:60' )
INPUT FROM @StreamName@;

END FLOW @ServerFlow@1;

END APPLICATION @AppName@;
DEPLOY APPLICATION @AppName@ with @AgentFlow@1 in AGENTS, @AgentFlow@2 in AGENTS, @ServerFlow@1 on any in default;
START APPLICATION @AppName@;

stop DBRTOCW;
 undeploy application DBRTOCW;
 drop application DBRTOCW cascade;
 CREATE APPLICATION DBRTOCW;

 Create Source MSSQLSource Using MSSqlReader
(
Username:'qatest',
Password:'w@ct10n',
DatabaseName:'qatest',
ConnectionURL:'10.77.61.30:1433',
Tables:'qatest.MssqlTocql_Alldatatypes',
ConnectionPoolSize:1,
Compression:'true'
)
OUTPUT TO Oracle_ChangeDataStream;

 CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM Oracle_ChangeDataStream;

 create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

 END APPLICATION DBRTOCW;

 deploy application DBRTOCW in default;

 start DBRTOCW;

create Target @TARGET_NAME@ using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:50'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@',
	members:'Table=@metadata(TableName),OpName=@metadata(OperationName)'
)
input from @STREAM@;

--This TQL only works with HL7v2 2.3 Messages, specifically generated from SimHospital Docker Simulator

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE STREAM @APPNAME@_AlertStream OF Global.AlertEvent;

CREATE OR REPLACE SOURCE @APPNAME@_src USING Global.TCPReader ()
PARSE USING Global.HL7v2Parser ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE CQ @APPNAME@_ADT_CQ
INSERT INTO @APPNAME@_ADT_CQOut
SELECT 
data.element("MSH").element("MSH.6").element("HD.1").getText() as Facility,
TO_DATEF(data.element("MSH").element("MSH.7").element("TS.1").getText(),'yyyyMMddHHmmss') as MsgTime,
data.element("PID").element("PID.3").element("CX.1").getText() as PatientID, 
data.element("PID").element("PID.5").element("XPN.1").getText() as FirstName, 
data.element("PID").element("PID.5").element("XPN.2").getText() as LastName, 
TO_DATEF(SLEFT(data.element("PID").element("PID.7").element("TS.1").getText(),8),'yyyyMMdd') as DOB, 
data.element("PID").element("PID.8").getText() as Gender, 
data.element("PID").element("PID.11").element("XAD.5").getText() as ZipCode,
data.element("PV1").element("PV1.2").getText() as PatientClass,  
data.element("EVN").element("EVN.1").getText() as EventCode,
TO_DATEF(data.element("EVN").element("EVN.2").element("TS.1").getText(),'yyyyMMddHHmmss') as EventTime 
FROM @APPNAME@_Stream where data.getName() like "ADT%";

CREATE OR REPLACE CQ @APPNAME@_ORU_CQ
INSERT INTO @APPNAME@_ORU_CQOut
SELECT 
data.element("MSH").element("MSH.6").element("HD.1").getText() as Facility,
TO_DATEF(data.element("MSH").element("MSH.7").element("TS.1").getText(),'yyyyMMddHHmmss') as MsgTime,
data.element("ORU_R01.RESPONSE").element("ORU_R01.PATIENT").element("PID").element("PID.3").element("CX.1").getText() as PatientID,
data.element("ORU_R01.RESPONSE").element("ORU_R01.ORDER_OBSERVATION").element("OBR").element("OBR.4").element("CE.1").getText() as OrderIdentifier,
data.element("ORU_R01.RESPONSE").element("ORU_R01.ORDER_OBSERVATION").element("OBR").element("OBR.4").element("CE.2").getText() as OrderText 
FROM @APPNAME@_Stream where data.getName() like "ORU%";

CREATE TARGET @APPNAME@_FileTarget USING FileWriter ()
FORMAT USING Global.XMLFormatter  (
  rootelement: 'document' )
INPUT FROM @APPNAME@_Stream;

CREATE TARGET @APPNAME@_OLTPTarget USING DatabaseWriter ()
INPUT FROM @APPNAME@_ADT_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_NoSqlTarget USING MongoDBWriter ()
INPUT FROM @APPNAME@_ADT_CQOut;

--CREATE OR REPLACE TARGET @APPNAME@_KafkaTarget USING KafkaWriter VERSION @KAFKA_VERSION@()
--FORMAT USING JSONFormatter (
--schemaFileName: 'avroSchema'
--)
--INPUT FROM @APPNAME@_ORU_CQOut;

CREATE TARGET @APPNAME@_DWHTarget USING BigQueryWriter ()
INPUT FROM @APPNAME@_ORU_CQOut;

CREATE TARGET @APPNAME@_ADLSTarget USING AdlsGen2Writer ()
FORMAT USING Global.JSONFormatter  ()
INPUT FROM @APPNAME@_ORU_CQOut;

CREATE OR REPLACE CQ @APPNAME@_ADT_FilterCQ
INSERT INTO @APPNAME@_AlertCQ
SELECT 
PatientID as PatientID,
PatientClass as PatientClass
FROM @APPNAME@_ADT_CQOut p where  p.PatientClass='E';;

CREATE OR REPLACE CQ @APPNAME@_AlertEventCQ
INSERT INTO @APPNAME@_AlertStream
SELECT 
'Patient Found' as name,
'Emergency' as keyVal,
'info' as severity,
'raise' as flag,
'Flagged patient `' + f.PatientID +'` is an Emergency Patient. NEED IMMEDIATE ATTENTION!!!' as message
FROM @APPNAME@_AlertCQ f;

CREATE OR REPLACE TARGET @APPNAME@_SlackTarget USING SlackAlertAdapter ()
INPUT FROM @APPNAME@_AlertStream;

END APPLICATION @APPNAME@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

create application KinesisTest RECOVERY 1 SECOND INTERVAL;
CREATE OR REPLACE SOURCE ora_reader USING OracleReader (
  Username: 'miner',
  Password: 'miner',
  ConnectionURL: '192.168.1.113:1521:ORCL',
  TABLES: 'QATEST.H_REGION;QATEST.H_NATION;QATEST.H_CUSTOMER',
  FetchSize: '1'
 )
OUTPUT TO DDLCDCStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	accesskeyid:'ACCESS_KEY',
	secretaccesskey:'SECRET_KEY'
)
format using JSONFormatter (
)
input from DDLCDCStream;

end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

STOP IS2noder.IS2Node;
UNDEPLOY APPLICATION IS2noder.IS2Node;
DROP APPLICATION IS2noder.IS2Node CASCADE;

CREATE APPLICATION IS2Node;

CREATE FLOW ISFLOW1;
----------------------------------------------------
CREATE source implicitSOurce USING FileReader (
      directory:'@TEST-DATA-PATH@',
      columndelimiter: ',',
      wildcard:'ISdata.csv',
      blocksize: 10240,
      positionByEOF:false
)
PARSE USING DSVParser (
      header:False,
      trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity double,
  size long,
  currentDate DateTime);

END FLOW ISFLOW1;
----------------------------------------------------

CREATE FLOW ISFLOW2;

CREATE CACHE cache1 USING CsvReader(
  directory: '@TEST-DATA-PATH@',
  wildcard: 'ISdata.csv',
  header: false,
  columndelimiter: ',',
  trimquote: false
  ) QUERY (keytomap:'productID') OF Atm;


CREATE STREAM newStream OF Atm;


CREATE CQ newCQ
INSERT INTO newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]), TO_DATE(data[5]) FROM
CsvStream;

CREATE WINDOW win1
OVER newStream
KEEP 50 rows;


CREATE CQ newCQ2
INSERT INTO newStream2
SELECT productID as A , stateID AS B, productWeight AS C, quantity AS D, size AS E, currentDate AS F FROM
newStream;


CREATE CQ newCQ3
INSERT INTO newStream3 PARTITION BY A
SELECT A,B,C,D,E,F FROM newStream2 order by C,D
link source event;

CREATE CQ newCQ4
INSERT INTO newStream4
SELECT count(productID),currentDate FROM newStream ORDER BY currentDate
link source event;

CREATE CQ newCQ5
INSERT INTO newStream5
SELECT x.*, y.* from cache1 x, newStream y WHERE x.productweight > 6 ORDER BY x.currentDate;


CREATE WACTIONSTORE WS1 CONTEXT OF Atm
EVENT TYPES(Atm );

CREATE CQ newCQ6
INSERT INTO WS1
SELECT * FROM newStream WHERE productID = '001';

CREATE CQ newCQ7
INSERT INTO newStream6
SELECT aa.productID FROM WS1 [push] aa, cache1 bb;


CREATE CQ newCQ8
INSERT INTO newStream7
SELECT Sum(X.size) FROM (Select size from win1 where productweight > 5) X;

END FLOW ISFLOW2;
----------------------------------------------------

END APPLICATION IS2Node;

STOP cacheCase;
UNDEPLOY APPLICATION cacheCase;
DROP APPLICATION cacheCase cascade;

CREATE APPLICATION cacheCase;


CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity double,
  size long,
  currentDate DateTime);

CREATE CACHE cAcHe1 USING CsvReader(
  directory: '@TEST-DATA-PATH@',
  wildcard: 'ISdata.csv',
  header: false,
  columndelimiter: ',',
  trimquote: false
  ) QUERY (keytomap:'PRODUCTID') OF Atm;


CREATE WACTIONSTORE WS1 CONTEXT OF Atm
EVENT TYPES
(Atm );


CREATE CQ cq1
INSERT INTO ws1
SELECT * FROM CACHE1;

END APPLICATION cacheCase;
DEPLOY APPLICATION cacheCase;
START cacheCase;

STOP APPLICATION app1;
STOP APPLICATION app2;
STOP APPLICATION app3;
STOP APPLICATION app4;
STOP APPLICATION app5;
STOP APPLICATION app6;
STOP APPLICATION app7;
UNDEPLOY APPLICATION app1;
UNDEPLOY APPLICATION app2;
UNDEPLOY APPLICATION app3;
UNDEPLOY APPLICATION app4;
UNDEPLOY APPLICATION app5;
UNDEPLOY APPLICATION app6;
UNDEPLOY APPLICATION app7;
DROP APPLICATION app1 CASCADE;
DROP APPLICATION app2 CASCADE;
DROP APPLICATION app3 CASCADE;
DROP APPLICATION app4 CASCADE;
DROP APPLICATION app5 CASCADE;
DROP APPLICATION app6 CASCADE;
DROP APPLICATION app7 CASCADE;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',
acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

CREATE APPLICATION app1 RECOVERY 1 SECOND INTERVAL;

create flow sourceflow;

create type type1(
  id String,
  name String,
  city string
);

CREATE STREAM rawstream OF Global.waevent persist using KafkaPropset;
CREATE STREAM kps_typedStream OF type1 partition by city persist using KafkaPropset;
CREATE STREAM sourcestream of Global.waevent;

CREATE OR REPLACE SOURCE s USING oracleReader  (
  Username:'qatest',
  Password:'qatest',
  ConnectionURL:'localhost:1521/xe',
  Tables:'QATEST.test%',
  FetchSize:1
 )
OUTPUT TO rawstream;

end flow sourceflow;
create flow targetflow;
create cq cq1
INSERT INTO sourcestream
SELECT * from rawstream;

CREATE CQ cq2
INSERT INTO kps_typedStream
SELECT TO_STRING(data[0]),
TO_STRING(data[1]),
TO_STRING(data[2])FROM rawstream;
end flow targetflow;

end application app1;
-- deploy application app1;
-- deploy application app1 with sourceflow in AGENTS, targetflow on any in default;

CREATE APPLICATION app2 RECOVERY 1 SECOND INTERVAL;

CREATE TARGET app2_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS1'
) INPUT FROM rawstream;


end application app2;
deploy application app2;


CREATE APPLICATION app3 RECOVERY 1 SECOND INTERVAL;

CREATE TARGET app3_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test02,QATEST.KPS2'
) INPUT FROM rawstream;

end application app3;
deploy application app3;


CREATE APPLICATION app4 RECOVERY 1 SECOND INTERVAL;

CREATE TARGET app4_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test03,QATEST.KPS3'
) INPUT FROM rawstream;

end application app4;
deploy application app4;


CREATE APPLICATION app5 RECOVERY 1 SECOND INTERVAL;

CREATE TARGET app5_target1 USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'snappy',
KafkaConfig:'compression.type=snappy'
)
FORMAT USING DSVFormatter ()
INPUT FROM kps_typedStream;

CREATE TARGET app5_target2 USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'gzip',
KafkaConfig:'compression.type=gzip'
)
FORMAT USING DSVFormatter ()
INPUT FROM rawstream;

CREATE TARGET app5_target3 USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'lz4',
KafkaConfig:'compression.type=lz4'
)
FORMAT USING DSVFormatter ()
INPUT FROM rawstream;


end application app5;
deploy application app5;

CREATE APPLICATION app6 RECOVERY 1 SECOND INTERVAL;

CREATE TARGET app6_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test03,QATEST.KPS4'
) INPUT FROM rawstream;

end application app6;
deploy application app6;

CREATE APPLICATION app7 RECOVERY 1 SECOND INTERVAL;

CREATE TARGET app7_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS_NEW1;qatest.test02,QATEST.KPS_NEW2;qatest.test03,QATEST.KPS_NEW3;'
) INPUT FROM rawstream;

end application app7;
deploy application app7;

stop application OneAgentWithMultiTester.AgentWithMultiReader;
undeploy application OneAgentWithMultiTester.AgentWithMultiReader;
drop application OneAgentWithMultiTester.AgentWithMultiReader cascade;

create application AgentWithMultiReader;


CREATE FLOW AgentFlow;

create source XMLSource using FileReader (
  Directory:'@TEST-DATA-PATH@',
  WildCard:'books.xml',
  positionByEOF:false
)
parse using XMLParser (
  RootNode:'/catalog/book'
)
OUTPUT TO XmlStream;

create source DSVCSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'customerdetails-agent.csv',
  charset: 'UTF-8',
  positionByEOF:false
)
parse using DSVParser (
  header:'no'
)
OUTPUT TO DSVCsvStream;

-- Read from File

create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'StoreNames.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CSVStream;

END FLOW AgentFlow;

--  Rest of the Stream and CQs are executed in Server flow

CREATE FLOW ServerFlow;

CREATE TARGET myout using LogWriter(name: XMLSource, filename:'@FEATURE-DIR@/logs/logXML.txt') input from XmlStream;
CREATE TARGET myout1 using LogWriter(name: DSVSource, filename:'@FEATURE-DIR@/logs/logDSV.txt', charset:'UTF-8') input from DSVCsvStream;
CREATE TARGET myout2 using LogWriter(name: CSVSource, filename:'@FEATURE-DIR@/logs/logCSV.txt') input from CSVStream;


END FLOW ServerFlow;

end application AgentWithMultiReader;
DEPLOY APPLICATION AgentWithMultiReader with AgentFlow in AGENTS, ServerFlow on any in default;

start AgentWithMultiReader;

CREATE TARGET @TARGET_NAME@ USING RedshiftWriter
	(
	  ConnectionURL: '@CONNECTION_URL@',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  accesskeyId: 'access_key',
	  secretaccesskey: 'secret_access',
	  Tables: 'tgt_table',
	  uploadpolicy:'eventcount:10,interval:1m'
	) INPUT FROM @STREAM@;

CREATE FLOW @STREAM@_SourceFlow;

CREATE SOURCE @SOURCE_NAME@ Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
   _h_ReturnDateTimeAs:'ZonedDateTime'
) OUTPUT TO @STREAM@;

END FLOW @STREAM@_SourceFlow;

STOP APPLICATION OJETTOBIGQUERY;
UNDEPLOY APPLICATION OJETTOBIGQUERY;
DROP APPLICATION OJETTOBIGQUERY CASCADE;

--create application 
CREATE APPLICATION OJETTOBIGQUERY RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE ojetSrc USING Ojet (
 ConnectionURL: '192.168.123.12:1521/ORCL',
 Tables: 'QATEST.OJETTOBIGQALLDATATYPE',
 Username: 'qatest',
 Password: 'qatest',
 FetchSize:1
) OUTPUT TO CDCStream;

CREATE OR REPLACE TARGET bqtables using BigqueryWriter(
 serviceAccountKey:"/Users/karthikmurugan/Downloads/bqtest-540227c31980.json",
 projectId:"bqtest-158706",
 datalocation: 'US',
 Tables: "QATEST.OJETTOBIGQALLDATATYPE,QATEST.OJETTOBIGQALLDATATYPE",
 BatchPolicy: "eventCount:1,Interval:90")
INPUT FROM CDCStream;


CREATE OR REPLACE TARGET T1 using SysOut(name :Foo2out) INPUT FROM CDCStream;

END APPLICATION OJETTOBIGQUERY;

DEPLOY APPLICATION OJETTOBIGQUERY;
START APPLICATION OJETTOBIGQUERY;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;


CREATE SOURCE @APPNAME@_Source USING MySqlReader
(
  Compression: false,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'mysql://localhost:3306',
  DatabaseName: 'waction',
  Tables: 'waction.test01',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root'
)
OUTPUT TO @APPNAME@_Stream;


CREATE TARGET @APPNAME@_Target USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	BatchPolicy: 'EventCount: 1; Interval: 1s',
	instanceId: 'qatest'
) INPUT FROM @APPNAME@_stream;

create Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_stream;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;

drop propertyset OrcToOrcPlatfm_App_KafkaPropset;
drop stream  OrcToOrcPlatfm_Stream CASCADE;


--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;

CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNEC4TION_URL@',
  Tables: '$table1',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING AzureSQLDWHWriter  (
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'QATEST.ORCTOORCPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING AzureSQLDWHWriter  (
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
 )
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '$table2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@4',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING AzureSQLDWHWriter  (
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'QATEST.ORCTOORCPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING AzureSQLDWHWriter  (
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
 )
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;

END APPLICATION @APPNAME@2;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING DatabaseReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

END APPLICATION @APPNAME@;

stop application ADW;
undeploy application ADW;
drop application ADW cascade;
CREATE APPLICATION ADW;

CREATE  SOURCE OjetIL USING DatabaseReader  
 (
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'@SOURCE-TABLES@',
 FetchSize:2000
) 
OUTPUT TO InitialLoadStream;

CREATE TARGET AzureDWInitialLoad USING AzureSQLDWHWriter(
ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
)
INPUT FROM InitialLoadStream;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

stop application MSSQLTransactionSupportFTMFTBTrue;
undeploy application MSSQLTransactionSupportFTMFTBTrue;
drop application MSSQLTransactionSupportFTMFTBTrue cascade;

CREATE APPLICATION MSSQLTransactionSupportFTMFTBTrue recovery 1 second interval;

Create Source ReadFromMSSQL2
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
FetchTransactionMetadata:'true',
FilterTransactionBoundaries: true,
Compression:'false',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportFTMFTBTrueStream;


CREATE TARGET WriteToMSSQL2 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC@,@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportFTMFTBTrueStream;

CREATE TARGET MSSqlReaderOutput2 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportFTMFTBTrueStream; 


CREATE OR REPLACE TARGET MSSQLFileOut2 USING FileWriter  ( 
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  directory:'@FEATURE-DIR@/logs/',
  filename: 'TransactionSupportFTMFTBTrue.txt'
 ) 
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 ) 
INPUT FROM MSSQLTransactionSupportFTMFTBTrueStream;

END APPLICATION MSSQLTransactionSupportFTMFTBTrue;
deploy application MSSQLTransactionSupportFTMFTBTrue;
start application MSSQLTransactionSupportFTMFTBTrue;

stop APPLICATION @AppName@;
Undeploy APPLICATION @AppName@;
drop APPLICATION @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @AgentFlow@1;
CREATE OR REPLACE SOURCE @SourceName@1 USING Global.MSJet (
  Tables: 'dbo.compsrc',
  username: 'qatest',
  DatabaseName: 'qatest',
  FetchTransactionMetadata: true,
  filterTransactionBoundaries: true,
  compression: false,
  ConnectionURL: '@ConnectionURL@1',
  Mode: '@mode@',
  CommittedTransactions: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
  SendBeforeImage: true,
  password: 'w3b@ct10n' )
OUTPUT TO @StreamName@;
END FLOW @AgentFlow@1;

CREATE FLOW @AgentFlow@2;
CREATE OR REPLACE SOURCE @SourceName@2 USING Global.MSJet (
  Tables: 'dbo.compsrc',
  username: 'qatest',
  DatabaseName: 'qatest',
  FetchTransactionMetadata: true,
  filterTransactionBoundaries: true,
  compression: false,
  ConnectionURL: '@ConnectionURL@2',
  CaptureType: '@captureType@',
  CommittedTransactions: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
  SendBeforeImage: true,
  password: 'w3b@ct10n' )
OUTPUT TO @StreamName@;
END FLOW @AgentFlow@2;

CREATE TARGET @SysTarget@ USING Global.SysOut (
  name: 'MS_CDC_SYSOUT' )
INPUT FROM @StreamName@;

CREATE FLOW @ServerFlow@1;
CREATE TARGET @TargetName@1 USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ParallelThreads: '',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: '@ConnectionURL@',
  StatementCacheSize: '50',
  DatabaseProviderType: 'Default',
  Username: 'qatest',
  Tables: 'dbo.compsrc,dbo.comptar',
  Password: 'w3b@ct10n',
  PreserveSourceTransactionBoundary: 'false',
  BatchPolicy: 'EventCount:1,Interval:60' )
INPUT FROM @StreamName@;
END FLOW @ServerFlow@1;

CREATE FLOW @ServerFlow@2;
CREATE TARGET @TargetName@2 USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ParallelThreads: '',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: '@ConnectionURL@',
  StatementCacheSize: '50',
  DatabaseProviderType: 'Default',
  Username: 'qatest',
  Tables: 'dbo.compsrc,dbo.comptar',
  Password: 'w3b@ct10n',
  PreserveSourceTransactionBoundary: 'false',
  BatchPolicy: 'EventCount:1,Interval:60' )
INPUT FROM @StreamName@;
END FLOW @ServerFlow@2;

END APPLICATION @AppName@;
DEPLOY APPLICATION @AppName@ with @AgentFlow@1 in AGENTS, @AgentFlow@2 in AGENTS, @ServerFlow@1 on any in default, @ServerFlow@2 on any in default;
START APPLICATION @AppName@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ (
  positionbyeof: false
)
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING ParquetFormatter (
  schemaFileName: 'parquetSchema'
)
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE FLOW @appname@AgentFlow;
    CREATE SOURCE @parquetsrc@ USING FileReader (
    wildcard: '',
    directory: '',
    positionbyeof: false )
    PARSE USING ParquetParser (
    )
    OUTPUT TO @appname@Stream;
END FLOW @appname@AgentFlow;

CREATE FLOW @appname@serverFlow;
    CREATE CQ @appname@CQ
    INSERT INTO @appname@CqOut
        SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;
    
    CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
    filename: '',
    directory: '',
    flushpolicy: 'EventCount:1,Interval:30s',
    rolloverpolicy: 'EventCount:10000,Interval:30s' )
    FORMAT USING ParquetFormatter  (
    schemaFileName: ''
    )
    INPUT FROM @appname@CqOut;
END FLOW @appname@serverFlow;

END APPLICATION @appname@;
DEPLOY APPLICATION @appname@ with @appname@AgentFlow in Agents, @appname@ServerFlow in default;
start application @appname@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING MySqlReader
(
  Compression: false,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'mysql://localhost:3306',
  DatabaseName: 'waction',
  Tables: 'waction.test01',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root',
  connectionRetryPolicy:'retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'waction.test01,@DATASET@.% KEYCOLUMNS(id,col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;

--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

STOP TestAlertsEmail.TestAlertsEmailApp;
UNDEPLOY APPLICATION TestAlertsEmail.TestAlertsEmailApp;
DROP APPLICATION TestAlertsEmail.TestAlertsEmailApp CASCADE;

CREATE APPLICATION TestAlertsEmailApp;

CREATE source rawSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:No,
  wildcard:'@TESTDATAFILE@',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO rawStream;

CREATE STREAM MyAlertStream OF Global.AlertEvent;
CREATE CQ GenerateMyAlerts
INSERT INTO MyAlertStream (name, keyVal, severity, flag, message)
SELECT "Testing Alerts", data[0], data[1], data[2], data[3]
FROM rawStream s;
CREATE TARGET output2 USING SysOut(name : alertsrecevied) input FROM MyAlertStream;

CREATE SUBSCRIPTION alertSubscription USING EmailAdapter
(
SMTPUSER:'@Alerts_Smtpuser@',
--, ${alerts.smtpuser}
SMTPUSER:' ${alerts.smtpuser}',
SMTPPASSWORD:'@Alerts_Smtppassword@',
smtpurl:"@Alerts_Smtpurl@",
starttls_enable:"@Alerts_Starttls_enable@",
smtp_auth:"@Alerts_Smtp_auth@",
subject:"@Alerts_Subject@",
emailList:"@Alerts_Emaillist@",
userIds:"@Alerts_UserId@",
threadCount:"@Alerts_Threadcount@",
@CONTENTTYPE@
senderEmail:"@Alerts_SenderEmail@",
)
INPUT FROM MyAlertStream;

END APPLICATION TestAlertsEmailApp;
DEPLOY APPLICATION TestAlertsEmail.TestAlertsEmailApp;
START TestAlertsEmail.TestAlertsEmailApp;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
 )
INPUT FROM @STREAM@2;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;

Create Source @SourceName@ Using Ojet

(
  Username:'c##qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',
  Tables:'CDB$ROOT."C##QATEST".ojet_src;ORCLPDB.QATEST.ojet_src',
  _h_useClassic:false,
  Fetchsize:1,
  Compression: true,
  SupportPDB:true,
  ReplicationSlotName:'null'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',
  Username:'c##qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'CDB$ROOT."C##QATEST".ojet_src,CDB$ROOT."C##QATEST".ojet_tgt'
) INPUT FROM @SRCINPUTSTREAM@;



create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 10 second interval;
create source CSVSource using FileReader (
	directory:'@DIR@',
	WildCard:'@WILDCARD@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  curr String,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       data[6],
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target BlobT using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:30,interval:5s'
)
format using AvroFormatter (
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

stop @appName@;
undeploy application @appName@;
drop application @appName@ cascade;

CREATE OR REPLACE APPLICATION @appName@;

-x-

CREATE SOURCE @appName@_Source USING Global.MysqlReader (
  Username: '@UserName@',
  ConnectionURL: '@ConnectionURL@',
  Password: '@Password@',
  Tables: '@SourceTable@' )
OUTPUT TO @appName@_st1 ;

-x-

CREATE TYPE @appName@_cache_output_Type (
 product_id java.lang.Integer,
 product_name java.lang.String,
 discount java.lang.Integer);

CREATE TYPE @appName@_cache_type (
 p_id java.lang.Integer KEY,
 category java.lang.String,
 quantity java.lang.Integer,
 discount java.lang.Integer);

CREATE OR REPLACE STREAM @appName@_cache_output_st OF @appName@_cache_output_Type;

CREATE OR REPLACE CACHE @appName@_cache_comp USING DatabaseReader (
  Query: 'select * from @LookUpTableForCache@',
  username: '@UserName@',
  FetchSize: 1,
  ConnectionURL: '@ConnectionURL@',
  password: '@Password@' )
QUERY (
  keytomap: 'p_id'
  )
OF  @appName@_cache_type;

CREATE OR REPLACE CQ @appName@_cache_output_cq
INSERT INTO @appName@_cache_output_st
SELECT
      p.data[0] as product_id,
      p.data[1] as product_name,
      pcache.discount as discount
   FROM
   @appName@_st1 p INNER JOIN @appName@_cache_comp pcache ON TO_INT(p.data[0]) = pcache.p_id;

CREATE OR REPLACE TARGET @appName@_cacheTarget USING Global.DatabaseWriter (
  CommitPolicy: 'EventCount:10,Interval:10',
  BatchPolicy: 'EventCount:10,Interval:10',
  Username: '@UserName@',
  ConnectionURL: '@ConnectionURL@',
  Password: '@Password@',
  Tables: '@cache_target_table@' )
INPUT FROM @appName@_cache_output_st;

-x-

CREATE TYPE @appName@_external_cache_type (
 p_id java.lang.Integer KEY,
 category java.lang.String,
 quantity java.lang.Integer,
 discount java.lang.Integer);

CREATE EXTERNAL CACHE @appName@_external_cache_comp (
  Columns: 'p_id,category,quantity,discount',
  KeyToMap: 'p_id',
  AdapterName: 'DatabaseReader',
  Table: '@LookUpTableForExternalCache@',
  Username: '@UserName@',
  ConnectionURL: '@ConnectionURL@',
  Password: '@Password@' )
OF @appName@_external_cache_type;

CREATE CQ @appName@_external_cache_output_cq
INSERT INTO @appName@_external_cache_output_st
SELECT p.data[0] as product_id,p.data[1] as product_name,ecp.quantity as quantity
  FROM @appName@_st1  p inner join @appName@_external_cache_comp ecp
  on TO_INT(p.data[0]) = ecp.p_id;

CREATE OR REPLACE TARGET @appName@_externalCacheTarget USING Global.DatabaseWriter (
  CommitPolicy: 'EventCount:10,Interval:10',
  BatchPolicy: 'EventCount:10,Interval:10',
  Username: '@UserName@',
  ConnectionURL: '@ConnectionURL@',
  Password: '@Password@',
  Tables: '@externalCache_target_table@' )
INPUT FROM @appName@_external_cache_output_st;

-x-

END APPLICATION @appName@;

create application XML;
create source CSVSource using FileReader (
	directory:'Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
	filename:'posdata_XML',
	rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'
)
format using XMLFormatter (
	rootelement:'document',
	elementtuple:'MerchantName:merchantid:text=merchantname'
)
input from TypedCSVStream;
end application XML;

--
-- Crash Recovery Test 5 with Jumping window and partitioned on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION N2S2CR5Tester.N2S2CRTest5;
UNDEPLOY APPLICATION N2S2CR5Tester.N2S2CRTest5;
DROP APPLICATION N2S2CR5Tester.N2S2CRTest5 CASCADE;
CREATE APPLICATION N2S2CRTest5 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest5;

CREATE SOURCE CsvSourceN2S2CRTest5 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN2S2CRTest5;

CREATE FLOW DataProcessingN2S2CRTest5;

CREATE TYPE CsvDataTypeN2S2CRTest5 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvDataTypeN2S2CRTest5 PARTITION BY merchantId;

CREATE CQ CsvToDataN2S2CRTest5
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsN2S2CRTest5 CONTEXT OF CsvDataTypeN2S2CRTest5
EVENT TYPES ( CsvDataTypeN2S2CRTest5 )
@PERSIST-TYPE@

CREATE CQ DataToWactionN2S2CRTest5
INSERT INTO WactionsN2S2CRTest5
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN2S2CRTest5;

END APPLICATION N2S2CRTest5;

stop @APPNAME@;
undeploy application @APPNAME@;
--drop exceptionstore admin.MSSQLServer_To_MSSQLServerApp_ExceptionStore;
drop application @APPNAME@ cascade;
create application @APPNAME@ use exceptionstore;


Create Source @SourceName@ Using MSSqlReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'@SourceTable@',
 ConnectionPoolSize:1,
 StartPosition:'EOF'
 ) Output To @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;


 CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@SourceTable@,@TargetTable@',
    CommitPolicy: 'Interval:5'

) INPUT FROM @SRCINPUTSTREAM@;

create or replace cq @cq@
insert into @finalstream@
select exceptionType,action,appName,entityType,entityName,className,message,relatedActivity from @APPNAME@_ExceptionStore;

Create target @targetfile@ using filewriter (
filename:'@APPNAME@_file.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using jsonFormatter()
input from @finalstream@;


end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop @APPNAME@_ExpStore;
undeploy application @APPNAME@_ExpStore;
drop application @APPNAME@_ExpStore cascade;
CREATE APPLICATION @APPNAME@_ExpStore;

CREATE TYPE @APPNAME@_ExpStore_CDCStreams_Type  (
  evtlist java.util.List
 );

CREATE STREAM @APPNAME@_ExpStore_CDCStreams OF @APPNAME@_ExpStore_CDCStreams_Type;

CREATE CQ @APPNAME@_ReadFromExpStore
INSERT INTO @APPNAME@_ExpStore_CDCStreams
select to_waevent(s.relatedObjects) as evtlist from admin.@APPNAME@_ExceptionStore [jumping 5 second] s;

CREATE STREAM @APPNAME@_ExpStore_CDCEventStream OF Global.WAEvent;

CREATE CQ @APPNAME@_ExpStore_GetCDCEvent
INSERT INTO @APPNAME@_ExpStore_CDCEventStream
SELECT com.webaction.proc.events.WAEvent.makecopy(cdcevent) FROM @APPNAME@_ExpStore_CDCStreams a, iterator(a.evtlist) cdcevent;

CREATE CQ @APPNAME@_ExpStore_JoinDataCQ
INSERT INTO @APPNAME@_ExpStore_JoinedDataStream
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1])
        from @APPNAME@_ExpStore_CDCEventStream f;

CREATE OR REPLACE TARGET @APPNAME@_ExpStore_WriteToFileAsJSON USING FileWriter  (
  filename: 'expEvent_MSSQL',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:1,interval:30',
  adapterName: 'FileWriter',
  directory: 'ExpStore_logs',
  rolloverpolicy: 'eventcount:6,interval:30s'
 )
FORMAT USING JSONFormatter  (
  handler: 'com.webaction.proc.JSONFormatter',
  formatterName: 'JSONFormatter',
  jsonMemberDelimiter: '\n',
  jsonobjectdelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true'
 )
INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;

CREATE TARGET @APPNAME@_ExpStore_dbtarget USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1,Interval:1',
CommitPolicy:'Interval:1,Eventcount:1',
Tables:'@TargetTable@'
) INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;

END APPLICATION @APPNAME@_ExpStore;

deploy application @APPNAME@_ExpStore;
start @APPNAME@_ExpStore;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallretaildata2M.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
storeId String,
nameId String,
city String,
state String

);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],
data[1],
data[2],
data[3]

FROM CsvStream;

create Target t using FileWriter(
filename:'EventNCDefault',
directory:'@FEATURE-DIR@/logs/',
sequence:'00',
--filelimit: '5',
rolloverpolicy:'eventcount:-100'
)
format using DSVFormatter (

)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetNegativeECDefault_actual.log') input from TypedCSVStream;
end application DSV;
DEPLOY APPLICATION DSV on any in default;
START DSV;
deploy application DSV;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW ;

CREATE STREAM Oracle_ChangeDataStream of Global.WAEvent;

CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: false,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.56.101:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream1;


CREATE CQ CQfilter
INSERT INTO Oracle_ChangeDataStream
select putuserdata (data1,'IntToInt', data[0]) from Oracle_ChangeDataStream1 data1
where (META(data1, 'OperationName').toString() =='INSERT' or META(data1, 'OperationName').toString() =='UPDATE');

CREATE STREAM Oracle_DataStream of Global.WAEvent;

CREATE CQ CQfilter1
INSERT INTO Oracle_DataStream
select * from Oracle_ChangeDataStream c where to_int(USERDATA(c, 'IntToInt'))<4;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:60',
  CommitPolicy: 'EventCount:1,Interval:60',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  Tables: 'QATEST.OracToCql_alldatatypes,test.oractocq_alldatatypes columnmap(IntToInt=IntToInt)',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_DataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

stop application JMSWriter.JMS;
undeploy application JMSWriter.JMS;
drop application JMSWriter.JMS cascade;

create application JMS;
create source JMSCSVSource using FileReader (
        directory:'@TEST-DATA-PATH@',
        WildCard:'AdhocQueryData2.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target JmsTarget  using JMSWriter (
		Provider:'@JMSWRITERPROVIDER@',
		Ctx:'@JMSWRITERCONTEXT@',
		messagetype: @MESSAGETYPE@,
		UserName:'@JMSWRITERUSERNAME@',
		Password:'@JMSWRITERPASSWORD@',
		@DESTINATIONTYPE@)
format using @JMSTARGETFORMATTERTYPE@ (
@JMSTARGETFORMATTERMEMBERS@
)
input from TypedCSVStream;

end Application Jms;

STOP APPLICATION AgenCQTester.CSV;
UNDEPLOY APPLICATION AgenCQTester.CSV;
DROP APPLICATION AgenCQTester.CSV cascade;

create application CSV;

CREATE FLOW AgentFlow;
create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails-agent.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStream;

CREATE TYPE MyType (
    PAN String,
    FNAME String
);

CREATE STREAM TypedStream of MyType;

CREATE CQ TypeConversionCQ
INSERT INTO TypedStream
SELECT data[0], data[1]
from CsvStream;

END FLOW AgentFlow;

CREATE FLOW ServerFlow;
CREATE TARGET myout1 using LogWriter(name: CQSource, filename:'@FEATURE-DIR@/logs/logCQ.txt') input from TypedStream;
END FLOW ServerFlow;

end application CSV;
DEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;
START CSV;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING JSONParser ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
wildcard: '',
directory: '',
positionbyeof: false )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING AvroFormatter  (
schemaFileName: 'AvroFileSchema'
)
INPUT FROM @appname@CqOut;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using AvroFormatter (
schemaFileName: 'AvroS3Schema'
)
input from @appname@CqOut;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using AvroFormatter (
schemaFileName: 'AvroAzureSchema'
)
INPUT FROM @appname@CqOut;

CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using AvroFormatter (
schemaFileName: 'AvroGCSSchema'
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCENAME@ USING IncrementalBatchReader  (
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: @startPosition@,
  PollingInterval: '120sec'
  )
  OUTPUT TO @STREAM@;

  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;

  CREATE TARGET @targetName@ USING DatabaseWriter(
    ConnectionURL:'@READER-URL@',
    Username:'@READER-UNAME@',
    Password:'@READER-PASSWORD@',
    BatchPolicy:'Eventcount:1,Interval:1',
    CommitPolicy:'Eventcount:1,Interval:1',
    Checkpointtable:'CHKPOINT',
    Tables:'@WATABLES@,@WATABLES@_target'
  ) INPUT FROM @STREAM@;

  END APPLICATION @APPNAME@;

  DEPLOY APPLICATION @APPNAME@;
  start application @APPNAME@;

STOP @appName@;
UNDEPLOY application @appName@;
DROP application @appName@ cascade;

CREATE APPLICATION @appName@ @recovery@;

CREATE SOURCE @appName@_MongoDBReader USING MongoDBReader
(
  QuiesceOnILCompletion:'true',
  collections:'',
  userName:'',
  password:'',
  connectionUrl:'',
  mode:''
)
OUTPUT TO @appName@_MOut;

CREATE CQ @appName@_MCQ1
INSERT INTO @appName@_MCQOut1
SELECT
data.get('_id').textValue() as _id,
data.get('index').toString() as index,
data.get('isActive').toString() as isActive,
data.get('age').toString() as age,
data.get('balance').toString() as balance,
data.get('name').textValue() as name,
data.get('gender').textValue() as gender,
data.get('company').textValue() as company,
data.get('email').textValue() as email,
data.get('phone').textValue() as phone,
data.get('registered').textValue() as registered,
data.get('latitude').toString() as latitude,
data.get('longitude').toString() as longitude
FROM @appName@_MOut m;;

CREATE TARGET @appName@_AzureEventHubWriter USING AzureEventHubWriter
(
  SASKey:'',
  EventHubName:'',
  EventHubNamespace:'',
  SASPolicyName:'',
  BatchPolicy:'Size:1000000,Interval:10s',
  ConsumerGroup:'default'
)
FORMAT USING JSONFormatter  (
 )
INPUT FROM @appName@_MCQOut1;

END APPLICATION @appName@;

CREATE STREAM @STREAM@_JSON OF Global.JsonNodeEvent;

CREATE TARGET @TARGET_NAME@ USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
) INPUT FROM @STREAM@_JSON;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW ;

 

CREATE OR REPLACE SOURCE DBSource USING DatabaseReader  ( 
  Username: '@SOURCE_USER@',
  Password_encrypted: false,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Query: "SELECT * FROM qatest.MssqlToCql_alldatatypes",
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SOURCE_PASS@'
 ) 
OUTPUT TO Oracle_ChangeDataStream;


CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1000,Interval:0',
  CommitPolicy: 'EventCount:1000,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '@GRPPOLICY@',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'MultipleEvents',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

create Target KafkaTarget using KafkaWriter VERSION '2.1.0' (
brokerAddress:'',
Topic:'',
Mode: 'Sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;linger.ms=30000'
)
FORMAT USING JSONFormatter (
members:'data')
input from CCBStream;

create source KafkaSource using KafkaReader VERSION '2.1.0'(
brokerAddress:'',
	Topic:''
)
parse using JSONParser ()
output to KafkaStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING JSONFormatter ()
INPUT FROM KafkaStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream1@RANDOM@ OF Global.waevent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc1 USING JMSReader (
  ProviderName: '',
  UserName: '',
  transactionpolicy: '',
  Provider: '',
  Ctx: '',
  EnableTransaction: '',
  QueueName: '',
  Topic:'',
  Password: '' )
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@PersistStream1@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt1 USING Global.FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING Global.DSVFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream1@RANDOM@;

CREATE STREAM @APPNAME@PersistStream2@RANDOM@ OF Global.JSONNodeEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc2 USING JMSReader (
  ProviderName: '',
  UserName: '',
  transactionpolicy: '',
  Provider: '',
  Ctx: '',
  EnableTransaction: '',
  QueueName: '',
  Topic:'',
  Password: '' )
PARSE USING JSONParser ()
OUTPUT TO @APPNAME@PersistStream2@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt2 USING Global.FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING Global.JSONFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream2@RANDOM@;

CREATE STREAM @APPNAME@PersistStream3@RANDOM@ OF Global.waevent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc3 USING JMSReader (
  ProviderName: '',
  UserName: '',
  transactionpolicy: '',
  Provider: '',
  Ctx: '',
  EnableTransaction: '',
  QueueName: '',
  Topic:'',
  Password: '' )
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@PersistStream3@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt3 USING Global.FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: ''  )
FORMAT USING Global.JSONFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream3@RANDOM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

stop application Postgres_To_PostgresApp;
undeploy application Postgres_To_PostgresApp;
drop application Postgres_To_PostgresApp cascade;

CREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE Postgres_Src1 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_1',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename0'
 ) 
OUTPUT TO Change_Data_Stream ;

CREATE OR REPLACE SOURCE Postgres_Src2 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_2',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename1'
 ) 
OUTPUT TO Change_Data_Stream ;


CREATE OR REPLACE SOURCE Postgres_Src3 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_3',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename2'
 ) 
OUTPUT TO Change_Data_Stream ;

CREATE OR REPLACE SOURCE Postgres_Src4 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_4',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename3'
 ) 
OUTPUT TO Change_Data_Stream ;


CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt1 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target0;public.tablename1, public.target0;public.tablename2, public.target0;public.tablename3, public.target0;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt2 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target1;public.tablename1, public.target1;public.tablename2, public.target1;public.tablename3, public.target1;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt3 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target2;public.tablename1, public.target2;public.tablename2, public.target2;public.tablename3, public.target2;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt4 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target3;public.tablename1, public.target3;public.tablename2, public.target3;public.tablename3, public.target3;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;


end application Postgres_To_PostgresApp;
deploy application Postgres_To_PostgresApp;
start Postgres_To_PostgresApp;

DROP TYPE MerchantActivityContext;
DROP TYPE MerchantTxRate;
DROP WACTIONSTORE MerchantActivity;

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count int,
  HourlyAve int,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count int,
  totalAmount double,
  hourlyAve int,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);

-- Loading WACTIONSTORE MerchantActivity to be accessed by PosAppWS.tql

CREATE WACTIONSTORE MerchantActivity CONTEXT OF DS.MerchantActivityContext
EVENT TYPES ( DS.MerchantTxRate )
@PERSIST-TYPE@

STOP APPLICATION KafkaWPTester.KWApp;
STOP APPLICATION KafkaWPTester.KRApp;
UNDEPLOY APPLICATION KafkaWPTester.KWApp;
UNDEPLOY APPLICATION KafkaWPTester.KRApp;
DROP APPLICATION KafkaWPTester.KWApp CASCADE;
DROP APPLICATION KafkaWPTester.KRApp CASCADE;

CREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;
GRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;
CONNECT KafkaWPTester KafkaWPTester;


CREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE CSVSource USING FileReader (
	directory:'/Users/saranyad/Product/IntegrationTests/TestData/kafka_tmp',
    WildCard:'mybanks*',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:',',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;


CREATE TYPE AccessLogType(
Col1 String,
Col2 String
);

CREATE STREAM TypedAccessLogStream1 OF AccessLogType;
CREATE STREAM TypedAccessLogStream2 OF AccessLogType;
CREATE STREAM TypedAccessLogStream3 OF AccessLogType;
CREATE STREAM TypedAccessLogStream4 OF AccessLogType;
CREATE STREAM TypedAccessLogStream5 OF AccessLogType;
CREATE STREAM TypedAccessLogStream6 OF AccessLogType;
CREATE STREAM TypedAccessLogStream7 OF AccessLogType;
CREATE STREAM TypedAccessLogStream8 OF AccessLogType;
CREATE STREAM TypedAccessLogStream9 OF AccessLogType;
CREATE STREAM TypedAccessLogStream10 OF AccessLogType;
CREATE STREAM TypedAccessLogStream11 OF AccessLogType;
CREATE STREAM TypedAccessLogStream12 OF AccessLogType;
CREATE STREAM TypedAccessLogStream13 OF AccessLogType;
CREATE STREAM TypedAccessLogStream14 OF AccessLogType;
CREATE STREAM TypedAccessLogStream15 OF AccessLogType;
CREATE STREAM TypedAccessLogStream16 OF AccessLogType;
CREATE STREAM TypedAccessLogStream17 OF AccessLogType;
CREATE STREAM TypedAccessLogStream18 OF AccessLogType;
CREATE STREAM TypedAccessLogStream19 OF AccessLogType;
CREATE STREAM TypedAccessLogStream20 OF AccessLogType;

CREATE CQ AcceeslogCQ
INSERT INTO TypedAccessLogStream
SELECT 
TO_STRING(data[0]) as Col1,
TO_STRING(data[1]) as Col2
FROM FileStream ; 

CREATE CQ AcceeslogCQ1
INSERT INTO TypedAccessLogStream1
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS WHERE FS.Col1 = '1'; 


create Target KW1 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test01',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream1;

CREATE CQ AcceeslogCQ2
INSERT INTO TypedAccessLogStream2
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '2'; 


create Target KW2 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test02',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream2;

CREATE CQ AcceeslogCQ3
INSERT INTO TypedAccessLogStream3
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '3'; 


create Target KW3 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test03',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream3;

CREATE CQ AcceeslogCQ4
INSERT INTO TypedAccessLogStream4
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '4'; 


create Target KW4 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test04',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream4;

CREATE CQ AcceeslogCQ5
INSERT INTO TypedAccessLogStream5
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '5'; 


create Target KW5 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test05',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream5;

CREATE CQ AcceeslogCQ6
INSERT INTO TypedAccessLogStream6
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '6'; 


create Target KW6 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test06',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream6;

CREATE CQ AcceeslogCQ7
INSERT INTO TypedAccessLogStream7
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '7'; 


create Target KW7 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test07',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream7;

CREATE CQ AcceeslogCQ8
INSERT INTO TypedAccessLogStream8
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '8'; 


create Target KW8 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test08',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream8;

CREATE CQ AcceeslogCQ9
INSERT INTO TypedAccessLogStream9
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '9'; 


create Target KW9 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test09',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream9;

CREATE CQ AcceeslogCQ10
INSERT INTO TypedAccessLogStream10
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '10'; 


create Target KW10 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test10',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream10;

CREATE CQ AcceeslogCQ11
INSERT INTO TypedAccessLogStream11
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '11'; 


create Target KW11 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test11',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream11;

CREATE CQ AcceeslogCQ12
INSERT INTO TypedAccessLogStream12
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '12'; 


create Target KW12 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test12',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream12;

CREATE CQ AcceeslogCQ13
INSERT INTO TypedAccessLogStream13
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '13'; 


create Target KW13 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test13',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream13;

CREATE CQ AcceeslogCQ14
INSERT INTO TypedAccessLogStream14
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '14'; 


create Target KW14 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test14',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream14;

CREATE CQ AcceeslogCQ15
INSERT INTO TypedAccessLogStream15
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '15'; 


create Target KW15 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test15',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream15;

CREATE CQ AcceeslogCQ16
INSERT INTO TypedAccessLogStream16
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '16'; 


create Target KW16 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test16',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream16;

CREATE CQ AcceeslogCQ17
INSERT INTO TypedAccessLogStream17
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '17'; 


create Target KW17 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test17',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream17;

CREATE CQ AcceeslogCQ18
INSERT INTO TypedAccessLogStream18
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '18'; 


create Target KW18 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test18',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream18;

CREATE CQ AcceeslogCQ19
INSERT INTO TypedAccessLogStream19
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '19'; 


create Target KW19 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test19',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream19;

CREATE CQ AcceeslogCQ20
INSERT INTO TypedAccessLogStream20
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '20'; 


create Target KW20 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test20',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream20;


END APPLICATION KWApp;

DEPLOY APPLICATION KWApp on any in default;



















-- Kafka Reader Apps

CREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;

CREATE SOURCE KR1 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test01',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;



CREATE SOURCE KR2 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test02',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream2;


CREATE SOURCE KR3 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test03',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream3;


CREATE SOURCE KR4 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test04',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream4;


CREATE SOURCE KR5 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test05',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream5;


CREATE SOURCE KR6 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test06',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream6;


CREATE SOURCE KR7 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test07',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream7;


CREATE SOURCE KR8 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test08',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream8;


CREATE SOURCE KR9 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test09',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream9;


CREATE SOURCE KR10 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test10',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream10;


CREATE SOURCE KR11 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test11',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream11;



CREATE SOURCE KR12 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test12',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream12;


CREATE SOURCE KR13 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test13',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream13;


CREATE SOURCE KR14 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test14',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream14;


CREATE SOURCE KR15 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test15',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream15;


CREATE SOURCE KR16 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test16',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream16;


CREATE SOURCE KR17 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test17',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream17;


CREATE SOURCE KR18 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test18',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream18;


CREATE SOURCE KR19 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test19',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream19;


CREATE SOURCE KR20 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test20',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream20;




CREATE TARGET DumpKafkaReaderStream1 USING FileWriter(
  name:KafkaROuput1,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_1',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream1;


CREATE TARGET DumpKafkaReaderStream2 USING FileWriter(
  name:KafkaROuput2,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_2',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream2;


CREATE TARGET DumpKafkaReaderStream3 USING FileWriter(
  name:KafkaROuput3,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_3',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream3;


CREATE TARGET DumpKafkaReaderStream4 USING FileWriter(
  name:KafkaROuput4,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_4',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream4;


CREATE TARGET DumpKafkaReaderStream5 USING FileWriter(
  name:KafkaROuput5,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_5',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream5;


CREATE TARGET DumpKafkaReaderStream6 USING FileWriter(
  name:KafkaROuput6,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_6',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream6;


CREATE TARGET DumpKafkaReaderStream7 USING FileWriter(
  name:KafkaROuput7,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_7',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream7;


CREATE TARGET DumpKafkaReaderStream8 USING FileWriter(
  name:KafkaROuput8,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_8',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream8;


CREATE TARGET DumpKafkaReaderStream9 USING FileWriter(
  name:KafkaROuput9,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_9',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream9;


CREATE TARGET DumpKafkaReaderStream10 USING FileWriter(
  name:KafkaROuput10,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_10',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream10;


CREATE TARGET DumpKafkaReaderStream11 USING FileWriter(
  name:KafkaROuput11,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_11',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream11;


CREATE TARGET DumpKafkaReaderStream12 USING FileWriter(
  name:KafkaROuput12,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_12',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream12;


CREATE TARGET DumpKafkaReaderStream13 USING FileWriter(
  name:KafkaROuput13,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_13',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream13;


CREATE TARGET DumpKafkaReaderStream14 USING FileWriter(
  name:KafkaROuput14,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_14',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream14;


CREATE TARGET DumpKafkaReaderStream15 USING FileWriter(
  name:KafkaROuput15,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_15',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream15;


CREATE TARGET DumpKafkaReaderStream16 USING FileWriter(
  name:KafkaROuput16,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_16',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream16;


CREATE TARGET DumpKafkaReaderStream17 USING FileWriter(
  name:KafkaROuput17,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_17',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream17;


CREATE TARGET DumpKafkaReaderStream18 USING FileWriter(
  name:KafkaROuput18,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_18',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream18;


CREATE TARGET DumpKafkaReaderStream19 USING FileWriter(
  name:KafkaROuput19,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_19',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream19;


CREATE TARGET DumpKafkaReaderStream20 USING FileWriter(
  name:KafkaROuput20,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_20',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream20;

END APPLICATION KRApp;
DEPLOY APPLICATION KRApp on any in default;

CREATE OR REPLACE APPLICATION @appname@ @recovery@ AUTORESUME MAXRETRIES 2 RETRYINTERVAL 60;

CREATE OR REPLACE SOURCE @appname@src USING Global.FileReader (
  adapterName: 'FileReader',
  rolloverstyle: 'Default',
  blocksize: 64,
  networkfilesystem: true,
  wildcard: @wildcard@,
  compressiontype: 'gzip',
  includesubdirectories: false,
  directory: @inp_directory@,
  skipbom: false,
  positionbyeof: false )
PARSE USING Global.DSVParser (
  trimwhitespace: false,
  linenumber: '-1',
  columndelimittill: '-1',
  trimquote: true,
  ignoreemptycolumn: false,
  separator: ':',
  parserName: 'DSVParser',
  quoteset: '\"',
  handler: 'com.webaction.proc.DSVParser_1_0',
  charset: 'UTF-8',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  columndelimiter: '|',
  blockascompleterecord: false,
  rowdelimiter: '\n',
  nocolumndelimiter: false,
  headerlineno: 0,
  header: true )                  
OUTPUT TO @appname@STREAM;
                                                                                                         
CREATE OR REPLACE CQ @appname@CQ
INSERT INTO @appname@CQ_Out
SELECT replaceStringRegex(f,'^$','NULL')
FROM @appname@STREAM f;

CREATE OR REPLACE CQ @appname@CQ1
INSERT INTO @appname@CQ_Out1
SELECT
CASE when TO_STRING(data[0])!='NULL' THEN TO_STRING(data[0]) ELSE data[0] END AS CLIENT_ID,
CASE when TO_STRING(data[1])!='NULL' THEN TO_STRING(data[1]) ELSE data[1] END AS ACCOUNT_NAME,
CASE when TO_STRING(data[2])!='NULL' THEN TO_DATE(data[2], 'MM/dd/yyyy') ELSE data[2] END AS EFFECTIVE_DATE,
CASE when TO_STRING(data[3])!='NULL' THEN TO_DATE(data[3], 'MM/dd/yyyy') ELSE data[3] END AS EXPIRATION_DATE,
CASE when TO_STRING(data[4])!='NULL' THEN TO_STRING(data[4]) ELSE data[4] END AS XREF,
CASE when TO_STRING(data[5])!='NULL' THEN TO_STRING(data[5]) ELSE data[5] END AS XREF_TYPE,
CASE when TO_STRING(data[6])!='NULL' THEN TO_STRING(data[6]) ELSE data[6] END AS XREF_DESCRIPTION,
CASE when TO_STRING(data[7])!='NULL' THEN TO_LONG(data[7]) ELSE data[7] END AS AUD_REC_ID,
CASE when TO_STRING(data[8])!='NULL' THEN TO_LONG(data[8]) ELSE data[8] END AS X_CLIENT_ID,
CASE when TO_STRING(data[9])!='NULL' THEN TO_LONG(data[9]) ELSE data[9] END AS X_XREF_ID,
CASE when TO_STRING(data[10])!='NULL' THEN TO_DATE(data[10], 'MM/dd/yyyy HH:mm:ss') ELSE data[10] END AS AUDIT_DATE,
DNOW() AS UDP_DB2_INSERT_TIMESTAMP,
DNOW() AS UDP_DB2_UPDATE_TIMESTAMP
FROM @appname@CQ_Out f;

CREATE OR REPLACE TARGET @appname@tgt USING Global.FileWriter (
  rolloverpolicy: 'EventCount:150,Interval:120s',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  flushpolicy: 'EventCount:150',
  filename: @tar_filename@,
  directory: @tar_directory@ )
FORMAT USING Global.JSONFormatter  (
   members:'CLIENT_ID,ACCOUNT_NAME,EXPIRATION_DATE,XREF,XREF_TYPE,XREF_DESCRIPTION,AUD_REC_ID,X_CLIENT_ID,X_XREF_ID'

)
INPUT FROM @appname@CQ_Out1;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE FLOW @APP_NAME@_SrcFlow;

CREATE OR REPLACE SOURCE @APP_NAME@_src USING FileReader (
directory:'',
WildCard:''
)
parse using DSVParser (
header:'no'
)
OUTPUT TO @APP_NAME@_Stream;
END FLOW @APP_NAME@_SrcFlow;

CREATE FLOW @APP_NAME@_TgtFlow;

CREATE OR REPLACE TYPE @APP_NAME@_Type  ( BUSINESS_NAME java.lang.String KEY,
MERCHANT_ID java.lang.String,
PRIMARY_ACCOUNT_NUMBER java.lang.String
 ) ;

CREATE OR REPLACE STREAM @APP_NAME@_Stream2 OF @APP_NAME@_Type;
CREATE OR REPLACE CQ @APP_NAME@_CQ
INSERT INTO @APP_NAME@_Stream2
SELECT data[0],data[1],data[2]
FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.FabricDataWarehouseWriter (
  Tables: '',
  ConnectionURL: '@CONN_URL@',
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  uploadpolicy: 'eventcount:1',
  AccountName: '@ACCOUNTNAME@')
INPUT FROM @APP_NAME@_Stream2;

END FLOW @APP_NAME@_TgtFlow;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@ WITH @APP_NAME@_SrcFlow IN agents,@APP_NAME@_TgtFlow IN default;
START APPLICATION @APP_NAME@;

STOP APPLICATION @appName@;
UNDEPLOY APPLICATION @appName@;
DROP APPLICATION @appName@ CASCADE;

CREATE APPLICATION @appName@;

CREATE OR REPLACE TYPE @EventType@ (
 CC_Number java.lang.String,
 Amount java.lang.String,
 TXN_Type java.lang.String,
 TXN_Timestamp java.lang.String);

CREATE SOURCE @sourceComp@ USING Global.FileReader (
  rolloverstyle: 'Default',
  blocksize: 64,
  wildcard: '@sourceFileName@',
  skipbom: true,
  directory: '@sourceFileDir@',
  includesubdirectories: false,
  positionbyeof: false )
PARSE USING Global.DSVParser (
  trimwhitespace: true,
  commentcharacter: '',
  linenumber: '-1',
  columndelimiter: ',',
  trimquote: true,
  columndelimittill: '-1',
  eventtype: '@EventType@',
  ignoreemptycolumn: false,
  separator: ':',
  quoteset: '\"',
  charset: 'UTF-8',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  blockascompleterecord: false,
  rowdelimiter: '\n',
  nocolumndelimiter: false,
  headerlineno: 0,
  header: false )
OUTPUT TO @sourceOutStream@;

CREATE OR REPLACE STREAM @sourceOutStream@ OF @EventType@;

CREATE CQ @match_patternCQ@
INSERT INTO @patternOutStream@
SELECT
LIST(A,B) as events,
COUNT(B) as count
FROM @sourceOutStream@ t
MATCH_PATTERN T A+ (W|B)
DEFINE
	A = t(TXN_Type = 'AUTH/HOLD'),
	B = t(TXN_Type = 'CHARGE'),
	T = TIMER(interval 3 minute),
	W = WAIT(T)
PARTITION BY t.CC_Number;;

CREATE CQ @cqForTarget1@
INSERT INTO @target1InStream@
SELECT events FROM @patternOutStream@ a
WHERE a.count > 0;;

CREATE CQ @cqForTarget2@
INSERT INTO @target2InStream@
SELECT events FROM @patternOutStream@ a
WHERE a.count = 0;;

CREATE TARGET @target1@ USING Global.FileWriter (
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  rolloverpolicy: 'EventCount:250000',
  directory: '@targetFilesDir@',
  filename: '@targetFile1@' )
FORMAT USING Global.JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n' )
INPUT FROM @target1InStream@;

CREATE TARGET @target2@ USING Global.FileWriter (
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  directory: '@targetFilesDir@',
  filename: '@targetFile2@',
  rolloverpolicy: 'EventCount:250000' )
FORMAT USING Global.JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n' )
INPUT FROM @target2InStream@;

END APPLICATION @appName@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.JsonNodeEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (
  ProviderName: '',
  Provider: '',
  Ctx: '',
  QueueName: '',
  Topic:'',
  UserName: '',
  Password: '',
  EnableTransaction: '',
  transactionpolicy: ''
 )
PARSE USING JSONParser ()
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING JSONFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream@RANDOM@;

END APPLICATION @APPNAME@;

stop PatternMatching.CSV;
undeploy application PatternMatching.CSV;
drop application PatternMatching.CSV cascade;

create application CSV RECOVERY 5 SECOND INTERVAL;

create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'ctest.csv',
  columndelimiter:',',
  positionByEOF:false
)
OUTPUT TO CsvStream;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  TO_INT(data[0]) as UserId,
	TO_INT(data[1]) as temp1,
        TO_DOUBLE(data[2]) as temp2,
	TO_STRING(data[3]) as temp3
FROM CsvStream;

-- scenario 1.1 check pattern alterations
CREATE CQ TypeConversionCsvCQ1
INSERT INTO TypedStream1
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A | B  | C
define A = UserDataStream(temp1 = 20), B= UserDataStream(temp2 = 30.40), C= UserDataStream(temp3 = 'Bret')
PARTITION BY UserId;

-- scenario 1.2 check pattern permutation with partition by
CREATE CQ TypeConversionCsvCQ2
INSERT INTO TypedStream2
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A & B & C
define A = UserDataStream(temp1 = 10), B= UserDataStream(temp2 = 20.30), C= UserDataStream(temp3 = 'zalak')
PARTITION BY UserId;

-- scenario 1.3 check pattern quantifire with partition by
CREATE CQ TypeConversionCsvCQ3
INSERT INTO TypedStream3
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A ? B ? C
define A = UserDataStream(temp1 = 10), B= UserDataStream(temp2 = 20.30), C= UserDataStream(temp3 = 'zalak')
PARTITION BY UserId;

-- scenario 1.4 check pattern quantifire with grouping and partition by
CREATE CQ TypeConversionCsvCQ4
INSERT INTO TypedStream4
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A ( B ? C)
define A = UserDataStream(temp1 = 10), B= UserDataStream(temp2 = 20.30), C= UserDataStream(temp3 = 'zalak')
PARTITION BY UserId;

-- scenario 1.5 check pattern overlapping and alteration with grouping and partition by
CREATE CQ TypeConversionCsvCQ5
INSERT INTO TypedStream5
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN (A # B) | C
define A = UserDataStream(temp1 = 30), B= UserDataStream(temp2 = 20.30), C= UserDataStream(temp3 = 'Bret')
PARTITION BY UserId;

-- scenario 1.6 check pattern quantifire with grouping and two partition by
CREATE CQ TypeConversionCsvCQ6
INSERT INTO TypedStream6
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A ? B ? C
define A = UserDataStream(temp1 = 10), B= UserDataStream(temp2 = 30.40), C= UserDataStream(temp3 = 'Bret')
PARTITION BY temp1,temp2;

-- scenario 1.7 check pattern quantifire(0 or 1),<=,>= with partition by
CREATE CQ TypeConversionCsvCQ7
INSERT INTO TypedStream7
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A * B * C
define A = UserDataStream(temp1 <= 10), B= UserDataStream(temp2 >= 30.40), C= UserDataStream(temp3 = 'zalak')
PARTITION BY UserId,temp3;

-- scenario 1.8 check pattern alteration and permutation using between values with partition by
CREATE CQ TypeConversionCsvCQ8
INSERT INTO TypedStream8
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A | B & C
define A = UserDataStream(temp1 between 10 and 40), B= UserDataStream(temp2 between 10.40 and 30.50), C= UserDataStream(temp3 != 'zalak')
PARTITION BY temp3;

-- scenario 1.9 check pattern or using <,>,!= values with partition by
CREATE CQ TypeConversionCsvCQ9
INSERT INTO TypedStream9
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A  B  C
define A = UserDataStream(temp1 > 20), B= UserDataStream(temp2 < 60), C= UserDataStream(temp3 != 'prajkta')
PARTITION BY UserId;

-- scenario 1.10 check pattern {m,n} using != values with partition by
CREATE CQ TypeConversionCsvCQ10
INSERT INTO TypedStream10
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN A{0,2} | B{1,3} & C
define A = UserDataStream(temp1 != 20), B= UserDataStream(temp2 != 40.10), C= UserDataStream(temp3 != 'bert')
PARTITION BY temp1;


CREATE WACTIONSTORE UserActivityInfo1
CONTEXT OF TypedStream1_Type
EVENT TYPES ( TypedStream1_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo2
CONTEXT OF TypedStream2_Type
EVENT TYPES ( TypedStream2_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo3
CONTEXT OF TypedStream3_Type
EVENT TYPES ( TypedStream3_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo4
CONTEXT OF TypedStream4_Type
EVENT TYPES ( TypedStream4_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo5
CONTEXT OF TypedStream5_Type
EVENT TYPES ( TypedStream5_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo6
CONTEXT OF TypedStream6_Type
EVENT TYPES ( TypedStream6_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo7
CONTEXT OF TypedStream7_Type
EVENT TYPES ( TypedStream7_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo8
CONTEXT OF TypedStream8_Type
EVENT TYPES ( TypedStream8_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo9
CONTEXT OF TypedStream9_Type
EVENT TYPES ( TypedStream9_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfo10
CONTEXT OF TypedStream10_Type
EVENT TYPES ( TypedStream10_Type )
@PERSIST-TYPE@

create Target t2 using SysOut(name:AgentTyped) input from TypedStream1;
create Target t3 using SysOut(name:AgentTyped1) input from TypedStream2;

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction1
INSERT INTO UserActivityInfo1
SELECT * FROM TypedStream1
LINK SOURCE EVENT;

CREATE CQ UserWaction2
INSERT INTO UserActivityInfo2
SELECT * FROM TypedStream2
LINK SOURCE EVENT;

CREATE CQ UserWaction3
INSERT INTO UserActivityInfo3
SELECT * FROM TypedStream3
LINK SOURCE EVENT;

CREATE CQ UserWaction4
INSERT INTO UserActivityInfo4
SELECT * FROM TypedStream4
LINK SOURCE EVENT;

CREATE CQ UserWaction5
INSERT INTO UserActivityInfo5
SELECT * FROM TypedStream5
LINK SOURCE EVENT;

CREATE CQ UserWaction6
INSERT INTO UserActivityInfo6
SELECT * FROM TypedStream6
LINK SOURCE EVENT;

CREATE CQ UserWaction7
INSERT INTO UserActivityInfo7
SELECT * FROM TypedStream7
LINK SOURCE EVENT;

CREATE CQ UserWaction8
INSERT INTO UserActivityInfo8
SELECT * FROM TypedStream8
LINK SOURCE EVENT;

CREATE CQ UserWaction9
INSERT INTO UserActivityInfo9
SELECT * FROM TypedStream9
LINK SOURCE EVENT;

CREATE CQ UserWaction10
INSERT INTO UserActivityInfo10
SELECT * FROM TypedStream10
LINK SOURCE EVENT;

end application CSV;
deploy application csv;
start csv;

STOP APPLICATION eh;
UNDEPLOY APPLICATION eh;
DROP APPLICATION eh CASCADE;
CREATE APPLICATION eh @Recovery@;

Create Source s1_orcl_w Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: 'localhost:1521:orcl',
 Tables:'QATEST.TEST_%',
 FetchSize:1
) 
Output To sourcestream;

Create Source s2_orcl Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: 'localhost:1521:orcl',
 Tables:'QATEST.TEST_01',
 FetchSize:1
) 
Output To sourcestream;

Create Source s3_orcl Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: 'localhost:1521:orcl',
 Tables:'QATEST1.TEST_01',
 FetchSize:1
) 
Output To sourcestream;


CREATE CQ OperationType
INSERT INTO OpsStream
SELECT  
CASE WHEN (META(c,"OperationName").toString() == "DELETE")
THEN putUserData(c, 'isDelete', 'true') 
ELSE
putUserData(c,'isDelete', 'false')
END
FROM sourcestream c;



CREATE OR REPLACE SOURCE s4_orcl_ibr USING IncrementalBatchReader  ( 
  FetchSize: 1,
  StartPosition: '%=0',
  Username: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@//dockerhost:1521/orcl',
  Tables: 'QATEST.TEST_%',
  CheckColumn: '%=id',
  Password: 'qatest' ) 
OUTPUT TO sourcestream ;

CREATE SOURCE s5_fr USING FileReader (
	directory:'/Users/saranyad/Product/IntegrationTests/TestData/',
    WildCard:'banks.csv',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:',',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;


CREATE TYPE cdctype(
  id int,
  name String  
);

CREATE STREAM cdctypestream OF cdctype;

CREATE CQ cdcstreamcq
INSERT INTO cdctypestream
SELECT TO_INT(p.data[0]), 
       TO_STRING(p.data[1])
FROM FileStream p;


create cq cqtowaevent 
insert into sourcestream
select convertTypedeventToWAevent(c, 'admin.cdctype')
from cdctypestream c;


create Target t1_dsv using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_01',
	E1P:'true',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
	PartitionKey:'@metadata(TableName)',
	ConsumerGroup:'reader',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
	--ParallelThreads:'2'
)
format using DSVFormatter ( 
)
input from sourcestream;





create Target t2_json using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_02',
	E1P:'true',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
	PartitionKey:'@userdata(isDelete)',
	ConsumerGroup:'reader',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:30s'
	--ParallelThreads:'2'
)
format using JSONFormatter ( 
)
input from OpsStream;


create Target t3_avro using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_03',
	E1P:'true',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
	PartitionKey:'@metadata(TableName)',
	ConsumerGroup:'reader',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
	--ParallelThreads:'2'
)
format using AvroFormatter (schemaFileName:'kafkaAvroTest_multipleReader.avsc') 
input from sourcestream;


END APPLICATION eh;

DEPLOY APPLICATION eh on any in default;

start application eh;

stop application GCSWriterTest;
undeploy application GCSWriterTest;
drop application GCSWriterTest cascade;
create application GCSWriterTest recovery 1 second interval;
Create Source OracleSource Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To OracleStream;

create Target OracleGCSTarget using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@',
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from OracleStream;

end application GCSWriterTest;
deploy application GCSWriterTest on all in default;
start application GCSWriterTest;

STOP AdhocTester.ws_one;
UNDEPLOY APPLICATION AdhocTester.ws_one;
DROP APPLICATION AdhocTester.ws_one cascade;

CREATE APPLICATION ws_one;

CREATE SOURCE wsSource USING CSVReader  ( 
  blocksize: 10240,
  charset: 'UTF-8',
  positionByEOF: false,
  columndelimiter: ',',
  directory: '@TEST-DATA-PATH@',
  eofdelay: 100,
  wildcard: 'sampleByData.csv',
  expected_column_count: 0,
  rowdelimiter: '\n',
  header: true,
  adapterName: 'CSVReader',
  quoteset: '\"',
  trimquote: true,
  skipbom: true
 ) 
OUTPUT TO QaStream ;

CREATE TYPE sampleS_Type  ( vID java.lang.Integer KEY, 
vInt java.lang.Integer , 
vDouble java.lang.Double , 
vLong java.lang.Long , 
vShort java.lang.Short , 
vFloat java.lang.Float  
 ) ;

CREATE STREAM sampleS OF sampleS_Type;

CREATE CQ csvTowsData 
INSERT INTO sampleS
SELECT  to_int(data[0]) as vID,
to_int(data[1]) as vInt,
to_double(data[1]) as vDouble,
to_long(data[1]) as vLong,
to_short(data[1]) as vShort,
to_float(data[1]) as vFloat
FROM QaStream
;

CREATE WACTIONSTORE oneWS  CONTEXT OF sampleS_Type
EVENT TYPES(sampleS_Type )
@PERSIST-TYPE@

CREATE CQ wsToWaction 
INSERT INTO oneWS
SELECT * FROM sampleS s
LINK SOURCE EVENT;

END APPLICATION ws_one;
DEPLOY APPLICATION ws_one on any in default;
START ws_one;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

--create application @APPNAME@ Recovery 5 Second Interval;
create application @APPNAME@;

create or replace type @APPNAME@emp_type(
Sno integer,
Empname string,
Doj string,
Country string,
CompanyName string
);

CREATE OR REPLACE SOURCE @APPNAME@File_Source1 using Filereader(
	directory:'@DIRECTORY@',
  wildcard:'File_empdata.csv',
  positionByEOF:false
)parse using dsvParser(
    header:'yes'
)
OUTPUT TO @APPNAME@FileSource_Stream1,
OUTPUT TO @APPNAME@FileSource_Stream1_automap MAP(filename:'File_empdata.csv');

CREATE OR REPLACE SOURCE @APPNAME@Init_Source1 USING DatabaseReader  (
  Username: 'qatest',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.EMP_INIT',
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: 'qatest'
 )
OUTPUT TO 	@APPNAME@InitialLoad_Stream1;

CREATE OR REPLACE SOURCE @APPNAME@CDC_Source1 USING OracleReader  (
  StartTimestamp: 'null',
  SupportPDB: false,
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  FetchSize: 1,
  DDLCaptureMode: 'All',
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.EMP',
  adapterName: 'OracleReader',
  Password: 'qatest',
  TransactionBufferType: 'Memory',
  DictionaryMode: 'OnlineCatalog',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: null,
  ReaderType: 'LogMiner',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '1MB',
  compression: true,
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO @APPNAME@CDC_Stream1 ;

create or replace stream @APPNAME@FileSource_cdc_init_TypedStream of @APPNAME@emp_type;
create or replace cq @APPNAME@file_typed_streamcq
insert into @APPNAME@FileSource_cdc_init_TypedStream
SELECT to_int(data[0]),
data[1],
data[2],
data[3],
data[4]
from @APPNAME@FileSource_Stream1;

create or replace cq @APPNAME@cdc_typed_streamcq
insert into @APPNAME@FileSource_cdc_init_TypedStream
SELECT to_int(data[0]),
data[1],
data[2],
data[3],
data[4]
from @APPNAME@CDC_Stream1;

create or replace cq @APPNAME@init_typed_streamcq
insert into @APPNAME@FileSource_cdc_init_TypedStream
SELECT to_int(data[0]),
data[1],
data[2],
data[3],
data[4]
from @APPNAME@InitialLoad_Stream1;

CREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target1 USING CassandraCosmosDBWriter  (
  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --CheckPointTable: '',
  FlushPolicy: 'EventCount:1000,Interval:60',
  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',
  AccountKey:'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  --ConsistencyLevel:'fh',
  --port:'10505',
  Tables: 'test.file_emp',
  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',
  adapterName: 'CassandraCosmosDBWriter'
 )
INPUT FROM @APPNAME@FileSource_Stream1_automap;

CREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target2 USING CassandraCosmosDBWriter  (
  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --CheckPointTable: '',
  FlushPolicy: 'EventCount:1000,Interval:60',
  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',
  AccountKey: 'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  --ConsistencyLevel:'fh',
  --port:'10505',
  Tables: 'QATEST.EMP,test.cdc_emp',
  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',
  adapterName: 'CassandraCosmosDBWriter'
 )
INPUT FROM @APPNAME@CDC_Stream1;

CREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target3 USING CassandraCosmosDBWriter  (
  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --CheckPointTable: '',
  FlushPolicy: 'EventCount:1000,Interval:60',
  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',
  AccountKey: 'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  --ConsistencyLevel:'fh',
  --port:'10505',
  Tables: 'QATEST.EMP_INIT,test.initialload_emp',
  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',
  adapterName: 'CassandraCosmosDBWriter'
 )
INPUT FROM @APPNAME@InitialLoad_Stream1;


CREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target4 USING CassandraCosmosDBWriter  (
  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --CheckPointTable: '',
  FlushPolicy: 'EventCount:1000,Interval:60',
  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',
  AccountKey: 'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  --ConsistencyLevel:'fh',
  --port:'10505',
  Tables: 'test.file_cdc_emp',
  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',
  adapterName: 'CassandraCosmosDBWriter'
 )
INPUT FROM @APPNAME@FileSource_cdc_init_TypedStream;

create or replace target @APPNAME@sys_file_tgt using sysout(
name:'foo_file'
)input from @APPNAME@FileSource_Stream1;

create or replace target @APPNAME@sys_cdc_tgt using sysout(
name:'foo_cdc'
)input from @APPNAME@CDC_Stream1;

create or replace target @APPNAME@sys_init_tgt using sysout(
name:'foo_init'
)input from @APPNAME@InitialLoad_Stream1;

End Application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

STOP APPLICATION TQLwithinTqlTester.TQLwithinTqlApp;
UNDEPLOY APPLICATION TQLwithinTqlTester.TQLwithinTqlApp;
DROP APPLICATION TQLwithinTqlTester.TQLwithinTqlApp CASCADE;

CREATE APPLICATION TQLwithinTqlApp;

@@FEATURE-DIR@/tql/TQLwithinTQL5.tql;

END APPLICATION TQLwithinTqlApp;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @APPNAME@_src Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
   _h_ReturnDateTimeAs:'ZonedDateTime'
) Output To @APPNAME@_stream;

create Target @APPNAME@_tgt using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_stream;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING Global.DeltaLakeWriter (
  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.testaswin',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING Global.DeltaLakeWriter (
  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.testaswin',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING Global.DeltaLakeWriter (
  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.testaswin',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING Global.DeltaLakeWriter (
  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.testaswin',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @STREAM@;


CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;;

create Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;

create application KinesisTest;
create source CSVSource using FileReader (
	directory:'/home/dz/src/product/Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  companyName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0], data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM'
)
format using DSVFormatter (
)
input from TypedCSVStream;
end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

--
-- Recovery Test 36 with two sources, two jumping attribute windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W/p -> CQ1 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS
--

STOP Recov36Tester.RecovTest36;
UNDEPLOY APPLICATION Recov36Tester.RecovTest36;
DROP APPLICATION Recov36Tester.RecovTest36 CASCADE;
CREATE APPLICATION RecovTest36 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION RecovTest36;

Stop @APPNAME@_App;
undeploy application @APPNAME@_App;
drop application @APPNAME@_App cascade;

CREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;
CREATE SOURCE @APPNAME@_Src USING Global.Ojet(
  Username:'@SOURCE_USER@',
  Password:'@SOURCE_PASSWORD@',
  ConnectionURL:'@SOURCE_URL@',
  Tables:'@SOURCE_TABLES@',
  ConnectionRetryPolicy:'@AUTO_CONNECTION_RETRY@',
  OJetConfig: '{ "OJET" : [ "retriable_errors:ORA-26804" ] }'
)
OUTPUT TO @APPNAME@_Stream;

CREATE TARGET @APPNAME@_SYS USING SysOut (
  name: '@APPNAME@_SYS' )
INPUT FROM @APPNAME@_Stream;

End APPLICATION @APPNAME@_App;

CREATE TARGET @TARGET_NAME@ USING SnowflakeWriter (
  ConnectionURL: '@SNOWFLAKE-URL@',
  username: '@SNOWFLAKE-USERNAME@',
  password: '@SNOWFLAKE-PASSWORD@',
  appendOnly:'false',
  Tables: '@TARGET-TABLES@'
 )
 INPUT FROM @STREAM@;

use PosTester;
alter application PosApp;

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
@PERSIST-TYPE@

end application PosApp;

alter application PosApp recompile;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ recovery 5 second Interval;

CREATE source @srcName@ USING Global.OracleReader ( 
 Username:'@srcusername@',
  Password:'@srcpassword@',
  ConnectionURL:'@srcurl@',
  Tables:'@srcschema@.@srctable@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries:'true'
  ) 
OUTPUT TO @outstreamname@;


CREATE OR REPLACE TARGET @tgtName@ USING Global.SalesforceWriter ( 
  autoAuthTokenRenewal: 'true', 
  sObjects: '@srcschema@.@srctable@,@tgtobject@ COLUMNMAP(num__c=a,Name=b)', 
  useConnectionProfile: 'false', 
  consumerSecret: '@tgtconsumersecret@', 
  JWTKeystorePath: '', 
  BatchPolicy:'EventCount:1,Interval:10s',
  CommitPolicy:'EventCount:1,Interval:10s', 
  Mode: 'APPENDONLY', 
  consumerKey: '@tgtconsumerkey@', 
  apiEndPoint: '@tgtapiurl@', 
  InMemory: 'true', 
  FieldDelimeter: 'COMMA', 
  ApplicationErrorCountThreshold: '0', 
  useBulkApi: 'true', 
  OAuthAuthorizationFlows: 'PASSWORD', 
  JWTCertificateName: '', 
  hardDelete: 'false', 
  Username: '@tgtusername@', 
  useQuotes: 'false', 
  adapterName: 'SalesforceWriter', 
  Password: '@tgtpassword@', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  securityToken: '@tgtsecuritytoken@', 
  JWTKeystorePassword: '' ) 
INPUT FROM @instreamname@;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop application SalesForceReaderTest;
undeploy application SalesForceReaderTest;
drop application SalesForceReaderTest cascade;

CREATE APPLICATION SalesForceReaderTest recovery 5 second interval;

CREATE OR REPLACE SOURCE SFPoller USING SalesForceReader 
(
  sObjects:'newobj__c',
  --sObject:'Campaign',
  pollingInterval:'1 min',
  autoAuthTokenRenewal:'true',
  Username:'siddhika@webaction.com',
  Password:'webaction@1234',
  securityToken:'qhNbKKmafFpanz8Y2oiM89UhR',
  consumerKey:'3MVG9ZL0ppGP5UrBayz85eLnPg69gWLaE8pA3uzwcFCZ9s.J0mgE7AKvPCEhTaop4uYRbBaDnGXHjnLmngG6P',
  consumerSecret:'2500119200751301808',
  apiEndPoint:'https://ap2.salesforce.com',
  mode:'InitialLoad',
  startTimestamp:null
)
OUTPUT TO DataStream;

create target tout using sysout(name : 'out')input from DataStream;

/*
CREATE TARGET dbtar USING DatabaseWriter( 
	BatchPolicy:'EventCount:100,Interval:10',
	CommitPolicy:'EventCount:100,Interval:10',
	ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
	Username:'qatest',
	Password:'qatest',
  Tables:'Position__c,QATEST.POS columnmap(Id=Id,numnum=numnum__c)'
  -- Tables: 'newobj__c,QATEST.SFTABLE1 columnmap(Id=Id,CHECKBOOL=checkbool__c,CURR=curr__c,DT=dt__c,TIME1=time1__c,DtTime=DtTime__c,EMAILID=emailid__c,NUM=num__c,PERCNT=percnt__c,PHN=phn__c,TXTLONG=txtlong__c,URL1=url1__c,TXT=txt__c)'
   ---,loc__latitude=loc__latitude__s,loc__longitude=loc__longitude__s)' 
) INPUT FROM DataStream;

*/
CREATE OR REPLACE TARGET Target2 using FileWriter
(
  filename:'Obj123.json',
  directory:'/Users/siddhika/Product/',
  rolloverpolicy:'EventCount:1'
)
FORMAT USING JSONFormatter ()
INPUT FROM DataStream;

END APPLICATION SalesForceReaderTest;
DEPLOY APPLICATION SalesForceReaderTest on any in default;
START SalesForceReaderTest;

stop @appname@;
undeploy application @appname@;
DROP APPLICATION @appname@ CASCADE;
CREATE APPLICATION @appname@;

CREATE SOURCE @appname@_src USING databaseReader  (
  Username: '@@',
  Password: '@@',
  ConnectionURL: '@@',
  Tables: '@@',
  FetchSize: '100'
 )
OUTPUT TO @appname@_ss;

----1st set of window and cache

CREATE JUMPING WINDOW @appname@_win OVER @appname@_ss KEEP @winsize@ ROWS;

CREATE TYPE @appname@_MapType
    (   
       id INTEGER,
        name STRING,
        city  STRING
    );
    
CREATE EXTERNAL CACHE @appname@_cach (
  AdapterName: 'DatabaseReader',
    ConnectionURL: '@url@',
    UserName: '@uname@',
    Password: '@pwd@',
   Table: '@tablename@2',
  FetchSize: 100,
  Columns: 'id,name,city',
  trimquote: false,
  KeyToMap: '@key@'
 )
 OF @appname@_MapType;   
 
CREATE TYPE @appname@_MapTypenew
    (   id_t            INTEGER,
        name_t           STRING,
        city_t            STRING,
        id_c            INTEGER,
        name_c            STRING,
        city_c            STRING
    );
    
CREATE STREAM @appname@_JoinedData OF @appname@_MapTypenew;

CREATE CQ @appname@_JoinDataCQ
INSERT INTO @appname@_JoinedData
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_STRING(f.data[2]),
        z.id,
        z.name,
        z.city
FROM @appname@_win f, @appname@_cach z
where TO_INT(f.data[0]) = z.id
@Ex@;

----2nd set of window and cache

CREATE JUMPING WINDOW @appname@_win2 OVER @appname@_ss KEEP @winsize@ ROWS;
 
 CREATE EXTERNAL CACHE @appname@_cach2 (
  AdapterName: 'DatabaseReader',
    ConnectionURL: '@url@',
    UserName: '@uname@',
    Password: '@pwd@',
   Table: '@tablename@3',
  FetchSize: 100,
  Columns: 'id,name,city',
  trimquote: false,
  KeyToMap: '@key@'
 )
 OF @appname@_MapType;
 
CREATE CQ @appname@_JoinDataCQ2
INSERT INTO @appname@_JoinedData
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_STRING(f.data[2]),
        z.id,
        z.name,
        z.city
FROM @appname@_win2 f, @appname@_cach2 z
where TO_INT(f.data[0]) = z.id
@Ex@;


----3rd set of window and cache

CREATE JUMPING WINDOW @appname@_win3 OVER @appname@_ss KEEP @winsize@ ROWS;
 
 CREATE EXTERNAL CACHE @appname@_cach3 (
  AdapterName: 'DatabaseReader',
    ConnectionURL: '@url@',
    UserName: '@uname@',
    Password: '@pwd@',
   Table: '@tablename@4',
  FetchSize: 100,
  Columns: 'id,name,city',
  trimquote: false,
  KeyToMap: '@key@'
 )
 OF @appname@_MapType;
 
CREATE CQ @appname@_JoinDataCQ3
INSERT INTO @appname@_JoinedData
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_STRING(f.data[2]),
        z.id,
        z.name,
        z.city
FROM @appname@_win3 f, @appname@_cach3 z
where TO_INT(f.data[0]) = z.id
@Ex@;


CREATE TARGET @appname@_tgt USING DatabaseWriter
(
  ConnectionURL:'@@',
  Username:'@@',
  Password:'@@',
  BatchPolicy:'Eventcount:10000,Interval:1',
  CommitPolicy:'Interval:1,Eventcount:10000',
  Tables:'@@'
) 
INPUT FROM @appname@_JoinedData;

END APPLICATION @appname@;
deploy application @appname@;
start @appname@;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'MINER.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@(
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:100,Interval:120',
  CommitPolicy: 'EventCount:100,Interval:120',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;


create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;

deploy application DBRTOCW on ANY in default;

start application DBRTOCW;

CREATE OR REPLACE APPLICATION @AppFeature@;

CREATE OR REPLACE SOURCE initialLoad_Src USING Global.DatabaseReader (
  FetchSize: 100,
  QuiesceOnILCompletion: false,
  Tables: '@SrcTable@',
  adapterName: 'DatabaseReader',
  Password: '@SrcPswd@',
  Username: '@SrcUserName@',
  ConnectionURL: '@SrcUrl@'
   )
OUTPUT TO @AppName@_Stream;

CREATE OR REPLACE TARGET initialLoadPostgres_Trg USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:100,Interval:60',
  StatementCacheSize: '50',
  ConnectionURL: '@TrgUrl@',
  Username: '@TrgUserName@',
  BatchPolicy: 'EventCount:100,Interval:60',
  Tables: '@SrcTable@,@TrgTable@',
  Password: '@TrgPswd@',
  adapterName: 'DatabaseWriter' )
INPUT FROM @AppName@_Stream;

END APPLICATION @AppName@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @SOURCE@ USING MSSQLReader
 (
   Username: '@LOGMINER-UNAME@',
   Password: '@LOGMINER-PASSWORD@',
   ConnectionURL: '@LOGMINER-URL@',
   DatabaseName:'qatest',
   Tables: '@SOURCE_TABLE@',
    Compression:false,
    AutoDisableTableCDC:false,FetchTransactionMetadata:true,
    StartPosition:'EOF'
 )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@1 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'false',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@2 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'false',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'true',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@3 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'true',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

--
-- Crash Recovery Test 3 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW -> CQ(aggregate) -> WS
--

STOP APPLICATION N2S2CR3Tester.N2S2CRTest3;
UNDEPLOY APPLICATION N2S2CR3Tester.N2S2CRTest3;
DROP APPLICATION N2S2CR3Tester.N2S2CRTest3 CASCADE;
CREATE APPLICATION N2S2CRTest3 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest3;

CREATE SOURCE CsvSourceN2S2CRTest3 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN2S2CRTest3;

CREATE FLOW DataProcessingN2S2CRTest3;

CREATE TYPE WactionTypeN2S2CRTest3 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount int,
  city String
);

CREATE STREAM DataStream OF WactionTypeN2S2CRTest3;

CREATE CQ CsvToDataN2S2CRTest3
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_INT(TO_DOUBLE(data[7])),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN2S2CRTest3 CONTEXT OF WactionTypeN2S2CRTest3
EVENT TYPES ( WactionTypeN2S2CRTest3 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN2S2CRTest3
INSERT INTO WactionsN2S2CRTest3
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

END FLOW DataProcessingN2S2CRTest3;

END APPLICATION N2S2CRTest3;

--
-- Crash Recovery Test 3 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW -> CQ(aggregate) -> WS
--

STOP APPLICATION KStreamN2S2CR3Tester.KStreamN2S2CRTest3;
UNDEPLOY APPLICATION KStreamN2S2CR3Tester.KStreamN2S2CRTest3;
DROP APPLICATION KStreamN2S2CR3Tester.KStreamN2S2CRTest3 CASCADE;

DROP USER KStreamN2S2CR3Tester;
DROP NAMESPACE KStreamN2S2CR3Tester CASCADE;
CREATE USER KStreamN2S2CR3Tester IDENTIFIED BY KStreamN2S2CR3Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR3Tester;
CONNECT KStreamN2S2CR3Tester KStreamN2S2CR3Tester;

CREATE APPLICATION KStreamN2S2CRTest3 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest3;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest3 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest3;

CREATE FLOW DataProcessingKStreamN2S2CRTest3;

CREATE TYPE WactionTypeKStreamN2S2CRTest3 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount int,
  city String
);

CREATE STREAM DataStream OF WactionTypeKStreamN2S2CRTest3;

CREATE CQ CsvToDataKStreamN2S2CRTest3
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_INT(TO_DOUBLE(data[7])),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest3 CONTEXT OF WactionTypeKStreamN2S2CRTest3
EVENT TYPES ( WactionTypeKStreamN2S2CRTest3 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsKStreamN2S2CRTest3
INSERT INTO WactionsKStreamN2S2CRTest3
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

END FLOW DataProcessingKStreamN2S2CRTest3;

END APPLICATION KStreamN2S2CRTest3;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.test01',
	FetchSize: '1',
	connectionRetryPolicy:'timeOut=00,retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(id,name) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true		
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

STOP OuterJoinTester.OJApp;
UNDEPLOY APPLICATION OuterJoinTester.OJApp;
DROP APPLICATION OuterJoinTester.OJApp cascade;

CREATE APPLICATION OJApp;

CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;

CREATE TYPE bankData
(
bankID Integer KEY,
bankName String
);

CREATE TYPE cardData
(
cardID Integer KEY,
cardName String
);

CREATE STREAM wsStream OF bankData;

CREATE CACHE cache1 USING CsvReader(
directory: '@TEST-DATA-PATH@',
wildcard: 'banks.csv',
header: No,
columndelimiter: ',',
trimquote: false
) QUERY (keytomap:'bankID') OF bankData;


CREATE CACHE cache2 USING CsvReader(
directory: '@TEST-DATA-PATH@',
wildcard: 'bankCards.csv',
header: No,
columndelimiter: ',',
trimquote: false
) QUERY (keytomap:'cardID') OF cardData;


CREATE WACTIONSTORE oneWS CONTEXT OF bankData
EVENT TYPES(bankData )
@PERSIST-TYPE@

CREATE CQ csvTobankData
INSERT INTO oneWS
SELECT TO_INT(data[0]), data[1] FROM QaStream;



END APPLICATION OJApp;

CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( 
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: '@CDC_URL@',
  Tables: '@Source1Tables@' ) 
OUTPUT TO @APPNAME@cdcStream;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@1 ( 
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',
  FetchSize: 20,
  DatabaseProviderType: 'Default',
  Table: '@Source3Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')  
OF @APPNAME@cachetype;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@2 ( 
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',
  FetchSize: 10,
  DatabaseProviderType: 'Default',
  Table: '@Source2Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')  
OF @APPNAME@cachetype;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ recovery 5 second interval;

CREATE OR REPLACE SOURCE @SOURCENAME@ USING IncrementalBatchReader  (
  FetchSize: 1000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: @startPosition@,
  PollingInterval: '20sec'
  )
  OUTPUT TO @STREAM@;

  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;

  CREATE TARGET @targetName@ USING DatabaseWriter(
    ConnectionURL:'@READER-URL@',
    Username:'@READER-UNAME@',
    Password:'@READER-PASSWORD@',
    BatchPolicy:'Eventcount:1000,Interval:1000',
    CommitPolicy:'Eventcount:1000,Interval:1000',
    Checkpointtable:'CHKPOINT',
    Tables:'@WATABLES@,@WATABLES@_target'
  ) INPUT FROM @STREAM@;

  END APPLICATION @APPNAME@;

  DEPLOY APPLICATION @APPNAME@;
  start application @APPNAME@;

STOP APPLICATION orrs;
	UNDEPLOY APPLICATION orrs;
	DROP APPLICATION orrs CASCADE;
	CREATE APPLICATION orrs;
	Create Source OraSource Using OracleReader 
	(
	 Username:'user-name',	
	 Password:'password',
	 ConnectionURL: 'src_url',
	 Tables:'src_table',
	 FilterTransactionBoundaries:true,
	 FetchSize:'fetch-size'
	) Output To LCRStream;
	
	CREATE TARGET RSTarget USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: '@ACCESS-KEY-ID@',
	  --secretaccesskey: '@SECRET-ACCESS-KEY@',
	  S3IAMRole:'@IAMROLE@',
	  Tables: 'tgt_table',
	  uploadpolicy:'eventcount:300,interval:1m'
	) INPUT FROM LCRStream;
	
	END APPLICATION orrs;
	deploy application orrs;
	START application orrs;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src1 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream1;

CREATE OR REPLACE CQ @APP_NAME@_CQ1
INSERT INTO @APP_NAME@_Stream6
SELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream1 d;

CREATE OR REPLACE SOURCE @APP_NAME@_src2 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream2;

CREATE OR REPLACE CQ @APP_NAME@_CQ2
INSERT INTO @APP_NAME@_Stream6
SELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream2 d;

CREATE OR REPLACE SOURCE @APP_NAME@_src3 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream3;

CREATE OR REPLACE CQ @APP_NAME@_CQ3
INSERT INTO @APP_NAME@_Stream6
SELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream3 d;

CREATE OR REPLACE SOURCE @APP_NAME@_src4 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream4;

CREATE OR REPLACE CQ @APP_NAME@_CQ4
INSERT INTO @APP_NAME@_Stream6
SELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream4 d;

CREATE OR REPLACE SOURCE @APP_NAME@_src5 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream5;

CREATE OR REPLACE CQ @APP_NAME@_CQ5
INSERT INTO @APP_NAME@_Stream6
SELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream5 d;


CREATE OR REPLACE TARGET @APP_NAME@_tgt1 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream6;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

create application ConsoleApplication;
create type someType(zip Int);
drop application ConsoleApplication;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING PostgreSQLReader  (
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 )
OUTPUT TO @STREAM@ ;

STOP AdhocTester.ws_one;
UNDEPLOY APPLICATION AdhocTester.ws_one;
DROP APPLICATION AdhocTester.ws_one cascade;

CREATE APPLICATION ws_one;

CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO QaStream;

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE ZipLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'AdhocQueryData.csv',
  header: Yes,
  columndelimiter: '	',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

Create TYPE wsData(
	CompanyNum String,
	CompanyName String KEY,
	CompanyCode int,
	Zip String
);


CREATE STREAM wsStream OF wsData;

--Select data from QaStream and insert into wsStream

CREATE CQ csvTowsData
INSERT INTO wsStream
SELECT  data[0],
    data[1],
    TO_INT(data[3]),
    data[9]
 FROM QaStream;




CREATE WACTIONSTORE oneWS CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@


CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT * FROM wsStream
LINK SOURCE EVENT;

END APPLICATION ws_one;

stop application @APPNAME@app3;
undeploy application @APPNAME@app3;
alter application @APPNAME@app3;
CREATE or replace TARGET @APPNAME@app3_targetNew USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS3_Alter'
) INPUT FROM @APPNAME@sourcestream;
alter application @APPNAME@app3 recompile;
deploy application @APPNAME@app3;

STOP APPLICATION @WRITERAPPNAME@;
UNDEPLOY APPLICATION @WRITERAPPNAME@;
DROP APPLICATION @WRITERAPPNAME@ CASCADE;
create or replace PROPERTYVARIABLE SRC_PASSWORD='@SOURCE_PASS@';
CREATE APPLICATION @WRITERAPPNAME@ RECOVERY 10 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE SOURCE @SOURCE@ USING OracleReader
(
FetchSize:1,
Username:'@SOURCE_USER@',
Password:'$SRC_PASSWORD',
ConnectionURL:'@CONNECTION_URL@',
Tables:'@SOURCE_TABLE@',
password_encrypted: 'true'
)
OUTPUT TO @STREAM1@;

CREATE OR REPLACE TYPE @TYPE@( 
datae java.util.HashMap , 
TABLE_NAME java.lang.String , 
OPS_NAME java.lang.String , 
DB_TIMESTAMP java.lang.String  ,
COMMITSCN java.lang.String ,
SCN java.lang.String ,
REC_INS_TIME java.lang.String );

CREATE CQ @CQ1@
INSERT INTO @STREAM2@
SELECT  
CASE WHEN (META(c,"OperationName").toString() == "DELETE")
THEN putUserData(c, 'isDelete', 'true') 
ELSE
putUserData(c,'isDelete', 'false')
END
FROM @STREAM1@ c;

CREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;

CREATE OR REPLACE CQ @CQ2@ 
INSERT INTO @STREAM3@
SELECT 
data(e),
META(e,"TableName").toString() as TABLE_NAME,
META(e, "OperationName").toString() as OPS_NAME,
META(e, "TimeStamp").toString() as DB_TIMESTAMP,
META(e,"COMMITSCN").toString() as COMMITSCN ,
META(e,"SCN").toString() as  SCN ,
DNOW().toString() as REC_INS_TIME
FROM @STREAM1@ e;

create Target @TARGET1@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC1',
ParallelThreads:'',
PartitionKey:'@metadata(TableName)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from @STREAM1@;


create Target @TARGET2@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC2',
ParallelThreads:'2',
PartitionKey:'TABLE_NAME',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from @STREAM3@;

create Target @TARGET3@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC3',
ParallelThreads:'',
PartitionKey:'@userdata(isDelete)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest.avsc')
input from @STREAM2@;

end application @WRITERAPPNAME@;
deploy application @WRITERAPPNAME@;
start @WRITERAPPNAME@;
stop application @READERAPPNAME@;
undeploy application @READERAPPNAME@;
drop application @READERAPPNAME@ cascade;
CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC1',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;


CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
filename:'@READERAPPNAME@_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream1;

CREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC2',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;


CREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC3',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest.avsc'
)
OUTPUT TO KafkaReaderStream3;


end application @READERAPPNAME@;

stop application PublishAPITester.NS;
undeploy application PublishAPITester.NS;
drop application PublishAPITester.NS cascade;

create application NS;

create Stream MyStream of Global.WAEvent;

CREATE TYPE bankData
(
bankID Integer KEY,
bankName String
);


CREATE TYPE moreBankData
(
bankID Integer KEY,
bankName String,
bankRouting long,
bankAmount double
);

create stream bankStream of bankData;

create stream dataStream of moreBankData;

--create Target t3 using SysOut(name:AgentOut) input from MyStream;

end application NS;

DEPLOY APPLICATION NS; 
start NS;

CREATE FLOW ServerFlow;

CREATE TARGET @TARGET_NAME@_sysout USING Global.SysOut (
  name: '@TARGET_NAME@_SysOut' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1'
 )
INPUT FROM @STREAM@;
END FLOW ServerFlow;

CREATE CQ @CQ_NAME@
INSERT INTO @EMB_STREAM@
@SELECT_QUERY@
FROM @STREAM@ e;

STOP application admin.app1;
undeploy application admin.app1;
drop application admin.app1 cascade;


CREATE APPLICATION app1;

CREATE SOURCE S1 USING Global.OracleReader (
  Tables: 'QATEST.TEST01',
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  OutboundServerProcessName: 'WebActionXStream',
  Password: 'qatest',
  Compression: false,
  ReaderType: 'LogMiner',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  FetchSize: 1,
  Password_encrypted: 'false',
  SupportPDB: false,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CDDLAction: 'Process',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  DictionaryMode: 'OnlineCatalog',
  QueueSize: 2048,
  CommittedTransactions: true,
  XstreamTimeOut: 600,
  CDDLCapture: false,
  TransactionBufferType: 'Disk',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '100MB',
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  DatabaseRole: 'Primary' )
OUTPUT TO buffer;

CREATE TARGET t USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'qatest',
  ParallelThreads: '',
  DatabaseProviderType: 'Oracle',
  CheckPointTable: 'CHKPOINT',
  Password_encrypted: 'false',
  CDDLAction: 'Process',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.TEST01,QATEST.TEST02',
  CommitPolicy: 'EventCount:1000,Interval:60',
  StatementCacheSize: '50',
  Username: 'qatest',
  BatchPolicy: 'EventCount:1000,Interval:60',
  PreserveSourceTransactionBoundary: 'false' )
INPUT FROM buffer;

END APPLICATION app1;

CREATE FLOW @STREAM@_SourceFlow;

CREATE SOURCE @SOURCE_NAME@ USING MSSqlReader (
 Username:'@READER-UNAME@',
 Password:'@READER-PASSWORD@',
 DatabaseName:'@DB-NAME@',
 ConnectionURL:'@CDC-READER-URL@',
 Tables:@WATABLES@,
 ConnectionPoolSize:2,
 Compression:false,
 StartPosition:'EOF'
) OUTPUT TO @STREAM@;

END FLOW @STREAM@_SourceFlow;

--
-- Recovery Test 5 with Jumping window and partitioned
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP Recov5Tester.RecovTest5;
UNDEPLOY APPLICATION Recov5Tester.RecovTest5;
DROP APPLICATION Recov5Tester.RecovTest5 CASCADE;
CREATE APPLICATION RecovTest5 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvData PARTITION BY merchantId;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION RecovTest5;

--use global;
CREATE OR REPLACE PROPERTYSET LDAP1 ( PROVIDER_URL:"@LDAP_URL@", SECURITY_AUTHENTICATION:@LDAP_AUTH@, SECURITY_PRINCIPAL: "@LDAP_PRINCIPAL@" , SECURITY_CREDENTIALS:@LDAP_CRED@, USER_BASE_DN:"@LDAP_DN@", User_userId:@LDAP_USERID@ );

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_scnRange: 1000,
 _h_eoffDelay: 10,
 SupportPDB: false,
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING MySqlReader
(
  Compression: false,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'mysql://localhost:3306',
  DatabaseName: 'waction',
  Tables: 'waction.test01',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root',
  connectionRetryPolicy:'retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'waction.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;

--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;


CREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING XMLParserV2 ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE CQ @APPNAME@_CQ
INSERT INTO @APPNAME@_CQOut
SELECT
data.attributeValue("merchantid") as merchantID,
data.getText() as companyName
FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_CQOut;

END APPLICATION @APPNAME@;

--
-- Crash Recovery Test 3 on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW -> CQ(aggregate) -> WS
--

STOP APPLICATION N4S4CR3Tester.N4S4CRTest3;
UNDEPLOY APPLICATION N4S4CR3Tester.N4S4CRTest3;
DROP APPLICATION N4S4CR3Tester.N4S4CRTest3 CASCADE;
CREATE APPLICATION N4S4CRTest3 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest3;

CREATE SOURCE CsvSourceN4S4CRTest3 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest3;

CREATE FLOW DataProcessingN4S4CRTest3;

CREATE TYPE WactionTypeN4S4CRTest3 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount int,
  city String
);

CREATE STREAM DataStream OF WactionTypeN4S4CRTest3;

CREATE CQ CsvToDataN4S4CRTest3
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_INT(TO_DOUBLE(data[7])),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN4S4CRTest3 CONTEXT OF WactionTypeN4S4CRTest3
EVENT TYPES ( WactionTypeN4S4CRTest3 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN4S4CRTest3
INSERT INTO WactionsN4S4CRTest3
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

END FLOW DataProcessingN4S4CRTest3;

END APPLICATION N4S4CRTest3;

--
-- Kafka Stream Recovery Test 1 with FileWriter as Target
-- Amudha, Striim, Inc.
--
-- S -> CQ -> KS -> WS

STOP KStreamRecov1Tester.KStreamRecovTest1wfwr;
UNDEPLOY APPLICATION KStreamRecov1Tester.KStreamRecovTest1wfwr;
DROP APPLICATION KStreamRecov1Tester.KStreamRecovTest1wfwr CASCADE;
DROP USER KStreamRecov1Tester;
DROP NAMESPACE KStreamRecov1Tester CASCADE;
CREATE USER KStreamRecov1Tester IDENTIFIED BY KStreamRecov1Tester;
-- GRANT 'Global:create,drop:deploymentgroup:*' TO USER KStreamRecov1Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov1Tester;
CONNECT KStreamRecov1Tester KStreamRecov1Tester;

CREATE APPLICATION KStreamRecovTest1wfwr RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE or REPLACE TYPE CsvStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM TypedStream OF CsvStreamType; 

CREATE CQ InsertEvents
INSERT INTO TypedStream
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE TARGET FileWrt USING FILEWRITER (
	directory:'@FEATURE-DIR@/logs/',
	FILENAME:'FileKafkaStream.log',
	flushpolicy:'eventcount:1'
--	rolloverpolicy:'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter(

) 
INPUT FROM TypedStream;


END APPLICATION KStreamRecovTest1wfwr;
DEPLOY APPLICATION KStreamRecovTest1wfwr;
START APPLICATION KStreamRecovTest1wfwr;

--
-- Recovery Test 12 with two sources, two jumping attribute windows, one wactionstore with recovery, and another wactionstore without -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W/p -> CQ1 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS2 (no persists)
--

STOP Recov12Tester.RecovTest12;
UNDEPLOY APPLICATION Recov12Tester.RecovTest12;
DROP APPLICATION Recov12Tester.RecovTest12 CASCADE;
CREATE APPLICATION RecovTest12 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsNoPersist CONTEXT OF WactionData
EVENT TYPES ( CsvData )
		PERSIST NONE USING ( ) ;

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWactionNoPersist
INSERT INTO WactionsNoPersist
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION RecovTest12;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING MariaDbXpandReader
(
Username: '@READER-UNAME@',
Password: '@READER-PASSWORD@',
ConnectionURL: '@CDC-READER-URL@',
Tables: @WATABLES@,
sendBeforeImage:'true',
FilterTransactionBoundaries: 'true'
) 
OUTPUT TO @STREAM@;

stop application DualGen;
undeploy application DualGen;
drop application DualGen cascade;
CREATE APPLICATION DualGen;

CREATE OR REPLACE TYPE DualEvent (
     Dummy DateTime,
    PhoneNo java.lang.String
);

CREATE OR REPLACE STREAM DualEvents OF DualEvent;

CREATE OR REPLACE CQ GenDual 
INSERT INTO DualEvents
SELECT
    TO_DATEF('28-FEB-22',"dd-MMM-yy") as Dummy,
    maskPhoneNumber('44 844 493 0787', "(\\\\d{0,4}\\\\s)(\\\\d{0,4}\\\\s)([0-9 ]+)", 1, 2) as PhoneNo
FROM
   heartbeat(interval 10 second) h;

   CREATE OR REPLACE CQ GenDual2 
INSERT INTO DualEvents
SELECT
   TO_DATEF('12/JAN/32',"dd/MMM/yy") as Dummy,
   maskPhoneNumber('12 345 678 9101', "(\\\\d{0,4}\\\\s)(\\\\d{0,4}\\\\s)([0-9 ]+)", 1, 2) as PhoneNo
FROM
   heartbeat(interval 30 second) h;

CREATE  SOURCE Orac_Src USING OracleReader  ( 
  Compression: false,
  DictionaryMode: 'OnlineCatalog',
  StartTimestamp: 'null',
  SupportPDB: false,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  DDLCaptureMode: 'All',
  CommittedTransactions: true,
  QueueSize: 2048,
  ReaderType: 'LogMiner',
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Username: 'miner',
  Password: 'miner',
  Tables: 'QATEST.HEARTBEAT',
  OutboundServerProcessName: 'WebActionXStream',
  Password_encrypted: false
 ) 
OUTPUT TO CDC_Events ;

CREATE OR REPLACE CQ cdcdual 
INSERT INTO DualEvents
SELECT
   TO_DATE(data[0]) as Dummy,
   data[1] as PhoneNo
FROM
   CDC_Events;


CREATE OR REPLACE TARGET DualSys USING SysOut  ( 
  name: 'heartbeat_out'
 ) 
INPUT FROM DualEvents;

CREATE TARGET DSVFormatterOut using FileWriter(
 filename:'HeartBeat_Reader.log',
 flushpolicy:'EventCount:16',
 rolloverpolicy:'EventCount:16,interval:117s')
FORMAT USING DSVFormatter ()
INPUT FROM DualEvents;


END APPLICATION DualGen;
deploy application DualGen;
start application DualGen;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE APPLICATION @APPNAME@ WITH ENCRYPTION @Recovery@ USE EXCEPTIONSTORE;
create flow @APPNAME@_SourceFlow;
CREATE SOURCE @APPNAME@_s USING FileReader(
  directory:'Samples/AppData',
  wildcard:'PO.JSON',
  positionByEOF:false
)
parse using JSONParser (
) OUTPUT TO @APPNAME@_ss1;
end flow @APPNAME@_SourceFlow;
create target @APPNAME@_t using sysout (name:ss1) input from @APPNAME@_ss1;
end application @APPNAME@;
deploy application @APPNAME@ @DP@;
start @APPNAME@;

stop bankApp;
undeploy application bankApp;

alter application bankApp;
CREATE OR REPLACE TYPE wsData  ( bankID java.lang.Integer KEY, 
nameOfBank java.lang.String
);

CREATE OR REPLACE WACTIONSTORE oneWS  CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@;



alter application bankApp recompile;
deploy application bankApp;
start application bankApp;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE @SourceName@ USING PostgreSQLReader  (
  ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: '@UserName@',
  Password_encrypted: false,
  ConnectionURL: '@SourceConnectionURL@',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: '@Password@',
  Tables: '@SourceTable@'
 ) 
OUTPUT TO @SRCINPUTSTREAM@;


CREATE OR REPLACE TARGET @targetsys@ USING SysOut (name: 'ora12_out') INPUT FROM @SRCINPUTSTREAM@;


CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@UserName@',
  Password_encrypted: 'false',
  BatchPolicy: 'EventCount:1,Interval:60',
  CommitPolicy: 'EventCount:1,Interval:60',
  ConnectionURL: '@TargetConnectionURL@',
  Tables: '@SourceTable@,@TargetTable@',

  adapterName: 'PostgreSQLReader',
  Password: '@Password@'
 )
INPUT FROM @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

STOP APPLICATION @APPNAME@app1;
STOP APPLICATION @APPNAME@app2;
STOP APPLICATION @APPNAME@app3;
STOP APPLICATION @APPNAME@app4;
STOP APPLICATION @APPNAME@app5;
UNDEPLOY APPLICATION @APPNAME@app1;
UNDEPLOY APPLICATION @APPNAME@app2;
UNDEPLOY APPLICATION @APPNAME@app3;
UNDEPLOY APPLICATION @APPNAME@app4;
UNDEPLOY APPLICATION @APPNAME@app5;
DROP APPLICATION @APPNAME@app1 CASCADE;
DROP APPLICATION @APPNAME@app2 CASCADE;
DROP APPLICATION @APPNAME@app3 CASCADE;
DROP APPLICATION @APPNAME@app4 CASCADE;
DROP APPLICATION @APPNAME@app5 CASCADE;

CREATE APPLICATION @APPNAME@app1 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;

create flow @APPNAME@agentflowps;
CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',
acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');


create type @APPNAME@type1(
  id String,
  name String,
  city string
);

CREATE STREAM @APPNAME@sourcestream OF Global.waevent persist using @APPNAME@KafkaPropset;
CREATE STREAM @APPNAME@kps_typedStream OF @APPNAME@type1 partition by city persist using @APPNAME@KafkaPropset;


CREATE OR REPLACE SOURCE @APPNAME@s USING oracleReader  ( 
  Username:'qatest',
  Password:'qatest',
  ConnectionURL:'localhost:1521/xe',
  Tables:'QATEST.test01',
  FetchSize:1
 ) 
OUTPUT TO @APPNAME@rawstream;

create cq @APPNAME@cq1
INSERT INTO @APPNAME@sourcestream
SELECT * from @APPNAME@rawstream;

CREATE CQ @APPNAME@cq2
INSERT INTO @APPNAME@kps_typedStream
SELECT TO_STRING(data[0]),
TO_STRING(data[1]),
TO_STRING(data[2])FROM @APPNAME@rawstream;
end flow @APPNAME@agentflowps;
end application @APPNAME@app1;
--deploy application app1;
--deploy application app1 with agentflowps on AGENTS;
@DEPLOY@;

CREATE APPLICATION @APPNAME@app2 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE TARGET @APPNAME@app2_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS1'
) INPUT FROM @APPNAME@sourcestream;


end application @APPNAME@app2;
deploy application @APPNAME@app2;


CREATE APPLICATION @APPNAME@app3 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE TARGET @APPNAME@app3_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS2'
) INPUT FROM @APPNAME@sourcestream;

end application @APPNAME@app3;
--deploy application app3;


CREATE APPLICATION @APPNAME@app4 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE TARGET @APPNAME@app4_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS3'
) INPUT FROM @APPNAME@sourcestream;

end application @APPNAME@app4;
--deploy application app4;


CREATE APPLICATION @APPNAME@app5 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE TARGET @APPNAME@app5_target1 USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'snappy1',
KafkaConfig:'compression.type=snappy'
) 
FORMAT USING DSVFormatter ()
INPUT FROM @APPNAME@kps_typedStream;

CREATE TARGET @APPNAME@app5_target2 USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'gzip1',
KafkaConfig:'compression.type=gzip'
) 
FORMAT USING DSVFormatter ()
INPUT FROM @APPNAME@sourcestream;

CREATE TARGET @APPNAME@app5_target3 USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'lz41',
KafkaConfig:'compression.type=lz4'
) 
FORMAT USING DSVFormatter ()
INPUT FROM @APPNAME@sourcestream;

end application @APPNAME@app5;
--deploy application app5;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:100,Interval:3',
  CommitPolicy: 'EventCount:100,Interval:3',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@1;

CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@1;

CREATE APPLICATION @AppName@;

CREATE OR REPLACE SOURCE initialLoad_Src USING Global.DatabaseReader (
  QuiesceOnILCompletion: false,
  Tables: '@SrcTableName@',
  adapterName: 'DatabaseReader',
  Password: '@Password@',
  Username: '@UserName@',
  ConnectionURL: '@Srcurl@',
   FetchSize: 10000)
OUTPUT TO CommonRawStream;

Create Source OrcReader_Src Using OracleReader(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@Srcurl@',
 Tables:'@SrcTableName@',
 Fetchsize:100)
Output To CommonRawStream;

CREATE OR REPLACE TARGET Postgres_Trg USING Global.DatabaseWriter (
  ConnectionURL: '@trgUrl@',
  Username: '@trgUsrName@',
  Tables: '@SrcTableName@,@trgTable@',
  Password: '@trgPswd@',
  CommitPolicy: 'EventCount:10000,Interval:60',
  adapterName: 'DatabaseWriter' )
INPUT FROM CommonRawStream;

CREATE TARGET filewriter_tgt USING Global.FileWriter (
 directory:'@trgDir@',
  filename: '@fileName@',
  flushpolicy: 'EventCount:10000,Interval:30s',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n' )
INPUT FROM CommonRawStream;

CREATE OR REPLACE TARGET BigQuery_Target USING Global.BigQueryWriter (
  streamingUpload: 'false',
  projectId: '@projectID@',
  Tables: '@SrcTableName@,@BQTableName@',
  optimizedMerge: 'false',
  ServiceAccountKey: '@ServiceAccountKey@',
  BatchPolicy: 'EventCount:1000000,Interval:90',
  Mode: 'APPENDONLY' )
INPUT from CommonRawStream;

END APPLICATION @AppName@;

stop application BigqueryBulkLoadMonMetrics_cdc;
undeploy application BigqueryBulkLoadMonMetrics_cdc;
drop application BigqueryBulkLoadMonMetrics_cdc cascade;

CREATE APPLICATION BigqueryBulkLoadMonMetrics_cdc;

CREATE FLOW BigqueryBulkLoadMonMetrics_cdc_SourceFlow;

CREATE SOURCE BigqueryBulkLoadMonMetrics_cdc_DBSource USING oracleReader ( 
  Username: 'qatest', 
  FetchSize: 10000, 
  Password_encrypted: 'false', 
  Password: 'JVaLv3ZpgQDY8R2ZxS38xg==', 
  Tables: 'QATEST.EMPLOYEE', 
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe' ) 
OUTPUT TO BigqueryBulkLoadMonMetrics_cdc_OutputStream;

END FLOW BigqueryBulkLoadMonMetrics_cdc_SourceFlow;

CREATE OR REPLACE TARGET BigqueryBulkLoadMonMetrics_cdc_BigQueryTarget1 USING BigQueryWriter ( 
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  streamingUpload: 'false', 
  projectId: 'striimqa-214712', 
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:100000, Interval:90', 
   AllowQuotedNewLines: 'false', 
  optimizedMerge: 'false', 
  TransportOptions: 'connectionTimeout=300, readTimeout=120', 
  adapterName: 'BigQueryWriter', 
  Mode: 'MERGE', 
  StandardSQL: 'true', 
  includeInsertId: 'true', 
  QuoteCharacter: '\"', 
  ServiceAccountKey: '/Users/jenniffer/Product2/IntegrationTests/TestData/google-gcs.json' ) 
INPUT FROM BigqueryBulkLoadMonMetrics_cdc_OutputStream;

END APPLICATION BigqueryBulkLoadMonMetrics_cdc;

deploy application BigqueryBulkLoadMonMetrics_cdc;
start application BigqueryBulkLoadMonMetrics_cdc;

--
-- Recovery Test 27 with two sources, two jumping time windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jt1W -> CQ1 -> WS
--   S2 -> Jt2W -> CQ2 -> WS
--

STOP Recov27Tester.RecovTest27;
UNDEPLOY APPLICATION Recov27Tester.RecovTest27;
DROP APPLICATION Recov27Tester.RecovTest27 CASCADE;
CREATE APPLICATION RecovTest27 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream1Second
OVER DataStream1 KEEP WITHIN 1 SECOND;

CREATE JUMPING WINDOW DataStream2Second
OVER DataStream2 KEEP WITHIN 2 SECOND;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data1ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream1Second p;

CREATE CQ Data2ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream2Second p;

END APPLICATION RecovTest27;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

CREATE SOURCE @SourceName@ USING MSSqlReader  ( 
TransactionSupport: false, 
  FetchTransactionMetadata: false, 
  autodisabletablecdc: true,
  Compression: false, 
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', 
  Password_encrypted: 'false', 
  ConnectionURL: '@ConnectionURL@', 
  Fetchsize: 0, 
  ConnectionPoolSize: 10, 
  StartPosition: 'EOF', 
  DatabaseName: 'qatest', 
  Username: '@UN@', 
  cdcRoleName: 'STRIIM_READER', 
  Password: '@PWD@', 
  Tables: '@sourcetable@', 
  FilterTransactionBoundaries: true, 
  SendBeforeImage: true, 
  ExcludedTables: 'qatest.CHKPOINT',
  AutoDisableTableCDC: false ) 
OUTPUT TO @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;


CREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'))) FROM @SRCINPUTSTREAM@ x; ;

CREATE TARGET @targetName@ USING DatabaseWriter  ( 
ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  ParallelThreads: '', 
  CheckPointTable: 'CHKPOINT', 
  Password_encrypted: 'false', 
  ConnectionURL: '@ConnectionURL@', 
  CommitPolicy: 'EventCount:1,Interval:60', 
  StatementCacheSize: '50', 
  DatabaseProviderType: 'Default', 
  Username: '@UN@', 
  Password: '@PWD@', 
  PreserveSourceTransactionBoundary: 'false', 
  BatchPolicy: 'EventCount:1,Interval:60', 
  Tables: '@TablemappingwithColmap@' ) 
INPUT FROM admin.sqlreader_cq_out;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.56.101:1521/orcl',
  Tables: 'QATEST.oracle_200',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;


create stream xferredDataStream1 of Global.WAEvent;

CREATE CQ CQ1
insert into xferredDataStream1
select  putuserdata (data1,'ID', data[0]) from Oracle_ChangeDataStream data1;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:4,Interval:60',
  CommitPolicy: 'EventCount:4,Interval:60',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  IgnorableExceptionCode:'PRIMARY KEY',
  Tables: 'QATEST.oracle_200,test.cassandra_200 columnmap(field1=field1,field2=field2,field3=field3,field4=field4,field5=field5,field6=field6,field7=field7,field8=field8,field9=field9,field10=field10,field11=field11,field12=field12,field13=field13,field14=field14,field15=field15,field16=field16,field17=field17,field18=field18,field19=field19,field20=field20,field21=field21,field22=field22,field23=field23,field24=field24,field25=field25,field26=field26,field27=field27,field28=field28,field29=field29,field30=field30,field31=field31,field32=field32,field33=field33,field34=field34,field35=field35,field36=field36,field37=field37,field38=field38,field39=field39,field40=field40,field41=field41,field42=field42,field43=field43,field44=field44,field45=field45,field46=field46,field47=field47,field48=field48,field49=field49,field50=field50,field51=field51,field52=field52,field53=field53,field54=field54,field55=field55,field56=field56,field57=field57,field58=field58,field59=field59,field60=field60,field61=field61,field62=field62,field63=field63,field64=field64,field65=field65,field66=field66,field67=field67,field68=field68,field69=field69,field70=field70,field71=field71,field72=field72,field73=field73,field74=field74,field75=field75,field76=field76,field77=field77,field78=field78,field79=field79,field80=field80,field81=field81,field82=field82,field83=field83,field84=field84,field85=field85,field86=field86,field87=field87,field88=field88,field89=field89,field90=field90,field91=field91,field92=field92,field93=field93,field94=field94,field95=field95,field96=field96,field97=field97,field98=field98,field99=field99,field100=field100,field101=field101,field102=field102,field103=field103,field104=field104,field105=field105,field106=field106,field107=field107,field108=field108,field109=field109,field110=field110,field111=field111,field112=field112,field113=field113,field114=field114,field115=field115,field116=field116,field117=field117,field118=field118,field119=field119,field120=field120,field121=field121,field122=field122,field123=field123,field124=field124,field125=field125,field126=field126,field127=field127,field128=field128,field129=field129,field130=field130,field131=field131,field132=field132,field133=field133,field134=field134,field135=field135,field136=field136,field137=field137,field138=field138,field139=field139,field140=field140,field141=field141,field142=field142,field143=field143,field144=field144,field145=field145,field146=field146,field147=field147,field148=field148,field149=field149,field150=field150,field151=@METADATA(OperationName),field152=field1,field153=@METADATA(SQLRedoLength),field154=@METADATA(SEQUENCE),field155=@METADATA(SegmentName),field156=@METADATA(OperationType),field157=@METADATA(TxnUserID),field158=@METADATA(ThreadID),field159=@METADATA(TxnUserID),field160=@USERDATA(ID),field161=@USERDATA(ID),field162=@USERDATA(ID),field163=@USERDATA(ID),field164=field3,field165=field150,field166=field151)',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM xferredDataStream1;

create Target t2 using SysOut(name:Foo2) input from xferredDataStream1;

END APPLICATION DBRTOCW;

deploy application DBRTOCW in  default;

start application DBRTOCW;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @srcName@ USING OracleReader (
 Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@'
)
OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@ USING DatabaseWriter (
  CheckPointTable: 'CHKPOINT', 
  ReplicationSlotName:'test_slot',
  Username:'@tgtusername@',
  Password:'@tgtpassword@',
  ConnectionURL:'@tgturl@',
  adapterName:'PostgreSQLReader',
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@'
)
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;

CREATE  APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE @srcName@ USING Global.DatabaseReader (
  ConnectionURL: '@srcurl@', 
  Tables: '@srcschema@.@srctable@',
  ReturnDateTimeAs: 'String', 
  FetchSize: 30000, 
  Username: '@srcusername@', 
  QuiesceOnILCompletion: true, 
  Password: '@srcpassword@', 
  DatabaseProviderType: 'DEFAULT' ) 
OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@  USING Global.DatabaseWriter ( 
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  CheckPointTable: 'CHKPOINT',
  Username:'@tgtusername@', 
  Password:'@tgtpassword@', 
  CDDLAction: 'Process', 
  StatementCacheSize: '50', 
  CommitPolicy: 'EventCount:30000,Interval:60', 
  ConnectionURL:'@tgturl@',
  DatabaseProviderType: 'Default', 
  PreserveSourceTransactionBoundary: 'false', 
  BatchPolicy: 'EventCount:30000,Interval:60', 
  Tables:'@srcschema@.@srctable@,@tgtschema@.@tgttable@', 
  adapterName: 'DatabaseWriter' ) 
INPUT FROM @instreamname@;

End APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.DatabaseReader (
  FetchSize: 1,
  ConnectionURL: '@ORACLE-URL@',
  Tables: '@SOURCE-TABLES@',
  Username: '@ORACLE-USERNAME@',
  Password: '@ORACLE-PASSWORD@'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt USING SnowflakeWriter

(
  ConnectionURL:'@SNOWFLAKE-URL@',
  username:'@SNOWFLAKE-USERNAME@',
  appendOnly:'false',
  Tables:'@TARGET-TABLES@',
  uploadpolicy:'eventcount:3,interval:10s',
  externalStageType:'local'
)
INPUT FROM @APP_NAME@_Stream;


END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

CREATE TARGET @TARGET@ USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: 'access_key',
	  --secretaccesskey: 'secret_access',
	  Tables: 'QATEST.%,QATEST.%',
	   S3IAMRole:'@IAMROLE@',
	uploadpolicy:'EventCount:7'
	) INPUT FROM @STREAM@;

end application @APPNAME@;

CREATE OR REPLACE APPLICATION @AppName@;

Create Source @AppName@_source Using OracleReader(
  Username:'@username@',
  Password:'@password@',
  ConnectionURL:'@url@',
  Tables:'@srctableName@',
  Fetchsize:1
)
Output To @AppName@_Stream;
CREATE OR REPLACE TARGET @AppName@_SF_Target USING Global.SnowflakeWriter (
  streamingUpload: 'false',
  useConnectionProfile:'true',
  connectionProfileName: 'admin.@SFCP@',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  columnDelimiter: '|',
  tables: '@srctableName@,@trgtableName@',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:1,interval:5m',
  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',
  externalStageType: 'Local',
  adapterName: 'SnowflakeWriter',
  fileFormatOptions: 'null_if = \"\"' )
INPUT FROM @AppName@_Stream;


CREATE OR REPLACE TARGET @AppName@_DB_Target USING Global.DeltaLakeWriter (
connectionProfileName: 'admin.@DBCP@',
useConnectionProfile:'true',
  Tables: '@srctableName@,@DBtrgtableName@',
  uploadPolicy: 'eventcount:100000,interval:60s'
)

INPUT FROM @AppName@_Stream;
END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

create or replace type @STREAM@orderBill(
id int,
name String,
cost float,
TableName string,
operationName String
);

create or replace stream @STREAM@_TYPED of @STREAM@OrderBill;

Create or replace CQ @STREAM@orderbillCQ
insert into @STREAM@_TYPED
select 
to_int(data[0]),data[1],to_float(data[2]),
meta(@STREAM@,'TableName'),Meta(@STREAM@,'OperationName') from @STREAM@;


create or replace Target @TARGET_NAME@ using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @STREAM@_TYPED;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @kafkasrc@ USING KafkaReader VERSION @KAFKAVERSION@ (
  brokerAddress: '',
  Topic: '',
  startOffset: '0' )
PARSE USING AvroParser (
  schemaregistryurl: 'http://localhost:8081/' )
OUTPUT TO @appname@Stream2;

CREATE TYPE @appname@ElementsOfNativeRecord1 (
 datarecord com.fasterxml.jackson.databind.JsonNode,
 before com.fasterxml.jackson.databind.JsonNode,
 metadata com.fasterxml.jackson.databind.JsonNode,
 userdata com.fasterxml.jackson.databind.JsonNode,
 datapresenceinfo com.fasterxml.jackson.databind.JsonNode,
 beforepresenceinfo com.fasterxml.jackson.databind.JsonNode);

CREATE TYPE @appname@completeRecord1 (
 completedata com.fasterxml.jackson.databind.JsonNode);

CREATE STREAM @appname@NativeRecordStream1 OF @appname@ElementsOfNativeRecord1;

CREATE STREAM @appname@CompleteRecordInJSONStream1 OF @appname@completeRecord1;

CREATE TARGET @filetarget@ USING FileWriter (
  filename: 'kafkaout',
  directory: ''
  rolloverpolicy: 'EventCount:10' )
FORMAT USING JSONFormatter  (
  members: 'datarecord' )
INPUT FROM @appname@NativeRecordStream1;

CREATE CQ @appname@GetNativeRecordInJSONCQ1
INSERT INTO @appname@NativeRecordStream1
SELECT
 completedata.get("data"),
 completedata.get("before"),
 completedata.get("metadata"),
 completedata.get("userdata"),
 completedata.get("datapresenceinfo"),
 completedata.get("beforepresenceinfo")
FROM @appname@CompleteRecordInJSONStream1;

CREATE CQ @appname@CQ2
INSERT INTO @appname@CompleteRecordInJSONStream1
SELECT
 AvroToJson(y.data)
 from @appname@Stream2 y;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@;
CREATE SOURCE @srcName@ USING Global.OracleReader ( 
  Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@') 
OUTPUT TO @instreamname@;

CREATE OR REPLACE TARGET @tgtName@ USING Global.SnowflakeWriter ( 
  connectionUrl: '@tgturl@', 
  tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@', 
  password: '@tgtpassword@',   
  username: '@tgtusername@', 
  uploadPolicy: 'eventcount:1,interval:5m', 
  authenticationType: 'Password',
  externalStageType: 'Local', 
  adapterName: 'SnowflakeWriter' ) 
INPUT FROM @outstreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

create flow serverFlow;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@;

end flow serverFlow;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING FILEReader (
    wildcard: '',
    directory: '',
    positionbyeof: false
 )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:10,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: ''
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE APPLICATION @APPNAME@ @RECOVERY@;

CREATE FLOW @APPNAME@AgentFlow;
CREATE OR REPLACE SOURCE @APPNAME@_src USING Global.GCSReader ()
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream;
END FLOW @APPNAME@AgentFlow;

CREATE FLOW @APPNAME@serverFlow;
CREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer ()
FORMAT USING DSVFormatter (
members:'data')
INPUT FROM @APPNAME@_Stream;
END FLOW @APPNAME@serverFlow;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ with @APPNAME@AgentFlow in Agents, @APPNAME@ServerFlow in default;
start application @APPNAME@;

CREATE APPLICATION SourcePosApp;

CREATE SOURCE PosCsvDataSource USING FileReader ( 
  directory: '@TEST-DATA-PATH@', 
  wildcard: 'posdata.csv', 
  positionbyeof: false ) 
PARSE USING DSVParser ( 
  charset: 'UTF-8' )
OUTPUT TO PosCsvStream;

CREATE TARGET PosSourceDump using FileWriter(
  directory: '@FEATURE-DIR@/logs',  
  filename: 'SourcePosAppData',
  rolloverpolicy: 'EventCount:6000000'
   )
FORMAT USING Global.DSVFormatter (
  members: 'data',
  charset: 'UTF-8' ) 
 input from PosCsvStream;

CREATE Source PosHourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt',
  positionByEOF:false )
PARSE USING DSVParser ( 
    charset: 'UTF-8' )
OUTPUT TO PosCacheSource1;

CREATE TARGET PosCacheDump1 using FileWriter(
  directory: '@FEATURE-DIR@/logs',  
  filename: 'SourcePosCacheData1',
  rolloverpolicy: 'EventCount:6000000' ) 
FORMAT USING DSVFormatter (
    members: 'data', 
    charset: 'UTF-8') 
 input from PosCacheSource1;
 
CREATE Source PosNameLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'MerchantNames.csv',
  positionByEOF:false )
PARSE USING DSVParser ( 
    charset: 'UTF-8' )
 OUTPUT TO PosCacheSource2;

CREATE TARGET PosCacheDump2 using FileWriter(
  directory: '@FEATURE-DIR@/logs',  
  filename: 'SourcePosCacheData2',
  rolloverpolicy: 'EventCount:6000000' ) 
FORMAT USING DSVFormatter (
    members: 'data', 
    charset: 'UTF-8')
 input from PosCacheSource2;
 
CREATE Source PosZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  positionByEOF:false ) 
PARSE USING DSVParser ( 
    columndelimiter:'\t',
    charset: 'UTF-8' )
OUTPUT To PosCacheSource3;

CREATE TARGET PosCacheDump3 using FileWriter(
  directory: '@FEATURE-DIR@/logs',  
  filename: 'SourcePosCacheData3',
  rolloverpolicy: 'EventCount:6000000') 
FORMAT USING DSVFormatter (
    members: 'data', 
    charset: 'UTF-8')
INPUT from PosCacheSource3;

END APPLICATION SourcePosApp;

--
-- Recovery Test 34 with two sources, two sliding time-count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5a9W/p  -> CQ1 -> WS
-- S2 -> Sc6a11W/p -> CQ2 -> WS
--

STOP Recov34Tester.RecovTest34;
UNDEPLOY APPLICATION Recov34Tester.RecovTest34;
DROP APPLICATION Recov34Tester.RecovTest34 CASCADE;
CREATE APPLICATION RecovTest34 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION RecovTest34;

use PosTester;
DROP TYPE MerchantTxRate;

stop application reconnect;
undeploy application reconnect;
drop application reconnect cascade;
CREATE APPLICATION reconnect recovery 1 second interval;

CREATE  SOURCE mssqlsource USING MssqlReader  ( 
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  ConnectionURL: '@URL@',
  Tables: '@TABLE@',
  FetchSize: 1
 ) 
OUTPUT TO sqlstream;

CREATE TARGET dbtarget USING CassandraWriter(
  ConnectionURL:'@URL@',
  Username:'@USERNAME@',
  Password:'@PASSWORD@',
  ConnectionRetryPolicy: 'retryInterval=15s,maxRetries=2',
  BatchPolicy:'EventCount:5,Interval:30',
  CommitPolicy:'EventCount:5,Interval:30',
  Tables: '@TABLES@'
 ) INPUT FROM sqlstream;

 create Target tSysOut using Sysout(name:OrgData) input from sqlstream;
 end application reconnect;
 deploy application reconnect;
 start application reconnect;

--
-- Recovery Test 20 with two sources going to one wactionstore
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> CQ1 -> WS
-- S2 -> CQ2 -> WS
--

STOP Recov20Tester.RecovTest20;
UNDEPLOY APPLICATION Recov20Tester.RecovTest20;
DROP APPLICATION Recov20Tester.RecovTest20 CASCADE;
CREATE APPLICATION RecovTest20 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions1
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ InsertWactions2
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream2;

END APPLICATION RecovTest20;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 1 second interval;

create flow agentflow;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@'
  --recordSelector: 'ARDPLKBX-RECORD:ARDPLKBX-RECORD-TYPE=ARDPLKBX-RECORD'
  --recordSelector: 'OH:MOH-SEG-ID=OH, OH2:OH2-SEG-ID=OH2, OHU:MOH-SEG-ID=OHU, OR1:OR1-SEG-ID=OR1, OR2:OR2-SEG-ID=OR2, OR3:OR3-SEG-ID=OR3, OR4:OR4-SEG-ID=OR4, OHM:OHM-SEG-ID=OHM, OD:OD-SEG-ID=OD, ODU:ODU-SEG-ID=ODU, OD1:OD1-SEG-ID=OD1, ODM:ODM-SEG-ID=ODM, OT:OT-SEG-ID=OT'
)
OUTPUT TO @APPNAME@Stream;

end flow agentflow;

create flow serverflow;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;


CREATE TYPE test_typeRe 
(
node_new com.fasterxml.jackson.databind.JsonNode,
node_name com.fasterxml.jackson.databind.JsonNode,
node_addr com.fasterxml.jackson.databind.JsonNode
);

Create stream cqAsJSONNodeStreamRe of test_typeRe;

CREATE CQ GetPOAsJsonNodesRe
INSERT into cqAsJSONNodeStreamRe
select 
data.get('ACCTS-RECORD'),
data.get('ACCTS-RECORD').get('NAME'),
data.get('ACCTS-RECORD').get('ADDRESS3')
from @APPNAME@Stream js;

create type finaldtypeRe
(ACCOUNT_NO int,
FIRST_NAME String,
LAST_NAME String,
ADDRESS1 String,
ADDRESS2 String,
CITY String,
STATE String,
ZIP_CODE int);

CREATE STREAM getdataStreamPS OF finaldtypeRe;

CREATE CQ getdataRe
INSERT into getdataStreamPS
select JSONGetInteger(x.node_new,"ACCOUNT-NO"),
JSONGetString(x.node_name,"FIRST-NAME"),
JSONGetString(x.node_name,"LAST-NAME"),
JSONGetString(x.node_new,"ADDRESS1"),
JSONGetString(x.node_new,"ADDRESS2"),
JSONGetString(x.node_addr,"CITY"),
JSONGetString(x.node_addr,"STATE"),
JSONGetInteger(x.node_addr,"ZIP-CODE")
from cqAsJSONNodeStreamRe x;

create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:1000,Interval:50',
  CommitPolicy: 'EventCount:1000,Interval:50',
  Tables: 'QATEST.@table@'
)
input from getdataStreamPS;
end flow serverflow;

end application @APPNAME@;
deploy application @APPNAME@ with agentflow on any in agents,serverflow in default; 
start application @APPNAME@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ recovery 5 second Interval;

CREATE SOURCE @src1Name@ USING MySQLReader (
  Username:'@srcusername@',
  Password:'@srcpassword@',
  ConnectionURL:'@srcurl@',
  Tables:'@srcschema@.@srctable@',
  BidirectionalMarkerTable: '@srcschema@.@srcbidirectionaltable@'
)
OUTPUT TO @outstream1name@;

CREATE TARGET @tgt1Name@ USING DatabaseWriter (
  ConnectionURL:'@tgturl@',
  Username:'@tgtusername@',
  Password:'@tgtpassword@',
  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
  CheckPointTable: 'CHKPOINT',
  BidirectionalMarkerTable: '@tgtschema@.@tgtbidirectionaltable@'
)
INPUT FROM @instream1name@;

CREATE SOURCE @src2Name@ USING MSSQLReader (
  ConnectionURL:'@tgturl@',
  DatabaseName:'@databasename@',
  Username:'@tgtusername@',
  Password:'@tgtpassword@',
  Tables:'@tgtschema@.@tgttable@',
  BidirectionalMarkerTable: '@tgtschema@.@tgtbidirectionaltable@',
  TransactionSupport: true
)
OUTPUT TO @outstream2name@;

CREATE TARGET @tgt2Name@ USING DatabaseWriter (
  Username:'@srcusername@',
  Password:'@srcpassword@',
  ConnectionURL:'@srcurl@',
  Tables: '@tgtschema@.@tgttable@,@srcschema@.@srctable@',
  CheckPointTable: 'CHKPOINT',
  BidirectionalMarkerTable: '@srcschema@.@srcbidirectionaltable@'
)
INPUT FROM @instream2name@;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop application MSSQLTransactionSupportMultiReaderWriter;
undeploy application MSSQLTransactionSupportMultiReaderWriter;
drop application MSSQLTransactionSupportMultiReaderWriter cascade;

CREATE APPLICATION MSSQLTransactionSupportMultiReaderWriter recovery 1 second interval;

Create Source ReadFromMSSQL4
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
--AutoDisableTableCDC:'true',
FetchTransactionMetadata:'true',
FilterTransactionBoundaries: false,
Compression:'false',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportMultiReaderWriterStream1;

Create Source ReadFromMSSQL5
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
--AutoDisableTableCDC:'true',
FetchTransactionMetadata:'true',
FilterTransactionBoundaries: false,
Compression:'true',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportMultiReaderWriterStream2;

CREATE TARGET WriteToMSSQL4 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC,@@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportMultiReaderWriterStream1;

CREATE TARGET WriteToMSSQL5 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC,@@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportMultiReaderWriterStream2;

/*CREATE TARGET MSSqlReaderOutput4 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportStream; 


/*CREATE OR REPLACE TARGET MSSQLFileOut4 USING FileWriter  ( 
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  directory:'@FEATURE-DIR@/logs/',
  filename: 'TransactionSupportAutoDisableTableCdcTrue.txt'
 ) 
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 ) 
INPUT FROM MSSQLTransactionSupportMultiReaderWriterStream;
*/
END APPLICATION MSSQLTransactionSupportMultiReaderWriter;
deploy application MSSQLTransactionSupportMultiReaderWriter;
start application MSSQLTransactionSupportMultiReaderWriter;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

CREATE SOURCE @SourceName@ USING PostgreSQLReader  ( 
ReplicationSlotName: 'test_slot',
adapterName: 'PostgreSQLReader',
TransactionSupport: false, 
  FetchTransactionMetadata: false, 
  Compression: false, 
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', 
  Password_encrypted: 'false', 
  ConnectionURL: '@ConnectionURL@', 
  Fetchsize: 0, 
  ConnectionPoolSize: 10, 
  StartPosition: 'EOF', 
  Username: '@UN@', 
  cdcRoleName: 'STRIIM_READER', 
  Password: '@PWD@', 
  Tables: 'qatest.%', 
  FilterTransactionBoundaries: true, 
  SendBeforeImage: true, 
  AutoDisableTableCDC: false ) 
OUTPUT TO @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;


CREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'), 'OpTime',META(x, 'TimeStamp'))) FROM @SRCINPUTSTREAM@ x; ;

CREATE TARGET @targetName@ USING DatabaseWriter  ( 
ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  ParallelThreads: '', 
  CheckPointTable: 'CHKPOINT', 
  Password_encrypted: 'false', 
  ConnectionURL: '@ConnectionURL@', 
  CommitPolicy: 'EventCount:1,Interval:60', 
  StatementCacheSize: '50', 
  DatabaseProviderType: 'Default', 
  Username: '@UN@', 
  Password: '@PWD@', 
  PreserveSourceTransactionBoundary: 'false', 
  BatchPolicy: 'EventCount:1,Interval:60', 
  Tables: '@TableMapping@' ) 
INPUT FROM admin.sqlreader_cq_out;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

use PosTester;
alter application PosApp;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startTime DateTime,
  count int,
  totalAmount double,
  hourlyAve int,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);

end application PosApp;
alter application PosApp recompile;

--
-- Crash Recovery Test 7 with Jumping window and partitioned on two node cluster with one agent
-- Bert Hashemi, WebAction, Inc.
--
-- S -> KafkaStream -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION N2S2CR7Tester.N2S2CRTest7;
UNDEPLOY APPLICATION N2S2CR7Tester.N2S2CRTest7;
DROP APPLICATION N2S2CR7Tester.N2S2CRTest7 CASCADE;
CREATE APPLICATION N2S2CRTest7 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest7;

CREATE SOURCE CsvSourceN2S2CRTest7 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;


CREATE TYPE CsvDataTypeN2S2CRTest7 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream of CsvDataTypeN2S2CRTest7 using KafkaProps;

CREATE CQ TransferToKafka
INSERT INTO KafkaCsvStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

END FLOW DataAcquisitionN2S2CRTest7;





CREATE FLOW DataProcessingN2S2CRTest7;

CREATE STREAM DataStream OF CsvDataTypeN2S2CRTest7 PARTITION BY merchantId;

CREATE CQ CsvToDataN2S2CRTest7
INSERT INTO DataStream
SELECT
    *
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsN2S2CRTest7 CONTEXT OF CsvDataTypeN2S2CRTest7
EVENT TYPES ( CsvDataTypeN2S2CRTest7 )
@PERSIST-TYPE@

CREATE CQ DataToWactionN2S2CRTest7
INSERT INTO WactionsN2S2CRTest7
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN2S2CRTest7;

END APPLICATION N2S2CRTest7;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

Create Source dataGenSrc Using MSSqlReader
(
 Username:'qatest',
 Password:'w3b@ct10n',
 DatabaseName:'qatest',
 ConnectionURL:'localhost:1433',
 Tables:'@tableNames@', 
 ConnectionPoolSize:1,
 FetchSize:1,
 StartPosition:'EOF'
 )
 Output To LCRStream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;

END APPLICATION DataGenSampleApp;

deploy application DataGenSampleApp;
start  application DataGenSampleApp;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;
CREATE OR REPLACE SOURCE @SourceName@ Using OJet
(
  Username: '@Username@',
  Password: '@Password@',
  ConnectionURL: '@ConnectionURL@',
  connectionRetryPolicy: @ConnectionRetryPolicy@,
  Tables: '@SourceTables@',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  Password_encrypted: 'false',
  CommittedTransactions: true,
  adapterName: 'OJet',
)OUTPUT TO @SRCINPUTSTREAM@;
create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 5 second iNTERVAL;
create source @SOURCE@ using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'dynamicdirectory.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO @STREAM@;

Create Type CSVType_@APPNAME@ (
  id String,
  name String,
  department String,
  yoj String,
  moj String,
  doj int
);

Create Stream Typed@STREAM@ of CSVType_@APPNAME@;

CREATE CQ CsvToPosData@APPNAME@
INSERT INTO Typed@STREAM@
SELECT data[0],
       data[1],
       data[2],
       data[3],
       data[4],
       TO_INT(data[5])
FROM @STREAM@;

create Target @TARGET@ using ADLSGen2Writer(
    accountname:'',
	sastoken:'',
	filesystemname:'',
	filename:'',
	directory:'',
	uploadpolicy:'eventcount:5'

)format using DSVFormatter (
)
input from Typed@STREAM@;

end application @APPNAME@;

deploy application @APPNAME@;
start application @APPNAME@;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
stop application @APPNAME3@;
undeploy application @APPNAME3@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;
drop application @APPNAME3@ cascade;

CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using OracleReader
(
  Compression:true,
  StartTimestamp:'null',
  CommittedTransactions:true,
  FilterTransactionBoundaries:true,
  Password_encrypted:'false',
  SendBeforeImage:true,
  XstreamTimeOut:600,
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',
  Tables:'qatest.oraMultiDownstream_src',
  adapterName:'OracleReader',
  Password:'qatest',
  DictionaryMode:'OfflineCatalog',
  FilterTransactionState:true,
  connectionRetryPolicy:'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType:'LogMiner',
  FetchSize: 1,
  Username:'qatest',
  OutboundServerProcessName:'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic:true,
  CDDLAction:'Quiesce_Cascade',
  CDDLCapture:'true'
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;


CREATE APPLICATION @APPNAME3@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName1@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME3@;
DEPLOY APPLICATION @APPNAME3@;
START APPLICATION @APPNAME3@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second Interval ;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectURL@',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.OJet (
  PrimaryDatabasePassword: '@PRIMARY_PASSWORD@',
  ConnectionURL: '@DOWNSTREAM_URL@',
  PrimaryDatabaseUsername: '@PRIMARY_USER@',
  Password: '@DOWNSTREAM_PASSWORD@',
  DownstreamCaptureMode: 'REAL_TIME',
  DownstreamCapture: true,
  PrimaryDatabaseConnectionURL: '@PRIMARY_URL@',
  Tables: '@SOURCE_TABLES@',
  Username: '@DOWNSTREAM_USER@' )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET_SYS@ USING Global.SysOut (
  name: 'Out' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter (
  ConnectionURL: '@TARGET_URL@',
  Username: '@TARGET_USER@',
  Password: '@TARGET_PASSWORD@',
  CheckPointTable: 'CHKPOINT',
  CommitPolicy: 'EventCount:1',
  Tables: '@TARGET_TABLES@',
  BatchPolicy: 'EventCount:1' )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

use global;

-- undeploy application T10;
drop application T10 cascade;

create application T10
RECOVERY 5 SECOND INTERVAL;

CREATE FLOW AgentFlow;
create source T10Source using CSVReader (
  directory:'Samples/AppData',
  header:Yes,
  wildcard:'customerdetails-recovery.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO T10Stream;
END FLOW AgentFlow;

CREATE FLOW ServerFlow;
create Target t using CSVWriter(fileName:AgentOut) input from T10Stream;
END FLOW ServerFlow;

end application T10;

DEPLOY APPLICATION T10 with AgentFlow in AGENTS, ServerFlow in SERVERS;

CREATE APPLICATION @APPNAME@ @RECOVERY@;

CREATE FLOW @APPNAME@AgentFlow;
CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream;
END FLOW @APPNAME@AgentFlow;

CREATE FLOW @APPNAME@serverFlow;
CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING DSVFormatter (
members:'data')
INPUT FROM @APPNAME@_Stream;
END FLOW @APPNAME@serverFlow;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@ with @APPNAME@AgentFlow in Agents, @APPNAME@ServerFlow in default;
start application @APPNAME@;

CREATE OR REPLACE SOURCE KafkaReaderSource USING Global.KafkaReader VERSION '2.1.0' 
( 
AutoMapPartition: true, 
brokerAddress: 'localhost:9092',       
KafkaConfigPropertySeparator: ',',
Topic: 'test011', 
startOffset: 0,
KafkaConfigValueSeparator: ':', 
KafkaConfig: 'request.timeout.ms:6001,session.timeout.ms:6000,security.protocol:SASL_SSL,sasl.mechanism:SCRAM-SHA-512,sasl.jaas.config:org.apache.kafka.common.security.scram.ScramLoginModule required username=kafkauser password=\"Oppenheimer\";,value.deserializer:com.striim.avro.deserializer.LengthDelimitedAvroRecordDeserializer' 
) 
PARSE USING Global.DSVParser () 
OUTPUT TO KafkaDevReadSourceStream;

\create Target @TARGET_NAME@ using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (

)
input from @STREAM@;

Stop Oracle_IRLogWriter;
Undeploy application Oracle_IRLogWriter;
drop application Oracle_IRLogWriter cascade;

CREATE APPLICATION Oracle_IRLogWriter WITH ENCRYPTION recovery 5 second interval;

CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( 
  FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.TDSOURCE',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.TEST01=ID;',
  PollingInterval: '5sec',
  ReturnDateTimeAs: 'String',
  startPosition:'striim.test01=0'
  )
  OUTPUT TO data_stream;

  CREATE OR REPLACE TARGET Oracle_IRSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

CREATE TARGET BinaryDump USING LogWriter(
  name: 'TeraData',
  filename:'TeraData.log'
)INPUT FROM data_stream;

END APPLICATION Oracle_IRLogWriter;

deploy application Oracle_IRLogWriter in default;

start application Oracle_IRLogWriter;

STOP DSAPP;
UNDEPLOY APPLICATION DSAPP;
DROP APPLICATION DSAPP CASCADE;

CREATE APPLICATION DSAPP;

-- CacheWaction WACTIONSTORE is being loaded from DSCache

CREATE OR REPLACE WACTIONSTORE CacheWaction CONTEXT OF T1
EVENT TYPES ( T1 )
@PERSIST-TYPE@

CREATE CQ CQ2Derby
INSERT INTO CacheWaction
select * from C1
LINK SOURCE EVENT;

END APPLICATION DSAPP;
DEPLOY APPLICATION DSAPP;
START APPLICATION DSAPP;

stop application APP_KAFKASOURCE_AGG;
undeploy application APP_KAFKASOURCE_AGG;
drop application APP_KAFKASOURCE_AGG cascade;

CREATE APPLICATION APP_KAFKASOURCE_AGG;

CREATE OR REPLACE TYPE STREAM_CQ_CALCULATELAG_Type (
 topic java.lang.String KEY,
 TotalTopicLag java.lang.Integer,
 lastdatatime org.joda.time.DateTime,
 status java.lang.String,
 is_green java.lang.Integer);

CREATE OR REPLACE TYPE STREAM_CQ_JOIN_KAFKA_SOURCES1_Type (
 Company java.lang.String KEY,
 TotalLast24hour java.lang.Integer,
 TotalLast1hour java.lang.Integer,
 TotalTopicLag java.lang.Integer,
 lastdatatime org.joda.time.DateTime,
 status java.lang.String,
 is_green java.lang.Integer,
 latitude java.lang.String,
 longitude java.lang.String,
 topic java.lang.String,
 city_name java.lang.String,
 city_id java.lang.Integer);

CREATE OR REPLACE TYPE STREAM_CQ_CALCULATE_HOURLYTOTALS_Type (
 topic java.lang.String KEY,
 TotalLast24hour java.lang.Integer,
 TotalLast1hour java.lang.Integer);

CREATE OR REPLACE CQ CQ_GET_LASTHOUR
INSERT INTO STREAM_CQ_GET_LASTHOUR
SELECT rawdatacount, topic,timerange from  ET_HOURLYTOTALS_KAFKADATA_FILE,JUMP_WND_1EVT_30SEC where timerange = DHOURS(DNOW())-1;

CREATE OR REPLACE CQ CQ_CALCULATE_HOURLY_TOTAL
INSERT INTO STREAM_CQ_CALCULATE_HOURLY_TOTAL
SELECT f.topic as topic, sum(f.rawdatacount) as TotalLast24hour, B.rawdatacount as TotalLast1hour FROM JUMP_WND_1EVT_1MIN h
   join SLIDE_WND_HOURLYTOTALS_KAFKADATA_FILE f on 1=1
   join STREAM_CQ_GET_LASTHOUR B on B.topic=f.topic
   Group by f.topic;

CREATE OR REPLACE EVENTTABLE ET_KAFKA_HOURLY_TOTAL USING STREAM (
  name: 'STREAM_CQ_CALCULATE_HOURLY_TOTAL' )
QUERY (
  keytomap: 'topic',
  persistPolicy: 'true' )
OF STREAM_CQ_CALCULATE_HOURLY_TOTAL_Type;

END APPLICATION APP_KAFKASOURCE_AGG;

CREATE APPLICATION @APPNAME@;

CREATE TYPE @APPNAME@type1 (
 companyName java.lang.String,
 merchantId java.lang.String,
 city java.lang.String);

CREATE STREAM @APPNAME@TypedStream OF @APPNAME@type1 PARTITION BY city;

CREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (
  wildcard: '',
  positionByEOF: false,
  directory: ''
  )
PARSE USING DSVParser (
header:'true'
)
OUTPUT TO @APPNAME@Stream;

CREATE OR REPLACE CQ @APPNAME@CQ
INSERT INTO @APPNAME@TypedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantID,
TO_STRING(data[10]) as city
FROM @APPNAME@Stream;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt1 USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@TypedStream;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt2 USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@TypedStream;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt3 USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@TypedStream;

END APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

create stream @stream1@ of Global.WAEvent;

CREATE TYPE @type@(
  id String,
  name String  
);

CREATE STREAM @typeStream@ OF @type@;

CREATE CQ @CQName1@
INSERT INTO @typeStream@
SELECT TO_STRING(p.data[0]), 
       TO_STRING(p.data[1])
FROM @SRCINPUTSTREAM@ p;

create Target @targetsys1@ using SysOut(name:TypeOut) input from @typeStream@;

create cq @CQName2@ 
insert into @stream1@
select convertTypedeventToWAevent(c, 'admin.@type@')
from @typeStream@ c;

create stream @CQOUTPUTSTREAM@ of Global.WAEvent;

CREATE OR REPLACE CQ @CQName@ INSERT INTO @CQOUTPUTSTREAM@ select @FUNCTION@ from @stream1@  s ;

create Target @targetsys2@ using SysOut(name:EventOut) input from @CQOUTPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:1',
  Tables: '@TargetTableMapping@'
 ) 
INPUT FROM @CQOUTPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.GGTrailReader (
  Tables:'@TABLES@',
  CDDLCapture: false,
  TrailDirectory: '@TRAIL_FILE_DIR@',
  TrailFilePattern: '@WILDCARD@',
  Compression: false,
  SupportColumnCharset: false,
  CDDLAction: 'Process',
  FilterTransactionBoundaries: true,
  adapterName: 'GGTrailReader',
  TrailByteOrder: '@ENDIAN@' )
OUTPUT TO @STREAM@;

CREATE TARGET @SOURCE_NAME@_sysout USING Global.SysOut (
  name: '@SOURCE_NAME@_SysOut' )
INPUT FROM @STREAM@;

create Target @TARGET_NAME@ using ADLSGen2Writer(
          accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'%@metadata(TableName)%',
        filename:'table.csv',
        uploadpolicy:'filesize:10M'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@',
	members:'Table=@metadata(TableName),OpName=@metadata(OperationName)'
)
input from @STREAM@;

stop application MSSQLTransactionSupportFTBTrue;
undeploy application MSSQLTransactionSupportFTBTrue;
drop application MSSQLTransactionSupportFTBTrue cascade;

CREATE APPLICATION MSSQLTransactionSupportFTBTrue recovery 1 second interval;

Create Source ReadFromMSSQL1
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
FetchTransactionMetadata:'false',
FilterTransactionBoundaries: true,
Compression:'false',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportFTBTrueStream;


CREATE TARGET WriteToMSSQL1 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC@,@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportFTBTrueStream;

CREATE TARGET MSSqlReaderOutput1 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportFTBTrueStream; 


CREATE OR REPLACE TARGET MSSQLFileOut1 USING FileWriter  ( 
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  directory:'@FEATURE-DIR@/logs/',
  filename: 'TransactionSupportFTBTrue.txt'
 ) 
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 ) 
INPUT FROM MSSQLTransactionSupportFTBTrueStream;

END APPLICATION MSSQLTransactionSupportFTBTrue;
deploy application MSSQLTransactionSupportFTBTrue;
start application MSSQLTransactionSupportFTBTrue;

STOP APPLICATION oraddl;
UNDEPLOY APPLICATION oraddl;
DROP APPLICATION oraddl CASCADE;
CREATE APPLICATION oraddl recovery 5 second interval;
 
Create Source Ora Using OracleReader 
(
 Username:'@user-name@',
 Password:'@password@',
 ConnectionURL:'src_url',
 Tables:'QATEST.ORACLEDDL%',
 DictionaryMode:OfflineCatalog,
 DDLCaptureMode : 'All',
 FetchSize:1
) Output To LogminerStream;

Create Target tgt using DatabaseWriter 
(
 Username:'@username@',
 Password:'@password@',
 ConnectionURL:'TGT_URL',
 BatchPolicy:'EventCount:1,Interval:1',
 CommitPolicy:'EventCount:1,Interval:1',
 IgnorableExceptionCode: '1,2290,942',
 Tables :'QATEST.ORACLEDDL%,QATEST2.%'
) input from LogminerStream;

CREATE TARGET cdcDump USING LogWriter(
name:testOuput,
directory:'/Users/abinandan/product/IntegrationTests/target/test-classes/testNG/AllTargetWriters/OracleDDLDatabaseWriter/logs',
filename:'oraclecdc.log',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING JSONFormatter ()
INPUT FROM LogminerStream;
end application oraddl;
deploy application oraddl;
start application oraddl;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE FLOW @APPNAME@AgentFlow;

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.waevent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (
  ProviderName: '',
  UserName: '',
  transactionpolicy: '',
  Provider: '',
  Ctx: '',
  EnableTransaction: '',
  QueueName: '',
  Password: '' )
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@PersistStream@RANDOM@;
END FLOW @APPNAME@AgentFlow;

CREATE FLOW @APPNAME@serverFlow;
CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING JSONFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream@RANDOM@;
END FLOW @APPNAME@serverFlow;

END APPLICATION @APPNAME@;

STOP APPLICATION bq;
UNDEPLOY APPLICATION bq;
DROP APPLICATION bq CASCADE;
CREATE APPLICATION bq recovery 1 second interval;

Create Source s Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: 'dockerhost:1521:orcl',
 Tables:'qatest.src_multi_target01',
 FetchSize:1
) 
Output To ss;

CREATE TARGET t1 USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
	Tables:'qatest.src_multi_target01,qatest.tgt_multi_target01 columnmap(col1=ID,col2=NAME,OperationName=@METADATA(OperationName),TableName=@METADATA(TableName))',
	datalocation: 'US',
	nullmarker: 'aaaa',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:10, Interval:30'	
) INPUT FROM ss;

CREATE TARGET t2 USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
	Tables:'qatest.src_multi_target01,qatest.tgt_multi_target02',
	datalocation: 'US',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:10, Interval:30'	
) INPUT FROM ss;

CREATE TARGET t3 USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
	Tables:'qatest.src_multi_target01,qatest.tgt_multi_target03',
	datalocation: 'US',
	nullmarker: 'NOTNULL',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:10, Interval:30'	
) INPUT FROM ss;

CREATE or replace TARGET t4 USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
	Tables:'qatest.src_multi_target01,qatest.tgt_multi_target04 columnmap(OperationName=@METADATA(OperationName))',
	datalocation: 'US',
	nullmarker: 'NULL',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:10, Interval:30'	
) INPUT FROM ss;

END APPLICATION bq;
DEPLOY APPLICATION bq;
START APPLICATION bq;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING MySqlReader
(
  Compression: false,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'mysql://localhost:3306',
  DatabaseName: 'waction',
  Tables: 'waction.test01',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root'
)
OUTPUT TO @APPNAME@_SS;


CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'waction.test01,@DATASET@.% KEYCOLUMNS(id,name) COLUMNMAP(id=id,col1=col11,col2=col12)',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[0]) != '2';

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

stop application GGTrailReaderApp;
undeploy application GGTrailReaderApp;
drop application GGTrailReaderApp cascade;

create application GGTrailReaderApp recovery 5 second interval;

create source GGTrailSource using GGTrailReader (
tRaildIrectory:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario1',
tRAilfilepattern:'n1*',
positionByEOF:false,
FilterTransactionBoundaries: true,
DefinitionFile:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario1/Scn1_beforeddl.def',
captureCDdl: true,
CDDLAction:'Process',
--CDDLAction:'Ignore',
TrailByTeOrder:'LittleEndian',
recoveryInterval: 5
)
OUTPUT TO GGTrailStream;

create Target t2 using SysOut(name:Foo2) input from GGTrailStream;

CREATE TARGET WriteCDCOracle1 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost/orcl',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1,Interval:1',
CommitPolicy:'Eventcount:1,Interval:1',
Checkpointtable:'RGRN_CHKPOINT',
Tables:'QATEST.GGDDL1,QATEST.GGDDL1_TGT'
) INPUT FROM GGTrailStream1;


end application GGTrailReaderApp;

deploy application GGTrailReaderApp;
start application GGTrailReaderApp;

stop application HDFSDSV;
undeploy application HDFSDSV;
drop application HDFSDSV cascade;

create application HDFSDSV;
create source HDFSCSVSource using HDFSReader (
	hadoopurl:'@HDFSREADERHADOOPURL@/home/hadoop/input/',
    WildCard:'posdata.csv',
	charset:'UTF-8',
    positionByEOF:false
)
parse using @HDFSCSVSOURCEFORMATTERTYPE@ (
	@HDFSCSVSOURCEFORMATTERMEMBERS@
)
OUTPUT TO HDFSCsvStream;
create Target HDFSDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/posdata') input from HDFSCsvStream;
end application HDFSDSV;

alter application oraddl;
Create or replace Source Ora Using OracleReader 
(
 Username:'@user-name@',
 Password:'@password@',
 ConnectionURL:'src_url',
 Tables:'QATEST.ORACLEDDL%',
 DictionaryMode:OfflineCatalog,
 DDLCaptureMode : 'All',
 FetchSize:2
) Output To LogminerStream;
ALTER APPLICATION oraddl RECOMPILE;
deploy application oraddl;
START application oraddl;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;
DROP EXCEPTIONSTORE @APP_NAME@_EXCEPTIONSTORE;

CREATE APPLICATION @APP_NAME@ @APP_PROPERTY@ USE EXCEPTIONSTORE;

CREATE SOURCE @APP_NAME@_Source Using @SOURCE_ADAPTER@ (

) OUTPUT TO @APP_NAME@DataStream;


CREATE TARGET @TARGETNAME@ USING @TARGET_ADAPTER@ (

) INPUT FROM @APP_NAME@DataStream;

END APPLICATION @APP_NAME@;

DEPLOY APPLICATION @APP_NAME@ IN DEFAULT;
START APPLICATION @APP_NAME@;

Stop application DEV20814.ValidateFile;
Undeploy application DEV20814.ValidateFile;
Drop application DEV20814.ValidateFile cascade;

CREATE APPLICATION ValidateFile;

CREATE STREAM AlertFileStream OF Global.AlertEvent;

CREATE SUBSCRIPTION FileAlert USING WebAlertAdapter( ) INPUT FROM AlertFileStream;

CREATE STREAM jsonFileStream OF Global.JsonNodeEvent;

CREATE  SOURCE readFromFile USING FileReader  (
  blocksize: 64,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  directory:'@TEST-DATA-PATH@',
  skipbom: true,
  wildcard: 'Postgres*'
 )
 PARSE USING JSONParser  (
 )
OUTPUT TO jsonFileStream ;

CREATE OPEN PROCESSOR alertFileOP USING AlertGenerator
(
   messagePrefix:'File: ',
   severity:'info',
   flag:'notify'
)
INSERT INTO AlertFileStream
FROM jsonFileStream;

END APPLICATION ValidateFile;

use PosTester;
DROP Source CsvDataSource;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @APPNAME@_src Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
   _h_ReturnDateTimeAs:'ZonedDateTime'
) Output To @APPNAME@_stream;

create Target @APPNAME@_tgt using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_stream;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ recovery 1 second interval;

CREATE OR REPLACE SOURCE @SOURCENAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)

OUTPUT TO @STREAM@ ;

CREATE TARGET @TARGETNAME@ using DatabaseWriter
(
    ConnectionURL: '@TARGETURL',
    username: '@TARGETUSERNAME@',
    Password: '@TARGETPASSWORD@',
    Tables: '@TARGETTABLE@',
    BatchPolicy:'EventCount:1,Interval:1',
    CommitPolicy:'EventCount:1,Interval:1'
)
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE CQ @CQName@ INSERT INTO @CQOUTPUTSTREAM@ select @FUNCTION@ from @SRCINPUTSTREAM@ s ;

create Target @targetsys@ using SysOut(name:Foo2) input from @CQOUTPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:1',
  Tables: '@TargetTableMapping@'
 ) 
INPUT FROM @CQOUTPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP APPLICATION ORACLETOBIGQUERY;
UNDEPLOY APPLICATION ORACLETOBIGQUERY;
DROP APPLICATION ORACLETOBIGQUERY CASCADE;

--create application 
CREATE APPLICATION OracleToBigquery RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE OracleSource USING OracleReader (
 ConnectionURL: '192.168.123.30:1521:ORCL',
 Tables: 'QATEST.E1PLOADTEST',
 Username: 'qatest',
 Password: 'qatest',
 FetchSize:100,
 OnlineCatalog:true,
 QueueSize:2000,
 CommittedTransactions:true,
 Compression:false
) OUTPUT TO CDCStream;

CREATE OR REPLACE TARGET bqtables using BigqueryWriter(
 BQServiceAccountConfigurationPath:"/Users/ravipathak/Downloads/bqtest-e287bcb47998.json",
 projectId:"bqtest-158706",
 Tables: "QATEST.E1PLOADTEST,issues.DEV11070",
 BatchPolicy: "eventCount:100,Interval:1")
INPUT FROM CDCStream;

CREATE OR REPLACE TARGET T1 using SysOut(
name : "some text"
)
INPUT FROM CDCStream;

END APPLICATION OracleToBigquery;

DEPLOY APPLICATION OracleToBigquery;
START APPLICATION OracleToBigquery;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 1 second interval;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@'
  --recordSelector: '@PROP8@'
)
OUTPUT TO @APPNAME@Stream;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;

/*
create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:1,Interval:5',
  CommitPolicy: 'EventCount:1,Interval:5',
  Tables: 'QATEST.@table@'
)
input from @APPNAME@Stream;*/
end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@ConnectionURL@',
 Tables:'@SourceTables@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;


CREATE CQ @cqName@ INSERT INTO admin.oraclereader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'), 'OpTime',META(x, 'TimeStamp'))) FROM @SRCINPUTSTREAM@ x; ;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@ConnectionURL@',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@TableMapping@'
) INPUT FROM oraclereader_cq_out;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP AES;
UNDEPLOY APPLICATION AES;
DROP APPLICATION AES CASCADE;

CREATE APPLICATION AES;


CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity double,
  size long,
  currentDate DateTime);

CREATE source implicitSOurce USING FileReader (
      directory:'@TEST-DATA-PATH@',
      columndelimiter: ',',
      wildcard:'ISdata.csv',
      blocksize: 10240,
      positionByEOF:false
)
PARSE USING DSVParser (
      header:False,
      trimquote:false
) OUTPUT TO CsvStream; 

CREATE TYPE wsType(
  quantity double KEY,
  currentDate DateTime
  );


CREATE STREAM newStream OF Atm;

CREATE CQ newCQ
INSERT INTO newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]), TO_DATE(data[5]) FROM
CsvStream;


CREATE WINDOW win1
OVER newStream
keep within 3 minute;

CREATE STREAM newStream2 of wsType;



CREATE WACTIONSTORE WS1 CONTEXT OF wsType
EVENT TYPES (wsType );


Create cq newCQ2
insert into ws1 (quantity,currentDate)
select quantity, currentDate from newStream;


END APPLICATION AES;
deploy APPLICATION AES;

--
-- Crash Recovery Test 1 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP APPLICATION N2S2CR1Tester.N2S2CRTest1;
UNDEPLOY APPLICATION N2S2CR1Tester.N2S2CRTest1;
DROP APPLICATION N2S2CR1Tester.N2S2CRTest1 CASCADE;
CREATE APPLICATION N2S2CRTest1 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest1;

CREATE SOURCE CsvSourceN2S2CRTest1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN2S2CRTest1;

CREATE FLOW DataProcessingN2S2CRTest1;

CREATE TYPE WactionTypeN2S2CRTest1 (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE WactionsN2S2CRTest1 CONTEXT OF WactionTypeN2S2CRTest1
EVENT TYPES ( WactionTypeN2S2CRTest1 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN2S2CRTest1
INSERT INTO WactionsN2S2CRTest1
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END FLOW DataProcessingN2S2CRTest1;

END APPLICATION N2S2CRTest1;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using PostgreSQLReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using AzureblobWriter(
    accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:7'
)
format using DSVFormatter (
)
input from @STREAM@;

end flow @APPNAME@_serverflow;

end application @APPNAME@;

stop FileReaderToKuduWriter;
undeploy application FileReaderToKuduWriter;
drop application FileReaderToKuduWriter cascade;

CREATE APPLICATION FileReaderToKuduWriter recovery 5 second interval ;;

CREATE OR REPLACE SOURCE CSVPoller USING FileReader (
        directory:'/Users/Striim/',
        WildCard:'typetest.csv',
        positionByEOF:false
)
parse using DSVParser (
        header:'yes'
)
OUTPUT TO CsvStream;

CREATE OR REPLACE TYPE CSVStream_Type  ( ID1 String KEY,
ID2 String
);

CREATE OR REPLACE STREAM CSVTypeStream OF CSVStream_Type;

CREATE OR REPLACE CQ CQ1
INSERT INTO CSVTypeStream
SELECT TO_STRING(data[0]),TO_STRING(data[1])
FROM CsvStream;

CREATE TARGET WriteintoKudu using KuduWriter (
KuduClientConfig:'master.addresses->192.168.56.101:7051;socketreadtimeout->10000;operationtimeout->30000',
Tables: 'INTEGRATIONTEST',
BatchPolicy: 'EventCount:1,Interval:10') INPUT FROM CSVTypeStream;

END APPLICATION FileReaderToKuduWriter;
deploy application FileReaderToKuduWriter;
start FileReaderToKuduWriter;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ Recovery 5 second interval;

create stream @APPNAME@_UserdataStream of Global.WAEvent;

create type @APPNAME@_Order_type(
id int,
order_id int,
zipcode int,
category String,
tablename string
);

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src1 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.order_%'
)
OUTPUT TO @APPNAME@_OrdersStream;

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src2 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_2',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.second_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream2;

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src3 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_3',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.third_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream3;

CREATE OR REPLACE SOURCE @APPNAME@Postgres_Src4 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_4',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.fourth_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream4;


Create CQ @APPNAME@_CQUser
insert into @APPNAME@_UserdataStream
select 
putuserdata (data,'Fileowner','FIRST_ORDER') from @APPNAME@_OrdersStream data;


Create CQ @APPNAME@_CQUser2
insert into @APPNAME@_UserdataStream
select 
putuserdata (data2,'Fileowner','SECOND_ORDER') from @APPNAME@_OrdersStream2 data2;


Create CQ @APPNAME@_CQUser3
insert into @APPNAME@_UserdataStream
select 
putuserdata (data3,'Fileowner','THIRD_ORDER') from @APPNAME@_OrdersStream3 data3;


Create CQ @APPNAME@_CQUser4
insert into @APPNAME@_UserdataStream
select 
putuserdata (data4,'Fileowner','FOURTH_ORDER') from @APPNAME@_OrdersStream4 data4;

create stream @APPNAME@_OrderTypedStream1 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream2 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream3 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream4 of @APPNAME@_Order_type;

CREATE CQ @APPNAME@_fin_cq
INSERT INTO @APPNAME@_OrderTypedStream1
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FIRST_ORDER';

CREATE CQ @APPNAME@_fin_cq2
INSERT INTO @APPNAME@_OrderTypedStream2
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'SECOND_ORDER';

CREATE CQ @APPNAME@_fin_cq3
INSERT INTO @APPNAME@_OrderTypedStream3
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'THIRD_ORDER';

CREATE CQ @APPNAME@_fin_cq4
INSERT INTO @APPNAME@_OrderTypedStream4
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FOURTH_ORDER';


create Target @APPNAME@_ADLSGen2_tgt1 using ADLSGen2Writer(
        filename:'event_data.csv',
        directory:'',
       	accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        uploadpolicy:'eventcount:8,interval:20s'
)
format using DSVFormatter (
    header:'true'
)
input from @APPNAME@_OrderTypedStream1; 

create Target @APPNAME@_ADLSGen2_tgt2 using ADLSGen2Writer(
        filename:'event_data.xml',
        directory:'',
		accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
   		uploadpolicy:'eventcount:8,interval:20s'
)
format using XMLFormatter (
  elementtuple: 'Order_id:id:order_id:zipcode:category:text=tablename',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from @APPNAME@_OrderTypedStream2; 

create Target @APPNAME@_ADLSGen2_tgt3 using ADLSGen2Writer(
        filename:'event_data.avro',
        directory:'',
		accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
  		uploadpolicy:'eventcount:8,interval:20s'
)
format using AvroFormatter (
  formatAs: 'Default',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA-FILE@'
)
input from @APPNAME@_OrderTypedStream3; 


create Target @APPNAME@_ADLSGen2_tgt4 using ADLSGen2Writer(
        filename:'event_data.json',
        directory:'',
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
  		uploadpolicy:'eventcount:8,interval:20s'
)
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@_OrderTypedStream4;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

-----------------------------------

stop application TargetServerApp3;
undeploy application TargetServerApp3;
drop application TargetServerApp3 cascade;


-- another app consuming from app running in agent

CREATE APPLICATION TargetServerApp3;
create flow flow4;

CREATE TARGET T4 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp3_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')
 FORMAT USING DSVFormatter ()
 INPUT FROM CsvStream;
end flow flow4;

END APPLICATION TargetServerApp3;
deploy application TargetServerApp3 with flow4 in default;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

create Target FileTarget using FileWriter(
    rolloverpolicy:'@UPLOAD-SIZE@',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using XMLFormatter (
charset:'@charset@',
rootelement:'@mem@'
)
input from CsvStream;

end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

--
-- Canon Test W32
-- Nicholas Keene, WebAction, Inc.
--
-- Test having one source leading to two data paths with unpartitioned
-- jumping count windows leading to one waction store.
--
-- S -> JWc5u -> CQ5 -> WS
-- S -> JWc2u -> CQ2 -> WS
--


UNDEPLOY APPLICATION NameW32.W32;
DROP APPLICATION NameW32.W32 CASCADE;
CREATE APPLICATION W32 RECOVERY 5 SECOND INTERVAL;




CREATE FLOW DataAcquisitionW32;


CREATE SOURCE CsvSourceW32 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW32;


END FLOW DataAcquisitionW32;




CREATE FLOW DataProcessingW32;

CREATE TYPE DataTypeW32 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW32 OF DataTypeW32;

CREATE CQ CSVStreamW32_to_DataStreamW32
INSERT INTO DataStreamW32
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW32;

CREATE JUMPING WINDOW JWc5uW32
OVER DataStreamW32
KEEP 5 ROWS;

CREATE JUMPING WINDOW JWc2uW32
OVER DataStreamW32
KEEP 2 ROWS;

CREATE WACTIONSTORE WactionStoreW32 CONTEXT OF DataTypeW32
EVENT TYPES ( DataTypeW32 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWc5uW32_to_WactionStoreW32
INSERT INTO WactionStoreW32
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc5uW32;

CREATE CQ JWc2uW32_to_WactionStoreW32
INSERT INTO WactionStoreW32
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc2uW32;

END FLOW DataProcessingW32;



END APPLICATION W32;

STOP APPLICATION EnvvarTester.envVar;
UNDEPLOY APPLICATION EnvvarTester.envVar;
DROP APPLICATION EnvvarTester.envVar CASCADE;

CREATE APPLICATION envVar;


CREATE SOURCE AccessLogSource USING FileReader (
directory:'@TEST-DATA-PATH@/envVar',
wildcard:'$FILENAME',
blocksize: $BLOCKSIZE,
positionByEOF:false
)
PARSE USING DSVParser (
columndelimiter:' ',
ignoreemptycolumn:'Yes',
quoteset:'[]~"',
separator:'~'
)
OUTPUT TO RawAccessStream;


END APPLICATION envVar;
DEPLOY APPLICATION envVar on any in default;
START envVar;

CREATE APPLICATION  @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE  @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'JsonNodeEvent.json',
positionByEOF:false
)
PARSE USING Global.JSONParser (
 )  OUTPUT TO  @AppName@_rawstream;

CREATE CQ @BuiltinFunc@CQ
INSERT INTO  @BuiltinFunc@_Stream
SELECT @BuiltinFunc@(x, 'Sno', data.get("_id"), 'Name', data.get("firstname"))
FROM @AppName@_rawstream x;

CREATE OR REPLACE CQ cq1
INSERT INTO RemoveUserData_Stream
SELECT
removeUserData(s1, 'Sno')
FROM @BuiltinFunc@_Stream s1;

CREATE OR REPLACE TARGET  @AppName@_FileTarget USING Global.FileWriter (
  flushpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
 directory: '@logs@',
  filename: '@BuiltinFunc@_JsonEventRemoveData',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  (
  handler: 'com.webaction.proc.JSONFormatter',
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  formatterName: 'JSONFormatter',
  jsonobjectdelimiter: '\n' )
INPUT FROM RemoveUserData_Stream;

End application  @AppName@;
Deploy application  @AppName@;
Start application  @AppName@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@;


CREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@;

DROP APPLICATION ns1.OPExample cascade;
DROP NAMESPACE ns1 cascade;
CREATE OR REPLACE NAMESPACE ns1;
USE ns1;
CREATE APPLICATION OPExample;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'PosDataPreview.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
)
OUTPUT TO CsvStream;
 
CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);

CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) 
QUERY (keytomap:'merchantId') 
OF MerchantHourlyAve;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
  TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
  DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
  TO_DOUBLE(data[7]) as amount,
  TO_INT(data[9]) as zip
FROM CsvStream;
 
CREATE CQ cq2
INSERT INTO SendToOPStream
SELECT makeList(dateTime) as dateTime,
  makeList(zip) as zip
FROM PosDataStream;
 
CREATE TYPE ReturnFromOPStream_Type ( time DateTime , val Integer );
CREATE STREAM ReturnFromOPStream OF ReturnFromOPStream_Type;

CREATE TARGET OPExampleTarget 
USING FileWriter (filename: 'OPExampleOut') 
FORMAT USING JSONFormatter() 
INPUT FROM ReturnFromOPStream;
 
END APPLICATION OPExample;

--
-- Recovery Test 24 with two sources, two sliding time-count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5a9W  -> CQ1 -> WS
-- S2 -> Sc6a11W -> CQ2 -> WS
--

STOP KStreamRecov24Tester.KStreamRecovTest24;
UNDEPLOY APPLICATION KStreamRecov24Tester.KStreamRecovTest24;
DROP APPLICATION KStreamRecov24Tester.KStreamRecovTest24 CASCADE;
DROP USER KStreamRecov24Tester;
DROP NAMESPACE KStreamRecov24Tester CASCADE;
CREATE USER KStreamRecov24Tester IDENTIFIED BY KStreamRecov24Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov24Tester;
CONNECT KStreamRecov24Tester KStreamRecov24Tester;

CREATE APPLICATION KStreamRecovTest24 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION KStreamRecovTest24;

--
-- Canon Master Test, Unpartitioned
-- Nicholas Keene, WebAction, Inc.
--


CREATE APPLICATION MasterUnpartitioned
RECOVERY 5 SECOND INTERVAL
;

CREATE OR REPLACE TYPE N100k_JUc100_NoPersist_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100k_JUc100_NoPersist_WS  CONTEXT OF N100k_JUc100_NoPersist_WS_Type
 PERSIST NONE USING ( 
 ) ;

CREATE OR REPLACE TYPE R10_SPc10_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE SOURCE N50k USING NumberSource ( 
  lowValue: '1',
  highValue: '500000',
  delayMillis: '0',
  delayNanos: '0',
  repeat: 'false'
 ) 
OUTPUT TO NumberStream2_Out;

CREATE OR REPLACE SOURCE N100k USING NumberSource ( 
  lowValue: '1',
  highValue: '1000000',
  delayMillis: '0',
  delayNanos: '0',
  repeat: 'false'
 ) 
OUTPUT TO N100k_Out;

CREATE OR REPLACE TYPE ComplexNumberType  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY , 
valueAsDateTime org.joda.time.DateTime , 
valueMod10 java.lang.Long , 
valueMod7 java.lang.Long , 
valueMod13 java.lang.Long  
 );

CREATE OR REPLACE TYPE R10_JPc5_x7_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE TYPE N100k_JUc100_SUc10_WS  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE TYPE N100K_N50K_TwoInputs_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100K_N50K_TwoInputs  CONTEXT OF N100K_N50K_TwoInputs_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE N100K_TwoInputs_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100K_TwoInputs_WS  CONTEXT OF N100K_TwoInputs_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE N100k_JUc100_M_JUc200_JUc15_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100k_JUc100_M_JUc200_JUc15_WS  CONTEXT OF N100k_JUc100_M_JUc200_JUc15_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE N100k_JUc100_Dupe_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100k_JUc100_Dupe  CONTEXT OF N100k_JUc100_Dupe_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE N100k_JUc100_SUc10_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY , 
count java.lang.Long  
 );

CREATE OR REPLACE WACTIONSTORE N100k_JUc100_SUc10_WS  CONTEXT OF N100k_JUc100_SUc10_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE N100k_JUc100_SUc10na_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE TYPE ComplexNumberType2  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY , 
valueAsTimeStamp org.joda.time.DateTime , 
valueMod10 java.lang.Long , 
valueMod7 java.lang.Long , 
valueMod13 java.lang.Long  
 );

CREATE OR REPLACE TYPE Type1  ( f java.lang.String KEY  
 );

CREATE OR REPLACE TYPE N50k_JUc100_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N50k_JUc100_WS  CONTEXT OF N50k_JUc100_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE Type2  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY , 
valueAsDateTime org.joda.time.DateTime , 
valueMod10 java.lang.Long , 
valueMod3 java.lang.Long , 
valueMod7 java.lang.Long  
 );

CREATE OR REPLACE TYPE N100k_JUc100_JUc10_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100k_JUc100_JUc10_WS  CONTEXT OF N100k_JUc100_JUc10_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE OneNumberType  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE STREAM N100k_Stream OF OneNumberType;

CREATE OR REPLACE CQ N100k_Convert 
INSERT INTO N100k_Stream
SELECT TO_DATE(data[0]), data[1]
 	 
FROM N100k_Out;

CREATE OR REPLACE JUMPING WINDOW N100k_JUc100 OVER N100k_Stream KEEP 100 ROWS;

CREATE OR REPLACE CQ N100k_JUc100_Pull_Dupe 
INSERT INTO N100k_JUc100_Dupe
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100 w;

CREATE OR REPLACE CQ N100k_JUc100_Pull3 
INSERT INTO N100k_JUc100_NoPersist_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100 w;

CREATE OR REPLACE CQ N100k_Merge_JUc100_Pull 
INSERT INTO N100K_TwoInputs_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100 w;

CREATE OR REPLACE JUMPING WINDOW N100k_JUc200 OVER N100k_Stream KEEP 200 ROWS;

CREATE OR REPLACE CQ N100K_JUc200_TwoInputs_Pull 
INSERT INTO N100K_N50K_TwoInputs
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc200 w;

CREATE OR REPLACE STREAM N50k_Stream OF OneNumberType;

CREATE OR REPLACE JUMPING WINDOW N50k_JUc100 OVER N50k_Stream KEEP 100 ROWS;

CREATE OR REPLACE CQ N50K_JUc100_TwoInputs_Pull 
INSERT INTO N100K_N50K_TwoInputs
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N50k_JUc100 w;

CREATE OR REPLACE CQ N50k_JUc100_Pull 
INSERT INTO N50k_JUc100_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N50k_JUc100 w;

CREATE OR REPLACE CQ N50k_Convert 
INSERT INTO N50k_Stream
SELECT TO_DATE(data[0]), data[1]
 	 
FROM NumberStream2_Out;

CREATE OR REPLACE STREAM N100k_JUc100_Out OF OneNumberType;

CREATE OR REPLACE JUMPING WINDOW N100k_JUc100_JUc10 OVER N100k_JUc100_Out KEEP 10 ROWS;

CREATE OR REPLACE CQ N100k_JUc100_JUc10_Pull 
INSERT INTO N100k_JUc100_JUc10_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100_JUc10 w;

CREATE OR REPLACE CQ N100K_TwoInputs_CQ2 
INSERT INTO N100K_TwoInputs_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100_JUc10 w;

CREATE OR REPLACE CQ N100k_JUc100_Pull2 
INSERT INTO N100k_JUc100_Out
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100 w;

CREATE OR REPLACE WINDOW N100k_JUc100_SUc10 OVER N100k_JUc100_Out KEEP 10 ROWS;

CREATE OR REPLACE CQ N100k_JUc100_SUc10_Pull 
INSERT INTO N100k_JUc100_SUc10_WS
SELECT FIRST(w.timestamp), FIRST(w.value), COUNT(w)
 	 
FROM N100k_JUc100_SUc10 w;

CREATE OR REPLACE STREAM N100k_JUc100_M_JUc200 OF OneNumberType;

CREATE OR REPLACE JUMPING WINDOW N100k_JUc100_M_JUc200_JUc15 OVER N100k_JUc100_M_JUc200 KEEP 15 ROWS;

CREATE OR REPLACE CQ N100k_JUc100_M_JUc200_JUc15_Pull 
INSERT INTO N100k_JUc100_M_JUc200_JUc15_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100_M_JUc200_JUc15 w;

CREATE OR REPLACE CQ N100k_Merge_JUc200 
INSERT INTO N100k_JUc100_M_JUc200
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc200 w;

CREATE OR REPLACE CQ N100k_Merge_JUc100 
INSERT INTO N100k_JUc100_M_JUc200
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100 w;

CREATE OR REPLACE STREAM N50K_x100_SUc5_Stream OF OneNumberType;

CREATE OR REPLACE WINDOW N50K_x100_SUc5_SUc10 OVER N50K_x100_SUc5_Stream KEEP 10 ROWS;

CREATE OR REPLACE STREAM N50K_x100_Stream OF OneNumberType;

CREATE OR REPLACE WINDOW N50K_x100_SUc5 OVER N50K_x100_Stream KEEP 5 ROWS;

CREATE OR REPLACE CQ N50K_x100_SUc5_Stream_Pull2 
INSERT INTO N50K_x100_SUc5_Stream
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N50K_x100_SUc5 w;

CREATE OR REPLACE CQ N50K_x100_Pop 
INSERT INTO N50K_x100_Stream
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N50k_Stream w
WHERE (w.value % 100) == 0;

CREATE OR REPLACE TYPE R10_JPc10_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE TYPE N100k_JUc100_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100k_JUc100_WS  CONTEXT OF N100k_JUc100_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE CQ N100k_JUc100_Pull 
INSERT INTO N100k_JUc100_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100 w;

CREATE OR REPLACE TYPE ComplexNumberType3  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY , 
valueAsDateTime org.joda.time.DateTime , 
valueMod10 java.lang.Long , 
valueMod7 java.lang.Long , 
valueMod3 java.lang.Long  
 );

CREATE OR REPLACE TYPE N50K_x100_SUc5_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N50K_x100_SUc5_WS  CONTEXT OF N50K_x100_SUc5_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE CQ N50K_x100_SUc5_Pull 
INSERT INTO N50K_x100_SUc5_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N50K_x100_SUc5 w;





END APPLICATION MasterUnpartitioned;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ Recovery 5 second interval;
--create application @APPNAME@;

--CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaProps(zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11');
--CREATE STREAM @APPNAME@kperststream of Global.WAEvent PERSIST USING @APPNAME@KafkaProps;

create type @APPNAME@employee
(
id integer,
name String,
company String
);
CREATE STREAM @APPNAME@Hana_TypedStream of @APPNAME@employee;

CREATE OR REPLACE SOURCE @APPNAME@OnPrem_Oracle USING OracleReader  (
  Compression: false,
  StartTimestamp: 'null',
  SupportPDB: false,
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  FetchSize: 1,
  DDLCaptureMode: 'All',
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'localhost:1521:xe',
 Tables: 'QATEST.EMP%',
-- Tables: 'QATEST.OracToCql_alldatatypes',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  TransactionBufferType: 'Memory',
  DictionaryMode: 'OnlineCatalog',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '1MB'
 )
OUTPUT TO @APPNAME@kperststream ;

create stream @APPNAME@UserdataStream1 of Global.WAEvent;

Create CQ @APPNAME@CQUser
insert into @APPNAME@UserdataStream1
select putuserdata (data1,'OperationName',META(data1,'OperationName').toString()) from @APPNAME@kperststream data1;

Create CQ @APPNAME@CQUser_typed
insert into @APPNAME@Hana_TypedStream
select 
to_int(data[0]),
data[1],
data[2]
from @APPNAME@UserdataStream1 u 
where USERDATA(u,'OperationName').toString()=='INSERT' and meta(u,'TableName').toString()="QATEST.EMP3";


CREATE OR REPLACE TARGET @APPNAME@DBTarget USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'SYSTEM',
  Password_encrypted: 'false',
  --ParallelThreads: '4',
  BatchPolicy: 'EventCount:1000,Interval:60',
  CommitPolicy: 'EventCount:100,Interval:60',
  --ExcludedTables: 'QATEST.EMP2;QATEST.EMP3',
  ConnectionURL: 'jdbc:sap://10.77.21.116:39013/?databaseName=striim&currentSchema=QA',
  Tables: 'QA.EMP3',
  adapterName: 'DatabaseWriter',
  Password: 'XgsL2qpACEIHrXXh4SueCg=='
 ) 
INPUT FROM @APPNAME@Hana_TypedStream;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;

drop propertyset OrcToOrcPlatfm_App_KafkaPropset;
drop stream  OrcToOrcPlatfm_Stream CASCADE;


--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;

CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNEC4TION_URL@',
  Tables: '$table1',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'QATEST.ORCTOORCPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '$table2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@4',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'QATEST.ORCTOORCPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;

END APPLICATION @APPNAME@2;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING S3Reader (
    bucketname:'',
    objectnameprefix:'',
     )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: 'ParquetFileSchema'
)
INPUT FROM @appname@CqOut;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using ParquetFormatter (
schemaFileName: 'ParquetS3Schema'
)
input from @appname@CqOut;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using ParquetFormatter (
schemaFileName: 'ParquetAzureSchema'
)
INPUT FROM @appname@CqOut;

CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using ParquetFormatter (
schemaFileName: 'ParquetGCSSchema'
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop ROLLUPMON_CDC;
undeploy application ROLLUPMON_CDC;
alter application ROLLUPMON_CDC;
CREATE or replace FLOW ROLLUPMON_CDC_flow;
Create or replace Source ROLLUPMON_CDC_Oraclesrc Using oraclereader(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',
 Tables:'QATEST.ROLLUPMON_TABLE1;QATEST.ROLLUPMON_TABLE2;QATEST.ROLLUPMON_TABLE3;QATEST.ROLLUPMON_TABLE4;QATEST.ROLLUPMON_TABLE5',
 Fetchsize:1000,
 connectionRetryPolicy:'maxRetries=4',
 TransactionBufferSpilloverSize:'200MB',
 _h_fetchexactrowcount: 'true'
)
Output To ROLLUPMON_CDC_OrcStrm;
END FLOW ROLLUPMON_CDC_flow;
alter application ROLLUPMON_CDC recompile;
DEPLOY APPLICATION ROLLUPMON_CDC;
start application ROLLUPMON_CDC;

--
-- Kafka Stream Recovery Test 1 New with internal test data generation
-- Bert Hashemi and Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> KS -> WS

STOP KStreamRecovTester.KStreamRecovTest;
UNDEPLOY APPLICATION KStreamRecovTester.KStreamRecovTest;
DROP APPLICATION KStreamRecovTester.KStreamRecovTest CASCADE;
DROP USER KStreamRecovTester;
DROP NAMESPACE KStreamRecovTester CASCADE;
CREATE USER KStreamRecovTester IDENTIFIED BY KStreamRecovTester;
-- GRANT 'Global:create,drop:deploymentgroup:*' TO USER KStreamRecov1Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecovTester;
CONNECT KStreamRecovTester KStreamRecovTester;

CREATE APPLICATION KStreamRecovTest RECOVERY 5 SECOND INTERVAL;

CREATE or REPLACE TYPE KafkaType(
  value java.lang.Long KEY
);

CREATE SOURCE KafkaSource USING NumberSource (
  lowValue: '1',
  highValue: '1003',
  delayMillis: '10',
  delayNanos: '0',
  repeat: 'false'
 )
OUTPUT TO NumberSourceOut;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaStream OF KafkaType persist using KafkaPropset;

CREATE OR REPLACE CQ KafkaStreamPopulate
INSERT INTO KafkaStream
SELECT data[1]
FROM NumberSourceOut;

CREATE WACTIONSTORE Wactions CONTEXT of KafkaType
@PERSIST-TYPE@

CREATE CQ WactionsPopulate
INSERT INTO Wactions
SELECT * FROM KafkaStream;

END APPLICATION KStreamRecovTest;

STOP PartiallyJumping1Tester.PartiallyJumping1;
UNDEPLOY APPLICATION PartiallyJumping1Tester.PartiallyJumping1;
DROP APPLICATION PartiallyJumping1Tester.PartiallyJumping1 CASCADE;
CREATE APPLICATION PartiallyJumping1;

create source CsvSource1 using FileReader
(
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'WindowsTest.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
)
 parse using DSVParser
(
	header:'yes',
	columndelimiter:','
)
OUTPUT TO CsvStream1;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);
CREATE TYPE CsvData1 (
  zip double
);

CREATE TYPE WactionData1 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData2 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData3 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData5 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData6 (
  zip double
);
CREATE TYPE WactionData7 (
  zip double
);

CREATE STREAM DataStream1 OF CsvData;

CREATE STREAM DataStream2 OF CsvData;

CREATE STREAM DataStream3 OF CsvData
PARTITION BY city;

CREATE STREAM DataStream4 OF CsvData;

CREATE STREAM DataStream5 OF CsvData
PARTITION BY city;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData3
INSERT INTO DataStream3
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData4
INSERT INTO DataStream4
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData5
INSERT INTO DataStream5
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData6
INSERT INTO DataStream6
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

CREATE CQ CsvToData7
INSERT INTO DataStream7
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

-- Count based partially jumping window
CREATE WINDOW DataStreamCount
OVER DataStream1 KEEP 5 ROWS
SLIDE 2
PARTITION BY companyName;

-- Time based partially jumping window
CREATE WINDOW DataStreamTime OVER DataStream2 KEEP
within 240 second
SLIDE 15 second;

-- Attribute based partially jumping window
CREATE WINDOW DataStreamAtrribute
OVER DataStream3 KEEP
RANGE 30 SECOND
ON dateTime
SLIDE 20 second;

-- Count + time based partially jumping window
CREATE WINDOW DataStreamCountTime
OVER DataStream4 KEEP
10 rows
within 150 second
SLIDE 10;

-- Attribute + time based partially jumping window
CREATE WINDOW DataStreamAttributeTime
OVER DataStream5 KEEP
range 50 second
ON dateTime
within 25 second
SLIDE 2 SECOND
PARTITION BY city;

CREATE WACTIONSTORE Wactions1 CONTEXT OF WactionData1
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions2 CONTEXT OF WactionData2
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions3 CONTEXT OF WactionData3
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions4 CONTEXT OF WactionData4
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions5 CONTEXT OF WactionData5
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions6 CONTEXT OF WactionData6
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions7 CONTEXT OF WactionData7
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE CQ Data1ToWaction
INSERT INTO Wactions1
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamCount p
group by companyName;

CREATE CQ Data2ToWaction
INSERT INTO Wactions2
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamTime p;

CREATE CQ Data3ToWaction
INSERT INTO Wactions3
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamAtrribute p;

CREATE CQ Data4ToWaction
INSERT INTO Wactions4
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamCountTime p;

CREATE CQ Data5ToWaction
INSERT INTO Wactions5
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamAttributeTime p
group by city;

CREATE CQ Data6ToWaction
INSERT INTO Wactions6
SELECT
    count(*)
FROM DataStreamCount p
group by companyName;

CREATE CQ Data7ToWaction
INSERT INTO Wactions7
SELECT
    count(*)
FROM DataStreamTime p;

END APPLICATION PartiallyJumping1;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;

 CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01;striim.test02;striim.test03',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',
 startPosition: 'striim.test01=1;striim.test02=-1;%=0',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream ;

  CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

create target AzureSQLDWHTarget1 using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'STRIIM.AUTO_LENGTHY_TABLE_NAME_FROM_INCREMENTAL_READER_TO_AZURE_STRUCTURED_QUERY_LANGUAGE_DATABASE_WAREHOUSE_WITH_128_CHARACTER_MAXIMUM,DBO.AUTOTEST_LENGTHY1 COLUMNMAP(Field1=AUTO_LENGTHY_COLUMNNAME_FROM_INCREMENTAL_READER_TO_AZURE_STRUCTURED_QUERY_LANGUAGE_DATABASE_WAREHOUSE_WITH_128_CHARACTER_MAXIMUM)',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream;

create target AzureSQLDWHTarget2 using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'STRIIM.AUTO_LENGTHY_TABLE_NAME_FROM_INCREMENTAL_READER_TO_AZURE_STRUCTURED_QUERY_LANGUAGE_DATABASE_WAREHOUSE_WITH_128_CHARACTER_MAXIMUM,DBO.AUTOTEST_LENGTHY2 COLUMNMAP(Field1=AUTO_LENGTHY_COLUMNNAME_FROM_INCREMENTAL_READER_TO_AZURE_STRUCTURED_QUERY_LANGUAGE_DATABASE_WAREHOUSE_WITH_128_CHARACTER_MAXIMUM,Field2=Field1026,Field3=Field1027,Field4=Field1028,Field5=Field1029,Field6=Field1030,Field7=Field1031,Field8=Field1032,Field9=Field1033,Field10=Field1034,Field11=Field1035,Field12=Field1036,Field13=Field1037,Field14=Field1038,Field15=Field1039,Field16=Field1040,Field17=Field1041,Field18=Field1042,Field19=Field1043,Field20=Field1044,Field21=Field1045,Field22=Field1046,Field23=Field1047,Field24=Field1048,Field25=Field1049,Field26=Field1050,Field27=Field1051,Field28=Field1052,Field29=Field1053,Field30=Field1054,Field31=Field1055,Field32=Field1056,Field33=Field1057,Field34=Field1058,Field35=Field1059,Field36=Field1060,Field37=Field1061,Field38=Field1062,Field39=Field1063,Field40=Field1064,Field41=Field1065,Field42=Field1066,Field43=Field1067,Field44=Field1068,Field45=Field1069,Field46=Field1070,Field47=Field1071,Field48=Field1072,Field49=Field1073,Field50=Field1074,Field51=Field1075,Field52=Field1076,Field53=Field1077,Field54=Field1078,Field55=Field1079,Field56=Field1080,Field57=Field1081,Field58=Field1082,Field59=Field1083,Field60=Field1084,Field61=Field1085,Field62=Field1086,Field63=Field1087,Field64=Field1088,Field65=Field1089,Field66=Field1090,Field67=Field1091,Field68=Field1092,Field69=Field1093,Field70=Field1094,Field71=Field1095,Field72=Field1096,Field73=Field1097,Field74=Field1098,Field75=Field1099,Field76=Field1100,Field77=Field1101,Field78=Field1102,Field79=Field1103,Field80=Field1104,Field81=Field1105,Field82=Field1106,Field83=Field1107,Field84=Field1108,Field85=Field1109,Field86=Field1110,Field87=Field1111,Field88=Field1112,Field89=Field1113,Field90=Field1114,Field91=Field1115,Field92=Field1116,Field93=Field1117,Field94=Field1118,Field95=Field1119,Field96=Field1120,Field97=Field1121,Field98=Field1122,Field99=Field1123,Field100=Field1124,Field101=Field1125,Field102=Field1126,Field103=Field1127,Field104=Field1128,Field105=Field1129,Field106=Field1130,Field107=Field1131,Field108=Field1132,Field109=Field1133,Field110=Field1134,Field111=Field1135,Field112=Field1136,Field113=Field1137,Field114=Field1138,Field115=Field1139,Field116=Field1140,Field117=Field1141,Field118=Field1142,Field119=Field1143,Field120=Field1144,Field121=Field1145,Field122=Field1146,Field123=Field1147,Field124=Field1148,Field125=Field1149,Field126=Field1150,Field127=Field1151,Field128=Field1152,Field129=Field1153,Field130=Field1154,Field131=Field1155,Field132=Field1156,Field133=Field1157,Field134=Field1158,Field135=Field1159,Field136=Field1160,Field137=Field1161,Field138=Field1162,Field139=Field1163,Field140=Field1164,Field141=Field1165,Field142=Field1166,Field143=Field1167,Field144=Field1168,Field145=Field1169,Field146=Field1170,Field147=Field1171,Field148=Field1172,Field149=Field1173,Field150=Field1174,Field151=Field1175,Field152=Field1176,Field153=Field1177,Field154=Field1178,Field155=Field1179,Field156=Field1180,Field157=Field1181,Field158=Field1182,Field159=Field1183,Field160=Field1184,Field161=Field1185,Field162=Field1186,Field163=Field1187,Field164=Field1188,Field165=Field1189,Field166=Field1190,Field167=Field1191,Field168=Field1192,Field169=Field1193,Field170=Field1194,Field171=Field1195,Field172=Field1196,Field173=Field1197,Field174=Field1198,Field175=Field1199,Field176=Field1200,Field177=Field1201,Field178=Field1202,Field179=Field1203,Field180=Field1204,Field181=Field1205,Field182=Field1206,Field183=Field1207,Field184=Field1208,Field185=Field1209,Field186=Field1210,Field187=Field1211,Field188=Field1212,Field189=Field1213,Field190=Field1214,Field191=Field1215,Field192=Field1216,Field193=Field1217,Field194=Field1218,Field195=Field1219,Field196=Field1220,Field197=Field1221,Field198=Field1222,Field199=Field1223,Field200=Field1224,Field201=Field1225,Field202=Field1226,Field203=Field1227,Field204=Field1228,Field205=Field1229,Field206=Field1230,Field207=Field1231,Field208=Field1232,Field209=Field1233,Field210=Field1234,Field211=Field1235,Field212=Field1236,Field213=Field1237,Field214=Field1238,Field215=Field1239,Field216=Field1240,Field217=Field1241,Field218=Field1242,Field219=Field1243,Field220=Field1244,Field221=Field1245,Field222=Field1246,Field223=Field1247,Field224=Field1248,Field225=Field1249,Field226=Field1250,Field227=Field1251,Field228=Field1252,Field229=Field1253,Field230=Field1254,Field231=Field1255,Field232=Field1256,Field233=Field1257,Field234=Field1258,Field235=Field1259,Field236=Field1260,Field237=Field1261,Field238=Field1262,Field239=Field1263,Field240=Field1264,Field241=Field1265,Field242=Field1266,Field243=Field1267,Field244=Field1268,Field245=Field1269,Field246=Field1270,Field247=Field1271,Field248=Field1272,Field249=Field1273,Field250=Field1274,Field251=Field1275,Field252=Field1276,Field253=Field1277,Field254=Field1278,Field255=Field1279,Field256=Field1280,Field257=Field1281,Field258=Field1282,Field259=Field1283,Field260=Field1284,Field261=Field1285,Field262=Field1286,Field263=Field1287,Field264=Field1288,Field265=Field1289,Field266=Field1290,Field267=Field1291,Field268=Field1292,Field269=Field1293,Field270=Field1294,Field271=Field1295,Field272=Field1296,Field273=Field1297,Field274=Field1298,Field275=Field1299,Field276=Field1300,Field277=Field1301,Field278=Field1302,Field279=Field1303,Field280=Field1304,Field281=Field1305,Field282=Field1306,Field283=Field1307,Field284=Field1308,Field285=Field1309,Field286=Field1310,Field287=Field1311,Field288=Field1312,Field289=Field1313,Field290=Field1314,Field291=Field1315,Field292=Field1316,Field293=Field1317,Field294=Field1318,Field295=Field1319,Field296=Field1320,Field297=Field1321,Field298=Field1322,Field299=Field1323,Field300=Field1324,Field301=Field1325,Field302=Field1326,Field303=Field1327,Field304=Field1328,Field305=Field1329,Field306=Field1330,Field307=Field1331,Field308=Field1332,Field309=Field1333,Field310=Field1334,Field311=Field1335,Field312=Field1336,Field313=Field1337,Field314=Field1338,Field315=Field1339,Field316=Field1340,Field317=Field1341,Field318=Field1342,Field319=Field1343,Field320=Field1344,Field321=Field1345,Field322=Field1346,Field323=Field1347,Field324=Field1348,Field325=Field1349,Field326=Field1350,Field327=Field1351,Field328=Field1352,Field329=Field1353,Field330=Field1354,Field331=Field1355,Field332=Field1356,Field333=Field1357,Field334=Field1358,Field335=Field1359,Field336=Field1360,Field337=Field1361,Field338=Field1362,Field339=Field1363,Field340=Field1364,Field341=Field1365,Field342=Field1366,Field343=Field1367,Field344=Field1368,Field345=Field1369,Field346=Field1370,Field347=Field1371,Field348=Field1372,Field349=Field1373,Field350=Field1374,Field351=Field1375,Field352=Field1376,Field353=Field1377,Field354=Field1378,Field355=Field1379,Field356=Field1380,Field357=Field1381,Field358=Field1382,Field359=Field1383,Field360=Field1384,Field361=Field1385,Field362=Field1386,Field363=Field1387,Field364=Field1388,Field365=Field1389,Field366=Field1390,Field367=Field1391,Field368=Field1392,Field369=Field1393,Field370=Field1394,Field371=Field1395,Field372=Field1396,Field373=Field1397,Field374=Field1398,Field375=Field1399,Field376=Field1400,Field377=Field1401,Field378=Field1402,Field379=Field1403,Field380=Field1404,Field381=Field1405,Field382=Field1406,Field383=Field1407,Field384=Field1408,Field385=Field1409,Field386=Field1410,Field387=Field1411,Field388=Field1412,Field389=Field1413,Field390=Field1414,Field391=Field1415,Field392=Field1416,Field393=Field1417,Field394=Field1418,Field395=Field1419,Field396=Field1420,Field397=Field1421,Field398=Field1422,Field399=Field1423,Field400=Field1424,Field401=Field1425,Field402=Field1426,Field403=Field1427,Field404=Field1428,Field405=Field1429,Field406=Field1430,Field407=Field1431,Field408=Field1432,Field409=Field1433,Field410=Field1434,Field411=Field1435,Field412=Field1436,Field413=Field1437,Field414=Field1438,Field415=Field1439,Field416=Field1440,Field417=Field1441,Field418=Field1442,Field419=Field1443,Field420=Field1444,Field421=Field1445,Field422=Field1446,Field423=Field1447,Field424=Field1448,Field425=Field1449,Field426=Field1450,Field427=Field1451,Field428=Field1452,Field429=Field1453,Field430=Field1454,Field431=Field1455,Field432=Field1456,Field433=Field1457,Field434=Field1458,Field435=Field1459,Field436=Field1460,Field437=Field1461,Field438=Field1462,Field439=Field1463,Field440=Field1464,Field441=Field1465,Field442=Field1466,Field443=Field1467,Field444=Field1468,Field445=Field1469,Field446=Field1470,Field447=Field1471,Field448=Field1472,Field449=Field1473,Field450=Field1474,Field451=Field1475,Field452=Field1476,Field453=Field1477,Field454=Field1478,Field455=Field1479,Field456=Field1480,Field457=Field1481,Field458=Field1482,Field459=Field1483,Field460=Field1484,Field461=Field1485,Field462=Field1486,Field463=Field1487,Field464=Field1488,Field465=Field1489,Field466=Field1490,Field467=Field1491,Field468=Field1492,Field469=Field1493,Field470=Field1494,Field471=Field1495,Field472=Field1496,Field473=Field1497,Field474=Field1498,Field475=Field1499,Field476=Field1500,Field477=Field1501,Field478=Field1502,Field479=Field1503,Field480=Field1504,Field481=Field1505,Field482=Field1506,Field483=Field1507,Field484=Field1508,Field485=Field1509,Field486=Field1510,Field487=Field1511,Field488=Field1512,Field489=Field1513,Field490=Field1514,Field491=Field1515,Field492=Field1516,Field493=Field1517,Field494=Field1518,Field495=Field1519,Field496=Field1520,Field497=Field1521,Field498=Field1522,Field499=Field1523,Field500=Field1524,Field501=Field1525,Field502=Field1526,Field503=Field1527,Field504=Field1528,Field505=Field1529,Field506=Field1530,Field507=Field1531,Field508=Field1532,Field509=Field1533,Field510=Field1534,Field511=Field1535,Field512=Field1536,Field513=Field1537,Field514=Field1538,Field515=Field1539,Field516=Field1540,Field517=Field1541,Field518=Field1542,Field519=Field1543,Field520=Field1544,Field521=Field1545,Field522=Field1546,Field523=Field1547,Field524=Field1548,Field525=Field1549,Field526=Field1550,Field527=Field1551,Field528=Field1552,Field529=Field1553,Field530=Field1554,Field531=Field1555,Field532=Field1556,Field533=Field1557,Field534=Field1558,Field535=Field1559,Field536=Field1560,Field537=Field1561,Field538=Field1562,Field539=Field1563,Field540=Field1564,Field541=Field1565,Field542=Field1566,Field543=Field1567,Field544=Field1568,Field545=Field1569,Field546=Field1570,Field547=Field1571,Field548=Field1572,Field549=Field1573,Field550=Field1574,Field551=Field1575,Field552=Field1576,Field553=Field1577,Field554=Field1578,Field555=Field1579,Field556=Field1580,Field557=Field1581,Field558=Field1582,Field559=Field1583,Field560=Field1584,Field561=Field1585,Field562=Field1586,Field563=Field1587,Field564=Field1588,Field565=Field1589,Field566=Field1590,Field567=Field1591,Field568=Field1592,Field569=Field1593,Field570=Field1594,Field571=Field1595,Field572=Field1596,Field573=Field1597,Field574=Field1598,Field575=Field1599,Field576=Field1600,Field577=Field1601,Field578=Field1602,Field579=Field1603,Field580=Field1604,Field581=Field1605,Field582=Field1606,Field583=Field1607,Field584=Field1608,Field585=Field1609,Field586=Field1610,Field587=Field1611,Field588=Field1612,Field589=Field1613,Field590=Field1614,Field591=Field1615,Field592=Field1616,Field593=Field1617,Field594=Field1618,Field595=Field1619,Field596=Field1620,Field597=Field1621,Field598=Field1622,Field599=Field1623,Field600=Field1624,Field601=Field1625,Field602=Field1626,Field603=Field1627,Field604=Field1628,Field605=Field1629,Field606=Field1630,Field607=Field1631,Field608=Field1632,Field609=Field1633,Field610=Field1634,Field611=Field1635,Field612=Field1636,Field613=Field1637,Field614=Field1638,Field615=Field1639,Field616=Field1640,Field617=Field1641,Field618=Field1642,Field619=Field1643,Field620=Field1644,Field621=Field1645,Field622=Field1646,Field623=Field1647,Field624=Field1648,Field625=Field1649,Field626=Field1650,Field627=Field1651,Field628=Field1652,Field629=Field1653,Field630=Field1654,Field631=Field1655,Field632=Field1656,Field633=Field1657,Field634=Field1658,Field635=Field1659,Field636=Field1660,Field637=Field1661,Field638=Field1662,Field639=Field1663,Field640=Field1664,Field641=Field1665,Field642=Field1666,Field643=Field1667,Field644=Field1668,Field645=Field1669,Field646=Field1670,Field647=Field1671,Field648=Field1672,Field649=Field1673,Field650=Field1674,Field651=Field1675,Field652=Field1676,Field653=Field1677,Field654=Field1678,Field655=Field1679,Field656=Field1680,Field657=Field1681,Field658=Field1682,Field659=Field1683,Field660=Field1684,Field661=Field1685,Field662=Field1686,Field663=Field1687,Field664=Field1688,Field665=Field1689,Field666=Field1690,Field667=Field1691,Field668=Field1692,Field669=Field1693,Field670=Field1694,Field671=Field1695,Field672=Field1696,Field673=Field1697,Field674=Field1698,Field675=Field1699,Field676=Field1700,Field677=Field1701,Field678=Field1702,Field679=Field1703,Field680=Field1704,Field681=Field1705,Field682=Field1706,Field683=Field1707,Field684=Field1708,Field685=Field1709,Field686=Field1710,Field687=Field1711,Field688=Field1712,Field689=Field1713,Field690=Field1714,Field691=Field1715,Field692=Field1716,Field693=Field1717,Field694=Field1718,Field695=Field1719,Field696=Field1720,Field697=Field1721,Field698=Field1722,Field699=Field1723,Field700=Field1724,Field701=Field1725,Field702=Field1726,Field703=Field1727,Field704=Field1728,Field705=Field1729,Field706=Field1730,Field707=Field1731,Field708=Field1732,Field709=Field1733,Field710=Field1734,Field711=Field1735,Field712=Field1736,Field713=Field1737,Field714=Field1738,Field715=Field1739,Field716=Field1740,Field717=Field1741,Field718=Field1742,Field719=Field1743,Field720=Field1744,Field721=Field1745,Field722=Field1746,Field723=Field1747,Field724=Field1748,Field725=Field1749,Field726=Field1750,Field727=Field1751,Field728=Field1752,Field729=Field1753,Field730=Field1754,Field731=Field1755,Field732=Field1756,Field733=Field1757,Field734=Field1758,Field735=Field1759,Field736=Field1760,Field737=Field1761,Field738=Field1762,Field739=Field1763,Field740=Field1764,Field741=Field1765,Field742=Field1766,Field743=Field1767,Field744=Field1768,Field745=Field1769,Field746=Field1770,Field747=Field1771,Field748=Field1772,Field749=Field1773,Field750=Field1774,Field751=Field1775,Field752=Field1776,Field753=Field1777,Field754=Field1778,Field755=Field1779,Field756=Field1780,Field757=Field1781,Field758=Field1782,Field759=Field1783,Field760=Field1784,Field761=Field1785,Field762=Field1786,Field763=Field1787,Field764=Field1788,Field765=Field1789,Field766=Field1790,Field767=Field1791,Field768=Field1792,Field769=Field1793,Field770=Field1794,Field771=Field1795,Field772=Field1796,Field773=Field1797,Field774=Field1798,Field775=Field1799,Field776=Field1800,Field777=Field1801,Field778=Field1802,Field779=Field1803,Field780=Field1804,Field781=Field1805,Field782=Field1806,Field783=Field1807,Field784=Field1808,Field785=Field1809,Field786=Field1810,Field787=Field1811,Field788=Field1812,Field789=Field1813,Field790=Field1814,Field791=Field1815,Field792=Field1816,Field793=Field1817,Field794=Field1818,Field795=Field1819,Field796=Field1820,Field797=Field1821,Field798=Field1822,Field799=Field1823,Field800=Field1824,Field801=Field1825,Field802=Field1826,Field803=Field1827,Field804=Field1828,Field805=Field1829,Field806=Field1830,Field807=Field1831,Field808=Field1832,Field809=Field1833,Field810=Field1834,Field811=Field1835,Field812=Field1836,Field813=Field1837,Field814=Field1838,Field815=Field1839,Field816=Field1840,Field817=Field1841,Field818=Field1842,Field819=Field1843,Field820=Field1844,Field821=Field1845,Field822=Field1846,Field823=Field1847,Field824=Field1848,Field825=Field1849,Field826=Field1850,Field827=Field1851,Field828=Field1852,Field829=Field1853,Field830=Field1854,Field831=Field1855,Field832=Field1856,Field833=Field1857,Field834=Field1858,Field835=Field1859,Field836=Field1860,Field837=Field1861,Field838=Field1862,Field839=Field1863,Field840=Field1864,Field841=Field1865,Field842=Field1866,Field843=Field1867,Field844=Field1868,Field845=Field1869,Field846=Field1870,Field847=Field1871,Field848=Field1872,Field849=Field1873,Field850=Field1874,Field851=Field1875,Field852=Field1876,Field853=Field1877,Field854=Field1878,Field855=Field1879,Field856=Field1880,Field857=Field1881,Field858=Field1882,Field859=Field1883,Field860=Field1884,Field861=Field1885,Field862=Field1886,Field863=Field1887,Field864=Field1888,Field865=Field1889,Field866=Field1890,Field867=Field1891,Field868=Field1892,Field869=Field1893,Field870=Field1894,Field871=Field1895,Field872=Field1896,Field873=Field1897,Field874=Field1898,Field875=Field1899,Field876=Field1900,Field877=Field1901,Field878=Field1902,Field879=Field1903,Field880=Field1904,Field881=Field1905,Field882=Field1906,Field883=Field1907,Field884=Field1908,Field885=Field1909,Field886=Field1910,Field887=Field1911,Field888=Field1912,Field889=Field1913,Field890=Field1914,Field891=Field1915,Field892=Field1916,Field893=Field1917,Field894=Field1918,Field895=Field1919,Field896=Field1920,Field897=Field1921,Field898=Field1922,Field899=Field1923,Field900=Field1924,Field901=Field1925,Field902=Field1926,Field903=Field1927,Field904=Field1928,Field905=Field1929,Field906=Field1930,Field907=Field1931,Field908=Field1932,Field909=Field1933,Field910=Field1934,Field911=Field1935,Field912=Field1936,Field913=Field1937,Field914=Field1938,Field915=Field1939,Field916=Field1940,Field917=Field1941,Field918=Field1942,Field919=Field1943,Field920=Field1944,Field921=Field1945,Field922=Field1946,Field923=Field1947,Field924=Field1948,Field925=Field1949,Field926=Field1950,Field927=Field1951,Field928=Field1952,Field929=Field1953,Field930=Field1954,Field931=Field1955,Field932=Field1956,Field933=Field1957,Field934=Field1958,Field935=Field1959,Field936=Field1960,Field937=Field1961,Field938=Field1962,Field939=Field1963,Field940=Field1964,Field941=Field1965,Field942=Field1966,Field943=Field1967,Field944=Field1968,Field945=Field1969,Field946=Field1970,Field947=Field1971,Field948=Field1972,Field949=Field1973,Field950=Field1974,Field951=Field1975,Field952=Field1976,Field953=Field1977,Field954=Field1978,Field955=Field1979,Field956=Field1980,Field957=Field1981,Field958=Field1982,Field959=Field1983,Field960=Field1984,Field961=Field1985,Field962=Field1986,Field963=Field1987,Field964=Field1988,Field965=Field1989,Field966=Field1990,Field967=Field1991,Field968=Field1992,Field969=Field1993,Field970=Field1994,Field971=Field1995,Field972=Field1996,Field973=Field1997,Field974=Field1998,Field975=Field1999,Field976=Field2000,Field977=Field2001,Field978=Field2002,Field979=Field2003,Field980=Field2004,Field981=Field2005,Field982=Field2006,Field983=Field2007,Field984=Field2008,Field985=Field2009,Field986=Field2010,Field987=Field2011,Field988=Field2012,Field989=Field2013,Field990=Field2014,Field991=Field2015,Field992=Field2016,Field993=Field2017,Field994=Field2018,Field995=Field2019,Field996=Field2020,Field997=Field2021,Field998=Field2022,Field999=Field2023,Field1000=Field2024,Field1001=Field2025,Field1002=Field2026,Field1003=Field2027,Field1004=Field2028,Field1005=Field2029,Field1006=Field2030,Field1007=Field2031,Field1008=Field2032,Field1009=Field2033,Field1010=Field2034,Field1011=Field2035,Field1012=Field2036,Field1013=Field2037,Field1014=Field2038,Field1015=Field2039,Field1016=Field2040,Field1017=Field2041,Field1018=Field2042,Field1019=Field2043,Field1020=Field2044,Field1021=Field2045,Field1022=Field2046)',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream;


CREATE TARGET log USING LOGWRITER
(
 name:log,
 filename:'LOGFILENAME'
)
INPUT FROM data_stream;


END APPLICATION IR;
deploy application IR;
start application IR;

-- The PosApp sample application demonstrates how a credit card
-- payment processor might use Striim to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.

stop test.PosApp;
undeploy application test.PosApp;
drop application test.PosApp cascade;

CREATE APPLICATION PosApp;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING FileReader (
  directory:'Samples/Customer/PosApp/appData',
  wildcard : '$admin.wildcard',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells Striim that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells Striim to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvToPosData

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP.
--
-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as
-- the types to be used in PosDataStream, which is created automatically.

-- The DATETIME field from the source is converted to both a dateTime
-- value, used as the event timestamp by the application, and (via the
-- function) an integer hourValue, which is used to look up
-- historical hourly averages from the HourlyAveLookup cache,
-- discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)
-- 17 (hourValue)
-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateOnly
--
-- The PosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: 'Samples/Customer/PosApp/appData',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startingTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;


-- WAction store MerchantActivity
--
-- The following group of statements create and populate the MerchantActivity
-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced
-- with merchant details from NameLookup cache and with latitude and longitude
-- values from the USAddressData cache.

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );


CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'Samples/Customer/PosApp/appData',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false
)
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: 'Samples/Customer/PosApp/appData',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;


-- CQ GenerateAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;



--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM AlertStream;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;


END APPLICATION PosApp;


CREATE DASHBOARD USING "Samples/Customer/PosApp/PosAppDashboard.json";

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@;

CREATE SOURCE @SOURCE_NAME@2 USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@;

drop application Component_Disappear cascade;
drop cq Test_Subquery_Cq;
drop stream Test_Subquery_Cq_out;

CREATE APPLICATION Component_Disappear;

create or replace CQ Test_Subquery_Cq
insert into Test_Subquery_Cq_out
SELECT f.topic as topic, sum(f.rawdatacount) as TotalLast24hour, B.rawdatacount as TotalLast1hour FROM JUMP_WND_1EVT_1MIN h
   join SLIDE_WND_HOURLYTOTALS_KAFKADATA_FILE f on 1=1
   join (SELECT rawdatacount, topic,timerange from  ET_HOURLYTOTALS_KAFKADATA_FILE,JUMP_WND_1EVT_30SEC where timerange = DHOURS(DNOW())-1) B on B.topic=f.topic
   Group by f.topic;

END APPLICATION Component_Disappear;

STOP application AlterTester.DSV;
undeploy application AlterTester.DSV;
drop application AlterTester.DSV cascade;


create application DSV;

create flow myFlowDSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

end flow myFlowDSV;
end application DSV;

create application FileXML;
create source XMLSource using FileReader (
	Directory:'@TEST-DATA-PATH@',
	WildCard:'books.xml',
	positionByEOF:false
)
parse using XMLParser (
	RootNode:'/catalog/book'
)
OUTPUT TO XmlStream;

-- Below Sysout is added to test DEV-23437.  Not directly validated in the test except the App should not crash with sysout target
CREATE TARGET XMLEventSYSout USING sysout  (
name: 'XMLEventSYSoutOut' )
INPUT FROM XmlStream;

create Target XMLDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/xmldata') input from XmlStream;
end application FileXML;

STOP noParser;
UNDEPLOY APPLICATION noParser;
DROP APPLICATION noParser CASCADE;

CREATE APPLICATION noParser;

CREATE TYPE Atm(
productID String KEY,
stateID String,
productWeight int,
quantity double,
size long,
currentDate DateTime);


CREATE CACHE cache1 USING FileReader(
  directory: '@TEST-DATA-PATH@',
  wildcard: 'ISdata.csv',
  header: false,
  columndelimiter: ',',
  trimquote: false
  ) QUERY (keytomap:'productID') OF Atm;

END APPLICATION noParser;

CREATE FLOW ServerFlow;


CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
Float_col Double,
BFLoat_col Double,
bdouble_col Double,
long_col String,
Table String,
Operation String
);

CREATE STREAM CDCFilteredStream OF LogType;

CREATE CQ ToFilteredStream
INSERT INTO CDCFilteredStream
SELECT data[0],
data[1],
data[2],
to_double(data[3]),
to_double(data[4]),
to_double(data[5]),
data[6],
META(a, "TableName"),
META(a, "OperationName")
from @STREAM@ a;



CREATE WINDOW CDCWindow
OVER CDCFilteredStream
KEEP 3 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

END FLOW ServerFlow;


CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
 )
INPUT FROM @STREAM@;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;

 CREATE OR REPLACE SOURCE Teradata_source_App2 USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01;striim.test02;striim.test03',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',
 startPosition: 'striim.test01=1;striim.test02=-1;%=0',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream1 ;

  CREATE OR REPLACE TARGET sys2 USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream1;

create target AzureSQLDWHTarget_app2 using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream1;


CREATE TARGET log USING LOGWRITER
(
 name:log,
 filename:'LOGFILENAME'
)
INPUT FROM data_stream1;


END APPLICATION IR;

deploy application IR;
start IR;

IMPORT static com.webaction.runtime.converters.DateConverter.*;

DROP APPLICATION SampleCDCReaderApp cascade;

CREATE APPLICATION SampleCDCReaderApp;
create source CDCSource using SampleReader (
	AgentPortNo:'2012',
	AgentIpAddress:'127.0.0.1',
	portno:'2020',
	ipaddress:'10.10.196.103',
	Name:'testsession',
    dataFile:'../conf/data.csv',
    metaFile:'../conf/metadata.csv',
	Tables:'POS;STU') output to dbstream,
	CheckUnSupportedTypeStream MAP (table:'STU');

CREATE TYPE CDCPosData(
    BUSINESSNAME String,
    BUSINESSNAME_HEX String,
    PRIMARYACCOUNTNUMBER String,
    POSDATACODE Integer,
    DATETIME org.joda.time.DateTime,
    EXPDATE String,
    CURRENCYCODE String,
    AUTHAMOUNT Float,
    TERMINALID String,
    ZIP String,
    CITY String,
    OPR String,
    TABLENAME String
);

CREATE STREAM CDCPosDataStream OF CDCPosData;

CREATE JUMPING WINDOW CDCPosDataWindow
OVER CDCPosDataStream KEEP 9 ROWS
PARTITION BY OPR;

CREATE CQ CDCCsvToPosData
INSERT INTO CDCPosDataStream
SELECT TO_STRING(data[0],"UTF-8"),
	   TO_HEX(data[0]),
       data[2],
       TO_INT(data[3]),
	   data[4],
	   data[5],
       data[6],
       TO_FLOAT(data[7]),
	   data[8],
       data[9],
	   data[10],
	META(x,"OperationName").toString(),
	META(x, "TableName").toString()
FROM dbstream x
WHERE not(META(x,"OperationName").toString() = "BEGIN") AND not(META(x,"OperationName").toString() = "COMMIT") AND not(META(x, "TableName").toString() is null) AND META(x, "TableName").toString() = "POS";

CREATE TYPE CDCSampleOperationData(
    TableName String,
    OperationName String,
    Count Integer
);

CREATE STREAM CDCSampleOperationDataStream OF CDCSampleOperationData;
-- PARTITION BY OperationName;


CREATE CQ CDCSampleOperationCheck
INSERT INTO CDCSampleOperationDataStream
SELECT x.TABLENAME,
CASE WHEN x.OPR = 'INSERT' THEN x.OPR
     WHEN x.OPR = 'DELETE' THEN x.OPR
     WHEN x.OPR = 'UPDATE' THEN x.OPR
     ELSE 'UNSUPPORTED OPREATION' END,
CASE WHEN x.OPR = 'INSERT' THEN COUNT(x.OPR)
     WHEN x.OPR = 'DELETE' THEN COUNT(x.OPR)
     WHEN x.OPR = 'UPDATE' THEN COUNT(x.OPR)
     ELSE 0 END
FROM CDCPosDataWindow x
GROUP BY OPR;

CREATE TARGET CDCOperationLog USING LogWriter(
	name:FILECDCP,
	filename:'@FEATURE-DIR@/logs/SampleCDCReaderOperationCheck.log'
--	filename:'a1.log'
) INPUT FROM CDCSampleOperationDataStream;

CREATE TARGET CDCLog USING LogWriter(
  name:SampleCDCReaderApp,
filename:'@FEATURE-DIR@/logs/SampleCDCReaderApp.log'
--  filename:'a.log'
) INPUT FROM CDCPosDataStream;

CREATE TARGET CDCLog1 USING LogWriter(
  name:SampleCDCReaderApp,
  filename:'@FEATURE-DIR@/logs/UnsupportedColumn.log'
--  filename:'a2.log'
) INPUT FROM CDCCheckUnSupportedTypeStream;

END APPLICATION SampleCDCReaderApp;

--deploy application SampleCDCReaderApp.SampleCDCReaderApp in default;

STOP APPLICATION HW ;
undeploy application HW ;
drop application HW cascade;

CREATE APPLICATION HW Recovery 5 second interval;

CREATE  SOURCE S USING OrReader  ( 
  Username: 'miner',
  Password: '@miner',
  ConnectionURL: '@conn-url@',
  Tables: '@src@',
  FetchSize: 1) 
OUTPUT TO hivestream;

Create Target T using HiveWriter (
  ConnectionURL:'@hive-url@',
  Username:'@uname@', 
            Password:'@pwd@',
        --hadoopurl:'hdfs://localhost:9000/',
        hadoopurl:'hdfs://dockerhost:9000/',
	        Mode:'incremental',
	        mergepolicy: 'eventcount:100,interval:1s',
            Tables:'@tgt-table@',
            hadoopConfigurationPath:'/Users/saranyad/Documents/hello/'
 )
INPUT FROM hivestream;


END APPLICATION HW;
deploy application HW on all in default;

Start application HW;

STOP virtualTester.VirtualApp;
UNDEPLOY APPLICATION virtualTester.VirtualApp;
DROP APPLICATION virtualTester.VirtualApp cascade;

CREATE APPLICATION VirtualApp;


CREATE OR REPLACE SOURCE wsSource USING FileReader
(
  directory:'@TEST-DATA-PATH@',
  wildcard:'AdhocQueryData2.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser
(
   header:True,
   columndelimiter:',',
   trimquote:false
)OUTPUT TO QaStream;

CREATE OR REPLACE Target TheData using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/QaStream.log') input from QaStream;

CREATE OR REPLACE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE OR REPLACE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'AdhocQueryData.csv'
)
parse using DSVParser
(
  header:'yes',
  columndelimiter: '	',
  trimquote:false

)QUERY (keytomap:'zip') OF USAddressData;

Create OR REPLACE TYPE wsData(
  CompanyNum String,
  CompanyName String KEY,
  CompanyCode int,
  Zip String
);


CREATE OR REPLACE STREAM wsStream OF wsData;

--Select data from QaStream and insert into wsStream

CREATE OR REPLACE CQ csvTowsData
INSERT INTO wsStream
SELECT data[0],
       data[1],
       TO_INT(data[3]),
       data[9]
FROM QaStream;


CREATE OR REPLACE WACTIONSTORE oneWS CONTEXT OF wsData
  EVENT TYPES (wsData );


CREATE OR REPLACE CQ wsToWaction
  INSERT INTO oneWS
  SELECT * FROM wsStream
  LINK SOURCE EVENT;

END APPLICATION VirtualApp;

stop Postgres_SQLDBWHEventTableApp;
undeploy application Postgres_SQLDBWHEventTableApp;
drop application Postgres_SQLDBWHEventTableApp cascade;
CREATE APPLICATION Postgres_SQLDBWHEventTableApp;

CREATE OR REPLACE SOURCE Postgres_Src1 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgres_src1',
  ExcludedTables: ''
 ) 
OUTPUT TO data_stream1;

CREATE OR REPLACE SOURCE Postgres_Src2 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgres_src2',
  ExcludedTables: ''
 ) 
OUTPUT TO data_stream2;

CREATE OR REPLACE SOURCE Postgres_Src3 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgres_src2',
  ExcludedTables: ''
 ) 
OUTPUT TO data_stream3;

Create Type EventType (
ID int,
NAME string,
COMPANY string
);

CREATE STREAM insertData1  of EventType;
CREATE STREAM deleteData1 of EventType;
CREATE STREAM joinData1 of EventType;
CREATE STREAM joinData2 of EventType;
CREATE STREAM deleteData2 of EventType;
CREATE STREAM OutStream of EventType;

CREATE CQ cq1 INSERT INTO insertData1  SELECT TO_INT(data[0]),data[1],data[2] FROM data_stream1;

CREATE CQ cq2 INSERT INTO deleteData1 SELECT TO_INT(data[0]),data[1],data[2] FROM data_stream2;

CREATE CQ cq3 INSERT INTO joinData1 SELECT TO_INT(data[0]),data[1],data[2] FROM data_stream3;

CREATE JUMPING WINDOW DataWin1 OVER deleteData1 KEEP 1 ROWS;

CREATE CQ TypedEventKeyCQ6 INSERT INTO deleteData2 SELECT * from DataWin1;

CREATE JUMPING WINDOW DataWin2 OVER joinData1 KEEP 1 ROWS;

CREATE CQ TypedEventKeyCQ5 INSERT INTO joinData2 SELECT * from DataWin2;

CREATE EVENTTABLE ETABLE1 using STREAM ( NAME: 'insertData1 ' )
--DELETE using STREAM ( NAME: 'deleteData1')
QUERY (keytomap:"ID", persistPolicy: 'true') OF EventType;

CREATE CQ cq4 INSERT INTO OutStream SELECT B.ID,B.NAME,B.COMPANY FROM joinData2 A, ETABLE1 B where A.ID=B.ID;

CREATE TARGET EventTableFW USING FileWriter
(filename:'BasicPostgres_SQLDBWHEventTableApp_RT.log',
 rolloverpolicy: 'EventCount:1000000')
FORMAT USING DSVFormatter () INPUT FROM OutStream;

create target Target_Azure using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'STRIIM',
        password: 'W3b@ct10n',
        AccountName: 'striimqatestdonotdelete',
        accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables:'dbo.autotest01',
        uploadpolicy:'eventcount:0,interval:0s'
) INPUT FROM OutStream;

END APPLICATION Postgres_SQLDBWHEventTableApp;
deploy application Postgres_SQLDBWHEventTableApp in default;
start Postgres_SQLDBWHEventTableApp;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using MSSqlReader
(
 Username:'@UserName@',
 Password:'@Password@',
 DatabaseName:'@DatabaseName@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'@SourceTable@',
 ConnectionPoolSize:1,
 StartPosition:'EOF'
 ) Output To @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;

create Target @targetFile@ using FileWriter(
  filename:'TestOut.log',
  directory:'@FileDirectoryPath@',
  rolloverpolicy:'eventcount:1000'
)
format using DSVFormatter (

)
input from @SRCINPUTSTREAM@;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

CREATE TARGET @TARGET_NAME@ USING SpannerWriter (
  instanceId: 'qatest',
  BatchPolicy: 'EventCount:10000,Interval:60',
  Tables: 'QATEST.ORACLETOSPANNER_SOURCETABLE%,oracletospannerdb.OracleToSpannertarget',
  adapterName: 'SpannerWriter',
  ServiceAccountKey: '/Users/jenniffer/Product2/IntegrationTests/TestData/google-gcs.json'
) INPUT FROM @STREAM@;

CREATE OR REPLACE  EMBEDDINGGENERATOR @EMB_NAME@ USING @MODEL@ (
'modelProvider': '@MODEL@',
'modelName': '@MODEL_NAME@',
'project': '@PROJECT@',
'location': '@LOCATION@',
'publisher': '@PUBLISHER@',
'serviceAccountKey': '@SERVICE_ACCOUNT_KEY@'
);

CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'data.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO @AppName@_rawstream;

CREATE CQ @BuiltinFunc@CQ
INSERT INTO @BuiltinFunc@Stream
SELECT updateUserData(x, 'Last_Date', data[5], 'Country', data[10])
FROM @AppName@_rawstream x;

CREATE OR REPLACE CQ cq1
INSERT INTO clearUserData_Stream
SELECT
clearUserData(s1)
FROM @BuiltinFunc@Stream s1;

CREATE OR REPLACE TARGET @AppName@_FileTarget USING Global.FileWriter ( 
  flushpolicy: 'EventCount:10000,Interval:30s', 
  adapterName: 'FileWriter', 
  directory: '@logs@',
  filename: '@BuiltinFunc@_ClearData', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING Global.JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter', 
  jsonMemberDelimiter: '\n', 
  EventsAsArrayOfJsonObjects: 'true', 
  formatterName: 'JSONFormatter', 
  jsonobjectdelimiter: '\n' ) 
INPUT FROM clearUserData_Stream;

End application @AppName@;
Deploy application @AppName@; 
Start application @AppName@;

stop @appname@;
undeploy application @appname@;
DROP APPLICATION @appname@ CASCADE;
CREATE APPLICATION @appname@;

CREATE SOURCE @appname@_src USING databaseReader  (
  Username: '@@',
  Password: '@@',
  ConnectionURL: '@@',
  Tables: '@@',
  FetchSize: '100'
 )
OUTPUT TO @appname@_ss;

CREATE JUMPING WINDOW @appname@_win OVER @appname@_ss KEEP @winsize@ ROWS;

CREATE TYPE @appname@_MapType
    (   
       id INTEGER,
        name STRING,
        city  STRING
    );
    
CREATE EXTERNAL CACHE @appname@_cach (
  AdapterName: 'DatabaseReader',
    ConnectionURL: '@url@',
    UserName: '@uname@',
    Password: '@pwd@',
   Table: '@tablename@',
  FetchSize: 100,
  Columns: 'id,name,city',
  trimquote: false,
  KeyToMap: '@key@'
 )
 OF @appname@_MapType;
 
CREATE TYPE @appname@_MapTypenew
    (   id_t            INTEGER,
        name_t           STRING,
        city_t            STRING,
        id_c            INTEGER,
        name_c            STRING,
        city_c            STRING
    );
    
CREATE STREAM @appname@_JoinedData OF @appname@_MapTypenew;

CREATE CQ @appname@_JoinDataCQ
INSERT INTO @appname@_JoinedData
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_STRING(f.data[2]),
        z.id,
        z.name,
        z.city
FROM @appname@_win f, @appname@_cach z
where TO_INT(f.data[0]) = z.id
@Ex@;

CREATE TARGET @appname@_tgt USING DatabaseWriter
(
  ConnectionURL:'@@',
  Username:'@@',
  Password:'@@',
  BatchPolicy:'Eventcount:10000,Interval:1',
  CommitPolicy:'Interval:1,Eventcount:10000',
  Tables:'@@'
) 
INPUT FROM @appname@_JoinedData;

END APPLICATION @appname@;
deploy application @appname@;
start @appname@;

create Application UdpXml;
create source UdpXMLSource using UDPReader (
	IpAddress:'127.0.0.1',
	PortNo:'3546'
)
parse using XMLParser (
    RootNode:'/catalog/book'
)
OUTPUT TO UdpXMLStream;
create Target UdpDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/xmldata') input from UdpXMLStream;
end Application UdpXml;

STOP QueryAggTester.ws_one;
UNDEPLOY APPLICATION QueryAggTester.ws_one;
DROP APPLICATION QueryAggTester.ws_one cascade;

CREATE APPLICATION ws_one;


CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;


CREATE TYPE wsData
(
bankID Integer KEY,
bankName String
);


CREATE STREAM wsStream OF wsData;

--Select data from QaStream and insert into wsStream

CREATE CQ csvTowsData
INSERT INTO wsStream
SELECT TO_INT(data[0]),data[1] FROM QaStream;

--create jumping window over data in wsStream

CREATE WACTIONSTORE oneWS
CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@

--get data from wsStream and place into wactionStore oneWS
CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT * FROM wsStream
LINK SOURCE EVENT;


END APPLICATION ws_one;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@  RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @srcName@ USING OracleReader (
 Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@'
)
OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@ USING Global.DeltaLakeWriter (
  personalAccessToken:'@tgtpassword@',
  hostname:'@tgthostname@',
  stageLocation:'/',
  Mode:'MERGE',
  AuthenticationType: 'PersonalAccessToken',
  Tables:'@srcschema@.@srctable@,@tgtschema@.@tgttable@',
  adapterName:'DeltaLakeWriter',
  personalAccessToken_encrypted:'false',
  optimizedMerge:'false',
  uploadPolicy:'eventcount:1,interval:10s',
  connectionUrl:'@tgturl@',
  IgnorableExceptionCode:'TABLE_NOT_FOUND',
  externalStageType:'DBFSROOT'
)
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop application ADW2;
undeploy application ADW2;
drop application ADW2 cascade;
CREATE APPLICATION ADW2;

CREATE  SOURCE SqlServerInitialLoad2 USING DatabaseReader  
 (
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'@SOURCE-TABLES@',
 FetchSize:2000
) 
OUTPUT TO InitialLoadStream2;

CREATE TARGET AzureDWInitialLoad2 USING AzureSQLDWHWriter(
ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
)
INPUT FROM InitialLoadStream2;

END APPLICATION ADW2;
deploy application ADW2;
start application ADW2;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING MySqlReader
(
  Compression: false,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'mysql://localhost:3306',
  DatabaseName: 'waction',
  Tables: 'waction.test01',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root',
  connectionRetryPolicy:'retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'waction.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true	
) INPUT FROM @APPNAME@_ss;
--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadpolicy:'EventCount:7'
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from @STREAM@;
end application @APPNAME@;

stop application Postgres_To_PostgresApp;
undeploy application Postgres_To_PostgresApp;
drop application Postgres_To_PostgresApp cascade;

CREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;

create type employee
(
id integer,
ename String,
operationname String,
LSN String
);
CREATE STREAM Postgres_TypedStream of employee;

CREATE OR REPLACE SOURCE Postgres_Src USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgres_src'
 ) 
OUTPUT TO Postgres_Change_Data_Stream;

create stream UserdataStream1 of Global.WAEvent;

Create CQ CQUser
insert into UserdataStream1
select putuserdata (data1,'OperationName',META(data1,'OperationName').toString()) from Postgres_Change_Data_Stream data1;

create CQ Cqfilter 
insert into Postgres_TypedStream
select 
to_int(data[0]),
data[1],
META(u,'OperationName').toString(),
META(u,'LSN').toString()
from UserdataStream1 u 
where USERDATA(u,'OperationName').toString()=='INSERT';

CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Postgres_TypedStream;

CREATE TARGET Postgres_FW USING FileWriter (
  filename:'Postgres_FW.log'
)
FORMAT USING DSVFormatter ()
INPUT FROM Postgres_TypedStream;

CREATE OR REPLACE TARGET Postgres_tgt USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:300',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Postgres_TypedStream;

end application Postgres_To_PostgresApp;
deploy application Postgres_To_PostgresApp;
start Postgres_To_PostgresApp;

stop ORAToBigquery;
undeploy application ORAToBigquery;
drop application ORAToBigquery cascade;

CREATE APPLICATION ORAToBigquery RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE Rac11g USING OracleReader ( 
  SupportPDB: false,
  SendBeforeImage: true,
  ReaderType: 'LogMiner',
  CommittedTransactions: false,
  FetchSize: 1,
  Password: 'manager',
  DDLTracking: false,
  StartTimestamp: 'null',
  OutboundServerProcessName: 'WebActionXStream',
  OnlineCatalog: true,
  ConnectionURL: '192.168.33.10:1521/XE',
  SkipOpenTransactions: false,
  Compression: false,
  QueueSize: 40000,
  RedoLogfiles: 'null',
  Tables: 'SYSTEM.GGAUTHORIZATIONS',
  Username: 'system',
  FilterTransactionBoundaries: true,
  adapterName: 'OracleReader',
  XstreamTimeOut: 600,
  connectionRetryPolicy: 'timeOut=30, retryInterval=60, maxRetries=3'
 ) 
OUTPUT TO DataStream;

CREATE OR REPLACE TARGET Target1 USING SysOut ( 
  name: "dstream"
 ) 
INPUT FROM DataStream;

CREATE OR REPLACE TARGET Target2 using BigqueryWriter(
  BQServiceAccountConfigurationPath:"/Users/ravipathak/Downloads/big-querytest-1963ae421e90.json",
  projectId:"big-querytest",
  Tables: "SYSTEM.GGAUTHORIZATIONS,testing1.ggauthorisation",
  parallelismCount: 2,
  BatchPolicy: "eventCount:100000,Interval:0")
INPUT FROM DataStream;

END APPLICATION ORAToBigquery;

deploy application ORAToBigquery;
start ORAToBigquery;

UNDEPLOY APPLICATION TcpDsvAgentTester.TcpDsvWithAgent;
DROP APPLICATION TcpDsvAgentTester.TcpDsvWithAgent cascade;

create Application TcpDsvWithAgent;


create source TcpDsvAgent using TCPReader
(
  IpAddress:'127.0.0.1',
  PortNo:'3549',
  charset: 'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO TcpDsvAgentStream;


CREATE TYPE UserDataType
(
  UserId String KEY,
  UserName String,
  CompanyName String,
  UserZip int,
  CompanyZip int
);

CREATE STREAM UserDataStream OF UserDataType PARTITION BY UserId;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  data[0],
        data[1],
        data[2],
        TO_INT(data[3]),
        TO_INT(data[4])
FROM TcpDsvAgentStream;


CREATE WACTIONSTORE UserActivityInfo
CONTEXT OF UserDataType
EVENT TYPES ( UserDataType )
PERSIST EVERY 6 second USING (
JDBC_DRIVER:'@WASTORE-DRIVER@',
JDBC_URL:'@WASTORE-URL@',
JDBC_USER:'@WASTORE-UNAME@',
JDBC_PASSWORD:'@WASTORE-PASSWORD@',
DDL_GENERATION:'drop-and-create-tables',
LOGGING_LEVEL:'SEVERE',
CONTEXT_TABLE:'USERTABLE',
EVENT_TABLE:'USEREVENTS'
);


--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction
INSERT INTO UserActivityInfo
SELECT * FROM UserDataStream
LINK SOURCE EVENT;



end Application TcpDsvWithAgent;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING MSSqlReader
(
  Compression: false,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.test01',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
)
OUTPUT TO @APPNAME@_SS;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(id,col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'NULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true	,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

CREATE OR REPLACE TARGET  @TARGET_NAME@ USING CassandraCosmosDBWriter  (
  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --CheckPointTable: '',
  FlushPolicy: 'EventCount:1,Interval:60',
  AccountEndpoint: 'cassandracosmostest.cassandra.cosmos.azure.com',
  AccountKey: 'pqDZvVgbdSCg7VzIzD77dAhPG2odGRZPLhAQA1qnZbAKoIDk6RuQX5r2phbRQFnR1l54qxOcvBXNdz8DeijYIg==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  --ConsistencyLevel:'fh',
  --port:'10505',
  Tables: 'QATEST.Source1,test.target1',
  --Tables: 'QATEST.OracToCql_alldatatypes,test.tgt_data',
  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',
  adapterName: 'CassandraCosmosDBWriter'
 )
 INPUT FROM @STREAM@;

STOP APPLICATION LongRunningQueryTester.LongRunningQuery;
UNDEPLOY APPLICATION LongRunningQueryTester.LongRunningQuery;
DROP APPLICATION LongRunningQueryTester.LongRunningQuery cascade;

CREATE APPLICATION LongRunningQuery;




CREATE TYPE RandomData(
myName String,
streetAddress String,
bankName String,
bankNumber int KEY,
bankAmount double
);


CREATE SOURCE ranDataSource using StreamReader(
OutputType: 'LongRunningQueryTester.RandomData',
noLimit: 'false',
maxRows: 0,
iterations: 2,
iterationDelay: 1000,
StringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',
NumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]G'
)OUTPUT TO CSVDataStream;

CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4])
FROM CSVDataStream;




CREATE TYPE myData(
myName String,
myAddress String,
myBankName String,
myBankNumber int KEY,
myBankAmount double
);

CREATE STREAM myDataStream OF myData;

CREATE CQ GetMyData
INSERT INTO MyDataStream
SELECT myName, streetAddress, bankName, bankNumber, bankAmount
FROM RandomDataStream;


CREATE WACTIONSTORE MyDataActivity
CONTEXT OF MyData
EVENT TYPES(myData )
@PERSIST-TYPE@

Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
Select * from myDataStream
LINK SOURCE EVENT;


END APPLICATION LongRunningQuery;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@;

create TYPE CountTYPE(numcol INT);

CREATE JUMPING WINDOW nEvents OVER @STREAM@ KEEP 10 ROWS;

CREATE STREAM TypedCountStream of CountTYPE;

CREATE CQ CountCQ INSERT INTO TypedCountStream SELECT TO_INT(data[0]) FROM nEvents;

STOP APPLICATION DBRTOCW;
UNDEPLOY APPLICATION DBRTOCW;
DROP APPLICATION DBRTOCW CASCADE;
CREATE APPLICATION DBRTOCW;

CREATE TYPE employee(
id String,
ename String
);

CREATE STREAM Oracle_ChangeDataStream of employee;

create source CSVSource using FileReader (
	directory:'/Users/jenniffer/Product2/IntegrationTests/TestData/',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (

)
OUTPUT TO FileStream;

CREATE CQ CQfilter
INSERT INTO Oracle_ChangeDataStream
select data[0],data[1] from FileStream;


create Target t2 using SysOut(name:OrgData) input from Oracle_ChangeDataStream;
CREATE OR REPLACE Target DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1000,Interval:60',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  Tables: 'test.employee',
  Password: 'cassandra',
  Password_encrypted: false
 )INPUT FROM Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;
DEPLOY APPLICATION DBRTOCW;
START APPLICATION DBRTOCW;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;
CREATE APPLICATION OracleToKudu RECOVERY 1 SECOND INTERVAL;
CREATE  SOURCE oracSource USING OracleReader  ( 
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.56.102:1521/orcl',
  Tables: 'QATEST.oracle_alldatatypes',
  OnlineCatalog: true,
  FetchSize: 1
 ) 
OUTPUT TO DataStream;
CREATE  TARGET WriteintoKudu USING KuduWriter  ( 
  kuduclientconfig: 'master.addresses->192.168.56.101:7051;socketreadtimeout->240;operationtimeout->1200',
  pkupdatehandlingmode: 'DELETEANDINSERT',
  tables: 'QATEST.ORACLE_ALLDATATYPES,KUDU_ALLDATATYPES',
  batchpolicy: 'EventCount:1,Interval:0'
 ) 
INPUT FROM DataStream;
END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

Undeploy application bq;
alter application bq;
CREATE OR REPLACE SOURCE S USING OracleReader  ( 
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.TABLE_TEST_1000100',
  DictionaryMode: offlineCatalog,
  FetchSize: '1'
 ) 
OUTPUT TO SS;
alter application bq recompile;
deploy application bq;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

END APPLICATION @APPNAME@;

stop application Postgres_To_PostgresApp;
undeploy application Postgres_To_PostgresApp;
drop application Postgres_To_PostgresApp cascade;

CREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE Postgres_src USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 ) 
OUTPUT TO Change_Data_Stream ;


CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET Postgres_tgt USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

end application Postgres_To_PostgresApp;
deploy application Postgres_To_PostgresApp;
start Postgres_To_PostgresApp;

CREATE OR REPLACE PROPERTYVARIABLE Partitionkey1='@metadata(RecordOffset)';
CREATE OR REPLACE PROPERTYVARIABLE Partitionkey2='Col1';
CREATE OR REPLACE PROPERTYVARIABLE OperationTimeout='500000';
CREATE OR REPLACE PROPERTYVARIABLE ConnectionRetryPolicy='Retries:5,RetryBackOff:1m';

STOP APPLICATION EH;
UNDEPLOY APPLICATION EH;
DROP APPLICATION EH CASCADE;
CREATE APPLICATION EH @Recovery@;
CREATE SOURCE s USING FileReader (
	directory:'Product/IntegrationTests/TestData/',
    WildCard:'posdata.csv',
	positionByEOF:false
	)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO ss;

CREATE TYPE userType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String
);

CREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;

CREATE CQ cq1
INSERT INTO userDefinedTypedStream
SELECT data[0],data[1],data[2], data[3],data[4]
FROM ss;

create Target t1 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	Partitionkey:'$Partitionkey1',
	--ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'$OperationTimeout',
	ConnectionRetryPolicy:'$ConnectionRetryPolicy'
)
format using AvroFormatter (
	schemaFileName:'kafkaAvroTest1.avsc')
input from ss;

create Target t2 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	--ParallelThreads:'2',
	Partitionkey:'$Partitionkey2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'$ConnectionRetryPolicy'
)
format using AvroFormatter (
	schemaFileName:'kafkaAvroTest2.avsc')
input from userDefinedTypedStream;

END APPLICATION EH;
DEPLOY APPLICATION EH;
start application EH;


STOP APPLICATION ER;
UNDEPLOY APPLICATION ER;
DROP APPLICATION ER CASCADE;
CREATE APPLICATION ER;
CREATE SOURCE ER_S1 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING avroParser (
schemaFileName:'kafkaAvroTest1.avsc'
)OUTPUT TO ER_SS1;
CREATE SOURCE ER_S2 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING avroParser (
schemaFileName:'kafkaAvroTest2.avsc'
)OUTPUT TO ER_SS2;

create Type CustType 
(writerdata com.fasterxml.jackson.databind.JsonNode
--TopicName java.lang.String,
--PartitionID java.lang.String
);

Create Stream datastream1 of CustType;
Create Stream datastream2 of CustType;

CREATE CQ CustCQ1
INSERT INTO datastream1
SELECT AvroToJson(s1.data)
--metadata.get("TopicName").toString() AS TopicName,
--metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS1 s1;

CREATE CQ CustCQ2
INSERT INTO datastream2
SELECT AvroToJson(s2.data)
--metadata.get("TopicName").toString() AS TopicName,
--metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS2 s2;

create Target ER_t1 using FileWriter (
filename:'FT1_5L_AVRO_RT',
directory:'FEATURE-DIR/logs/',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from datastream1;

create Target ER_t2 using FileWriter (
filename:'FT2_5L_AVRO_RT',
directory:'FEATURE-DIR/logs/',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from datastream2;
end application ER;
deploy application ER;

stop @appName@;
undeploy application @appName@;
drop application @appName@ cascade;
CREATE APPLICATION @appName@ RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE @appName@_Source USING OracleReader
(
  FilterTransactionBoundaries:true,
  ConnectionURL:'@ConnectionURL@',
  Tables:'@OrcTable@',
  Password:'@Password@',
  fetchsize:'1',
  connectionRetryPolicy:'timeOut=30, retryInterval=30, maxRetries=3',
  Username:'@Username@'
)
OUTPUT TO @appName@_Stream;

CREATE OR REPLACE TARGET @appName@_Target1 USING Global.BigQueryWriter (
  ColumnDelimiter: '|',
  NullMarker: 'NULL',
  ConnectionRetryPolicy: 'retryInterval=30,\n maxRetries=3',
  streamingUpload: 'false',
  Mode: 'Merge',
  projectId: '@ProjectId@',
  Encoding: 'UTF-8',
  TransportOptions: 'connectionTimeout=300,\n readTimeout=120',
  Tables: '@OrcTable@,@BqTable@',
  AllowQuotedNewlines: 'false',
  CDDLAction: 'Process',
  adapterName: 'BigQueryWriter',
  serviceAccountKey: '@GCS-AuthPath@',
  optimizedMerge: 'true',
  StandardSQL: 'true',
  includeInsertId: 'true',
  QuoteCharacter: '\"',
  BatchPolicy: 'eventCount:100,Interval:10' )
INPUT FROM @appName@_Stream;

End application @appName@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
	filename:'JsonTargetAdd',
	directory:'@FEATURE-DIR@/logs/',
	sequence:'00',
	rolloverpolicy:'FileSizeRollingPolicy,filesize:33 + 44M'
)
format using JSONFormatter (
	members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)

input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizeAdd_actual.log') input from TypedCSVStream;
end application DSV;

--
-- Recovery Test 36 with two sources, two jumping attribute windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W/p -> CQ1 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS
--

STOP Recov36Tester.RecovTest36;
UNDEPLOY APPLICATION Recov36Tester.RecovTest36;
DROP APPLICATION Recov36Tester.RecovTest36 CASCADE;

DROP USER KStreamRecov36Tester;
DROP NAMESPACE KStreamRecov36Tester CASCADE;
CREATE USER KStreamRecov36Tester IDENTIFIED BY KStreamRecov36Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov36Tester;
CONNECT KStreamRecov36Tester KStreamRecov36Tester;

CREATE APPLICATION KStreamRecovTest36 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION KStreamRecovTest36;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
  directory: '',
  positionByEOF: false,
  WildCard: '' )
PARSE USING ParquetParser (
 )
OUTPUT TO @appname@Streams;

CREATE OR REPLACE CQ @appname@CQOrder3
INSERT INTO @appname@Stream3
SELECT
PUTUSERDATA(s,'schemaName',s.data.getSchema().getName())
FROM @appname@Streams s;

CREATE TARGET @filetarget@ USING Global.FileWriter (
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000,Interval:30s',
  directory: '',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  filename: 'AvroOutFile',
  DataEncryptionKeyPassphrase_encrypted: 'true',
  rolloverpolicy: 'EventCount:390,Interval:30m' )
FORMAT USING Global.AvroFormatter  (
  schemaFileName: 'AvroSchema',
  formatAs: 'default',
  schemaregistryConfiguration: '' )
INPUT FROM @appname@Stream3;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

Alter application app1;

CREATE OR REPLACE SOURCE s USING oracleReader  ( 
  Username:'qatest',
  Password:'qatest',
  ConnectionURL:'localhost:1521/xe',
  Tables:'QATEST.test%',
  FetchSize:2
 ) 
OUTPUT TO rawstream;
Alter application app1 recompile;



Alter application app2;
CREATE or replace TARGET app2_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Interval:60',
CommitPolicy:'Interval:60',
Tables:'qatest.test01,QATEST.KPS1'
) INPUT FROM rawstream;
Alter application app2 recompile;




Alter application app3;

CREATE or replace TARGET app3_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Interval:60',
CommitPolicy:'Interval:60',
Tables:'qatest.test02,QATEST.KPS2'
) INPUT FROM rawstream;

Alter application app3 recompile;



Alter application app4;

CREATE or replace TARGET app4_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Interval:60',
CommitPolicy:'Interval:60',
Tables:'qatest.test03,QATEST.KPS3'
) INPUT FROM rawstream;

Alter application app4 recompile;

create application KinesisTest;
CREATE OR REPLACE SOURCE ora_reader USING OracleReader (
  Username: 'miner',
  Password: 'miner',
  ConnectionURL: '192.168.1.113:1521:ORCL',
  TABLES: 'QATEST.H_REGION;QATEST.H_NATION;QATEST.H_CUSTOMER',
  FetchSize: '1'
 )
OUTPUT TO DDLCDCStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	accesskeyid:'ACCESS_KEY',
	secretaccesskey:'SECRET_KEY'
)
format using JSONFormatter (
)
input from DDLCDCStream;

end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

CREATE FLOW @STREAM@_SourceFlow;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING PostgreSQLReader  ( 
 ReplicationSlotName: 'slotname',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 ) 
OUTPUT TO @STREAM@ ;

END FLOW @STREAM@_SourceFlow;

--
-- Recovery Test 35 with two sources, two jumping count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5W/p -> CQ1 -> WS
--   S2 -> Jc6W/p -> CQ2 -> WS
--

STOP Recov35Tester.RecovTest35;
UNDEPLOY APPLICATION Recov35Tester.RecovTest35;
DROP APPLICATION Recov35Tester.RecovTest35 CASCADE;
CREATE APPLICATION RecovTest35 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION RecovTest35;

--
-- Canon Test W60
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for a partitioned sliding attribute window
--
-- S -> SWa5p -> CQ -> WS
--


UNDEPLOY APPLICATION NameW60.W60;
DROP APPLICATION NameW60.W60 CASCADE;
CREATE APPLICATION W60 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW60;

CREATE SOURCE CsvSourceW60 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW60;

END FLOW DataAcquisitionW60;




CREATE FLOW DataProcessingW60;

CREATE TYPE DataTypeW60 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW60 OF DataTypeW60;

CREATE CQ CSVStreamW60_to_DataStreamW60
INSERT INTO DataStreamW60
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW60;

CREATE WINDOW SWa5pW60
OVER DataStreamW60
KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY word;

CREATE WACTIONSTORE WactionStoreW60 CONTEXT OF DataTypeW60
EVENT TYPES ( DataTypeW60 KEY(word) )
@PERSIST-TYPE@

CREATE CQ SWa5pW60_to_WactionStoreW60
INSERT INTO WactionStoreW60
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM SWa5pW60
GROUP BY word;

END FLOW DataProcessingW60;



END APPLICATION W60;

STOP APPLICATION ORACLETOBIGQUERY;
UNDEPLOY APPLICATION ORACLETOBIGQUERY;
DROP APPLICATION ORACLETOBIGQUERY CASCADE;

--create application 
CREATE APPLICATION ORACLETOBIGQUERY RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE OracleSource USING OracleReader (
 ConnectionURL: '192.168.123.12:1521/ORCL',
 Tables: 'QATEST.ORATOBIGQALLDATATYPE',
 Username: 'qatest',
 Password: 'qatest',
 FetchSize:1
) OUTPUT TO CDCStream;

CREATE OR REPLACE TARGET bqtables using BigqueryWriter(
 BQServiceAccountConfigurationPath:"/Users/karthikmurugan/Downloads/bqtest-540227c31980.json",
 projectId:"bqtest-158706",
 Tables: "QATEST.ORATOBIGQALLDATATYPE,QATEST.ORATOBIGQALLDATATYPE",
 BatchPolicy: "eventCount:1,Interval:90")
INPUT FROM CDCStream;


CREATE OR REPLACE TARGET T1 using SysOut(name : "some text") INPUT FROM CDCStream;

END APPLICATION ORACLETOBIGQUERY;

DEPLOY APPLICATION ORACLETOBIGQUERY;
START APPLICATION ORACLETOBIGQUERY;

stop application APP_HEARTBEATS;
undeploy application APP_HEARTBEATS;
drop application APP_HEARTBEATS cascade;

CREATE APPLICATION APP_HEARTBEATS;

CREATE OR REPLACE CQ CQ_HB_90SEC 
INSERT INTO STREAM_CQ_HB_90SEC 
select makeWAEvent(dnow()) as dummy from heartbeat(interval 90 second) h;

CREATE OR REPLACE CQ CQ_HB_10SEC 
INSERT INTO STREAM_CQ_HB_10SEC 
select makeWAEvent(dnow()) as dummy from heartbeat(interval 10 second) h;

CREATE OR REPLACE CQ CQ_HB_1MIN 
INSERT INTO STREAM_CQ_HB_1MIN 
select makeWAEvent(dnow()) as dummy from heartbeat(interval 1 minute) h;

CREATE OR REPLACE CQ CQ_HB_30SEC 
INSERT INTO STREAM_CQ_HB_30SEC 
select makeWAEvent(dnow()) as dummy from heartbeat(interval 30 second) h;

CREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_90SEC OVER STREAM_CQ_HB_90SEC 
KEEP 1 ROWS;

CREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_10SEC OVER STREAM_CQ_HB_10SEC 
KEEP 1 ROWS;

CREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_1MIN OVER STREAM_CQ_HB_1MIN 
KEEP 1 ROWS;

CREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_30SEC OVER STREAM_CQ_HB_30SEC 
KEEP 1 ROWS;

END APPLICATION APP_HEARTBEATS;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@;
CREATE SOURCE  @srcName@  USING Global.OJet ( 
  Username:'@srcusername@',
  Password:'@srcpassword@', 
  Tables:'@srcschema@.@srctable@',
  ConnectionURL:'@srcurl@',
  Tables:'@srcschema@.@srctable@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries:'true'
) 
OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@ USING DatabaseWriter (
  CheckPointTable: 'CHKPOINT', 
  ReplicationSlotName:'test_slot',
  Username:'@tgtusername@',
  Password:'@tgtpassword@',
  ConnectionURL:'@tgturl@',
  adapterName:'PostgreSQLReader',
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@'
)
INPUT FROM @outstreamname@;

End APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

DROP APPLICATION ns1.OPExample cascade;
DROP NAMESPACE ns1 cascade;
CREATE OR REPLACE NAMESPACE ns1;
USE ns1;
CREATE APPLICATION OPExample;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'PosDataPreview.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
)
OUTPUT TO CsvStream;
 
CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);

CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) 
QUERY (keytomap:'merchantId') 
OF MerchantHourlyAve;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
  TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
  DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
  TO_DOUBLE(data[7]) as amount,
  TO_INT(data[9]) as zip
FROM CsvStream;
 
CREATE CQ cq2
INSERT INTO SendToOPStream
SELECT makeList(dateTime) as dateTime,
  makeList(zip) as zip
FROM PosDataStream;
 
CREATE TYPE ReturnFromOPStream_Type ( time DateTime , val Integer );
CREATE STREAM ReturnFromOPStream OF ReturnFromOPStream_Type;

CREATE TARGET OPExampleTarget 
USING FileWriter (filename: 'OPExampleOut') 
FORMAT USING JSONFormatter() 
INPUT FROM ReturnFromOPStream;

CREATE OPEN PROCESSOR testOp USING Global.TupleConverter ( lastItemSeen: 0, ahead: 1 )
INSERT INTO ReturnFromOPStream FROM SendToOPStream ENRICH WITH HourlyAveLookup;
 
END APPLICATION OPExample;

STOP OuterJoinTester.OJApp;
UNDEPLOY APPLICATION OuterJoinTester.OJApp;
DROP APPLICATION OuterJoinTester.OJApp cascade;

CREATE APPLICATION OJApp;



CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;



CREATE source wsSource2 USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'bankCards.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO stream2;



CREATE TYPE bankData
(
bankID Integer KEY,
bankName String
);

CREATE TYPE cardData
(
cardID Integer KEY,
cardName String
);


CREATE STREAM wsStream OF bankData;
CREATE STREAM wsStream2 OF cardData;

CREATE JUMPING WINDOW win1 OVER wsStream KEEP 20 rows;


CREATE JUMPING WINDOW win2 OVER stream2 KEEP 4 rows;

CREATE WACTIONSTORE oneWS CONTEXT OF bankData
EVENT TYPES(bankData )
@PERSIST-TYPE@

CREATE WACTIONSTORE twoWS CONTEXT OF cardData
EVENT TYPES(cardData )
@PERSIST-TYPE@


--Select data from QaStream and insert into wsStream

CREATE CQ csvTobankData
INSERT INTO oneWS
SELECT TO_INT(data[0]), data[1] FROM QaStream;

CREATE CQ csvTobankData2
INSERT INTO twoWS
SELECT TO_INT(data[0]), data[1] FROM stream2;


END APPLICATION OJApp;

STOP APPLICATION @Appname@;
UNDEPLOY APPLICATION @Appname@;
DROP APPLICATION @Appname@ CASCADE;
unload OPEN PROCESSOR "OP_SCM_PATH";

CREATE APPLICATION @Appname@ RECOVERY 10 SECOND INTERVAL;

Create flow AgentFlow;

CREATE  SOURCE @Appname@s USING databaseReader  (
Username: 'qatest',
Password: 'qatest',
ConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',
Tables: 'QATEST.TGT_T%')
OUTPUT TO @Appname@Stream1;

CREATE SOURCE @Appname@sd USING databaseReader  (
Username: 'qatest',
Password: 'qatest',
ConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',
Tables: 'QATEST.TGT_T%')
OUTPUT TO @Appname@Stream2;

LOAD OPEN PROCESSOR "OP_SCM_PATH";

CREATE STREAM  @Appname@outStream OF Global.WAEvent;

CREATE OPEN PROCESSOR @Appname@op USING TUPLECONVERTER (
   magicprop:"ASIA"
)
INSERT INTO @Appname@outStream
FROM @Appname@Stream1;

CREATE OR REPLACE ROUTER @Appname@router1 INPUT FROM @Appname@outStream s CASE
WHEN meta(s,"TableName").toString()='QATEST.OPA_INPUTR1' THEN ROUTE TO @Appname@ss1,
WHEN meta(s,"TableName").toString()='QATEST.OPA_INPUTR2' THEN ROUTE TO @Appname@ss2,
ELSE ROUTE TO @Appname@ss_else;


CREATE OR REPLACE ROUTER @Appname@router2 INPUT FROM @Appname@Stream2 s CASE
WHEN meta(s,"TableName").toString()='QATEST.OPA_OUTPUTR1' THEN ROUTE TO @Appname@ss1,
WHEN meta(s,"TableName").toString()='QATEST.OPA_OUTPUTR2' THEN ROUTE TO @Appname@ss2,
ELSE ROUTE TO @Appname@ss_else;

end flow AgentFlow;

create flow nflow;

create Target @Appname@Target_1 using FileWriter
(
directory: 'testApr22',
filename:'%@metadata(TableName)%',
flushpolicy:'eventCount:1000,Interval:90s',
RollOverPolicy:'eventCount:1000,Interval:90s'
)
FORMAT USING dsvFormatter ()
input from @Appname@ss1;

create Target @Appname@Target_2 using FileWriter
(
directory: 'testApr22',
filename:'%@metadata(TableName)%',
flushpolicy:'eventCount:1000,Interval:90s',
RollOverPolicy:'eventCount:1000,Interval:90s'
)
FORMAT USING dsvFormatter ()
input from @Appname@ss2;

end flow nflow;

end application @Appname@;
deploy application @Appname@ with AgentFlow in agents, nflow in default;
start @Appname@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ Recovery 5 second interval;
--create application @APPNAME@;

create type @APPNAME@employee
(
id integer,
name String,
company String
);
CREATE STREAM @APPNAME@cosmoscassandra_TypedStream of @APPNAME@employee;

CREATE OR REPLACE SOURCE @APPNAME@OnPrem_Oracle USING OracleReader  (
  Compression: false,
  StartTimestamp: 'null',
  SupportPDB: false,
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  FetchSize: 1,
  DDLCaptureMode: 'All',
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'localhost:1521:xe',
 Tables: 'QATEST.EMP%',
-- Tables: 'QATEST.OracToCql_alldatatypes',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  TransactionBufferType: 'Memory',
  DictionaryMode: 'OnlineCatalog',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '1MB'
 )
OUTPUT TO @APPNAME@kperststream ;

create stream @APPNAME@UserdataStream1 of Global.WAEvent;

Create CQ @APPNAME@CQUser
insert into @APPNAME@UserdataStream1
select putuserdata (data1,'OperationName',META(data1,'OperationName').toString()) from @APPNAME@kperststream data1;

Create CQ @APPNAME@CQUser_typed
insert into @APPNAME@cosmoscassandra_TypedStream
select 
to_int(data[0]),
data[1],
data[2]
from @APPNAME@UserdataStream1 u 
where USERDATA(u,'OperationName').toString()=='INSERT' and meta(u,'TableName').toString()="QATEST.EMP3";


CREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target1 USING CassandraCosmosDBWriter  ( 
  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --CheckPointTable: '',
  FlushPolicy: 'EventCount:1000,Interval:60',
  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',
  AccountKey:'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  --ConsistencyLevel:'fh',
  --port:'10505',
  Tables: 'test.emp_tgt',
  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',
  adapterName: 'CassandraCosmosDBWriter'
 ) 
INPUT FROM @APPNAME@cosmoscassandra_TypedStream;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR;

 CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01;striim.test02;striim.test03',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',
 startPosition: 'striim.test01=1;striim.test02=-1;%=0',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream ;

  CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

create target AzureSQLDWHTarget using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream;


CREATE TARGET log USING LOGWRITER
(
 name:log,
 filename:'LOGFILENAME'
)
INPUT FROM data_stream;


END APPLICATION IR;

deploy application IR;
start IR;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING ADLSReader ()
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE CQ @APPNAME@_CQ
INSERT INTO @APPNAME@_CQOut
SELECT
    data[0] as BusinessName,
    data[1] as MerchantId,
    data[2] as PosDataCode,
    data[3] as AccNumber,
    data[4] as DateTime,
    data[5] as ExpDate,
    data[6] as CurrencyCode,
    data[7] as AuthAmount,
    data[8] as TerminalId,
    data[9] as Zip,
    data[10] as City
FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_KafkaTarget USING Global.KafkaWriter VERSION @KAFKA_VERSION@(
  brokerAddress: '',
  Topic: '',
  Mode: 'Sync' )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_BlobTarget USING Global.AzureBlobWriter (
  containername: '',
  blobname: '',
  accountaccesskey: '',
  accountname: '',
  foldername: '' )
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_DWHTarget USING Global.BigQueryWriter (
  Tables: '',
  BatchPolicy: '',
  projectId: '',
  ServiceAccountKey: '' )
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_NoSqlTarget USING Global.MongoDBWriter (
  AuthDB: 'admin',
  ConnectionURL: '',
  Username: '',
  collections: '',
  Password: '' )
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_FileTarget USING Global.FileWriter (
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '',
  filename: '' )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_OLTPTarget USING Global.DatabaseWriter (
  ConnectionURL: '',
  Password: '',
  Username: '',
  Tables: '' )
INPUT FROM @APPNAME@_CQOut;

CREATE SOURCE @APPNAME@_KafkaSource USING KafkaReader VERSION @KAFKA_VERSION@ (
  brokerAddress: '',
  Topic: '',
  startOffset: '0' )
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@_Stream2;

CREATE OR REPLACE TARGET @APPNAME@_KafkaFileTarget USING Global.FileWriter (
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '',
  filename: '' )
FORMAT USING JSONFormatter(
members:'data')
INPUT FROM @APPNAME@_Stream2;

END APPLICATION @APPNAME@;

--
-- Recovery Test 50 is a simple Kafka test
-- Nicholas Keene WebAction, Inc.
--
-- S -> CQ -> KSu -> JWc10 -> WS
--

STOP Recov50Tester.RecovTest50;
UNDEPLOY APPLICATION Recov50Tester.RecovTest50;
DROP APPLICATION Recov50Tester.RecovTest50 CASCADE;
CREATE APPLICATION RecovTest50 RECOVERY 5 SECOND INTERVAL;

CREATE or REPLACE TYPE KafkaType(
  value java.lang.Long KEY  
);

CREATE SOURCE KafkaSource USING NumberSource ( 
  lowValue: '1',
  highValue: '1003',
  delayMillis: '10',
  delayNanos: '0',
  repeat: 'false'
 ) 
OUTPUT TO NumberSourceOut;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaStream OF KafkaType using KafkaProps;

CREATE OR REPLACE CQ KafkaStreamPopulate 
INSERT INTO KafkaStream
SELECT data[1]
FROM NumberSourceOut;

CREATE JUMPING WINDOW SizeTenWindow
OVER KafkaStream KEEP 10 ROWS;


CREATE WACTIONSTORE Wactions CONTEXT of KafkaType
@PERSIST-TYPE@

CREATE CQ WactionsPopulate
INSERT INTO Wactions
SELECT * FROM SizeTenWindow;

END APPLICATION RecovTest50;

CREATE APPLICATION DSV RECOVERY 1 SECOND INTERVAL;

CREATE FLOW agentflow;

CREATE OR REPLACE SOURCE CSVSource USING Global.FileReader (
  charset: 'UTF-8',
  adapterName: 'FileReader',
  rolloverstyle: 'Default',
  positionByEOF: false,
  WildCard: 'posdata.csv',
  blocksize: 64,
  skipbom: true,
  includesubdirectories: false,
  directory: 'Samples/AppData' )
PARSE USING Global.DSVParser (
  trimwhitespace: false,
  linenumber: '-1',
  columndelimiter: ',',
  columndelimittill: '-1',
  trimquote: true,
  ignoreemptycolumn: false,
  separator: ':',
  parserName: 'DSVParser',
  quoteset: '\"',
  handler: 'com.webaction.proc.DSVParser_1_0',
  charset: 'UTF-8',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  blockascompleterecord: false,
  rowdelimiter: '\n',
  nocolumndelimiter: false,
  headerlineno: 0,
  header: true )
OUTPUT TO CsvStream;

END FLOW agentflow;

CREATE OR REPLACE TARGET t USING Global.FileWriter (
  directory: '@TEST-DATA-PATH@',
  flushpolicy: 'EventCount:10000,Interval:30s',
  members: 'data',
  rolloveronddl: 'true',
  adapterName: 'FileWriter',
  rolloverpolicy: 'TimeIntervalRollingPolicy,rotationinterval:5s',
  filename: 'Foo' )
FORMAT USING Global.DSVFormatter  (
  quotecharacter: '\"',
  handler: 'com.webaction.proc.DSVFormatter',
  columndelimiter: ',',
  formatterName: 'DSVFormatter',
  nullvalue: 'NULL',
  usequotes: 'false',
  rowdelimiter: '\n',
  standard: 'none',
  header: 'false' )
INPUT FROM CsvStream;

END APPLICATION DSV;

stop ROLLUPMON_CDC;
undeploy application ROLLUPMON_CDC;
alter application ROLLUPMON_CDC;
CREATE or replace FLOW ROLLUPMON_CDC_flow;
Create or replace Source ROLLUPMON_CDC_Oraclesrc Using oraclereader(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',
 Tables:'QATEST.ROLLUPMON_TABLE1;QATEST.ROLLUPMON_TABLE2;QATEST.ROLLUPMON_TABLE3;QATEST.ROLLUPMON_TABLE4;QATEST.ROLLUPMON_TABLE5',
 Fetchsize:1000,
 connectionRetryPolicy:'maxRetries=4',
 _h_fetchexactrowcount: 'true'
)
Output To ROLLUPMON_CDC_OrcStrm;
END FLOW ROLLUPMON_CDC_flow;
alter application ROLLUPMON_CDC recompile;
DEPLOY APPLICATION ROLLUPMON_CDC;
start application ROLLUPMON_CDC;

STOP cacheRefresher.cacheApp;
UNDEPLOY APPLICATION cacheRefresher.cacheApp;
DROP APPLICATION cacheRefresher.cacheApp cascade;

CREATE APPLICATION cacheApp;


CREATE TYPE bankData
(
bankID Integer KEY,
bankName String
);

CREATE CACHE cache1 USING CsvReader(
directory: '@TEST-DATA-PATH@',
wildcard: 'banks_cache.csv',
header: No,
columndelimiter: ',',
trimquote: false,
positionbyEOF: false
) QUERY (keytomap:'bankID', refreshinterval:'20 second') OF bankData;



END APPLICATION cacheApp;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:@EC@'
)
format using DSVFormatter (
)
input from @STREAM@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using MySQLReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;
CREATE APPLICATION DSV;

Create type ScanResultType (
  timestamp1 String,
  rssi String
);

Create Stream ScanResultStream of ScanResultType;

CREATE  SOURCE CSVSource USING FileReader (
  directory: '@TEST-DATA-PATH@',
  WildCard: 'sample.json',
  positionByEOF: false
 )
 PARSE USING JSONParser (
  eventType: 'RollOverTester.ScanResultType',
  fieldName: 'scanresult'
 )
OUTPUT TO ScanResultStream;

CREATE TARGET KafkaSYSOUT USING FileWriter (
  filename: 'EventType',
  flushinterval: '0',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy: 'eventcount:1,sequence:00'
 )
format using JSONFormatter (
  members:'timestamp1,rssi'
)
INPUT FROM ScanResultStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/EventType_actual.log') input from ScanResultStream;

END APPLICATION DSV;

STOP application JSONFormatterTester.DSV;
undeploy application JSONFormatterTester.DSV;
drop application JSONFormatterTester.DSV cascade;

create application DSV;

create source CSVSmallPosDataSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvSmallPosDataStream;

create source CSVPosDataSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'posdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvPosDataStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVPosDataStream of CSVType;
Create Stream TypedCSVSmallPosDataStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVPosDataStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvPosDataStream;

CREATE CQ CsvToSmallPosData
INSERT INTO TypedCSVSmallPosDataStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvSmallPosDataStream;



/**
* 3.4.5.b FileWriter JsonFileSize 333MB
**/
create Target JSONFormatterFileSize using FileWriter(
  filename:'JsonTargetFS',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'FileSizeRollingPolicy,filesize:333M,sequence:00'
)
format using JSONFormatter (
  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)
input from TypedCSVPosDataStream;

/**
* 3.5.1.b FileWriter Json EventCount 2000
**/
create Target JSONFormatterEventCount using FileWriter(
  filename:'JsonTargetEC',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'eventcount:2000,sequence:00'
)
format using JSONFormatter (
  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)
input from TypedCSVSmallPosDataStream;


/**
*  FileWriter Json events as array of json objects :true.
**/

create Target JSONFormatterEventCountT using FileWriter(
  filename:'JsonFormatterPropT',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'eventcount:500000,sequence:00'
)
format using JSONFormatter (
  members:'merchantname,merchantid,zip',
  jsonObjectDelimiter:'|',
  jsonMemberDelimiter:'~',
  EventsAsArrayOfJsonObjects : true
)
input from TypedCSVSmallPosDataStream;

/**
*  FileWriter Json events as array of json objects :false.
**/

create Target JSONFormatterEventCountF using FileWriter(
  filename:'JsonFormatterPropF',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'eventcount:500000,sequence:00'
)
format using JSONFormatter (
  members:'merchantname,merchantid,zip',
  jsonObjectDelimiter:'|',
  jsonMemberDelimiter:'~',
  EventsAsArrayOfJsonObjects : false
)
input from TypedCSVSmallPosDataStream;


create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizeBig_actual.log') input from TypedCSVPosDataStream;

end application DSV;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[2]) = 'Null' ;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 SupportPDB: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.test01',
	FetchSize: '1',
	connectionRetryPolicy:'timeOut=00,retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace @APPNAME@_TARGET T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'Eventcount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true		
) INPUT FROM @APPNAME@_ss;
--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;;

Create Source OracleSource1 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source OracleSource2 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source OracleSource3 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source OracleSource4 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source OracleSource5 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

create target AzureTarget using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',  
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;
END APPLICATION ADW;
deploy application ADW;
start application ADW;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 5 Second interval;
create source @APPNAME@_Source using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'dynamicdirectory.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO @APPNAME@_Stream;

Create Type @APPNAME@_type (
  id String,
  name String,
  department String,
  yoj String,
  moj String,
  doj int
);

Create Stream @APPNAME@_typedstream of @APPNAME@_type;

CREATE CQ @APPNAME@_cq
INSERT INTO @APPNAME@_typedstream
SELECT data[0],
       data[1],
       data[2],
       data[3],
       data[4],
       TO_INT(data[5])
FROM @APPNAME@_stream;

create Target @APPNAME@_target using ADLSGen1Writer(
        filename:'',
        directory:'',
        datalakestorename:'',
        clientid:'',
        authtokenendpoint:'',
        clientkey:'',
		rolloverpolicy:'eventcount:5'
)
format using DSVFormatter (
)
input from @APPNAME@_typedstream; 

end application @APPNAME@;

deploy application @APPNAME@;
start application @APPNAME@;

CREATE APPLICATION IR recovery 5 second interval;

CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
 
  FetchSize: 5000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=t1',
 startPosition: '%=-1',
  PollingInterval: '20sec'
  )
  OUTPUT TO data_stream;
create target AzureSQLDWHTarget using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10000,interval:300s'
) INPUT FROM data_stream;
  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

END APPLICATION IR;
deploy application IR;
start IR;

CREATE or replace TARGET @TARGET_NAME@ USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:10',
StandardSQL:true	
) INPUT FROM @STREAM@;

STOP bq;
UNDEPLOY APPLICATION bq;
DROP APPLICATION bq CASCADE;

CREATE APPLICATION bq RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE BQ_source USING OracleReader
(
	Username:'qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
  OnlineCatalog:true,
  FetchSize:'1',
  Tables: 'QATEST.sourceTable'
)
OUTPUT TO SS;


CREATE or replace TARGET BQ_target USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
    Tables:'QATEST.sourceTable,qatest.% keycolumns(RONUM)',
    mode:'Appendonly',
    datalocation: 'US',
	nullmarker: 'NULL',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:100,Interval:10'	
) INPUT FROM ss;

CREATE OR REPLACE TARGET bq_SysOut USING Global.SysOut (name: 'wa') INPUT FROM ss;

END APPLICATION bq;
DEPLOY APPLICATION bq;
START APPLICATION bq;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;
Create Source oracSource
 Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;
CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

use PosTester;
DROP CQ CsvToPosData;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING AvroFormatter (
schemaFileName: '@SCHEMAFILE@'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING DSVFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

END APPLICATION @APPNAME@;

STOP APPLICATION @APP_NAME@1;
STOP APPLICATION @APP_NAME@2;

UNDEPLOY APPLICATION @APP_NAME@1;
UNDEPLOY APPLICATION @APP_NAME@2;

DROP APPLICATION @APP_NAME@1 CASCADE;
DROP APPLICATION @APP_NAME@2 CASCADE;

CREATE APPLICATION @APP_NAME@1;

CREATE OR REPLACE SOURCE @APP_NAME@1_Source USING @SOURCE_ADAPTER@  ( 
  Tables:'@SRC_TABLE@'
) OUTPUT TO @APP_NAME@_In1;

CREATE OR REPLACE TARGET @APP_NAME@1_Target USING @TARGET_ADAPTER@  ( 
  Tables: '@SRC_TABLE@, @TGT_TABLE@1', 
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  CheckPointTable: '@CHK_TABLE@', 
) INPUT FROM @APP_NAME@_In1;

CREATE TARGET @APP_NAME@_SysOut1
USING SysOut(name:@APP_NAME@1Sys)
INPUT FROM @APP_NAME@_In1;

END APPLICATION @APP_NAME@1;

DEPLOY APPLICATION @APP_NAME@1;
START APPLICATION @APP_NAME@1;


CREATE APPLICATION @APP_NAME@2;

CREATE OR REPLACE SOURCE @APP_NAME@2_Source USING @SOURCE_ADAPTER@  ( 
  Tables:'@SRC_TABLE@'
) OUTPUT TO @APP_NAME@_In2;

CREATE OR REPLACE TARGET @APP_NAME@2_Target USING @TARGET_ADAPTER@  ( 
  Tables: '@SRC_TABLE@, @TGT_TABLE@2', 
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  CheckPointTable: '@CHK_TABLE@'
) INPUT FROM @APP_NAME@_In2;

CREATE TARGET @APP_NAME@_SysOut2
USING SysOut(name:@APP_NAME@2Sys)
INPUT FROM @APP_NAME@_In2;

END APPLICATION @APP_NAME@2;

DEPLOY APPLICATION @APP_NAME@2;
START APPLICATION @APP_NAME@2;

use consoletest;
alter application noApp;

CREATE source CsvDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'posdata.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;


end application noApp;

alter application noApp recompile;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;

drop propertyset MysqlToMysqlPlatfm_App_KafkaPropset;
drop stream  MysqlToMysqlPlatfm_Stream CASCADE;


--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;
					
CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using MySQLReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNEC4TION_URL@',
  Tables: '$table1',
  adapterName: 'MySQLReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using MySQLReader( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@2',
  adapterName: 'MySQLReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'WACTION.MYSQLTOMYSQLPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using MySQLReader( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '$table2',
  adapterName: 'MySQLReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using MySQLReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'WACTION.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@4',
  adapterName: 'MySQLReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'WACTION.MYSQLTOMYSQLPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;

END APPLICATION @APPNAME@2;

stop Quiesce_CDC_BQ_TARGET;
undeploy application Quiesce_CDC_BQ_TARGET;
alter application Quiesce_CDC_BQ_TARGET;
Create or replace TARGET Quiesce_CDC_BigQueryTrg USING BigQueryWriter (
  serviceAccountKey: '@SERVICEACCOUNTKEY@',
  projectId:'@PROJECTID@',
  BatchPolicy:'Interval:10',
  _h_maxParallelStreamingRequests: '10',
  Tables:'QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE1'
)
INPUT FROM Quiesce_CDC_OrcStrm;
alter application Quiesce_CDC_BQ_TARGET recompile;
DEPLOY APPLICATION Quiesce_CDC_BQ_TARGET;
start application Quiesce_CDC_BQ_TARGET;

CREATE APPLICATION @APPNAME@;

CREATE TYPE @APPNAME@_completeRecord(
completedata com.fasterxml.jackson.databind.JsonNode);
CREATE stream @APPNAME@_CompleteRecordInJSONStream of @APPNAME@_completeRecord;

CREATE SOURCE @APPNAME@_src USING KafkaReader VERSION @KAFKA_VERSION@ ()
PARSE USING AvroParser (
schemaFileName: 'avroSchema'
)
OUTPUT TO @APPNAME@_Stream2;

CREATE CQ @APPNAME@_CQ1
 INSERT INTO @APPNAME@_CompleteRecordInJSONStream
 SELECT
 AvroToJson(y.data)
 from @APPNAME@_Stream2 y;

CREATE CQ @APPNAME@_GetNativeRecordInJSONCQ
INSERT INTO @APPNAME@_NativeRecordStream
SELECT
 completedata.get("Facility").toString() as Facility,
 completedata.get("MsgTime").toString() as MsgTime,
 completedata.get("PatientID").toString() as PatientID,
 completedata.get("OrderIdentifier").toString() as OrderIdentifier,
 completedata.get("OrderText").toString() as OrderText
FROM @APPNAME@_CompleteRecordInJSONStream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING Global.FileWriter ()
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@_NativeRecordStream;

END APPLICATION @APPNAME@;

STOP BuiltInTester.test_todate_builtinfunc;
UNDEPLOY APPLICATION BuiltInTester.test_todate_builtinfunc;
DROP APPLICATION BuiltInTester.test_todate_builtinfunc CASCADE;

CREATE APPLICATION test_todate_builtinfunc;

CREATE OR REPLACE TYPE date_cq_Type (
 gapdate org.joda.time.DateTime,
 randomdate org.joda.time.DateTime);

CREATE SOURCE date_file USING FileReader (
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  directory: '@TEST-DATA-PATH@',
  skipbom: true,
  wildcard: 'anomalyBound.csv'
 )
 PARSE USING DSVParser  (
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: '-1',
  nocolumndelimiter: false,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: true,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 )
OUTPUT TO date_file_stream;

CREATE OR REPLACE WACTIONSTORE datef_ws CONTEXT OF date_cq_Type EVENT TYPES ( date_cq_Type ) USING ( storageProvider: 'elasticsearch' );

CREATE OR REPLACE CQ date_cq INSERT INTO datef_ws SELECT TO_DATEF("1983-04-01", 'yyyy-MM-dd') as gapdate, TO_DATEF("2021-02-18", 'yyyy-MM-dd') as randomdate FROM date_file_stream d;

END APPLICATION test_todate_builtinfunc;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING DataBaseReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

END APPLICATION @APPNAME@;

Create Source @SOURCE_NAME@ Using MSSqlReader
(
 Username:'@READER-UNAME@',
 Password:'@READER-PASSWORD@',
 DatabaseName:'qatest',
 ConnectionURL:'@CDC-READER-URL@',
 Tables:@WATABLES@,
 ConnectionPoolSize:2,
 Compression:false,
 StartPosition:'EOF'
)
Output To @STREAM@;

CREATE APPLICATION ExportApp2_API;

CREATE OR REPLACE SOURCE ExportApp2_API_FileSource USING FileReader ( 
  wildcard: 'posdata.csv', 
  positionByEOF: false, 
  directory: '@testdata_dir@' ) 
PARSE USING DSVParser ( 
 ) 
OUTPUT TO ExportApp2_API_SampleStream;

CREATE OR REPLACE TARGET ExportApp2_API_NullTarget USING NullWriter ( 
 ) 
INPUT FROM ExportApp2_API_SampleStream;

END APPLICATION ExportApp2_API;

drop namespace hubspot cascade force;
create namespace hubspot;
use hubspot;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ RECOVERY 30 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;

CREATE FLOW @AppName@_SourceFlow;

CREATE SOURCE @srcName@ USING Global.HubSpotReader ( 
  PollingInterval: '1m', 
  StartPosition: '%=-1', 
  Tables: '@objectname@s__c', 
  ThreadPoolCount: '10', 
  ConnectionPoolSize: '20', 
  ClientSecret_encrypted: 'false', 
  RefreshToken: '', 
  RefreshToken_encrypted: 'false', 
  Mode: 'automated', 
  useConnectionProfile: false,
  AuthMode: 'PrivateAppToken', 
  PrivateAppToken: '@accesstoken@', 
  ClientSecret: '', 
  ClientId: '', 
  PrivateAppToken_encrypted: 'false', 
  MigrateSchema: true ) 
OUTPUT TO @outstreamname@;

END FLOW @AppName@_SourceFlow;

CREATE TARGET @tgtName@ USING Global.BigQueryWriter ( 
  projectId: '@projectId@',
  batchPolicy: 'eventcount:10000,interval:2', 
  streamingUpload: 'true', 
  Mode: 'MERGE', 
  CDDLAction : 'process',
  CDDLOptions: '{\"CreateTable\":{\"action\":\"IgnoreIfExists\",\"options\":[{\"CreateSchema\":{\"action\":\"IgnoreIfExists\"}}]}}', 
  ServiceAccountKey: '@keyFileName@', 
  Tables: '%,@tgtschema@.%' ) 
INPUT FROM @instreamname@;

End APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop application ps1;
stop application ps2;
undeploy application ps1;
undeploy application ps2;
drop application ps1 cascade;
drop application ps2 cascade;
CREATE application ps1 recovery 5 second interval;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',
acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

create type type1(
  companyName String,
  merchantId String,
  city string
);

CREATE STREAM kps1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM kps2 OF Global.waevent persist using KafkaPropset;
CREATE STREAM kps3 OF Global.waevent persist using KafkaPropset;
CREATE STREAM kps4 OF Global.waevent persist using KafkaPropset;
CREATE STREAM kps5_typedStream OF type1 partition by city persist using KafkaPropset;

create source s using FileReader (
        directory:'/Users/saranyad/Product/IntegrationTests/TestData',
        wildcard:'posdata.csv',
        positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:',',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO kps;

CREATE CQ cq1
INSERT INTO kps1
SELECT *
FROM kps;

CREATE CQ cq2
INSERT INTO kps2
SELECT *
FROM kps;

CREATE CQ cq3
INSERT INTO kps3
SELECT *
FROM kps;

CREATE CQ cq4
INSERT INTO kps4
SELECT *
FROM kps;


CREATE CQ cq5
INSERT INTO kps5_typedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantId,
TO_STRING(data[10]) as city
FROM kps;

end application ps1;

CREATE application ps2 recovery 5 second interval;

create type type2(
  companyName String,
  merchantId String,
  city string
);

CREATE STREAM kps1_typedStream OF type2 partition by city;
CREATE STREAM kps2_typedStream OF type2 partition by city;
CREATE STREAM kps3_typedStream OF type2 partition by city;
CREATE STREAM kps4_typedStream OF type2 partition by city;

CREATE CQ cq6
INSERT INTO kps1_typedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantId,
TO_STRING(data[10]) as city
FROM kps1;

CREATE CQ cq7
INSERT INTO kps2_typedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantId,
TO_STRING(data[10]) as city
FROM kps2;

CREATE CQ cq8
INSERT INTO kps3_typedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantId,
TO_STRING(data[10]) as city
FROM kps3;

CREATE CQ cq9
INSERT INTO kps4_typedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantId,
TO_STRING(data[10]) as city
FROM kps4;

CREATE TARGET target1 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'QATEST.KPS1'
) INPUT FROM kps1_typedStream;

CREATE TARGET target2 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'QATEST.KPS2'
) INPUT FROM kps2_typedStream;

CREATE TARGET target3 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'QATEST.KPS3'
) INPUT FROM kps3_typedStream;

CREATE TARGET target4 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'QATEST.KPS4'
) INPUT FROM kps4_typedStream;

CREATE TARGET target5 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'QATEST.KPS5'
) INPUT FROM kps5_typedStream;

end application ps2;
--DEPLOY APPLICATION KafkaWriterApp with sourceFlow  ON ANY IN DEFAULT, targetFlow ON ALL IN DEFAULT;

stop application PCAPTester.PCAPTest;
undeploy application PCAPTester.PCAPTest;
drop application PCAPTester.PCAPTest cascade;

CREATE APPLICATION PCAPTest;


    CREATE OR REPLACE TYPE PCAPData (
      ts DateTime,
      srcIp String,
      srcPort String,
      dstIp String,
      dstPort String,
      connection String
    );
    
    CREATE OR REPLACE SOURCE PCAPSource USING PCAPReader ( 
      snaplen: '65536',
      wildcard: false,
      directory: '@TEST-DATA-PATH@',
      file: 'pcapTest.pcap',
      library: '/usr/lib/libpcap.dylib',
      live: false 
     )
    OUTPUT TO PCAPOut;
    
    CREATE OR REPLACE STREAM PCAPDataStream OF PCAPData;

    CREATE OR REPLACE CQ GetPCAPData
    INSERT INTO PCAPDataStream
    SELECT TO_DATE(ts), 
           srcAddress,
           TO_STRING(srcPort),
           dstAddress,
           TO_STRING(dstPort),
           transportType + '/' + srcAddress + ':' + srcPort + '/' + dstAddress + ':' + dstPort
    FROM PCAPOut;
    



END APPLICATION PCAPTest;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE OR REPLACE APPLICATION @APPNAME@ recovery 5 second interval;

CREATE FLOW @APPNAME@_Agent_flow;

CREATE OR REPLACE SOURCE @SourceName@1 USING Global.MSJet (
  Tables: 'dbo.compsrc',
    username: 'qatest',
    DatabaseName: 'qatest',
    FetchTransactionMetadata: true,
    filterTransactionBoundaries: true,
    compression: false,
    ConnectionURL: '10.211.55.3:1433',
    CommittedTransactions: true,
    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
    SendBeforeImage: true,
    password: 'w3b@ct10n' )
OUTPUT TO @SRCINPUTSTREAM@;

END FLOW @APPNAME@_Agent_flow;

CREATE FLOW @APPNAME@_Agent_flow1;

CREATE OR REPLACE SOURCE @SourceName@2 USING Global.MSJet (
  Tables: 'dbo.compsrc',
    username: 'qatest',
    DatabaseName: 'qatest',
    FetchTransactionMetadata: true,
    filterTransactionBoundaries: true,
    compression: false,
    ConnectionURL: '10.211.55.3:1433',
    CommittedTransactions: true,
    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
    SendBeforeImage: true,
    password: 'w3b@ct10n' )
OUTPUT TO @SRCINPUTSTREAM@1;

END FLOW @APPNAME@_Agent_flow1;

CREATE FLOW @APPNAME@_server_flow;

CREATE OR REPLACE TARGET @targetName@1 USING Global.DatabaseWriter
(
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  BatchPolicy:'EventCount:10,Interval:60',
  CommitPolicy:'EventCount:10,Interval:60',
  ParallelThreads:'',
  CheckPointTable:'CHKPOINT',
  Password_encrypted:'false',
  Tables:'qatest.MSJEtsrc1,qatest.MSJEtar1;qatest.MSJEtsrc2,qatest.MSJEtar2;',
  CDDLAction:'Process',
  Password:'w3b@ct10n',
  StatementCacheSize:'50',
  ConnectionURL:'jdbc:sqlserver://10.211.55.3:1433;databaseName=qatest',
  DatabaseProviderType:'Default',
  Username:'qatest',
  PreserveSourceTransactionBoundary:'false',
  adapterName:'DatabaseWriter'
)
INPUT FROM @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@2 USING Global.DatabaseWriter
(
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  BatchPolicy:'EventCount:10,Interval:60',
  CommitPolicy:'EventCount:10,Interval:60',
  ParallelThreads:'',
  CheckPointTable:'CHKPOINT',
  Password_encrypted:'false',
  Tables:'qatest.MSJEtsrc1,qatest.MSJEtar1;qatest.MSJEtsrc2,qatest.MSJEtar2;',
  CDDLAction:'Process',
  Password:'w3b@ct10n',
  StatementCacheSize:'50',
  ConnectionURL:'jdbc:sqlserver://10.211.55.3:1433;databaseName=qatest',
  DatabaseProviderType:'Default',
  Username:'qatest',
  PreserveSourceTransactionBoundary:'false',
  adapterName:'DatabaseWriter'
)
INPUT FROM @SRCINPUTSTREAM@1;

CREATE TARGET @targetsys@ USING Global.SysOut (
  name: '@targetsys@' )
INPUT FROM @SRCINPUTSTREAM@;

END FLOW @APPNAME@_server_flow;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@ with @APPNAME@_Agent_flow in AGENTS, @APPNAME@_Agent_flow1 in AGENTS ,@APPNAME@_server_flow on any in default;
START APPLICATION @APPNAME@;

STOP APPLICATION HW ;
undeploy application HW ;
drop application HW cascade;

CREATE APPLICATION HW Recovery 5 second interval;

CREATE  SOURCE S USING OracleReader  ( 
  Username: 'miner',
  Password: '@miner',
  ConnectionURL: '@conn-url@',
  Tables: '@src@',
  FetchSize: 1) 
OUTPUT TO hivestream;

Create Target T using ClouderaHiveWriter (
  ConnectionURL:'@hive-url@',
  Username:'@uname@', 
            Password:'@pwd@',
            hadoopurl:'hdfs://dockerhost:9000/',
	        Mode:'incremental',
	        mergepolicy: 'eventcount:5,interval:1s',
            Tables:'@tgt-table@',
            hadoopConfigurationPath:'/Users/saranyad/Documents/hello/'
 )
INPUT FROM hivestream;


END APPLICATION HW;
deploy application HW on all in default;

Start application HW;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.test01',
	FetchSize: '1',
	connectionRetryPolicy:'timeOut=00,retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'Eventcount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true		
) INPUT FROM @APPNAME@_ss;
--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.e1ptest%',
	FetchSize: '1'
)
OUTPUT TO @APPNAME@_SS;


CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.e1ptest%,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12)',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'Interval:2',
StandardSQL:true,
optimizedMerge:true		
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

Create Source @SOURCE_NAME@
 Using DatabaseReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
 Tables:'QATEST.DATE_TIME',
 --Query:"SELECT * FROM qatest.oracle_alldatatypes",
 FetchSize:10000,
 QueueSize:2048,
) Output To @STREAM@;

STOP APPLICATION testApp;
UNDEPLOY APPLICATION testApp;
DROP APPLICATION testApp CASCADE;
-- DROP EXCEPTIONSTORE testApp_exceptionstore;

CREATE APPLICATION testApp WITH ENCRYPTION RECOVERY 10 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE OR REPLACE SOURCE testApp_Source USING OracleReader  (
  Username:'qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
  OnlineCatalog:true,
  FetchSize:'1',
  Tables: 'QATEST.sourceTable'
  ) OUTPUT TO testApp_Stream  ;

CREATE OR REPLACE TARGET testApp_Target USING BigQueryWriter  (
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  projectId:'striimqa-214712',
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:5,Interval:120',
  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', 
  AllowQuotedNewLines: 'false', 
  adapterName: 'BigQueryWriter', 
  optimizedMerge: 'true', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', 
  StandardSQL: 'true', 
  QuoteCharacter: '\"', 
  Tables: 'QATEST.sourceTable,transOption_test.targettable',
  Mode: 'MERGE',
  StandardSQL: 'true',
  _h_TransportOptions:'connectionTimeout=30s, readTimeout=12s',
  QuoteCharacter: '\"'
  ) INPUT FROM testApp_Stream;

CREATE OR REPLACE TARGET testApp_SysOut USING Global.SysOut (name: 'wa') INPUT FROM testApp_Stream;

END APPLICATION testApp;
DEPLOY APPLICATION testApp;
START testApp;

stop APPLICATION MultiLogApp;
undeploy APPLICATION MultiLogApp;
drop APPLICATION MultiLogApp cascade;

CREATE APPLICATION MultiLogApp;

-- This sample application shows how Striim could be used monitor and correlate logs 
-- from web and application server logs from the same web application. See the discussion 
-- in the "Sample Applications" section of the Striim documentation for additional 
-- discussion.


CREATE FLOW MonitorLogs;

-- MonitorLogs sets up the two log sources used by this application. In a real-world
--implementation, each source could be reading many logs from many servers.

-- The web server logs are in Apache NCSA extended/ combined log format plus response time:
-- "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-agent}i\" %D"
-- See apache.org for more information.

CREATE SOURCE AccessLogSource USING FileReader (
  directory:'Samples/Customer/MultiLogApp/appData',
  wildcard:'access_log',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~',
  trimwhitespace:true
)
OUTPUT TO RawAccessStream;

CREATE TYPE AccessLogEntry (
    srcIp String KEY,
    userId String,
    sessionId String,
    accessTime DateTime,
    request String,
    code integer,
    size integer,
    referrer String,
    userAgent String,
    responseTime integer
);
CREATE STREAM AccessStream OF AccessLogEntry;

CREATE CQ ParseAccessLog 
INSERT INTO AccessStream
SELECT data[0], data[2], MATCH(data[4], ".*jsessionId=(.*) "),
       TO_DATE(data[3], "dd/MMM/yyyy:HH:mm:ss.SSS Z"), data[4], TO_INT(data[5]), TO_INT(data[6]),
       data[7], data[8], TO_INT(data[9])
FROM RawAccessStream;

-- The application server logs are in Apache's Log4J format.

CREATE SOURCE Log4JSource USING FileReader (
  directory:'Samples/Customer/MultiLogApp/appData',
  wildcard:'log4jLog.xml',
  positionByEOF:false
) 
PARSE USING XMLParser(
  rootnode:'/log4j:event',
  columnlist:'log4j:event/@timestamp,log4j:event/@level,log4j:event/log4j:message,log4j:event/log4j:throwable,log4j:event/log4j:locationInfo/@class,log4j:event/log4j:locationInfo/@method,log4j:event/log4j:locationInfo/@file,log4j:event/log4j:locationInfo/@line'
)
OUTPUT TO RawXMLStream;

CREATE TYPE Log4JEntry (
  logTime DateTime,
  level String,
  message String,
  api String,
  sessionId String,
  userId String,
  sobject String,
  xception String,
  className String,
  method String,
  fileName String,
  lineNum String
);
CREATE STREAM Log4JStream OF Log4JEntry;

CREATE CQ ParseLog4J
INSERT INTO Log4JStream
SELECT TO_DATE(TO_LONG(data[0])), data[1], data[2], 
       MATCH(data[2], '\\\\[api=([a-zA-Z0-9]*)\\\\]'),
       MATCH(data[2], '\\\\[session=([a-zA-Z0-9\\-]*)\\\\]'),
       MATCH(data[2], '\\\\[user=([a-zA-Z0-9\\-]*)\\\\]'),
       MATCH(data[2], '\\\\[sobject=([a-zA-Z0-9]*)\\\\]'),
       data[3], data[4], data[5], data[6], data[7]
FROM RawXMLStream;

END FLOW MonitorLogs;


CREATE FLOW ErrorsAndWarnings;

-- ErrorsAndWarnings creates a sliding window (Log4JErrorWarningActivity) containing 
-- the 300 most recent errors and warnings in the application server log. The 
-- ZeroContentCheck and LargeRTCheck flows join events from this window with access log 
-- events.

-- The type Log4JEntry was already defined by the MonitorLogs flow.
CREATE STREAM Log4ErrorWarningStream OF Log4JEntry;

CREATE CQ GetLog4JErrorWarning
INSERT INTO Log4ErrorWarningStream
SELECT l FROM Log4JStream l
WHERE l.level = 'ERROR' OR l.level = 'WARN';

CREATE WINDOW Log4JErrorWarningActivity 
OVER Log4ErrorWarningStream KEEP 300 ROWS;

END FLOW ErrorsAndWarnings;


-- HackerCheck sends an alert when an access log srcIp value is on a blacklist.

CREATE FLOW HackerCheck;

CREATE TYPE IPEntry (
    ip String
);

/* CREATE CACHE BlackListLookup using CSVReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'multiLogBlackList.txt',
  header: No,
  columndelimiter: ','
) QUERY (keytomap:'ip') OF IPEntry; */

CREATE CACHE BlackListLookup using FileReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'multiLogBlackList.txt'
)
PARSE USING DSVParser ( )
QUERY (keytomap:'ip') OF IPEntry;


CREATE STREAM HackerStream OF AccessLogEntry;

CREATE CQ FindHackers
INSERT INTO HackerStream
SELECT ale 
FROM AccessStream ale, BlackListLookup bll
WHERE ale.srcIp = bll.ip;

CREATE TYPE UnusualContext (
    typeOfActivity String,
    accessTime DateTime,
    accessSessionId String,
    srcIp String KEY,
    userId String,
    country String,
    city String,
    lat double,
    lon double
);
CREATE TYPE MergedEntry (
    accessTime DateTime,
    accessSessionId String,
    srcIp String KEY,
    userId String,
    request String,
    code integer,
    size integer,
    referrer String,
    userAgent String,
    responseTime integer,
    logTime DateTime,
    logSessionId String,
    level String,
    message String,
    api String,
    sobject String,
    xception String,
    className String,
    method String,
    fileName String,
    lineNum String
);
CREATE WACTIONSTORE UnusualActivity 
CONTEXT OF UnusualContext 
EVENT TYPES (MergedEntry, AccessLogEntry)
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ GenerateHackerContext
INSERT INTO UnusualActivity
SELECT 'HackAttempt', accessTime, sessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM HackerStream
LINK SOURCE EVENT;

CREATE STREAM HackingAlertStream OF Global.AlertEvent;

CREATE CQ SendHackingAlerts 
INSERT INTO HackingAlertStream 
SELECT 'HackingAlert', ''+accessTime, 'warning', 'raise', 
        'Possible Hacking Attempt from ' + srcIp + ' in ' + IP_COUNTRY(srcIp)
FROM HackerStream; 

CREATE SUBSCRIPTION HackingAlertSub USING WebAlertAdapter( ) INPUT FROM HackingAlertStream;

END FLOW HackerCheck;


-- LargeRTCheck sends an alert when an access log responseTime value exceeds 2000 
-- microseconds.

CREATE FLOW LargeRTCheck;

CREATE STREAM LargeRTStream of AccessLogEntry;

CREATE CQ FindLargeRT
INSERT INTO LargeRTStream
SELECT ale
FROM AccessStream ale
WHERE ale.responseTime > 2000;

CREATE WINDOW LargeRTActivity 
OVER LargeRTStream KEEP 100 ROWS;

CREATE STREAM LargeRTAPIStream OF MergedEntry;

CREATE CQ MergeLargeRTAPI
INSERT INTO LargeRTAPIStream
SELECT lrt.accessTime, lrt.sessionId, lrt.srcIp, lrt.userId, lrt.request, 
       lrt.code, lrt.size, lrt.referrer, lrt.userAgent, lrt.responseTime,
       log4j.logTime, log4j.sessionId, log4j.level, log4j.message, log4j.api, log4j.sobject, log4j.xception, 
       log4j.className, log4j.method, log4j.fileName, log4j.lineNum
FROM LargeRTActivity lrt, Log4JErrorWarningActivity log4j
WHERE lrt.sessionId = log4j.sessionId
      AND lrt.accessTime = log4j.logTime;   

CREATE CQ GenerateLargeRTContext
INSERT INTO UnusualActivity
SELECT 'LargeResponseTime', accessTime, accessSessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM LargeRTAPIStream
LINK SOURCE EVENT;

CREATE STREAM LargeRTAlertStream OF Global.AlertEvent;

CREATE CQ SendLargeRTAlerts 
INSERT INTO LargeRTAlertStream 
SELECT 'LargeRTAlert', ''+accessTime, 'warning', 'raise', 
        'Long response time for call from ' + userId + ' api ' + api + ' message ' + message
FROM LargeRTAPIStream; 

CREATE SUBSCRIPTION LargeRTAlertSub USING WebAlertAdapter( ) INPUT FROM LargeRTAlertStream;

END FLOW LargeRTCheck;


-- ProxyCheck sends an alert when an access log srcIP value is on a list of suspicious 
-- proxies.

CREATE FLOW ProxyCheck;

/* CREATE CACHE ProxyLookup using CSVReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'multiLogProxies.txt',
  header: No,
  columndelimiter: ','
) QUERY (keytomap:'ip') OF IPEntry; */

CREATE CACHE ProxyLookup using FileReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'multiLogProxies.txt'
)
PARSE USING DSVParser ( )
QUERY (keytomap:'ip') OF IPEntry; 


CREATE STREAM ProxyStream OF AccessLogEntry;

CREATE CQ FindProxies
INSERT INTO ProxyStream
SELECT ale 
FROM AccessStream ale, ProxyLookup pl
WHERE ale.srcIp = pl.ip;

CREATE CQ GenerateProxyContext
INSERT INTO UnusualActivity
SELECT 'ProxyAccess', accessTime, sessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM ProxyStream
LINK SOURCE EVENT;


CREATE STREAM ProxyAlertStream OF Global.AlertEvent;

CREATE CQ SendProxyAlerts 
INSERT INTO ProxyAlertStream 
SELECT 'ProxyAlert', ''+accessTime, 'warning', 'raise', 
        'Possible use of Proxy from ' + srcIp + ' in ' + IP_COUNTRY(srcIp) + ' for user ' + userId 
FROM ProxyStream; 

CREATE SUBSCRIPTION ProxyAlertSub USING WebAlertAdapter( ) INPUT FROM ProxyAlertStream;

END FLOW ProxyCheck;


-- ZeroContentCheck sends an alert when an access log entry's code value is 200 (that is,
-- the HTTP request succeeded) but the size value is 0 (the return had no content).

CREATE FLOW ZeroContentCheck;

CREATE STREAM ZeroContentStream of AccessLogEntry;

CREATE CQ FindZeroContent
INSERT INTO ZeroContentStream
SELECT ale
FROM AccessStream ale
WHERE ale.code = 200 AND ale.size = 0;

CREATE WINDOW ZeroContentActivity 
OVER ZeroContentStream KEEP 100 ROWS;

CREATE STREAM ZeroContentAPIStream OF MergedEntry;

CREATE CQ MergeZeroContentAPI
INSERT INTO ZeroContentAPIStream
SELECT zcs.accessTime, zcs.sessionId, zcs.srcIp, zcs.userId, zcs.request, 
       zcs.code, zcs.size, zcs.referrer, zcs.userAgent, zcs.responseTime,
       log4j.logTime, log4j.sessionId, log4j.level, log4j.message, log4j.api, log4j.sobject, log4j.xception, 
       log4j.className, log4j.method, log4j.fileName, log4j.lineNum
FROM ZeroContentActivity zcs, Log4JErrorWarningActivity log4j
WHERE zcs.sessionId = log4j.sessionId
      AND zcs.accessTime = log4j.logTime;   

CREATE CQ GenerateZeroContentContext
INSERT INTO UnusualActivity
SELECT 'ZeroContent', accessTime, accessSessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM ZeroContentAPIStream
LINK SOURCE EVENT;


CREATE TYPE ZeroContentEventListType (
    srcIp String KEY,
    code integer,
    size integer,
    level String,
    message String,
    xception String);
    
CREATE WACTIONSTORE ZeroContentEventList
CONTEXT OF ZeroContentEventListType 
EVENT TYPES (ZeroContentEventListType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );


CREATE CQ GenerateZeroContentEventList
INSERT INTO ZeroContentEventList
SELECT srcIp, code, size, level, message, xception
FROM ZeroContentAPIStream;


CREATE STREAM ZeroContentAlertStream OF Global.AlertEvent;

CREATE CQ SendZeroContentAlerts 
INSERT INTO ZeroContentAlertStream 
SELECT 'ZeroContentAlert', ''+accessTime, 'warning', 'raise', 
        'Zero content returned in call from ' + userId + ' api ' + api + ' message ' + message
FROM ZeroContentAPIStream; 

CREATE SUBSCRIPTION ZeroContentAlertSub USING WebAlertAdapter( ) INPUT FROM ZeroContentAlertStream;

END FLOW ZeroContentCheck;


-- ErrorHandling is functionally identical to ErrorFlow.SaasMonitorApp. It sends an alert 
-- when an error message appears in the application server log.

CREATE FLOW ErrorHandling;

CREATE STREAM ErrorStream OF Log4JEntry;

CREATE CQ GetErrors 
INSERT INTO ErrorStream 
SELECT log4j 
FROM Log4ErrorWarningStream log4j WHERE log4j.level = 'ERROR';

CREATE STREAM ErrorAlertStream OF Global.AlertEvent;

CREATE CQ SendErrorAlerts 
INSERT INTO ErrorAlertStream 
SELECT 'ErrorAlert', ''+logTime, 'error', 'raise', 'Error in log ' + message 
FROM ErrorStream;

CREATE SUBSCRIPTION ErrorAlertSub USING WebAlertAdapter( ) INPUT FROM ErrorAlertStream;

END FLOW ErrorHandling;


-- WarningHandling is a minor variation on WarningFlow.SaasMonitorApp. It sends an alert 
-- once an hour with the count of warnings for each api call for which there has been at 
-- least one alert.

CREATE FLOW WarningHandling;

CREATE STREAM WarningStream OF Log4JEntry;

CREATE CQ GetWarnings 
INSERT INTO WarningStream 
SELECT log4j 
FROM Log4ErrorWarningStream log4j WHERE log4j.level = 'WARN';

CREATE JUMPING WINDOW WarningWindow 
OVER WarningStream KEEP WITHIN 60 MINUTE ON logTime;

CREATE STREAM WarningAlertStream OF Global.AlertEvent;

CREATE CQ SendWarningAlerts 
INSERT INTO WarningAlertStream 
SELECT 'WarningAlert', ''+logTime, 'warning', 'raise', 
        COUNT(logTime) + ' Warnings in log for api ' + api 
FROM WarningWindow 
GROUP BY api 
HAVING count(logTime) > 1;

CREATE SUBSCRIPTION WarningAlertSub USING WebAlertAdapter( ) INPUT FROM WarningAlertStream;

END FLOW WarningHandling;


-- InfoFlow is functionally similar to InfoFlow.SaasMonitorApp. Its output is used by 
-- ApiFlow, CompanyApiFlow, and UserApiFlow.

CREATE FLOW InfoFlow;

CREATE TYPE UserInfo (
  userId String, 
  userName String, 
  company String,  
  userZip String,  
  companyZip String
);

/* CREATE CACHE MLogUserLookup using CSVReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'multiLogUser.csv',
  header: No,
  columndelimiter: ','
) QUERY (keytomap:'userId') OF UserInfo; */

CREATE CACHE MLogUserLookup using FileReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'multiLogUser.csv'
)
PARSE USING DSVParser ( )
QUERY (keytomap:'userId') OF UserInfo;

CREATE STREAM InfoStream OF Log4JEntry;

CREATE CQ GetInfo 
INSERT INTO InfoStream 
SELECT log4j 
FROM Log4JStream log4j WHERE log4j.level = 'INFO';

CREATE TYPE ApiCall (
  userId String, 
  api String, 
  sobject String, 
  logTime DateTime, 
  userName String, 
  company String,  
  userZip String,  
  companyZip String
);
CREATE STREAM ApiEnrichedStream OF ApiCall;

CREATE CQ GetUserDetails 
INSERT INTO ApiEnrichedStream 
SELECT a.userId, a.api, a.sobject, a.logTime, u.userName, u.company, u.userZip, u.companyZip 
FROM InfoStream a, MLogUserLookup u 
WHERE a.userId = u.userId;

END FLOW InfoFlow;


-- ApiFlow populates the dashboard's Detail - ApiActivity page and the pie chart on the 
-- Overview page.

CREATE FLOW ApiFlow;

CREATE TYPE ApiUsage (
  api String key, 
  sobject String, 
  count integer, 
  logTime DateTime
);

CREATE TYPE ApiContext (
  api String key, 
  count integer, 
  logTime DateTime
);

CREATE WACTIONSTORE ApiActivity 
CONTEXT OF ApiContext 
EVENT TYPES (ApiUsage )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE JUMPING WINDOW ApiWindow 
OVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime 
PARTITION BY api;

CREATE STREAM ApiUsageStream OF ApiUsage;

CREATE CQ GetApiUsage 
INSERT INTO ApiUsageStream 
SELECT a.api, a.sobject, 
       COUNT(a.userId), FIRST(a.logTime) 
FROM ApiWindow a 
GROUP BY a.api, a.sobject HAVING FIRST(a.logTime) IS NOT NULL;

CREATE JUMPING WINDOW ApiSummaryWindow 
OVER ApiUsageStream KEEP WITHIN 1 HOUR ON logTime 
PARTITION BY api;

CREATE CQ GetApiSummaryUsage 
INSERT INTO ApiActivity 
SELECT a.api,  
       sum(a.count), first(a.logTime)
FROM ApiSummaryWindow a 
GROUP BY a.api
LINK SOURCE EVENT;

END FLOW ApiFlow;


-- CompanyApiFlow populates the dashboard's Detail - CompanyApiActivity page and the bar 
-- chart on the Overview page. It also sends an alert when an API call is used by a 
-- company more than 1500 times during the flow's one-hour jumping window.

CREATE FLOW CompanyApiFlow;

CREATE TYPE CompanyApiUsage (
  company String key, 
  companyZip String, 
  companyLat double, 
  companyLong double, 
  api String, 
  count integer, 
  unusual integer,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE TYPE CompanyApiContext (
  company String key, 
  companyZip String, 
  companyLat double, 
  companyLong double, 
  count integer, 
  unusual integer,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE WACTIONSTORE CompanyApiActivity 
CONTEXT OF CompanyApiContext 
EVENT TYPES (CompanyApiUsage )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE JUMPING WINDOW CompanyApiWindow 
OVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime 
PARTITION BY company;

CREATE STREAM CompanyApiUsageStream OF CompanyApiUsage;

CREATE TYPE MLogUSAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

/*CREATE CACHE MLogZipLookup using CSVReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'USAddresses.txt',
  header: Yes,
  columndelimiter: ','
) QUERY (keytomap:'zip') OF MLogUSAddressData; */

CREATE CACHE MLogZipLookup using FileReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'USAddresses.txt'
)
PARSE USING DSVParser (
  header: Yes
)
QUERY (keytomap:'zip') OF MLogUSAddressData;


CREATE CQ GetCompanyApiUsage 
INSERT INTO CompanyApiUsageStream 
SELECT a.company, a.companyZip, z.latVal, z.longVal, 
       a.api, COUNT(a.sobject), 
       CASE WHEN COUNT(a.sobject) > 1500 THEN 1
            ELSE 0 END,
       CASE WHEN COUNT(a.sobject) > 1500 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.sobject),
       FIRST(a.logTime) 
FROM CompanyApiWindow a, MLogZipLookup z 
WHERE a.companyZip = z.zip 
GROUP BY a.company, a.api HAVING FIRST(a.logTime) IS NOT NULL;

CREATE JUMPING WINDOW CompanyWindow 
OVER CompanyApiUsageStream KEEP WITHIN 1 HOUR ON logTime 
PARTITION BY company;

CREATE CQ GetCompanyUsage 
INSERT INTO CompanyApiActivity 
SELECT a.company, a.companyZip, a.companyLat, a.companyLong, 
       SUM(a.count), SUM(a.unusual), 
       CASE WHEN SUM(a.unusual) > 0 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.topObject),
       FIRST(a.logTime) 
FROM CompanyWindow a 
GROUP BY a.company
LINK SOURCE EVENT;

CREATE STREAM CompanyAlertStream OF Global.AlertEvent;

CREATE CQ SendCompanyApiAlerts 
INSERT INTO CompanyAlertStream 
SELECT 'CompanyAPIAlert', ''+logTime, 'warning', 'raise', 
       'Company ' + company + ' has used api ' + api + ' ' + count + ' times for ' + topObject 
FROM CompanyApiUsageStream 
WHERE unusual = 1;

CREATE SUBSCRIPTION CompanyAlertSub USING WebAlertAdapter( ) INPUT FROM CompanyAlertStream;

END FLOW CompanyApiFlow;


-- UserApiFlow populates the dashboard's Detail - UserApiActivity page and the US map on 
-- the Overview page. It also sends an alert when an API call is used by a user more than 
-- 125 times during the flow's one-hour window.

CREATE FLOW UserApiFlow;

CREATE TYPE UserApiUsage (
  userId String key, 
  userName String, 
  userZip String, 
  userLat double, 
  userLong double, 
  company String, 
  api String, 
  count integer, 
  unusual integer,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE TYPE UserApiContext (
  userId String key, 
  userName String, 
  userZip String, 
  userLat double, 
  userLong double, 
  company String, 
  count integer, 
  unusual integer,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE WACTIONSTORE UserApiActivity
CONTEXT OF UserApiContext 
EVENT TYPES (  UserApiUsage ) 
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE JUMPING WINDOW UserApiWindow 
OVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY userId;

CREATE STREAM UserApiUsageStream OF UserApiUsage;

CREATE CQ GetUserApiUsage 
INSERT INTO UserApiUsageStream 
SELECT a.userId, a.userName, a.userZip, z.latVal, z.longVal, a.company,
       a.api, COUNT(a.sobject), 
       CASE WHEN COUNT(a.sobject) > 125 THEN 1
            ELSE 0 END,
       CASE WHEN COUNT(a.sobject) > 125 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.sobject),
       FIRST(a.logTime) 
FROM UserApiWindow a, MLogZipLookup z 
WHERE a.userZip = z.zip 
GROUP BY a.userId, a.api HAVING FIRST(a.logTime) IS NOT NULL;

CREATE JUMPING WINDOW UserWindow 
OVER UserApiUsageStream KEEP WITHIN 1 HOUR ON logTime 
PARTITION BY userId;

CREATE CQ GetUserUsage 
INSERT INTO UserApiActivity 
SELECT a.userId, a.userName, a.userZip, a.userLat, a.userLong, 
       a.company, SUM(a.count), SUM(a.unusual), 
       CASE WHEN SUM(a.unusual) > 0 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.topObject),
       FIRST(a.logTime) 
FROM UserWindow a 
GROUP BY a.userId
LINK SOURCE EVENT;

CREATE STREAM UserAlertStream OF Global.AlertEvent;

CREATE CQ SendUserApiAlerts 
INSERT INTO UserAlertStream 
SELECT 'UserAPIAlert', ''+logTime, 'warning', 'raise', 
       'User ' + userName + ' has used api ' + api + ' ' + count + ' times for ' + topObject 
FROM UserApiUsageStream 
WHERE unusual = 1;

CREATE SUBSCRIPTION UserAlertSub USING WebAlertAdapter( ) INPUT FROM UserAlertStream;

END FLOW UserApiFlow;

END APPLICATION MultiLogApp;

Deploy application MultiLogApp;
Start application MultiLogApp;

--
-- Recovery Test 99
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov99Tester.RecovTest99;
UNDEPLOY APPLICATION Recov99Tester.RecovTest99;
DROP APPLICATION Recov99Tester.RecovTest99 CASCADE;
CREATE APPLICATION RecovTest99 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW BasicComponentTestsFlow;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

-- unpartitioned windows

CREATE WINDOW DataStreamSliding3U
OVER DataStream KEEP 3 ROWS;

CREATE WINDOW DataStreamSliding11U
OVER DataStream KEEP 11 ROWS;

CREATE JUMPING WINDOW DataStreamJumping5U
OVER DataStream KEEP 5 ROWS;

CREATE JUMPING WINDOW DataStreamJumping7U
OVER DataStream KEEP 7 ROWS;

CREATE JUMPING WINDOW DataStream5MinutesU
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6MinutesU
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;


-- partitioned windows

CREATE WINDOW DataStreamSliding3P
OVER DataStream KEEP 3 ROWS
PARTITION BY merchantId;

CREATE WINDOW DataStreamSliding11P
OVER DataStream KEEP 11 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStreamJumping5P
OVER DataStream KEEP 5 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStreamJumping7P
OVER DataStream KEEP 7 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream5MinutesP
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6MinutesP
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;





-- THE TEST OUTPUTS


-- Wactions01 receives data straight through from the Source
-- should contain the exact data emitted
CREATE WACTIONSTORE Wactions_S1 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions01
INSERT INTO Wactions01
SELECT *
FROM DataStream;








-- test test boundaries for jumping/sliding partitioned/unpartitioned aggregate/non-aggregate

-- SLIDING

-- Wactions_SW3UN data goes through an unpartitioned sliding window then a non-aggregate CQ

CREATE WACTIONSTORE Wactions_SW3UN CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_SW3UN
INSERT INTO Wactions_SW3UN
SELECT *
FROM DataStreamSliding3U;


-- Wactions_SW3UN data goes through a partitioned sliding window then a non-aggregate CQ

CREATE WACTIONSTORE Wactions_SW3PN CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_SW3PN
INSERT INTO Wactions_SW3PN
SELECT *
FROM DataStreamSliding3P;


-- Wactions_SW3UN data goes through an unpartitioned sliding window then an aggregate CQ

CREATE WACTIONSTORE Wactions_SW3UA CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_SW3UA
INSERT INTO Wactions_SW3UA
SELECT FIRST(*)
FROM DataStreamSliding3U;


-- Wactions_SW3UN data goes through a partitioned sliding window then an aggregate CQ

CREATE WACTIONSTORE Wactions_SW3PA CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_SW3PA
INSERT INTO Wactions_SW3PA
SELECT FIRST(*)
FROM DataStreamSliding3P;



-- JUMPING


-- Wactions_JW3UN data goes through an unpartitioned jumping window then a non-aggregate CQ

CREATE WACTIONSTORE Wactions_JW5UN CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_JW5UN
INSERT INTO Wactions_JW5UN
SELECT *
FROM DataStreamJumping5U;


-- Wactions_JW5UN data goes through a partitioned sliding jumping then a non-aggregate CQ

CREATE WACTIONSTORE Wactions_JW5PN CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_JW5PN
INSERT INTO Wactions_JW5PN
SELECT *
FROM DataStreamJumping5P;


-- Wactions_JW5UN data goes through an unpartitioned jumping window then an aggregate CQ

CREATE WACTIONSTORE Wactions_JW5UA CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_JW5UA
INSERT INTO Wactions_JW5UA
SELECT FIRST(*)
FROM DataStreamJumping5U;


-- Wactions_JW5UN data goes through a partitioned jumping window then an aggregate CQ

CREATE WACTIONSTORE Wactions_JW5PA CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_JW5PA
INSERT INTO Wactions_JW5PA
SELECT FIRST(*)
FROM DataStreamJumping5P;







END FLOW BasicComponentTestsFlow;










CREATE FLOW ComplexScenariosFlow;





CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;








-- Wactions03 data goes through an unpartitioned jumping window then an aggregate CQ
-- should contain one waction for every 5 events
CREATE WACTIONSTORE Wactions03 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions03
INSERT INTO Wactions03
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStreamJumping5 p;


-- Wactions04 data goes through a size-6 unpartitioned jumping window then an aggregate CQ
-- should contain one waction for every 6 events
CREATE WACTIONSTORE Wactions04 CONTEXT OF WactionType
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data6ToWactios04
INSERT INTO Wactions04
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;





END FLOW ComplexScenariosFlow;






END APPLICATION RecovTest99;

stop application AzureApp2;
undeploy application AzureApp2;
drop application AzureApp2 cascade;

create application AzureApp2
RECOVERY 5 second interval;
create source CSVSource2 using FileReader (
	directory:'@DIR@',
	WildCard:'@WILDCARD@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream2;

Create Type CSVType2 (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream2 of CSVType2;

CREATE CQ CsvToPosData2
INSERT INTO TypedCSVStream2
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream2;

create Target BlobT2 using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:100000'
)
format using DSVFormatter (
)
input from TypedCSVStream2;
end application AzureApp2;
deploy application AzureApp2 in default;
start application AzureApp2;

STOP rest2.applicationApi;
UNDEPLOY APPLICATION rest2.applicationApi;
DROP APPLICATION rest2.applicationApi cascade;

CREATE APPLICATION applicationApi;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)

PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;


CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startingTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
@PERSIST-TYPE@


CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false,
  trimwhitespace:true
) 
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'zip') OF USAddressData;


CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;
        
END APPLICATION applicationApi;

stop ROLLUPMON_IL;
undeploy application ROLLUPMON_IL;
alter application ROLLUPMON_IL;
CREATE or replace FLOW ROLLUPMON_IL_flow;
Create or replace Source ROLLUPMON_IL_Oraclesrc Using databasereader(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',
 Tables:'QATEST.ROLLUPMON_TABLE1;QATEST.ROLLUPMON_TABLE2;QATEST.ROLLUPMON_TABLE3;QATEST.ROLLUPMON_TABLE4;QATEST.ROLLUPMON_TABLE5',
 _h_fetchexactrowcount: 'true',
FetchSize:1000
)
Output To ROLLUPMON_IL_OrcStrm;
END FLOW ROLLUPMON_IL_flow;
alter application ROLLUPMON_IL recompile;
deploy application ROLLUPMON_IL;
start application ROLLUPMON_IL;

--
-- Recovery Test 42 with two sources and two WactionStores. A variety of partitioned windows in between
-- assure that we are testing a complicated recovery scenario.
--
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Stream -> JWc5 -> WS1
--   S2 -> Stream -> JWc10 -> WS2
--

STOP Recov42Tester.RecovTest42;
UNDEPLOY APPLICATION Recov42Tester.RecovTest42;
DROP APPLICATION Recov42Tester.RecovTest42 CASCADE;
CREATE APPLICATION RecovTest42 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10242,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10242,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM DataStreamTop OF CsvData using KafkaProps;

CREATE CQ Csv1ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ Csv2ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;










CREATE JUMPING WINDOW LeftJWc5
OVER DataStreamTop KEEP 5 ROWS;

CREATE JUMPING WINDOW RightJWc10
OVER DataStreamTop KEEP 10 ROWS;



CREATE WACTIONSTORE WactionsLeft CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsRight CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ ToWactionsLeft
INSERT INTO WactionsLeft
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM LeftJWc5 p;

CREATE CQ ToWactionsRight
INSERT INTO WactionsRight
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM RightJWc10 p;

END APPLICATION RecovTest42;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 10 second interval;
create source CS using FileReader (
	directory:'@DIR@',
	WildCard:'@WILDCARD@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target T using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:50'
)
format using DSVFormatter (
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

STOP APPLICATION OneAgentEncryptionTester.CSV;
UNDEPLOY APPLICATION OneAgentEncryptionTester.CSV;
DROP APPLICATION OneAgentEncryptionTester.CSV cascade;

create application CSV WITH ENCRYPTION;

CREATE FLOW AgentFlow;
create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails-agent.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStream1;

END FLOW AgentFlow;

CREATE FLOW ServerFlow;

CREATE TYPE MyTypeCsv(
PAN String,
FNAME String KEY,
LNAME String,
ADDRESS String,
CITY String,
STATE String,
ZIP String,
GENDER String
);

CREATE STREAM TypedStreamCsv of MyTypeCsv;

CREATE CQ TypeConversionCQCsv
INSERT INTO TypedStreamCsv
SELECT
data[0],
data[1],
data[2],
data[3],
data[4],
data[5],
data[6],
data[7]
from CsvStream1;

CREATE WACTIONSTORE StoreInfoCsv CONTEXT OF MyTypeCsv
EVENT TYPES ( MyTypeCsv )
@PERSIST-TYPE@

CREATE CQ StoreWactionCsv
INSERT INTO StoreInfoCsv
SELECT * FROM TypedStreamCsv
LINK SOURCE EVENT;


END FLOW ServerFlow;

end application CSV;

DEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;
START CSV;

CREATE TYPE PosData(
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);
CREATE STREAM PosDataStream OF PosData PARTITION BY merchantId;

--
-- Recovery Test 27 with two sources, two jumping time windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jt1W -> CQ1 -> WS
--   S2 -> Jt2W -> CQ2 -> WS
--

STOP KStreamRecov27Tester.KStreamRecovTest27;
UNDEPLOY APPLICATION KStreamRecov27Tester.KStreamRecovTest27;
DROP APPLICATION KStreamRecov27Tester.KStreamRecovTest27 CASCADE;
DROP USER KStreamRecov27Tester;
DROP NAMESPACE KStreamRecov27Tester CASCADE;
CREATE USER KStreamRecov27Tester IDENTIFIED BY KStreamRecov27Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov27Tester;
CONNECT KStreamRecov27Tester KStreamRecov27Tester;

CREATE APPLICATION KStreamRecovTest27 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream1Second
OVER DataStream1 KEEP WITHIN 1 SECOND;

CREATE JUMPING WINDOW DataStream2Second
OVER DataStream2 KEEP WITHIN 2 SECOND;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data1ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream1Second p;

CREATE CQ Data2ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream2Second p;

END APPLICATION KStreamRecovTest27;

--
-- Recovery Test 32 with two sources, two sliding attribute windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sa5W/p -> CQ1 -> WS
-- S2 -> Sa6W/p -> CQ2 -> WS
--

STOP Recov32Tester.RecovTest32;
UNDEPLOY APPLICATION Recov32Tester.RecovTest32;
DROP APPLICATION Recov32Tester.RecovTest32 CASCADE;
CREATE APPLICATION RecovTest32 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION RecovTest32;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@;
CREATE  SOURCE @SourceName@ USING MSSqlReader  ( 
  Username: '@UserName@',
  Password: '@Password@',
  DatabaseName: 'qatest',
  ConnectionURL: '@SourceConnectionURL@',
  Tables: 'qatest.@SourceTable@',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @SRCINPUTSTREAM@;
create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;
CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:1',
  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
 ) 
INPUT FROM @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
wildcard: '',
directory: '',
positionbyeof: false )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: ''
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (
  positionByEOF: false,
  wildcard: '',
  directory: '' )
PARSE USING XMLParserV2 (
  rootnode: 'document/MerchantName' )
OUTPUT TO @APPNAME@Stream;

CREATE CQ @APPNAME@CQ
INSERT INTO @APPNAME@CQStream
SELECT
data.attributeValue("merchantid") as merchantid,
data.getText() as MerchantName
FROM @APPNAME@Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@CQStream;

END APPLICATION @APPNAME@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  _h_BindEmptyStringasNull : 'true'
 )
INPUT FROM @STREAM@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '@GRPPOLICY@',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'MultipleEvents',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
  filename: '%@metadata(FileName)%',
  directory: '@DIR@',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter  ()
INPUT FROM CCBStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

--
-- Crash Recovery Test 2 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS
--

STOP APPLICATION KStreamN2S2CR2Tester.KStreamN2S2CRTest2;
UNDEPLOY APPLICATION KStreamN2S2CR2Tester.KStreamN2S2CRTest2;
DROP APPLICATION KStreamN2S2CR2Tester.KStreamN2S2CRTest2 CASCADE;

DROP USER KStreamN2S2CR2Tester;
DROP NAMESPACE KStreamN2S2CR2Tester CASCADE;
CREATE USER KStreamN2S2CR2Tester IDENTIFIED BY KStreamN2S2CR2Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR2Tester;
CONNECT KStreamN2S2CR6Tester KStreamN2S2CR2Tester;

CREATE APPLICATION KStreamN2S2CRTest2 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest2;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest2;

CREATE FLOW DataProcessingKStreamN2S2CRTest2;

CREATE TYPE WactionTypeKStreamN2S2CRTest2 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionTypeKStreamN2S2CRTest2;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest2 CONTEXT OF WactionTypeKStreamN2S2CRTest2
EVENT TYPES ( WactionTypeKStreamN2S2CRTest2 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsKStreamN2S2CRTest2
INSERT INTO WactionsKStreamN2S2CRTest2
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingKStreamN2S2CRTest2;

END APPLICATION KStreamN2S2CRTest2;

--
-- Recovery Test 34 with two sources, two sliding time-count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5a9W/p  -> CQ1 -> WS
-- S2 -> Sc6a11W/p -> CQ2 -> WS
--

STOP KStreamRecov3Tester.KStreamRecovTest34;
UNDEPLOY APPLICATION KStreamRecov3Tester.KStreamRecovTest34;
DROP APPLICATION KStreamRecov34Tester.KStreamRecovTest34 CASCADE;

DROP USER KStreamRecov34Tester;
DROP NAMESPACE KStreamRecov34Tester CASCADE;
CREATE USER KStreamRecov34Tester IDENTIFIED BY KStreamRecov34Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov34Tester;
CONNECT KStreamRecov34Tester KStreamRecov34Tester;

CREATE APPLICATION KStreamRecovTest34 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION KStreamRecovTest34;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSV1Source using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'RFC4180.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'no'
)
OUTPUT TO Csv1Stream;

create Target t using FileWriter(
  filename:'FileWriterStandardRFC4180',
  directory:'@FEATURE-DIR@/logs/',
  standard : 'RFC4180',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (
members:'data'
)
input from Csv1Stream;

end application DSV;

--
-- Recovery Test 25 with two sources, two jumping count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5W -> CQ1 -> WS
--   S2 -> Jc6W -> CQ2 -> WS
--

STOP KStreamRecov25Tester.KStreamRecovTest25;
UNDEPLOY APPLICATION KStreamRecov25Tester.KStreamRecovTest25;
DROP APPLICATION KStreamRecov25Tester.KStreamRecovTest25 CASCADE;
DROP USER KStreamRecov25Tester;
DROP NAMESPACE KStreamRecov25Tester CASCADE;
CREATE USER KStreamRecov25Tester IDENTIFIED BY KStreamRecov25Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov25Tester;
CONNECT KStreamRecov25Tester KStreamRecov25Tester;

CREATE APPLICATION KStreamRecovTest25 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION KStreamRecovTest25;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.test01',
  connectionRetryPolicy:'retryInterval=1,maxRetries=3'
 ) 
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'public.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true	
) INPUT FROM @APPNAME@_ss;
--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

stop OracleReaderToDBWriter;
Undeploy application OracleReaderToDBWriter;
alter application OracleReaderToDBWriter;
CREATE OR REPLACE SOURCE Oraclesrc USING OracleReader  ( 
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Tables: 'QATEST.Orcalesrc',
  FetchSize: '3'
 ) 
OUTPUT TO OrcStrm;
alter application OracleReaderToDBWriter recompile;
DEPLOY APPLICATION OracleReaderToDBWriter with Hz_Agent_flow on any in AGENTS;
start application OracleReaderToDBWriter;

UNDEPLOY APPLICATION admin.BadDeployGroup;
DROP APPLICATION admin.BadDeployGroup cascade;

CREATE APPLICATION BadDeployGroup;

-- This sample application demonstrates how WebAction could be used
-- by a retail chain to generate real-time reports on products and
-- stores and to send alerts of unusual activity.


CREATE FLOW BadSourceFlow;

-- RetailDataSource is the primary data source for this application.
--
-- ParseOrderData discards the fields not needed by this application and puts the
-- data into the appropriate Java types.
--
-- ParseOrderData outputs to RetailOrders stream, the start point of the
-- RetailProductFlow and RetailStoreFlow flows.

CREATE SOURCE RetailDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'retaildata2M.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO Orders;

-- A stream's type must be declared before the stream, and a CQ's
-- output stream must be defined before the CQ. Hence type-stream-CQ
-- sequences like the following are very common.

-- output for ParseOrderData
CREATE TYPE OrderType(
  storeId      String,
  orderId      String,
  sku          String,
  orderAmount  double,
  dateTime     DateTime,
  hourValue    int,
  state        String,
  city         String,
  zip          String
);
CREATE STREAM RetailOrders Of OrderType;

CREATE CQ ParseOrderData
INSERT INTO RetailOrders
SELECT  data[0],
        data[6],
        data[7],
        TO_DOUBLE(SRIGHT(data[8],1)),
        TO_DATE(data[9],'yyyyMMddHHmmss'),
        DHOURS(TO_DATE(data[9],'yyyyMMddHHmmss')),
        data[3],
        data[2],
        data[4]
FROM Orders;

END FLOW BadSourceFlow;


CREATE FLOW BadProductFlow;

-- This flow populates the ProductActivity WAction store, which
-- provides data for dashboard reports on sales by product.

-- defines context for WAction store
CREATE TYPE ProductActivityContext(
  sku String  KEY,
  OrderCount int,
  SalesAmount double,
  StartTime DateTime
);

-- defines event types for Waction store
CREATE TYPE ProductTrackingType (
  sku String KEY,
  OrderCount int,
  SalesAmount double,
  StartTime DateTime
);

CREATE WACTIONSTORE ProductActivity
CONTEXT OF ProductActivityContext
EVENT TYPES ( ProductTrackingType )
PERSIST NONE USING ( ) ;

-- input for GetProductActivity
CREATE JUMPING WINDOW ProductData_15MIN
OVER RetailOrders
KEEP WITHIN 15 MINUTE ON dateTime
PARTITION BY sku;

CREATE STREAM ProductTrackingStream OF ProductTrackingType;

-- aggregates data and populates WAction store
CREATE CQ GetProductActivity
INSERT INTO ProductTrackingStream
SELECT pd.sku, COUNT(*), SUM(pd.orderAmount), FIRST(pd.dateTime)
FROM ProductData_15MIN pd
GROUP BY pd.sku;

CREATE CQ TrackProductActivity
INSERT INTO ProductActivity
SELECT sku, OrderCount, SalesAmount, StartTime
FROM ProductTrackingStream
LINK SOURCE EVENT;

END FLOW BadProductFlow;


CREATE FLOW BadStoreFlow;

-- This flow populates the StoreActivity WAction store, which provides
-- data for dashboard reports on reports on stores, and sends alerts when
-- sales volumes are higher or lower than expected.

-- RetailStoreFlow part 1 - GetStoreActivity
--
-- The RetailData_5MIN window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions. (Aggregate functions cannot
-- be used with unbound real-time data.)
--
-- The HourlyStoreSales_Cache cache provides historical averages for the
-- current hour for each merchant

-- input for GetStoreActivity
CREATE JUMPING WINDOW RetailData_5MIN
     OVER RetailOrders
     KEEP WITHIN 5 MINUTE ON dateTime
     PARTITION BY storeId;

-- input for GetStoreActivity
CREATE TYPE StoreHourlyAvg(
  storeId String,
  hourValue int,
  hourlyAvg int,
  hourlyItemCnt int
);
CREATE CACHE HourlyStoreSales_Cache using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'storehourlyData.txt',
  header: Yes,
  columndelimiter: ','
) QUERY (keytomap:'storeId') OF StoreHourlyAvg;

-- output for GetStoreActivity
CREATE TYPE StoreOrdersTrackingType (
  storeId String KEY,
  state String,
  city  String,
  zip   String,
  StartTime DateTime,
  ordersCount int,
  salesAmount double,
  hourlyAvg int,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM StoreOrdersTracking OF StoreOrdersTrackingType;

CREATE CQ GetStoreActivity
INSERT INTO StoreOrdersTracking
SELECT rd.storeId, rd.state, rd.city, rd.zip, first(rd.dateTime),
       COUNT(rd.storeId), SUM(rd.orderAmount), l.hourlyAvg/6,
       l.hourlyAvg/6 + l.hourlyAvg/8,
       l.hourlyAvg/6 - l.hourlyAvg/10,
       '<NOTSET>', '<NOTSET>'
FROM RetailData_5MIN rd, HourlyStoreSales_Cache l
WHERE rd.storeId = l.storeId AND rd.hourValue = l.hourValue
GROUP BY rd.storeId;
-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyStoreSales_Cache cache. The category and status fields
-- are left unset to be populated by the next query.


-- RetailStoreFlow part 2 - GetStoreStatus
--
-- This query sets the count values used by the dashboard map and the
-- status values used to trigger alerts.

-- uses type previously defined for StoreOrdersTracking
CREATE STREAM StoreOrdersTracking_Status OF StoreOrdersTrackingType;

CREATE CQ GetStoreStatus
INSERT INTO StoreOrdersTracking_Status
SELECT storeId, state, city, zip, StartTime,
       ordersCount, salesAmount, hourlyAvg, upperLimit, lowerLimit,
       CASE
         WHEN salesAmount > (upperLimit + 2000) THEN 'HOT'
         WHEN salesAmount > upperLimit THEN 'MEDIUM'
         WHEN salesAmount < lowerLimit THEN 'COLD'
         ELSE 'COOL' END,
       CASE
         WHEN salesAmount > upperLimit THEN 'TOOHIGH'
         WHEN salesAmount < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM StoreOrdersTracking;


-- RetailStoreFlow part 3 - create and populate the StoreActivity WAction store

-- input for CQ TrackStoreActivity
CREATE TYPE StoreNameData(
  storeId       String KEY,
  storeName     String
);
CREATE CACHE StoreNameLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'StoreNames.csv',
  header: Yes,
  columndelimiter: ','
) QUERY(keytomap:'storeId') OF StoreNameData;

-- input for CQ TrackStoreActivity
CREATE TYPE RetailUSAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);
CREATE CACHE ZipCodeLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  header: Yes,
  columndelimiter: '	'
) QUERY (keytomap:'zip') OF RetailUSAddressData;

-- defines WAction store context
CREATE TYPE StoreActivityContext(
  storeId String KEY,
  StartTime DateTime,
  StoreName String,
  Category String,
  Status String,
  OrderCount int,
  salesamount double,
  HourlyAvg int,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

-- StoreOrdersTrackingType previously defined for StoreOrdersTracking
CREATE WACTIONSTORE StoreActivity
CONTEXT OF StoreActivityContext
EVENT TYPES (StoreOrdersTrackingType )
PERSIST NONE USING ();

CREATE CQ TrackStoreActivity
INSERT INTO StoreActivity
SELECT s.storeId,
  s.StartTime,
  n.storeName,
  s.category,
  s.status,
  s.ordersCount,
  s.salesAmount,
  s.hourlyAvg,
  s.upperLimit,
  s.lowerLimit,
  z.zip,
  z.city,
  s.state,
  z.latVal,
  z.longVal
FROM StoreOrdersTracking_Status s, StoreNameLookup n, ZipCodeLookup z
WHERE s.storeId = n.storeId AND s.zip = z.zip
LINK SOURCE EVENT;


-- RetailStoreFlow part 4 - send alerts


CREATE STREAM RetailAlertStream OF Global.AlertEvent;

CREATE CQ RetailRetailGenerateAlerts
INSERT INTO RetailAlertStream
SELECT n.storeName, s.storeId,
        CASE
          WHEN s.Status = 'OK' THEN 'info'
          ELSE 'warning' END,
        CASE
          WHEN s.Status = 'OK' THEN 'cancel'
          ELSE 'raise' END,
        CASE
          WHEN s.Status = 'OK' THEN 'Store ' + n.storeName + ' amount of $'+ s.salesAmount + ' is back between $' + s.lowerLimit + ' and $' +s.upperLimit
          WHEN s.Status = 'TOOHIGH' THEN 'Store ' + n.storeName + ' amount of $'+ s.salesAmount + ' is above upper limit of $' + s.upperLimit
          WHEN s.Status = 'TOOLOW' THEN 'Store ' + n.storeName + ' amount of $'+ s.salesAmount + ' is below lower limit of $' + s.lowerLimit
          ELSE ''
          END
FROM StoreOrdersTracking_Status s, StoreNameLookup n
WHERE s.storeId = n.storeId;

END FLOW BadStoreFlow;


-- load dashboard visualization settings from file

--CREATE VISUALIZATION BadDeployGroup "Samples/Customer/RetailApp/RetailApp_visualization_settings.json";

-- The following statement defines the user and delivery method for alerts.
CREATE SUBSCRIPTION BadAlertSub USING WebAlertAdapter( ) INPUT FROM RetailAlertStream;


END APPLICATION BadDeployGroup;

create target @TARGET_NAME@ using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
) INPUT FROM @STREAM@;

CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'data.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO @AppName@_rawstream;

CREATE OR REPLACE STREAM @BuiltinFunc@_Stream OF Global.WAEVent;
CREATE OR REPLACE STREAM CombineStream OF Global.WAEVent;

CREATE OR REPLACE CQ cq1
INSERT INTO @BuiltinFunc@_Stream
SELECT
@BuiltinFunc@(s1, 'city',data[5])
FROM @AppName@_rawstream s1;

CREATE OR REPLACE CQ cq2
INSERT INTO CombineStream
Select *
FROM @BuiltinFunc@_Stream s4;

CREATE OR REPLACE CQ cq3
INSERT INTO CombineStream
select *
FROM @AppName@_rawstream s5;

CREATE OR REPLACE TARGET @AppName@_FileTarget USING Global.FileWriter ( 
  flushpolicy: 'EventCount:10000,Interval:30s', 
  adapterName: 'FileWriter', 
  directory: '@logs@',
  filename: '@BuiltinFunc@_Data', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING Global.JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter', 
  jsonMemberDelimiter: '\n', 
  EventsAsArrayOfJsonObjects: 'true', 
  formatterName: 'JSONFormatter', 
  jsonobjectdelimiter: '\n' ) 
INPUT FROM CombineStream;

End application @AppName@;
Deploy application @AppName@; 
Start application @AppName@;

CREATE APPLICATION @APPNAME@ RECOVERY 5 second interval;
CREATE SOURCE @APPNAME@_src USING OracleReader ()
OUTPUT TO @APPNAME@_Stream;

CREATE TARGET @APPNAME@_trgt USING AzureBlobWriter()
format using DSVFormatter ()
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ recovery 1 second interval;

CREATE SOURCE @SOURCENAME@ USING OracleReader
(
    Username: '@SRCUSERNAME@',
    Password: '@SRCPASSWORD@',
    ConnectionURL: '@SRCURL',
    Tables: '@SRCTABLE',
    FetchSize: '@FETCHSIZE@',
    CommittedTransactions: true
)

OUTPUT TO @STREAM@ ;

CREATE TARGET @TARGETNAME@ using DatabaseWriter
(
    ConnectionURL: '@TARGETURL',
    username: '@TARGETUSERNAME@',
    Password: '@TARGETPASSWORD@',
    Tables: '@TARGETTABLE@',
    BatchPolicy:'EventCount:1,Interval:1',
    CommitPolicy:'EventCount:1,Interval:1'
)
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

STOP OneAgentTester.CSV;
UNDEPLOY APPLICATION OneAgentTester.CSV;
DROP APPLICATION OneAgentTester.CSV CASCADE;
CONNECT ADMIN abc;
DROP USER OneAgentTester;
USE ADMIN;
DROP NAMESPACE OneAgentTester CASCADE;
drop dg AGENTS;
drop dg LocalServer;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@;
CREATE  SOURCE @SourceName@ USING DatabaseReader  ( 
  Username: '@UserName@',
  Password: '@Password@',
  DatabaseName: 'qatest',
  ConnectionURL: '@SourceConnectionURL@',
  Tables: 'qatest.@SourceTable@',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @SRCINPUTSTREAM@;
create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;
CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:1',
  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
 ) 
INPUT FROM @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@2;

--
-- Recovery Test T20
-- Nicholas Keene, WebAction, Inc.
--
-- Snum -> CQ -> WS
--


UNDEPLOY APPLICATION NameT20.T20;
DROP APPLICATION NameT20.T20 CASCADE;
CREATE APPLICATION T20;




CREATE FLOW DataAcquisitionT20;


CREATE SOURCE CsvSourceT20 USING NumberSource ( 
  lowValue: '1',
  highValue: '1003',
  delayMillis: '10',
  delayNanos: '0',
  repeat: 'false'
 ) 
OUTPUT TO OutputStreamT20;


END FLOW DataAcquisitionT20;




CREATE FLOW DataProcessingT20;


Create Target OutputTargetT20
Using Sysout (name: 'OutputTargetT20')
Input From OutputStreamT20;


END FLOW DataProcessingT20;



END APPLICATION T20;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING IncrementalBatchReader  ( 
  FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: '@CHECKCOLUMN@',
 startPosition: '%=-1',
  PollingInterval: '5sec'
  )
  OUTPUT TO @STREAM@;

--
-- Kafka Stream Agent Checkpoint Recovery tests
-- Bert Hashemi and Zalak Shah, WebAction, Inc.
--

stop application recoveryTestAgent.KSRecovCSV;
undeploy application recoveryTestAgent.KSRecovCSV;
drop application recoveryTestAgent.KSRecovCSV cascade;
DROP USER recoveryTestAgent;
DROP NAMESPACE recoveryTestAgent CASCADE;
CREATE USER recoveryTestAgent IDENTIFIED BY recoveryTestAgent;
GRANT create,drop ON deploymentgroup Global.* TO USER recoveryTestAgent;
CONNECT recoveryTestAgent recoveryTestAgent;

create application KSRecovCSV
RECOVERY 5 SECOND INTERVAL;

CREATE FLOW AgentFlow;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails-recovery.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO KafkaCsvStream;

END FLOW AgentFlow;

CREATE FLOW ServerFlow;
CREATE TYPE UserDataType
(
  UserId String KEY,
  UserName String
);

CREATE STREAM UserDataStream OF UserDataType PARTITION BY UserId;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  data[0],
        data[1]
FROM KafkaCsvStream;

CREATE WACTIONSTORE UserActivityInfo
CONTEXT OF UserDataType
EVENT TYPES ( UserDataType )
@PERSIST-TYPE@

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction
INSERT INTO UserActivityInfo
SELECT * FROM UserDataStream
LINK SOURCE EVENT;
END FLOW ServerFlow;

END APPLICATION KSRecovCSV;
DEPLOY APPLICATION KSRecovCSV with AgentFlow in AGENTS, ServerFlow on any in default;
START KSRecovCSV;

--
-- Recovery Test 6 with sliding window and partitioned feature
-- Nicholas Keene, Bert Hashemi WebAction, Inc.
--
-- S -> CQ -> SW(partitioned) -> CQ(no aggregate) -> WS
--

STOP Recov6Tester.RecovTest6;
UNDEPLOY APPLICATION Recov6Tester.RecovTest6;
DROP APPLICATION Recov6Tester.RecovTest6 CASCADE;
CREATE APPLICATION RecovTest6 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvData PARTITION BY merchantId;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

CREATE WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION RecovTest6;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;;

Create Source Ojetsrc Using Ojet
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;


create target AzureTarget1 using AzureSQLDWHWriter (
		CoNNectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',  
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;


create target AzureTarget2 using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        UserName: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@', 
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;


create target AzureTarget3 using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        UserName: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@', 
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

create target AzureTarget4 using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        UserName: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@', 
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

--
-- Crash Recovery Test 2 on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS
--

STOP APPLICATION N4S4CR2Tester.N4S4CRTest2;
UNDEPLOY APPLICATION N4S4CR2Tester.N4S4CRTest2;
DROP APPLICATION N4S4CR2Tester.N4S4CRTest2 CASCADE;
CREATE APPLICATION N4S4CRTest2 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest2;

CREATE SOURCE CsvSourceN4S4CRTest2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest2;

CREATE FLOW DataProcessingN4S4CRTest2;

CREATE TYPE WactionTypeN4S4CRTest2 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionTypeN4S4CRTest2;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN4S4CRTest2 CONTEXT OF WactionTypeN4S4CRTest2
EVENT TYPES ( WactionTypeN4S4CRTest2 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN4S4CRTest2
INSERT INTO WactionsN4S4CRTest2
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN4S4CRTest2;

END APPLICATION N4S4CRTest2;

CREATE APPLICATION DsPosApp;

CREATE OR REPLACE CQ CQPosApp
INSERT INTO PosStrm
select * from DS.MerchantActivity
LINK SOURCE EVENT;

-- WACTIONSTORE PosWaction is being loaded from MerchantActivity from DSWaction.tql

CREATE WACTIONSTORE PosWaction CONTEXT OF DS.MerchantActivityContext
EVENT TYPES ( DS.MerchantTxRate )
@PERSIST-TYPE@


CREATE CQ CQPosWaction
INSERT INTO PosWaction
select * from DS.PosStrm
LINK SOURCE EVENT;


END APPLICATION DsPosApp;
DEPLOY APPLICATION DsPosApp;
START APPLICATION DsPosApp;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ USE EXCEPTIONSTORE TTL : '7d' ;

CREATE SOURCE @APPNAME@_DBSource USING DatabaseReader (
  Tables: '"qatest"."dbo"."emp"',
  Username: 'qatest',
  Password: '5wZ8jZNAU1dzU0bPbxhATA==',
  FetchSize: 10000,
  Password_encrypted: 'false',
  QuiesceOnILCompletion: 'true',
  DatabaseProviderType: 'SQLSERVER',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;DatabaseName=qatest' )
OUTPUT TO @APPNAME@_OutputStream;

CREATE OR REPLACE TARGET @APPNAME@_Target USING Global.BigQueryWriter (
  ColumnDelimiter: '|',
  NullMarker: 'NULL',
  streamingUpload: 'false',
  projectId: 'striimqa-214712',
  Encoding: 'UTF-8',
  batchPolicy: 'eventcount:10,interval:60',
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30',
  AllowQuotedNewLines: 'false',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  TransportOptions: 'connectionTimeout=300, readTimeout=120',
  adapterName: 'BigQueryWriter',
  Mode: 'APPENDONLY',
  ServiceAccountKey: 'Platform/UploadedFiles/google-gcs.json',
  StandardSQL: 'true',
  includeInsertId: 'true',
  QuoteCharacter: '\"',
  Tables: '"qatest"."dbo"."%",DEV_30875.%' )
INPUT FROM @APPNAME@_OutputStream;

END APPLICATION @APPNAME@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING Global.FabricDataWarehouseWriter (
  Tables: '',
  StorageAccessDriverType: 'WASBS',
  ConnectionURL: '',
  Username: '',
  AccountAccessKey: '',
  Mode: 'APPENDONLY',
  Password: '',
  uploadpolicy: 'eventcount:1,interval:10s',
  AccountName: '')
INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;;

create Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;

STOP APPLICATION orrs;
UNDEPLOY APPLICATION orrs;
DROP APPLICATION orrs CASCADE;
CREATE APPLICATION orrs;
Create Source oraSource Using DatabaseReader
(
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'QATEST.ORACLETOREDSHIFTIL1;QATEST.ORACLETOREDSHIFTIL2',
 FilterTransactionBoundaries:true,
 FetchSize:1000
) Output To LCRStream;

CREATE TARGET RSTarget USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: 'access_key',
	  --secretaccesskey: 'secret_access',
	  S3IAMRole:'@IAMROLE@',
	  Tables: 'QATEST.ORACLETOREDSHIFT%,QATEST.ORACLETOREDSHIFT%',
	  uploadpolicy:'eventcount:1000,interval:1m'
	) INPUT FROM LCRStream;
	
END APPLICATION orrs;
DEPLOY APPLICATION orrs;
START APPLICATION orrs;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;

Create Source oracSource
 Using DatabaseReader
(
 Username:'oracle_username',
 Password:'oracle_password',
 ConnectionURL:'oracle_connection',
 Tables:'src_tables',
 Query:"SELECT * FROM qatest.oracle_alldatatypes",
 OnlineCatalog:true,
 FetchSize:10000,
 QueueSize:2048,
 CommittedTransactions:true,
 Compression:false
) Output To DataStream;
CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkuPDaTehAnDliNgmOdE:'DELETEANDINSERT',
tables: 'tgt_tables',
batchpolicy: 'EventCount:10000,Interval:30')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

stop application BigqueryBulkLoadMonMetrics;
undeploy application BigqueryBulkLoadMonMetrics;
drop application BigqueryBulkLoadMonMetrics cascade;

CREATE APPLICATION BigqueryBulkLoadMonMetrics;

CREATE FLOW BigqueryBulkLoadMonMetrics_SourceFlow;

CREATE SOURCE BigqueryBulkLoadMonMetrics_DBSource USING DatabaseReader ( 
  Username: 'qatest', 
  DatabaseProviderType: 'ORACLE', 
  FetchSize: 10000, 
  Password_encrypted: 'false', 
  QuiesceOnILCompletion: 'true', 
  Password: 'JVaLv3ZpgQDY8R2ZxS38xg==', 
  Tables: 'QATEST.EMPLOYEE', 
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe' ) 
OUTPUT TO BigqueryBulkLoadMonMetrics_OutputStream;

END FLOW BigqueryBulkLoadMonMetrics_SourceFlow;

CREATE OR REPLACE TARGET BigqueryBulkLoadMonMetrics_BigQueryTarget1 USING BigQueryWriter ( 
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  streamingUpload: 'false', 
  projectId: 'striimqa-214712', 
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:1000000, Interval:90', 
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30', 
  AllowQuotedNewLines: 'false', 
  optimizedMerge: 'false', 
  TransportOptions: 'connectionTimeout=300, readTimeout=120', 
  adapterName: 'BigQueryWriter', 
  Mode: 'MERGE', 
  Tables: 'QATEST.EMPLOYEE,DEV22862jen.sample', 
  StandardSQL: 'true', 
  includeInsertId: 'true', 
  QuoteCharacter: '\"', 
  ServiceAccountKey: '/Users/jenniffer/Product2/IntegrationTests/TestData/google-gcs.json' ) 
INPUT FROM BigqueryBulkLoadMonMetrics_OutputStream;

END APPLICATION BigqueryBulkLoadMonMetrics;

deploy application BigqueryBulkLoadMonMetrics;
start application BigqueryBulkLoadMonMetrics;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ RECOVERY 1 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @SourceName@ USING PostgreSQLReader  ( 
 ReplicationSlotName: 'striim_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src'
 ) 
OUTPUT TO @SRCINPUTSTREAM@ ;

CREATE OR REPLACE SOURCE @SourceName@1 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src'
 ) 
OUTPUT TO @SRCINPUTSTREAM@1 ;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy:'EventCount:1000,Interval:60',
CommitPolicy:'EventCount:1000,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@1 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy:'EventCount:1000,Interval:60',
CommitPolicy:'EventCount:1000,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM @SRCINPUTSTREAM@1;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;

Create Source @SourceName@ Using OracleReader

(
  Username:'c##qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',
  Tables:'CDB$ROOT."C##QATEST".ojet_src;ORCLPDB.QATEST.ojet_src',
  _h_useClassic:false,
  Fetchsize:1,
  Compression: true,
  SupportPDB:true,
  ReplicationSlotName:'null'
)
Output To @SRCINPUTSTREAM@;

CREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'))) FROM @SRCINPUTSTREAM@ x; ;


CREATE TARGET @targetName@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',
  Username:'c##qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'CDB$ROOT."C##QATEST".ojet_src,CDB$ROOT."C##QATEST".ojet_tgt'
) INPUT FROM admin.sqlreader_cq_out;



create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

-- CacheTestWithDB.tql
-- Author: Gopi Putty; Date: Oct,11, 2016. 
-- A cache backed by db table.
/*
Oracle:
CREATE TABLE PRODUCT_INV ( SKU INT PRIMARY KEY NOT NULL,  NAME varchar2(20) );
INSERT INTO PRODUCT_INV (SKU, NAME)
*/

USE ADMIN;
STOP APPLICATION CACHETEST2.CACHEBACKEDWITHDB;
UNDEPLOY APPLICATION CACHETEST2.CACHEBACKEDWITHDB;
DROP APPLICATION CACHETEST2.CACHEBACKEDWITHDB CASCADE;
DROP NAMESPACE CACHETEST2 CASCADE; 
CREATE NAMESPACE CACHETEST2;
USE CACHETEST2; 

CREATE APPLICATION CACHEBACKEDWITHDB;

CREATE TYPE CTYPE(
  SKU int,
  Name String
);

CREATE STREAM skuStream OF CTYPE;

CREATE OR REPLACE CACHE BKRequestLookup USING DatabaseReader (
ConnectionURL:'jdbc:oracle:thin:@10.1.186.105:1521:orcl',
Username:'GOPI',
Password:'gopi',
Query: "
SELECT  PI.SKU, PI.NAME FROM GOPI.PRODUCT_INV PI"
) QUERY (keytomap:'SKU', refreshinterval: '60000000',charset:'UTF-8',replicas:1) OF CTYPE;

END APPLICATION CACHEBACKEDWITHDB;
DEPLOY APPLICATION CACHEBACKEDWITHDB;

USE ADMIN;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

CREATE  SOURCE @QuerySource@1 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@1;
create Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;
 CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@1;

CREATE  SOURCE @QuerySource@2 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@2;
create Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;
 CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@2;

CREATE  SOURCE @QuerySource@3 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@3;
create Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;
 CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@3;

CREATE  SOURCE @QuerySource@4 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@4;
create Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;
 CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@4;

CREATE  SOURCE @QuerySource@5 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@5;
create Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;
 CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@5;

CREATE  SOURCE @QuerySource@6 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@6;
create Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;
 CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@6;

CREATE  SOURCE @QuerySource@7 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@7;
create Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;
 CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@7;

CREATE  SOURCE @QuerySource@8 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@8;
create Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;
 CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@8;

CREATE  SOURCE @QuerySource@9 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@9;
create Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;
 CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@9;

CREATE  SOURCE @QuerySource@10 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@10;
create Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;
 CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@10;
end application @APPNAME@;

stop application reconnect;
undeploy application reconnect;
drop application reconnect cascade;
CREATE APPLICATION reconnect recovery 1 second interval;

CREATE  SOURCE OracleSource USING OracleReader  ( 
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  ConnectionURL: '@URL@',
  Tables: 'qatest.ReconnectTestSource;qatest.ReconnectTestSourceDummy',
  FetchSize: '@FETCHSIZE@'
 )output to sqlstream;
 --output to sqlstream MAP (table:'qatest.ReconnectTestSource');

CREATE TARGET dbtarget USING DatabaseWriter(
  ConnectionURL:'@URL@',
  Username:'@USERNAME@',
  Password:'@PASSWORD@',
  ConnectionRetryPolicy: 'retryInterval=10s, maxRetries=4',
  BatchPolicy:'EventCount:5000,Interval:30',
  CommitPolicy:'EventCount:30000,Interval:30',
  Tables: '@TABLES@'
 ) INPUT FROM sqlstream;
 
 create target filetgt using filewriter(
 filename:'recovering_validation',
 rolloverpolicy:'eventcount:1000000,interval:100m',
 flushpolicy:'eventcount:1000000,interval:100m'
 )format using dsvformatter()
 input from sqlstream;

--create Target tSysOut using Sysout(name:OrgData) input from sqlstream;
 end application reconnect;
 deploy application reconnect;
 start application reconnect;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_source USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 ) 
OUTPUT TO @APPNAME@_Stream ;

CREATE TARGET @APPNAME@_Target USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	BatchPolicy: 'EventCount: 1; Interval: 1s',
	instanceId: 'qatest'
) INPUT FROM @APPNAME@_stream;

create Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_stream;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING S3Reader (
    bucketname:'',
    objectnameprefix:'',
     )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING AvroFormatter  (
schemaFileName: 'AvroFileSchema'
)
INPUT FROM @appname@CqOut;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using AvroFormatter (
schemaFileName: 'AvroS3Schema'
)
input from @appname@CqOut;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using AvroFormatter (
schemaFileName: 'AvroAzureSchema'
)
INPUT FROM @appname@CqOut;

CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using AvroFormatter (
schemaFileName: 'AvroGCSSchema'
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ recovery 5 second Interval;

CREATE SOURCE @srcName@ USING SalesForceReader ( 
  customObjects: false, 
  autoAuthTokenRenewal: 'true',
  pollingInterval: '1 min', 
  sObjects: '@srcobject@', 
  useConnectionProfile: 'false',
  consumerSecret: '@srcconsumersecret@',
  consumerKey: '@srcconsumerkey@', 
  Username: '@srcusername@',
  Password: '@srcpassword@',
  mode: 'Automated',
  connectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  securityToken: '@srcsecuritytoken@',
  apiEndPoint: '@srcapiurl@',
  MigrateSchema: true, 
  threadPoolSize: 5
)

OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@ USING Global.DeltaLakeWriter (
  personalAccessToken:'@tgtpassword@',
  hostname:'@tgthostname@',
  stageLocation:'/',
  Mode:'MERGE',
  AuthenticationType: 'PersonalAccessToken',
  Tables:'@srcschema@,@tgtschema@.@tgttable@ COLUMNMAP()',
  adapterName:'DeltaLakeWriter',
  personalAccessToken_encrypted:'false',
  optimizedMerge:'false',
  uploadPolicy:'eventcount:1,interval:10s',
  connectionUrl:'@tgturl@',
  IgnorableExceptionCode:'TABLE_NOT_FOUND',
  externalStageType:'DBFSROOT'
)
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

 create flow myagentflow;

CREATE OR REPLACE SOURCE @APPNAME@DB_emp1 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO @APPNAME@Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE @APPNAME@DB_emp2 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO @APPNAME@Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE @APPNAME@DB_emp3 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO @APPNAME@Oracle_ChangeDataStream;

end flow myagentflow;

create flow myserverflow;

CREATE OR REPLACE TARGET @APPNAME@DB_etarget USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:100,Interval:10',
  CommitPolicy: 'EventCount:100,Interval:10',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  Tables: '',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM @APPNAME@Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from @APPNAME@Oracle_ChangeDataStream;

end flow myserverflow;

END APPLICATION @APPNAME@;

deploy application @APPNAME@ on ALL in default with myagentflow on all in Agents, myserverflow on all in  default;

start @APPNAME@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@;
CREATE SOURCE @srcName@ USING Global.OracleReader ( 
  Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@') 
OUTPUT TO @instreamname@;

CREATE OR REPLACE TARGET @tgtName@ USING Global.ServiceNowWriter ( 
  MaxConnections: 20, 
  ClientSecret: '@clientsecret@', 
  ApplicationErrorCountThreshold: 0, 
  BatchPolicy: 'eventCount:10000, Interval:60', 
  ConnectionUrl: '@tgturl@', 
  Password: '@tgtpassword@', 
  BatchAPI: false,  
  ConnectionTimeOut: 60, 
  Mode: 'APPENDONLY', 
  ClientID: '@clientid@', 
  Tables: '@srcschema@.@srctable@,@tgttable@ COLUMNMAP()', 
  ConnectionRetries: 3, 
  useConnectionProfile: false, 
  UserName: '@tgtusername@', 
  adapterName: 'ServiceNowWriter' ) 
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

--
-- Canon Test W02
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for an unpartitioned jumping count window
--
-- S -> JWc101uW02 -> CQ -> WS
--


UNDEPLOY APPLICATION NameW02.W02;
DROP APPLICATION NameW02.W02 CASCADE;
CREATE APPLICATION W02 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW02;


CREATE SOURCE CsvSourceW02 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW02;


END FLOW DataAcquisitionW02;



CREATE FLOW DataProcessingW02;

CREATE TYPE DataTypeW02 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW02 OF DataTypeW02;

CREATE CQ CSVStreamW02_to_DataStreamW02
INSERT INTO DataStreamW02
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW02;

CREATE JUMPING WINDOW JWc101uW02
OVER DataStreamW02
KEEP 101 ROWS;

CREATE WACTIONSTORE WactionStoreW02 CONTEXT OF DataTypeW02
EVENT TYPES ( DataTypeW02 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWc101uW02_to_WactionStoreW02
INSERT INTO WactionStoreW02
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc101uW02;

END FLOW DataProcessingW02;



END APPLICATION W02;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

CREATE SOURCE @SourceName@ USING MySqlReader  ( 
TransactionSupport: false, 
  FetchTransactionMetadata: false, 
  Compression: false, 
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', 
  Password_encrypted: 'false', 
  ConnectionURL: '@ConnectionURL@', 
  Fetchsize: 0, 
  ConnectionPoolSize: 10, 
  Username: '@UN@', 
  cdcRoleName: 'STRIIM_READER', 
  Password: '@PWD@', 
  Tables: 'qatest.%', 
  FilterTransactionBoundaries: true, 
  SendBeforeImage: true, 
  AutoDisableTableCDC: false ) 
OUTPUT TO @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;


CREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'), 'OpTime',META(x, 'TimeStamp'))) FROM @SRCINPUTSTREAM@ x; ;

CREATE TARGET @targetName@ USING DatabaseWriter ( 
ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  ParallelThreads: '', 
  CheckPointTable: 'CHKPOINT', 
  Password_encrypted: 'false', 
  ConnectionURL: '@ConnectionURL@', 
  CommitPolicy: 'EventCount:1,Interval:60', 
  StatementCacheSize: '50', 
  DatabaseProviderType: 'Default', 
  Username: '@UN@', 
  Password: '@PWD@', 
  PreserveSourceTransactionBoundary: 'false', 
  BatchPolicy: 'EventCount:1,Interval:60', 
  Tables: 'qatest.%, dbo.% columnmap(opt_type=@USERDATA(OpType),opt_time=@USERDATA(OpTime));' ) 
INPUT FROM admin.sqlreader_cq_out;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@
 Using OracleReader
(
Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@SourceConnectURL@',
Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

--
-- Recovery Test 25 with two sources, two jumping count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5W -> CQ1 -> WS
--   S2 -> Jc6W -> CQ2 -> WS
--

STOP Recov25Tester.RecovTest25;
UNDEPLOY APPLICATION Recov25Tester.RecovTest25;
DROP APPLICATION Recov25Tester.RecovTest25 CASCADE;
CREATE APPLICATION RecovTest25 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION RecovTest25;

--
-- Canon Test W10
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for an unpartitioned sliding count window
--
-- S -> SWc5u -> CQ -> WS
--


UNDEPLOY APPLICATION NameW10.W10;
DROP APPLICATION NameW10.W10 CASCADE;
CREATE APPLICATION W10 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW10;

CREATE SOURCE CsvSourceW10 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW10;

END FLOW DataAcquisitionW10;


CREATE FLOW DataProcessingW10;

CREATE TYPE DataTypeW10 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW10 OF DataTypeW10;

CREATE CQ CSVStreamW10_to_DataStreamW10
INSERT INTO DataStreamW10
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW10;

CREATE WINDOW SWc5uW10
OVER DataStreamW10
KEEP 5 ROWS;

CREATE WACTIONSTORE WactionStoreW10 CONTEXT OF DataTypeW10
EVENT TYPES ( DataTypeW10 KEY(word) )
@PERSIST-TYPE@

CREATE CQ SWc5uW10_to_WactionStoreW10
INSERT INTO WactionStoreW10
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM SWc5uW10;

END FLOW DataProcessingW10;



END APPLICATION W10;

stop @APPNAME@;
undeploy application @APPNAME@;
--drop exceptionstore admin.Oracle12C_To_Oracle12CApp_ExceptionStore;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ use exceptionstore;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@',
    CommitPolicy: 'Interval:5'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;
create or replace cq @cq@
insert into @finalstream@
select exceptionType,action,appName,entityType,entityName,className,message,relatedActivity from @APPNAME@_ExceptionStore;

Create target @targetfile@ using filewriter (
filename:'@APPNAME@_file.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using jsonFormatter()
input from @finalstream@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop @APPNAME@_ExpStore;
undeploy application @APPNAME@_ExpStore;
drop application @APPNAME@_ExpStore cascade;
CREATE APPLICATION @APPNAME@_ExpStore;

CREATE TYPE @APPNAME@_ExpStore_CDCStreams_Type  (
  evtlist java.util.List  
 );

CREATE STREAM @APPNAME@_ExpStore_CDCStreams OF @APPNAME@_ExpStore_CDCStreams_Type;

CREATE CQ @APPNAME@_ReadFromExpStore 
INSERT INTO @APPNAME@_ExpStore_CDCStreams
select to_waevent(s.relatedObjects) as evtlist from admin.@APPNAME@_ExceptionStore [jumping 5 second] s;

CREATE STREAM @APPNAME@_ExpStore_CDCEventStream OF Global.WAEvent;

CREATE CQ @APPNAME@_ExpStore_GetCDCEvent 
INSERT INTO @APPNAME@_ExpStore_CDCEventStream
SELECT com.webaction.proc.events.WAEvent.makecopy(cdcevent) FROM @APPNAME@_ExpStore_CDCStreams a, iterator(a.evtlist) cdcevent;

CREATE CQ @APPNAME@_ExpStore_JoinDataCQ
INSERT INTO @APPNAME@_ExpStore_JoinedDataStream
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1])
        from @APPNAME@_ExpStore_CDCEventStream f;
        
CREATE OR REPLACE TARGET @APPNAME@_ExpStore_WriteToFileAsJSON USING FileWriter  ( 
  filename: 'expEvent_Oracle',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:1,interval:30',
  adapterName: 'FileWriter',
  directory: 'ExpStore_logs',
  rolloverpolicy: 'eventcount:6,interval:30s'
 ) 
FORMAT USING JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter',
  formatterName: 'JSONFormatter',
  jsonMemberDelimiter: '\n',
  jsonobjectdelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true'
 ) 
INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;
        
CREATE TARGET @APPNAME@_ExpStore_dbtarget USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1,Interval:1',
CommitPolicy:'Interval:1,Eventcount:1',
Tables:'@TargetTable@'
) INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;

END APPLICATION @APPNAME@_ExpStore;

deploy application @APPNAME@_ExpStore;
start @APPNAME@_ExpStore;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

create source @SourceName1@ USING IncrementalBatchReader
(
  FetchSize: 10,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: '@startPosition@',
  PollingInterval: '20sec'
)
OUTPUT TO @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:@targetsys@) input from @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
  ConnectionURL:'@READER-URL@',
  Username:'@READER-UNAME@',
  Password:'@READER-PASSWORD@',
  BatchPolicy:'Eventcount:1,Interval:1',
  CommitPolicy:'Eventcount:1,Interval:1',
  Checkpointtable:'RGRN_CHKPOINT',
  Tables:'@WATABLES@,@WATABLES@_target'
) INPUT FROM @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;

DEPLOY APPLICATION @APPNAME@;
start application @APPNAME@;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
drop application @APPNAME1@ cascade;

CREATE APPLICATION @APPNAME1@;

create source @SourceName2@ USING IncrementalBatchReader
(
FetchSize: 10,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn1@,
  startPosition: '@startPosition1@',
  PollingInterval: '20sec'
)
OUTPUT TO @SRCINPUTSTREAM1@;

create Target @targetsys1@ using SysOut(name:@targetsys1@) input from @SRCINPUTSTREAM1@;

CREATE TARGET @targetName1@ USING DatabaseWriter(
  ConnectionURL:'@READER-URL@',
  Username:'@READER-UNAME@',
  Password:'@READER-PASSWORD@',
  BatchPolicy:'Eventcount:1,Interval:1',
  CommitPolicy:'Eventcount:1,Interval:1',
  Checkpointtable:'RGRN_CHKPOINT',
  Tables:'@WATABLES_target'
) INPUT FROM @SRCINPUTSTREAM1@;

END APPLICATION @APPNAME1@;

DEPLOY APPLICATION @APPNAME1@;
start application @APPNAME1@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ recovery 5 second Interval;
CREATE SOURCE @srcName@ USING PostgreSQLReader  

(
  ReplicationSlotName:'@srcreplicationslot@',
  FilterTransactionBoundaries:'true',
  Username:'@srcusername@',
  Password_encrypted:false,
  ConnectionURL: '@srcurl@',
  adapterName:'PostgreSQLReader',
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  Password:'@srcpassword@',
  Tables:'@srcschema@.@srctable@'
) 
OUTPUT TO @outstreamname@ ;

CREATE OR REPLACE TARGET @tgtName@ USING DatabaseWriter  

(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'@tgtusername@',
  BatchPolicy:'EventCount:1,Interval:0',
  CommitPolicy:'EventCount:1,Interval:0',
  ConnectionURL:'@tgturl@',
  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
  Password:'@tgtpassword@'
) 
INPUT FROM @instreamname@;
End APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;


CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using MysqlReader
(
   adapterName: MysqlReader,
   CDDLAction: Process,
   CDDLCapture: false,
   Compression: false,
   ConnectionURL: jdbc:mysql://localhost:3306/waction,
   FilterTransactionBoundaries: true,
   Password: ReaderPassword,
   SendBeforeImage: true,
   Tables: srcTable,
   Username: ReaderUsername
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;

CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatc (
  StartPosition: '@startPosition@',
  ConnectionURL: '@sourceURL@',
  Username: '@userName@',
  Tables: '@tables@',
  CheckColumn: '@checkColum@',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  Password: '@password@',
  Password_encrypted: 'false',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

stop httpjsonapp;
undeploy application httpjsonapp;
drop APPLICATION httpjsonapp cascade;
create application httpjsonapp;
create source HTTPSource using HTTPReader (
        IpAddress:'127.0.0.1',
        PortNo:'10001'
) OUTPUT TO HttpDataStream;

create Target HttpDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/httpjsondata') input from HttpDataStream;
end application httpjsonapp;

--
-- Recovery Test 8
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov8Tester.RecovTest8;
UNDEPLOY APPLICATION Recov8Tester.RecovTest8;
DROP APPLICATION Recov8Tester.RecovTest8 CASCADE;
CREATE APPLICATION RecovTest8 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END APPLICATION RecovTest8;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING MSSqlReader
(
  Compression: false,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.test01',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
)
OUTPUT TO @APPNAME@_SS;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'NULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true	,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

--
-- Recovery Test 21 with two sources, two sliding count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5W -> CQ1 -> WS
-- S2 -> Sc6W -> CQ2 -> WS
--

STOP Recov21Tester.RecovTest21;
UNDEPLOY APPLICATION Recov21Tester.RecovTest21;
DROP APPLICATION Recov21Tester.RecovTest21 CASCADE;
CREATE APPLICATION RecovTest21 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION RecovTest21;

--
-- Canon Test W50
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for a partitioned sliding count window
--
-- S -> SWc5p -> CQ -> WS
--


UNDEPLOY APPLICATION NameW50.W50;
DROP APPLICATION NameW50.W50 CASCADE;
CREATE APPLICATION W50 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW50;

CREATE SOURCE CsvSourceW50 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW50;

END FLOW DataAcquisitionW50;


CREATE FLOW DataProcessingW50;

CREATE TYPE DataTypeW50 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW50 OF DataTypeW50;

CREATE CQ CSVStreamW50_to_DataStreamW50
INSERT INTO DataStreamW50
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW50;

CREATE WINDOW SWc5pW50
OVER DataStreamW50
KEEP 5 ROWS
PARTITION BY word;

CREATE WACTIONSTORE WactionStoreW50 CONTEXT OF DataTypeW50
EVENT TYPES ( DataTypeW50 KEY(word) )
@PERSIST-TYPE@

CREATE CQ SWc5pW50_to_WactionStoreW50
INSERT INTO WactionStoreW50
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM SWc5pW50
GROUP BY word;

END FLOW DataProcessingW50;



END APPLICATION W50;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

stop application @APPNAME@2;
undeploy application @APPNAME@2;
drop application @APPNAME@2 cascade;

create application @APPNAME@ recovery 1 second interval;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@'
)
OUTPUT TO @APPNAME@Stream;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;



CREATE TYPE test_typeJo (
 GG String,
CORP String,
DIV String,
YR FLOAT,
WK INT,
SOURCEID String,
JENUM INT,
ACCTPRIME String,
ACCTSUB String,
AMTTYPE String,
FAC String,
DEPT String,
SECT String,
REF String,
AMT FLOAT,
UNITAMT INT,
POSTINGDATE String,
EFFECTIVEDATE String,
SOURCEDESC String,
SEQUENCENUMBER INT,
SYSTEMN String
);

Create stream cqAsJSONNodeStreamJo of test_typeJo;

CREATE CQ cqAsJSONNodeStreamJo
INSERT into JSONNodeStreamJo
    select 
    data.get('GG'),
data.get('CORP'),
data.get('DIV'),
TO_FLOAT(data.get('YR')),
TO_INT(data.get('WK')),
data.get('SOURCE-ID'),
TO_INT(data.get('JE-NUM')),
data.get('ACCT-PRIME'),
data.get('ACCT-SUB'),
data.get('AMT-TYPE'),
data.get('FAC'),
data.get('DEPT'),
data.get('SECT'),
data.get('REF'),
TO_FLOAT(data.get('AMT')),
TO_INT(data.get('UNIT-AMT')),
data.get('POSTING-DATE'),
data.get('EFFECTIVE-DATE'),
data.get('SOURCE-DESC'),
TO_INT(data.get('SEQUENCE-NUMBER')),
data.get('SYSTEM-N')
from @APPNAME@Stream js;

create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:100,Interval:5',
  CommitPolicy: 'EventCount:100,Interval:5',
  Tables: 'QATEST.@table@'
)
input from JSONNodeStreamJo;

end application @APPNAME@;

-----------------------------------------------
create application @APPNAME@2 recovery 1 second interval;

create source @APPNAME@2_SRC Using FileReader(
	directory:'@DIRECTORY2@',
	WildCard:'@FILENAME2@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP12@',
  dataFileFont: '@PROP22@',
  copybookSplit: '@PROP32@',
  dataFileOrganization: '@PROP42@',
  copybookDialect: '@PROP52@', 
  skipIndent:'@PROP62@',
  DatahandlingScheme:'@PROP72@'
 )
OUTPUT TO @APPNAME@2Stream;

create Target @APPNAME@2Target using FileWriter(
    filename :'@FILE2@',
    directory : '@FOLDER2@'
)
format using JsonFormatter (
)
input from @APPNAME@2Stream;


CREATE TYPE test_typeRe 
(
node_new com.fasterxml.jackson.databind.JsonNode,
node_name com.fasterxml.jackson.databind.JsonNode,
node_addr com.fasterxml.jackson.databind.JsonNode
);

Create stream cqAsJSONNodeStreamRe of test_typeRe;

CREATE CQ GetPOAsJsonNodesRe
INSERT into cqAsJSONNodeStreamRe
select 
data.get('ACCTS-RECORD'),
data.get('ACCTS-RECORD').get('NAME'),
data.get('ACCTS-RECORD').get('ADDRESS3')
from @APPNAME@2Stream js;

create type finaldtypeRe
(ACCOUNT_NO int,
FIRST_NAME String,
LAST_NAME String,
ADDRESS1 String,
ADDRESS2 String,
CITY String,
STATE String,
ZIP_CODE int);

CREATE STREAM getdataStreamPS OF finaldtypeRe;

CREATE CQ getdataRe
INSERT into getdataStreamPS
select JSONGetInteger(x.node_new,"ACCOUNT-NO"),
JSONGetString(x.node_name,"FIRST-NAME"),
JSONGetString(x.node_name,"LAST-NAME"),
JSONGetString(x.node_new,"ADDRESS1"),
JSONGetString(x.node_new,"ADDRESS2"),
JSONGetString(x.node_addr,"CITY"),
JSONGetString(x.node_addr,"STATE"),
JSONGetInteger(x.node_addr,"ZIP-CODE")
from cqAsJSONNodeStreamRe x;

create Target @APPNAME@2DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:1000,Interval:50',
  CommitPolicy: 'EventCount:1000,Interval:50',
  Tables: 'QATEST.@table2@'
)
input from getdataStreamPS;

end application @APPNAME@2;
deploy application @APPNAME@2 on all in default;
deploy application @APPNAME@ on all in default;

start application @APPNAME@2;
start application @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;
CREATE  SOURCE @FirstSourceName@ USING DatabaseReader  ( 
  ConnectionURL: '@SourceConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  Tables: '@SourceTable@',
  ReplicationSlotName: 'null'
 ) OUTPUT TO @SRCFirstINPUTSTREAM@;

 CREATE  SOURCE @MiddleSourceName@ USING DatabaseReader  ( 
  ConnectionURL: '@SourceConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  Tables: '@SourceTable@',
  ReplicationSlotName: 'null'
 )
OUTPUT TO @SRCMiddleINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCMiddleINPUTSTREAM@;

CREATE  TARGET @FirsttargetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
 ) 
 INPUT FROM @SRCFirstINPUTSTREAM@;

 CREATE  TARGET @MiddletargetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
 )
INPUT FROM @SRCMiddleINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

--CREATE APPLICATION @APPNAME@;
create application @APPNAME@ Recovery 5 second Interval;

--create or replace flow @APPNAME@_agentflow;

CREATE OR REPLACE SOURCE @APPNAME@_source USING DatabaseReader  (
  Username: 'qatest',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.EMP_INIT',
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: 'qatest'
 )
OUTPUT TO @APPNAME@_stream ;

--end flow @APPNAME@_@APPNAME@_agentflow;

CREATE OR REPLACE TARGET @APPNAME@_target USING CassandraCosmosDBWriter  (
  AccountEndpoint: 'cassandracosmostest.cassandra.cosmos.azure.com',
  AccountKey: 'pqDZvVgbdSCg7VzIzD77dAhPG2odGRZPLhAQA1qnZbAKoIDk6RuQX5r2phbRQFnR1l54qxOcvBXNdz8DeijYIg==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  Tables: 'QATEST.Source1,test.target1',
  adapterName: 'CassandraCosmosDBWriter'
 )
 INPUT FROM @APPNAME@_stream;

 END APPLICATION @APPNAME@;

deploy application @APPNAME@;
 --deploy application @APPNAME@ with agentflow in agents;
 start application @APPNAME@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING CosmosDBWriter  (
  BatchPolicy: 'EventCount:1000,Interval:1',
  ServiceEndpoint: 'https://souvik.documents.azure.com:443/',
  ConnectionPoolSize: '10',
  AccessKey: 'z1CfmzAy5QwB5MN7bWIinM9gYmJn7zWo1wOaadvaCErqaTCqlb7srpAx7muPYWhJwnYq3plOQoBNENn1xPmkfQ==',
  adapterName: 'CosmosDBWriter',
  Collections: 'QATEST.TEST,db2.TEST keycolumns(id)',
  ConnectionRetryPolicy: 'RetryInterval:1,MaxRetries:0',
  KeySeparator: ':',
  AccessKey_encrypted: false
 )
INPUT FROM @STREAM@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@ USING Ojet
(
 Username:'@OJET-UNAME@',
Password:'@OJET-PASSWORD@',
ConnectionURL:'@OCI-URL@',
Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectURL@',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

-- detectFraudsterApp1.tql, detectFraudsterApp1_vis_settings.json, apiCallLogs.txt are needed to run this example.
-- gamelogs simulates api call logs for an online game. xferCur represents virtual currency transfer.
-- If this method is called by same user more than once in 1000 records its anomaly and hence waction.

IMPORT static com.webaction.runtime.converters.DateConverter.*;

UNDEPLOY APPLICATION admin.detectFraudsterApp1;
DROP APPLICATION admin.detectFraudsterApp1 CASCADE;
CREATE APPLICATION detectFraudsterApp1;


/* READ DATA FROM A CSV FILE. USE CSV-READER  */
CREATE source gameCSVDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@/persistence',
  header:No,
  wildcard:'apiCallLogs.txt',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO PCsvStream;

create jumping window apiwindow over PCsvStream keep 1000 rows;

CREATE TYPE apiCallData(
  date DateTime,
  apiName String,
  amount int,
  username String key
);

CREATE STREAM apiCallStream OF apiCallData;

/* FROM INPUT DATA READ ABOVE, CREATE EVENTS OF TYPE apiCallData
 * TO_INT(data[10]) > 100000 --> 3 WACTIONS
 * TO_INT(data[10]) > 50000 --> 8 WACTIONS
 * TO_INT(data[10]) > 25000 --> 11 WACTIONS
 * TO_INT(data[10]) > 20000 --> 14 WACTIONS
 * */

CREATE CQ gameCQ
INSERT INTO apiCallStream
SELECT
    TO_DATE(TO_LONG(data[3])),
    data[10],
    TO_INT(data[11]),
    data[12]
FROM apiwindow
where cast (data[10] as String)  =  'xferCur' and  TO_INT(data[11]) > 50000;

create TYPE apiCallContext(
  date DateTime,
  apiName String,
  amount int,
  username String key
);

/* PERSISTING TO DERBY  */
CREATE WACTIONSTORE fraudWactionsDerby
CONTEXT OF apiCallContext
EVENT TYPES (apiCallData )
PERSIST EVERY 2 second USING
(
JDBC_DRIVER:'@WASTORE-DRIVER@',
JDBC_URL:'@WASTORE-URL@;CREATE=true',
JDBC_USER:'@WASTORE-UNAME@',
JDBC_PASSWORD:'@WASTORE-PASSWORD@',
DDL_GENERATION:'drop-and-create-tables',
LOGGING_LEVEL:'SEVERE'
);


CREATE CQ populateFraudWactionsDerby
INSERT INTO fraudWactionsDerby
SELECT date, apiName, amount, username
FROM apiCallStream
LINK SOURCE EVENT;


/* PERSISTING TO MYSQL  */
/*
CREATE WACTIONSTORE fraudWactionsMySQL
CONTEXT OF apiCallContext
EVENT TYPES (apiCallData )
PERSIST EVERY 3 second USING
(
JDBC_DRIVER:'com.mysql.jdbc.Driver',
JDBC_URL:'jdbc:mysql://127.0.0.1:3306/test',
JDBC_USER:root,
JDBC_PASSWORD:'root',
DDL_GENERATION:'drop-and-create-tables',
LOGGING_LEVEL:'SEVERE'
);

CREATE CQ populateFraudWactionsMySQL
INSERT INTO fraudWactionsMySQL
SELECT date, apiName, amount, username
FROM apiCallStream
LINK SOURCE EVENT;
*/
/* PERSISTING TO MONGODB  */
/*
CREATE WACTIONSTORE fraudWactionsMongo
CONTEXT OF apiCallContext
EVENT TYPES (apiCallData )
PERSIST EVERY 5 second USING
(
NOSQL_PROPERTY:'localhost:27017',
TARGET_DATABASE:'org.eclipse.persistence.nosql.adapters.mongo.MongoPlatform',
DDL_GENERATION:'drop-and-create-tables',
DB_NAME:db
);


CREATE CQ populateFraudWactionsMongo
INSERT INTO fraudWactionsMongo
SELECT date, apiName, amount, username
FROM apiCallStream
LINK SOURCE EVENT;
*/



--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM PCsvStream;
--CREATE TARGET output2 USING SysOut(name : selectedinput) input FROM apiCallStream;


CREATE VISUALIZATION detectFraudsterApp1 "@TEST-DATA-PATH@/json/detectFraudsterApp1_vis_settings.json";

END APPLICATION detectFraudsterApp1;

undeploy application dev15823;
alter application dev15823;

CREATE TYPE ModifyNotNull (
  x int,
  y string
);

CREATE STREAM FormattedStream OF ModifyNotNull;

CREATE  CQ InsertWactions
INSERT INTO FormattedStream
SELECT
    TO_INT(data[0]),
   	"notnull"
FROM LogminerStream;

Create or replace Target test using SysOut (name:test) input from FormattedStream;

CREATE OR REPLACE TARGET WriteCDCMySQL USING DatabaseWriter  ( 
  Username: '@USERNAME@',
  BatchPolicy: 'Eventcount:5,Interval:5',
  CommitPolicy: 'Eventcount:5,Interval:5',
  ConnectionURL: '@URL@',
  Tables: '@TABLES@',
  Checkpointtable: 'CHKPOINT',
  --IgnorableExceptionCode:'1062',
  Password: '@PASSWORD@'
 ) 
INPUT FROM FormattedStream;

END APPLICATION dev15823;
alter application dev15823 recompile;
deploy application dev15823;
start dev15823;

--
-- Crash Recovery Test 1 on two node cluster with Kafka Stream
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP APPLICATION KStreamN2S2CR1Tester.KStreamN2S2CRTest1;
UNDEPLOY APPLICATION KStreamN2S2CR1Tester.KStreamN2S2CRTest1;
DROP APPLICATION KStreamN2S2CR1Tester.KStreamN2S2CRTest1 CASCADE;

DROP USER KStreamN2S2CR1Tester;
DROP NAMESPACE KStreamN2S2CR1Tester CASCADE;
CREATE USER KStreamN2S2CR1Tester IDENTIFIED BY KStreamN2S2CR1Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR1Tester;
CONNECT KStreamN2S2CR1Tester KStreamN2S2CR1Tester;

CREATE APPLICATION KStreamN2S2CRTest1 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest1;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest1;

CREATE FLOW DataProcessingKStreamN2S2CRTest1;

CREATE TYPE WactionTypeKStreamN2S2CRTest1 (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest1 CONTEXT OF WactionTypeKStreamN2S2CRTest1
EVENT TYPES ( WactionTypeKStreamN2S2CRTest1 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsKStreamN2S2CRTest1
INSERT INTO WactionsKStreamN2S2CRTest1
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

END FLOW DataProcessingKStreamN2S2CRTest1;

END APPLICATION KStreamN2S2CRTest1;

STOP APPLICATION testApp;
UNDEPLOY APPLICATION testApp;
DROP APPLICATION testApp CASCADE;
CREATE APPLICATION testApp recovery 5 SECOND Interval;


  CREATE OR REPLACE SOURCE testApp_Source Using PostgreSQLReader( 
  
  ReplicationSlotName:'test_slot',
  FilterTransactionBoundaries:'true',
  Username:'waction',
  Password_encrypted:false,
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  adapterName:'PostgreSQLReader',
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  Password:'w@ct10n',
  Tables:'public.sourceTable',
  ExcludedTables:'public.chkpoint'
 ) OUTPUT TO PGtoBQ_Stream;


CREATE OR REPLACE TARGET testApp_Target USING BigQueryWriter  (
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  projectId:'striimqa-214712',
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:5,Interval:120',
  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', 
  AllowQuotedNewLines: 'false', 
  adapterName: 'BigQueryWriter', 
  optimizedMerge: 'true', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', 
  StandardSQL: 'true', 
  QuoteCharacter: '\"', 
  Tables: 'public.sourceTable,BQAllpl.oratobqtgt',
  Mode: 'MERGE',
  StandardSQL: 'true',
  QuoteCharacter: '\"'
  ) INPUT FROM PGtoBQ_Stream;

CREATE OR REPLACE TARGET testApp_SysOut USING Global.SysOut (name: 'wa') INPUT FROM PGtoBQ_Stream;

END APPLICATION testApp;
DEPLOY APPLICATION testApp;
START testApp;

--To be used with NonCdcAdapterCommons.startExceptionStoreApp()
CREATE APPLICATION @APPNAME@;

CREATE CQ @APPNAME@CQ INSERT INTO @APPNAME@CQOut
select x.exceptionType,
x.action,
x.appName,
x.entityType,
x.entityName,
x.className,
x.message,
x.relatedActivity,
x.relatedObjects,
x.relatedEntity,
x.exceptionCode
from @EXCEPTIONAPPNAME@_ExceptionStore x;

CREATE TARGET @APPNAME@Trgt USING FileWriter ()
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@CQOut;

END APPLICATION @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

cCREATE TARGET @TARGET@ USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: 'access_key',
	  --secretaccesskey: 'secret_access',
	  Tables: 'QATEST.%,QATEST.%',
	   S3IAMRole:'@IAMROLE@',
	uploadpolicy:'EventCount:7'
	) INPUT FROM @STREAM@;
	
end flow @APPNAME@_serverflow;

end application @APPNAME@;

INPUT FROM @STREAM@;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;

 CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01;striim.test02;striim.test03',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',
 startPosition: 'striim.test01=1;striim.test02=-1;%=0',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream ;

  CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

create target AzureSQLDWHTarget using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream;


CREATE TARGET log USING LOGWRITER
(
 name:log,
 filename:'LOGFILENAME'
)
INPUT FROM data_stream;


END APPLICATION IR;

deploy application IR;
start IR;

CREATE APPLICATION @AppName@;

CREATE OR REPLACE SOURCE Filereader_Src USING FileReader  (
   WildCard: 'posdata100.csv',
  directory: '@SrcDir@',
  positionbyeof: false)
 PARSE USING DSVParser  (
 )
OUTPUT TO CsvStream ;

CREATE OR REPLACE SOURCE initialLoad_Src USING Global.DatabaseReader (
  QuiesceOnILCompletion: false,
  Tables: '@SrcTableName@',
  adapterName: 'DatabaseReader',
  Password: '@Password@',
  Username: '@UserName@',
  ConnectionURL: '@Srcurl@',
   FetchSize: 10000)
OUTPUT TO ILStream;

Create Type CSVType (
  companyid String,
  merchantId String
);

CREATE STREAM CommonTypedStream OF CSVType;


CREATE OR REPLACE  CQ CsvToPosData
INSERT INTO CommonTypedStream
SELECT
TO_STRING(data[0]).replaceAll("COMPANY ", ""),
data[1]
FROM CsvStream;

CREATE CQ cq1
INSERT INTO CommonTypedStream
SELECT data[0],data[1]
FROM ILStream;


CREATE OR REPLACE TARGET Postgres_Trg USING Global.DatabaseWriter (
  ConnectionURL: '@trgUrl@',
  Username: '@trgUsrName@',
  Tables: '@trgTable@',
  Password: '@trgPswd@',
  CommitPolicy: 'EventCount:10000,Interval:60',
  adapterName: 'DatabaseWriter' )
INPUT FROM CommonTypedStream;

CREATE TARGET filewriter_tgt USING Global.FileWriter (
 directory:'@trgDir@',
  filename: '@fileName@',
  flushpolicy: 'EventCount:10000,Interval:30s',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n' )
INPUT FROM CommonTypedStream;

CREATE OR REPLACE TARGET BigQuery_Target USING Global.BigQueryWriter (
  streamingUpload: 'false',
  projectId: '@projectID@',
  Tables: '@BQTableName@',
  optimizedMerge: 'false',
  ServiceAccountKey: '@ServiceAccountKey@',
  BatchPolicy: 'EventCount:1000000,Interval:90',
  Mode: 'APPENDONLY' )
INPUT from CommonTypedStream;

END APPLICATION @AppName@;

STOP APPLICATION VerticaTester.VW;
UNDEPLOY APPLICATION VerticaTester.VW;
DROP APPLICATION VerticaTester.VW CASCADE;
CREATE APPLICATION VW;

CREATE SOURCE CSVSource USING CSVReader
(
  directory:'@TEST-DATA-PATH@',
  header:No,
  wildcard:'trade.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStream;


CREATE TYPE VerticaStream_t
(
  NameField   String,
  ValueField  String
);

CREATE STREAM VerticaStream OF VerticaStream_t;

CREATE CQ VW_q
  INSERT INTO VerticaStream
  SELECT DATA[0],
         DATA[1]
  FROM CsvStream;

CREATE TARGET WriteToVertica USING VerticaWriter
(
  dbHostName:'@HOST@',
  dbUser:'@USER@',
  dbPassword: '@PASS@',
  dbName: '@DBNAME@',
  tableName: '@SCHEMA@.TRADE'
)
INPUT FROM VerticaStream;

END APPLICATION VW;

use PosTester;
alter application PosApp;

CREATE source CsvDataSource USING CSVReader (
  directory:'Samples/Customer/PosApp/appData',
  header:Yes,
  wildcard:'posdata.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

end application PosApp;

alter application PosApp recompile;

-- Creating a namespace ensures there won't be conflicts with the regular version of
-- PosApp. The only difference between this version and the regular version is
-- that the CQ that parses the source stream includes a PAUSE clauses that introduces a
-- 40-millisecond pause after each event is read, simulating the way the dashboard would
-- work with real-time data.
Stop PosAppImplicit.PosAppImplicit;
undeploy application PosAppImplicit.PosAppImplicit;
drop application PosAppImplicit.PosAppImplicit cascade;


-- The PosApp sample application demonstrates how a credit card
-- payment processor might use WebAction to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.

CREATE Application PosAppImplicit;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells WebAction that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells WebAction to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvToPosData
--
-- A stream's type must be declared before the stream, and a CQ's
-- output stream must be defined before the CQ. Hence type-stream-CQ
-- sequences like the following are very common.

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       To_String(data[9]) as zip
FROM CsvStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP. These correspond to the merchantId,
-- dateTime, hourValue, amount, and zip fields in PosDataStream, as
-- defined by the PosData type.
--
-- The DATETIME field from the source is converted to both a DateTime
-- value, used as the event timestamp by the application, and an int,
-- which is used to look up historical hourly averages from the
-- HourlyAveLookup cache, discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)

-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateOnly
--
-- The PosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue int,
  hourlyAve int
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId as merchantId,
       p.zip as zip,
       FIRST(p.dateTime) as startingTime,
       COUNT(p.merchantId) as count,
       SUM(p.amount) as totalAmount,
       l.hourlyAve/12 as hourlyAve,
       l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END as upperLimit,
       l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END as lowerLimit,
       '<NOTSET>' as category,
       '<NOTSET>' as status
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId as merchantId,
       zip as zip,
       startingTime as startingTime,
       count as count,
       totalAmount as totalAmount,
       hourlyAve as hourlyAve,
       upperLimit as upperLimit,
       lowerLimit as lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END as category,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END as status
FROM MerchantTxRateOnlyStream;


-- WAction store MerchantActivity
--
-- The following group of statements create and populate the MerchantActivity
-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced
-- with merchant details from NameLookup cache and with latitude and longitude
-- values from the USAddressData cache.

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count int,
  HourlyAve int,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);
CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count int,
  totalAmount double,
  hourlyAve int,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
@PERSIST-TYPE@

CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false
)
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;


-- CQ GenerateAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;


-- The following statement loads visualization (Dashboard) settings
-- from a file.


--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM AlertStream;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;


END APPLICATION PosAppImplicit;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING MSSqlReader
(
  Compression: false,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.test01',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
)
OUTPUT TO @APPNAME@_SS;


CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(id,name) COLUMNMAP(id=id,col1=col11,col2=col12)',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

--
-- Kafka Stream Recovery Test 1
-- Bert Hashemi and Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> KS -> WS

STOP KStreamAvroRecov1Tester.KStreamAvroRecovTest1;
UNDEPLOY APPLICATION KStreamAvroRecov1Tester.KStreamAvroRecovTest1;
DROP APPLICATION KStreamAvroRecov1Tester.KStreamAvroRecovTest1 CASCADE;
DROP USER KStreamAvroRecov1Tester;
DROP NAMESPACE KStreamAvroRecov1Tester CASCADE;
CREATE USER KStreamAvroRecov1Tester IDENTIFIED BY KStreamAvroRecov1Tester;
-- GRANT 'Global:create,drop:deploymentgroup:*' TO USER KStreamAvroRecov1Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamAvroRecov1Tester;
CONNECT KStreamAvroRecov1Tester KStreamAvroRecov1Tester;

CREATE APPLICATION KStreamAvroRecovTest1 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250', dataformat:'avro');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE or REPLACE TYPE KafkaCsvStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF KafkaCsvStreamType 
EVENT TYPES ( KafkaCsvStreamType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

END APPLICATION KStreamAvroRecovTest1;

stop tpcc;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;


Create Source @SourceName@
 Using Ojet
(
 Username:'@READER-UNAME@',
 Password:'@READER-PASSWORD@',
 ConnectionURL:'@CDC-READER-URL@',
 Tables: '@WATABLES-SRC@',
 FetchSize:1,
 QueueSize:25000,
 CommittedTransactions:true,
 Compression:true,
 CaptureDDL: true,
 SendBeforeImage:true
) Output To @SRCINPUTSTREAM@;


create Target @targetsys@ using SysOut(name:OrgData) input from DataStream;

CREATE TARGET @targetName@ USING databasewriter(
  Username: '@WRITER-UNAME@',
  Password: '@WRITER-PASSWORD@',
  ConnectionURL:'@WRITER-URL@',
  BatchPolicy:'EventCount:1,Interval:1',
  CommitPolicy:'EventCount:1,Interval:1',
  Tables: '@WATABLES-SRC@,@WATABLES-TGT@'
) INPUT FROM @SRCINPUTSTREAM@;


END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP APPLICATION @appname@routerApp;
UNDEPLOY APPLICATION @appname@routerApp;
DROP APPLICATION @appname@routerApp CASCADE;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',
  acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

CREATE APPLICATION @appname@routerApp RECOVERY 10 SECOND INTERVAL;

CREATE  SOURCE @appname@OraSource USING OracleReader  (
Username: 'qatest',
Password: 'qatest',
ConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',
Tables: 'QATEST.TGT_T%',
 FetchSize:'100'
)
OUTPUT TO @appname@MasterStream1;

-- CREATE STREAM @appname@ss1 OF Global.waevent persist using Global.DefaultKafkaProperties;
-- CREATE STREAM @appname@ss2 OF Global.waevent persist using Global.DefaultKafkaProperties;
-- CREATE STREAM @appname@ss3 OF Global.waevent persist using Global.DefaultKafkaProperties;

CREATE STREAM @appname@ss1 OF Global.waevent PERSIST USING KafkaPropset;
CREATE STREAM @appname@ss2 OF Global.waevent PERSIST USING KafkaPropset;
CREATE STREAM @appname@ss3 OF Global.waevent PERSIST USING KafkaPropset;

CREATE OR REPLACE ROUTER @appname@tablerouter1 INPUT FROM @appname@MasterStream1 s CASE
WHEN meta(s,"TableName").toString()='QATEST.TGT_T1' THEN ROUTE TO @appname@ss1,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T2' THEN ROUTE TO @appname@ss2,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T3' THEN ROUTE TO @appname@ss3,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T4' THEN ROUTE TO @appname@ss4,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T5' THEN ROUTE TO @appname@ss5,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T6' THEN ROUTE TO @appname@ss6,
ELSE ROUTE TO @appname@ss_else;

create Target @appname@FileTarget_1 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'
)
FORMAT USING dsvFormatter ()
input from @appname@ss1;

create Target @appname@FileTarget_2 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'
)
FORMAT USING dsvFormatter ()
input from @appname@ss2;

create Target @appname@FileTarget_3 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from @appname@ss3;

CREATE OR REPLACE TARGET @appname@KafkaTarget_4 USING KafkaWriter VERSION '0.11.0' (
  brokerAddress: 'localhost:9092',
  Topic: 'target4'
 )
FORMAT USING JSONFormatter  (
 )
INPUT FROM @appname@ss4;

CREATE OR REPLACE TARGET @appname@KafkaTarget_5 USING KafkaWriter VERSION '0.11.0' (
  brokerAddress: 'localhost:9092',
  Topic: 'target5'
 )
FORMAT USING JSONFormatter  (
 )
INPUT FROM @appname@ss5;

CREATE OR REPLACE TARGET @appname@KafkaTarget_6 USING KafkaWriter VERSION '0.11.0' (
  brokerAddress: 'localhost:9092',
  Topic: 'target6'
 )
FORMAT USING JSONFormatter  (
 )
INPUT FROM @appname@ss6;




end application @appname@routerApp;
deploy application @appname@routerApp;
start @appname@routerApp;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 10 second interval;
create source CSVSource using FileReader (
	directory:'@DIR@',
	WildCard:'@WILDCARD@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  curr String,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       data[6],
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target BlobT using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	uploadpolicy:'EventCount:100,interval:5s'
)
format using XMLFormatter (
rootelement:'document',
elementtuple:'MerchantName:merchantId:text=merchantname'
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

CREATE OR REPLACE APPLICATION @AppName@ USE EXCEPTIONSTORE TTL : '7d';

Create Source @AppName@_source Using OracleReader(
  Username:'@username@',
  Password:'@password@',
  ConnectionURL:'@url@',
  Tables:'@srctableName@',
  Fetchsize:1
)
Output To @AppName@_Stream;
CREATE OR REPLACE TARGET @AppName@_Target USING Global.SnowflakeWriter (
  streamingUpload: 'false',
  useConnectionProfile:'true',
  connectionProfileName: '@CP@',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  columnDelimiter: '|',
  tables: '@srctableName@,@trgtableName@',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:1000,interval:60s',
  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',
  externalStageType: 'Local',
  adapterName: 'SnowflakeWriter',
  fileFormatOptions: 'null_if = \"\"' )
INPUT FROM @AppName@_Stream;
END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

--
-- Recovery Multi Node Test 1 with uniqe objects to this App
-- Nicholas Keene, Bert Hashemi WebAction, Inc.
--
-- App with Single Source, CQ and Waction Store to be deployed on two or more node
-- Full app on node1 and one Source on node2 so source reads from node2 only.
--
-- S -> CQ -> WS
--


CREATE APPLICATION RecovTestMN01
RECOVERY 5 SECOND INTERVAL;



CREATE FLOW DataAcquisition;

CREATE SOURCE CsvSourceMN01 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamMN01;

END FLOW DataAcquisition;



CREATE FLOW DataProcessing;

CREATE TYPE WactionTypeMN01 (
  merchantId String,
  dateTime DateTime,
  amount double,
  city String,
  serverName String KEY
);

CREATE STREAM DataStreamMN01 OF WactionTypeMN01
PARTITION BY serverName;

CREATE CQ CsvToDataMN01
INSERT INTO DataStreamMN01
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10],
    data[11]
FROM CsvStreamMN01;

CREATE WINDOW DataWindowMN01
OVER DataStreamMN01 KEEP WITHIN 30 SECOND ON dateTime
PARTITION BY serverName;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionTypeMN01
EVENT TYPES ( WactionTypeMN01 )
PERSIST EVERY 1 second USING (
JDBC_DRIVER:'@WASTORE-DRIVER@',  JDBC_URL:'@WASTORE-URL@;CREATE=true',
JDBC_USER:'@WASTORE-UNAME@', JDBC_PASSWORD:'@WASTORE-PASSWORD@', pu_name:@WASTORE-TYPE@,
DDL_GENERATION:'create-or-extend-tables',  LOGGING_LEVEL:'SEVERE' );

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    *
FROM DataWindowMN01;

END FLOW DataProcessing;



END APPLICATION RecovTestMN01;


DEPLOY APPLICATION RecovTestMN01 WITH DataAcquisition ON ALL IN agents, DataProcessing ON ALL IN servers;

START RecovTestMN01;

STOP OuterJoinTester.OJApp;
UNDEPLOY APPLICATION OuterJoinTester.OJApp;
DROP APPLICATION OuterJoinTester.OJApp cascade;

CREATE APPLICATION OJApp;


CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;


CREATE source wsSource2 USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'bankCards.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO stream2;



CREATE TYPE bankData
(
bankID Integer KEY,
bankName String
);

CREATE TYPE cardData
(
cardID Integer KEY,
cardName String
);


CREATE STREAM wsStream OF bankData;
CREATE STREAM wsStream2 OF cardData;


--Select data from QaStream and insert into wsStream

CREATE CQ csvTobankData
INSERT INTO wsStream
SELECT TO_INT(data[0]), data[1] FROM QaStream;

CREATE CQ csvTobankData2
INSERT INTO wsStream2
SELECT TO_INT(data[0]), data[1] FROM stream2;

CREATE JUMPING WINDOW win1 OVER wsStream KEEP 20 rows;


CREATE JUMPING WINDOW win2 OVER wsStream2 KEEP 4 rows;

END APPLICATION OJApp;

STOP APPLICATION KafkaWPTester.KWApp;
STOP APPLICATION KafkaWPTester.KRApp;
UNDEPLOY APPLICATION KafkaWPTester.KWApp;
UNDEPLOY APPLICATION KafkaWPTester.KRApp;
DROP APPLICATION KafkaWPTester.KWApp CASCADE;
DROP APPLICATION KafkaWPTester.KRApp CASCADE;

CREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;
GRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;
CONNECT KafkaWPTester KafkaWPTester;


CREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE CSVSource USING FileReader (
	directory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',
    WildCard:'access_log_half',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;

CREATE TYPE AccessLogType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;

CREATE CQ AceeslogCQ
INSERT INTO TypedAccessLogStream
SELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]
FROM FileStream;

create Target DsvWriter using KafkaWriter VERSION '2.1.0' ( 
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V2dsvExistTopic',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream;


create Target JsonWriter using KafkaWriter VERSION '2.1.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V2jsonExistTopic',
	Mode:'Sync'
    )
 
FORMAT USING JSONFormatter ()
input from TypedAccessLogStream;


create Target AvroWriter using KafkaWriter VERSION '2.1.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V2avroExistTopic',
    Mode:'Sync'
    )
 
FORMAT USING AvroFormatter (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')
input from TypedAccessLogStream;


END APPLICATION KWApp;

DEPLOY APPLICATION KWApp on any in default;


-- Kafka Reader Apps

CREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;


CREATE TYPE AccessLogType2(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE Stream KafkaDSVReaderStream of AccessLogType2;
CREATE Stream KafkaJsonReaderStream of AccessLogType2;
CREATE Stream KafkaAvroReaderStream of AccessLogType2;


CREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V2dsvExistTopic',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;




CREATE CQ DSVtoKafkaDSVReaderStream
INSERT INTO KafkaDSVReaderStream
SELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]
FROM KafkaReaderStream1;

CREATE TARGET DSVDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V2_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaDSVReaderStream;

CREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V2jsonExistTopic',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;




CREATE CQ DSVtoKafkaJsonReaderStream
INSERT INTO KafkaJsonReaderStream
SELECT 
data.get('Col1').textValue(),
data.get('Col2').textValue(),
data.get('Col3').textValue(),
data.get('Col4').textValue(),
data.get('Col5').textValue(),
data.get('Col6').textValue(),
data.get('Col7').textValue(),
data.get('Col8').textValue(),
data.get('Col9').textValue(),
data.get('Col10').textValue()
FROM KafkaReaderStream2;

CREATE TARGET JSONDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V2_RT_JSON')
FORMAT USING DSVFormatter()
INPUT FROM KafkaJsonReaderStream;

CREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V2avroExistTopic',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'
)
OUTPUT TO KafkaReaderStream3;



CREATE Stream KRTypedStream of AccessLogType;

CREATE CQ DSVtoKafkaAvroReaderStream
INSERT INTO KafkaAvroReaderStream
SELECT    
data.get("Col1").toString(), 
data.get("Col2").toString(), 
data.get("Col3").toString(), 
data.get("Col4").toString(),
data.get("Col5").toString(), 
data.get("Col6").toString(), 
data.get("Col7").toString(), 
data.get("Col8").toString(),
data.get("Col9").toString(), 
data.get("Col10").toString()
FROM KafkaReaderStream3;

CREATE TARGET AVRODump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V2_RT_AVRO')
FORMAT USING DSVFormatter()
INPUT FROM KafkaAvroReaderStream;

END APPLICATION KRApp;
DEPLOY APPLICATION KRApp on any in default;

--
-- Recovery Test 40 with two sources and two WactionStores. A variety of partitioned windows in between
-- assure that we are testing a complicated recovery scenario.
--
-- NOTE THIS APP IS INCONSISTENT AND NOT COMPATIBLE WITH THE CURRENT VERSION OF RECOVERY BECAUSE IT HAS COMBINING STREAMS
--
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> JWc2 -> JWc5 -> WS1
--   S2 -> JWc2 -> JWc7 -> WS2
--

STOP Recov40Tester.RecovTest40;
UNDEPLOY APPLICATION Recov40Tester.RecovTest40;
DROP APPLICATION Recov40Tester.RecovTest40 CASCADE;
CREATE APPLICATION RecovTest40 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStreamTop OF CsvData;

CREATE CQ Csv1ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ Csv2ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;








CREATE JUMPING WINDOW TopJWc2
OVER DataStreamTop KEEP 2 ROWS;







CREATE STREAM DataStreamLeft OF CsvData;
CREATE STREAM DataStreamRight OF CsvData;

CREATE CQ DataToLeft
INSERT INTO DataStreamLeft
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM TopJWc2 p;

CREATE CQ DataToRight
INSERT INTO DataStreamRight
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM TopJWc2 p;






CREATE JUMPING WINDOW LeftJWc5
OVER DataStreamLeft KEEP 5 ROWS;

CREATE JUMPING WINDOW RightJWc10
OVER DataStreamRight KEEP 10 ROWS;



CREATE WACTIONSTORE WactionsLeft CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsRight CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ ToWactionsLeft
INSERT INTO WactionsLeft
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM LeftJWc5 p;

CREATE CQ ToWactionsRight
INSERT INTO WactionsRight
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM RightJWc10 p;

END APPLICATION RecovTest40;

--
-- Canon Test W20
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for an unpartitioned sliding attribute window
--
-- S -> SWa5u -> CQ -> WS
--


UNDEPLOY APPLICATION NameW20.W20;
DROP APPLICATION NameW20.W20 CASCADE;
CREATE APPLICATION W20 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW20;

CREATE SOURCE CsvSourceW20 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW20;

END FLOW DataAcquisitionW20;



CREATE FLOW DataProcessingW20;

CREATE TYPE DataTypeW20 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW20 OF DataTypeW20;

CREATE CQ CSVStreamW20_to_DataStreamW20
INSERT INTO DataStreamW20
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW20;

CREATE WINDOW SWa5uW20
OVER DataStreamW20
KEEP WITHIN 5 SECOND ON dateTime;

CREATE WACTIONSTORE WactionStoreW20 CONTEXT OF DataTypeW20
EVENT TYPES ( DataTypeW20 KEY(word) )
@PERSIST-TYPE@

CREATE CQ SWa5uW20_to_WactionStoreW20
INSERT INTO WactionStoreW20
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM SWa5uW20;

END FLOW DataProcessingW20;



END APPLICATION W20;

create application TestApp;
create source CSVSource using FileReader (
	directory:'Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ TestCQ
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
	filename:'posdata_JSON',
	rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'
)
format using JSONFormatter (
	members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)

input from TypedCSVStream;
end application TestApp;

deploy application TestApp;
start application TestApp;

stop ORAToBigquery;
undeploy application ORAToBigquery;
drop application ORAToBigquery cascade;
CREATE APPLICATION ORAToBigquery RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE Rac11g USING OracleReader  ( 
  SupportPDB: false,
  SendBeforeImage: true,
  ReaderType: 'LogMiner',
  CommittedTransactions: false,
  FetchSize: 1,
  Password: 'manager',
  DDLTracking: 'true',
  StartTimestamp: 'null',
  OutboundServerProcessName: 'WebActionXStream',
  OnlineCatalog: true,
  ConnectionURL: '192.168.33.10:1521/XE',
  SkipOpenTransactions: false,
  Compression: false,
  QueueSize: 40000,
  RedoLogfiles: 'null',
  Tables: 'SYSTEM.POSTABLE',
  Username: 'ravi',
  FilterTransactionBoundaries: true,
  adapterName: 'OracleReader',
  XstreamTimeOut: 600,
  connectionRetryPolicy: 'timeOut=30, retryInterval=60, maxRetries=3'
 ) 
OUTPUT TO DataStream;
CREATE OR REPLACE TARGET Target1 USING SysOut ( 
  name: "dstream"
 ) 
INPUT FROM DataStream;
CREATE OR REPLACE TARGET Target2 using BigqueryWriter(
  BQServiceAccountConfigurationPath:"/Users/ravipathak/Downloads/big-querytest-1963ae421e90.json",
  projectId:"big-querytest",
  Tables: "SYSTEM.GGAUTHORIZATIONS,testing1.ggauthorisation",
  parallelismCount: 2,
  BatchPolicy: "eventCount:100000,Interval:0")
INPUT FROM DataStream;
END APPLICATION ORAToBigquery;
deploy application ORAToBigquery;
start ORAToBigquery;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;

drop propertyset PGtoPGPlatfm_App_KafkaPropset;
drop stream  PGToPGPlatfm_Stream CASCADE;


CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;
					
CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using PostgreSQLReader(
  ReplicationSlotName: 'slotname',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: '$table1',
  ExcludedTables:'public.chkpoint'
 ) OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using PostgreSQLReader( 
  ReplicationSlotName: 'slotname',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: '@TABLENAME@2',
  ExcludedTables:'public.chkpoint'
 ) OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'WACTION.PGToPGPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using PostgreSQLReader( 
  ReplicationSlotName: 'slotname',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: '$table2',
  ExcludedTables:'public.chkpoint'
 ) OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using PostgreSQLReader(
  ReplicationSlotName: 'slotname',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: '@TABLENAME@4',  
  ExcludedTables:'public.chkpoint'
 ) OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'WACTION.PGToPGPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;

END APPLICATION @APPNAME@2;

--
-- Canon Test W30
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for an unpartitioned jumping count window
--
-- S -> JWc5u -> CQ -> WS
--


UNDEPLOY APPLICATION NameW30.W30;
DROP APPLICATION NameW30.W30 CASCADE;
CREATE APPLICATION W30 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW30;


CREATE SOURCE CsvSourceW30 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW30;


END FLOW DataAcquisitionW30;



CREATE FLOW DataProcessingW30;

CREATE TYPE DataTypeW30 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW30 OF DataTypeW30;

CREATE CQ CSVStreamW30_to_DataStreamW30
INSERT INTO DataStreamW30
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW30;

CREATE JUMPING WINDOW JWc5uW30
OVER DataStreamW30
KEEP 5 ROWS;

CREATE WACTIONSTORE WactionStoreW30 CONTEXT OF DataTypeW30
EVENT TYPES ( DataTypeW30 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWc5uW30_to_WactionStoreW30
INSERT INTO WactionStoreW30
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc5uW30;

END FLOW DataProcessingW30;



END APPLICATION W30;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.WAEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader ()
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING Global.FileWriter ()
FORMAT USING Global.JSONFormatter  (
  members: 'data'
)
INPUT FROM @APPNAME@PersistStream@RANDOM@;

END APPLICATION @APPNAME@;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 10 second interval;
create source CSVSource using FileReader (
	directory:'@DIR@',
	WildCard:'@WILDCARD@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  curr String,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       data[6],
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target BlobT using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:50,interval:5s'
)
format using JSONFormatter (
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

create Target FileTarget using FileWriter(
    rolloverpolicy:'@UPLOAD-SIZE@',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JSONFormatter (
charset:'@charset@',
members:'@mem@'
)
input from CsvStream;

end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

create or replace Target Quiesce_orcl_kwTARGET1 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'Quiesce_orcl_kw_dsv_sync_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(COMMIT_TIMESTAMP)',
Mode:'sync',
KafkaConfig: 'batch.size=1048576;linger.ms=300000;')
FORMAT USING dsvFormatter ()
input from Quiesce_orcl_kwss;

create or replace type @STREAM@details(
ID INT,
name String,
company String);

create or replace stream @STREAM@_TYPED of @STREAM@details PARTITION BY name;

Create or replace CQ @STREAM@detailsCQ
insert into @STREAM@_TYPED
select 
to_int(data[0]),data[1],data[2]
from @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING FileWriter  ( 
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
 ) Format using DSVFormatter()
INPUT FROM @STREAM@_TYPED;

Stop application appname;
undeploy application appname;
drop application appname cascade;

create application appname use exceptionstore;

  


CREATE SOURCE OracleSource USING OracleReader  ( 
Username: 'qatest', 
  Tables: 'QATEST.TEST01', 
  FetchSize: 100, 
  Password_encrypted: 'false', 
  ConnectionURL: 'localhost:1521:xe', 
  DictionaryMode: 'OnlineCatalog', 
  queuesize: 25000, 
  Password: 'qatest'
  ) 
OUTPUT TO SS_oracle;

CREATE TYPE CDCtestnMapType
    (   id INTEGER,
        name STRING,
        cost DOUBLE
    );


CREATE EXTERNAL CACHE c_ext_oracle (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
    DatabaseProviderType: 'Default',
  FetchSize: 1,
  Password: 'qatest',
  Columns: 'id,name,cost',
  Table: 'QATEST.SRCCACHE12',
  trimquote: false,
  Username: 'qatest',
  keytomap: 'id',
  AdapterName:'DatabaseReader',
  connectionRetryPolicy: 'timeOut=5, retryInterval=5, maxRetries=10'
 )
OF CDCtestnMapType;
 

CREATE EXTERNAL CACHE c_oracle  (
 ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  DatabaseProviderType: 'Default',
  FetchSize: 1,
  Password: 'qatest',
  Columns: 'id,name,cost',
  Table: 'QATEST.SRCCACHE12',
  trimquote: false,
  Username: 'qatest',
  keytomap: 'id',
  AdapterName:'DatabaseReader',
  connectionRetryPolicy: 'timeOut=5, retryInterval=5, maxRetries=10' 
 )
 OF  CDCtestnMapType;
 
 CREATE TYPE CDCtestnMapTypenew
    (   id_t INTEGER,
        name_t STRING,
        cost_t DOUBLE,
        id_c INTEGER,
        name_c STRING,
        cost_c DOUBLE
    );
    
create stream JoinedDataStreamOracle of CDCtestnMapTypenew;
create stream JoinedDataStreamOraclenull of CDCtestnMapTypenew;

    
CREATE CQ JoinDataCQOracle1
INSERT INTO JoinedDataStreamOracle
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_DOUBLE(f.data[2]),
        z.id,
        z.name,
        z.cost
FROM SS_oracle f inner join c_ext_oracle z
on TO_INT(f.data[0]) = z.id;

CREATE CQ JoinDataCQOracle2 
INSERT INTO JoinedDataStreamOraclenull
SELECT  TO_INT(s.data[0]),
        TO_STRING(s.data[1]),
        TO_DOUBLE(s.data[2]),
        z.id,
        z.name,
        z.cost
FROM c_oracle f left outer join c_ext_oracle z
on z.id=f.id RIGHT OUTER JOIN SS_oracle s
on  TO_INT(s.data[0])=z.id
WHERE (TO_STRING(META(s, "OperationName")) = "INSERT");



CREATE TARGET Target_DBWriter USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1000,Interval:60',
CommitPolicy:'Eventcount:10,Interval:60',
Tables:'QATEST.test03'
) INPUT FROM JoinedDataStreamOracle;

CREATE TARGET Target_DBWriter2 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1000,Interval:60',
CommitPolicy:'Eventcount:10,Interval:60',
Tables:'QATEST.test04'
) INPUT FROM JoinedDataStreamOraclenull;

end application appname;
deploy application appname;
start appname;

STOP UpdatableCacher.UpdatableCache;
UNDEPLOY APPLICATION UpdatableCacher.UpdatableCache;
DROP APPLICATION UpdatableCacher.UpdatableCache CASCADE;
CREATE APPLICATION UpdatableCacher.UpdatableCache;

CREATE TYPE MerchantHourlyAve(
  merchantId String KEY,
  hourlyAve Integer,
  theDate DateTime,
  dVal Double
);


CREATE source CsvDataSource USING FileReader (
      directory:'@TEST-DATA-PATH@',
      columndelimiter: ',',
      wildcard:'ucData.csv',
      blocksize: 10240,
      positionByEOF:false
)
PARSE USING DSVParser (
      header:No,
      trimquote:false
) OUTPUT TO CsvStream;


CREATE STREAM S1 OF MerchantHourlyAve;

CREATE CQ cq1
	insert into S1
		SELECT data[0],
				TO_INT(data[1]),
				TO_DATE(data[2]),
				TO_DOUBLE(data[3])
		FROM CsvStream;


CREATE EVENTTABLE ET1 using STREAM (
  NAME: 'S1'
) QUERY (keytomap:'dVal', persistPolicy: 'true' ) OF MerchantHourlyAve;


CREATE EVENTTABLE ET2 using STREAM (
  NAME: 'S1'
) QUERY (keytomap:'merchantId' ) OF MerchantHourlyAve;



END APPLICATION UpdatableCache;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

CREATE OR REPLACE SOURCE @SOURCE@ USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '85d7qFnwTW8=',
  Password_encrypted: 'true',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

CREATE OR REPLACE TARGET @TARGET@ USING @TARGET_ADAPTER@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

end flow @APPNAME@_serverflow;

END APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d';

CREATE OR REPLACE SOURCE @SOURCE@1 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
 )
OUTPUT TO @STREAM@1;

CREATE OR REPLACE SOURCE @SOURCE@2 USING PostgreSQLReader  (
    ReplicationSlotName:'test_slot',
    FilterTransactionBoundaries:'true',
    Username:'@SOURCE_USER@',
    Password_encrypted:false,
    ConnectionURL:'@CONNECTION_URL@',
    adapterName:'PostgreSQLReader',
    ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
    Password:'@SOURCE_PASS@',
    Tables:'@SOURCE_TABLE@',
    ExcludedTables:'public.chkpoint',
 )
OUTPUT TO @STREAM@1;

CREATE OR REPLACE SOURCE @SOURCE@3 USING MySQLReader (
    Username: '@READER-UNAME@',
    Password: '@READER-PASSWORD@',
    ConnectionURL: '@CDC-READER-URL@',
    Tables: @WATABLES@,
    sendBeforeImage:'true',
    FilterTransactionBoundaries: 'true',
    ExcludedTables: 'waction.CHKPOINT'
)
OUTPUT TO @STREAM@1;

 -- CREATE OR REPLACE SOURCE @SOURCE@4 USING MariaDbXpandReader
 -- (
 -- Username: '@READER-UNAME@',
 -- Password: '@READER-PASSWORD@',
 -- ConnectionURL: '@CDC-READER-URL@',
 -- Tables: @WATABLES@,
 -- sendBeforeImage:'true',
 -- FilterTransactionBoundaries: 'true',
 -- ExcludedTables: 'test.CHKPOINT'
 -- )
 -- OUTPUT TO @STREAM@1;

CREATE OR REPLACE TARGET @TARGET@1 USING SnowflakeWriter (
  ConnectionURL: '@SNOWFLAKE-URL@',
  username: '@SNOWFLAKE-USERNAME@',
  password: '@SNOWFLAKE-PASSWORD@',
  appendOnly:'false',
  uploadPolicy: 'eventcount:1,interval:5m',
  Tables: '@TARGET-TABLES@'
 )
INPUT FROM @STREAM@1;

CREATE OR REPLACE TARGET @TARGET@2 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@WATABLES@',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true
)
INPUT FROM @STREAM@1;



END APPLICATION @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
wildcard: '',
directory: '',
positionbyeof: false )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: ''
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop PatternMatchingTimer.CSV;
undeploy application PatternMatchingTimer.CSV;
drop application PatternMatchingTimer.CSV cascade;

create application CSV;

create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'ctest.csv',
  columndelimiter:',',
  positionByEOF:false
)
OUTPUT TO CsvStream;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  TO_INT(data[0]) as UserId,
	    TO_INT(data[1]) as temp1,
        TO_DOUBLE(data[2]) as temp2,
	    TO_STRING(data[3]) as temp3
FROM CsvStream;

-- scenario 1.1 check pattern using timer within 10 seconds and wait
CREATE CQ TypeConversionTimerCQ1
INSERT INTO TypedStream1
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN T A (W | B | C)
define T = timer(interval 10 second),
A = UserDataStream(temp1 >= 20), B= UserDataStream(temp2 < 30.40), C= UserDataStream(temp3 = 'Bret'), W = wait(T)
PARTITION BY UserId;

-- scenario 1.2 check pattern using timer within 20 seconds
CREATE CQ TypeConversionTimerCQ2
INSERT INTO TypedStream2
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN T A C
define T = timer(interval 20 second), B= UserDataStream(temp2 < 30.40), C= UserDataStream(temp3 = 'zalak'),
A = UserDataStream(temp1 >= 20)
PARTITION BY UserId;

-- scenario 1.3 check pattern using timer within 5 seconds with between values
CREATE CQ TypeConversionTimerCQ3
INSERT INTO TypedStream3
SELECT UserId as typeduserid,
	   A.temp1 as typedtemp1
from UserDataStream
MATCH_PATTERN T A
define T = timer(interval 5 second),
A = UserDataStream(temp1 between 10 and 40)
PARTITION BY UserId;

-- scenario 1.4 check pattern using timer which match no events
CREATE CQ TypeConversionTimerCQ4
INSERT INTO TypedStream4
SELECT UserId as typeduserid
from UserDataStream
MATCH_PATTERN T W
define T = timer(interval 50 second), W = wait(T)
PARTITION BY UserId;

-- scenario 1.5 check pattern using stop timer
CREATE CQ TypeConversionTimerCQ5
INSERT INTO TypedStream5
SELECT UserId as typeduserid,
       A.temp1 as typedtemp1,
       B.temp2 as typedtemp2
from UserDataStream
MATCH_PATTERN T A C T2 B
define
T = timer(interval 50 second),
A = UserDataStream(temp1 between 10 and 40),
C = stoptimer(T),
T2 = timer(interval 30 second),
B = UserDataStream(temp2 >= 20)
PARTITION BY UserId;

CREATE WACTIONSTORE UserActivityInfoTimer1
CONTEXT OF TypedStream1_Type
EVENT TYPES ( TypedStream1_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTimer2
CONTEXT OF TypedStream2_Type
EVENT TYPES ( TypedStream2_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTimer3
CONTEXT OF TypedStream3_Type
EVENT TYPES ( TypedStream3_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTimer4
CONTEXT OF TypedStream4_Type
EVENT TYPES ( TypedStream4_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTimer5
CONTEXT OF TypedStream5_Type
EVENT TYPES ( TypedStream5_Type )
@PERSIST-TYPE@

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction1
INSERT INTO UserActivityInfoTimer1
SELECT * FROM TypedStream1
LINK SOURCE EVENT;

CREATE CQ UserWaction2
INSERT INTO UserActivityInfoTimer2
SELECT * FROM TypedStream2
LINK SOURCE EVENT;

CREATE CQ UserWaction3
INSERT INTO UserActivityInfoTimer3
SELECT * FROM TypedStream3
LINK SOURCE EVENT;

CREATE CQ UserWaction4
INSERT INTO UserActivityInfoTimer4
SELECT * FROM TypedStream4
LINK SOURCE EVENT;

CREATE CQ UserWaction5
INSERT INTO UserActivityInfoTimer5
SELECT * FROM TypedStream5
LINK SOURCE EVENT;

end application CSV;
deploy application csv;
start csv;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src1 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE SOURCE @APP_NAME@_src2 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE SOURCE @APP_NAME@_src3 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE SOURCE @APP_NAME@_src4 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE SOURCE @APP_NAME@_src5 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;


CREATE OR REPLACE TARGET @APP_NAME@_tgt1 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

STOP DSLAPP;
UNDEPLOY APPLICATION DSLAPP;
DROP APPLICATION DSLAPP CASCADE;

CREATE APPLICATION DSLAPP;

-- CacheWaction WACTIONSTORE is being loaded from DSCache

CREATE OR REPLACE WACTIONSTORE CacheWactionDSL CONTEXT OF DS.T1
EVENT TYPES ( DS.T1 )
@PERSIST-TYPE@

CREATE CQ DSLDerby
INSERT INTO CacheWactionDSL
select * from DS.C1
LINK SOURCE EVENT;

END APPLICATION DSLAPP;
DEPLOY APPLICATION DSLAPP;
START APPLICATION DSLAPP;

CREATE OR REPLACE TARGET @appName@_AzureEventHubWriter USING AzureEventHubWriter (
  SASKey: '',
  EventHubName: '',
  EventHubNamespace: '',
  SASPolicyName: '',
  BatchPolicy: 'Size:1000000,Interval:10s' )
FORMAT USING JSONFormatter  (
 )
INPUT FROM @appName@_MCQOut1;

--
-- Kafka Stream Recovery Test 11
-- Bert Hashemi and Nicholas Keene, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS1
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS2
--

STOP Recov11Tester.KStreamRecovTest11;
UNDEPLOY APPLICATION Recov11Tester.KStreamRecovTest11;
DROP APPLICATION Recov11Tester.KStreamRecovTest11 CASCADE;
DROP USER KStreamRecov11Tester;
DROP NAMESPACE KStreamRecov11Tester CASCADE;
CREATE USER KStreamRecov11Tester IDENTIFIED BY KStreamRecov11Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov11Tester;
CONNECT KStreamRecov11Tester KStreamRecov11Tester;

CREATE APPLICATION KStreamRecovTest11 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions1 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions2 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions1
INSERT INTO Wactions1
SELECT
    *
FROM DataStream5Minutes;

CREATE CQ InsertWactions2
INSERT INTO Wactions2
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION KStreamRecovTest11;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

--
-- Recovery Test 41 with two sources, two jumping count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5W/p -> CQ1 -> WS
--   S2 -> Jc6W/p -> CQ2 -> WS
--

STOP Recov41Tester.RecovTest41;
UNDEPLOY APPLICATION Recov41Tester.RecovTest41;
DROP APPLICATION Recov41Tester.RecovTest41 CASCADE;
CREATE APPLICATION RecovTest41 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION RecovTest41;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using AzureblobWriter(
    accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:7'
)
format using DSVFormatter (
)
input from @STREAM@;

end flow @APPNAME@_serverflow;

end application @APPNAME@;

STOP APPLICATION SystemTimeTester.SystemTimeWindows;
UNDEPLOY APPLICATION SystemTimeTester.SystemTimeWindows;
DROP APPLICATION SystemTimeTester.SystemTimeWindows cascade;


CREATE APPLICATION SystemTimeWindows;

CREATE TYPE RandomData(
bankNumber int KEY,
bankName String
);

CREATE  SOURCE ranDataSource USING StreamReader (
  OutputType: 'SystemTimeTester.RandomData',
  noLimit: 'false',
  isSeeded: 'true',
  maxRows: 0,
  iterations: 30,
  iterationDelay: 1000,
  StringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',
  NumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]R'
 )
OUTPUT TO CSVDataStream;

CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT data[0], data[1]
FROM CSVDataStream;

CREATE @WINDOWTYPE@ WINDOW tierone OVER RandomDataStream keep within 20 second;

CREATE STREAM onetwostream OF RandomData;

CREATE CQ onetwocq
INSERT INTO onetwostream
SELECT bankNumber,bankName
FROM tierone
where  bankNumber >= 300
order by bankName;

CREATE WACTIONSTORE MyDataActivity  CONTEXT OF RandomData
EVENT TYPES ( RandomData  )
@PERSIST-TYPE@

Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
SELECT bankNumber,bankName from @FROMSTREAM@
where  bankNumber >= 300
order by bankName
LINK SOURCE EVENT;

END APPLICATION SystemTimeWindows;
deploy application SystemTimeWindows;
start application SystemTimeWindows;

Stop @APPNAME@_App;
undeploy application @APPNAME@_App;
drop application @APPNAME@_App cascade;

CREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;
CREATE SOURCE @APPNAME@_Source USING Global.Ojet

(
  Username:'@SOURCE_USER@',
  Password:'@SOURCE_PASSWORD@',
  ConnectionURL:'@SOURCE_URL@',
  Tables:'@SOURCE_TABLES@'
)
OUTPUT TO @APPNAME@_Stream;

CREATE TARGET @APPNAME@_SYS USING SysOut (
  name: '@APPNAME@_SYS' )
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_CDB_Target USING DatabaseWriter

(
  Username:'@TARGET_CDB_USER@',
  ConnectionURL:'@TARGET_CDB_URL@',
  Tables:'@TARGET_CDB_TABLES@',
  Password:'@TARGET_CDB_PASSWORD@',
  BatchPolicy:'EventCount:1,Interval:1',
  CommitPolicy:'EventCount:1,Interval:1'
)
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_PDB_Target USING DatabaseWriter

(
  Username:'@TARGET_PDB_USER@',
  ConnectionURL:'@TARGET_PDB_URL@',
  Tables:'@TARGET_PDB_TABLES@',
  Password:'@TARGET_PDB_PASSWORD@',
  BatchPolicy:'EventCount:1,Interval:1',
  CommitPolicy:'EventCount:1,Interval:1'
)
INPUT FROM @APPNAME@_Stream;

End APPLICATION @APPNAME@_App;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE @SOURCE@ USING MSSQLReader
 (
   Username: '@LOGMINER-UNAME@',
   Password: '@LOGMINER-PASSWORD@',
   ConnectionURL: '@LOGMINER-URL@',
   DatabaseName:'qatest',
   Tables: '@SOURCE_TABLE@',
    Compression:false,
    AutoDisableTableCDC:false,FetchTransactionMetadata:true,
    StartPosition:'EOF'
 )
OUTPUT TO @STREAM@;


CREATE OR REPLACE TARGET @TARGET@1 USING Global.DeltaLakeWriter (
  uploadPolicy: 'eventcount:10,interval:10s',
  Mode: 'MERGE',
  hostname: ' ',
  Tables: ' ',
  stageLocation: '/',
  CDDLAction: 'Process',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken: ' ',
  connectionUrl: ' '  )

INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@2 USING Global.DeltaLakeWriter (
  uploadPolicy: 'eventcount:10,interval:10s',
  Mode: 'MERGE',
  optimizedMerge: 'true',
  hostname: ' ',
  Tables: ' ',
  stageLocation: '/',
  CDDLAction: 'Process',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken: ' ',
  connectionUrl: ' '  )

INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@3 USING Global.DeltaLakeWriter (
  uploadPolicy: 'eventcount:10,interval:10s',
  Mode: 'APPENDONLY',
  hostname: ' ',
  Tables: ' ',
  stageLocation: '/',
  CDDLAction: 'Process',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken: ' ',
  connectionUrl: ' '  )

INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCENAME@ USING IncrementalBatchReader  (
  FetchSize: 10,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: '@startPosition@',
  PollingInterval: '20sec'
  )
  OUTPUT TO @STREAM@;

  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;

  CREATE TARGET @targetName1@ USING DatabaseWriter(
    ConnectionURL:'@READER-URL@',
    Username:'@READER-UNAME@',
    Password:'@READER-PASSWORD@',
    BatchPolicy:'Eventcount:1,Interval:1',
    CommitPolicy:'Eventcount:1,Interval:1',
    Checkpointtable:'CHKPOINT',
    Tables:'@WATABLES@,@WATABLES@_target'
  ) INPUT FROM @STREAM@;

  create Target @targetsys1@ using SysOut(name:@targetsys1@) input from @STREAM@;

    CREATE TARGET @targetName2@ USING DatabaseWriter(
      ConnectionURL:'@READER-URL@',
      Username:'@READER-UNAME@',
      Password:'@READER-PASSWORD@',
      BatchPolicy:'Eventcount:1,Interval:1',
      CommitPolicy:'Eventcount:1,Interval:1',
      Checkpointtable:'CHKPOINT',
      Tables:'@WATABLES@,@WATABLES@_target1'
    ) INPUT FROM @STREAM@;

  END APPLICATION @APPNAME@;

  DEPLOY APPLICATION @APPNAME@;
  start application @APPNAME@;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

Create Source dataGenSrc using MySQLReader
(
 Username:'root',
 Password:'w@ct10n',
 ConnectionURL: 'mysql://127.0.0.1:3306/waction',
 Tables:'@tableNames@'
 )
 Output To LCRStream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;

END APPLICATION DataGenSampleApp;

--
-- Crash Recovery Test 6 with Jumping window and partitioned on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> KafkaStream -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION KStreamN2S2CR6Tester.KStreamN2S2CRTest6;
UNDEPLOY APPLICATION KStreamN2S2CR6Tester.KStreamN2S2CRTest6;
DROP APPLICATION KStreamN2S2CR6Tester.KStreamN2S2CRTest6 CASCADE;
DROP USER KStreamN2S2CR6Tester;
DROP NAMESPACE KStreamN2S2CR6Tester CASCADE;
CREATE USER KStreamN2S2CR6Tester IDENTIFIED BY KStreamN2S2CR6Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR6Tester;
CONNECT KStreamN2S2CR6Tester KStreamN2S2CR6Tester;

CREATE APPLICATION KStreamN2S2CRTest6 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest6;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest6 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest6;

CREATE FLOW DataProcessingKStreamN2S2CRTest6;

CREATE TYPE CsvDataTypeKStreamN2S2CRTest6 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvDataTypeKStreamN2S2CRTest6 PARTITION BY merchantId;

CREATE CQ CsvToDataKStreamN2S2CRTest6
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest6 CONTEXT OF CsvDataTypeKStreamN2S2CRTest6
EVENT TYPES ( CsvDataTypeKStreamN2S2CRTest6 )
@PERSIST-TYPE@

CREATE CQ DataToWactionKStreamN2S2CRTest6
INSERT INTO WactionsKStreamN2S2CRTest6
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingKStreamN2S2CRTest6;

END APPLICATION KStreamN2S2CRTest6;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE OR REPLACE APPLICATION @APPNAME@ recovery 5 second interval;

CREATE FLOW @APPNAME@_Agent_flow;

CREATE OR REPLACE SOURCE @SourceName@1 USING Global.MSJet (
  Tables: 'dbo.compsrc',
    username: 'qatest',
    DatabaseName: 'qatest',
    FetchTransactionMetadata: true,
    filterTransactionBoundaries: true,
    compression: true,
    ConnectionURL: '10.211.55.3:1433',
    CommittedTransactions: true,
    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
    SendBeforeImage: true,
    password: 'w3b@ct10n' )
OUTPUT TO @SRCINPUTSTREAM@;

END FLOW @APPNAME@_Agent_flow;

CREATE FLOW @APPNAME@_Agent_flow1;

CREATE OR REPLACE SOURCE @SourceName@2 USING Global.MSJet (
  Tables: 'dbo.compsrc',
    username: 'qatest',
    DatabaseName: 'qatest',
    FetchTransactionMetadata: true,
    filterTransactionBoundaries: true,
    compression: false,
    ConnectionURL: '10.211.55.3:1433',
    CommittedTransactions: true,
    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
    SendBeforeImage: true,
    password: 'w3b@ct10n' )
OUTPUT TO @SRCINPUTSTREAM@;

END FLOW @APPNAME@_Agent_flow1;

CREATE FLOW @APPNAME@_server_flow;

CREATE OR REPLACE TARGET @targetName@ USING Global.DatabaseWriter
(
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  BatchPolicy:'EventCount:10,Interval:60',
  CommitPolicy:'EventCount:10,Interval:60',
  ParallelThreads:'',
  CheckPointTable:'CHKPOINT',
  Password_encrypted:'false',
  Tables:'qatest.MSJEtsrc1,qatest.MSJEtar1;qatest.MSJEtsrc2,qatest.MSJEtar2;',
  CDDLAction:'Process',
  Password:'w3b@ct10n',
  StatementCacheSize:'50',
  ConnectionURL:'jdbc:sqlserver://10.211.55.3:1433;databaseName=qatest',
  DatabaseProviderType:'Default',
  Username:'qatest',
  PreserveSourceTransactionBoundary:'false',
  adapterName:'DatabaseWriter'
)
INPUT FROM @SRCINPUTSTREAM@;

CREATE TARGET @targetsys@ USING Global.SysOut (
  name: '@targetsys@' )
INPUT FROM @SRCINPUTSTREAM@;

END FLOW @APPNAME@_server_flow;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@ with @APPNAME@_Agent_flow in AGENTS, @APPNAME@_Agent_flow1 in AGENTS ,@APPNAME@_server_flow on any in default;
START APPLICATION @APPNAME@;

CREATE APPLICATION KafkaReader;

CREATE OR REPLACE TYPE KafkaSourceStr2_Type  ( seq java.lang.Integer
 );

CREATE OR REPLACE STREAM KafkaSourceStr2 OF KafkaSourceStr2_Type;

CREATE  JUMPING WINDOW GetTargData OVER KafkaSourceStr2 KEEP 1000000 ROWS;

CREATE OR REPLACE SOURCE KafkaSource USING KafkaReader VERSION '0.11.0' (
  KafkaConfigPropertySeparator: ';',
  startOffset: 0,
  adapterName: 'KafkaReader',
  Topic: 'kafkaTopic7',
  AutoMapPartition: true,
  brokerAddress: 'localhost:9092',
  KafkaConfigValueSeparator: '=',
  KafkaConfig: 'max.partition.fetch.bytes=10485760;fetch.min.bytes=1048576;fetch.max.wait.ms=1000;receive.buffer.bytes=2000000;poll.timeout.ms=10000;request.timeout.ms=60001;session.timeout.ms=60000'
 )
 PARSE USING DSVParser  (
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: 0,
  nocolumndelimiter: false,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: false,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 )
OUTPUT TO KafkaSourceStr1 ;

CREATE OR REPLACE CQ GetKafkaDataQuery
INSERT INTO KafkaSourceStr2
SELECT TO_INT(data[1]) as seq
FROM KafkaSourceStr1;

CREATE  TYPE KafkaSourceStr3_Type  ( SUMKafkaSourceStr2seq java.lang.Long
 );

CREATE STREAM KafkaSourceStr3 OF KafkaSourceStr3_Type;

CREATE OR REPLACE CQ GetTheSum
INSERT INTO KafkaSourceStr3
SELECT SUM(GetTargData .seq)
FROM GetTargData;

CREATE OR REPLACE TARGET KafkaFile USING FileWriter  (
  filename: 'TargetResults',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:10000,interval:30',
  adapterName: 'FileWriter',
  directory: '@FEATURE-DIR@/logs',
  rolloverpolicy: 'eventcount:10000,interval:30s'
 )
FORMAT USING DSVFormatter  (   nullvalue: 'NULL',
  standard: 'none',
  handler: 'com.webaction.proc.DSVFormatter',
  formatterName: 'DSVFormatter',
  usequotes: 'false',
  rowdelimiter: '\n',
  quotecharacter: '\"',
  header: 'false',
  columndelimiter: ','
 )
INPUT FROM KafkaSourceStr3;

END APPLICATION KafkaReader;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;;

create Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING ParquetFormatter (
schemaFileName: '@SCHEMAFILE@',
members:'data'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING DSVFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

END APPLICATION @APPNAME@;

--
-- Recovery Test 31 with two sources, two sliding count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5W/p -> CQ1 -> WS
-- S2 -> Sc6W/p -> CQ2 -> WS
--

STOP KStreamRecov31Tester.KStreamRecovTest31;
UNDEPLOY APPLICATION KStreamRecov31Tester.KStreamRecovTest31;
DROP APPLICATION KStreamRecov31Tester.KStreamRecovTest31 CASCADE;

DROP USER KStreamRecov31Tester;
DROP NAMESPACE KStreamRecov31Tester CASCADE;
CREATE USER KStreamRecov31Tester IDENTIFIED BY KStreamRecov31Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov31Tester;
CONNECT KStreamRecov31Tester KStreamRecov31Tester;

CREATE APPLICATION KStreamRecovTest31 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION KStreamRecovTest31;

stop MySQLToHBase;
undeploy application MySQLToHBase;
drop application MySQLToHBase cascade;

CREATE APPLICATION MySQLToHBase RECOVERY 5 SECOND INTERVAL;;

CREATE SOURCE MySQLCDCIn USING MySQLReader (
Username:'root',
Password:'root',
ConnectionURL:'mysql://192.168.123.14:3306',
Database:'qatest',
Tables:'qatest.BUSINESS'
)
OUTPUT TO DataStream;

CREATE TARGET MySQLCDCOut
USING SysOut(name:MySQLCDC)
INPUT FROM DataStream;

CREATE OR REPLACE TARGET Target2 using HBaseWriter(
  HBaseConfigurationPath:"/Users/ravipathak/soft/hbase-1.1.5/conf/hbase-site.xml",
  Tables: "qatest.BUSINESS,PKTEST.data",
  PKUpdateHandlingMode: "DELETEANDINSERT",
  BatchPolicy: "eventCount:1")
INPUT FROM DataStream;

END APPLICATION MySQLToHBase;

deploy application MySQLToHBase;
start MySQLToHBase;

STOP application consoleconsoletest.noApp;
undeploy application consoleconsoletest.noApp;
drop application consoleconsoletest.noApp cascade;

DROP USER consoletest;
DROP NAMESPACE consoletest CASCADE;
CREATE USER consoletest IDENTIFIED BY consoletest;

--Creating two roles
Create role consoletest.R1;
Create role consoletest.R2;

--Granting consoletest.dev to new role R1 and granting single role of Drop to R2
Grant consoletest.dev to role consoletest.R1;
Grant deploy on application consoletest.* to role consoletest.dev;
Grant undeploy on application consoletest.* to role consoletest.dev;
Revoke update on Target consoletest.noApp from role consoletest.dev;
Revoke select on WACTIONSTORE consoletest.* from role consoletest.dev;
Revoke consoletest.admin from user consoletest;
Grant drop on application consoletest.* to role consoletest.R2;

--Granting new roles to the user
Grant consoletest.R1 to user consoletest;
Grant consoletest.R2 to user consoletest;

--Creating application
Create application consoletest.noApp;

CREATE TYPE consoletest.Atm(
productID String KEY,
stateID String,
productWeight int,
quantity double,
size long,
currentDate DateTime);

CREATE source consoletest.implicitSOurce USING FileReader (
directory:'@TEST-DATA-PATH@',
columndelimiter: ',',
wildcard:'ISdata.csv',
blocksize: 10240,
positionByEOF:false
)
PARSE USING DSVParser (
header:False,
trimquote:false
) OUTPUT TO consoletest.CsvStream;

CREATE TYPE consoletest.wsType(
quantity double KEY,
currentDate DateTime
);

CREATE STREAM consoletest.newStream OF consoletest.Atm;

CREATE CQ consoletest.newCQ
INSERT INTO consoletest.newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]),TO_DATE(data[5]) FROM
consoletest.CsvStream;

CREATE WINDOW consoletest.win1
OVER consoletest.newStream
keep within 3 minute;

CREATE STREAM consoletest.newStream2 of consoletest.wsType;

CREATE WACTIONSTORE consoletest.WS1 CONTEXT OF consoletest.wsType
EVENT TYPES(consoletest.wsType );

Create cq consoletest.newCQ2
insert into consoletest.ws1 (quantity,currentDate)
select quantity, currentDate from consoletest.newStream;

Create Target consoletest.Trace Using Sysout (name: 'Trace') Input From consoletest.newStream;

End Application consoletest.noApp;


Connect consoletest consoletest;


Alter application noApp;

create or replace type consoletest.wsType(quantity double KEY, size long, currentDate DateTime);

Create or replace Target consoletest.Trace1 Using Sysout (name:'Trace1') Input From consoletest.newStream;

end application consoletest.noApp;
alter application consoletest.noApp recompile;

Deploy application consoletest.noApp;
Start application consoletest.noApp;

-- The following step should fail as expected 
Select count (*) from WS1;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 10 second interval;
CREATE SOURCE OracleSource USING OracleReader
(
    Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
 Tables:'@TABLES@',
    FetchSize: 1
)
OUTPUT TO CsvStream;

Create Type CSVType (
  tablename String,
  data java.util.HashMap  
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT TO_LOWER(META(s, "TableName").toString()) as tablename,
       DATA(s) as data
FROM CsvStream s;

create Target BlobT using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:5,interval:5s'
)
format using AvroFormatter (
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

--
-- Recovery Test 24 with two sources, two sliding time-count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5a9W  -> CQ1 -> WS
-- S2 -> Sc6a11W -> CQ2 -> WS
--

STOP Recov24Tester.RecovTest24;
UNDEPLOY APPLICATION Recov24Tester.RecovTest24;
DROP APPLICATION Recov24Tester.RecovTest24 CASCADE;
CREATE APPLICATION RecovTest24 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION RecovTest24;

stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@;

CREATE SOURCE @SourceName@ USING PostgreSQLReader  ( 
ReaderType: 'LogMiner', 
  Password_encrypted: 'false', 
  SupportPDB: false, 
  ReplicationSlotName: 'test_slot',
  QuiesceMarkerTable: 'QUIESCEMARKER', 
  QueueSize: 2048, 
  CommittedTransactions: true, 
  Username: '@UserName@', 
  TransactionBufferType: 'Memory', 
  TransactionBufferDiskLocation: '.striim/LargeBuffer', 
  OutboundServerProcessName: 'WebActionXStream', 
  Password: '@Password@', 
  DDLCaptureMode: 'All', 
  Compression: false, 
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', 
  FetchSize: 1, 
  Tables: '@SourceTables@', 
  DictionaryMode: 'OnlineCatalog', 
  XstreamTimeOut: 600, 
  TransactionBufferSpilloverSize: '1MB', 
  FilterTransactionBoundaries: true, 
  ConnectionURL: '@ConnectionURL@', 
  SendBeforeImage: true ) 
OUTPUT TO @AppStream@  ;

CREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream SELECT data[2], to_string(to_date(data[2]), "dd-MMM-yy hh.mm.ss") FROM @AppStream@ o ;

CREATE  TARGET @targetsys@ USING Global.SysOut  ( 
name: 'ora1_sys' ) 
INPUT FROM admin.ZDT_cq_stream;

create Target @TargetFile@ using FileWriter(
  filename:'toStringOut.log',
  directory:'@FilePath@',
  rolloverpolicy:'eventcount:1000'
)
format using DSVFormatter (

)
input from admin.ZDT_cq_stream;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

--
-- Recovery Test 28 with two sources, two jumping time-count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5a9W  -> CQ1 -> WS
--   S2 -> Jc6a11W -> CQ2 -> WS
--

STOP Recov28Tester.RecovTest28;
UNDEPLOY APPLICATION Recov28Tester.RecovTest28;
DROP APPLICATION Recov28Tester.RecovTest28 CASCADE;
CREATE APPLICATION RecovTest28 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION RecovTest28;

--
-- Recovery Test 38 with two sources, two jumping time-count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5a3W/p -> CQ1 -> WS
--   S2 -> Jc6a4W/p -> CQ2 -> WS
--

STOP KStreamRecov38Tester.KStreamRecovTest38;
UNDEPLOY APPLICATION KStreamRecov38Tester.KStreamRecovTest38;
DROP APPLICATION KStreamRecov38Tester.KStreamRecovTest38 CASCADE;

DROP USER KStreamRecov38Tester;
DROP NAMESPACE KStreamRecov38Tester CASCADE;
CREATE USER KStreamRecov38Tester IDENTIFIED BY KStreamRecov38Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov38Tester;
CONNECT KStreamRecov38Tester KStreamRecov38Tester;

CREATE APPLICATION KStreamRecovTest38 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE JUMPING WINDOW DataStream5Rows3Seconds
OVER DataStream1 KEEP 5 ROWS WITHIN 3 SECOND
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Rows4Seconds
OVER DataStream2 KEEP 6 ROWS WITHIN 4 SECOND
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataStream5Rows3Seconds
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Rows3Seconds p
GROUP BY p.merchantId;

CREATE CQ DataStream6Rows4Seconds
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Rows4Seconds p
GROUP BY p.merchantId;

END APPLICATION KStreamRecovTest38;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
directory:'@FEATURE-DIR@/logs',
filename:'PosDataFS',
rolloverpolicy:'filesize:1M,sequence:00'
)
format using DSVFormatter (

)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizePosData_actual.log') input from TypedCSVStream;

end application DSV;

STOP APPLICATION DBWriterTester.DBWriterApp;
UNDEPLOY APPLICATION DBWriterTester.DBWriterApp;
DROP APPLICATION DBWriterTester.DBWriterApp CASCADE;

DROP USER DBWriterTester;
DROP NAMESPACE DBWriterTester CASCADE;
CREATE USER DBWriterTester IDENTIFIED BY DBWriterTester;
GRANT create,drop ON deploymentgroup Global.* To user DBWriterTester;
CONNECT DBWriterTester DBWriterTester;

CREATE APPLICATION DBWriterApp;
CREATE OR REPLACE SOURCE Source_DBReader USING OracleReader  (
-- StartTimestamp: '@CDC-STARTUPTIME@',
Username: 'miner',
Password: 'miner',
ConnectionURL: '//10.1.186.110:1521/orcl',
TABLES: 'qatest.alltype1;qatest.dbr_marker;',
FetchSize: '1',
committedtransactions: true,
CatalogMode: 'Offline',
BatchPolicy: 'eventCount:10'
)
OUTPUT TO DBReaderStrm;
CREATE OR REPLACE TARGET Target_DBWriter USING DatabaseWriter  (
Tables: 'QATEST.ALLTYPE1,QATEST.ALLTYPE1;QATEST.DBR_MARKER,QATEST.DBR_MARKER;',
Username: 'qatest',
PasSword: 'qatest',
ConnecTionURL: 'jdbc:oracle:thin:@//10.1.110.142:1521/orcl',
BatchPolicy: 'eventCount:10'
)
INPUT FROM DBReaderStrm;
END APPLICATION DBWriterApp;
deploy application DBWriterApp;
start application DBWriterApp;

STOP APPLICATION @WRITERAPPNAME@;
UNDEPLOY APPLICATION @WRITERAPPNAME@;
DROP APPLICATION @WRITERAPPNAME@ CASCADE;
create or replace PROPERTYVARIABLE SRC_PASSWORD='@PROP_VAR@';
CREATE APPLICATION @WRITERAPPNAME@ RECOVERY 10 SECOND INTERVAL;
-- USE EXCEPTIONSTORE;

CREATE SOURCE @SOURCE@ USING OracleReader
(
FetchSize:1,
Username:'@SOURCE_USER@',
Password:'85d7qFnwTW8=',
--Password:'$SRC_PASSWORD',
ConnectionURL:'@CONNECTION_URL@',
Tables:'@SOURCE_TABLE@',
password_encrypted: 'true'
)
OUTPUT TO @STREAM1@;

CREATE OR REPLACE TYPE @TYPE@( 
datae java.util.HashMap , 
TABLE_NAME java.lang.String , 
OPS_NAME java.lang.String , 
DB_TIMESTAMP java.lang.String  ,
COMMITSCN java.lang.String ,
SCN java.lang.String ,
REC_INS_TIME java.lang.String );

CREATE CQ @CQ1@
INSERT INTO @STREAM2@
SELECT  
CASE WHEN (META(c,"OperationName").toString() == "DELETE")
THEN putUserData(c, 'isDelete', 'true') 
ELSE
putUserData(c,'isDelete', 'false')
END
FROM @STREAM1@ c;

CREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;

CREATE OR REPLACE CQ @CQ2@ 
INSERT INTO @STREAM3@
SELECT 
data(e),
META(e,"TableName").toString() as TABLE_NAME,
META(e, "OperationName").toString() as OPS_NAME,
META(e, "TimeStamp").toString() as DB_TIMESTAMP,
META(e,"COMMITSCN").toString() as COMMITSCN ,
META(e,"SCN").toString() as  SCN ,
DNOW().toString() as REC_INS_TIME
FROM @STREAM1@ e;

create Target @TARGET1@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC1',
ParallelThreads:'',
PartitionKey:'@metadata(TableName)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from @STREAM1@;


create Target @TARGET2@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC2',
ParallelThreads:'2',
PartitionKey:'TABLE_NAME',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from @STREAM3@;

create Target @TARGET3@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC3',
ParallelThreads:'',
PartitionKey:'@userdata(isDelete)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest.avsc')
input from @STREAM2@;

end application @WRITERAPPNAME@;
deploy application @WRITERAPPNAME@;
start @WRITERAPPNAME@;
stop application @READERAPPNAME@;
undeploy application @READERAPPNAME@;
drop application @READERAPPNAME@ cascade;
CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC1',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;


CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
filename:'@READERAPPNAME@_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream1;

CREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC2',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;


CREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC3',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest.avsc'
)
OUTPUT TO KafkaReaderStream3;


end application @READERAPPNAME@;

--
-- Crash Recovery Test 4 on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS
-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS
--

STOP APPLICATION N4S4CR4Tester.N4S4CRTest4;
UNDEPLOY APPLICATION N4S4CR4Tester.N4S4CRTest4;
DROP APPLICATION N4S4CR4Tester.N4S4CRTest4 CASCADE;
CREATE APPLICATION N4S4CRTest4 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest4;

CREATE SOURCE CsvSourceN4S4CRTest4 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest4;

CREATE FLOW DataProcessingN4S4CRTest4;

CREATE TYPE CsvDataN4S4CRTest4 (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionDataN4S4CRTest4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream OF CsvDataN4S4CRTest4;

CREATE CQ CsvToDataN4S4CRTest4
INSERT INTO DataStream
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN4S4CRTest4 CONTEXT OF WactionDataN4S4CRTest4
EVENT TYPES ( CsvDataN4S4CRTest4 )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO WactionsN4S4CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO WactionsN4S4CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END FLOW DataProcessingN4S4CRTest4;

END APPLICATION N4S4CRTest4;

use RetailTester;
DROP FLOW RetailSourceFlow cascade;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;;

Create Source MSSqlsource Using MSSqlReader
(
 Username:'@SQL-USERNAME',
 Password:'@SQL-PASSWORD',
 DatabaseName:'@DATABASE-NAME@',
 ConnectionURL: '@SQLSERVER-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'1'
) 
Output To str;

create target MssqlAzureTarget using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '@GRPPOLICY@',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'MultipleEvents',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE TARGET MongoTarget USING MongoDBWriter (
  Username: '',
  AuthDB: 'admin',
  ConnectionURL: '',
  batchpolicy: 'EventCount:10, Interval:30',
  Password: '',
  collections: '' )
INPUT FROM CCBStream;

create Target BlobTarget using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,Interval:30s'
)
format using JSONFormatter (
)
INPUT FROM CCBStream;


CREATE OR REPLACE TARGET GCSTarget USING GCSWriter (
    bucketname:'',
    objectname:'',
    projectId:'',
    uploadPolicy:'EventCount:10,Interval:30s'
)
format using JSONFormatter ()
INPUT FROM CCBStream;

create Target KafkaTarget using KafkaWriter VERSION @KAFKAVERSION@ (
brokerAddress:'',
Topic:'',
Mode: 'Sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;linger.ms=30000'
)
FORMAT USING JSONFormatter (
members:'data')
input from CCBStream;

CREATE TARGET S3Target USING Global.S3Writer (
  bucketname: '',
  objectname: '',
  uploadpolicy: 'EventCount:10,Interval:30s' )
FORMAT USING Global.JSONFormatter  ()
INPUT FROM CCBStream;

create source KafkaSource using KafkaReader VERSION @KAFKAVERSION@(
brokerAddress:'',
	Topic:''
)
parse using JSONParser ()
output to KafkaStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:10,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING JSONFormatter  ()
INPUT FROM KafkaStream;


end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

Stop application PosAppToFW;
UNDEPLOY APPLICATION PosAppToFW;
DROP APPLICATION PosAppToFW CASCADE;

CREATE APPLICATION PosAppToFW RECOVERY 1 MINUTE INTERVAL AUTORESUME MAXRETRIES 2 RETRYINTERVAL 60;

CREATE source CsvPosDataSource USING FileReader (
  WildCard: '',
  directory: '',
positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE OR REPLACE CQ CsvPosAppCq 
INSERT INTO FromCsvPosAppCq 
SELECT TO_STRING(data[1]) as merchantId,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;


CREATE TARGET FileWriterTarget USING Global.FileWriter ( 
 
  flushpolicy: 'EventCount:1000,Interval:30s', 
  directory: '',
  filename: '' ) 
FORMAT USING Global.DSVFormatter  ( 
  quotecharacter: '\"', 
  columndelimiter: ',', 
  usequotes: 'false', 
  rowdelimiter: '\n', 
  standard: 'none', 
  header: 'false' ) 
INPUT FROM FromCsvPosAppCq;

end application PosAppToFW;
deploy application PosAppToFW;
start application PosAppToFW;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE Source @SourceName@ USING Global.MSSqlReader (
  PollingInterval: 5,
  FetchTransactionMetadata: false,
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  TransactionSupport: true,
  StartPosition: 'NOW',
  Tables: 'dbo.sourceTable',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n',
  ConnectionPoolSize: 10,
  Compression: true,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'qatest',
  Username: 'qatest',
  FetchSize: 0,
  IntegratedSecurity: false,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  AutoDisableTableCDC: false )
OUTPUT TO @SRCINPUTSTREAM@;

CREATE TARGET @targetsys@ USING Global.SysOut (
  name: 'sysout' )
INPUT FROM @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
  BatchPolicy: 'EventCount:10000,Interval:30',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:10000,Interval:100',
  StatementCacheSize: '50',
  Password: 'w3b@ct10n',
  Username: 'qatest',
  IgnorableExceptionCode: '547,DUPLICATE_ROW_EXISTS,NO_OP_UPDATE,NO_OP_DELETE,NO_OP_PKUPDATE',
  DatabaseProviderType: 'SQLServer',
  PreserveSourceTransactionBoundary: 'false',
  Tables: 'dbo.sourceTable,dbo.targetTable',
  VendorConfiguration: 'enableIdentityInsert=true',
  adapterName: 'DatabaseWriter' )
INPUT FROM @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE OR REPLACE TYPE @appname@CQOUT1_Type (
 companyName java.lang.String,
 merchantId java.lang.String,
 dateTime org.joda.time.DateTime,
 hourValue java.lang.String,
 amount java.lang.String,
 zip java.lang.String,
 FileName java.lang.String);

CREATE SOURCE @parquetsrc@ USING FILEReader (
    wildcard: '',
    directory: '',
    positionbyeof: false
 )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE OR REPLACE STREAM @appname@CQOUT1 OF @appname@CQOUT1_Type;
CREATE OR REPLACE CQ @appname@CQ_PQEvent
INSERT INTO @appname@CQOUT1
    Select
    data.get("companyName").toString(),
    data.get("merchantId").toString(),
    TO_DATE(data.get("dateTime").toString()),
    data.get("hourValue").toString(),
    data.get("amount").toString(),
    data.get("zip").toString(),
    metadata.get("FileName").toString()
    FROM @appname@Stream p;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using JSONFormatter ()
input from @appname@CQOUT1;

CREATE OR REPLACE TARGET @dbtarget@ USING DatabaseWriter (
  Tables: '',
  ConnectionURL:'',
  Username:'',
  Password:'',
  CommitPolicy: 'EventCount:10,Interval:0',
  BatchPolicy:'EventCount:10,Interval:0'
)
INPUT FROM @appname@CQOUT1;

create Target @jsontarget@ using FileWriter(
filename: '',
directory: '',
flushpolicy: 'EventCount:10,Interval:30s',
rolloverpolicy: 'EventCount:10,Interval:30s' )
format using JSONFormatter ()
INPUT FROM @appname@CQOUT1;

create Target @xmltarget@ using FileWriter(
filename: '',
directory: '',
flushpolicy: 'EventCount:10,Interval:30s',
rolloverpolicy: 'EventCount:10,Interval:30s' )
format using XMLFormatter (
    rootelement:'',
    elementtuple:'',
    charset:'UTF-8'
)
INPUT FROM @appname@CQOUT1;

create Target @dsvtarget@ using FileWriter(
filename: '',
directory: '',
flushpolicy: 'EventCount:10,Interval:30s',
rolloverpolicy: 'EventCount:10,Interval:30s' )
format using DSVFormatter ()
INPUT FROM @appname@CQOUT1;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE APPLICATION TestAPP
CREATE TYPE TestType(
    id java.lang.Long KEY,
    name java.lang.String
);

CREATE STREAM TestStream OF TestType;




CREATE CQ TestCQ
INSERT INTO TestStream
SELECT
    h.value,
    TO_STRING(DNOW())
FROM heartbeat(interval 1 second) h;

END APPLICATION TestAPP;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@;
CREATE  SOURCE @SourceName@ USING MSSqlReader  ( 
  Username: '@UserName@',
  Password: '@Password@',
  DatabaseName: 'qatest',
  ConnectionURL: '@SourceConnectionURL@',
  Tables: 'qatest.@SourceTable@',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @SRCINPUTSTREAM@;
create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;
CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:1',
  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
 ) 
INPUT FROM @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

--
-- Recovery Test 1
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov1Tester.RecovTest1;
UNDEPLOY APPLICATION Recov1Tester.RecovTest1;
DROP APPLICATION Recov1Tester.RecovTest1 CASCADE;
CREATE APPLICATION RecovTest1 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END APPLICATION RecovTest1;

CREATE TYPE PosData(
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.JsonNodeEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (
  ProviderName: '',
  Provider: '',
  Ctx: '',
  QueueName: '',
  Topic:'',
  UserName: '',
  Password: '',
  EnableTransaction: '',
  transactionpolicy: ''
 )
PARSE USING JSONParser ()
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING JSONFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream@RANDOM@;

END APPLICATION @APPNAME@;

stop CSVToHBase;
undeploy application CSVToHBase;
drop application CSVToHBase cascade;

CREATE APPLICATION CSVToHBase;

CREATE OR REPLACE SOURCE CSVPoller USING FileReader ( 
	directory:'/Users/ravipathak/webactionrepo/Product',
	WildCard:'smallpos.csv',
	positionByEOF:false
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

CREATE OR REPLACE TYPE CSVStream_Type  ( BUSINESS_NAME java.lang.String KEY, 
MERCHANT_ID java.lang.String, 
PRIMARY_ACCOUNT_NUMBER java.lang.String  
 ) ;

CREATE OR REPLACE STREAM CSVTypeStream OF CSVStream_Type;

CREATE OR REPLACE CQ CQ1 
INSERT INTO CSVTypeStream
SELECT data[0],data[1],data[2]
FROM CsvStream;

CREATE OR REPLACE TARGET Target1 USING SysOut ( 
  name: "dstream"
 ) 
INPUT FROM CsvStream;

CREATE OR REPLACE TARGET Target2 using HBaseWriter(
 HBaseConfigurationPath:"/Users/ravipathak/soft/hbase-1.1.5/conf/hbase-site.xml",
  Tables: "maprtest.maprdata",
  --FamilyNames: "maprdata",
  BatchPolicy: "eventCount:1")
INPUT FROM CSVTypeStream;

END APPLICATION CSVToHBase;

deploy application CSVToHBase;
start CSVToHBase;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

--
-- Recovery Test 3
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> JW -> CQ(aggregate) -> WS
--

STOP KStreamRecov3Tester.KStreamRecovTest3;
UNDEPLOY APPLICATION KStreamRecov3Tester.KStreamRecovTest3;
DROP APPLICATION KStreamRecov3Tester.KStreamRecovTest3 CASCADE;

DROP USER KStreamRecov3Tester;
DROP NAMESPACE KStreamRecov3Tester CASCADE;
CREATE USER KStreamRecov3Tester IDENTIFIED BY KStreamRecov3Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov3Tester;
CONNECT KStreamRecov3Tester KStreamRecov3Tester;

CREATE APPLICATION KStreamRecovTest3 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount int,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_INT(TO_DOUBLE(data[7])),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

END APPLICATION KStreamRecovTest3;

--
-- Recovery Test 41 with two sources, two jumping count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5W/p -> CQ1 -> WS
--   S2 -> Jc6W/p -> CQ2 -> WS
--

STOP KStreamRecov41Tester.KStreamRecovTest41;
UNDEPLOY APPLICATION KStreamRecov41Tester.KStreamRecovTest41;
DROP APPLICATION KStreamRecov41Tester.KStreamRecovTest41 CASCADE;
DROP USER KStreamRecov41Tester;
DROP NAMESPACE KStreamRecov41Tester CASCADE;
CREATE USER KStreamRecov41Tester IDENTIFIED BY KStreamRecov41Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov41Tester;
CONNECT KStreamRecov41Tester KStreamRecov41Tester;

CREATE APPLICATION KStreamRecovTest41 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION KStreamRecovTest41;

stop application DS.DSAPP;
undeploy application DS.DSAPP;

stop application DS.MyPosApp;
undeploy application DS.MyPosApp;

stop application DS.DsPosApp;
undeploy application DS.DsPosApp;

stop application DSL.DSLAPP;
undeploy application DSL.DSLAPP;

unload cache DS.C1;
unload wactionstore DS.MerchantActivity;

drop namespace DS cascade;
drop user DS cascade;

drop namespace DSL cascade;
drop user DSL cascade;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  FetchSize:1,
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@;

STOP NamedQTester.NamedQueryApp;
UNDEPLOY APPLICATION NamedQTester.NamedQueryApp;
DROP APPLICATION NamedQTester.NamedQueryApp cascade;

CREATE APPLICATION NamedQueryApp;

CREATE TYPE cacheType( EventID String, 
			Word String, 
			datetime Datetime);

CREATE CACHE adhcache using CSVReader (
  directory: '@TEST-DATA-PATH@/',
  wildcard: 'Canon1000.csv',
  header: Yes,
  columndelimiter: ',',
  trimquote:false
) QUERY (keytomap:'EventID') OF cacheType;



END APPLICATION NamedQueryApp;
DEPLOY APPLICATION NamedQueryApp;
START APPLICATION NamedQueryApp;

DROP NAMEDQUERY NamedQTester.nqone;
DROP NAMEDQUERY NamedQTester.nqtwo;
DROP NAMEDQUERY NamedQTester.nqthree;
CREATE NAMEDQUERY nqone select * from adhcache;
CREATE NAMEDQUERY nqtwo select * from adhcache;
CREATE NAMEDQUERY nqthree select * from adhcache;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'SingleEvent',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
  filename: '%@metadata(FileName)%',
  directory: '@DIR@',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter  ()
INPUT FROM CCBStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

CREATE or replace TARGET @TARGET_NAME@ USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:600',
StandardSQL:true	
) INPUT FROM @STREAM@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ RECOVERY 30 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;

CREATE FLOW @AppName@_SourceFlow;

CREATE SOURCE @srcName@ USING ZendeskReader ( 
  MaxConnections: 20, 
  Password_encrypted: 'false', 
  ThreadPoolCount: '10', 
  ConnectionTimeOut: 60, 
  ConnectionPoolSize: '20', 
  MigrateSchema: 'true', 
  Mode: 'Automated', 
  authflow: 'false', 
  Username: '@srcusername@', 
  Password: '@srcpassword@', 
  ConnectionRetries: '3', 
  ZendeskObjects: 'sessions;settings;targets;tickets', 
  AccessToken: '', 
  ConnectionUrl: '@connectionUrl@' ) 
OUTPUT TO @outstreamname@;

END FLOW @AppName@_SourceFlow;

CREATE TARGET @tgtName@ USING Global.BigQueryWriter ( 
  Tables: '%,zendeskallresize.%', 
  projectId: '@projectId@', 
  batchPolicy: 'eventcount:10000,interval:60', 
  ServiceAccountKey: '@keyFileName@', 
  streamingUpload: 'true', 
  Mode: 'MERGE', 
  CDDLOptions: '{\"CreateTable\":{\"action\":\"IgnoreIfExists\",\"options\":[{\"CreateSchema\":{\"action\":\"IgnoreIfExists\"}}]}}' ) 
INPUT FROM @outstreamname@;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

CREATE SOURCE @APPNAME@_Source USING MSSqlReader
(
  Compression: false,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.test01',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
)
OUTPUT TO @APPNAME@_stream;

CREATE TARGET @APPNAME@_Target USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	BatchPolicy: 'EventCount: 1; Interval: 1s',
	instanceId: 'qatest'
) INPUT FROM @APPNAME@_stream;

create Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_stream;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

CREATE APPLICATION @WRITERAPPNAME@ RECOVERY 1 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

CREATE SOURCE @SOURCE@ USING Ojet
(
FetchSize:1,
Username:'@SOURCE_USER@',
Password:'@pass@',
--Password:'$SRC_PASSWORD',
ConnectionURL:'@CONNECTION_URL@',
Tables:'@SOURCE_TABLE@',
)
OUTPUT TO @STREAM1@;


end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

CREATE OR REPLACE TYPE @TYPE@( 
datae java.util.HashMap , 
TABLE_NAME java.lang.String , 
OPS_NAME java.lang.String , 
DB_TIMESTAMP java.lang.String  ,
COMMITSCN java.lang.String ,
SCN java.lang.String ,
REC_INS_TIME java.lang.String );

CREATE CQ @CQ1@
INSERT INTO @STREAM2@
SELECT  
CASE WHEN (META(c,"OperationName").toString() == "DELETE")
THEN putUserData(c, 'isDelete', 'true') 
ELSE
putUserData(c,'isDelete', 'false')
END
FROM @STREAM1@ c;

CREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;

CREATE OR REPLACE CQ @CQ2@ 
INSERT INTO @STREAM3@
SELECT 
data(e),
META(e,"TableName").toString() as TABLE_NAME,
META(e, "OperationName").toString() as OPS_NAME,
META(e, "TimeStamp").toString() as DB_TIMESTAMP,
META(e,"COMMITSCN").toString() as COMMITSCN ,
META(e,"SCN").toString() as  SCN ,
DNOW().toString() as REC_INS_TIME
FROM @STREAM1@ e;

create Target @TARGET1@ using KafkaWriter VERSION @kafakaversion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC1',
ParallelThreads:'',
PartitionKey:'@metadata(TableName)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from @STREAM1@;


create Target @TARGET2@ using KafkaWriter VERSION @kafakaversion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC2',
ParallelThreads:'2',
PartitionKey:'TABLE_NAME',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from @STREAM3@;

create Target @TARGET3@ using KafkaWriter VERSION @kafakaversion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC3',
ParallelThreads:'',
PartitionKey:'@userdata(isDelete)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest.avsc')
input from @STREAM2@;

end application @WRITERAPPNAME@;

CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafakaversion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC1',
        startOffset:0       
)
PARSE USING DSVParser ()
OUTPUT TO KafkaReaderStream1;

CREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafakaversion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC2',
        startOffset:0          
)
PARSE USING JSONParser ()
OUTPUT TO KafkaReaderStream2;

CREATE TARGET kafkaDumpJSON USING FileWriter(
filename:'@READERAPPNAME@_RT_JSON')
FORMAT USING JSONFormatter()
INPUT FROM KafkaReaderStream2;

CREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafakaversion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC3',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest.avsc'
)
OUTPUT TO KafkaReaderStream3;

CREATE TARGET kafkaDumpAVRO USING FileWriter(
filename:'@READERAPPNAME@_RT_AVRO')
FORMAT USING AvroFormatter(
    schemaFileName:'@avro_schema_file@'
)
INPUT FROM KafkaReaderStream3;

end flow @APPNAME@_serverflow;
end application @READERAPPNAME@;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:'@eof@',
	charset:'@charset@'
)
parse using DSVParser (
	header:'@header@'
)
OUTPUT TO CsvStream;

create Target FileTarget using FileWriter(
    rolloverpolicy:'eventcount:100',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using DSVFormatter (
)
input from CsvStream;

end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

CONNECT ADMIN abc;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING XMLParser (
  rootnode: ''
)
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

CREATE CQ @CQ_NAME@
INSERT INTO @EMB_STREAM@
@SELECT_QUERY@
FROM @STREAM@ e;

CREATE TARGET @CQ_NAME@_sysout USING Global.SysOut (
  name: '@CQ_NAME@_sysout' )
INPUT FROM @EMB_STREAM@;

Stop Oracle_Agent;
Undeploy application Oracle_Agent;
drop application Oracle_Agent cascade;

CREATE APPLICATION Oracle_Agent;

CREATE FLOW test_SourceFlow;

CREATE  SOURCE Oracle_Source USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.124.25:1521/orcl',
  Tables: 'QATEST.%',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO Oracle_ChangeDataStream ;

END FLOW test_SourceFlow;

CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Oracle_ChangeDataStream;

END APPLICATION Oracle_Agent;
DEPLOY APPLICATION Oracle_Agent ON ANY IN default WITH test_SourceFlow ON ANY IN AGENTS;
start Oracle_Agent;

--
-- Crash Recovery Test 5 with Jumping window and partitioned on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION N4S4CR5Tester.N4S4CRTest5;
UNDEPLOY APPLICATION N4S4CR5Tester.N4S4CRTest5;
DROP APPLICATION N4S4CR5Tester.N4S4CRTest5 CASCADE;
CREATE APPLICATION N4S4CRTest5 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest5;

CREATE SOURCE CsvSourceN4S4CRTest5 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest5;

CREATE FLOW DataProcessingN4S4CRTest5;

CREATE TYPE CsvDataN4S4CRTest5 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvDataN4S4CRTest5 PARTITION BY merchantId;

CREATE CQ CsvToDataN4S4CRTest5
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsN4S4CRTest5 CONTEXT OF CsvDataN4S4CRTest5
EVENT TYPES ( CsvDataN4S4CRTest5 )
@PERSIST-TYPE@

CREATE CQ DataToWactionN4S4CRTest5
INSERT INTO WactionsN4S4CRTest5
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN4S4CRTest5;

END APPLICATION N4S4CRTest5;

--
-- Recovery Test 13 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same compound key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> CW(p#p) -> CQ -> WS
--

STOP Recov13Tester.RecovTest13;
UNDEPLOY APPLICATION Recov13Tester.RecovTest13;
DROP APPLICATION Recov13Tester.RecovTest13 CASCADE;
CREATE APPLICATION RecovTest13 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTest10Data.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  partKey String KEY,
  serialNumber int,
  partKey2 String KEY
);

CREATE TYPE WactionData (
  partKey String KEY,
  serialNumber int
);

CREATE STREAM DataStream OF CsvData PARTITION BY partKey, partKey2;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    TO_INT(data[1]),
    data[0]
FROM CsvStream;

CREATE JUMPING WINDOW DataStreamTwoItems
OVER DataStream KEEP 2 ROWS
PARTITION BY partKey, partKey2;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    first(partKey),
    to_int(first(serialNumber))
FROM DataStreamTwoItems
GROUP BY partKey, partKey2;

END APPLICATION RecovTest13;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @SOURCE@ USING Ojet  ( 
  FilterTransactionBoundaries: true,
  ConnectionURL: '@OCI-URL@',
  Tables: '@SOURCE_TABLE@',
  Password: '@OJET-PASSWORD@',
  fetchsize: 1,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Username: '@OJET-UNAME@'
 ) 
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@1 USING BigQueryWriter ( 
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  streamingUpload: 'false', 
  BatchPolicy: 'eventCount:1000000, Interval:90', 
  Tables: 'sample.tab', 
  ServiceAccountKey: '', 
  projectId: 'striimqa-214712', 
  Encoding: 'UTF-8', 
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30', 
  AllowQuotedNewLines: 'false', 
  CDDLAction: 'Process', 
  optimizedMerge: 'false', 
  TransportOptions: 'connectionTimeout=300, readTimeout=120', 
  Mode: 'MERGE', 
  StandardSQL: 'true', 
  includeInsertId: 'true', 
  QuoteCharacter: '\"' ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@2 USING BigQueryWriter ( 
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  streamingUpload: 'false', 
  BatchPolicy: 'eventCount:1000000, Interval:90', 
  Tables: 'sample.tab', 
  ServiceAccountKey: '', 
  projectId: 'striimqa-214712', 
  Encoding: 'UTF-8', 
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30', 
  AllowQuotedNewLines: 'false', 
  CDDLAction: 'Process', 
  optimizedMerge: 'true', 
  TransportOptions: 'connectionTimeout=300, readTimeout=120', 
  Mode: 'MERGE', 
  StandardSQL: 'true', 
  includeInsertId: 'true', 
  QuoteCharacter: '\"' ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@3 USING BigQueryWriter ( 
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  streamingUpload: 'false', 
  BatchPolicy: 'eventCount:1000000, Interval:90', 
  Tables: 'sample.tab', 
  ServiceAccountKey: '', 
  projectId: 'striimqa-214712', 
  Encoding: 'UTF-8', 
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30', 
  AllowQuotedNewLines: 'false', 
  CDDLAction: 'Process', 
  optimizedMerge: 'false', 
  TransportOptions: 'connectionTimeout=300, readTimeout=120', 
  Mode: 'APPENDONLY', 
  StandardSQL: 'true', 
  includeInsertId: 'true', 
  QuoteCharacter: '\"' ) 
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;;

Create Source Ojetsrc1 Using Ojet
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source Ojetsrc2 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source Ojetsrc3 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source Ojetsrc4 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source Ojetsrc5 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

create target AzureTarget using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',  
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;
END APPLICATION ADW;
deploy application ADW;
start application ADW;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
 )
INPUT FROM @STREAM@;

STOP APPLICATION BooleanExpressionsTester.BooleanExpressionsApp;
UNDEPLOY APPLICATION BooleanExpressionsTester.BooleanExpressionsApp;
DROP APPLICATION BooleanExpressionsTester.BooleanExpressionsApp CASCADE;

CREATE APPLICATION BooleanExpressionsApp;

CREATE source wsSource USING FileReader (
directory:'@TEST-DATA-PATH@',
wildcard:'bool.csv',
blocksize: 10240,
positionByEOF:false
)
PARSE USING DSVParser (
header:No,
trimquote:false
) OUTPUT TO QaStream;

CREATE TYPE wsData
(
bankbool boolean,
bankID integer key
);


CREATE WACTIONSTORE oneWS
CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@



CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT To_boolean(data[0]),to_int(data[1]) FROM QaStream
LINK SOURCE EVENT;


END APPLICATION BooleanExpressionsApp;

stop application ThreeAgentTester.CSV;
undeploy application ThreeAgentTester.CSV;
drop application ThreeAgentTester.CSV cascade;

create application CSV;

CREATE FLOW AgentFlowOne;
create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStreamOne;
END FLOW AgentFlowOne;

CREATE FLOW AgentFlowTwo;
create source CSVSourceOne using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStreamTwo;
END FLOW AgentFlowTwo;

CREATE FLOW AgentFlowThree;
create source CSVSourceTwo using CSVReader (
  Directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStreamThree;
END FLOW AgentFlowThree;

CREATE FLOW ServerFlow;

CREATE TARGET FileDumpOne Using FileWriter (
directory:'@FEATURE-DIR@/logs/',
filename:'SourceDumpFromStreamOne_%yyyy-MM-dd_HH_mm_ss_SSS%.csv',
rolloverpolicy:'eventcount:39'

)
format using DSVFormatter (
)
input from CsvStreamOne;
CREATE TYPE UserDataType
(
  UserId String KEY,
  UserName String
);

CREATE STREAM UserDataStream OF UserDataType PARTITION BY UserId;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  data[0],
        data[1]
FROM CsvStreamOne;

CREATE TARGET FileDumpTwo Using FileWriter (
directory:'@FEATURE-DIR@/logs/',
filename:'CQDumpFromUserDataStream_%yyyy-MM-dd_HH_mm_ss_SSS%.csv',
rolloverpolicy:'eventcount:39'

)
format using DSVFormatter (
)
input from UserDataStream;

CREATE WACTIONSTORE UserActivityInfo
CONTEXT OF UserDataType
EVENT TYPES ( UserDataType )
@PERSIST-TYPE@

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction
INSERT INTO UserActivityInfo
SELECT * FROM UserDataStream
LINK SOURCE EVENT;
END FLOW ServerFlow;

END APPLICATION CSV;
DEPLOY APPLICATION CSV with AgentFlowOne on all in AGENTS, AgentFlowTwo on all in AGENTS,AgentFlowThree on all in AGENTS,ServerFlow on any in default;
START CSV;

CREATE APPLICATION @APPNAME@;

CREATE TYPE @APPNAME@type1 (
 companyName java.lang.String,
 merchantId java.lang.String,
 city java.lang.String);

CREATE STREAM @APPNAME@TypedStream OF @APPNAME@type1 PARTITION BY city;

CREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (
  wildcard: '',
  positionByEOF: false,
  directory: ''
  )
PARSE USING DSVParser (
header:'true'
)
OUTPUT TO @APPNAME@Stream;

CREATE OR REPLACE CQ @APPNAME@CQ
INSERT INTO @APPNAME@TypedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantID,
TO_STRING(data[10]) as city
FROM @APPNAME@Stream;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@TypedStream;

END APPLICATION @APPNAME@;

undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;
Create Source @APPNAME@_s Using DatabaseReader
(
 Username:'@UNAME@',
 Password:'@PASSWORD@',
 ConnectionURL:'@SRCURL@',
 Tables:'QATEST.HIVE_IL_%',
 FetchSize:1,
 QuiesceOnILCompletion: true
)
Output To @APPNAME@_ss;


create Target @APPNAME@_t using HiveWriter(
            ConnectionURL:'@TGTURL@',
            Username:'@TGTUNAME@', 
            Password:'@TGTPASSWORD@',
            hadoopurl:'hdfs://dockerhost:9000/',
	        Mode:'initialLoad',
	        mergepolicy: '@MERGEPOLICY@',
            Tables:'QATEST.HIVE_IL_01,default.hive_il_01;QATEST.HIVE_IL_02,default.hive_il_02',
            hadoopConfigurationPath:'@CONF@'
	) input from @APPNAME@_ss;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

--
-- Recovery Test 13 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same compound key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> CW(p#p) -> CQ -> WS
--

STOP Recov13Tester.RecovTest13;
UNDEPLOY APPLICATION Recov13Tester.RecovTest13;
DROP APPLICATION Recov13Tester.RecovTest13 CASCADE;
CREATE APPLICATION RecovTest13 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTest10Data.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  partKey String KEY,
  serialNumber int,
  partKey2 String KEY
);

CREATE TYPE WactionData (
  partKey String KEY,
  serialNumber int
);

CREATE STREAM DataStream OF CsvData PARTITION BY partKey, partKey2;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    TO_INT(data[1]),
    data[0]
FROM CsvStream;

CREATE JUMPING WINDOW DataStreamTwoItems
OVER DataStream KEEP 2 ROWS
PARTITION BY partKey, partKey2;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    first(partKey),
    to_int(first(serialNumber))
FROM DataStreamTwoItems
GROUP BY partKey, partKey2;

END APPLICATION RecovTest13;

STOP APPLICATION DGLimitServerApp1;
UNDEPLOY APPLICATION DGLimitServerApp1;
DROP APPLICATION DGLimitServerApp1 CASCADE;
CREATE APPLICATION DGLimitServerApp1;
CREATE FLOW DGLimitServerAgentFlow;
CREATE OR REPLACE SOURCE DGLimitServerApp1_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO DGLimitServerApp1_SampleStream;
END FLOW DGLimitServerAgentFlow;
CREATE FLOW DGLimitServerServerFlow;
CREATE OR REPLACE TARGET DGLimitServerApp1_NullTarget using NullWriter()
INPUT FROM DGLimitServerApp1_SampleStream;
END FLOW DGLimitServerServerFlow;
END APPLICATION DGLimitServerApp1;
deploy application DGLimitServerApp1 on any in ServerDG1 with DGLimitServerAgentFlow on any in Agents, DGLimitServerServerFlow on any in ServerDG1;
START APPLICATION DGLimitServerApp1;

STOP APPLICATION DGLimitServerApp2;
UNDEPLOY APPLICATION DGLimitServerApp2;
DROP APPLICATION DGLimitServerApp2 CASCADE;
CREATE APPLICATION DGLimitServerApp2;
CREATE FLOW DGLimitServerAgentFlow2;
CREATE OR REPLACE SOURCE DGLimitServerApp2_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO DGLimitServerApp2_SampleStream;
END FLOW DGLimitServerAgentFlow2;
CREATE FLOW DGLimitServerServerFlow2;
CREATE OR REPLACE TARGET DGLimitServerApp2_NullTarget using NullWriter()
INPUT FROM DGLimitServerApp2_SampleStream;
END FLOW DGLimitServerServerFlow2;
END APPLICATION DGLimitServerApp2;
deploy application DGLimitServerApp2 on any in ServerDG1 with DGLimitServerAgentFlow2 on any in Agents, DGLimitServerServerFlow2 on any in ServerDG1;
START APPLICATION DGLimitServerApp2;

STOP APPLICATION ExceedApp1;
UNDEPLOY APPLICATION ExceedApp1;
DROP APPLICATION ExceedApp1 CASCADE;
CREATE APPLICATION ExceedApp1;
CREATE FLOW ExceedAgentFlow;
CREATE OR REPLACE SOURCE ExceedApp1_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO ExceedApp_SampleStream;
END FLOW ExceedAgentFlow;
CREATE FLOW ExceedServerFlow;
CREATE OR REPLACE TARGET ExceedApp1_NullTarget using NullWriter()
INPUT FROM ExceedApp_SampleStream;
END FLOW ExceedServerFlow;
END APPLICATION ExceedApp1;
deploy application ExceedApp1 on any in ServerDG1 with ExceedAgentFlow on any in Agents, ExceedServerFlow on any in ServerDG1;

UNDEPLOY APPLICATION FileToKWriterUpgrade;

DROP APPLICATION FileToKWriterUpgrade cascade;

CREATE APPLICATION FileToKWriterUpgrade;

CREATE TYPE Type1
(
 City String,
 Col2 String,
 Col3 String,
 Col4 String
);

-- Create a stream of type Type1

CREATE STREAM TypedStream OF Type1;

-- Create a source using FileReader

CREATE SOURCE KafkaCSVSource USING FILEREADER
(
 directory:'@DIRECTORY@',
 WildCard:'city*.dsv',
 positionByEOF:false,
 charset:'UTF-8'
)
PARSE USING DSVPARSER
(
 columndelimiter:',',
 ignoreemptycolumn:'Yes'
)
OUTPUT TO FileStream;

-- Read from raw stream to typed stream using CQ

CREATE CQ RawStreamCQ
INSERT INTO TypedStream
SELECT 
 data[0],
 data[1],
 data[2],
 data[3] 
FROM FileStream;

-- Load the KafkaWriter from TypedStream

CREATE TARGET KWriter USING KAFKAWRITER VERSION '0.9.0'
(
 brokerAddress:'localhost:9092',
 Topic:'@TOPIC@',
 KafkaMessageFormatVersion:v2
)
FORMAT USING DSVFORMATTER()
INPUT FROM TypedStream;


CREATE SOURCE KReader USING KAFKAREADER VERSION '0.9.0'
(
 brokerAddress:'localhost:9092',
 Topic:'@TOPIC@',
 charset : 'UTF-8',
 KafkaConfig:'retry.backoff.ms=5000',
 startOffset:0
)
PARSE USING DSVParser (
)

OUTPUT TO KafkaReaderStream;

CREATE TARGET LogKafkaReaderStream USING LOGWRITER
(
 name:KafkaLOuput,
 filename:'@LOGFILENAME@',
 flushpolicy : 'flushcount:1',
 rolloverpolicy : 'EventCount:10000,Interval:30s'
)
INPUT FROM KafkaReaderStream;


END APPLICATION FileToKWriterUpgrade;

DEPLOY APPLICATION FileToKWriterUpgrade;

START APPLICATION FileToKWriterUpgrade;

STOP APPLICATION LoadMarketsAppTester.LoadMarketsApp;
UNDEPLOY APPLICATION LoadMarketsAppTester.LoadMarketsApp;
DROP APPLICATION LoadMarketsAppTester.LoadMarketsApp CASCADE;

CREATE APPLICATION LoadMarketsApp;

CREATE OR REPLACE SOURCE MarketSource USING FileReader (
 directory:'@TEST-DATA-PATH@',
  wildcard: 'marketlocations.json',
  blocksize: '64',
  charset: 'UTF-8',
  positionbyeof: 'false',
  rolloverpolicy: 'DefaultFileComparator',
  skipbom: 'true'
 )
 PARSE USING JSONParser (
  fieldName: 'locations'
 )
OUTPUT TO RawMarketLocationsStream;


CREATE OR REPLACE CQ ConvertRawShapes
INSERT INTO MarketStream
SELECT e.data.get("marketid").textValue() as id,
       e.data.get("marketname").textValue() as name,
       e.data.withArray("geopoints") as points,
       DNOW() as lastUpdated
FROM RawMarketLocationsStream e;

CREATE OR REPLACE CQ SplitLatLon
INSERT INTO PontStream
SELECT
  s.id as id,
  s.name as name,
  lat as lat ,
  lon as lon,
  s.lastUpdated as lastUpdated
FROM MarketStream s, iterator(s.points, (lat String, lon String)) a;

CREATE OR REPLACE WINDOW MarketWindow OVER MarketStream KEEP WITHIN 1 HOUR ON lastUpdated;


-- added by nambi ,Reason - for filtering out "lastUpdated", which has current time, is to have static expected results

CREATE OR REPLACE CQ SplitLatLon2
INSERT INTO PontStream2
SELECT
  s.id as id,
  s.name as name,
  lat as lat ,
  lon as lon
FROM MarketStream s, iterator(s.points, (lat String, lon String)) a;

CREATE TYPE wsData
(
  id String,
  name String,
  lat String,
  lon String
);


CREATE WACTIONSTORE oneWS
CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@

CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT
  id,
  name,
  lat ,
  lon
FROM PontStream2
LINK SOURCE EVENT;

END APPLICATION LoadMarketsApp;

stop application ADW;
undeploy application ADW;
drop application ADW cascade;
CREATE APPLICATION ADW;

CREATE  SOURCE OracleInitialLoad USING DatabaseReader  
 (
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'@SOURCE-TABLES@',
 FetchSize:2000
) 
OUTPUT TO InitialLoadStream;

CREATE TARGET AzureDWInitialLoad USING AzureSQLDWHWriter(
ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
)
INPUT FROM InitialLoadStream;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@_stream;

stop application @AppName@;
Undeploy application @AppName@;
Alter application @AppName@;

CREATE FLOW @AppName@_Agent_flow;

CREATE OR REPLACE SOURCE @AppName@_FileReaderSource USING FileReader (
wildcard: 'posdata.csv', 
  positionByEOF: false, 
  blocksize: 10100,
  rolloverpolicy: 'EventCount:100,Interval:30s', 
  directory: '@dir@' ) 
PARSE USING DSVParser ( 
  trimquote: false, 
  header: 'Yes' ) 
OUTPUT TO @AppName@_CsvStream;

END FLOW @AppName@_Agent_flow;

alter application @AppName@ recompile;
DEPLOY APPLICATION @AppName@ with @AppName@_Agent_flow on any in AGENTS;
start application @AppName@;

stop ADW;
undeploy application ADW;
drop application ADW cascade;

CREATE APPLICATION ADW recovery 5 second interval;
create flow agentflow;
CREATE OR REPLACE SOURCE CSVPoller USING FileReader (
directory:'@DIRECTORY@',
WildCard:'posdata5L.csv',
positionByEOF:false
)
parse using DSVParser (
header:'no'
)
OUTPUT TO CsvStream;
end flow agentflow;

create flow targetflow;
CREATE OR REPLACE TYPE CSVStream_Type  ( BUSINESS_NAME java.lang.String KEY,
MERCHANT_ID java.lang.String,
PRIMARY_ACCOUNT_NUMBER java.lang.String
 ) ;

CREATE OR REPLACE STREAM CSVTypeStream OF CSVStream_Type;
CREATE OR REPLACE CQ CQ1
INSERT INTO CSVTypeStream
SELECT data[0],data[1],data[2]
FROM CsvStream;

create target WriteToAzureSQLWH using AzureSQLDWHWriter (
       ConnectionURL: '@CONNECTION-URL@',
       username: '@USERNAME@',
       password: '@PASSWORD@',
	   AccountName: '@STORAGE-ACCOUNT@',
       AccountAccessKey: '@ACCESS-KEY@',
      Tables: '@TARGET-TABLE@',
      uploadpolicy:'eventcount:20000,interval:1m'
) INPUT FROM CSVTypeStream;
end flow targetflow;
END APPLICATION ADW;
deploy application ADW with agentflow in agents,targetflow in default;
start ADW;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING MSSqlReader
(
  Compression: false,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.test01',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
)
OUTPUT TO @APPNAME@_SS;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'NULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

stop Application zoneDateTime_CaseSensistive_CQ;
undeploy application zoneDateTime_CaseSensistive_CQ;
drop application zoneDateTime_CaseSensistive_CQ cascade;

create application zoneDateTime_CaseSensistive_CQ;

CREATE OR REPLACE SOURCE csvinput USING FileReader  ( 
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '@ResourceDirectory@',
  skipbom: true,
  wildcard: 'ZonedDateTime_data.csv'
 ) 
 PARSE USING DSVParser  ( 
  _h_ReturnDateTimeAs: 'ZonedDateTime'
 ) 
OUTPUT TO csvinput_out ;

CREATE  TYPE cq_zonedtime_out_Type  ( 
a java.lang.Integer , 
b string,
c string,
d string,
e string,
f string
--b java.time.ZonedDateTime , 
--c java.time.ZonedDateTime ,
--d java.time.ZonedDateTime , 
--e java.time.ZonedDateTime ,
--f java.time.ZonedDateTime   
 );

CREATE STREAM cq_zonedtime_out OF cq_zonedtime_out_Type;

CREATE OR REPLACE   CQ cq_zonedtime 
INSERT INTO cq_zonedtime_out
SELECT TO_INT(data[0]) as a,
TO_STRING(TO_ZONEDDATETIME(data[1],"dd-MMM-yy hh.mm.ss.SSSSSSSSS a"),"dd-MMM-yy hh.mm.ss.SSSSSSSSS a") as b,
TO_STRING(TO_ZONEDDATETIME(data[2], "dd-MMM-yy hh.mm.ss.SSSSSSSSS a z"),"dd-MMM-yy hh.mm.ss.SSSSSSSSS a z") as c,
TO_STRING(TO_ZONEDDATETIME(data[3], "EEEEE dd-MMM-yy hh.mm.ss.SSSSSSSSS a"),"EEEEE dd-MMM-yy hh.mm.ss.SSSSSSSSS a") as d,
TO_STRING(TO_ZONEDDATETIME(data[4], "E dd-MMM-yy hh.mm.ss.SSSSSSSSS a z"),"E dd-MMM-yy hh.mm.ss.SSSSSSSSS a z") as e,
TO_STRING(TO_ZONEDDATETIME(data[5], "EEEE dd-MMMM-yy hh.mm.ss.SSSSSSSSS a z"),"EEEE dd-MMMM-yy hh.mm.ss.SSSSSSSSS a z") as f
FROM csvinput_out c;

create or replace target sample_out using Sysout(
  name:'Foo'
)INPUT FROM cq_zonedtime_out;

create Target File_tgt using FileWriter(
  filename:'ZonedDateTime_ActualOutput.csv',
  directory:'@OutputDirectory@',
  rolloverpolicy:'eventcount:1000'
)
format using DSVFormatter (

)
input from cq_zonedtime_out;

END APPLICATION zoneDateTime_CaseSensistive_CQ;

deploy application zoneDateTime_CaseSensistive_CQ;

start application zoneDateTime_CaseSensistive_CQ;

create application DhcpLog;
create source DHCPLogSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'Dhcp*',
	charset:'UTF-8',
	positionByEOF:false
) PARSE USING DHCPLogParser (
	rowdelimiter:'\r\n',
	LineNumber:33
)
OUTPUT TO DHCPLogStream;
create Target DHCPDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/dhcp_log') input from DHCPLogStream;
end application DhcpLog;

STOP TestAlertsEmail.TestAlertsEmailApp;
UNDEPLOY APPLICATION TestAlertsEmail.TestAlertsEmailApp;
DROP APPLICATION TestAlertsEmail.TestAlertsEmailApp CASCADE;

CREATE APPLICATION TestAlertsEmailApp;

CREATE source rawSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:No,
  wildcard:'alerts_csv.txt',
  coldelimiter:' ',
  positionByEOF:false
) OUTPUT TO rawStream;

CREATE STREAM MyAlertStream OF Global.AlertEvent;
CREATE CQ GenerateMyAlerts
INSERT INTO MyAlertStream (name, keyVal, severity, flag, message)
SELECT "Testing Alerts", data[0], data[1], data[2], data[3]
FROM rawStream s;
CREATE TARGET output2 USING SysOut(name : alertsrecevied) input FROM MyAlertStream;

CREATE SUBSCRIPTION alertSubscription USING EmailAdapter
(SMTPUSER:'zalakalerts@gmail.com',
SMTPPASSWORD:'paloalto',
smtpurl:'smtp.gmail.com',
starttls_enable:'true',
smtp_auth:'true',
subject:"Test Email Alerts With Security Enabled",
emailList:"siddhika@striim.com,s.henry@striim.com,saranya@striim.com,invalidmailid.@striim.com",
userIds:'admin',
threadCount:"5",
senderEmail:"doga@striim.com"
)
INPUT FROM MyAlertStream;

END APPLICATION TestAlertsEmailApp;
DEPLOY APPLICATION TestAlertsEmail.TestAlertsEmailApp;
START TestAlertsEmail.TestAlertsEmailApp;

--
-- Recovery Test 36 with two sources, two jumping attribute windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W/p -> CQ1 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS
--

STOP NameW03.W03;
UNDEPLOY APPLICATION NameW03.W03;
DROP APPLICATION NameW03.W03 CASCADE;
CREATE APPLICATION W03 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  wildcard:'Canon1000.csv',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[2],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[3])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[2],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[3])
FROM CsvStream2;

CREATE JUMPING WINDOW JWc51uW03
OVER DataStream1 KEEP 51 ROWS;

CREATE JUMPING WINDOW JWc101uW03
OVER DataStream2 KEEP 101 ROWS;

CREATE WACTIONSTORE WactionsW03 CONTEXT OF WactionData
EVENT TYPES ( CsvData KEY(merchantId) )
@PERSIST-TYPE@

CREATE CQ JWc51uW03_to_WactionsW03
INSERT INTO WactionsW03
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM JWc51uW03 p
GROUP BY p.merchantId;

CREATE CQ JWc101uW03_to_WactionsW03
INSERT INTO WactionsW03
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM JWc101uW03 p
GROUP BY p.merchantId;

END APPLICATION W03;

STOP APPLICATION persistencetester.ranGenAppES;
UNDEPLOY APPLICATION persistencetester.ranGenAppES;
DROP APPLICATION persistencetester.ranGenAppES cascade;

CREATE APPLICATION ranGenAppES;

CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity int,
  size int,
  myDate datetime
);

CREATE  SOURCE liveSource USING StreamReader (
  OutputType: 'Atm',
  noLimit: 'false',
  maxRows: 20,
  iterations: 0,
  iterationDelay: 2000,
  StringSet: 'productID[1001-1002-1003-1004],stateID[BofA-WellsFargo-Chase-NYBank]',
  NumberSet: 'productWeight[3-3]R,quantity[0-200]R,size[250-1250]R'
 )
OUTPUT TO CsvStream;

CREATE STREAM ranDataStream OF Atm;

CREATE CQ ranToRanData
INSERT INTO ranDataStream
SELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4]), TO_DATE(data[5])
FROM CsvStream;

CREATE WACTIONSTORE RanGenDataStore
CONTEXT OF Atm
EVENT TYPES ( Atm )
@PERSIST-TYPE@

CREATE CQ TrackRanGenDataActivity
INSERT INTO RanGenDataStore
Select * from ranDataStream
LINK SOURCE EVENT;


END APPLICATION ranGenAppES;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING DSVFormatter (
members:'data')
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.test01'
 ) 
OUTPUT TO @APPNAME@_SS;


CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'public.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12)',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

stop application app2PS;
undeploy application app2PS;
drop application app2PS cascade;

create application app2PS;

create target File_TargerPS2 using FileWriter
(
directory : '',
filename : ''
)
format using DSVFormatter()
input from Recoveryss2;

end application app2PS;

deploy application app2PS;
start application app2PS;

--
-- Recovery Test 14 with one source, a pattern matching CQ, and one WactionStore
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> PM-CQ -> WS
--

STOP Recov14Tester.RecovTest14;
UNDEPLOY APPLICATION Recov14Tester.RecovTest14;
DROP APPLICATION Recov14Tester.RecovTest14 CASCADE;
CREATE APPLICATION RecovTest14 RECOVERY 10 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  companyName String,
  merchantId String,
  dataCode int KEY,
  expDate String
);

CREATE TYPE WactionData (
companyName1 String KEY,
companyName2 String,
companyName3 String,
dataCode1 int,
dataCode2 int,
dataCode3 int
);

CREATE STREAM DataStream OF CsvData;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_INT(data[3]),
    data[5]
FROM CsvStream;





CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@


CREATE CQ PatternMatchCQ
INSERT INTO Wactions
SELECT A.companyName as companyName1,
        B.companyName as companyName2,
        C.companyName as companyName3,
        A.dataCode as dataCode1,
        B.dataCode as dataCode2,
        C.dataCode as dataCode3
from DataStream
MATCH_PATTERN A E*  B D* C
DEFINE
A=DataStream(dataCode = 0),
E=DataStream(datacode != 1),
B=DataStream(dataCode = 1),
D= Datastream(datacode != 2),
C=DataStream(dataCode = 2);






CREATE WINDOW RestartFromSpecificLocation
OVER CsvStream KEEP 8 ROWS;





END APPLICATION RecovTest14;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[2]) = 'Null' ;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

undeploy application RedshiftColmap;
alter application RedshiftColmap;

CREATE OR REPLACE SOURCE OracleSource USING OracleReader  (
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: '@SOURCE_TABLES@',
  FetchSize: 1
 ) Output To LogminerStream;
     
CREATE OR REPLACE TARGET RedshiftTarget USING RedshiftWriter
	(
	  ConnectionURL: '@TARGET-URL@',
	  Username: '@TARGET-UNAME@',
	  Password: '@TARGET-PASSWORD@',
	  bucketname: '@BUCKETNAME@',
	  --accesskeyId: '@ACCESS-KEY-ID@',
	  --secretaccesskey: '@SECRET-ACCESS-KEY@',
	  S3IAMRole:'@IAMROLE@',
	  Tables: '@TARGET-TABLES@',
	  uploadpolicy:'eventcount:1,interval:5s',
	  Mode:'incremental'
	) INPUT FROM LogminerStream;
	
END APPLICATION RedshiftColmap;
ALTER APPLICATION RedshiftColmap RECOMPILE;
deploy application RedshiftColmap;
START application RedshiftColmap;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SourceName@ Using MSSqlReader
(
 Username:'qatest',
 Password:'w3b@ct10n',
 DatabaseName:'qatest',
 ConnectionURL:'localhost:1433',
 Tables:'qatest.source1',
 ConnectionPoolSize:1,
 StartPosition:'EOF'
 ) Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter ( 
CheckPointTable: 'CHKPOINT',
Username: 'qatest',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',
Tables: 'QATEST.SOURCE1,qatest.target1',
Password: 'qatest'
)INPUT FROM @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetsys@ USING SysOut (name: 'ora12_out') INPUT FROM @SRCINPUTSTREAM@;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

CREATE TARGET @TARGET_NAME@ USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest',
	batchPolicy: 'EventCount: 10000,Interval:600s'
) INPUT FROM @STREAM@;

Create Source @SOURCE_NAME@ Using OracleReader
(
 Compression: false,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  --StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
) Output To @STREAM@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'smallposdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
	filename:'TargetPosDataXmlFSDefault',
	directory:'@FEATURE-DIR@/logs/',
    sequence:'00',
	rolloverpolicy:'FileSizeRollingPolicy'
)
format using XMLFormatter (
	rootelement:'document',
	elementtuple:'MerchantName:zip:text=merchantname'
)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlDFS_actual.log') input from TypedCSVStream;

end application DSV;

CREATE APPLICATION ValidateFiles;

CREATE OR REPLACE TYPE AggregatorOutput_Type  ( SUMsthingSUMtthing java.lang.Long );

CREATE OR REPLACE STREAM AggregatorOutput OF AggregatorOutput_Type;

CREATE OR REPLACE TARGET DiffFile USING FileWriter  (
  filename: 'Difference',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:10000,interval:30',
  adapterName: 'FileWriter',
  directory: '@FEATURE-DIR@/logs',
  rolloverpolicy: 'eventcount:10000,interval:30s'
 )
FORMAT USING DSVFormatter  (   nullvalue: 'NULL',
  standard: 'none',
  handler: 'com.webaction.proc.DSVFormatter',
  formatterName: 'DSVFormatter',
  usequotes: 'false',
  rowdelimiter: '\n',
  quotecharacter: '\"',
  header: 'false',
  columndelimiter: ','
 )
INPUT FROM AggregatorOutput;

CREATE OR REPLACE TYPE ewfew_Type  ( thing java.lang.Integer );

CREATE OR REPLACE TYPE newTargetST_Type  ( thing java.lang.Integer );

CREATE OR REPLACE STREAM newTargetST OF newTargetST_Type;

CREATE OR REPLACE JUMPING WINDOW AllTargetSums OVER newTargetST KEEP WITHIN 15 SECOND;

CREATE OR REPLACE SOURCE TargetSums USING FileReader  (
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '@FEATURE-DIR@/logs',
  skipbom: true,
  wildcard: 'TargetResults.00'
 )
 PARSE USING DSVParser  (
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: '-1',
  nocolumndelimiter: true,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: false,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 )
OUTPUT TO compareTstream ;

CREATE OR REPLACE CQ GetDemTargetValues
INSERT INTO newTargetST
SELECT TO_LONG(data[0]) as thing
FROM compareTstream;

CREATE OR REPLACE SOURCE SourceSums USING FileReader  (
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '@FEATURE-DIR@/logs',
  skipbom: true,
  wildcard: 'SourceResults.00'
 )
 PARSE USING DSVParser  (
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: '-1',
  nocolumndelimiter: true,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: false,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 )
OUTPUT TO compareStream ;

CREATE OR REPLACE TYPE newST_Type  ( thing java.lang.Integer );

CREATE OR REPLACE STREAM newST OF newST_Type;

CREATE OR REPLACE JUMPING WINDOW AllSourceSums OVER newST KEEP WITHIN 15 SECOND;

CREATE OR REPLACE CQ Aggregator
INSERT INTO AggregatorOutput
SELECT SUM(s.thing) - SUM(t.thing)
FROM AllSourceSums s, AllTargetSums t;

CREATE TARGET output1 USING SysOut(name : SrcItem) input FROM compareStream;
CREATE TARGET output2 USING SysOut(name : TrgItem) input FROM compareTstream;
CREATE TARGET output3 USING SysOut(name : AggregatorItem) input FROM AggregatorOutput;

CREATE OR REPLACE CQ GetdemValues
INSERT INTO newST
SELECT TO_LONG(data[0]) as thing
FROM compareStream;

END APPLICATION ValidateFiles;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 1 second interval;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@'
 )
OUTPUT TO @APPNAME@Stream;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;


CREATE TYPE test_typeRecov 
(
node_new com.fasterxml.jackson.databind.JsonNode,
node_name com.fasterxml.jackson.databind.JsonNode,
node_addr com.fasterxml.jackson.databind.JsonNode
);

Create stream cqAsJSONNodeStreamRecov of test_typeRecov;

CREATE CQ GetPOAsJsonNodesRecov
INSERT into cqAsJSONNodeStreamRecov
select 
data.get('ACCTS-RECORD'),
data.get('ACCTS-RECORD').get('NAME'),
data.get('ACCTS-RECORD').get('ADDRESS3')
from @APPNAME@Stream js;

create type finaldtypeRecov
(ACCOUNT_NO int,
FIRST_NAME String,
LAST_NAME String,
ADDRESS1 String,
ADDRESS2 String,
CITY String,
STATE String,
ZIP_CODE int);

CREATE STREAM getdataStreamPSRecov OF finaldtypeRecov;

CREATE CQ getdataRecov
INSERT into getdataStreamPSRecov
select JSONGetInteger(x.node_new,"ACCOUNT-NO"),
JSONGetString(x.node_name,"FIRST-NAME"),
JSONGetString(x.node_name,"LAST-NAME"),
JSONGetString(x.node_new,"ADDRESS1"),
JSONGetString(x.node_new,"ADDRESS2"),
JSONGetString(x.node_addr,"CITY"),
JSONGetString(x.node_addr,"STATE"),
JSONGetInteger(x.node_addr,"ZIP-CODE")
from cqAsJSONNodeStreamRecov x;

create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:1000,Interval:50',
  CommitPolicy: 'EventCount:1000,Interval:50',
  Tables: 'QATEST.@table@'
)
input from getdataStreamPSRecov;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

CREATE Source dataGenSrc Using PostgreSQLReader  ( 
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: true,
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://127.0.0.1:5432/webaction?stringtype=unspecified',
  Tables:'@tableNames@',
  adapterName: 'PostgreSQLReader',
  Password: 'xFzvJYZf1b8=',
  Password_encrypted: 'true'
 ) 
Output To LCRStream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;

END APPLICATION DataGenSampleApp;

create or replace propertyvariable TEAMSREFTOKEN = '@refershtoken@';
 create or replace propertyvariable TEAMSCLIENTSECRET = '@clientSecret@';
 create or replace propertyvariable TEAMSCHANNELURL = '@channelUrl@';
 create or replace propertyvariable TEAMSCLIENTID = '@clientID@';

CREATE APPLICATION @AppName@ WITH ENCRYPTION EXCEPTIONHANDLER (AdapterException: 'IGNORE', ArithmeticException: 'IGNORE', ClassCastException: 'IGNORE', ConnectionException: 'IGNORE', InvalidDataException: 'IGNORE', NullPointerException: 'IGNORE', NumberFormatException: 'IGNORE', SystemException: 'IGNORE', UnExpectedDDLException: 'IGNORE', UnknownException: 'IGNORE')  USE EXCEPTIONSTORE TTL : '7d' ;

CREATE OR REPLACE SOURCE OracleReader_AlertSource USING oraclereader (
  ConnectionURL: '@ConnectionURl@',
  Tables: '@table@',
  Password: '@password@',
  Username: '@username@' )
OUTPUT TO AlertUpgradeStream;

CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE STREAM SlackAndTeamsStream OF Global.SlackAlertEvent;

CREATE OR REPLACE TARGET SmartAlertUpgradeSysOut USING SysOut (
  name: 'SmartAlertUpgradeSysOut' )
INPUT FROM AlertUpgradeStream;

CREATE OR REPLACE CQ GenerateAlertsForWebAndEmail
INSERT INTO AlertStream
SELECT data[1],
 data[1],
 'info',
 'raise',
'Welcome\n to Striim'
FROM AlertUpgradeStream;


 CREATE OR REPLACE CQ GenerateAlertsForSlackAndTeams
 INSERT INTO SlackAndTeamsStream
 SELECT data[1],
  data[1],
 'info',
 'raise',
 'Welcome\n to Striim',
 'TestChannel'
 FROM AlertUpgradeStream;


CREATE TARGET TeamsAlertSender USING Global.TeamsAlertAdapter (
  refreshToken: '$TEAMSREFTOKEN',
  clientSecret: '$TEAMSCLIENTSECRET',
  channelURL: '$TEAMSCHANNELURL',
  clientID: '$TEAMSCLIENTID' )
INPUT FROM SlackAndTeamsStream;


CREATE SUBSCRIPTION WebAlertSender USING WebAlertAdapter (
  isSubscription: 'true',
  channelName: 'admin_PosAppWebAlert' )
INPUT FROM AlertStream;


CREATE TARGET slackalertSender USING Global.SlackAlertAdapter (
  OauthToken: '@oauth@',
  ChannelName: '@slackChannel@',
  OauthToken_encrypted: 'false' )
INPUT FROM SlackAndTeamsStream;

CREATE SUBSCRIPTION EmailAlertsender USING EmailAdapter (
  smtpurl: '@smtpUrl@',
  starttls_enable: '@starttls@',
  SMTPUSER: '@smtpuser@',
  SMTPPASSWORD: '@stmpPwsd@',
  emailList: '@emailList@',
  subject: '@subject@',
  senderEmail: '@senderEmail@',
  SMTPPASSWORD_encrypted: 'false')
INPUT FROM AlertStream;

END APPLICATION @AppName@;

create Target @TARGET_NAME@ using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (

)
input from @STREAM@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @APPNAME@_src Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
) Output To @APPNAME@_stream;

create type @APPNAME@_type(
id int,
name String,
cost float,
Lvalue long,
PreviousPaid double,
dateOrder DateTime,
DwoTime Datetime,
DeliverDate Datetime,
TableName string,
OperationName String
);

create or replace stream @APPNAME@_typed_Stream of @APPNAME@_type;

Create CQ @APPNAME@_TypedCQ
insert into @APPNAME@_typed_Stream
select
to_int(data[0]),data[1],to_float(data[2]),to_long(data[3]),to_double(data[4]),
to_Date(data[5]),to_Date(data[6]),to_Date(data[7]),
meta(@APPNAME@_stream,'TableName'),
Meta(@APPNAME@_stream,'OperationName') from @APPNAME@_stream;


create Target @APPNAME@_tgt using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'eventCount:5',
    ServiceAccountKey:'@file-path@'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_typed_Stream;




end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

STOP twoNodeCacheRefresh.AppTest;
UNDEPLOY APPLICATION twoNodeCacheRefresh.AppTest;
DROP APPLICATION twoNodeCacheRefresh.AppTest cascade;

CREATE APPLICATION AppTest;

CREATE FLOW THESOURCEFLOW;
----------------------------------------------------

CREATE TYPE GenType
(
  GenID Integer KEY,
  GenCode double,
  GenState String,
  GenDate DateTime,
  GenLong Long
);


CREATE SOURCE GenSource USING StreamReader(
	OutputType: 'twoNodeCacheRefresh.GenType',
	noLimit: 'false',
	maxRows: 100,
	iterationDelay: 5,
	StringSet: 'GenState[CA-FL]',
	NumberSet: 'GenID[0-0]Linc,GenCode[99-99]Linc,GenLong[4999999-4999999]G'
)OUTPUT TO rawGenStream;


END FLOW THESOURCEFLOW;
----------------------------------------------------

CREATE FLOW MAINPROCESSFLOW;

CREATE TYPE theGenType
(
  theGenID Integer KEY,
  theGenCode double,
  theGenState String,
  theGenDate DateTime,
  theGenLong Long
);

CREATE STREAM GenStream OF theGenType;

CREATE CQ GenCQ1
INSERT INTO GenStream
SELECT TO_INT(data[0]),
	   TO_DOUBLE(data[1]),
	   data[2],
	   TO_DATE(data[3]),
	   TO_LONG(data[4])
FROM rawGenStream;


CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'addy.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false
) QUERY (keytomap:'zip', refreshinterval:'20 second', skipinvalid:'true') OF USAddressData;


CREATE TYPE MergedData
(
  country String,
  zip String,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String,
  theGenID Integer,
  theGenCode double,
  theGenState String,
  theGenDate DateTime,
  theGenLong Long
);

CREATE STREAM SanDiegoDataStream OF MergedData;
CREATE STREAM MiamiDataStream OF MergedData;

-- join the data
CREATE CQ SanDiegoCQData
INSERT INTO SanDiegoDataStream
SELECT z.country, z.zip, z.city, z.state, z.stateCode,
       z.fullCity, z.someNum, z.pad, z.latVal,
       z.longVal, z.empty, z.empty2, gs.theGenID,
       gs.theGenCode, gs.theGenState, gs.theGenDate, gs.theGenLong
       FROM GenStream gs, ZipLookup z where gs.theGenState = 'CA' AND z.zip LIKE '921%';

CREATE CQ MiamiCQData
INSERT INTO MiamiDataStream
SELECT z.country, z.zip, z.city, z.state, z.stateCode,
       z.fullCity, z.someNum, z.pad, z.latVal,
       z.longVal, z.empty, z.empty2, gs.theGenID,
       gs.theGenCode, gs.theGenState, gs.theGenDate, gs.theGenLong
       FROM GenStream gs, ZipLookup z where gs.theGenState = 'FL' AND z.zip LIKE '331%';


CREATE TYPE MiamiData
(
    zip String KEY,
    fullCity String,
    theGenCode double,
    theGenState String
);

CREATE WACTIONSTORE MiamiStore
CONTEXT OF MiamiData
EVENT TYPES(MergedData )
PERSIST NONE USING ( );


CREATE CQ MiamiStoreCQ
INSERT INTO MiamiStore
SELECT zip, fullCity, theGenCode, theGenState
FROM MiamiDataStream
LINK SOURCE EVENT;


CREATE TYPE SanDiegoData
(
    zip String KEY,
    fullCity String,
    theGenCode double,
    theGenState String
);

CREATE WACTIONSTORE SanDiegoStore
CONTEXT OF SanDiegoData
EVENT TYPES(MergedData )
PERSIST NONE USING ( );


CREATE CQ SanDiegoStoreCQ
INSERT INTO SanDiegoStore
SELECT zip, fullCity, theGenCode, theGenState
FROM SanDiegoDataStream
LINK SOURCE EVENT;


END FLOW MAINPROCESSFLOW;
----------------------------------------------------

END APPLICATION AppTest;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;
Create Source Ojetsrc Using Ojet
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;


create target AzureTarget using AzureSQLDWHWriter (
ConnectionURL: '@SQLDW-URL@',
username: '@SQLDW-USERNAME@',
password: '@SQLDW-PASSWORD@',
AccountName: '@STORAGEACCOUNT@',
AccountAccessKey: '@ACCESSKEY@',
Tables: '@TARGET-TABLES@',
uploadpolicy:'@EVENT-COUNT@',
Mode:'Merge'
) INPUT FROM str;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using DatabaseReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@;

CREATE SOURCE @SourceName@ USING OracleReader  (
ReaderType: 'LogMiner',
  Password_encrypted: 'false',
  SupportPDB: false,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  QueueSize: 2048,
  CommittedTransactions: true,
  Username: '@UserName@',
  TransactionBufferType: 'Memory',
  _h_ReturnDateTimeAs: 'ZonedDateTime',
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  OutboundServerProcessName: 'WebActionXStream',
  Password: '@Password@',
  DDLCaptureMode: 'All',
  Compression: false,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  FetchSize: 1,
  Tables: '@SourceTables@',
  DictionaryMode: 'OnlineCatalog',
  XstreamTimeOut: 600,
  TransactionBufferSpilloverSize: '1MB',
  StartTimestamp: 'null',
  FilterTransactionBoundaries: true,
  StartSCN: null,
  ConnectionURL: '@ConnectionURL@',
  SendBeforeImage: true )
OUTPUT TO @AppStream@  ;

CREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream SELECT data[2], to_string(data[2], "dd-MMM-yy hh.mm.ss") FROM @AppStream@ o ;

CREATE  TARGET @targetsys@ USING Global.SysOut  (
name: 'ora1_sys' )
INPUT FROM admin.ZDT_cq_stream;

create Target @TargetFile@ using FileWriter(
  filename:'toStringOut.log',
  directory:'@FilePath@',
  rolloverpolicy:'eventcount:1000'
)
format using DSVFormatter (

)
input from admin.ZDT_cq_stream;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.test01',
	FetchSize: '1'
)
OUTPUT TO @APPNAME@_SS;


CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(id,name) COLUMNMAP(id=id,col1=col11,col2=col12)',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true		
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

--
-- Crash Recovery Test 2 on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS
--

STOP APPLICATION N4S4CR2Tester.N4S4CRTest2;
UNDEPLOY APPLICATION N4S4CR2Tester.N4S4CRTest2;
DROP APPLICATION N4S4CR2Tester.N4S4CRTest2 CASCADE;
CREATE APPLICATION N4S4CRTest2 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest2;

CREATE SOURCE CsvSourceN4S4CRTest2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest2;

CREATE FLOW DataProcessingN4S4CRTest2;

CREATE TYPE WactionTypeN4S4CRTest2 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionTypeN4S4CRTest2;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN4S4CRTest2 CONTEXT OF WactionTypeN4S4CRTest2
EVENT TYPES ( WactionTypeN4S4CRTest2 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN4S4CRTest2
INSERT INTO WactionsN4S4CRTest2
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN4S4CRTest2;

END APPLICATION N4S4CRTest2;

STOP APPLICATION EH;
UNDEPLOY APPLICATION EH;
DROP APPLICATION EH CASCADE;
CREATE APPLICATION EH recovery 5 second interval;
CREATE Source s USING PostgreSQLReader  ( 
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Tables: 'public.tablename1000%') 
OUTPUT TO ss ;

CREATE OR REPLACE TYPE jsontype( 
datae java.util.HashMap , 
TABLE_NAME java.lang.String , 
OPS_NAME java.lang.String , 
DB_TIMESTAMP java.lang.String );

CREATE STREAM cq_json_out OF jsontype PARTITION BY TABLE_NAME;

CREATE OR REPLACE CQ cq_json 
INSERT INTO cq_json_out
SELECT 
data(e),
META(e,"TableName").toString() as TABLE_NAME,
META(e, "OperationName").toString() as OPS_NAME,
META(e, "TimeStamp").toString() as DB_TIMESTAMP
FROM ss e;

CREATE CQ cq1
INSERT INTO TypedAccessLogStream1
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000101'; 

create Target t1 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_101',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_101',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream1;

CREATE CQ cq2
INSERT INTO TypedAccessLogStream2
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000102'; 

create Target t2 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_102',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_102',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream2;

CREATE CQ cq3
INSERT INTO TypedAccessLogStream3
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000103'; 

create Target t3 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_103',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_103',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream3;

CREATE CQ cq4
INSERT INTO TypedAccessLogStream4
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000104'; 

create Target t4 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_104',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_104',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream4;

CREATE CQ cq5
INSERT INTO TypedAccessLogStream5
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000105'; 

create Target t5 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_105',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_105',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream5;

CREATE CQ cq6
INSERT INTO TypedAccessLogStream6
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000106'; 

create Target t6 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_106',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_106',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream6;

CREATE CQ cq7
INSERT INTO TypedAccessLogStream7
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000107'; 

create Target t7 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_107',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_107',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream7;

CREATE CQ cq8
INSERT INTO TypedAccessLogStream8
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000108'; 

create Target t8 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_108',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_108',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream8;

CREATE CQ cq9
INSERT INTO TypedAccessLogStream9
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000109'; 

create Target t9 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_109',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_109',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream9;

-- CREATE CQ cq10
-- INSERT INTO TypedAccessLogStream10
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000110'; 
-- 
-- create Target t10 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_110',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_110',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream10;


-- CREATE CQ cq11
-- INSERT INTO TypedAccessLogStream11
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000111'; 
-- 
-- create Target t11 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_111',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_111',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream11;
-- 
-- CREATE CQ cq12
-- INSERT INTO TypedAccessLogStream12
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000112'; 
-- 
-- create Target t12 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_112',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_112',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream12;
-- 
-- CREATE CQ cq13
-- INSERT INTO TypedAccessLogStream13
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000113'; 
-- 
-- create Target t13 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_113',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_113',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream13;
-- 
-- CREATE CQ cq14
-- INSERT INTO TypedAccessLogStream14
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000114'; 
-- 
-- create Target t14 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_114',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_114',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream14;
-- 
-- CREATE CQ cq15
-- INSERT INTO TypedAccessLogStream15
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000115'; 
-- 
-- create Target t15 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_115',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_115',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream15;
-- 
-- CREATE CQ cq16
-- INSERT INTO TypedAccessLogStream16
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000116'; 
-- 
-- create Target t16 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_116',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_116',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream16;
-- 
-- CREATE CQ cq17
-- INSERT INTO TypedAccessLogStream17
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000117'; 
-- 
-- create Target t17 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_117',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_117',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream17;
-- 
-- CREATE CQ cq18
-- INSERT INTO TypedAccessLogStream18
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000118'; 
-- 
-- create Target t18 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_118',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_118',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream18;
-- 
-- CREATE CQ cq19
-- INSERT INTO TypedAccessLogStream19
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000119'; 
-- 
-- create Target t19 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_119',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_119',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream19;
-- 
-- CREATE CQ cq20
-- INSERT INTO TypedAccessLogStream20
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000120'; 
-- 
-- create Target t20 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_120',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_120',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream20;


END APPLICATION EH;
DEPLOY APPLICATION EH;
start application EH;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING PostgreSQLReader  (
  ReplicationSlotName: 'Slot_Name',
  FilterTransactionBoundaries: 'true',
  Username: 'User_Name',
  ConnectionURL: 'Connection_URL',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'Password',
  Tables: 'Tables',
  PostgresConfig:'{"ReplicationPluginConfig": {"Name": "WAL2JSON", "Format": "2"}}'
 )
OUTPUT TO @STREAM@ ;

CREATE TARGET @SOURCE_NAME@_sysout USING Global.SysOut (
  name: '@SOURCE_NAME@_sysout' )
INPUT FROM @STREAM@;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;


stop OracleToKudu_ExpStore;
undeploy application OracleToKudu_ExpStore;
drop application OracleToKudu_ExpStore cascade;

--drop exceptionstore admin.OracleToKudu_ExceptionStore;
CREATE APPLICATION OracleToKudu use exceptionstore;
Create Source oracSource Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;

CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;



CREATE APPLICATION OracleToKudu_ExpStore;

CREATE TYPE OracleToKudu_ExpStore_CDCStreams_Type  (
 exceptionType java.lang.String,
  action java.lang.String,
  appName java.lang.String,
  entityType java.lang.String,
  entityName java.lang.String,
  className java.lang.String,
  message java.lang.String 
 );

CREATE STREAM OracleToKudu_ExpStore_CDCStreams OF OracleToKudu_ExpStore_CDCStreams_Type;

CREATE CQ OracleToKudu_ReadFromExpStore
INSERT INTO OracleToKudu_ExpStore_CDCStreams
select s.exceptionType,s.action,s.appName,s.entityType,s.entityName,s.className,s.message from admin.OracleToKudu_ExceptionStore [jumping 10 second] s;
        
CREATE OR REPLACE TARGET OracleToKudu_ExpStore_WriteToFileAsJSON USING FileWriter  ( 
  filename: 'expEvent_Kudu.log',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:2,interval:30',
  adapterName: 'FileWriter',
  directory: 'ExpStore_logs',
  rolloverpolicy: 'eventcount:2,interval:30s'
 ) 
FORMAT USING JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter',
  formatterName: 'JSONFormatter',
  jsonMemberDelimiter: '\n',
  jsonobjectdelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true'
 ) 
INPUT FROM OracleToKudu_ExpStore_CDCStreams;
        
END APPLICATION OracleToKudu_ExpStore;

deploy application OracleToKudu_ExpStore;
start OracleToKudu_ExpStore;

STOP APPLICATION TCPReader.TCPAPP;
UNDEPLOY APPLICATION TCPReader.TCPAPP;
DROP APPLICATION TCPReader.TCPAPP cascade;

CREATE APPLICATION TCPAPP;


CREATE SOURCE Tsource USING TCPReader (
@CHARSET@,
IpAddress:'@TCPREADERIPADDR@',
PortNo:'@TCPREADERPORT@'
)
PARSE USING @TSOURCEFORMATTERTYPE@ (
@TSOURCEFORMATTERMEMBERS@
)
OUTPUT TO TCPStream;


END APPLICATION TCPAPP;
deploy application TCPAPP in default;
start application TCPAPP;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

create source @SourceName1@ USING IncrementalBatchReader
(
  FetchSize: 10,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: '@startPosition@',
  PollingInterval: '20sec'
)
OUTPUT TO @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:@targetsys@) input from @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
  ConnectionURL:'@READER-URL@',
  Username:'@READER-UNAME@',
  Password:'@READER-PASSWORD@',
  BatchPolicy:'Eventcount:1,Interval:1',
  CommitPolicy:'Eventcount:1,Interval:1',
  Checkpointtable:'RGRN_CHKPOINT',
  Tables:'@WATABLES@,@WATABLES@_target'
) INPUT FROM @SRCINPUTSTREAM@;

create source @SourceName2@ USING IncrementalBatchReader
(
FetchSize: 10,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn1@,
  startPosition: '@startPosition1@',
  PollingInterval: '20sec'
)
OUTPUT TO @SRCINPUTSTREAM1@;

create Target @targetsys1@ using SysOut(name:@targetsys1@) input from @SRCINPUTSTREAM1@;

CREATE TARGET @targetName1@ USING DatabaseWriter(
  ConnectionURL:'@READER-URL@',
  Username:'@READER-UNAME@',
  Password:'@READER-PASSWORD@',
  BatchPolicy:'Eventcount:1,Interval:1',
  CommitPolicy:'Eventcount:1,Interval:1',
  Checkpointtable:'RGRN_CHKPOINT',
  Tables:'@WATABLES_target'
) INPUT FROM @SRCINPUTSTREAM1@;

END APPLICATION @APPNAME@;

DEPLOY APPLICATION @APPNAME@;
start application @APPNAME@;

drop namespace stripe cascade force;
create namespace stripe;
use stripe;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ RECOVERY 30 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;

CREATE FLOW @AppName@_SourceFlow;

CREATE SOURCE @srcName@ USING StripeReader ( 
 PollingInterval: '5m', 
  AccountId: '', 
  ApiKey: '@srcurl@', 
  StartPosition: '%=-1', 
  ThreadPoolCount: '10', 
  ConnectionPoolSize: '20', 
  RefreshToken: '', 
  useConnectionProfile: false,
  Mode: 'Automated', 
  IncrementalLoadMarker: 'Created', 
  ApiKey_encrypted: 'false', 
  ConnectedAccount: 'false', 
  ClientSecret: '', 
  ClientId: '', 
  Tables: 'Charges', 
  AuthMode: 'ApiKey', 
  MigrateSchema: true ) 
  OUTPUT TO @outstreamname@;

END FLOW @AppName@_SourceFlow;

CREATE TARGET Tgtstripebigquerysanity USING Global.BigQueryWriter ( 
  projectId: '@projectId@',
  batchPolicy: 'eventcount:10000,interval:2', 
  streamingUpload: 'true', 
  Mode: 'MERGE', 
  CDDLOptions: '{\"CreateTable\":{\"action\":\"IgnoreIfExists\",\"options\":[{\"CreateSchema\":{\"action\":\"IgnoreIfExists\"}}]}}', 
  ServiceAccountKey: 'UploadedFiles/google-gcs-test.json', 
  Tables: '%,rishi.%' ) 
INPUT FROM @outstreamname@;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP WStester.WSusingApp;
UNDEPLOY APPLICATION WStester.WSusingApp;
DROP APPLICATION WStester.WSusingApp CASCADE;

CREATE APPLICATION WSusingApp;

CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity int,
  size int);

CREATE SOURCE liveSource using StreamReader(
  OutputType: 'WStester.Atm',
  noLimit: 'false',
  maxRows: 20,
  iterations: 0,
  iterationDelay: 100,
  StringSet: 'productID[001-002-003-004],stateID[AS-CA-WA-NY]',
  NumberSet: 'productWeight[3-3]R,quantity[20-20]R,size[250-250]R'
  )OUTPUT TO CsvStream;


CREATE STREAM newStream OF Atm;

CREATE CQ newCQ
INSERT INTO newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_INT(data[3]), TO_INT(data[4]) FROM
CsvStream;

create target myOut8 using SysOut(name : testOut8) input from newStream;

CREATE WACTIONSTORE streamActivityMEM CONTEXT OF Atm
EVENT TYPES ( Atm )
USING MEMORY;

CREATE WACTIONSTORE streamActivityES CONTEXT OF Atm
EVENT TYPES ( Atm );

CREATE WACTIONSTORE streamActivityES2 CONTEXT OF Atm
EVENT TYPES ( Atm )
USING ( storageProvider: 'elasticsearch' );

CREATE WACTIONSTORE streamActivityDerby CONTEXT OF Atm
EVENT TYPES ( Atm )
USING (
storageProvider:'jdbc',
persistence_interval:'10 sec',
JDBC_DRIVER:'@WASTORE-DRIVER@',
JDBC_URL:'@WASTORE-URL@',
JDBC_USER:'@WASTORE-UNAME@',
JDBC_PASSWORD:'@WASTORE-PASSWORD@',
DDL_GENERATION:'drop-and-create-tables',
LOGGING_LEVEL:'SEVERE');

CREATE CQ newCQ2
INSERT INTO streamActivityMEM
SELECT * FROM newStream
link source event;

CREATE CQ newCQ3
INSERT INTO streamActivityES
SELECT * FROM newStream
link source event;

CREATE CQ newCQ4
INSERT INTO streamActivityES2
SELECT * FROM newStream
link source event;

CREATE CQ newCQ5
INSERT INTO streamActivityDerby
SELECT * FROM newStream
link source event;

END APPLICATION WSusingApp;

STOP APPLICATION TQLwithinTqlTester.TQLwithinTqlApp;
UNDEPLOY APPLICATION TQLwithinTqlTester.TQLwithinTqlApp;
DROP APPLICATION TQLwithinTqlTester.TQLwithinTqlApp CASCADE;

CREATE APPLICATION TQLwithinTqlApp;

@@FEATURE-DIR@/tql/TQLwithinTQL2.tql;
@@FEATURE-DIR@/tql/TQLwithinTQL4.tql;

END APPLICATION TQLwithinTqlApp;

use PosTester;
DROP WACTIONSTORE MerchantActivity;

STOP TestAlertsSMS.TestAlertsSmsApp;
UNDEPLOY APPLICATION TestAlertsSMS.TestAlertsSmsApp;
DROP APPLICATION TestAlertsSMS.TestAlertsSmsApp CASCADE;

CREATE APPLICATION TestAlertsSmsApp;

CREATE source rawSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:No,
  wildcard:'alerts_csv.txt',
  coldelimiter:' ',
  positionByEOF:false
) OUTPUT TO rawStream;

CREATE STREAM MyAlertStream OF Global.AlertEvent;

CREATE CQ GenerateMyAlerts
INSERT INTO MyAlertStream (name, keyVal, severity, flag, message)
SELECT "Testing Alerts", trimStr(data[0]), trimStr(data[1]), trimStr(data[2]), trimStr(data[3])
FROM rawStream s;

CREATE TARGET output2 USING SysOut(name : alertsrecevied) input FROM MyAlertStream;

CREATE SUBSCRIPTION smsAlertSubscription4 USING ClickatellSMSAdapter
(
clickatelluserName:"@Alerts_smsuser@",
CLICKaTELLpasSWORD:"@Alerts_smspassword@",
clickatellapiID:"@Alerts_smsid@",
userIds:"@Alerts_smsuserid@",
threadCount:"@Alerts_smsthreadcount@",
senderNumber:"@Alerts_smssenderno@",
phoneNumberList:"@Alerts_smsphonelist@" ) INPUT FROM MyAlertStream;

END APPLICATION TestAlertsSmsApp;
DEPLOY APPLICATION TestAlertsSMS.TestAlertsSmsApp;
START TestAlertsSMS.TestAlertsSmsApp;

Stop Teradata_LogWriter;
Undeploy application Teradata_LogWriter;
drop application Teradata_LogWriter cascade;

CREATE APPLICATION Teradata_LogWriter WITH ENCRYPTION recovery 5 second interval;

CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.TDSOURCE',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.TEST01=ID;',
  PollingInterval: '5sec',
  ReturnDateTimeAs: 'String',
  startPosition:'striim.test01=0'
  )
  OUTPUT TO data_stream;

  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

CREATE TARGET BinaryDump USING LogWriter(
  name: 'TeraData',
  filename:'TeraData.log'
)INPUT FROM data_stream;

END APPLICATION Teradata_LogWriter;

deploy application Teradata_LogWriter in default;

start application Teradata_LogWriter;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'posdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'JsonTarget',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'FileSizeRollingPolicy,filesize:333M,sequence:00'
)
format using JSONFormatter (
  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)

input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizeBig_actual.log') input from TypedCSVStream;
end application DSV;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;


Create Source @SourceName@ Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'jdbc:oracle:thin:@//192.168.124.25:1522/orcl',
 Tables:'qatest.sourcetable',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;


CREATE TARGET @targetsys@ USING Global.SysOut (
  name: 'sysout' )
INPUT FROM @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
  BatchPolicy: 'EventCount:10000,Interval:30',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:10000,Interval:100',
  StatementCacheSize: '50',
  Password: 'w3b@ct10n',
  Username: 'qatest',
  IgnorableExceptionCode: '547,DUPLICATE_ROW_EXISTS,NO_OP_UPDATE,NO_OP_DELETE,NO_OP_PKUPDATE',
  DatabaseProviderType: 'SQLServer',
  PreserveSourceTransactionBoundary: 'false',
  Tables: 'dbo.sourceTable,dbo.targetTable',
  VendorConfiguration: 'enableIdentityInsert=true',
  adapterName: 'DatabaseWriter' )
INPUT FROM @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;

stop application CDCTester.CDCTest;
undeploy application CDCTester.CDCTest;
drop application CDCTester.CDCTest cascade;

create application CDCTest;

Create Source Rac11g Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:$Tables,
 FetchSize:$FetchSize,
 QueueSize:$QueueSize

)
Output To LCRStream;


end application CDCTest;
deploy application CDCTest;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@;

create TYPE CountTYPE(numcol INT);

CREATE JUMPING WINDOW nEvents OVER @STREAM@ KEEP 10 ROWS;

CREATE STREAM TypedCountStream of CountTYPE;

CREATE CQ CountCQ INSERT INTO TypedCountStream SELECT TO_INT(data[0]) FROM nEvents;

--
-- Crash Recovery Test 3 on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW -> CQ(aggregate) -> WS
--

STOP APPLICATION N4S4CR3Tester.N4S4CRTest3;
UNDEPLOY APPLICATION N4S4CR3Tester.N4S4CRTest3;
DROP APPLICATION N4S4CR3Tester.N4S4CRTest3 CASCADE;
CREATE APPLICATION N4S4CRTest3 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest3;

CREATE SOURCE CsvSourceN4S4CRTest3 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest3;

CREATE FLOW DataProcessingN4S4CRTest3;

CREATE TYPE WactionTypeN4S4CRTest3 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount int,
  city String
);

CREATE STREAM DataStream OF WactionTypeN4S4CRTest3;

CREATE CQ CsvToDataN4S4CRTest3
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_INT(TO_DOUBLE(data[7])),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN4S4CRTest3 CONTEXT OF WactionTypeN4S4CRTest3
EVENT TYPES ( WactionTypeN4S4CRTest3 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN4S4CRTest3
INSERT INTO WactionsN4S4CRTest3
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

END FLOW DataProcessingN4S4CRTest3;

END APPLICATION N4S4CRTest3;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE @SOURCE@ USING MSSQLReader  ( 
  FilterTransactionBoundaries: true,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Username: '@SOURCE_USER@',
  DatabaseName: 'qatest',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@ USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;


END APPLICATION @APPNAME@;

deploy application @APPNAME@ on ANY in default;

start application @APPNAME@;

CREATE APPLICATION @AppName@;

CREATE SOURCE @AppName@_FileReaderSource USING FileReader ( 
  wildcard: 'posdata.csv', 
  positionByEOF: false, 
  blocksize: 10240, 
  directory: 'Samples/PosApp/appData' ) 
PARSE USING DSVParser ( 
  trimquote: false, 
  header: 'Yes' ) 
OUTPUT TO @AppName@_CsvStream;

CREATE TYPE n1 (
 c1 java.lang.Integer KEY,
 _plan java.lang.String AS "plan");

CREATE OR REPLACE TARGET @AppName@_NullWriterTrg USING NullWriter ( 
 ) 
INPUT FROM @AppName@_CsvStream;

END APPLICATION @AppName@;
DEPLOY APPLICATION @AppName@;
START APPLICATION @AppName@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
alter APPLICATION @APPNAME@;
CREATE OR REPLACE SOURCE @APPNAME@src USING Global.FileReader 
(
  adapterName:'FileReader',
  rolloverstyle:'Default',
  blocksize:64,
  networkfilesystem:true,
  wildcard:'client_xref*',
  compressiontype:'gzip',
  includesubdirectories:false,
  directory:'@APPNAME@',
  skipbom:false,
  positionbyeof:false
)
PARSE USING Global.DSVParser 
(
  trimwhitespace:false,
  linenumber: '-1',
  columndelimittill: '-1',
  trimquote: true,
  ignoreemptycolumn: false,
  separator: ':',
  parserName: 'DSVParser',
  quoteset: '\"',
  handler:'com.webaction.proc.DSVParser_1_0',
  charset:'UTF-8',
  ignoremultiplerecordbegin:'true',
  ignorerowdelimiterinquote:false,
  columndelimiter:'|',
  blockascompleterecord:false,
  rowdelimiter:'\n',
  nocolumndelimiter:false,
  headerlineno:0,
  header:'true'
)                  
OUTPUT TO @APPNAME@STREAM;
ALTER APPLICATION @APPNAME@ RECOMPILE;
deploy application @APPNAME@;
start @APPNAME@;

stop OracleReaderToDBWriter;
undeploy application OracleReaderToDBWriter;
drop application OracleReaderToDBWriter cascade;

CREATE APPLICATION  OracleReaderToDBWriter RECOVERY 1 MINUTE INTERVAL AUTORESUME MAXRETRIES 2 RETRYINTERVAL 60;

CREATE FLOW Hz_Agent_flow;

Create Source Oraclesrc
 Using OracleReader
(
 Username:'',
 Password:'',
 ConnectionURL:'',
 Tables:'',
 Fetchsize:1
)
Output To OrcStrm;

END FLOW Hz_Agent_flow;


CREATE TARGET OracleTrg USING DatabaseWriter(
ConnectionURL:'',
  Username:'',
  Password:'',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: ''
) 
INPUT FROM OrcStrm;

END APPLICATION OracleReaderToDBWriter;
DEPLOY APPLICATION OracleReaderToDBWriter with Hz_Agent_flow on any in AGENTS;
start application OracleReaderToDBWriter;

CREATE APPLICATION SourceFraudApp;

CREATE SOURCE FraudCsvDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:no,
  wildcard:'fraudPosData.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO FraudCsvStream;

CREATE TARGET FraudSourceDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceFraudAppData') input from FraudCsvStream;

CREATE SOURCE FraudZipLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  header: no,
  columndelimiter: '	',
  positionByEOF:false
) OUTPUT TO FraudCacheSource1;

CREATE TARGET FraudCacheDump1 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceFraudCacheData1') input from FraudCacheSource1;

CREATE SOURCE FraudNameLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'MerchantNames.csv',
  header: no,
  columndelimiter: ',',
  positionByEOF:false
) OUTPUT TO FraudCacheSource2;

CREATE TARGET FraudCacheDump2 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceFraudCacheData2') input from FraudCacheSource2;

CREATE SOURCE FraudCustomerLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'customerdetails.csv',
  header: no,
  columndelimiter: ',',
  positionByEOF:false
) OUTPUT TO FraudCacheSource3;

CREATE TARGET FraudCacheDump3 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceFraudCacheData3') input from FraudCacheSource3;


END APPLICATION SourceFraudApp;

CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (
  ConnectionURL: @sourceURL@,
  Username: '@userName',
  Tables: '@tables@',
  CheckColumn: '@checkColumn',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  Password: '@password',
  Password_encrypted: 'false',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using DatabaseReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using AzureblobWriter(
    accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:7'
)
format using DSVFormatter (
)
input from @STREAM@;

end application @APPNAME@;

STOP ModifyBeforeDataTester.ModifyBeforeData;
UNDEPLOY APPLICATION ModifyBeforeDataTester.ModifyBeforeData;
DROP APPLICATION ModifyBeforeDataTester.ModifyBeforeData CASCADE;

CREATE APPLICATION ModifyBeforeData;

create source GGTrailSource using FileReader (
    directory:'@TEST-DATA-PATH@/OGG/oracle_alltypes_122',
    WildCard:'ld*',
    positionByEOF:false
) parse using GGTrailParser (
    FilterTransactionBoundaries: true,
    metadata:'@TEST-DATA-PATH@/OGG/oracle_alltypes_122/def_oracle_alltypes122.def',
    compression:false
)
OUTPUT TO SourceStream;

CREATE STREAM ModifiedBeforeStream OF Global.WAEvent;
CREATE STREAM ModifiedDataStream OF Global.WAEvent;

CREATE OR REPLACE CQ ModifierBeforeCQ
INSERT INTO ModifiedBeforeStream
SELECT * FROM SourceStream
where not(before is null)
modify(before[1] = data[1].toString().replaceAll(".", "B"));

CREATE OR REPLACE CQ ModifierDataCQ
INSERT INTO ModifiedDataStream
SELECT * FROM SourceStream
where not(before is null)
modify(data[1] = before[1].toString().replaceAll(".", "D"));

-- Generate the final filtered stream to write to the waction store.
CREATE OR REPLACE CQ CopyBeforeCQ
INSERT INTO FilteredBeforeStream
SELECT *
FROM (SELECT TO_STRING(before[1]) as tcolBefore
      FROM ModifiedBeforeStream) as src
where src.tcolBefore like 'BBBBBB%';

CREATE OR REPLACE CQ CopyDataCQ
INSERT INTO FilteredDataStream
SELECT *
FROM (SELECT TO_STRING(data[1]) as tcolData
      FROM ModifiedDataStream) as src
where src.tcolData like "DDDDDD%";

-- Need duplicate types due to a limitation of Waction Stores: two Waction Stores
-- should not share the same type for CONTEXT OF.
CREATE TYPE WactionType1 (
  colTest String KEY
);

CREATE TYPE WactionType2 (
  colTest String KEY
);

CREATE WACTIONSTORE WactionsBefore CONTEXT OF WactionType1
EVENT TYPES ( WactionType1 )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsData CONTEXT OF WactionType2
EVENT TYPES ( WactionType2 )
@PERSIST-TYPE@

CREATE CQ InsertWactions1
INSERT INTO WactionsBefore
SELECT tcolBefore as colTest
FROM FilteredBeforeStream;

CREATE CQ InsertWactions2
INSERT INTO WactionsData
SELECT tcolData as colTest
FROM FilteredDataStream;

END APPLICATION ModifyBeforeData;

--
-- Recovery Test 40 with two sources and two WactionStores. A variety of partitioned windows in between
-- assure that we are testing a complicated recovery scenario.
--
-- NOTE THIS APP IS INCONSISTENT AND NOT COMPATIBLE WITH THE CURRENT VERSION OF RECOVERY BECAUSE IT HAS COMBINING STREAMS
--
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> JWc2 -> JWc5 -> WS1
--   S2 -> JWc2 -> JWc7 -> WS2
--

STOP Recov40Tester.RecovTest40;
UNDEPLOY APPLICATION Recov40Tester.RecovTest40;
DROP APPLICATION Recov40Tester.RecovTest40 CASCADE;
CREATE APPLICATION RecovTest40 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStreamTop OF CsvData;

CREATE CQ Csv1ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ Csv2ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;








CREATE JUMPING WINDOW TopJWc2
OVER DataStreamTop KEEP 2 ROWS;







CREATE STREAM DataStreamLeft OF CsvData;
CREATE STREAM DataStreamRight OF CsvData;

CREATE CQ DataToLeft
INSERT INTO DataStreamLeft
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM TopJWc2 p;

CREATE CQ DataToRight
INSERT INTO DataStreamRight
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM TopJWc2 p;






CREATE JUMPING WINDOW LeftJWc5
OVER DataStreamLeft KEEP 5 ROWS;

CREATE JUMPING WINDOW RightJWc10
OVER DataStreamRight KEEP 10 ROWS;



CREATE WACTIONSTORE WactionsLeft CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsRight CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ ToWactionsLeft
INSERT INTO WactionsLeft
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM LeftJWc5 p;

CREATE CQ ToWactionsRight
INSERT INTO WactionsRight
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM RightJWc10 p;

END APPLICATION RecovTest40;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@ Using Ojet
(
Username:'@OJET-UNAME@',
Password:'@OJET-PASSWORD@',
ConnectionURL:'@OCI-URL@',
Tables:'@SourceTable@',
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
Username:'@UN@',
Password:'@PWD@',
BatchPolicy:'EventCount:1,Interval:1',
Tables: '@Tablemapping@'
)INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

create Application netflow;
create source NetflowSource using UDPReader (
	IpAddress:'127.0.0.1',
	PortNo:'3546'
)
parse using NetflowParser (
)
OUTPUT TO NetflowStream;

create type NetflowV5_Type (
vers int,
sys_uptime long,
unix_sec long,
unix_nsec long,
flow_sequence long,
engine_type string,
engine_id string,

src_ip string,
dst_ip string,
next_hop  string,
input_idx int,
output_idx  int,

flow_pkt_cnt long,
l3_cnt long,
first_sys_uptime long,
last_sys_uptime long,
src_port int,

dst_port int,
unused1 string,
tcp_flg string,
protocol_type string,
tos string,

src_as int,
dst_as int,
src_mask string,
dst_mask string,
unused2 int

);

CREATE STREAM NetflowV5Stream of NetflowV5_Type;

CREATE CQ NetflowCQ
INSERT INTO NetflowV5Stream
SELECT META(x,'version'),META(x,'sys_uptime'),META(x,'unix_sec'),META(x,'unix_nsec'),META(x,'flow_sequence'),META(x,'engine_type').toString(),META(x,'engine_id').toString(),
       VALUE(x,'src_ip'), VALUE(x,'dst_ip'), VALUE(x,'next_hop'), VALUE(x,'input_idx'), VALUE(x,'output_idx'),
       VALUE(x,'flow_pkt_cnt'), VALUE(x,'l3_cnt'), VALUE(x,'first_sys_uptime'), VALUE(x,'last_sys_uptime'), VALUE(x,'src_port'),
       VALUE(x,'dst_port'), VALUE(x,'unused1').toString(), VALUE(x,'tcp_flg').toString(), VALUE(x,'protocol_type').toString(), VALUE(x,'tos').toString(),
       VALUE(x,'src_as'), VALUE(x,'dst_as'), VALUE(x,'src_mask').toString(), VALUE(x,'dst_mask').toString(), VALUE(x,'unused2')
FROM NetflowStream x;

create Target dump1 using LogWriter(name:'NetflowV5',fileName:'@FEATURE-DIR@/logs/netflow.out') input from NetflowV5Stream;
end Application netflow;

stop @appName@;
undeploy application @appName@;
drop application @appName@ cascade;

CREATE APPLICATION @appName@ USE EXCEPTIONSTORE TTL : '7d' AUTORESUME MAXRETRIES 1000 RETRYINTERVAL 60;

CREATE OR REPLACE SOURCE @appName@_Source USING Global.MysqlReader (
  ConnectionURL: '@ConnectionURL@',
  Tables: '@dbTable@',
  Password: '@Password@',
  Username: '@Username@',
  adapterName: 'MysqlReader')
OUTPUT TO @appName@st1;

CREATE CQ @appName@CQ
INSERT INTO @appName@st2
SELECT data[0] as Id,data[1] as Description FROM @appName@st1 s;;

create target @appName@_tgt using NullWriter() input from @appName@st2;

END APPLICATION @appName@;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'smallposdata10rows.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


@@FEATURE-DIR@/tql/TQLwithinTQL3.tql;

--
-- Recovery Test 33 with two sources, two sliding time windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> St1W/p -> CQ1 -> WS
-- S2 -> St2W/p -> CQ2 -> WS
--

STOP KStreamRecov33Tester.KStreamRecovTest33;
UNDEPLOY APPLICATION KStreamRecov33Tester.KStreamRecovTest33;
DROP APPLICATION KStreamRecov33Tester.KStreamRecovTest33 CASCADE;

DROP USER KStreamRecov33Tester;
DROP NAMESPACE KStreamRecov33Tester CASCADE;
CREATE USER KStreamRecov33Tester IDENTIFIED BY KStreamRecov33Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov33Tester;
CONNECT KStreamRecov33Tester KStreamRecov33Tester;

CREATE APPLICATION KStreamRecovTest33 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 1 SECOND
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 2 SECOND
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION KStreamRecovTest33;

create or replace PROPERTYVARIABLE SRC_PASSWORD='@PROP_VAR@';
CREATE APPLICATION @WRITERAPPNAME@ RECOVERY 10 SECOND INTERVAL;
-- USE EXCEPTIONSTORE;

CREATE SOURCE @SOURCE@ USING Ojet
(
FetchSize:1,
Username:'@SOURCE_USER@',
Password:'@pass@',
--Password:'$SRC_PASSWORD',
ConnectionURL:'@CONNECTION_URL@',
Tables:'@SOURCE_TABLE@',
)
OUTPUT TO @STREAM1@;

CREATE OR REPLACE TYPE @TYPE@( 
datae java.util.HashMap , 
TABLE_NAME java.lang.String , 
OPS_NAME java.lang.String , 
DB_TIMESTAMP java.lang.String  ,
COMMITSCN java.lang.String ,
SCN java.lang.String ,
REC_INS_TIME java.lang.String );

CREATE CQ @CQ1@
INSERT INTO @STREAM2@
SELECT  
CASE WHEN (META(c,"OperationName").toString() == "DELETE")
THEN putUserData(c, 'isDelete', 'true') 
ELSE
putUserData(c,'isDelete', 'false')
END
FROM @STREAM1@ c;

CREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;

CREATE OR REPLACE CQ @CQ2@ 
INSERT INTO @STREAM3@
SELECT 
data(e),
META(e,"TableName").toString() as TABLE_NAME,
META(e, "OperationName").toString() as OPS_NAME,
META(e, "TimeStamp").toString() as DB_TIMESTAMP,
META(e,"COMMITSCN").toString() as COMMITSCN ,
META(e,"SCN").toString() as  SCN ,
DNOW().toString() as REC_INS_TIME
FROM @STREAM1@ e;

create Target @TARGET1@ using KafkaWriter VERSION @kafakaversion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC1',
ParallelThreads:'',
PartitionKey:'@metadata(TableName)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from @STREAM1@;

create Target @TARGET2@ using KafkaWriter VERSION @kafakaversion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC2',
ParallelThreads:'2',
PartitionKey:'TABLE_NAME',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from @STREAM3@;

create Target @TARGET3@ using KafkaWriter VERSION @kafakaversion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC3',
ParallelThreads:'',
PartitionKey:'@userdata(isDelete)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest.avsc')
input from @STREAM2@;

end application @WRITERAPPNAME@;

CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafakaversion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC1',
        startOffset:0       
)
PARSE USING DSVParser ()
OUTPUT TO KafkaReaderStream1;

CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
filename:'@READERAPPNAME@_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream1;

CREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafakaversion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC2',
        startOffset:0          
)
PARSE USING JSONParser ()
OUTPUT TO KafkaReaderStream2;

CREATE TARGET kafkaDumpJSON USING FileWriter(
filename:'@READERAPPNAME@_RT_JSON')
FORMAT USING JSONFormatter()
INPUT FROM KafkaReaderStream2;

CREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafakaversion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC3',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest.avsc'
)
OUTPUT TO KafkaReaderStream3;

CREATE CQ CQAvro_Json 
INSERT INTO Avro_Json  
SELECT AvroToJSON(u.data) FROM KafkaReaderStream3 u;;

CREATE TARGET kafkaDumpAVRO USING FileWriter(
filename:'@READERAPPNAME@_RT_AVRO')
FORMAT USING Global.JSONFormatter  ()
INPUT FROM Avro_Json;

end application @READERAPPNAME@;

stop application app2;
undeploy application app2;
alter application app2;
CREATE or replace TARGET app2_targetNew USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS1_Alter'
) INPUT FROM sourcestream;
alter application app2 recompile;
deploy application app2;

stop application app3;
undeploy application app3;
alter application app3;

CREATE or replace TARGET app3_targetNew USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test02,QATEST.KPS2_Alter'
) INPUT FROM sourcestream;
alter application app3 recompile;
deploy application app3;


stop application app4;
undeploy application app4;
alter application app4;
CREATE TARGET app4_targetNew USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test03,QATEST.KPS3_Alter'
) INPUT FROM sourcestream;
alter application app4 recompile;
deploy application app4;


stop application app5;
undeploy application app5;
alter application app5;
CREATE or replace TARGET app5_target1_New USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'snappy_Alter',
KafkaConfig:'compression.type=snappy'
) 
FORMAT USING DSVFormatter ()
INPUT FROM kps_typedStream;

CREATE or replace TARGET app5_target2_New USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'gzip_Alter',
KafkaConfig:'compression.type=gzip'
) 
FORMAT USING DSVFormatter ()
INPUT FROM sourcestream;

CREATE or replace TARGET app5_target3_New USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'lz4_Alter',
KafkaConfig:'compression.type=lz4'
) 
FORMAT USING DSVFormatter ()
INPUT FROM sourcestream;

alter application app5 recompile;
deploy application app5;

stop application reconnect;
undeploy application reconnect;
drop application reconnect cascade;
CREATE APPLICATION reconnect recovery 1 second interval;

CREATE  SOURCE mssqlsource USING MssqlReader  ( 
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  ConnectionURL: '@URL@',
  Tables: '@TABLE@',
  FetchSize: 1
 ) 
OUTPUT TO sqlstream;

CREATE TARGET dbtarget USING DatabaseWriter(
  ConnectionURL:'@URL@',
  Username:'@USERNAME@',
  Password:'@PASSWORD@',
  ConnectionRetryPolicy: 'retryInterval=15s,maxRetries=2',
  BatchPolicy:'EventCount:5,Interval:30',
  CommitPolicy:'EventCount:5,Interval:30',
  Tables: '@TABLES@'
 ) INPUT FROM sqlstream;

 create Target tSysOut using Sysout(name:OrgData) input from sqlstream;
 end application reconnect;
 deploy application reconnect;
 start application reconnect;

--
-- Recovery Test 1
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov1Tester.RecovTest1;
UNDEPLOY APPLICATION Recov1Tester.RecovTest1;
DROP APPLICATION Recov1Tester.RecovTest1 CASCADE;
CREATE APPLICATION RecovTest1 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END APPLICATION RecovTest1;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE OR REPLACE TYPE @appname@CQOUT1_Type (
 companyName java.lang.String,
 merchantId java.lang.String,
 dateTime org.joda.time.DateTime,
 hourValue java.lang.String,
 amount java.lang.String,
 zip java.lang.String,
 FileName java.lang.String);

CREATE SOURCE @parquetsrc@ USING S3Reader (
    bucketname:'',
    objectnameprefix:''
     )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE OR REPLACE STREAM @appname@CQOUT1 OF @appname@CQOUT1_Type;
CREATE OR REPLACE CQ @appname@CQ_PQEvent
INSERT INTO @appname@CQOUT1
    Select
    data.get("companyName").toString(),
    data.get("merchantId").toString(),
    TO_DATE(data.get("dateTime").toString()),
    data.get("hourValue").toString(),
    data.get("amount").toString(),
    data.get("zip").toString(),
    metadata.get("FileName").toString()
    FROM @appname@Stream p;

CREATE OR REPLACE TARGET @avrotarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING AvroFormatter  (
schemaFileName: 'AvroFileSchema'
)
INPUT FROM @appname@CQOUT1;

create Target @jsontarget@ using FileWriter(
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
format using JSONFormatter ()
INPUT FROM @appname@CQOUT1;

create Target @xmltarget@ using FileWriter(
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
format using XMLFormatter (
    rootelement:'',
    elementtuple:'',
    charset:'UTF-8'
)
INPUT FROM @appname@CQOUT1;

create Target @dsvtarget@ using FileWriter(
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
format using DSVFormatter ()
INPUT FROM @appname@CQOUT1;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

use admin;
drop namespace @namespace@ cascade;
create namespace @namespace@;
use @namespace@;
CREATE APPLICATION @appName@;
create flow @flowName@;
CREATE SOURCE @appName@_s1 USING Global.FileReader (
  wildcard: 'posdata100.csv',
  blocksize: 64,
  directory: '@TestDataDir@',
  positionByEOF:false)
PARSE USING Global.DSVParser ()
OUTPUT TO @appName@_st1;
end flow srcFlow;
create target @appName@_t1 using NullWriter() input from @appName@_st1;
END APPLICATION @appName@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src1 USING Global.GCSReader ()
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream1;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer ()
FORMAT USING DSVFormatter (
members:'data')
INPUT FROM @APPNAME@_Stream1;

CREATE OR REPLACE SOURCE @APPNAME@_src2 USING Global.GCSReader ()
PARSE USING Global.JSONParser ()
OUTPUT TO @APPNAME@_Stream2;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_Stream2;

CREATE OR REPLACE SOURCE @APPNAME@_src3 USING GCSReader ()
PARSE USING AvroParser ()
OUTPUT TO @APPNAME@_Stream3;

CREATE CQ @APPNAME@_CQ3
INSERT INTO @APPNAME@_CQOut3
SELECT AvroToJson(data,false) FROM @APPNAME@_Stream3;

CREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_CQOut3;

CREATE OR REPLACE SOURCE @APPNAME@_src4 USING Global.GCSReader ()
PARSE USING Global.XMLParser ()
OUTPUT TO @APPNAME@_Stream4;

CREATE OR REPLACE TARGET @APPNAME@_trgt4 USING S3Writer ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_Stream4;

END APPLICATION @APPNAME@;

--
-- Recovery Test 37 with two sources, two jumping time windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jt1W/p -> CQ1 -> WS
--   S2 -> Jt2W/p -> CQ2 -> WS
--

STOP KStreamRecov37Tester.KStreamRecovTest37;
UNDEPLOY APPLICATION KStreamRecov37Tester.KStreamRecovTest37;
DROP APPLICATION KStreamRecov37Tester.KStreamRecovTest37 CASCADE;

DROP USER KStreamRecov37Tester;
DROP NAMESPACE KStreamRecov37Tester CASCADE;
CREATE USER KStreamRecov37Tester IDENTIFIED BY KStreamRecov37Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov37Tester;
CONNECT KStreamRecov37Tester KStreamRecov37Tester;

CREATE APPLICATION KStreamRecovTest37 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 1 SECOND
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 2 SECOND
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION KStreamRecovTest37;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@DataSrc USING OracleReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
)
OUTPUT TO @APPNAME@DataStream;

CREATE OR REPLACE TARGET @APPNAME@DataTrgt USING MongoDBWriter (
  ConnectionURL: '',
  Username: '',
  Password: '',
  collections: ''
  AuthDB: '',
  batchpolicy: 'EventCount:1000, Interval:30',
 )
INPUT FROM @APPNAME@DataStream;

CREATE OR REPLACE SOURCE @APPNAME@_src USING MongoDBReader (
  ConnectionURL: '',
  Username: '',
  password: '',
  authDB: '',
  collections: '',
  mode: 'Incremental'
  )
OUTPUT TO @APPNAME@stream;

CREATE CQ @APPNAME@CQ
INSERT INTO @APPNAME@CQSTREAM
SELECT data.get("NUM_COL").toString() AS NUM_COL,
  data.get("CHAR_COL").toString() AS CHAR_COL,
  data.get("VARCHAR2_COL").toString() AS VARCHAR2_COL,
  data.get("FLOAT_COL").toString() AS FLOAT_COL,
  data.get("BINARY_FLOAT_COL").toString() AS BINARY_FLOAT_COL,
  data.get("BINARY_DOUBLE_COL").toString() AS BINARY_DOUBLE_COL,
  data.get("DATE_COL").toString() AS DATE_COL,
  data.get("TIMESTAMP_COL").toString() AS TIMESTAMP_COL
FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING AvroFormatter (
schemaFileName: '@SCHEMAFILE@'
)
INPUT FROM @APPNAME@CQSTREAM;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING DSVFormatter ()
INPUT FROM @APPNAME@CQSTREAM;

END APPLICATION @APPNAME@;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;

 CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01;striim.test02;striim.test03',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream ;

  CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

create target AzureSQLDWHTarget using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream;


CREATE TARGET log USING LOGWRITER
(
 name:log,
 filename:'LOGFILENAME'
)
INPUT FROM data_stream;


END APPLICATION IR;

deploy application IR;
start IR;

stop application CDCTester.CDCTest;
undeploy application CDCTester.CDCTest;
drop application CDCTester.CDCTest cascade;

create application CDCTest;

Create Source Rac11g Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'QATEST.SAMPLETEST2',
 FetchSize:1,
 QueueSize:2048
)
Output To LCRStream;


end application CDCTest;
deploy application CDCTest;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @srcName@ USING OracleReader (
  Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@'
)
OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@ USING Global.SpannerWriter ( 
   Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
  InstanceID: '@instanceId@', 
  ServiceAccountKey: '@keyFileName@', 
  CheckpointTable: 'CHKPOINT', 
  ProjectId: '@projectId@', 
  PrivateServiceConnectEndpoint: '@privatelinkname@', 
  BatchPolicy: 'EventCount: 1, Interval: 60s') 
INPUT FROM @instreamname@;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

--
-- Crash Recovery Test 4 on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS
-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS
--

STOP APPLICATION N4S4CR4Tester.N4S4CRTest4;
UNDEPLOY APPLICATION N4S4CR4Tester.N4S4CRTest4;
DROP APPLICATION N4S4CR4Tester.N4S4CRTest4 CASCADE;
CREATE APPLICATION N4S4CRTest4 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest4;

CREATE SOURCE CsvSourceN4S4CRTest4 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest4;

CREATE FLOW DataProcessingN4S4CRTest4;

CREATE TYPE CsvDataN4S4CRTest4 (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionDataN4S4CRTest4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream OF CsvDataN4S4CRTest4;

CREATE CQ CsvToDataN4S4CRTest4
INSERT INTO DataStream
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN4S4CRTest4 CONTEXT OF WactionDataN4S4CRTest4
EVENT TYPES ( CsvDataN4S4CRTest4 )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO WactionsN4S4CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO WactionsN4S4CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END FLOW DataProcessingN4S4CRTest4;

END APPLICATION N4S4CRTest4;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING DatabaseReader  (
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  Tables: @SOURCE_TABLE@,
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true'
 )
OUTPUT TO @STREAM@;

CREATE ROUTER @SOURCE_NAME@_ROUTER INPUT FROM @STREAM@ cs CASE
WHEN meta(cs,"OperationName").toString()='SELECT' THEN ROUTE TO @STREAM@1, ELSE ROUTE TO @STREAM@2;

CREATE OR REPLACE APPLICATION @AppName@;

Create Source @AppName@_source Using OracleReader(
  Username:'@username@',
  Password:'@password@',
  ConnectionURL:'@url@',
  Tables:'@srctableName@',
  Fetchsize:1
)
Output To @AppName@_Stream;


CREATE OR REPLACE TARGET @AppName@_DB_Target USING Global.DeltaLakeWriter (
connectionProfileName: 'admin.@DBCP@',
   useConnectionProfile: 'true',
  Tables: '@srctableName@,@trgtableName@',
  uploadPolicy: 'eventcount:100000,interval:60s'
)

INPUT FROM @AppName@_Stream;
END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

stop application Postgres_To_PostgresApp;
undeploy application Postgres_To_PostgresApp;
drop application Postgres_To_PostgresApp cascade;

CREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE Postgres_src USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 ) 
OUTPUT TO Change_Data_Stream ;


CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET Postgres_tgt USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:100,Interval:60',
CommitPolicy: 'EventCount:100,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

end application Postgres_To_PostgresApp;
deploy application Postgres_To_PostgresApp;
start Postgres_To_PostgresApp;

--
-- Recovery Test 4
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS
-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS
--

STOP KStreamRecov4Tester.KStreamRecovTest4;
UNDEPLOY APPLICATION KStreamRecov4Tester.KStreamRecovTest4;
DROP APPLICATION KStreamRecov4Tester.KStreamRecovTest4 CASCADE;
DROP USER KStreamRecov4Tester;
DROP NAMESPACE KStreamRecov4Tester CASCADE;
CREATE USER KStreamRecov4Tester IDENTIFIED BY KStreamRecov4Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov4Tester;
CONNECT KStreamRecov4Tester KStreamRecov4Tester;

CREATE APPLICATION KStreamRecovTest4 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream OF CsvData;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION KStreamRecovTest4;

--
-- Crash Recovery Test 4 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS
-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS
--

STOP APPLICATION N2S2CR4Tester.N2S2CRTest4;
UNDEPLOY APPLICATION N2S2CR4Tester.N2S2CRTest4;
DROP APPLICATION N2S2CR4Tester.N2S2CRTest4 CASCADE;
CREATE APPLICATION N2S2CRTest4 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest4;

CREATE SOURCE CsvSourceN2S2CRTest4 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN2S2CRTest4;

CREATE FLOW DataProcessingN2S2CRTest4;

CREATE TYPE CsvDataN2S2CRTest4 (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionTypeN2S2CRTest4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream OF CsvDataN2S2CRTest4;

CREATE CQ CsvToDataN2S2CRTest4
INSERT INTO DataStream
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN2S2CRTest4 CONTEXT OF WactionTypeN2S2CRTest4
EVENT TYPES ( CsvDataN2S2CRTest4 )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO WactionsN2S2CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO WactionsN2S2CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END FLOW DataProcessingN2S2CRTest4;

END APPLICATION N2S2CRTest4;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadpolicy:'EventCount:7'
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from @STREAM@;

end flow @APPNAME@_serverflow;

end application @APPNAME@;

STOP APPLICATION testApp;
UNDEPLOY APPLICATION testApp;
DROP APPLICATION testApp CASCADE;
-- DROP EXCEPTIONSTORE testApp_exceptionstore;

CREATE APPLICATION testApp WITH ENCRYPTION RECOVERY 10 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE OR REPLACE SOURCE testApp_Source USING OracleReader  (
  Username:'qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
  OnlineCatalog:true,
  FetchSize:'1',
  Tables: 'QATEST.srctb'
  ) OUTPUT TO testApp_Stream  ;

CREATE OR REPLACE TARGET testApp_Target USING BigQueryWriter  (
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  projectId:'striimqa-214712',
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:5,Interval:120',
  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', 
  AllowQuotedNewLines: 'false', 
  adapterName: 'BigQueryWriter', 
  optimizedMerge: 'true', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', 
  StandardSQL: 'true', 
  QuoteCharacter: '\"', 
  Tables: 'QATEST.srctb,.oratobqtgt',
  Mode: 'MERGE',
  StandardSQL: 'true',
  _h_TransportOptions:'connectionTimeout=30s, readTimeout=12s',
  QuoteCharacter: '\"'
  ) INPUT FROM testApp_Stream;

CREATE OR REPLACE TARGET testApp_SysOut USING Global.SysOut (name: 'wa') INPUT FROM testApp_Stream;

END APPLICATION testApp;
DEPLOY APPLICATION testApp;
START testApp;

--
-- Recovery Test 33 with two sources, two sliding time windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> St1W/p -> CQ1 -> WS
-- S2 -> St2W/p -> CQ2 -> WS
--

STOP Recov33Tester.RecovTest33;
UNDEPLOY APPLICATION Recov33Tester.RecovTest33;
DROP APPLICATION Recov33Tester.RecovTest33 CASCADE;
CREATE APPLICATION RecovTest33 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 1 SECOND
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 2 SECOND
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION RecovTest33;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@
 Using Ojet
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@'
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE APPLICATION @APPNAME@ RECOVERY 10 SECOND INTERVAL;

 create flow Mysqlflow;
CREATE SOURCE MysqlToDBRoutersource USING MysqlReader
(
  Username: '',
  Password: '',
  Tables: '',
  ConnectionURL: '',
  Password_encrypted: 'false',
  connectionRetryPolicy: 'retryInterval=30, maxRetries=3'
)
OUTPUT TO RouterTestMasterStream;

end flow Mysqlflow;

CREATE OR REPLACE ROUTER RouterTestRs1 INPUT FROM RouterTestMasterStream s CASE
WHEN meta(s,"TableName").toString()='waction.source1' THEN ROUTE TO RouterTestTyped1,
WHEN meta(s,"TableName").toString()='waction.source2' THEN ROUTE TO RouterTestTyped2,
ELSE ROUTE TO RouterTestTypedElse;

CREATE TARGET MysqlToDBRoutertarget1 USING DatabaseWriter(
   Username: '',
   Password: '',
   Tables: '',
   ConnectionURL: '',
   Password_encrypted: 'false',
   connectionRetryPolicy: 'retryInterval=30, maxRetries=3'
) INPUT FROM RouterTestTyped1;


CREATE TARGET MysqlToDBRoutertarget2 USING DatabaseWriter(
    Username: '',
    Password: '',
    Tables: '',
    ConnectionURL: '',
    Password_encrypted: 'false',
    connectionRetryPolicy: 'retryInterval=30, maxRetries=3'
) INPUT FROM RouterTestTyped2;


end application @APPNAME@;
deploy application @APPNAME@ with Mysqlflow in AGENTs;
start application @APPNAME@;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 5 second interval;
create source CSVSource using FileReader (
	directory:'@DIR@',
	WildCard:'@WILDCARD@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target BlobT using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:50'
)
format using DSVFormatter (
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

STOP APPLICATION EH;
UNDEPLOY APPLICATION EH;
DROP APPLICATION EH CASCADE;
CREATE APPLICATION EH @Recovery@;
CREATE SOURCE s USING FileReader (
	directory:'Product/IntegrationTests/TestData/',
    WildCard:'posdata.csv',
	positionByEOF:false
	)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO ss;

CREATE TYPE userType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String
);

CREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;

CREATE CQ cq1
INSERT INTO userDefinedTypedStream
SELECT data[0],data[1],data[2], data[3],data[4]
FROM ss;

create Target t1 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	Partitionkey:'@metadata(RecordOffset)',
	--ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using xmlFormatter(
rootelement:'data')
input from ss;

create Target t2 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	Partitionkey:'Col1',
	--ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using xmlFormatter(
    rootelement:'document',
	elementtuple:'Col1:Col2:text=Col1'
)
input from userDefinedTypedStream;

END APPLICATION EH;
DEPLOY APPLICATION EH;
start application EH;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;


create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;

deploy application DBRTOCW on ANY in default;

start application DBRTOCW;

--
-- Recovery Test 7
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov7Tester.RecovTest7;
UNDEPLOY APPLICATION Recov7Tester.RecovTest7;
DROP APPLICATION Recov7Tester.RecovTest7 CASCADE;
CREATE APPLICATION RecovTest7 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM wacStream OF WactionType;

CREATE CQ InsertWactions
INSERT INTO wacStream
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE WINDOW waWindow
OVER wacStream KEEP WITHIN 1 SECOND ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT *
FROM waWindow
LINK SOURCE EVENT;

END APPLICATION RecovTest7;

CREATE APPLICATION BankDataApp;

CREATE TYPE MoreBankData (
  bankID java.lang.Integer KEY,
  bankName java.lang.String,
  bankRouting java.lang.Long,
  bankAmount java.lang.Double
);

CREATE STREAM BankDataStream OF MoreBankData;

CREATE TARGET SysOut USING Global.SysOut ( 
  name: 'SysOut'
) 
INPUT FROM BankDataStream;

END APPLICATION BankDataApp;

DEPLOY APPLICATION BankDataApp;
START BankDataApp;

--
-- Kafka Stream with KryoParser Kafka Reader without Recovery Test 2 
-- Bert Hashemi and Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> KS -> WS
-- S -> K -> CQ -> WS

STOP KStreamKryoParserTester2WOR.KStreamKryoParserTest2WOR;
UNDEPLOY APPLICATION KStreamKryoParserTester2WOR.KStreamKryoParserTest2WOR;
DROP APPLICATION KStreamKryoParserTester2WOR.KStreamKryoParserTest2WOR CASCADE;
DROP USER KStreamKryoParserTester2WOR;
DROP NAMESPACE KStreamKryoParserTester2WOR CASCADE;
CREATE USER KStreamKryoParserTester2WOR IDENTIFIED BY KStreamKryoParserTester2WOR;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamKryoParserTester2WOR;
CONNECT KStreamKryoParserTester2WOR KStreamKryoParserTester2WOR;

-- CREATE APPLICATION KStreamKryoParserTest2WOR RECOVERY 5 SECOND INTERVAL;
CREATE APPLICATION KStreamKryoParserTest2WOR;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'1');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE or REPLACE TYPE KafkaCsvStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF KafkaCsvStreamType 
EVENT TYPES ( KafkaCsvStreamType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE or REPLACE TYPE KafkaStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

--CREATE STREAM KafkaStream OF KafkaStreamType;
CREATE STREAM KafkaStream OF Global.waevent;

CREATE SOURCE KafkaSource USING KafkaReader
(
        brokerAddress:'localhost:9092',
        Topic:'KStreamKryoParserTester2WOR_KafkaCsvStream',
        PartitionIDList:'0',
        startOffset:0
)
PARSE USING KryoParser ()
OUTPUT TO KafkaStream;

CREATE WACTIONSTORE KRWactions CONTEXT OF KafkaStreamType
EVENT TYPES ( KafkaStreamType )
@PERSIST-TYPE@

CREATE CQ KRInsertWactions
INSERT INTO KRWactions
SELECT TO_STRING(data[1]) as merchantId,
    TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
    TO_DOUBLE(data[7]) as amount,
    TO_STRING(data[10]) as city 
FROM KafkaStream;

END APPLICATION KStreamKryoParserTest2WOR;

stop application dev15823;
undeploy application dev15823;
drop application dev15823 cascade;
CREATE APPLICATION dev15823 RECOVERY 1 SECOND INTERVAL;

CREATE  SOURCE OracleSource USING OracleReader  (
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  ConnectionURL: '@URL@',
  Tables: '@source-table@',
  FetchSize: 1
 )
OUTPUT TO LogminerStream;

--Create or replace Target test using SysOut (name:test) input from MySQLTestStream;

CREATE OR REPLACE TARGET WriteCDCMySQL USING DatabaseWriter  (
  Username: '@USERNAME@',
  BatchPolicy: 'Eventcount:5,Interval:300',
  CommitPolicy: 'Eventcount:5,Interval:300',
  ConnectionURL: '@URL@',
  Tables: '@TABLES@',
  Checkpointtable: 'CHKPOINT',
  Password: '@PASSWORD@'
 )
INPUT FROM LogminerStream;

END APPLICATION dev15823;
deploy application dev15823;
start dev15823;

CREATE APPLICATION @AppName@;

CREATE OR REPLACE SOURCE ReadHTTPPort USING Global.HTTPReader  ( 
deferresponsetimeout: '10000ms', 
  keystore: '@dataFile@server.keystore.jks', 
  deferresponse: true, 
  keystorepassword: 'w@ct10n', 
  ipaddress: 'localhost', 
  threadcount: 10, 
  keystorepassword_encrypted: 'false', 
  adapterName: 'HTTPReader', 
  keystoretype: 'JKS', 
  portno: '@port@', 
  authenticateclient: true ) 
OUTPUT TO HTTPReaderOUT  ;

CREATE OR REPLACE TARGET sysout USING Global.SysOut  ( 
name: 'sysout' ) 
INPUT FROM HTTPReaderOUT;

END APPLICATION @AppName@;
deploy application @AppName@;
Start @AppName@;

--
-- Canon Test W70
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for a partitioned jumping count window
--
-- S -> JWc5p -> CQ -> WS
--


UNDEPLOY APPLICATION NameW70.W70;
DROP APPLICATION NameW70.W70 CASCADE;
CREATE APPLICATION W70 RECOVERY 5 SECOND INTERVAL;




CREATE FLOW DataAcquisitionW70;


CREATE SOURCE CsvSourceW70 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW70;


END FLOW DataAcquisitionW70;




CREATE FLOW DataProcessingW70;

CREATE TYPE DataTypeW70 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW70 OF DataTypeW70;

CREATE CQ CSVStreamW70_to_DataStreamW70
INSERT INTO DataStreamW70
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW70;

CREATE JUMPING WINDOW JWc5pW70
OVER DataStreamW70
KEEP 5 ROWS
PARTITION BY word;

CREATE WACTIONSTORE WactionStoreW70 CONTEXT OF DataTypeW70
EVENT TYPES ( DataTypeW70 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWc5pW70_to_WactionStoreW70
INSERT INTO WactionStoreW70
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc5pW70
GROUP BY word;

END FLOW DataProcessingW70;



END APPLICATION W70;

--
-- Recovery Test 5 with Jumping window and partitioned
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP KStreamRecov5Tester.KStreamRecovTest5;
UNDEPLOY APPLICATION KStreamRecov5Tester.KStreamRecovTest5;
DROP APPLICATION KStreamRecov5Tester.KStreamRecovTest5 CASCADE;
DROP USER KStreamRecov5Tester;
DROP NAMESPACE KStreamRecov5Tester CASCADE;
CREATE USER KStreamRecov5Tester IDENTIFIED BY KStreamRecov5Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov5Tester;
CONNECT KStreamRecov5Tester KStreamRecov5Tester;

CREATE APPLICATION KStreamRecovTest5 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvData PARTITION BY merchantId;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION KStreamRecovTest5;

CREATE APPLICATION @APPNAME@;

CREATE TYPE @APPNAME@type1 (
 companyName java.lang.String,
 merchantId java.lang.String
 );

CREATE STREAM @APPNAME@TypedStream OF @APPNAME@type1;

CREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (
  wildcard: '',
  positionByEOF: false,
  directory: ''
  )
PARSE USING DSVParser (
header:'true'
)
OUTPUT TO @APPNAME@Stream;

CREATE OR REPLACE CQ @APPNAME@CQ
INSERT INTO @APPNAME@TypedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantId
FROM @APPNAME@Stream;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING XMLFormatter(
  rootelement:'JMSXMLIN',
  elementtuple:'companyName:merchantId:text=companyName'
)
INPUT FROM @APPNAME@TypedStream;

END APPLICATION @APPNAME@;

stop MySQLToAzure;
undeploy application MySQLToAzure;
DROP APPLICATION MySQLToAzure CASCADE;
CREATE APPLICATION MySQLToAzure recovery 5 second interval;;

Create Source MySQLSOURCE Using MySQLReader
(
  Username: '@MYSQL-USERNAME@',
  Password: '@MYSQL-PASSWORD@',
  ConnectionURL: '@MYSQL-URL@',
  Database:'@MYSQL-DATABASE@',
  Tables: '@SOURCE_TABLES@'
) 
Output To str;


CREATE  TARGET t4 USING SysOut  ( 
  name: 'sqltors'
 ) 
INPUT FROM str;

create target MySQLAzureTarget using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

END APPLICATION MySQLToAzure;
deploy application MySQLToAzure;
start application MySQLToAzure;

STOP APPLICATION eh;
UNDEPLOY APPLICATION eh;
DROP APPLICATION eh CASCADE;
CREATE APPLICATION eh @Recovery@;
create flow AgentFlow;
CREATE OR REPLACE SOURCE s USING IncrementalBatchReader  ( 
  FetchSize: 1,
  StartPosition:'QATEST.IBR01=-1;QATEST.IBR02=-1;QATEST.IBR03=0;QATEST.IBR04=0;QATEST.IBR05=1;QATEST.IBR06=2018-09-20 06:43:59;QATEST.%=-1',
  Username: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@//dockerhost:1521/orcl',
  Tables: 'QATEST.IBR%',
  CheckColumn: 'QATEST.IBR01=id;QATEST.IBR03=id;QATEST.IBR05=id;QATEST.%=t1',
  Password: 'qatest' ) 
OUTPUT TO sourcestream ;


CREATE TYPE cdctype(
  id int,
  name String  
);

CREATE STREAM cdctypestream OF cdctype;

CREATE CQ cdcstreamcq
INSERT INTO cdctypestream
SELECT TO_INT(p.data[0]), 
       TO_STRING(p.data[1])
FROM sourcestream p;

end flow AgentFlow;

create flow serverFlow;

create Target t1_dsv using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_01',
	E1P:'true',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
	PartitionKey:'id',
	OperationTimeoutMS:'120000',
    BatchPolicy:'Size:256000,Interval:30s',
    ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
	--ParallelThreads:'2'
)
format using DSVFormatter ( 
)
input from cdctypestream;

create Target t2_json using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_02',
	E1P:'true',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
	PartitionKey:'id',
	OperationTimeoutMS:'140000',
	BatchPolicy:'Size:200000,Interval:1m',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m',
	ConsumerGroup:'reader')
format using JSONFormatter ( 
)
input from cdctypestream;

create Target t3_avro using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_03',
	E1P:'true',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
	PartitionKey:'id',
	OperationTimeoutMS:'140000',
	BatchPolicy:'Size:256000,Interval:1h',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m',
	ConsumerGroup:'reader')
format using AvroFormatter (
schemaFileName:'kafkaAvroTest_Agent_ibr_orcl.avsc'
)input from cdctypestream;
end flow serverFlow;

END APPLICATION eh;
--deploy application eh;
deploy application eh with AgentFlow in Agents, ServerFlow in default;

start application eh;

STOP APPLICATION ER;
UNDEPLOY APPLICATION ER;
DROP APPLICATION ER CASCADE;
CREATE APPLICATION ER;
CREATE STREAM ER_SS2 OF Global.JsonNodeEvent;

CREATE SOURCE ER_S1 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING dsvParser (
)OUTPUT TO ER_SS1;

CREATE SOURCE ER_S2 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING jsonparser (members:'data')
OUTPUT TO ER_SS2;


CREATE SOURCE ER_S3 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING avroParser (
schemaFileName:'kafkaAvroTest_Agent_ibr_orcl.avsc'
)OUTPUT TO ER_SS3;


create Type CustType 
(writerdata com.fasterxml.jackson.databind.JsonNode,
TopicName java.lang.String,
PartitionID java.lang.String);

Create Stream datastream3 of CustType;

CREATE CQ CustCQ3
INSERT INTO datastream3
SELECT AvroToJson(s3.data),
metadata.get("TopicName").toString() AS TopicName,
metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS3 s3;



create Target ER_t1 using FileWriter (
filename:'FT1_5L_AVRO_RT',
directory:'FEATURE-DIR/logs/',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from ER_SS1;

create Target ER_t2 using FileWriter (
filename:'FT2_JSON_RT',
directory:'FEATURE-DIR/logs/',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000'	
)
format using jsonFormatter(members: 'data' )
input from ER_SS2;

create Target ER_t3 using FileWriter (
filename:'FT2_JSON_RT',
directory:'FEATURE-DIR/logs/',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from datastream3;


end application ER;
deploy application ER;

STOP APPLICATION EH;
UNDEPLOY APPLICATION EH;
DROP APPLICATION EH CASCADE;
CREATE APPLICATION EH recovery 5 second interval;
create flow AgentFlow;
CREATE SOURCE EH_SOURCE USING MySQLReader (
  Compression: true,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: '@CONNECTION_URL@',
  DatabaseName: 'waction',
  Tables: 'waction.Test01',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)
OUTPUT TO EH_SS;
end flow AgentFlow;
create flow serverFlow;
create Target EH_TARGET using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_01',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_01_cg',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:1000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter()
input from EH_SS;
end flow serverFlow;

END APPLICATION EH;

--DEPLOY APPLICATION EH;
deploy application eh with AgentFlow in Agents, ServerFlow in default;

start application EH;

STOP APPLICATION ER;
UNDEPLOY APPLICATION ER;
DROP APPLICATION ER CASCADE;
CREATE APPLICATION ER;
CREATE STREAM ER_SS1 OF Global.JsonNodeEvent;

CREATE SOURCE ER_S1 USING AzureEventHubReader (
    EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_01',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
    ConsumerGroup: 'test_01_cg',
	startSeqNo:'0'
	)
PARSE USING jsonparser ()
OUTPUT TO ER_SS1;


create Target ER_t1 using FileWriter (
filename:'MYSQL_EH_ER_JSON_RT',
directory:'FEATURE-DIR/logs/',
rolloverpolicy:'eventcount:5000000'
)
format using jsonFormatter (members:'data')
input from ER_SS1;
end application ER;
deploy application ER;

stop tpcc;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;


Create Source @SourceName@
 Using OracleReader
(
 Username:'@READER-UNAME@',
 Password:'@READER-PASSWORD@',
 ConnectionURL:'@CDC-READER-URL@',
 Tables: '@WATABLES-SRC@',
 FetchSize:1,
 QueueSize:25000,
 CommittedTransactions:true,
 Compression:true,
 CaptureDDL: true,
 SendBeforeImage:true
) Output To @SRCINPUTSTREAM@;


create Target @targetsys@ using SysOut(name:OrgData) input from DataStream;

CREATE TARGET @targetName@ USING databasewriter(
  Username: '@WRITER-UNAME@',
  Password: '@WRITER-PASSWORD@',
  ConnectionURL:'@WRITER-URL@',
  BatchPolicy:'EventCount:1,Interval:1',
  CommitPolicy:'EventCount:1,Interval:1',
  Tables: '@WATABLES-SRC@,@WATABLES-TGT@'
) INPUT FROM @SRCINPUTSTREAM@;


END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src1 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: '@ORACLE-URL@',
  Tables: '@SOURCE-TABLES@',
  Username: '@ORACLE-USERNAME@',
  Password: '@ORACLE-PASSWORD@'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt1 USING SnowflakeWriter

(
  ConnectionURL:'@SNOWFLAKE-URL@',
  username:'@SNOWFLAKE-USERNAME@',
  appendOnly:'false',
  Tables:'@TARGET-TABLES@',
  uploadpolicy:'eventcount:3,interval:10s',
  externalStageType:'local'
)
INPUT FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt2 USING SnowflakeWriter

(
  ConnectionURL:'@SNOWFLAKE-URL@',
  username:'@SNOWFLAKE-USERNAME@',
  appendOnly:'false',
  Tables:'@TARGET-TABLES@',
  uploadpolicy:'eventcount:3,interval:10s',
  externalStageType:'local'
)
INPUT FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt3 USING SnowflakeWriter

(
  ConnectionURL:'@SNOWFLAKE-URL@',
  username:'@SNOWFLAKE-USERNAME@',
  appendOnly:'false',
  Tables:'@TARGET-TABLES@',
  uploadpolicy:'eventcount:3,interval:10s',
  externalStageType:'local'
)
INPUT FROM @APP_NAME@_Stream;


END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;
CREATE  SOURCE @SourceName@ USING MSSqlReader  ( 
  Username: '@Username@',
  Password: '@Password@',
  DatabaseName: 'qatest',
  connectionRetryPolicy: @ConnectionRetryPolicy@,
  ConnectionURL: '@ConnectionURL@',
  Tables: '@SourceTables@',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @SRCINPUTSTREAM@;
create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

CREATE OR REPLACE PROPERTYSET LDAP1 ( PROVIDER_URL:"ldap://10.77.12.210:389", SECURITY_AUTHENTICATION:simple, SECURITY_PRINCIPAL: "praveen,dc=qa,dc=webaction,dc=com" , SECURITY_CREDENTIALS:InvalidPwd, USER_BASE_DN:"dc=qa,dc=webaction,dc=com", User_userId:cn );

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@ RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE @parquetsrc@ USING FileReader (
wildcard: '',
directory: '',
positionbyeof: false )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: ''
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING PostgreSQLReader  (
  ReplicationSlotName: 'Slot_Name',
  FilterTransactionBoundaries: 'true',
  Username: 'User_Name',
  ConnectionURL: 'Connection_URL',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'Password',
  Tables: 'Tables'
 )
OUTPUT TO @STREAM@ ;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 USING PostgreSQLReader  (
  ReplicationSlotName: 'Slot_Name',
  FilterTransactionBoundaries: 'true',
  Username: 'User_Name',
  ConnectionURL: 'Connection_URL',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'Password',
  Tables: 'Tables'
 )
OUTPUT TO @STREAM@ ;

CREATE TARGET @SOURCE_NAME@_sysout USING Global.SysOut (
  name: '@SOURCE_NAME@_sysout' )
INPUT FROM @STREAM@;

CREATE OR REPLACE PROPERTYVARIABLE Mode='sync';
CREATE OR REPLACE PROPERTYVARIABLE BatchPolicy='Size:900000,Interval:1';

create application KinesisTest;
create source CSVSource using FileReader (
	directory:'/home/dz/src/product/Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  companyName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0], data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	BatchPolicy: '$BatchPolicy',
    Mode: '$Mode'	
)
format using JSONFormatter (
)
input from TypedCSVStream;
end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'posdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'TargetPosDataXmlFS',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'FileSizeRollingPolicy,filesize:101M,sequence:00'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:merchantid:text=merchantname'
)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlFS_actual.log') input from TypedCSVStream;

end application DSV;

--
-- Recovery Test 37 with two sources, two jumping time windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jt1W/p -> CQ1 -> WS
--   S2 -> Jt2W/p -> CQ2 -> WS
--

STOP Recov37Tester.RecovTest37;
UNDEPLOY APPLICATION Recov37Tester.RecovTest37;
DROP APPLICATION Recov37Tester.RecovTest37 CASCADE;
CREATE APPLICATION RecovTest37 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 1 SECOND
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 2 SECOND
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION RecovTest37;

CREATE SOURCE @SOURCE_NAME@ USING Global.FileReader (
  positionbyeof: false )
PARSE USING Global.DSVParser (
 )
OUTPUT TO @STREAM@;

CREATE TARGET @SOURCE_NAME@_sysout USING Global.SysOut (
  name: '@SOURCE_NAME@_sysout' )
INPUT FROM @STREAM@;

STOP APPLICATION RouterTester.RouterApp;
UNDEPLOY APPLICATION RouterTester.RouterApp;
DROP APPLICATION RouterTester.RouterApp CASCADE;

--DROP namespace RouterTester;
--CREATE namespace RouterTester;
--USE RouterTester;

CREATE APPLICATION RouterApp RECOVERY 20 SECOND INTERVAL;

CREATE OR REPLACE SOURCE CsvDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'posdata100.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO InputStreamNonTyped;

CREATE TYPE MyDataType (
  colId String,
  colStr String,
  colInt int,
  colFloat float
);

CREATE OR REPLACE STREAM InputStreamTyped of MyDataType;

CREATE OR REPLACE CQ  CreateTypedStreamCQ
INSERT INTO InputStreamTyped (colId, colStr, colInt, colFloat)
SELECT
  TO_STRING(src.data[1]) as colId,
  TO_STRING(src.data[0]) as colStr,
  TO_INT(src.data[3]) as colInt,
  TO_FLOAT(src.data[7]) as colFloat
FROM InputStreamNonTyped src;

-- Router on the stream of Generated type. It tests the predicate with expressions on
-- integer, float and string types.
CREATE OR REPLACE ROUTER TestRouterTyped INPUT FROM InputStreamTyped CASE
WHEN colInt <= 3 and colFloat     < 100  and colStr like 'COMPANY%' THEN ROUTE TO StreamTyped1,
WHEN colInt <= 6 and colFloat * 3 < 200  and colStr like 'COMPANY%' THEN ROUTE TO StreamTyped2,
ELSE ROUTE TO StreamTypedElse;

-- Router on the stream of Non-generated Type. Predicates with expressions over
-- integer and float types.
CREATE OR REPLACE ROUTER TestRouterNonTyped INPUT FROM InputStreamNonTyped CASE
WHEN TO_INT(data[3]) <= 3 and TO_FLOAT(data[7])     < 100 and TO_STRING(data[0]) like 'COMPANY%' THEN ROUTE TO StreamNonTyped1,
WHEN TO_INT(data[3]) <= 6 and TO_FLOAT(data[7]) * 3 < 200 and TO_STRING(data[0]) like 'COMPANY%' THEN ROUTE TO StreamNonTyped2,
ELSE ROUTE TO StreamNonTypedElse;

-- Output Streams to populate the waction stores (for non-typed stream only).
CREATE OR REPLACE STREAM StreamWactions1 of MyDataType;
CREATE OR REPLACE STREAM StreamWactions2 of MyDataType;
CREATE OR REPLACE STREAM StreamWactionsElse of MyDataType;

CREATE OR REPLACE CQ ConvertCQNonTyped1
INSERT INTO StreamWactions1 (colId, colStr, colInt, colFloat)
SELECT
  TO_STRING(src.data[1]) as colId,
  TO_STRING(src.data[0]) as colStr,
  TO_INT(src.data[3]) as colInt,
  TO_FLOAT(src.data[7]) * 3 as colFloat
FROM StreamNonTyped1 src;

CREATE OR REPLACE CQ ConvertCQNonTyped2
INSERT INTO StreamWactions2 (colId, colStr, colInt, colFloat)
SELECT
  TO_STRING(src.data[1]) as colId,
  TO_STRING(src.data[0]) as colStr,
  TO_INT(src.data[3]) as colInt,
  TO_FLOAT(src.data[7]) * 3 as colFloat
FROM StreamNonTyped2 src;

CREATE OR REPLACE CQ ConvertCQNonTypedElse
INSERT INTO StreamWactionsElse (colId, colStr, colInt, colFloat)
SELECT
  TO_STRING(src.data[1]) as colId,
  TO_STRING(src.data[0]) as colStr,
  TO_INT(src.data[3]) as colInt,
  TO_FLOAT(src.data[7]) * 3 as colFloat
FROM StreamNonTypedElse src;

CREATE TYPE WactionsTypedType1 (
  colId  String,
  colStr String,
  colInt int,
  colFloat float
);

CREATE TYPE WactionsTypedType2 (
  colId  String,
  colStr String,
  colInt int,
  colFloat float
);

CREATE TYPE WactionsTypedTypeElse (
  colId  String,
  colStr String,
  colInt int,
  colFloat float
);

CREATE TYPE WactionsNonTypedType1 (
  colId  String,
  colStr String,
  colInt int,
  colFloat float
);

CREATE TYPE WactionsNonTypedType2 (
  colId  String,
  colStr String,
  colInt int,
  colFloat float
);

CREATE TYPE WactionsNonTypedTypeElse (
  colId  String,
  colStr String,
  colInt int,
  colFloat float
);


CREATE WACTIONSTORE WactionsTyped1 CONTEXT OF WactionsTypedType1
EVENT TYPES ( WactionsTypedType1 )
@PERSIST-TYPE@;

CREATE WACTIONSTORE WactionsTyped2 CONTEXT OF WactionsTypedType2
EVENT TYPES ( WactionsTypedType2 )
@PERSIST-TYPE@;

CREATE WACTIONSTORE WactionsTypedElse CONTEXT OF WactionsTypedTypeElse
EVENT TYPES ( WactionsTypedTypeElse )
@PERSIST-TYPE@;

CREATE WACTIONSTORE WactionsNonTyped1 CONTEXT OF WactionsNonTypedType1
EVENT TYPES ( WactionsNonTypedType1 )
@PERSIST-TYPE@;

CREATE WACTIONSTORE WactionsNonTyped2 CONTEXT OF WactionsNonTypedType2
EVENT TYPES ( WactionsNonTypedType2 )
@PERSIST-TYPE@;

CREATE WACTIONSTORE WactionsNonTypedElse CONTEXT OF WactionsNonTypedTypeElse
EVENT TYPES ( WactionsNonTypedTypeElse )
@PERSIST-TYPE@;

CREATE CQ InsertWactionsTyped1
INSERT INTO WactionsTyped1
SELECT *
FROM StreamTyped1;

CREATE CQ InsertWactionsTyped2
INSERT INTO WactionsTyped2
SELECT *
FROM StreamTyped2;

CREATE CQ InsertWactionsTypedElse
INSERT INTO WactionsTypedElse
SELECT *
FROM StreamTypedElse;

CREATE CQ InsertWactionsNonTyped1
INSERT INTO WactionsNonTyped1
SELECT *
FROM StreamWactions1;

CREATE CQ InsertWactionsNonTyped2
INSERT INTO WactionsNonTyped2
SELECT *
FROM StreamWactions2;

CREATE CQ InsertWactionsNonTypedElse
INSERT INTO WactionsNonTypedElse
SELECT *
FROM StreamWactionsElse;

END APPLICATION RouterApp;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;

CREATE FLOW OracleTokudu1;
	Create Source oracSource1
	 Using OracleReader
	(
	 Username:'@LOGMINER-UNAME@',
 	Password:'@LOGMINER-PASSWORD@',
 	ConnectionURL:'@LOGMINER-URL@',
 	Tables:'@SOURCE_TABLES@',
 	OnlineCatalog:true,
 	FetchSize:1
	) Output To DataStream1;

	CREATE TARGET WriteintoKudu1 using KuduWriter (
	kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
	pkupdatehandlingmode:'@MODE@',
	tables: '@TARGET_TABLES@',
	batchpolicy: 'EventCount:1,Interval:0')
	INPUT FROM DataStream1;
END FLOW OracleTokudu1;

CREATE FLOW OracleTokudu2;
	Create Source oracSource2
	 Using OracleReader
	(
	 Username:'@LOGMINER-UNAME@',
 	Password:'@LOGMINER-PASSWORD@',
 	ConnectionURL:'@LOGMINER-URL@',
 	Tables:'@SOURCE_TABLES@',
 	OnlineCatalog:true,
 	FetchSize:1
	) Output To DataStream2;

	CREATE TARGET WriteintoKudu2 using KuduWriter (
	kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
	pkupdatehandlingmode:'@MODE@',
	tables: '@TARGET_TABLES@',
	batchpolicy: 'EventCount:10,Interval:10')
	INPUT FROM DataStream2;
END FLOW OracleTokudu2;

CREATE FLOW OracleTokudu3;
	Create Source oracSource3
	 Using OracleReader
	(
	 Username:'@LOGMINER-UNAME@',
 	Password:'@LOGMINER-PASSWORD@',
 	ConnectionURL:'@LOGMINER-URL@',
 	Tables:'@SOURCE_TABLES@',
 	OnlineCatalog:true,
 	FetchSize:1
	) Output To DataStream3;

	CREATE TARGET WriteintoKudu3 using KuduWriter (
	kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
	pkupdatehandlingmode:'@MODE@',
	tables: '@TARGET_TABLES@',
	batchpolicy: 'EventCount:5,Interval:120')
	INPUT FROM DataStream3;
END FLOW OracleTokudu3;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

/* *******************************************************************
 Ibasis App for monitoring network with pings to different locations 
****************************************************************** */

use global;
drop namespace PingMonitorBatchApp cascade;
create namespace PingMonitorBatchApp;
use PingMonitorBatchApp;

CREATE APPLICATION PingMonitorBatchApp;

create	flow PingMonitorBatchAppFlow;	

CREATE TYPE PingDateEntry(
    MaxTestDateTime  DateTime KEY
 );


CREATE TYPE PINGEVENT(
        SERVICE_NAME  String,
        TEST_DATETIME DateTime,
	SOURCE_VERIFIER_ALIAS String,
	TARGET_VERIFIER_ALIAS  String,
        DELAY_AVG float,
	PACKET_LOST float,
        JITTER_AVG float,
        SEQ_NO     long
);


CREATE CACHE PINGEVENTCACHE  using DatabaseReader (
   ConnectionURL:'jdbc:oracle:thin:@server1204v.ivanet.net:1960:prtld',
   Username:'xtract',
   Password:'xtract123',
   Query:'select SERVICE_NAME,TEST_DATETIME,SOURCE_VERIFIER_ALIAS,TARGET_VERIFIER_ALIAS,NVL(DELAY_AVG,0),NVL(PACKET_LOST,0),NVL(JITTER_AVG,0),SEQ_NO  from ir34_ping_site where TEST_DATETIME > (select NVL(to_date(\'1970-01-01\', \'YYYY-MM-DD\') + (MAX(MaxTestDateTime)/ 86400000), sysdate -10/24) from IR34SiteLastDate)order by TEST_DATETIME ASC'
 ) QUERY (keytomap:'SOURCE_VERIFIER_ALIAS', refreshinterval: '60 second', publishonrefresh: 'true') OF PINGEVENT;


create Target trace0 using CSVWriter (filename:'logs/PINGEVENTCACHE.csv') input from PINGEVENTCACHE;

CREATE WACTIONSTORE IR34ElasticSearchWS  
CONTEXT OF   PingDateEntry 
EVENT TYPES ( PingDateEntry  )
PERSIST IMMEDIATE
USING ( storageProvider: "elasticsearch" );


CREATE WACTIONSTORE IR34SiteLastDateWS CONTEXT OF PingDateEntry
 		EVENT TYPES ( PingDateEntry )
 	    PERSIST EVERY 50 second USING (
 	    JDBC_DRIVER:'oracle.jdbc.driver.OracleDriver',
 	    JDBC_URL:'jdbc:oracle:thin:@server1204v.ivanet.net:1960:prtld',
 	    JDBC_USER:'xtract',
 	    JDBC_PASSWORD:'xtract123',
 	    DDL_GENERATION:'create-or-extend-tables',
            CONTEXT_TABLE:'IR34SiteLastDate'
 	   );	
	   

CREATE CQ populateMaxDate
 INSERT INTO IR34SiteLastDateWS
 select TO_DATE(MAX(TO_LONG(TEST_DATETIME)))
 from  PINGEVENTCACHE
 , heartbeat(interval '20' second, 1 , 1) hb;


 
/* create Target trace1 using CSVWriter (filename:'logs/IR34SiteLastDateWS.csv') input from IR34SiteLastDateWS; */


CREATE TYPE DELAYTHRESHTYPE(
        SITE_KEY  STRING,
        SOURCE_SITE STRING,
        TARGET_SITE STRING,
        DELAY_THRESH FLOAT,
        PROFILE_ID   INT
);


CREATE CACHE DELAYTHRESHCACHE  USING DATABASEREADER (
   ConnectionURL:'jdbc:oracle:thin:@timstsnap05.ivanet.net:1521:timsrdg2',
   Username:'network_cfg',
   Password:'network_cfg',
   QUERY: 'select src.site_abbreviation || \'-\' || tgt.site_abbreviation as SITE_KEY, src.site_abbreviation as SOURCE_SITE, tgt.site_abbreviation as TARGET_SITE,D.DELAY as DELAY_THRESH, d.PROFILE_ID from site_attributes src, geography gsrc, threshold_delay d, site_attributes tgt, geography gtgt where  Src.GEO_ABBREVIATION = Gsrc.GEO_ABBREVIATION  and src.GEO_ABBREVIATION = D.SOURCE_VERIFIER_GEO  and tgt.GEO_ABBREVIATION = D.TARGET_VERIFIER_GEO  and tgt.GEO_ABBREVIATION = Gtgt.GEO_ABBREVIATION  and tgt.PROFILE_ID = D.PROFILE_ID  and d.profile_id = 1'
 ) QUERY (KEYTOMAP:'SITE_KEY', REFRESHINTERVAL: '60 SECOND', publishonrefresh: 'true') OF DELAYTHRESHTYPE;


CREATE TARGET TRACE2 USING CSVWRITER (FILENAME:'logs/DELAYTHRESHCACHE.CSV') INPUT FROM DELAYTHRESHCACHE;


CREATE TYPE ALERTENTRY(
	ALERTKEY STRING KEY,
 	TOLOC STRING,
 	FROMLOC STRING,	
        STATUS STRING,
	INTSTATUS INT,
	ACTUAL_DELAY INT,
	DELAY_THRESHOLD INT,
        TESTTIME DATETIME
 );

CREATE STREAM PingStreamRaw OF PINGEVENT;

CREATE CQ PROCESSSortCQ
 INSERT INTO PingStreamRaw
 select  SERVICE_NAME  ,
        TEST_DATETIME ,
        SOURCE_VERIFIER_ALIAS ,
        TARGET_VERIFIER_ALIAS  ,
        DELAY_AVG ,
        PACKET_LOST ,
        JITTER_AVG ,
        SEQ_NO     
from PINGEVENTCACHE,  heartbeat(interval '30' second) hb;

CREATE SORTER PingSorterStream OVER
PingStreamRaw ON TEST_DATETIME OUTPUT TO SortedPingStream
WITHIN 70 second  
OUTPUT ERRORS TO fooErrorStream;


 CREATE STREAM ALERTENTRYSTREAM OF ALERTENTRY;

 CREATE CQ PROCESSCQ1
 INSERT INTO ALERTENTRYSTREAM
 SELECT DW.SOURCE_VERIFIER_ALIAS + " - " + DW.TARGET_VERIFIER_ALIAS,
        DW.TARGET_VERIFIER_ALIAS, 
        DW.SOURCE_VERIFIER_ALIAS,         
        CASE WHEN DW.DELAY_AVG > DT.DELAY_THRESH THEN "WARNING" 
             ELSE "NORMAL" 
        END,
        CASE WHEN DW.DELAY_AVG > DT.DELAY_THRESH  THEN 2
             ELSE 1
        END,
        DW.DELAY_AVG,
        DT.DELAY_THRESH,
        DW.TEST_DATETIME
 FROM SortedPingStream DW  , DELAYTHRESHCACHE DT 
 WHERE DT.SOURCE_SITE = DW.SOURCE_VERIFIER_ALIAS
   AND DT.TARGET_SITE = DW.TARGET_VERIFIER_ALIAS;

CREATE TARGET TRACE4 USING CSVWRITER (FILENAME:'logs/ALERTENTRIES.CSV') INPUT FROM ALERTENTRYSTREAM;

CREATE WINDOW DELAYWARNWINDOW OVER  ALERTENTRYSTREAM KEEP WITHIN 20 minute ON TESTTIME;

CREATE TYPE ALERTAGGENTRY(
        ALERTKEY STRING KEY,
        ALERT_STATUS STRING,
        NUMBER_OF_VIOLATIONS INT,
        DELAY_THRESHOLD INT,
        FIRSTTIME DATETIME
 );



CREATE STREAM ALERTAGGSTREAM OF ALERTAGGENTRY;


CREATE CQ PROCAGGCQ1 
 	INSERT INTO ALERTAGGSTREAM
		SELECT  ALERTKEY,
                        CASE WHEN COUNT(*) > 5 THEN 'CRITICAL'
                             WHEN COUNT(*) >= 3 AND COUNT(*) <= 5 THEN 'MAJOR'
                             WHEN COUNT(*) >=1 AND COUNT(*) < 3 THEN 'MINOR'
                        ELSE 'NORMAL'
                        END ,
                        COUNT(*),
                        DELAY_THRESHOLD,
                        FIRST(TESTTIME)
		FROM DELAYWARNWINDOW   
                WHERE STATUS = 'WARNING'
		GROUP BY ALERTKEY, STATUS,DELAY_THRESHOLD;


-- new waction stores

CREATE WACTIONSTORE PING_DETAILS
  CONTEXT OF ALERTENTRY
  EVENT TYPES (ALERTENTRY )
  PERSIST EVERY 60 second USING (
  JDBC_DRIVER:'oracle.jdbc.driver.OracleDriver',
  JDBC_URL:'jdbc:oracle:thin:@timstsnap05.ivanet.net:1521:timsrdg2',
  JDBC_USER:'network_cfg', JDBC_PASSWORD:'network_cfg',
  DDL_GENERATION:'create-or-extend-tables',
--  DDL_GENERATION:'drop-and-create-tables',
  CONTEXT_TABLE:'PING_DETAILS'
  );

CREATE CQ STOREDETAILSCQ
       INSERT INTO PING_DETAILS
              SELECT * FROM DELAYWARNWINDOW
              WHERE STATUS <> 'NORMAL';

CREATE WACTIONSTORE PingStore CONTEXT OF ALERTAGGENTRY
        EVENT TYPES( ALERTAGGENTRY )
        PERSIST NONE USING ( ) ;

CREATE CQ LoadPingStoreCQ
                INSERT INTO PingStore
                SELECT *
                FROM ALERTAGGSTREAM s;

-- end of stores 



CREATE STREAM AlertStream OF Global.AlertEvent;


CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT 'FROM TO SITES',      
       ALERTKEY, 
CASE
WHEN ALERT_STATUS != 'NORMAL' THEN 'warning'
ELSE 'info' END,
CASE
WHEN  (DNOW()  > DADD(FIRSTTIME, DMINS(20)) OR ALERT_STATUS = 'NORMAL')   THEN 'cancel'
ELSE 'raise' END,
CASE
WHEN ALERT_STATUS  != 'NORMAL' THEN ALERT_STATUS + ' ALERT FOR ' + ALERTKEY + ' has delay of over threshold ' + TO_STRING(DELAY_THRESHOLD) + ' has ' + TO_STRING(NUMBER_OF_VIOLATIONS) + '  violations over limit over the hour ' + TO_STRING(FIRSTTIME,'MM/dd/yyyy HH:mm:ss')  ELSE ''
END
    FROM ALERTAGGSTREAM 
WHERE NUMBER_OF_VIOLATIONS > 1;
--    FROM RESULTSSTORE; 


CREATE TARGET TRACE3 USING CSVWRITER (FILENAME:'logs/ALERTS.CSV') INPUT FROM AlertStream;

CREATE SUBSCRIPTION PingAppEmailAlert
USING EmailAdapter (
smtpurl:'smtp.nyc.ibasis.net:25',
smtp_auth:'false',
starttls_enable:'false',
subject:"IR34 Alert from Webaction", emailList:"pgopal@ibasis.net,ed@webaction.com",senderEmail:"webactionsupport@ibasis.net"
)
INPUT FROM AlertStream;

CREATE SUBSCRIPTION PosAppWebAlert
USING WebAlertAdapter( )
INPUT FROM AlertStream;


END flow PingMonitorBatchAppFlow;	

/*	
CREATE TYPE AlarmThreshold(
	profile int,
	threshholdValue int
);

CREATE CACHE AlarmThresholdCache using DatabaseReader (
   ConnectionURL:'jdbc:oracle:thin:@10.1.110.128:1521:orcl',
   Username:'scott',
   Password:'tiger',
   Query:'select * from scott.ALARM_THRESHOLD'
 ) QUERY (keytomap:'profile', refreshinterval: '60000000') OF AlarmThreshold; 
 

CREATE TYPE pingEvent(
	toLoc String,
	fromLoc String,	
        delay float,
        jitter float,
	packloss float,
        dateTime DateTime
);

CREATE STREAM dataStream OF pingEvent;

 Create Target trace1 Using Sysout (name:'rawping') input From LCRStream;

 
 CREATE CQ renderEvents
 INSERT INTO dataStream
 SELECT
 	x.TOLOCATION, x.FROMLOCACTION,  TO_FLOAT(x.DELAYAMT), TO_FLOAT(x.JITTERAMT), TO_FLOAT(x.PACKLOSSAMT), DNOW()
 FROM LCRStream1 x;
 
CREATE  jumping WINDOW  dataWindow OVER dataStream KEEP WITHIN 30 second; 
 
 CREATE TYPE AlertEntry(
	alertKey String KEY,
 	toLoc String,
 	fromLoc String,	
    status String,
	intStatus int,
    dateTime DateTime
 );

 CREATE STREAM AlertEntryStream OF AlertEntry;

 CREATE CQ processCQ1
 INSERT INTO AlertEntryStream
 select dw.toLoc + dw.fromLoc,
        dw.toLoc, 
        dw.fromLoc,         
        CASE WHEN dw.delay > c.threshholdValue THEN "WARNING" 
             ELSE "NORMAL" 
        END,
        CASE WHEN dw.delay > c.threshholdValue THEN 2
             ELSE 1
        END,
        dw.dateTime
 from dataWindow dw, AlarmThresholdCache c;
	  	 
 Create Target trace2 Using Sysout (name:'alert') input From AlertEntryStream;
*/ 
 
END APPLICATION PingMonitorBatchApp;
 -- create dashboard using "ClientApps/Overstock/ProdIndexDash.json";

CREATE APPLICATION  @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE  @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'JsonNodeEvent.json',
positionByEOF:false
)
PARSE USING Global.JSONParser (
 )  OUTPUT TO  @AppName@_rawstream;

CREATE CQ @BuiltinFunc@CQ
INSERT INTO  @BuiltinFunc@_Stream
SELECT @BuiltinFunc@(x, 'Sno', data.get("_id"), 'Name', data.get("firstname"))
FROM @AppName@_rawstream x;

CREATE OR REPLACE CQ cq1
INSERT INTO ClearUserData_Stream
SELECT
clearUserData(s1)
FROM @BuiltinFunc@_Stream s1;

CREATE OR REPLACE TARGET  @AppName@_FileTarget USING Global.FileWriter (
  flushpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
 directory: '@logs@',
  filename: '@BuiltinFunc@_JsonEventClearData',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  (
  handler: 'com.webaction.proc.JSONFormatter',
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  formatterName: 'JSONFormatter',
  jsonobjectdelimiter: '\n' )
INPUT FROM ClearUserData_Stream;

End application  @AppName@;
Deploy application  @AppName@;
Start application  @AppName@;

stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@;

CREATE SOURCE @SourceName@ USING MSSQLReader  ( 
ReaderType: 'LogMiner', 
  Password_encrypted: 'false', 
  DatabaseName: 'qatest',
  SupportPDB: false, 
  QuiesceMarkerTable: 'QUIESCEMARKER', 
  QueueSize: 2048, 
  CommittedTransactions: true, 
  Username: '@UserName@', 
  TransactionBufferType: 'Memory', 
  TransactionBufferDiskLocation: '.striim/LargeBuffer', 
  OutboundServerProcessName: 'WebActionXStream', 
  Password: '@Password@', 
  DDLCaptureMode: 'All', 
  Compression: false, 
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', 
  FetchSize: 1, 
  Tables: '@SourceTables@', 
  DictionaryMode: 'OnlineCatalog', 
  XstreamTimeOut: 600, 
  TransactionBufferSpilloverSize: '1MB', 
  StartTimestamp: 'null', 
  FilterTransactionBoundaries: true, 
  StartSCN: 'null', 
  ConnectionURL: '@ConnectionURL@', 
  SendBeforeImage: true ) 
OUTPUT TO @AppStream@  ;

CREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream SELECT data[2], to_string(to_date(data[2]), "dd-MMM-yy hh.mm.ss") FROM @AppStream@ o ;

CREATE  TARGET @targetsys@ USING Global.SysOut  ( 
name: 'ora1_sys' ) 
INPUT FROM admin.ZDT_cq_stream;

create Target @TargetFile@ using FileWriter(
  filename:'toStringOut.log',
  directory:'@FilePath@',
  rolloverpolicy:'eventcount:1000'
)
format using DSVFormatter (

)
input from admin.ZDT_cq_stream;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP application FileWriterDSVTester.DSV;
undeploy application FileWriterDSVTester.DSV;
drop application FileWriterDSVTester.DSV cascade;

create application DSV;

create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;


CREATE SOURCE CSVPosDataSource USING FileReader (
  directory: '@TEST-DATA-PATH@',
  WildCard: 'posdata.csv',
  positionByEOF: false,
  charset: 'UTF-8'
 )
 PARSE USING DSVParser (
  header: 'yes'
 )
OUTPUT TO PosDataCsvStream;

create source CSVMerchantNamesSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvMerchantNamesStream;

create source CSVSmallRetailSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallretaildata2M.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvSmallRetailStream;


Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Type CSVMerchantNamesType (
  merchantId String,
  merchantName String
);

Create Type CSVSmallRetailType (
storeId String,
nameId String,
city String,
state String
);

Create Stream TypedCSVStream of CSVType;
Create Stream TypedPosDataCSVStream of CSVType;
Create Stream TypedCSVMerchantNamesStream of CSVMerchantNamesType;
Create Stream TypedCSVSmallRetailStream of CSVSmallRetailType;


CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;


CREATE CQ CsvPosData
INSERT INTO TypedPosDataCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM PosDataCsvStream;


CREATE CQ CsvToMerchantNames
INSERT INTO TypedCSVMerchantNamesStream
SELECT data[0],
       data[1]
FROM CsvMerchantNamesStream;


CREATE CQ CsvToSmallRetailData
INSERT INTO TypedCSVSmallRetailStream
SELECT data[0],
       data[1],
       data[2],
       data[3]
FROM CsvSmallRetailStream;

/**
* 3.4.5.c FileWriter DSV ParserNegativeEC 100
**/
create Target DSVNegativeEventCount using FileWriter(
filename:'EventNCDefault',
directory:'@FEATURE-DIR@/logs/',
sequence:'00',
--filelimit: '5',
rolloverpolicy:'eventcount:-100',
buffersize:1
)
format using DSVFormatter (

)
input from TypedCSVSmallRetailStream;


/**
* 3.3.1.c FileWriter DSV TimeInterval
**/
create Target DSVTimeIntervalRollingPolicy using FileWriter(
  filename:'MerchantTIRP',
  sequence:'00',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00',
  buffersize:1
)
format using DSVFormatter (

)
input from TypedCSVMerchantNamesStream;

/**
* 3.4.6.d FileWriter DSV DefaultRP
**/
create Target DSVDefaultRollingPolicy using FileWriter(
directory:'@FEATURE-DIR@/logs/',
filename:'PosData',
rolloverpolicy:'EventCount:5000000',
buffersize:1
)
format using DSVFormatter (

)
input from TypedCSVStream;

/**
* 3.4.5.d FileWriter DSV EventCount 100
**/
create Target DSVEventCountDecimal using FileWriter(
filename:'Events',
directory:'@FEATURE-DIR@/logs/',
rolloverpolicy:'eventcount:200,sequence:00',
buffersize:1
)
format using DSVFormatter (

)
input from TypedCSVStream;

/**
* 3.3.1.a FileWriter DSVFileSize 1MB
**/
create Target DSVFileSize using FileWriter(
directory:'@FEATURE-DIR@/logs',
filename:'PosDataFS',
rolloverpolicy:'FileSizeRollingPolicy,filesize:1M,sequence:00',
buffersize:1
)
format using DSVFormatter (

)
input from TypedCSVStream;

/**
* 3.4.5.a FileWriter DSVFileSizeRoundUp 3MB
**/
create Target DSVFileSizeDecimal using FileWriter(
  directory:'@FEATURE-DIR@/logs/',
  filename:'RoundUPPosData',
  rolloverpolicy:'FileSizeRollingPolicy,filesize:2.5M,sequence:00',
  buffersize:1
)
format using DSVFormatter (

)
input from TypedCSVStream;


/**
* 3.1.1.b FileWriter DSV EventCount
**/

CREATE OR REPLACE TARGET DSVEventCount USING FileWriter (
  filename: 'TargetDefault',
  directory:'@FEATURE-DIR@/logs/',
  sequence:'00',
  flushinterval: '0',
  rolloverpolicy:'EventCount:5000000',
  buffersize:1
 )
 format using DSVFormatter (

)
INPUT FROM TypedPosDataCSVStream;


create Target TargetSmallPosData using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizePosDataDefault_actual.log') input from TypedCSVStream;

end application DSV;

STOP Istreamer.ISAPP;
UNDEPLOY APPLICATION Istreamer.ISAPP;
DROP APPLICATION Istreamer.ISAPP CASCADE;

CREATE APPLICATION ISAPP;


CREATE source implicitSource USING FileReader (
      directory:'@TEST-DATA-PATH@',
      columndelimiter: ',',
      wildcard:'ISdata.csv',
      blocksize: 10240,
      positionByEOF:false
)
PARSE USING DSVParser (
      header:False,
      trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity double,
  size long,
  currentDate DateTime);

CREATE CACHE cache1 USING CsvReader(
  directory: '@TEST-DATA-PATH@',
  wildcard: 'ISdata.csv',
  header: false,
  columndelimiter: ',',
  trimquote: false
  ) QUERY (keytomap:'productID') OF Atm;


CREATE STREAM newStream OF Atm;


CREATE CQ newCQ
INSERT INTO newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]), TO_DATE(data[5]) FROM
CsvStream;

CREATE WINDOW win1
OVER newStream
KEEP 50 rows;


CREATE CQ newCQ2
INSERT INTO newStream2
SELECT productID as A , stateID AS B, productWeight AS C, quantity AS D, size AS E, currentDate AS F FROM
newStream;


CREATE CQ newCQ3
INSERT INTO newStream3 PARTITION BY A
SELECT A,B,C,D,E,F FROM newStream2
link source event;

CREATE CQ newCQ4
INSERT INTO newStream4
SELECT count(productID),currentDate FROM newStream ORDER BY currentDate
link source event;

CREATE CQ newCQ5
INSERT INTO newStream5
SELECT x.*, y.* from cache1 x, newStream y WHERE x.productweight > 6 ORDER BY x.currentDate, x.productID;


CREATE WACTIONSTORE WS1 CONTEXT OF Atm
EVENT TYPES(Atm );

CREATE CQ newCQ6
INSERT INTO WS1
SELECT * FROM newStream WHERE productID = '001';

CREATE CQ newCQ7
INSERT INTO newStream6
SELECT aa.productID FROM WS1 [push] aa, cache1 bb;

CREATE CQ newCQ8
INSERT INTO newStream7
SELECT Sum(X.size) FROM (Select size from win1 where productweight > 5) X;

CREATE CQ newCQ9
INSERT INTO newStream8
SELECT count(productID) FROM WS1 [push] ORDER BY productID;


END APPLICATION ISAPP;
deploy APPLICATION ISAPP;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL ;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes1',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource2 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes2',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource3 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes3',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'TEST.user_chkpoint',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: 'QATEST.OracToCql_alldatatypes1,test.oractocq_alldatatypes columnmap(NumericToBigint=NumericToBigint);QATEST.OracToCql_alldatatypes2,test.oractocq_alldatatypes columnmap(NumericToBigint=NumericToBigint);QATEST.OracToCql_alldatatypes3,test.oractocq_alldatatypes columnmap(NumericToBigint=NumericToBigint)',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;


END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING ParquetParser (
  retryWait: '1m'
)
OUTPUT TO @APPNAME@_Stream;

CREATE CQ @APPNAME@_CQ
INSERT INTO @APPNAME@_CQOut
SELECT putUserData(x, 'folderName','Parquet') FROM @APPNAME@_Stream x;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING ParquetFormatter (
  schemaFileName: 'parquetSchema',
  members:'data'
)
INPUT FROM @APPNAME@_CQOut;

END APPLICATION @APPNAME@;

STOP TQLwithinTqlApp;
UNDEPLOY APPLICATION TQLwithinTqlApp;
DROP APPLICATION TQLwithinTqlApp CASCADE;

CREATE APPLICATION TQLwithinTqlApp;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',

  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


END APPLICATION TQLwithinTqlApp;
DEPLOY APPLICATION TQLwithinTqlApp;
START TQLwithinTqlApp;

@@FEATURE-DIR@/tql/TQLTobeCalled.tql