{"tql": "stop DataGenSampleApp;\nundeploy application DataGenSampleApp;\ndrop application DataGenSampleApp cascade;\n\n\nCREATE APPLICATION DataGenSampleApp;\n\nCreate Source dataGenSrc USING MariaDBReader  ( \n  Username:'qatest',\n  Password:'w3b@ct10n',\n  ConnectionURL:'jdbc:mariadb://10.77.21.53:3306/qatest',\n  Tables: '@tableNames@',\n  ClusterSupport: 'Galera'\n ) \n Output To LCRStream;\n\ncreate Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;\n\nEND APPLICATION DataGenSampleApp;", "generated_queries": "1. What are the steps involved in deploying and configuring an application named DataGenSampleApp that utilizes MariaDB as a data source with Galera cluster support?\n2. How can I specify a MariaDB connection for a Source component in an application called DataGenSampleApp using a specific username, password, and connection URL?\n3. What actions need to be taken to stop, undeploy, and drop an application named DataGenSampleApp, along with its associated components like Source and Target, in a TQL query format?", "file_name": "dataGenMariaDBTqlTemplate.tql"}
{"tql": "stop DBRTOCW;\n undeploy application DBRTOCW;\n drop application DBRTOCW cascade;\n CREATE APPLICATION DBRTOCW;\n\n Create Source MSSQLSource Using MSSqlReader\n(\nUsername:'qatest',\nPassword:'w@ct10n',\nDatabaseName:'qatest',\nConnectionURL:'10.77.61.30:1433',\nTables:'qatest.MssqlTocql_Alldatatypes',\nConnectionPoolSize:1,\nCompression:'true'\n)\nOUTPUT TO Oracle_ChangeDataStream;\n\n CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n ) \nINPUT FROM Oracle_ChangeDataStream;\n\n create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\n END APPLICATION DBRTOCW;\n\n deploy application DBRTOCW in default;\n\n start DBRTOCW;", "generated_queries": "1. What are the steps involved in creating and deploying an application named DBRTOCW that integrates data between a Microsoft SQL Server source and an Oracle target with specific configurations like compression enabled and batch policies set?\n2. How can I configure a source connection to a Microsoft SQL Server database named 'qatest' with a specific table 'qatest.MssqlTocql_Alldatatypes' and output the data changes to an Oracle target database using Talend ETL tool?\n3. Can you provide a detailed process of creating an application in Talend called DBRTOCW that reads data from an Oracle Change Data Capture stream and writes it to an Oracle target named 'DBTarget' with specific batch and commit policies set?", "file_name": "MssqlToCassandra.tql"}
{"tql": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY;\n\nCreate Source @APPNAME@_src Using OracleReader\n(\n Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1000,\n -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n   _h_ReturnDateTimeAs:'ZonedDateTime'\n) Output To @APPNAME@_stream;\n\ncreate Target @APPNAME@_tgt using FileWriter(\n  filename:'CompressedMerchant.gz',\n  directory:'/logs/',\n  rolloverpolicy:'EventCount:10000'\n)\nformat using ParquetFormatter (\n\tschemaFileName:'@FILENAME@',\n\tFormatAs:'@FORMATAS@'\n)\ninput from @APPNAME@_stream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;", "generated_queries": "1. How can I create a data pipeline that extracts data from an Oracle database table, compresses it, and then writes it to a Parquet file in a specific directory with a rollover policy based on event count?\n2. What are the steps required to stop, undeploy, drop, create, configure, and start an application named @APPNAME@ in a Talend environment?\n3. Can you provide the configuration details for setting up an OracleReader source to extract data from a specific source table using connection details like URL, username, and password, with specified settings for compression, fetch size, and transaction handling?", "file_name": "ParquetFormatterNoRecovery.tql"}
{"tql": "CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (\n  Tables: '',\n  ConnectionURL: '',\n  Password: '',\n  Username: ''\n  )\nOUTPUT TO @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING AvroFormatter (\nschemaFileName: '@SCHEMAFILE@'\n)\nINPUT FROM @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING DSVFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nEND APPLICATION @APPNAME@;", "generated_queries": "1. What is the process for creating and configuring an application with multiple sources and targets, including an Oracle source and S3 targets with different data formats?\n\n2. How can I set up an application that reads data from an Oracle database, streams it into multiple S3 buckets, each having a different data format (Avro, JSON, DSV), and specifies the necessary connection details and configurations?\n\n3. Can you show me an example of defining an application that uses an OracleReader as the data source and S3Writers as the targets, with each S3Writer having a different data format (Avro, JSON, DSV), and how the schema files are referenced in the configuration?", "file_name": "OracleCDCToS3MutliTargetsAvro.tql"}
{"tql": "CREATE APPLICATION KafkaReader;\n\nCREATE OR REPLACE TYPE KafkaSourceStr2_Type  ( seq java.lang.Integer\n );\n\nCREATE OR REPLACE STREAM KafkaSourceStr2 OF KafkaSourceStr2_Type;\n\nCREATE  JUMPING WINDOW GetTargData OVER KafkaSourceStr2 KEEP 1000000 ROWS;\n\nCREATE OR REPLACE SOURCE KafkaSource USING KafkaReader VERSION '0.11.0' (\n  KafkaConfigPropertySeparator: ';',\n  startOffset: 0,\n  adapterName: 'KafkaReader',\n  Topic: 'kafkaTopic7',\n  AutoMapPartition: true,\n  brokerAddress: 'localhost:9092',\n  KafkaConfigValueSeparator: '=',\n  KafkaConfig: 'max.partition.fetch.bytes=10485760;fetch.min.bytes=1048576;fetch.max.wait.ms=1000;receive.buffer.bytes=2000000;poll.timeout.ms=10000;request.timeout.ms=60001;session.timeout.ms=60000'\n )\n PARSE USING DSVParser  (\n  charset: 'UTF-8',\n  handler: 'com.webaction.proc.DSVParser_1_0',\n  linenumber: 0,\n  nocolumndelimiter: false,\n  trimwhitespace: false,\n  columndelimiter: ',',\n  columndelimittill: '-1',\n  ignoremultiplerecordbegin: 'true',\n  ignorerowdelimiterinquote: false,\n  parserName: 'DSVParser',\n  separator: ':',\n  blockascompleterecord: false,\n  ignoreemptycolumn: false,\n  rowdelimiter: '\\n',\n  header: false,\n  headerlineno: 0,\n  quoteset: '\\\"',\n  trimquote: true\n )\nOUTPUT TO KafkaSourceStr1 ;\n\nCREATE OR REPLACE CQ GetKafkaDataQuery\nINSERT INTO KafkaSourceStr2\nSELECT TO_INT(data[1]) as seq\nFROM KafkaSourceStr1;\n\nCREATE  TYPE KafkaSourceStr3_Type  ( SUMKafkaSourceStr2seq java.lang.Long\n );\n\nCREATE STREAM KafkaSourceStr3 OF KafkaSourceStr3_Type;\n\nCREATE OR REPLACE CQ GetTheSum\nINSERT INTO KafkaSourceStr3\nSELECT SUM(GetTargData .seq)\nFROM GetTargData;\n\nCREATE OR REPLACE TARGET KafkaFile USING FileWriter  (\n  filename: 'TargetResults',\n  rolloveronddl: 'true',\n  flushpolicy: 'eventcount:10000,interval:30',\n  adapterName: 'FileWriter',\n  directory: '@FEATURE-DIR@/logs',\n  rolloverpolicy: 'eventcount:10000,interval:30s'\n )\nFORMAT USING DSVFormatter  (   nullvalue: 'NULL',\n  standard: 'none',\n  handler: 'com.webaction.proc.DSVFormatter',\n  formatterName: 'DSVFormatter',\n  usequotes: 'false',\n  rowdelimiter: '\\n',\n  quotecharacter: '\\\"',\n  header: 'false',\n  columndelimiter: ','\n )\nINPUT FROM KafkaSourceStr3;\n\nEND APPLICATION KafkaReader;", "generated_queries": "1. How can I set up a real-time data processing pipeline to read data from a Kafka topic named 'kafkaTopic7', calculate the sum of a specific field in the incoming data, and output the aggregated results to a file named 'TargetResults' in a specific directory?\n  \n2. What configuration parameters are required to connect to a Kafka cluster with address 'localhost:9092', and how can I efficiently parse and process the data received from this Kafka cluster using TQL?\n\n3. Can you provide an example of creating a TQL query that reads streaming data from a Kafka topic, calculates the total sum of a specific field in the data stream, and writes the results to a file using DSV formatting for further analysis and reporting?", "file_name": "KafkaReader.tql"}
{"tql": "STOP APPLICATION @WRITERAPPNAME@;\nUNDEPLOY APPLICATION @WRITERAPPNAME@;\nDROP APPLICATION @WRITERAPPNAME@ CASCADE;\n\nCREATE APPLICATION @WRITERAPPNAME@ RECOVERY 1 SECOND INTERVAL;\n\ncreate flow @APPNAME@_agentflow;\n\nCREATE SOURCE @SOURCE@ USING OracleReader\n(\nFetchSize:1,\nUsername:'@SOURCE_USER@',\nPassword:'85d7qFnwTW8=',\nConnectionURL:'@CONNECTION_URL@',\nTables:'@SOURCE_TABLE@',\npassword_encrypted: 'true'\n)\nOUTPUT TO @STREAM1@;\n\n\nend flow @APPNAME@_agentflow;\n\ncreate flow @APPNAME@_serverflow;\n\nCREATE OR REPLACE TYPE @TYPE@( \ndatae java.util.HashMap , \nTABLE_NAME java.lang.String , \nOPS_NAME java.lang.String , \nDB_TIMESTAMP java.lang.String  ,\nCOMMITSCN java.lang.String ,\nSCN java.lang.String ,\nREC_INS_TIME java.lang.String );\n\nCREATE CQ @CQ1@\nINSERT INTO @STREAM2@\nSELECT  \nCASE WHEN (META(c,\"OperationName\").toString() == \"DELETE\")\nTHEN putUserData(c, 'isDelete', 'true') \nELSE\nputUserData(c,'isDelete', 'false')\nEND\nFROM @STREAM1@ c;\n\nCREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;\n\nCREATE OR REPLACE CQ @CQ2@ \nINSERT INTO @STREAM3@\nSELECT \ndata(e),\nMETA(e,\"TableName\").toString() as TABLE_NAME,\nMETA(e, \"OperationName\").toString() as OPS_NAME,\nMETA(e, \"TimeStamp\").toString() as DB_TIMESTAMP,\nMETA(e,\"COMMITSCN\").toString() as COMMITSCN ,\nMETA(e,\"SCN\").toString() as  SCN ,\nDNOW().toString() as REC_INS_TIME\nFROM @STREAM1@ e;\n\ncreate Target @TARGET1@ using KafkaWriter VERSION @kafkaAdpVersion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC1',\nParallelThreads:'',\nPartitionKey:'@metadata(TableName)',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING dsvFormatter ()\ninput from @STREAM1@;\n\n\ncreate Target @TARGET2@ using KafkaWriter VERSION @kafkaAdpVersion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC2',\nParallelThreads:'2',\nPartitionKey:'TABLE_NAME',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING jsonFormatter ()\ninput from @STREAM3@;\n\ncreate Target @TARGET3@ using KafkaWriter VERSION @kafkaAdpVersion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC3',\nParallelThreads:'',\nPartitionKey:'@userdata(isDelete)',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING avroFormatter (\nschemaFileName:'KafkaAvroTest.avsc')\ninput from @STREAM2@;\n\nend application @WRITERAPPNAME@;\ndeploy application @WRITERAPPNAME@;\nstart @WRITERAPPNAME@;\nstop application @READERAPPNAME@;\nundeploy application @READERAPPNAME@;\ndrop application @READERAPPNAME@ cascade;\nCREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafkaAdpVersion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC1',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream1;\n\n\nCREATE TARGET kafkaDumpDSV USING FileWriter(\nname:kafkaOuputDSV,\nfilename:'@READERAPPNAME@_RT_DSV')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaReaderStream1;\n\nCREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafkaAdpVersion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC2',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO KafkaReaderStream2;\n\n\nCREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafkaAdpVersion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC3',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'KafkaAvroTest.avsc'\n)\nOUTPUT TO KafkaReaderStream3;\n\nend flow @APPNAME@_serverflow;\n\nend application @READERAPPNAME@;\ndeploy application @READERAPPNAME@;", "generated_queries": "1. How can I configure a KafkaWriter target in TQL to write data to multiple topics with different formatting and partitioning based on different streams' data attributes?\n  \n2. Can you provide the TQL query structure for creating multiple flows in an application that processes data from Oracle, transforms it, and then writes it to Kafka topics using various formatters and parsers?\n\n3. What are the steps involved in setting up and configuring TQL queries to deploy and manage two separate applications - one for writing data from Oracle to Kafka and another for reading data from Kafka and storing it in files using different parsers in each flow?", "file_name": "orcl_kw_11_agent.tql"}
{"tql": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\nCREATE APPLICATION @APPNAME@ RECOVERY 10 SECOND INTERVAL;\n\n create flow Mysqlflow;\nCREATE SOURCE MysqlToDBRoutersource USING MysqlReader\n(\n  Username: '',\n  Password: '',\n  Tables: '',\n  ConnectionURL: '',\n  Password_encrypted: 'false',\n  connectionRetryPolicy: 'retryInterval=30, maxRetries=3'\n)\nOUTPUT TO RouterTestMasterStream;\n\nend flow Mysqlflow;\n\nCREATE OR REPLACE ROUTER RouterTestRs1 INPUT FROM RouterTestMasterStream s CASE\nWHEN meta(s,\"TableName\").toString()='waction.source1' THEN ROUTE TO RouterTestTyped1,\nWHEN meta(s,\"TableName\").toString()='waction.source2' THEN ROUTE TO RouterTestTyped2,\nELSE ROUTE TO RouterTestTypedElse;\n\nCREATE TARGET MysqlToDBRoutertarget1 USING DatabaseWriter(\n   Username: '',\n   Password: '',\n   Tables: '',\n   ConnectionURL: '',\n   Password_encrypted: 'false',\n   connectionRetryPolicy: 'retryInterval=30, maxRetries=3'\n) INPUT FROM RouterTestTyped1;\n\n\nCREATE TARGET MysqlToDBRoutertarget2 USING DatabaseWriter(\n    Username: '',\n    Password: '',\n    Tables: '',\n    ConnectionURL: '',\n    Password_encrypted: 'false',\n    connectionRetryPolicy: 'retryInterval=30, maxRetries=3'\n) INPUT FROM RouterTestTyped2;\n\n\nend application @APPNAME@;\ndeploy application @APPNAME@ with Mysqlflow in AGENTs;\nstart application @APPNAME@;", "generated_queries": "1. How can I stop, undeploy, drop, create with recovery interval of 10 seconds, and then deploy and start an application named \"@APPNAME@\" along with a flow called \"Mysqlflow\" using a series of specific configurations for sources, routers, and targets?\n2. What are the routing rules set up in the TQL query for the RouterTestRs1 router based on the meta information \"TableName\" from the input stream \"RouterTestMasterStream\"?\n3. Can you provide details on the database connection configurations used by the DatabaseWriters for the MysqlToDBRoutertarget1 and MysqlToDBRoutertarget2 targets in the TQL query?", "file_name": "routerMySQL.tql"}
{"tql": "--\n-- Crash Recovery Test 2 on four node all server cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS\n--\n\nSTOP APPLICATION N4S4CR2Tester.N4S4CRTest2;\nUNDEPLOY APPLICATION N4S4CR2Tester.N4S4CRTest2;\nDROP APPLICATION N4S4CR2Tester.N4S4CRTest2 CASCADE;\nCREATE APPLICATION N4S4CRTest2 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN4S4CRTest2;\n\nCREATE SOURCE CsvSourceN4S4CRTest2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN4S4CRTest2;\n\nCREATE FLOW DataProcessingN4S4CRTest2;\n\nCREATE TYPE WactionTypeN4S4CRTest2 (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionTypeN4S4CRTest2;\n\nCREATE CQ CsvToWaction\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsN4S4CRTest2 CONTEXT OF WactionTypeN4S4CRTest2\nEVENT TYPES ( WactionTypeN4S4CRTest2 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsN4S4CRTest2\nINSERT INTO WactionsN4S4CRTest2\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND FLOW DataProcessingN4S4CRTest2;\n\nEND APPLICATION N4S4CRTest2;", "generated_queries": "1. What is the process flow for Crash Recovery Test 2 on a four-node all-server cluster developed by Bert Hashemi at WebAction, Inc.?\n2. How is real-time data acquisition and processing implemented in the N4S4CRTest2 application using TQL?\n3. Can you provide details on the windowing and event handling strategies utilized in the Crash Recovery Test 2 application?", "file_name": "KStreamN4S4CRTest2.tql"}
{"tql": "stop OracleToKudu;\nundeploy application OracleToKudu;\ndrop application OracleToKudu cascade;\n\nCREATE APPLICATION OracleToKudu RECOVERY 5 SECOND INTERVAL;\nCreate Source oracSource\n Using OracleReader\n(\n Username:'@LOGMINER-UNAME@',\n Password:'@LOGMINER-PASSWORD@',\n ConnectionURL:'@LOGMINER-URL@',\n Tables:'@SOURCE_TABLES@',\n OnlineCatalog:true,\n FetchSize:1\n) Output To DataStream;\nCREATE TARGET WriteintoKudu using KuduWriter (\nkuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',\npkupdatehandlingmode:'@MODE@',\ntables: '@TARGET_TABLES@',\nConnectionRetryPolicy: 'retryInterval=40,maxRetries=7',\nbatchpolicy: 'EventCount:20,Interval:60')\nINPUT FROM DataStream;\n\nEND APPLICATION OracleToKudu;\ndeploy application OracleToKudu in default;\nstart OracleToKudu;", "generated_queries": "1. How can I set up real-time replication from an Oracle database to a Kudu table?\n2. What are the configuration parameters required to deploy and run an application for replicating data from Oracle to Kudu?\n3. Can you provide the steps to start and monitor the OracleToKudu application for continuous data replication?", "file_name": "OracleToKuduRecovery.tql"}
{"tql": "STOP APPLICATION EH;\nUNDEPLOY APPLICATION EH;\nDROP APPLICATION EH CASCADE;\nCREATE APPLICATION EH recovery 5 second interval;\nCREATE Source s USING PostgreSQLReader  ( \n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Tables: 'public.tablename1000%') \nOUTPUT TO ss ;\n\nCREATE OR REPLACE TYPE jsontype( \ndatae java.util.HashMap , \nTABLE_NAME java.lang.String , \nOPS_NAME java.lang.String , \nDB_TIMESTAMP java.lang.String );\n\nCREATE STREAM cq_json_out OF jsontype PARTITION BY TABLE_NAME;\n\nCREATE OR REPLACE CQ cq_json \nINSERT INTO cq_json_out\nSELECT \ndata(e),\nMETA(e,\"TableName\").toString() as TABLE_NAME,\nMETA(e, \"OperationName\").toString() as OPS_NAME,\nMETA(e, \"TimeStamp\").toString() as DB_TIMESTAMP\nFROM ss e;\n\nCREATE CQ cq1\nINSERT INTO TypedAccessLogStream1\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000101'; \n\ncreate Target t1 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_101',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_101',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream1;\n\nCREATE CQ cq2\nINSERT INTO TypedAccessLogStream2\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000102'; \n\ncreate Target t2 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_102',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_102',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream2;\n\nCREATE CQ cq3\nINSERT INTO TypedAccessLogStream3\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000103'; \n\ncreate Target t3 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_103',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_103',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream3;\n\nCREATE CQ cq4\nINSERT INTO TypedAccessLogStream4\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000104'; \n\ncreate Target t4 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_104',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_104',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream4;\n\nCREATE CQ cq5\nINSERT INTO TypedAccessLogStream5\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000105'; \n\ncreate Target t5 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_105',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_105',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream5;\n\nCREATE CQ cq6\nINSERT INTO TypedAccessLogStream6\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000106'; \n\ncreate Target t6 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_106',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_106',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream6;\n\nCREATE CQ cq7\nINSERT INTO TypedAccessLogStream7\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000107'; \n\ncreate Target t7 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_107',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_107',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream7;\n\nCREATE CQ cq8\nINSERT INTO TypedAccessLogStream8\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000108'; \n\ncreate Target t8 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_108',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_108',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream8;\n\nCREATE CQ cq9\nINSERT INTO TypedAccessLogStream9\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000109'; \n\ncreate Target t9 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_109',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_109',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream9;\n\n-- CREATE CQ cq10\n-- INSERT INTO TypedAccessLogStream10\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000110'; \n-- \n-- create Target t10 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_110',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_110',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream10;\n\n\n-- CREATE CQ cq11\n-- INSERT INTO TypedAccessLogStream11\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000111'; \n-- \n-- create Target t11 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_111',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_111',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream11;\n-- \n-- CREATE CQ cq12\n-- INSERT INTO TypedAccessLogStream12\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000112'; \n-- \n-- create Target t12 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_112',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_112',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream12;\n-- \n-- CREATE CQ cq13\n-- INSERT INTO TypedAccessLogStream13\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000113'; \n-- \n-- create Target t13 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_113',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_113',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream13;\n-- \n-- CREATE CQ cq14\n-- INSERT INTO TypedAccessLogStream14\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000114'; \n-- \n-- create Target t14 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_114',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_114',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream14;\n-- \n-- CREATE CQ cq15\n-- INSERT INTO TypedAccessLogStream15\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000115'; \n-- \n-- create Target t15 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_115',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_115',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream15;\n-- \n-- CREATE CQ cq16\n-- INSERT INTO TypedAccessLogStream16\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000116'; \n-- \n-- create Target t16 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_116',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_116',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream16;\n-- \n-- CREATE CQ cq17\n-- INSERT INTO TypedAccessLogStream17\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000117'; \n-- \n-- create Target t17 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_117',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_117',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream17;\n-- \n-- CREATE CQ cq18\n-- INSERT INTO TypedAccessLogStream18\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000118'; \n-- \n-- create Target t18 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_118',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_118',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream18;\n-- \n-- CREATE CQ cq19\n-- INSERT INTO TypedAccessLogStream19\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000119'; \n-- \n-- create Target t19 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_119',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_119',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream19;\n-- \n-- CREATE CQ cq20\n-- INSERT INTO TypedAccessLogStream20\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000120'; \n-- \n-- create Target t20 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_120',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_120',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream20;\n\n\nEND APPLICATION EH;\nDEPLOY APPLICATION EH;\nstart application EH;", "generated_queries": "1. How can I configure a stream to read data from a PostgreSQL database and send it to an Azure Event Hub using JSON formatting?\n2. What is the process for creating continuous queries that filter and partition data based on specific table names and route them to different Azure Event Hubs?\n3. How can I deploy an application named \"EH\" that consists of multiple stream-processing components and connections to Azure Event Hubs for real-time data processing and event streaming?", "file_name": "postgrescdc_eh.tql"}
{"tql": "create application CSVToJSON;\ncreate source CSVSource using FileReader (\n\tdirectory:'Samples/AppData',\n\tWildCard:'posdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\n\tfilename:'posdata_JSON',\n\trolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'\n)\nformat using JSONFormatter (\n\tmembers:'merchantname,merchantid,dateTime,hourValue,amount,zip'\n)\n\ninput from TypedCSVStream;\nend application CSVToJSON;\n\ndeploy application CSVToJSON;\nstart application CSVToJSON;", "generated_queries": "1. How can I convert a CSV file containing POS data into JSON format using TQL?\n2. What is the process for parsing a CSV file with headers into a structured stream of data fields in TQL?\n3. How can I deploy and start an application in TQL that converts a CSV file to JSON and writes the output to a target file with specified formatting and rolling policies?", "file_name": "FileWriterWithJSONFormatter.tql"}
{"tql": "stop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@;\n\nCREATE SOURCE @SourceName@ USING PostgreSQLReader  ( \nReaderType: 'LogMiner', \n  Password_encrypted: 'false', \n  SupportPDB: false, \n  ReplicationSlotName: 'test_slot',\n  QuiesceMarkerTable: 'QUIESCEMARKER', \n  QueueSize: 2048, \n  CommittedTransactions: true, \n  Username: '@UserName@', \n  TransactionBufferType: 'Memory', \n  TransactionBufferDiskLocation: '.striim/LargeBuffer', \n  OutboundServerProcessName: 'WebActionXStream', \n  Password: '@Password@', \n  DDLCaptureMode: 'All', \n  Compression: false, \n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', \n  FetchSize: 1, \n  Tables: '@SourceTables@', \n  DictionaryMode: 'OnlineCatalog', \n  XstreamTimeOut: 600, \n  TransactionBufferSpilloverSize: '1MB', \n  FilterTransactionBoundaries: true, \n  ConnectionURL: '@ConnectionURL@', \n  SendBeforeImage: true ) \nOUTPUT TO @AppStream@  ;\n\nCREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream SELECT data[2], to_string(to_date(data[2]), \"dd-MMM-yy hh.mm.ss\") FROM @AppStream@ o ;\n\nCREATE  TARGET @targetsys@ USING Global.SysOut  ( \nname: 'ora1_sys' ) \nINPUT FROM admin.ZDT_cq_stream;\n\ncreate Target @TargetFile@ using FileWriter(\n  filename:'toStringOut.log',\n  directory:'@FilePath@',\n  rolloverpolicy:'eventcount:1000'\n)\nformat using DSVFormatter (\n\n)\ninput from admin.ZDT_cq_stream;\n\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;", "generated_queries": "1. What are the configuration settings used for the PostgreSQL Reader when reading data from a specified set of tables and replicating it with a specific Replication Slot Name?\n\n2. How is data transformation configured within the application flow, specifically the Continuous Query (CQ) that inserts selected data into a target table and formats the date column before inserting?\n\n3. What are the details of the defined targets in the application, including the Global.SysOut target system and a file writer target that stores data in a specified file format and directory with rollover policy based on event count?", "file_name": "toStringZonedDateTime_postgres.tql"}
{"tql": "--\n-- Recovery Test 13 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same compound key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> CW(p#p) -> CQ -> WS\n--\n\nSTOP Recov13Tester.RecovTest13;\nUNDEPLOY APPLICATION Recov13Tester.RecovTest13;\nDROP APPLICATION Recov13Tester.RecovTest13 CASCADE;\nCREATE APPLICATION RecovTest13 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTest10Data.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE CsvData (\n  partKey String KEY,\n  serialNumber int,\n  partKey2 String KEY\n);\n\nCREATE TYPE WactionData (\n  partKey String KEY,\n  serialNumber int\n);\n\nCREATE STREAM DataStream OF CsvData PARTITION BY partKey, partKey2;\n\nCREATE CQ CsvToData\nINSERT INTO DataStream\nSELECT\n    data[0],\n    TO_INT(data[1]),\n    data[0]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStreamTwoItems\nOVER DataStream KEEP 2 ROWS\nPARTITION BY partKey, partKey2;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction\nINSERT INTO Wactions\nSELECT\n    first(partKey),\n    to_int(first(serialNumber))\nFROM DataStreamTwoItems\nGROUP BY partKey, partKey2;\n\nEND APPLICATION RecovTest13;", "generated_queries": "1. What is the process for recovering test 13 with two sources, two jumping windows, and one wactionstore, all partitioned on the same compound key?\n2. How can I create a real-time data processing application named RecovTest13 with recovery set at a 5-second interval, using CSV data sources and transforming the data into wactions stored in a particular partition scheme?\n3. Can you explain the flow of data from a CSV source to the creation of wactions in the context of WactionData, including the intermediate steps involving jumping windows and partitioning the data based on specific keys?", "file_name": "RecovTest13.tql"}
{"tql": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()\nPARSE USING XMLParserV2 ()\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE OR REPLACE CQ @APPNAME@_CQ\nINSERT INTO @APPNAME@_CQOut\nSELECT\ndata.attributeValue(\"merchantid\") as merchantID,\ndata.getText() as companyName\nFROM @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()\nFORMAT USING JSONFormatter ()\nINPUT FROM @APPNAME@_CQOut;\n\nEND APPLICATION @APPNAME@;", "generated_queries": "1. What is the merchant ID and company name associated with the data coming from the specified source in the application named @APPNAME@?\n   \n2. How is the XML data being parsed and processed within the application @APPNAME@?\n   \n3. In which format is the data being output from the query results of the continuous query @APPNAME@_CQ in the application @APPNAME@ being presented before being fed into the target system?", "file_name": "XmlV2ToJson.tql"}
{"tql": "use PosTester;\nalter application PosApp;\n\nCREATE TYPE MerchantTxRate(\n  merchantId String KEY,\n  zip String,\n  startTime DateTime,\n  count int,\n  totalAmount double,\n  hourlyAve int,\n  upperLimit double,\n  lowerLimit double,\n  category String,\n  status String\n);\n\nend application PosApp;\nalter application PosApp recompile;", "generated_queries": "1. How many transactions were processed by each merchant in a specific zip code within a certain timeframe?\n2. What is the average hourly transaction amount for each merchant in a specified category?\n3. Which merchants have exceeded the upper transaction amount limit set in the system for a particular category?", "file_name": "createType.tql"}
{"tql": "STOP APPLICATION ORACLETOBIGQUERY;\nUNDEPLOY APPLICATION ORACLETOBIGQUERY;\nDROP APPLICATION ORACLETOBIGQUERY CASCADE;\n\n--create application \nCREATE APPLICATION ORACLETOBIGQUERY RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE OR REPLACE SOURCE OracleSource USING OracleReader (\n ConnectionURL: '192.168.123.12:1521/ORCL',\n Tables: 'QATEST.ORATOBIGQALLDATATYPE',\n Username: 'qatest',\n Password: 'qatest',\n FetchSize:1\n) OUTPUT TO CDCStream;\n\nCREATE OR REPLACE TARGET bqtables using BigqueryWriter(\n BQServiceAccountConfigurationPath:\"/Users/karthikmurugan/Downloads/bqtest-540227c31980.json\",\n projectId:\"bqtest-158706\",\n Tables: \"QATEST.ORATOBIGQALLDATATYPE,QATEST.ORATOBIGQALLDATATYPE\",\n BatchPolicy: \"eventCount:1,Interval:90\")\nINPUT FROM CDCStream;\n\n\nCREATE OR REPLACE TARGET T1 using SysOut(name : \"some text\") INPUT FROM CDCStream;\n\nEND APPLICATION ORACLETOBIGQUERY;\n\nDEPLOY APPLICATION ORACLETOBIGQUERY;\nSTART APPLICATION ORACLETOBIGQUERY;", "generated_queries": "1. Can you provide the steps to deploy and start an application named \"ORACLETOBIGQUERY\" that reads data from an Oracle database and writes it to BigQuery using a CDC stream?\n\n2. How can I create an application that handles the synchronization of data between an Oracle database table called \"QATEST.ORATOBIGQALLDATATYPE\" and two BigQuery tables named \"QATEST.ORATOBIGQALLDATATYPE\" with a recovery interval of 5 seconds?\n\n3. What are the configurations needed to set up a CDC stream to capture changes from an Oracle database table and then write these changes to BigQuery tables using a service account configuration in a TQL script for the application \"ORACLETOBIGQUERY\"?", "file_name": "OracleToBigQueryAlldatatypes.tql"}
{"tql": "stop application reconnect;\nundeploy application reconnect;\ndrop application reconnect cascade;\nCREATE APPLICATION reconnect recovery 1 second interval;\n\nCREATE  SOURCE mssqlsource USING MssqlReader  ( \n  Username: '@USERNAME@',\n  Password: '@PASSWORD@',\n  ConnectionURL: '@URL@',\n  Tables: '@TABLE@',\n  FetchSize: 1\n ) \nOUTPUT TO sqlstream;\n\nCREATE TARGET dbtarget USING CassandraWriter(\n  ConnectionURL:'@URL@',\n  Username:'@USERNAME@',\n  Password:'@PASSWORD@',\n  ConnectionRetryPolicy: 'retryInterval=15s,maxRetries=2',\n  BatchPolicy:'EventCount:5,Interval:30',\n  CommitPolicy:'EventCount:5,Interval:30',\n  Tables: '@TABLES@'\n ) INPUT FROM sqlstream;\n\n create Target tSysOut using Sysout(name:OrgData) input from sqlstream;\n end application reconnect;\n deploy application reconnect;\n start application reconnect;", "generated_queries": "1. What is the data replication architecture for the 'reconnect' application, including the source, target, and any specified retry policies?\n2. Can you provide details on the database connection configurations, such as the credentials and connection settings, for the CassandraWriter target in the 'reconnect' application?\n3. How is the 'reconnect' application's recovery mechanism configured to handle failures, specifically in terms of the recovery interval and any specified retry policies?", "file_name": "oracle_CassandraReconnect.tql"}
{"tql": "STOP APPLICATION App1;\nUNDEPLOY APPLICATION App1;\nDROP APPLICATION App1 CASCADE;\nCREATE APPLICATION App1;\nCREATE FLOW AgentFlow;\nCREATE OR REPLACE SOURCE App1_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App1_SampleStream;\nEND FLOW AgentFlow;\nCREATE FLOW ServerFlow;\nCREATE OR REPLACE TARGET App1_NullTarget using NullWriter()\nINPUT FROM App1_SampleStream;\nEND FLOW ServerFlow;\nEND APPLICATION App1;\ndeploy application App1 on any in ServerDG1 with AgentFlow on any in Agents, ServerFlow on any in ServerDG1;\nSTART APPLICATION App1;\n\nSTOP APPLICATION App2;\nUNDEPLOY APPLICATION App2;\nDROP APPLICATION App2 CASCADE;\nCREATE APPLICATION App2;\nCREATE FLOW AgentFlow2;\nCREATE OR REPLACE SOURCE App2_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App2_SampleStream;\nEND FLOW AgentFlow2;\nCREATE FLOW ServerFlow2;\nCREATE OR REPLACE TARGET App2_NullTarget using NullWriter()\nINPUT FROM App2_SampleStream;\nEND FLOW ServerFlow2;\nEND APPLICATION App2;\ndeploy application App2 on any in ServerDG1 with AgentFlow2 on any in Agents, ServerFlow2 on any in ServerDG1;\nSTART APPLICATION App2;\n\nSTOP APPLICATION App3;\nUNDEPLOY APPLICATION App3;\nDROP APPLICATION App3 CASCADE;\nCREATE APPLICATION App3;\nCREATE OR REPLACE SOURCE App3_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App3_SampleStream;\nCREATE OR REPLACE TARGET App3_NullTarget using NullWriter()\nINPUT FROM App3_SampleStream;\nEND APPLICATION App3;\nDEPLOY APPLICATION App3;\nSTART APPLICATION App3;\n\nSTOP APPLICATION App4;\nUNDEPLOY APPLICATION App4;\nDROP APPLICATION App4 CASCADE;\nCREATE APPLICATION App4;\nCREATE OR REPLACE SOURCE App4_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App4_SampleStream;\nCREATE OR REPLACE TARGET App4_NullTarget using NullWriter()\nINPUT FROM App4_SampleStream;\nEND APPLICATION App4;\nDEPLOY APPLICATION App4 ON ONE IN ServerDG1;\nSTART APPLICATION App4;\n\nSTOP APPLICATION App5;\nUNDEPLOY APPLICATION App5;\nDROP APPLICATION App5 CASCADE;\nCREATE APPLICATION App5;\nCREATE OR REPLACE SOURCE App5_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App5_SampleStream;\nCREATE OR REPLACE TARGET App5_NullTarget using NullWriter()\nINPUT FROM App5_SampleStream;\nEND APPLICATION App5;\nDEPLOY APPLICATION App5 ON ONE IN ServerDG1;\nSTART APPLICATION App5;\n\nSTOP APPLICATION App6;\nUNDEPLOY APPLICATION App6;\nDROP APPLICATION App6 CASCADE;\nCREATE APPLICATION App6;\nCREATE OR REPLACE SOURCE App6_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App6_SampleStream;\nCREATE OR REPLACE TARGET App6_NullTarget using NullWriter()\nINPUT FROM App6_SampleStream;\nEND APPLICATION App6;\nDEPLOY APPLICATION App6 ON ONE IN ServerDG1;\nSTART APPLICATION App6;\n\nSTOP APPLICATION App7;\nUNDEPLOY APPLICATION App7;\nDROP APPLICATION App7 CASCADE;\nCREATE APPLICATION App7;\nCREATE OR REPLACE SOURCE App7_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App7_SampleStream;\nCREATE OR REPLACE TARGET App7_NullTarget using NullWriter()\nINPUT FROM App7_SampleStream;\nEND APPLICATION App7;\nDEPLOY APPLICATION App7 ON ONE IN ServerDG1;\nSTART APPLICATION App7;\n\nSTOP APPLICATION App8;\nUNDEPLOY APPLICATION App8;\nDROP APPLICATION App8 CASCADE;\nCREATE APPLICATION App8;\nCREATE OR REPLACE SOURCE App8_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App8_SampleStream;\nCREATE OR REPLACE TARGET App8_NullTarget using NullWriter()\nINPUT FROM App8_SampleStream;\nEND APPLICATION App8;\nDEPLOY APPLICATION App8 ON ONE IN ServerDG1;\nSTART APPLICATION App8;\n\n\nSTOP APPLICATION App9;\nUNDEPLOY APPLICATION App9;\nDROP APPLICATION App9 CASCADE;\nCREATE APPLICATION App9;\nCREATE OR REPLACE SOURCE App9_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App9_SampleStream;\nCREATE OR REPLACE TARGET App9_NullTarget using NullWriter()\nINPUT FROM App9_SampleStream;\nEND APPLICATION App9;\nDEPLOY APPLICATION App9;\nSTART APPLICATION App9;\n\nSTOP APPLICATION App10;\nUNDEPLOY APPLICATION App10;\nDROP APPLICATION App10 CASCADE;\nCREATE APPLICATION App10;\nCREATE OR REPLACE SOURCE App10_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App10_SampleStream;\nCREATE OR REPLACE TARGET App10_NullTarget using NullWriter()\nINPUT FROM App10_SampleStream;\nEND APPLICATION App10;\nDEPLOY APPLICATION App10;\nSTART APPLICATION App10;\n\nSTOP APPLICATION App11;\nUNDEPLOY APPLICATION App11;\nDROP APPLICATION App11 CASCADE;\nCREATE APPLICATION App11;\nCREATE OR REPLACE SOURCE App11_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App11_SampleStream;\nCREATE OR REPLACE TARGET App11_NullTarget using NullWriter()\nINPUT FROM App11_SampleStream;\nEND APPLICATION App11;\nDEPLOY APPLICATION App11;\nSTART APPLICATION App11;\n\nSTOP APPLICATION App12;\nUNDEPLOY APPLICATION App12;\nDROP APPLICATION App12 CASCADE;\nCREATE APPLICATION App12;\nCREATE OR REPLACE SOURCE App12_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App12_SampleStream;\nCREATE OR REPLACE TARGET App12_NullTarget using NullWriter()\nINPUT FROM App12_SampleStream;\nEND APPLICATION App12;\nDEPLOY APPLICATION App12;\nSTART APPLICATION App12;\n\nSTOP APPLICATION App13;\nUNDEPLOY APPLICATION App13;\nDROP APPLICATION App13 CASCADE;\nCREATE APPLICATION App13;\nCREATE FLOW AgentFlow13;\nCREATE OR REPLACE SOURCE App13_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App13_SampleStream;\nEND FLOW AgentFlow13;\nCREATE FLOW ServerFlow13;\nCREATE OR REPLACE TARGET App13_NullTarget using NullWriter()\nINPUT FROM App13_SampleStream;\nEND FLOW ServerFlow13;\nEND APPLICATION App13;\ndeploy application App13 on any in ServerDG1 with AgentFlow13 on any in Agents, ServerFlow13 on any in ServerDG1;\nSTART APPLICATION App13;", "generated_queries": "1. What are the steps and configurations required to deploy and start multiple applications (App1, App2, ..., App13) in a server environment using TQL?\n\n2. How can I define and manage various flows (AgentFlow, ServerFlow, AgentFlow2, ServerFlow2, ..., AgentFlow13, ServerFlow13) for different applications in a data processing pipeline using TQL?\n\n3. Can you provide a detailed example of how to set up file sources, parsers, streams, and targets for multiple applications (App1, App2, ..., App13) in TQL to process CSV data files in a server environment?", "file_name": "DGAgent.tql"}
{"tql": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n\nCREATE SOURCE @APPNAME@_S USING MSSqlReader\n(\n  Compression: false,\n  cdcRoleName: 'STRIIM_READER',\n  DatabaseName: 'QATEST',\n  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',\n  ConnectionPoolSize: 1,\n  FetchTransactionMetadata: false,\n  StartPosition: 'EOF',\n  Username: 'qatest',\n  SendBeforeImage: true,\n  AutoDisableTableCDC: true,\n  ConnectionURL: 'localhost:1433',\n  Tables: 'qatest.test01',\n  adapterName: 'MSSqlReader',\n  Password: 'w3b@ct10n'\n)\nOUTPUT TO @APPNAME@_SS;\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'NULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:0',\nStandardSQL:true\t,\noptimizedMerge:true\t\n) INPUT FROM @APPNAME@_ss;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\n--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\nSTART APPLICATION @APPNAME@;", "generated_queries": "1. How can I stop, undeploy, drop, create, and deploy an application named \"AppName\" with specific configurations and sources from MSSql and targets to BigQuery using TQL?\n   \n2. Can you show me the TQL query to create an application named \"AppName\" with a source from MSSql database table \"qatest.test01\", and output this data to a BigQuery table in dataset \"@DATASET@\" with specific mappings and merge settings?\n   \n3. What TQL query should I use to start an application named \"AppName\" after configuring it to use AgentFlow in Agents and ServerFlow in default mode with the specified source and target configurations?", "file_name": "mssql_bq_optimizedMerge_diff_PK.tql"}
{"tql": "--\n-- Crash Recovery Test 2 on two node cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS\n--\n\nSTOP APPLICATION N2S2CR2Tester.N2S2CRTest2;\nUNDEPLOY APPLICATION N2S2CR2Tester.N2S2CRTest2;\nDROP APPLICATION N2S2CR2Tester.N2S2CRTest2 CASCADE;\nCREATE APPLICATION N2S2CRTest2 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN2S2CRTest2;\n\nCREATE SOURCE CsvSourceN2S2CRTest2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN2S2CRTest2;\n\nCREATE FLOW DataProcessingN2S2CRTest2;\n\nCREATE TYPE WactionTypeN2S2CRTest2 (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionTypeN2S2CRTest2;\n\nCREATE CQ CsvToWaction\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsN2S2CRTest2 CONTEXT OF WactionTypeN2S2CRTest2\nEVENT TYPES ( WactionTypeN2S2CRTest2 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsN2S2CRTest2\nINSERT INTO WactionsN2S2CRTest2\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND FLOW DataProcessingN2S2CRTest2;\n\nEND APPLICATION N2S2CRTest2;", "generated_queries": "1. Which command is used to stop and undeploy the N2S2CRTest2 application on a two-node cluster in the specified TQL query?\n2. How is the data acquisition flow defined for the N2S2CRTest2 application in the provided TQL query?\n3. What event types are specified for the WactionStore creation in the N2S2CRTest2 application as per the TQL query?", "file_name": "N2S2CRTest2.tql"}
{"tql": "stop application AzureApp;\nundeploy application AzureApp;\ndrop application AzureApp cascade;\n\ncreate application AzureApp\nRECOVERY 10 second interval;\ncreate source CSVSource using FileReader (\n\tdirectory:'@DIR@',\n\tWildCard:'@WILDCARD@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  curr String,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       data[6],\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target BlobT using AzureBlobWriter(\n\taccountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:30,interval:5s'\n)\nformat using AvroFormatter (\n)\ninput from TypedCSVStream;\nend application AzureApp;\ndeploy application AzureApp in default;\nstart application AzureApp;", "generated_queries": "1. Can you provide the step-by-step deployment process for the AzureApp application, including data source setup, stream parsing, type creation, and target configuration?\n\n2. How is data transformation performed for the AzureApp application, specifically converting CSV data to a structured format with defined fields like merchant ID, date/time, currency, amount, and zip code?\n\n3. What are the settings and policies configured for streaming data from a CSV source to an Azure Blob storage using an Avro formatter within the AzureApp application deployment?", "file_name": "FileWAzurewithAvro.tql"}
{"tql": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ recovery 1 second interval;\n\nCREATE SOURCE @SOURCENAME@ USING OracleReader\n(\n    Username: '@SRCUSERNAME@',\n    Password: '@SRCPASSWORD@',\n    ConnectionURL: '@SRCURL',\n    Tables: '@SRCTABLE',\n    FetchSize: '@FETCHSIZE@',\n    CommittedTransactions: true\n)\n\nOUTPUT TO @STREAM@ ;\n\nCREATE TARGET @TARGETNAME@ using DatabaseWriter\n(\n    ConnectionURL: '@TARGETURL',\n    username: '@TARGETUSERNAME@',\n    Password: '@TARGETPASSWORD@',\n    Tables: '@TARGETTABLE@',\n    BatchPolicy:'EventCount:1,Interval:1',\n    CommitPolicy:'EventCount:1,Interval:1'\n)\nINPUT FROM @STREAM@;\n\nEND APPLICATION @APPNAME@;\n\nDEPLOY APPLICATION @APPNAME@;\nSTART APPLICATION @APPNAME@;", "generated_queries": "1. How can I stop, undeploy, drop, create, and deploy a specific application in a TQL script with recovery settings, specified data sources, and targets, and then start the application?\n\n2. What are the steps involved in managing an application in TQL, including halting its operation, removing it, creating it with recovery settings, defining data sources and targets, and then restarting it?\n\n3. Can you provide a detailed TQL script showing how to configure an application with specific recovery settings, data sources from Oracle, and a database target for writing outputs in small batches with a given interval?", "file_name": "OracleTemplate.tql"}
{"tql": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@ recovery 5 second Interval;\ncreate source @srcName@ USING MySQLReader\n(\n  Username:'@srcusername@',\n  Password:'@srcpassword@',\n  ConnectionURL:'@srcurl@',\n  Tables:'@srcschema@.@srctable@',\n  sendBeforeImage:'true',\n  FilterTransactionBoundaries:'true'\n) \nOUTPUT TO @outstreamname@;\n\nCREATE OR REPLACE TARGET @tgtName@ USING DatabaseWriter\n(\n  CheckPointTable:'CHKPOINT',\n  Username:'@tgtusername@',\n  Password:'@tgtpassword@',\n  BatchPolicy:'EventCount:1,Interval:0',\n  CommitPolicy:'EventCount:1,Interval:0',\n  ConnectionURL:'@tgturl@',\n  Tables:'@srcschema@.@srctable@,@tgtschema@.@tgttable@'\n) \nINPUT FROM @instreamname@;\n\n\nEnd APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;", "generated_queries": "1. How can I create a new application named \"@AppName@\" with a recovery interval of 5 seconds, reading data from a MySQL source with specific credentials and outputting to a specified stream, and writing to a database target with specific credentials and table mappings?\n2. Can you provide me with the TQL commands to stop, undeploy, drop, and redeploy an application named \"@AppName@\"?\n3. What are the steps needed to create a new namespace, switch to it, and start an application named \"@AppName@\" that reads data from a MySQL source and writes it to a specified database target in real time?", "file_name": "mysqltomysql.tql"}
{"tql": "CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( \n  Username: '@SRC_USERNAME@',\n  Password: '@SRC_PASSWORD@',\n  ConnectionURL: '@CDC_URL@',\n  Tables: '@Source1Tables@',\n  FetchSize: 1) \nOUTPUT TO @APPNAME@cdcStream;", "generated_queries": "1. What is the process for creating a change data capture (CDC) reader in the application, including specifying the username, password, connection URL, tables to monitor, and fetch size?\n2. How can I set up a continuous stream of data changes from specific tables in a database, ensuring the CDC reader fetches updates with a specific fetch size of 1?\n3. Can you explain the configuration needed to establish a CDC stream from a source by defining the connection parameters, tables of interest, and the output destination in the application?", "file_name": "DBWriter_Alter_Oracle.tql"}
{"tql": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@ RECOVERY 1 SECOND INTERVAL;\nCREATE SOURCE @srcName@ USING OracleReader (\n Username: '@srcusername@',\n  Password: '@srcpassword@',\n  ConnectionURL: '@srcurl@',\n  Tables: '@srcschema@.@srctable@'\n)\nOUTPUT TO @outstreamname@;\n\nCREATE OR REPLACE TARGET @tgtName@ USING DatabaseWriter\n(\n  DatabaseProviderType:'Default',\n  CheckPointTable:'CHKPOINT',\n  PreserveSourceTransactionBoundary:'false',\n  Username:'@tgtusername@',\n  BatchPolicy:'EventCount:1,Interval:0',\n  CommitPolicy:'EventCount:1,Interval:0',\n  ConnectionURL:'@tgturl@',\n  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',\n  Password:'@tgtpassword@'\n)\nINPUT FROM @instreamname@;\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;", "generated_queries": "1. How can I create a new data processing application in the TQL environment with a source connection to an Oracle database and a target connection to a database writer, specifying various configuration details such as table mappings and batch policies?\n\n2. What steps are involved in undeploying and redeploying an application named @AppName@ in the TQL environment, including stopping the application, dropping the existing application, and recreating it with recovery settings and source to target streaming configurations?\n\n3. Can you provide a detailed guide on setting up a data pipeline in TQL that transfers data from a specific Oracle table named @srctable@ to a target database table named @tgttable@, utilizing different connection credentials and checkpoint configurations for data reliability and integrity?", "file_name": "oracletosqlserver.tql"}
{"tql": "CREATE TARGET @TARGET_NAME@ USING SpannerWriter (\n\tTables: 'QATEST.%,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tinstanceId: 'qatest'\n) INPUT FROM @STREAM@;", "generated_queries": "1. What is the process for creating a target for data ingestion using SpannerWriter in TQL?\n2. How can I set up a target named \"@TARGET_NAME@\" to write data to specific tables in the \"QATEST\" database and \"testdb.test\" table in a Spanner database named \"qatest\"?\n3. Can you provide a step-by-step guide on configuring a target in TQL for writing data from a data stream using the SpannerWriter module?", "file_name": "SpannerWriter.tql"}
{"tql": "--\n-- Recovery Test 37 with two sources, two jumping time windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jt1W/p -> CQ1 -> WS\n--   S2 -> Jt2W/p -> CQ2 -> WS\n--\n\nSTOP KStreamRecov37Tester.KStreamRecovTest37;\nUNDEPLOY APPLICATION KStreamRecov37Tester.KStreamRecovTest37;\nDROP APPLICATION KStreamRecov37Tester.KStreamRecovTest37 CASCADE;\n\nDROP USER KStreamRecov37Tester;\nDROP NAMESPACE KStreamRecov37Tester CASCADE;\nCREATE USER KStreamRecov37Tester IDENTIFIED BY KStreamRecov37Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov37Tester;\nCONNECT KStreamRecov37Tester KStreamRecov37Tester;\n\nCREATE APPLICATION KStreamRecovTest37 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP WITHIN 1 SECOND\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP WITHIN 2 SECOND\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nEND APPLICATION KStreamRecovTest37;", "generated_queries": "1. How can I create a recovery test in TQL that involves two data sources, two jumping time windows, and a single wactionstore all partitioned on the same key?\n2. How can I set up a TQL query to convert CSV data from two different sources into structured data streams, apply jumping time windows on them, and store the aggregated results in a wactionstore?\n3. Can you provide an example of creating continuous queries in TQL to map CSV data to structured data streams, process them using jumping time windows, and store the summarized data in a wactionstore?", "file_name": "KStreamRecovTest37.tql"}
{"tql": "--\n-- Recovery Test 23 with two sources, two sliding time windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> St1W -> CQ1 -> WS\n-- S2 -> St2W -> CQ2 -> WS\n--\n\nSTOP KStreamRecov23Tester.KStreamRecovTest23;\nUNDEPLOY APPLICATION KStreamRecov23Tester.KStreamRecovTest23;\nDROP APPLICATION KStreamRecov23Tester.KStreamRecovTest23 CASCADE;\nDROP USER KStreamRecov23Tester;\nDROP NAMESPACE KStreamRecov23Tester CASCADE;\nCREATE USER KStreamRecov23Tester IDENTIFIED BY KStreamRecov23Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov23Tester;\nCONNECT KStreamRecov23Tester KStreamRecov23Tester;\n\nCREATE APPLICATION KStreamRecovTest23 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP WITHIN 1 SECOND;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP WITHIN 2 SECOND;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2;\n\nEND APPLICATION KStreamRecovTest23;", "generated_queries": "1. How can I set up a recovery test with two sources, two sliding time windows, and one wactionstore without any partitioning in TQL?\n2. What are the steps to create a TQL application for a recovery test with a 5-second interval using CSV data sources and a Kafka stream?\n3. How can I define and process CSV data streams into specific data fields using TQL, within the context of a time-based window for analysis and transformation into wactions?", "file_name": "KStreamRecovTest23.tql"}
{"tql": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@ recovery 1 second interval;\n\ncreate source @APPNAME@_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using CobolCopybookParser (\ncopybookFileName : '@TD@/@PROP1@',\n  dataFileFont: '@PROP2@',\n  copybookSplit: '@PROP3@',\n  dataFileOrganization: '@PROP4@',\n  copybookDialect: '@PROP5@', \n  skipIndent:'@PROP6@',\n  DatahandlingScheme:'@PROP7@'\n  --recordSelector: '@PROP8@'\n)\nOUTPUT TO @APPNAME@Stream;\n\ncreate Target @APPNAME@Target using FileWriter(\n    filename :'@FILE@',\n    directory : '@FOLDER@'\n)\nformat using JsonFormatter (\n)\ninput from @APPNAME@Stream;\n\nCREATE TYPE test_type (\n account_no com.fasterxml.jackson.databind.JsonNode,\n first_name com.fasterxml.jackson.databind.JsonNode,\n last_name com.fasterxml.jackson.databind.JsonNode,\n addr1 com.fasterxml.jackson.databind.JsonNode,\nAddr2 com.fasterxml.jackson.databind.JsonNode,\nCity com.fasterxml.jackson.databind.JsonNode,\nState com.fasterxml.jackson.databind.JsonNode,\nZip com.fasterxml.jackson.databind.JsonNode\n);\n\nCreate stream cqAsJSONNodeStream of test_type;\n\nCREATE CQ GetPOAsJsonNodes\nINSERT into cqAsJSONNodeStream\n    select \n    data.get('ACCTS-RECORD').get('ACCOUNT-NO'),\ndata.get('ACCTS-RECORD').get('NAME').get('FIRST-NAME'),\ndata.get('ACCTS-RECORD').get('NAME').get('LAST-NAME'),\ndata.get('ACCTS-RECORD').get('ADDRESS1'),\ndata.get('ACCTS-RECORD').get('ADDRESS2'),\ndata.get('ACCTS-RECORD').get('ADDRESS3').get('CITY'),\ndata.get('ACCTS-RECORD').get('ADDRESS3').get('STATE'),\ndata.get('ACCTS-RECORD').get('ADDRESS3').get('ZIP-CODE')\nfrom @APPNAME@Stream js;\n\ncreate type finaldtype(\n      ACCOUNT_NO String, \n      FIRST_NAME String,\n      LAST_NAME String,\n      ADDRESS1 String,\n      ADDRESS2 String,\n      CITY String,\n      STATE String,\n      ZIP_CODE String\n);\n\nCREATE CQ getdata\nINSERT into getdataStream\n    select account_no.toString(),\n    first_name.toString(),\n    last_name.toString(),\n    addr1.toString(),\n    Addr2.toString(),\n    City.toString(),\n    State.toString(),\n    Zip.toString()\nfrom cqAsJSONNodeStream x;\n\ncreate Target @APPNAME@DBTarget using DatabaseWriter(\n  Username: 'qatest',\n  Password: 'qatest',\n  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',\n  BatchPolicy: 'EventCount:10000',\n  CommitPolicy: 'EventCount:10000',\n  Tables: 'QATEST.@APPNAME@'\n)\ninput from getdataStream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@ on all in default;\nstart application @APPNAME@;", "generated_queries": "1. How can I create an application named \"@APPNAME@\" with a recovery interval of 1 second, source data from a directory \"@DIRECTORY@\" with a wildcard \"@FILENAME@\", and parse it using a Cobol Copybook Parser for further processing?\n  \n2. How can I extract specific data fields like account number, first name, last name, address, city, state, and zip code from a stream of JSON data processed by a complex query named \"GetPOAsJsonNodes\" and convert them into a stream of a custom type \"finaldtype\" for database storage using an Oracle Database Writer?\n\n3. What steps are involved in stopping, undeploying, dropping, creating, parsing, transforming, and storing data from a source file in a specific format into a database table using an application named \"@APPNAME@\" in a Oracle database environment?", "file_name": "cobolparserACCT.tql"}
{"tql": "stop application AzureDLSGen1_sanity;\nundeploy application AzureDLSGen1_sanity;\ndrop application AzureDLSGen1_sanity cascade;\n\n\ncreate application AzureDLSGen1_sanity recovery 5 Second interval;\ncreate source CSVSource using FileReader (\n        directory:'./Samples/AppData/',\n        WildCard:'dynamicdirectory.csv',\n        positionByEOF:false,\n        charset:'UTF-8'\n)\nparse using DSVParser (\n        header:'no'\n)\nOUTPUT TO CsvStream;\n\ncreate Target WriteToADLSGen1 using ADLSGen1Writer(\n        filename:'',\n        directory:'',\n        datalakestorename:'',\n        clientid:'',\n        authtokenendpoint:'',\n        clientkey:'',\n\t\trolloverpolicy:'eventcount:100000'\n)\nformat using DSVFormatter (\n)\ninput from CsvStream; \n\nend application AzureDLSGen1_sanity;\n\ndeploy application AzureDLSGen1_sanity;\nstart application AzureDLSGen1_sanity;", "generated_queries": "1. What actions are being taken to set up a data pipeline named AzureDLSGen1_sanity in Talend Real-Time Big Data Platform?\n  \n2. Can you provide details on the configuration of the source and target components used in the AzureDLSGen1_sanity application in Talend Real-Time Big Data Platform?\n\n3. How can I start and deploy the AzureDLSGen1_sanity data pipeline in Talend Real-Time Big Data Platform after creating it with specific recovery settings and utilizing CSV and ADLSGen1 components?", "file_name": "AzureDLS_GenRecovery.tql"}
{"tql": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\ncreate application @appname@ recovery 1 second interval;\n\nCREATE SOURCE @parquetsrc@ USING FileReader (\n  directory: '',\n  positionByEOF: false,\n  WildCard: '' )\nPARSE USING ParquetParser (\n )\nOUTPUT TO @appname@Streams;\n\nCREATE OR REPLACE CQ @appname@CQOrder3\nINSERT INTO @appname@Stream3\nSELECT\nPUTUSERDATA(s,'schemaName',s.data.getSchema().getName())\nFROM @appname@Streams s;\n\nCREATE TARGET @adlstarget@ USING Global.ADLSGen2Writer (\n    accountname:'',\n  \tsastoken:'',\n  \tfilesystemname:'',\n  \tfilename:'',\n  \tdirectory:'',\n  \tuploadpolicy:'eventcount:10' )\n\nformat using AvroFormatter (\n)\nINPUT FROM @appname@Stream3;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;", "generated_queries": "1. How can I set up a data pipeline to read Parquet files, process the data using a custom schema, and write the results to Azure Data Lake Storage Gen2 in near real-time?\n   \n2. Can you show me the commands to stop, undeploy, and drop a streaming application named \"@appname@\" with a recovery mechanism set to 1 second interval?\n\n3. What are the steps to create a continuous query in the streaming application \"@appname@\" that extracts schema information and then inserts the modified data into another stream named \"@appname@Stream3\"?", "file_name": "FileParquetToGen2Avro.tql"}
{"tql": "stop ORAToBigquery;\nundeploy application ORAToBigquery;\ndrop application ORAToBigquery cascade;\n\nCREATE APPLICATION ORAToBigquery RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE Rac11g USING OracleReader ( \n  SupportPDB: false,\n  SendBeforeImage: true,\n  ReaderType: 'LogMiner',\n  CommittedTransactions: false,\n  FetchSize: 1,\n  Password: 'manager',\n  DDLTracking: false,\n  StartTimestamp: 'null',\n  OutboundServerProcessName: 'WebActionXStream',\n  OnlineCatalog: true,\n  ConnectionURL: '192.168.33.10:1521/XE',\n  SkipOpenTransactions: false,\n  Compression: false,\n  QueueSize: 40000,\n  RedoLogfiles: 'null',\n  Tables: 'SYSTEM.GGAUTHORIZATIONS',\n  Username: 'system',\n  FilterTransactionBoundaries: true,\n  adapterName: 'OracleReader',\n  XstreamTimeOut: 600,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=60, maxRetries=3'\n ) \nOUTPUT TO DataStream;\n\nCREATE OR REPLACE TARGET Target1 USING SysOut ( \n  name: \"dstream\"\n ) \nINPUT FROM DataStream;\n\nCREATE OR REPLACE TARGET Target2 using BigqueryWriter(\n  BQServiceAccountConfigurationPath:\"/Users/ravipathak/Downloads/big-querytest-1963ae421e90.json\",\n  projectId:\"big-querytest\",\n  Tables: \"SYSTEM.GGAUTHORIZATIONS,testing1.ggauthorisation\",\n  parallelismCount: 2,\n  BatchPolicy: \"eventCount:100000,Interval:0\")\nINPUT FROM DataStream;\n\nEND APPLICATION ORAToBigquery;\n\ndeploy application ORAToBigquery;\nstart ORAToBigquery;", "generated_queries": "1. How can I deploy and start an application called ORAToBigquery which reads data from an Oracle database and writes it to BigQuery using specific configurations?\n\n2. What are the steps to undeploy, drop, and redeploy an application named ORAToBigquery with a recovery interval of 5 seconds, reading data from an Oracle database and writing it to BigQuery with certain settings?\n\n3. Can you provide the TQL commands required to create an application named ORAToBigquery that streams data from an Oracle database table called SYSTEM.GGAUTHORIZATIONS and writes it to BigQuery tables SYSTEM.GGAUTHORIZATIONS and testing1.ggauthorisation in parallel with specific batch policies and configurations?", "file_name": "template.tql"}
{"tql": "CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING DatabaseReader (\n  Tables: '',\n  ConnectionURL: '',\n  Password: '',\n  Username: ''\n  )\nOUTPUT TO @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nEND APPLICATION @APPNAME@;", "generated_queries": "1. How can I create or replace an application named \"@APPNAME@\" with a specified recovery configuration?\n2. How do I set up a source to read data from a database for the application named \"@APPNAME@\"?\n3. What steps are needed to configure a target to write data to an S3 bucket using JSON formatting for the application named \"@APPNAME@\"?", "file_name": "OracleILToS3.tql"}
{"tql": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@ Using DatabaseReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'qatest.@SourceTable@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;", "generated_queries": "1. How can I undeploy, drop, and redeploy an application named \"@APPNAME@\" in a TQL script while ensuring that it has a recovery interval of 5 seconds?\n  \n2. In a TQL script, how can I create a source named \"@SourceName@\" using a DatabaseReader with specific connection details, and then output the data to an input stream named \"@SRCINPUTSTREAM@\" where the data will be read by both a DatabaseWriter and a SysOut target?\n\n3. Could you provide an example of a TQL script that sets up real-time data processing for tables in a database, ensuring that every event is immediately processed and written to specified tables in the same database while also printing the data to an output system named \"Foo2\"?", "file_name": "PostgresDBR.tql"}
{"tql": "stop ADW;\nundeploy application ADW;\nDROP APPLICATION ADW CASCADE;\nCREATE APPLICATION ADW recovery 5 second interval;\nCreate Source OracleSource Using OracleReader\n(\n Username:'@ORACLE-USERNAME',\n Password:'@ORACLE-PASSWORD',\n ConnectionURL: '@ORACLE-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'@FETCH-SIZE@'\n) \nOutput To str;\n\n\ncreate target AzureTarget using AzureSQLDWHWriter (\n\t\tConnectionURL: '@SQLDW-URL@',\n        username: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@',\n        uploadpolicy:'@EVENT-COUNT@'\n) INPUT FROM str;\n\ncreate Target t2 using SysOut(name:Foo2) input from str;\n\nEND APPLICATION ADW;\ndeploy application ADW;\nstart application ADW;", "generated_queries": "1. How can I set up a real-time data replication pipeline from an Oracle database to an Azure SQL Data Warehouse with a recovery interval of 5 seconds?\n2. What are the steps to undeploy, drop, and redeploy an application named ADW that involves replicating data from an Oracle source to an Azure SQL Data Warehouse target?\n3. Can you provide details on the target tables and upload policy specified in the TQL query for the data replication application named ADW?", "file_name": "OracleToAzure.tql"}
{"tql": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCreate Source @SourceName1@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM1@;\nCreate Source @SourceName2@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM2@;\nCreate Source @SourceName3@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM3@;\nCreate Source @SourceName4@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM4@;\nCreate Source @SourceName5@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM5@;\nCreate Source @SourceName6@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM6@;\nCreate Source @SourceName7@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM7@;\nCreate Source @SourceName8@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM8@;\nCreate Source @SourceName9@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM9@;\nCreate Source @SourceName10@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM10@;\n\nCREATE TARGET @targetName1@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM1@;\nCREATE TARGET @targetName2@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM2@;\nCREATE TARGET @targetName3@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM3@;\nCREATE TARGET @targetName4@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM4@;\nCREATE TARGET @targetName5@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM5@;\nCREATE TARGET @targetName6@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM6@;\nCREATE TARGET @targetName7@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM7@;\nCREATE TARGET @targetName8@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM8@;\nCREATE TARGET @targetName9@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM9@;\nCREATE TARGET @targetName10@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM10@;\n\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;", "generated_queries": "1. What are the steps involved in creating and deploying an application named \"@APPNAME@\" using Ojet for data ingestion, processing, and writing to an Oracle database table using DatabaseWriter in real-time?\n  \n2. How can I configure 10 different sources and targets for data integration within an application named \"@APPNAME@\" where the data is fetched from an Oracle database using Ojet and written to an Oracle table with a batch policy of 'EventCount:1, Interval:1' through DatabaseWriter?\n\n3. What is the process to start, deploy, and manage real-time data integration flows in an application named \"@APPNAME@\" to ensure continuous ingestion and processing of data from multiple sources to multiple targets?", "file_name": "OjetALMMultipleReader.tql"}
{"tql": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@ Using PostgreSQLReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'qatest.@SourceTable@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;", "generated_queries": "1. How can I set up an application named \"@APPNAME@\" that reads data from a PostgreSQL database table \"qatest.@SourceTable@\" and writes it to two different target tables \"qatest.@SourceTable@\" and \"qatest.@TargetTable@\"?\n   \n2. What is the recovery interval set for the application \"@APPNAME@\" in the TQL script?\n   \n3. Can you provide the details of the source connection used in the TQL query to extract data for the application \"@APPNAME@\"?", "file_name": "PostgresCDC.tql"}
{"tql": "--\n-- Recovery Test T20\n-- Nicholas Keene, WebAction, Inc.\n--\n-- Snum -> CQ -> WS\n--\n\n\nUNDEPLOY APPLICATION NameT20.T20;\nDROP APPLICATION NameT20.T20 CASCADE;\nCREATE APPLICATION T20;\n\n\n\n\nCREATE FLOW DataAcquisitionT20;\n\n\nCREATE SOURCE CsvSourceT20 USING NumberSource ( \n  lowValue: '1',\n  highValue: '1003',\n  delayMillis: '10',\n  delayNanos: '0',\n  repeat: 'false'\n ) \nOUTPUT TO OutputStreamT20;\n\n\nEND FLOW DataAcquisitionT20;\n\n\n\n\nCREATE FLOW DataProcessingT20;\n\n\nCreate Target OutputTargetT20\nUsing Sysout (name: 'OutputTargetT20')\nInput From OutputStreamT20;\n\n\nEND FLOW DataProcessingT20;\n\n\n\nEND APPLICATION T20;", "generated_queries": "1. What is the process for deploying the application T20 and its associated flows for data acquisition and data processing?\n2. Can you provide details on the source and target components used in Flow DataAcquisitionT20 and Flow DataProcessingT20 within application T20?\n3. How can I undeploy the application NameT20.T20, drop it with cascading effects, and recreate it as application T20 using TQL commands?", "file_name": "T20.tql"}
{"tql": "stop application GGTrailReaderApp;\nundeploy application GGTrailReaderApp;\ndrop application GGTrailReaderApp cascade;\n\ncreate application GGTrailReaderApp recovery 5 second interval;\n\ncreate source GGTrailSource using GGTrailReader (\ntRaildIrectory:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario1',\ntRAilfilepattern:'n1*',\npositionByEOF:false,\nFilterTransactionBoundaries: true,\nDefinitionFile:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario1/Scn1_beforeddl.def',\ncaptureCDdl: true,\nCDDLAction:'Process',\n--CDDLAction:'Ignore',\nTrailByTeOrder:'LittleEndian',\nrecoveryInterval: 5\n)\nOUTPUT TO GGTrailStream;\n\ncreate Target t2 using SysOut(name:Foo2) input from GGTrailStream;\n\nCREATE TARGET WriteCDCOracle1 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost/orcl',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:1,Interval:1',\nCommitPolicy:'Eventcount:1,Interval:1',\nCheckpointtable:'RGRN_CHKPOINT',\nTables:'QATEST.GGDDL1,QATEST.GGDDL1_TGT'\n) INPUT FROM GGTrailStream1;\n\n\nend application GGTrailReaderApp;\n\ndeploy application GGTrailReaderApp;\nstart application GGTrailReaderApp;", "generated_queries": "1. How can I configure an application called GGTrailReaderApp to read data from a specific directory, apply capture DDL actions, and output to a stream called GGTrailStream with specified recovery intervals and target configurations?\n  \n2. What are the target configurations for writing CDC data to an Oracle database with specific connection details, batch and commit policies, checkpoint table settings, and target table selections within the GGTrailReaderApp application?\n\n3. Can you guide me on the steps required to stop, undeploy, drop, create, configure source and targets, and then deploy and start an application named GGTrailReaderApp for processing data streams using the TQL language syntax provided here?", "file_name": "scn1.tql"}
{"tql": "DROP APPLICATION ns1.OPExample cascade;\nDROP NAMESPACE ns1 cascade;\nCREATE OR REPLACE NAMESPACE ns1;\nUSE ns1;\nCREATE APPLICATION OPExample;\n\nCREATE source CsvDataSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'PosDataPreview.csv',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n)\nOUTPUT TO CsvStream;\n \nCREATE TYPE MerchantHourlyAve(\n  merchantId String,\n  hourValue integer,\n  hourlyAve integer\n);\n\nCREATE CACHE HourlyAveLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'hourlyData.txt'\n)\nPARSE USING DSVParser (\n  header: Yes,\n  trimquote:false,\n  trimwhitespace:true\n) \nQUERY (keytomap:'merchantId') \nOF MerchantHourlyAve;\n\nCREATE CQ CsvToPosData\nINSERT INTO PosDataStream partition by merchantId\nSELECT TO_STRING(data[1]) as merchantId,\n  TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\n  DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,\n  TO_DOUBLE(data[7]) as amount,\n  TO_INT(data[9]) as zip\nFROM CsvStream;\n \nCREATE CQ cq2\nINSERT INTO SendToOPStream\nSELECT makeList(dateTime) as dateTime,\n  makeList(zip) as zip\nFROM PosDataStream;\n \nCREATE TYPE ReturnFromOPStream_Type ( time DateTime , val Integer );\nCREATE STREAM ReturnFromOPStream OF ReturnFromOPStream_Type;\n\nCREATE TARGET OPExampleTarget \nUSING FileWriter (filename: 'OPExampleOut') \nFORMAT USING JSONFormatter() \nINPUT FROM ReturnFromOPStream;\n\nCREATE OPEN PROCESSOR testOp USING Global.TupleConverter ( lastItemSeen: 0, ahead: 1 )\nINSERT INTO ReturnFromOPStream FROM SendToOPStream ENRICH WITH HourlyAveLookup;\n \nEND APPLICATION OPExample;", "generated_queries": "1. Who are the merchants with the highest hourly average sales based on the data provided in the 'hourlyData.txt' file?\n   \n2. How can I set up a real-time data processing pipeline to convert CSV data from a file named 'PosDataPreview.csv' into a structured format and calculate average sales data by hour for each merchant?\n\n3. What are the steps involved in creating an application named 'OPExample' that processes CSV data, enriches it with hourly average lookup data, and outputs the results to a file named 'OPExampleOut' in JSON format?", "file_name": "op2.tql"}
{"tql": "create application FileXML;\ncreate source XMLSource using FileReader (\n\tDirectory:'@TEST-DATA-PATH@',\n\tWildCard:'books.xml',\n\tpositionByEOF:false\n)\nparse using XMLParser (\n\tRootNode:'/catalog/book'\n)\nOUTPUT TO XmlStream;\n\n-- Below Sysout is added to test DEV-23437.  Not directly validated in the test except the App should not crash with sysout target\nCREATE TARGET XMLEventSYSout USING sysout  (\nname: 'XMLEventSYSoutOut' )\nINPUT FROM XmlStream;\n\ncreate Target XMLDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/xmldata') input from XmlStream;\nend application FileXML;", "generated_queries": "1. How can I create an application in order to parse and output specific XML data from a file named \"books.xml\" located in a specified directory?\n   \n2. Can you provide a query that demonstrates how to set up a target to log XML data parsed from a specific root node within an XML file?\n\n3. What TQL commands would I use to create a source that reads an XML file and parses specific elements within it before sending the parsed data to different output targets such as a system output or a log file?", "file_name": "FileReaderWithXMLParser.tql"}
{"tql": "CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE OR REPLACE SOURCE @APPNAME@DataSrc USING OracleReader (\n  Tables: '',\n  ConnectionURL: '',\n  Password: '',\n  Username: ''\n)\nOUTPUT TO @APPNAME@DataStream;\n\nCREATE OR REPLACE TARGET @APPNAME@DataTrgt USING MongoDBWriter (\n  ConnectionURL: '',\n  Username: '',\n  Password: '',\n  collections: ''\n  AuthDB: '',\n  batchpolicy: 'EventCount:1000, Interval:30',\n )\nINPUT FROM @APPNAME@DataStream;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING MongoDBReader (\n  ConnectionURL: '',\n  Username: '',\n  password: '',\n  authDB: '',\n  collections: '',\n  mode: 'Incremental'\n  )\nOUTPUT TO @APPNAME@stream;\n\nCREATE CQ @APPNAME@CQ\nINSERT INTO @APPNAME@CQSTREAM\nSELECT data.get(\"NUM_COL\").toString() AS NUM_COL,\n  data.get(\"CHAR_COL\").toString() AS CHAR_COL,\n  data.get(\"VARCHAR2_COL\").toString() AS VARCHAR2_COL,\n  data.get(\"FLOAT_COL\").toString() AS FLOAT_COL,\n  data.get(\"BINARY_FLOAT_COL\").toString() AS BINARY_FLOAT_COL,\n  data.get(\"BINARY_DOUBLE_COL\").toString() AS BINARY_DOUBLE_COL,\n  data.get(\"DATE_COL\").toString() AS DATE_COL,\n  data.get(\"TIMESTAMP_COL\").toString() AS TIMESTAMP_COL\nFROM @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING AvroFormatter (\nschemaFileName: '@SCHEMAFILE@'\n)\nINPUT FROM @APPNAME@CQSTREAM;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING DSVFormatter ()\nINPUT FROM @APPNAME@CQSTREAM;\n\nEND APPLICATION @APPNAME@;", "generated_queries": "1. What is the process for creating a data pipeline application that reads data from an Oracle source, processes it using specified transformations, and writes the output to MongoDB and S3 targets in a structured format?\n\n2. How can incremental data loading be achieved from a MongoDB source to a target stream using transformation rules for specific columns in a data processing application?\n\n3. Can you provide details on setting up S3 output targets with different formatting options, such as Avro, JSON, and DSV, for an application that processes data streams and writes the transformed data to cloud storage?", "file_name": "MongoToS3MutliTargetsAvro.tql"}
{"tql": "stop application AzureApp;\nundeploy application AzureApp;\ndrop application AzureApp cascade;\n\ncreate application AzureApp\nRECOVERY 10 second interval;\nCREATE SOURCE OracleSource USING OracleReader\n(\n    Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n Tables:'@TABLES@',\n    FetchSize: 1\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  tablename String,\n  data java.util.HashMap  \n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT TO_LOWER(META(s, \"TableName\").toString()) as tablename,\n       DATA(s) as data\nFROM CsvStream s;\n\ncreate Target BlobT using AzureBlobWriter(\n\taccountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:5,interval:5s'\n)\nformat using AvroFormatter (\n)\ninput from TypedCSVStream;\nend application AzureApp;\ndeploy application AzureApp in default;\nstart application AzureApp;", "generated_queries": "1. What is the process for deploying an application named AzureApp that streams data from Oracle, processes it to convert to CSV format, and then saves it to an Azure Blob storage using Avro format?\n2. How can I configure a continuous query in TQL to convert CSV data streamed from Oracle tables into a specific Avro format and store it in an Azure Blob storage with specific upload policies?\n3. Can you provide the step-by-step instructions for setting up an application in TQL named AzureApp that pulls data from specified tables in an Oracle database, processes it into CSV format, transforms it into Avro format, and sends it to an Azure Blob container with specific configuration settings for event count and upload interval?", "file_name": "OracleAzurewithAvroLoad.tql"}
{"tql": "STOP APPLICATION @appname@routerApp;\nUNDEPLOY APPLICATION @appname@routerApp;\nDROP APPLICATION @appname@routerApp CASCADE;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',\n  acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\nCREATE APPLICATION @appname@routerApp RECOVERY 10 SECOND INTERVAL;\n\nCREATE  SOURCE @appname@OraSource USING OracleReader  (\nUsername: 'qatest',\nPassword: 'qatest',\nConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',\nTables: 'QATEST.TGT_T%',\n FetchSize:'100'\n)\nOUTPUT TO @appname@MasterStream1;\n\n-- CREATE STREAM @appname@ss1 OF Global.waevent persist using Global.DefaultKafkaProperties;\n-- CREATE STREAM @appname@ss2 OF Global.waevent persist using Global.DefaultKafkaProperties;\n-- CREATE STREAM @appname@ss3 OF Global.waevent persist using Global.DefaultKafkaProperties;\n\nCREATE STREAM @appname@ss1 OF Global.waevent PERSIST USING KafkaPropset;\nCREATE STREAM @appname@ss2 OF Global.waevent PERSIST USING KafkaPropset;\nCREATE STREAM @appname@ss3 OF Global.waevent PERSIST USING KafkaPropset;\n\nCREATE OR REPLACE ROUTER @appname@tablerouter1 INPUT FROM @appname@MasterStream1 s CASE\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T1' THEN ROUTE TO @appname@ss1,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T2' THEN ROUTE TO @appname@ss2,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T3' THEN ROUTE TO @appname@ss3,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T4' THEN ROUTE TO @appname@ss4,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T5' THEN ROUTE TO @appname@ss5,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T6' THEN ROUTE TO @appname@ss6,\nELSE ROUTE TO @appname@ss_else;\n\ncreate Target @appname@FileTarget_1 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n)\nFORMAT USING dsvFormatter ()\ninput from @appname@ss1;\n\ncreate Target @appname@FileTarget_2 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n)\nFORMAT USING dsvFormatter ()\ninput from @appname@ss2;\n\ncreate Target @appname@FileTarget_3 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n\n)\nFORMAT USING dsvFormatter ()\ninput from @appname@ss3;\n\nCREATE OR REPLACE TARGET @appname@KafkaTarget_4 USING KafkaWriter VERSION '0.11.0' (\n  brokerAddress: 'localhost:9092',\n  Topic: 'target4'\n )\nFORMAT USING JSONFormatter  (\n )\nINPUT FROM @appname@ss4;\n\nCREATE OR REPLACE TARGET @appname@KafkaTarget_5 USING KafkaWriter VERSION '0.11.0' (\n  brokerAddress: 'localhost:9092',\n  Topic: 'target5'\n )\nFORMAT USING JSONFormatter  (\n )\nINPUT FROM @appname@ss5;\n\nCREATE OR REPLACE TARGET @appname@KafkaTarget_6 USING KafkaWriter VERSION '0.11.0' (\n  brokerAddress: 'localhost:9092',\n  Topic: 'target6'\n )\nFORMAT USING JSONFormatter  (\n )\nINPUT FROM @appname@ss6;\n\n\n\n\nend application @appname@routerApp;\ndeploy application @appname@routerApp;\nstart @appname@routerApp;", "generated_queries": "1. How can I configure a Kafka property set with specific parameters like bootstrap brokers, batch size, and replication factor in my application called \"routerApp\"?\n2. Which tables from my Oracle database are being routed to different Kafka streams in the application \"routerApp\" based on their table name?\n3. What are the target destinations for the data coming from the Kafka streams ss4, ss5, and ss6 in the application \"routerApp\"?", "file_name": "routerToKafka.tql"}
{"tql": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;\n\nCreate Source @SourceName@ Using Ojet\n\n(\n  Username:'c##qatest',\n  Password:'qatest',\n  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',\n  Tables:'CDB$ROOT.\"C##QATEST\".ojet_src;ORCLPDB.QATEST.ojet_src',\n  _h_useClassic:false,\n  Fetchsize:1,\n  Compression: true,\n  SupportPDB:true,\n  ReplicationSlotName:'null'\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter\n(\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',\n  Username:'c##qatest',\n  Password:'qatest',\n  BatchPolicy:'EventCount:1,Interval:1',\n  Tables:'CDB$ROOT.\"C##QATEST\".ojet_src,CDB$ROOT.\"C##QATEST\".ojet_tgt'\n) INPUT FROM @SRCINPUTSTREAM@;\n\n\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;", "generated_queries": "1. What is the configuration for the data source connection used in application @APPNAME@?\n2. How many different targets are defined in application @APPNAME@ and what are their respective configurations?\n3. Can you provide details about the recovery settings for application @APPNAME@?", "file_name": "Ojet_DBWPDBCDB1.tql"}
{"tql": "CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING DataBaseReader (\n  Tables: '',\n  ConnectionURL: '',\n  Password: '',\n  Username: ''\n  )\nOUTPUT TO @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nEND APPLICATION @APPNAME@;", "generated_queries": "1. How can I create an application with recovery enabled to handle data processing tasks efficiently?\n2. How do I configure a source in my application to read data from a database using a specific connection URL, username, and password?\n3. What is the process for setting up two different S3 targets in my application to write data in JSON format with specific configurations for each?", "file_name": "OracleILToS3_2Targets.tql"}
{"tql": "create application KinesisTest;\ncreate source CSVSource using FileReader (\n\tdirectory:'/home/dz/src/product/Samples/AppData',\n\tWildCard:'posdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  companyName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0], data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate or replace Target t using KinesisWriter (\n\tregionName:'TARGET_REGION',\n\tstreamName:'TARGET_STREAM'\n)\nformat using DSVFormatter (\n)\ninput from TypedCSVStream;\nend application KinesisTest;\ndeploy application KinesisTest in default;\nstart application KinesisTest;", "generated_queries": "1. What is the total amount spent on purchases made by a specific company during the last hour?\n   \n2. How many transactions were recorded in the CSV file posdata.csv for a particular zip code?\n\n3. Can you provide a breakdown of the hourly sales amounts for a specific merchant ID within the given CSV data source?", "file_name": "KinesisTest.tql"}
{"tql": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE application @APPNAME@ @Recovery@ AUTORESUME MAXRETRIES 2 RETRYINTERVAL 10;\n\ncreate type @APPNAME@type1(\n  companyName String,\n  merchantId String,\n  city string\n);\n\ncreate type @APPNAME@type2(\n  c1 integer,\n  c2 String,\n  c3 string\n);\n\ncreate type @APPNAME@type3(\nc1 integer\n);\n\ncreate type @APPNAME@type4(\nc1 integer,\nc2 integer\n);\n\ncreate stream @APPNAME@in_memory_typedStream of @APPNAME@type1 partition by city;\ncreate stream @APPNAME@in_memory_typedStream_num of @APPNAME@type2;\ncreate stream @APPNAME@in_memory_typedStream_num1 of @APPNAME@type2;\ncreate stream @APPNAME@in_memory_typedStream_num2 of @APPNAME@type2;\ncreate stream @APPNAME@in_memory_typedStream_num3 of @APPNAME@type2;\ncreate stream @APPNAME@in_memory_typedStream_num4 of @APPNAME@type2;\ncreate stream @APPNAME@in_memory_typedStream_num5 of @APPNAME@type2;\ncreate stream @APPNAME@finalstream6 of @APPNAME@type4;\n\ncreate source @APPNAME@s using FileReader (\n        directory:'Product/IntegrationTests/TestData/',\n        wildcard:'posdata5L.csv',\n        positionByEOF:false,\n        Charset:'UTF-8'\n)\nPARSE USING DSVParser (\n  columndelimiter:',',\n  ignoreemptycolumn:'Yes',\n  quoteset:'[]~\"',\n  header: true,\n  separator:'~'\n\n)\nOUTPUT TO @APPNAME@in_memory_rawStream;\n\n\ncreate CQ @APPNAME@cq1\nINSERT INTO @APPNAME@kps_waevent\nSELECT *\nFROM @APPNAME@in_memory_rawStream  ;\n\ncreate CQ @APPNAME@cq2\nINSERT INTO @APPNAME@in_memory_typedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\"),\nTO_STRING(data[1]),\nTO_STRING(data[10])\nFROM @APPNAME@kps_waevent ;\n\ncreate CQ @APPNAME@cq3\nINSERT INTO @APPNAME@in_memory_typedStream_num1\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\nFROM @APPNAME@in_memory_typedStream;\n-- order by c3;\n\ncreate CQ @APPNAME@cq4\nINSERT INTO @APPNAME@in_memory_typedStream_num2\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\nFROM @APPNAME@in_memory_typedStream;\n\ncreate CQ @APPNAME@cq5\nINSERT INTO @APPNAME@in_memory_typedStream_num3\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\nFROM @APPNAME@in_memory_typedStream;\n\ncreate CQ @APPNAME@cq6\nINSERT INTO @APPNAME@in_memory_typedStream_num4\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\nFROM @APPNAME@in_memory_typedStream;\n\ncreate CQ @APPNAME@cq7\nINSERT INTO @APPNAME@in_memory_typedStream_num5\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\nFROM @APPNAME@in_memory_typedStream;\n\nCREATE CQ @APPNAME@cq8\nINSERT INTO @APPNAME@in_memory_typedStream_num6\nSELECT TO_INT(companyName) as c1\nFROM @APPNAME@in_memory_typedStream;\n\n\nCREATE JUMPING WINDOW @APPNAME@DataStream1_100000Rows\nOVER @APPNAME@in_memory_typedStream_num1 KEEP 100000 ROWS;\n\n\nCREATE JUMPING WINDOW @APPNAME@DataStream2_100000Rows\nOVER @APPNAME@in_memory_typedStream_num2 KEEP 100000 ROWS;\n\n\nCREATE JUMPING WINDOW @APPNAME@DataStream3_100000Rows\nOVER @APPNAME@in_memory_typedStream_num3 KEEP 100000 ROWS;\n\n\nCREATE JUMPING WINDOW @APPNAME@DataStream4_100000Rows\nOVER @APPNAME@in_memory_typedStream_num4 KEEP 100000 ROWS;\n\n\nCREATE JUMPING WINDOW @APPNAME@DataStream5_100000Rows\nOVER @APPNAME@in_memory_typedStream_num5 KEEP 100000 ROWS;\n\nCREATE JUMPING WINDOW @APPNAME@DataStream6_100000Rows\nOVER @APPNAME@in_memory_typedStream_num6 KEEP 100000 ROWS;\n\ncreate CQ @APPNAME@cq9\nINSERT INTO @APPNAME@finalstream1\nSELECT c1 FROM @APPNAME@DataStream1_100000Rows sample by c1;\n\ncreate CQ @APPNAME@cq10\nINSERT INTO @APPNAME@finalstream2\nSELECT c1 FROM @APPNAME@DataStream2_100000Rows sample by c1 selectivity 0.1;\n\ncreate CQ @APPNAME@cq11\nINSERT INTO @APPNAME@finalstream3\nSELECT c1 FROM @APPNAME@DataStream3_100000Rows sample by c1 selectivity 0.25;\n\n\ncreate CQ @APPNAME@cq12\nINSERT INTO @APPNAME@finalstream4\nSELECT c1 FROM @APPNAME@DataStream4_100000Rows sample by c1 selectivity 0.05;\n--SELECT count(*) FROM @APPNAME@DataStream4Rows10000Seconds sample by c1 selectivity 0.05;\n\ncreate CQ @APPNAME@cq13\nINSERT INTO @APPNAME@finalstream5\nSELECT c1 FROM @APPNAME@DataStream5_100000Rows sample by c1 selectivity 0.01;\n\ncreate CQ @APPNAME@cq14\nINSERT INTO @APPNAME@finalstream6\nSELECT c1,c1 as c2 FROM @APPNAME@DataStream6_100000Rows sample by c1,c2 selectivity 0.01;\n\ncreate target @APPNAME@target1 using filewriter (\nfilename:'FEATURE-DIR/logs/@APPNAME@target1.log',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000,sequence:00'\n)\nformat using dsvFormatter()\ninput from @APPNAME@finalstream1;\n\ncreate target @APPNAME@target2 using filewriter (\nfilename:'FEATURE-DIR/logs/@APPNAME@target2.log',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000,sequence:00'\n)\nformat using dsvFormatter()\ninput from @APPNAME@finalstream2;\n\ncreate target @APPNAME@target3 using filewriter (\nfilename:'FEATURE-DIR/logs/@APPNAME@target3.log',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000,sequence:00'\n)\nformat using dsvFormatter()\ninput from @APPNAME@finalstream3;\n\ncreate target @APPNAME@target4 using filewriter (\nfilename:'FEATURE-DIR/logs/@APPNAME@target4.log',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000,sequence:00'\n)\nformat using dsvFormatter()\ninput from @APPNAME@finalstream4;\n\ncreate target @APPNAME@target5 using filewriter (\nfilename:'FEATURE-DIR/logs/@APPNAME@target5.log',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000,sequence:00'\n)\nformat using dsvFormatter()\ninput from @APPNAME@finalstream5;\n\nCREATE WACTIONSTORE @APPNAME@Wactions1 CONTEXT OF @APPNAME@type3\nEVENT TYPES ( @APPNAME@type2 )\nUSING ( storageProvider:'elasticsearch' );\n\nCREATE WACTIONSTORE @APPNAME@Wactions2 CONTEXT OF @APPNAME@type3\nEVENT TYPES ( @APPNAME@type2 )\nUSING ( storageProvider:'elasticsearch' );\n\nCREATE WACTIONSTORE @APPNAME@Wactions3 CONTEXT OF @APPNAME@type3\nEVENT TYPES ( @APPNAME@type2 )\nUSING ( storageProvider:'elasticsearch' );\n\nCREATE WACTIONSTORE @APPNAME@Wactions4 CONTEXT OF @APPNAME@type4\nEVENT TYPES ( @APPNAME@type4 )\nUSING ( storageProvider:'elasticsearch' );\n\nCREATE WACTIONSTORE @APPNAME@Wactions5 CONTEXT OF @APPNAME@type4\nEVENT TYPES ( @APPNAME@type4 )\nUSING ( storageProvider:'elasticsearch' );\n\n--sampling twice: one in finalstream1 and another in select query.\nCREATE CQ @APPNAME@cq15\nINSERT INTO @APPNAME@Wactions1\nSELECT FIRST(p.c1) FROM @APPNAME@finalstream1 p GROUP BY p.c1 sample by p.c1 ;\n\n--sampling once: results will be same as target2 and target1.\nCREATE CQ @APPNAME@cq16\nINSERT INTO @APPNAME@Wactions2\nSELECT * from @APPNAME@finalstream1 order by c1 desc limit 10000 ;\n\n--sampling twice: one in finalstream1 and another in select query.\nCREATE CQ @APPNAME@cq17\nINSERT INTO @APPNAME@Wactions3\nSELECT * from @APPNAME@finalstream1 order by c1 sample by c1;\n\n--sampling using 2 fields, 2800 for single field and 332 for 2 field\nCREATE CQ @APPNAME@cq18\nINSERT INTO @APPNAME@Wactions4\nSELECT c1,c1 from @APPNAME@finalstream6 order by c1 sample by c1;\n\n--same as Wactions4 - here selectivity alone varies, so output is 8\nCREATE CQ @APPNAME@cq19\nINSERT INTO @APPNAME@Wactions5\nSELECT c1,c1 from @APPNAME@finalstream6 order by c1 sample by c1 selectivity 0.0001;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\n--start @APPNAME@;", "generated_queries": "1. How can I set up a data processing application to read and parse data from a specific directory into different streams based on their structure and content, and then sample and store subsets of this data for further analysis?\n\n2. What steps are involved in creating continuous queries to filter and transform data within specific streams, and then sample and extract subsets of this processed data based on specific criteria for storage and further processing?\n\n3. In a data processing application workflow, how can jumping windows be defined over certain streams to efficiently manage and analyze the data flow, and then sample and export specific subsets of this data to external files for storage and archival purposes?", "file_name": "SampleByClause.tql"}
{"tql": "--\n-- Recovery Test 22 with two sources, two sliding attribute windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Sa5W -> CQ1 -> WS\n-- S2 -> Sa6W -> CQ2 -> WS\n--\n\nSTOP KStreamRecov22Tester.KStreamRecovTest22;\nUNDEPLOY APPLICATION KStreamRecov22Tester.KStreamRecovTest22;\nDROP APPLICATION KStreamRecov22Tester.KStreamRecovTest22 CASCADE;\nDROP USER KStreamRecov22Tester;\nDROP NAMESPACE KStreamRecov22Tester CASCADE;\nCREATE USER KStreamRecov22Tester IDENTIFIED BY KStreamRecov22Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov22Tester;\nCONNECT KStreamRecov22Tester KStreamRecov22Tester;\n\nCREATE APPLICATION KStreamRecovTest22 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2;\n\nEND APPLICATION KStreamRecovTest22;", "generated_queries": "1. What sources are being used in the Recovery Test 22 application, and how are they connected to the sliding attribute windows and Wactionstore?\n\n2. Can you provide details on the data processing flow in the Recovery Test 22 application, including how the CSV data is transformed into a structured format before being stored in the Wactionstore?\n\n3. How is real-time data windowing implemented in the Recovery Test 22 application, and what is the difference in windowing configuration between DataStream5Minutes1 and DataStream5Minutes2?", "file_name": "KStreamRecovTest22.tql"}
{"tql": "--\n-- Recovery Test 36 with two sources, two jumping attribute windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Ja5W/p -> CQ1 -> WS\n-- S2 -> Ja6W/p -> CQ2 -> WS\n--\n\nSTOP Recov36Tester.RecovTest36;\nUNDEPLOY APPLICATION Recov36Tester.RecovTest36;\nDROP APPLICATION Recov36Tester.RecovTest36 CASCADE;\n\nDROP USER KStreamRecov36Tester;\nDROP NAMESPACE KStreamRecov36Tester CASCADE;\nCREATE USER KStreamRecov36Tester IDENTIFIED BY KStreamRecov36Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov36Tester;\nCONNECT KStreamRecov36Tester KStreamRecov36Tester;\n\nCREATE APPLICATION KStreamRecovTest36 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nEND APPLICATION KStreamRecovTest36;", "generated_queries": "1. How can I create a recovery test scenario in which data from two CSV sources is processed and stored in two Kafka streams with specific partitioning settings, transformed into structured data, and aggregated into separate waction events within a defined time interval?\n  \n2. What are the steps involved in setting up a TQL query to process CSV data from two different sources, partition it based on merchant ID, create jumping windows to analyze the data at 5-minute and 6-minute intervals, and then store the aggregated results in a waction store using a Kafka event persistence configuration?\n\n3. Can you provide a detailed example of creating continuous queries to convert CSV data streams into structured data, define jumping windows to analyze the data within specific time frames, and insert aggregated results into a waction store based on specific partitioning and event types?", "file_name": "KStreamRecovTest36.tql"}
{"tql": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ recovery 5 second interval;\nCREATE  SOURCE @SourceName@ USING MySqlReader  ( \n  Username: '@Username@',\n  Password: '@Password@',\n  DatabaseName: 'qatest',\n  connectionRetryPolicy: @ConnectionRetryPolicy@,\n  ConnectionURL: '@ConnectionURL@',\n  Tables: '@SourceTables@',\n  ConnectionPoolSize: 1,\n  ReplicationSlotName: 'null'\n ) \nOUTPUT TO @SRCINPUTSTREAM@;\ncreate Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;", "generated_queries": "1. How can I create a data pipeline in my application that reads from a MySQL database table and streams the data to a target system named \"Foo4\"?\n   \n2. What is the process to deploy an application named \"@APPNAME@\" in a TQL environment, including the creation of a recovery mechanism with a 5-second interval and setting up a source to stream data from a MySQL database table?\n   \n3. Can you provide the steps to stop, undeploy, drop, create, and start an application named \"@APPNAME@\" in a TQL environment, along with creating a source that reads data from specific tables in a MySQL database and outputs it to a target system named \"Foo4\"?", "file_name": "ReadersReconnect_mysql.tql"}
{"tql": "STOP JSONRecoveryApp;\n\nUNDEPLOY APPLICATION admin.JSONRecoveryApp;\nDROP APPLICATION admin.JSONRecoveryApp cascade;\n\nCREATE APPLICATION JSONRecoveryApp RECOVERY 1 SECOND INTERVAL;\n\nCREATE TYPE Emptype (\nfirstName String,\nlastName String );\n\nCREATE STREAM EmpStream of Emptype;\n\ncreate source CSVSource using FileReader (\n\tdirectory:'/Users/bhashemi/Product/IntegrationTests/TestData/jsonRecov',\n\tWildCard:'jsonRecov*.json',\n\tpositionByEOF:false\n) PARSE USING\nJSONParser (\neventType:''\n) OUTPUT TO EmpStream;\n\nCREATE WACTIONSTORE jsonWactions CONTEXT OF Emptype\nEVENT TYPES ( Emptype )\n@PERSIST-TYPE@\n\nCREATE CQ InsertjsonWactions\nINSERT INTO jsonWactions\nSELECT firstName, lastName\nFROM EmpStream;\n\nCREATE TARGET jsonRecovSYSOUT using SysOut(name:jsonrecov) INPUT FROM EmpStream;\nEND APPLICATION JSONRecoveryApp;\n\ndeploy application JSONRecoveryApp in default;\nSTART JSONRecoveryApp;", "generated_queries": "1. How can I undeploy and drop a specific application named JSONRecoveryApp in my system?\n2. What is the TQL query to create a stream called EmpStream of Emptype in an application named JSONRecoveryApp with a specified directory and file format for data ingestion?\n3. How do I create a continuous query (CQ) named InsertjsonWactions that inserts data from the EmpStream into a WActionStore named jsonWactions in an application called JSONRecoveryApp?", "file_name": "KStreamJSONRecoveryApp.tql"}
{"tql": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING FileReader  (\n  blocksize: 64,\n  positionbyeof: false,\n  rolloverstyle: 'Default',\n  includesubdirectories: false,\n  adapterName: 'FileReader',\n  directory: '/Users/jenniffer/Downloads',\n  skipbom: true,\n  wildcard: 'dk000000000'\n )\n PARSE USING GGTrailParser  (\n  handler: 'com.webaction.proc.GGTrailParser_1_0',\n  metadata: '@METADATA@',\n  FilterTransactionBoundaries: true,\n  TrailByteOrder: '@BYTEORDDER@',\n  Tables: '@TABLES@',\n  parserName: 'GGTrailParser',\n  _h_ReturnDateTimeAs: '@DATETIME@',\n  Compression:'@COMPRESSION@'\n )OUTPUT to @STREAM@;", "generated_queries": "1. Which directory is the FileReader source configured to read data from?\n2. How is the FileReader source handling blocksize and rollover style for the data it reads?\n3. What type of parser is being used to parse data in this TQL query, and what specific configuration settings are being applied to it?", "file_name": "FileReader_GGTrailParser.tql"}
{"tql": "--\n-- Crash Recovery Test 3 on two node cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> JW -> CQ(aggregate) -> WS\n--\n\nSTOP APPLICATION N2S2CR3Tester.N2S2CRTest3;\nUNDEPLOY APPLICATION N2S2CR3Tester.N2S2CRTest3;\nDROP APPLICATION N2S2CR3Tester.N2S2CRTest3 CASCADE;\nCREATE APPLICATION N2S2CRTest3 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN2S2CRTest3;\n\nCREATE SOURCE CsvSourceN2S2CRTest3 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN2S2CRTest3;\n\nCREATE FLOW DataProcessingN2S2CRTest3;\n\nCREATE TYPE WactionTypeN2S2CRTest3 (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount int,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionTypeN2S2CRTest3;\n\nCREATE CQ CsvToDataN2S2CRTest3\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_INT(TO_DOUBLE(data[7])),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsN2S2CRTest3 CONTEXT OF WactionTypeN2S2CRTest3\nEVENT TYPES ( WactionTypeN2S2CRTest3 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsN2S2CRTest3\nINSERT INTO WactionsN2S2CRTest3\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.merchantId),\n    FIRST(p.dateTime),\n    SUM(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nEND FLOW DataProcessingN2S2CRTest3;\n\nEND APPLICATION N2S2CRTest3;", "generated_queries": "1. What is the process for conducting Crash Recovery Test 3 on a two-node cluster according to Bert Hashemi from WebAction, Inc.?\n2. How can I stop, undeploy, and drop the application N2S2CR3Tester.N2S2CRTest3?\n3. Can you provide details on how data is acquired, processed, and stored during Crash Recovery Test 3 on a two-node cluster as specified in the TQL query by Bert Hashemi from WebAction, Inc.?", "file_name": "N2S2CRTest3.tql"}
{"tql": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second Interval ;\n\nCreate Source @SourceName@\n Using OracleReader\n(\n Username:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@SourceConnectURL@',\n Tables:'@SourceTable@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectURL@',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;", "generated_queries": "1. What are the steps required to deploy and start an application named \"@APPNAME@\" that reads data from an Oracle database and writes it to a target database using a specific batch policy?\n2. How can I create and configure a source named \"@SourceName@\" that connects to an Oracle database with a specific username and password to read data from a specific table with a fetch size of 1?\n3. What process should be followed to undeploy, drop, and recreate an application named \"@APPNAME@\" with a recovery interval of 5 seconds before deploying it back in the default environment and starting it?", "file_name": "OracleMon.tql"}
{"tql": "--\n-- Kafka Stream Agent Checkpoint Recovery tests\n-- Bert Hashemi and Zalak Shah, WebAction, Inc.\n--\n\nstop application recoveryTestAgent.KSRecovCSV;\nundeploy application recoveryTestAgent.KSRecovCSV;\ndrop application recoveryTestAgent.KSRecovCSV cascade;\nDROP USER recoveryTestAgent;\nDROP NAMESPACE recoveryTestAgent CASCADE;\nCREATE USER recoveryTestAgent IDENTIFIED BY recoveryTestAgent;\nGRANT create,drop ON deploymentgroup Global.* TO USER recoveryTestAgent;\nCONNECT recoveryTestAgent recoveryTestAgent;\n\ncreate application KSRecovCSV\nRECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW AgentFlow;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\ncreate source CSVSource using CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'customerdetails-recovery.csv',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO KafkaCsvStream;\n\nEND FLOW AgentFlow;\n\nCREATE FLOW ServerFlow;\nCREATE TYPE UserDataType\n(\n  UserId String KEY,\n  UserName String\n);\n\nCREATE STREAM UserDataStream OF UserDataType PARTITION BY UserId;\n\nCREATE CQ ParseUserData\nINSERT INTO UserDataStream\nSELECT  data[0],\n        data[1]\nFROM KafkaCsvStream;\n\nCREATE WACTIONSTORE UserActivityInfo\nCONTEXT OF UserDataType\nEVENT TYPES ( UserDataType )\n@PERSIST-TYPE@\n\n--get data from UserDataStream and place into wactionStore UserWaction\nCREATE CQ UserWaction\nINSERT INTO UserActivityInfo\nSELECT * FROM UserDataStream\nLINK SOURCE EVENT;\nEND FLOW ServerFlow;\n\nEND APPLICATION KSRecovCSV;\nDEPLOY APPLICATION KSRecovCSV with AgentFlow in AGENTS, ServerFlow on any in default;\nSTART KSRecovCSV;", "generated_queries": "1. How can I set up Kafka Stream Agent Checkpoint Recovery tests in my application?\n2. What is the process involved in creating a stream of Kafka Csv data in an application using TQL?\n3. Can you explain the steps required to deploy an application called KSRecovCSV with specific flows on different agents using TQL?", "file_name": "KStreamAgentCheckpoint.tql"}
{"tql": "Stop application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\nCREATE APPLICATION DSV;\n\nCREATE FLOW HTTPsource;\n\nCREATE SOURCE HTTPSOURCE USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'http.txt',\n  skipbom: true,\n  rolloverpolicy: 'DefaultFileComparator',\n  blocksize: '64',\n  charset: 'UTF-8',\n  positionbyeof: false\n )\n PARSE USING DSVParser (\n  trimquote: true,\n  columndelimiter: ' ',\n  rowdelimiter: '\\n',\n  header: false,\n  quoteset: '{}'\n )\nOUTPUT TO RawHTTPStream;\n\nEND FLOW HTTPSource;\n\nCREATE FLOW main;\n\nCREATE TYPE HTTPLogEntry (\n     start_time String,\n     srcIp      String KEY,\n     port       String,\n     method     String,\n     url        String,\n     error_num  String,\n     status     String,\n     end_time   String,\n     host       String\n );\n\nCREATE STREAM HTTPStream OF HTTPLogEntry;\n\nCREATE CQ ParseHTTPLog\n  INSERT INTO HTTPStream\n  SELECT  MATCH(data[1],'start\\\\s+(\\\\w+.\\\\w+)'),\n          matchIP(data[2]),\n          MATCH(data[3],'port\\\\W+(\\\\w+)'),\n          MATCH(data[4],'method\\\\W+(\\\\w+)'),\n          data[5],\n          data[7],\n          data[8],\n          data[9],\n          data[10]\n  FROM RawHTTPStream;\n\ncreate Target t using FileWriter(\n  filename:'XmlTrimQuote',\n  sequence:'00',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:10s,sequence:00'\n)\nformat using DSVFormatter (\n\n)\ninput from HTTPStream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TrimQuoteTest_actual.log') input from HTTPStream;\n\nEND FLOW main;\n\nEND APPLICATION DSV;", "generated_queries": "1. What is the process for creating an application named \"DSV\" with specific file reading and parsing configurations for HTTP log data?\n   \n2. How can I deploy and configure a flow within the \"DSV\" application that reads HTTP log data from a file, parses it using a custom DSVParser, and outputs the parsed data to a stream named \"HTTPStream\"?\n\n3. In the context of the \"DSV\" application, how do I define and execute a continuous query (CQ) named \"ParseHTTPLog\" that extracts specific fields from raw HTTP log data and inserts them into a stream named \"HTTPStream\"?", "file_name": "TrimQuoteTest.tql"}
{"tql": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n\nCREATE SOURCE @APPNAME@_S USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.test01'\n ) \nOUTPUT TO @APPNAME@_SS;\n\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'public.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12)',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:10',\nStandardSQL:true\t\n) INPUT FROM @APPNAME@_ss;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\nSTART APPLICATION @APPNAME@;", "generated_queries": "1. How can I deploy and start an application named @APPNAME@ which reads data from a PostgreSQL database table \"public.test01\" and writes it to a BigQuery dataset with specific column mappings and merge mode in a specific project and location?\n2. What is the process to stop, undeploy, drop, create, and deploy an application called @APPNAME@ that utilizes a PostgreSQLReader source connected to a local PostgreSQL database, and a BigQueryWriter target writing to a BigQuery dataset in the US region?\n3. Can you provide the steps to configure an application named @APPNAME@ that replicates data from a PostgreSQL database using a specific replication slot, and writes it to a BigQuery table with custom batch and retry policies, encoding, and column mappings?", "file_name": "postgres_bq.tql"}
{"tql": "CREATE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE FLOW @APPNAME@AgentFlow;\nCREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()\nPARSE USING Global.DSVParser ()\nOUTPUT TO @APPNAME@_Stream;\nEND FLOW @APPNAME@AgentFlow;\n\nCREATE FLOW @APPNAME@serverFlow;\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()\nFORMAT USING DSVFormatter (\nmembers:'data')\nINPUT FROM @APPNAME@_Stream;\nEND FLOW @APPNAME@serverFlow;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@ with @APPNAME@AgentFlow in Agents, @APPNAME@ServerFlow in default;\nstart application @APPNAME@;", "generated_queries": "1. How can I create an application with a specific name and recovery option using TQL?\n2. What are the steps involved in creating a flow for an application named '@APPNAME@' using TQL?\n3. How can I deploy an application named '@APPNAME@' with specific flows assigned to Agents and default using TQL?", "file_name": "ADLSReaderToS3Agent.tql"}
{"tql": "STOP application XmlFormatterTester.DSV;\nundeploy application XmlFormatterTester.DSV;\ndrop application XmlFormatterTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSVSmallPosDataSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'smallposdata.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO CsvSmallPosDataStream;\n\ncreate source CSVPosDataSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'posdata.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO CsvPosDataStream;\n\n\nCreate Type CSVPosDataType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVSmallPosDataStream of CSVPosDataType;\nCreate Stream TypedCSVPosDataStream of CSVPosDataType;\n\n\nCREATE CQ CsvToSmallPosData\nINSERT INTO TypedCSVSmallPosDataStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvSmallPosDataStream;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVPosDataStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvPosDataStream;\n\n/**\n*  3.2.1.b FileWriter XML TimeInterval\n**/\ncreate Target XmlFormatterTimeInterval using FileWriter(\n  filename:'TargetPosDataXmlTIOpt',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00'\n)\nformat using XMLFormatter (\n  rootelement:'document',\n  elementtuple:'MerchantName:merchantid:text=merchantname',\n  charset:'UTF-8'\n)\ninput from TypedCSVSmallPosDataStream;\n\n/**\n* 3.2.1.c FileWriter XMLFileSize 101MB\n**/\ncreate Target XmlFormatterFileSize101 using FileWriter(\n  filename:'TargetPosDataXmlFS',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'FileSizeRollingPolicy,filesize:101M,sequence:00'\n)\nformat using XMLFormatter (\n  rootelement:'document',\n  elementtuple:'MerchantName:merchantid:text=merchantname',\n  charset:'UTF-8'\n)\ninput from TypedCSVPosDataStream;\n\n/**\n* 3.2.1.d FileWriter XMLDefaultFS 10 MB\n**/\ncreate Target XmlFormatterDefault using FileWriter(\n  filename:'TargetPosDataXmlFSDefault',\n  directory:'@FEATURE-DIR@/logs/',\n    sequence:'00',\n  rolloverpolicy:'FileSizeRollingPolicy'\n)\nformat using XMLFormatter (\n  rootelement:'document',\n  elementtuple:'MerchantName:zip:text=merchantname',\n  charset:'UTF-8'\n)\ninput from TypedCSVSmallPosDataStream;\n\n\n/**\n* 3.2.1.e FileWriter XML EventCount 2000\n**/\ncreate Target XmlFormatterEventCount2000 using FileWriter(\n  filename:'TargetPosDataXmlEC',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'eventcount:2000,sequence:00'\n)\nformat using XMLFormatter (\n  rootelement:'document',\n  elementtuple:'MerchantName:zip:text=merchantname',\n  charset:'UTF-8'\n)\ninput from TypedCSVSmallPosDataStream;\n\ncreate Target LogWriterSmallPosData using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlDFS_actual.log') input from TypedCSVSmallPosDataStream;\n\nend application DSV;", "generated_queries": "1. Which types of data sources are being used in the DSV application for processing CSV files?\n2. What are the different targets and formats specified for writing XML formatted data in the DSV application?\n3. Can you provide details about the logging setup in the DSV application for the TypedCSVSmallPosDataStream stream?", "file_name": "XMLFormatter.tql"}
{"tql": "--\n-- Recovery Test 12 with two sources, two jumping attribute windows, one wactionstore with recovery, and another wactionstore without -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Ja5W/p -> CQ1 -> WS\n-- S2 -> Ja6W/p -> CQ2 -> WS\n-- S2 -> Ja6W/p -> CQ2 -> WS2 (no persists)\n--\n\nSTOP Recov12Tester.RecovTest12;\nUNDEPLOY APPLICATION Recov12Tester.RecovTest12;\nDROP APPLICATION Recov12Tester.RecovTest12 CASCADE;\nCREATE APPLICATION RecovTest12 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE WactionsNoPersist CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n\t\tPERSIST NONE USING ( ) ;\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWactionNoPersist\nINSERT INTO WactionsNoPersist\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nEND APPLICATION RecovTest12;", "generated_queries": "1. How can I set up a recovery test with two sources, two jumping attribute windows, one wactionstore with recovery, and another wactionstore without recovery, all partitioned on the same key?\n2. What are the steps to create an application named RecovTest12 that utilizes CSV sources, jumping windows, and wactionstores with and without persistence, all involving data transformation and aggregation based on specified partition keys?\n3. How can I deploy and manage an application called RecovTest12 that involves reading data from CSV files, transforming it into structured formats, partitioning it based on merchant ID, and storing aggregated results in wactionstores with different persistence configurations?", "file_name": "RecovTest12.tql"}
{"tql": "undeploy application dev15823;\nalter application dev15823;\n\nCREATE TYPE ModifyNotNull (\n  x int,\n  y string\n);\n\nCREATE STREAM FormattedStream OF ModifyNotNull;\n\nCREATE  CQ InsertWactions\nINSERT INTO FormattedStream\nSELECT\n    TO_INT(data[0]),\n   \t\"notnull\"\nFROM LogminerStream;\n\nCreate or replace Target test using SysOut (name:test) input from FormattedStream;\n\nCREATE OR REPLACE TARGET WriteCDCMySQL USING DatabaseWriter  ( \n  Username: '@USERNAME@',\n  BatchPolicy: 'Eventcount:5,Interval:5',\n  CommitPolicy: 'Eventcount:5,Interval:5',\n  ConnectionURL: '@URL@',\n  Tables: '@TABLES@',\n  Checkpointtable: 'CHKPOINT',\n  --IgnorableExceptionCode:'1062',\n  Password: '@PASSWORD@'\n ) \nINPUT FROM FormattedStream;\n\nEND APPLICATION dev15823;\nalter application dev15823 recompile;\ndeploy application dev15823;\nstart dev15823;", "generated_queries": "1. Can you provide details about the target destinations and sources for the application \"dev15823\"?\n2. What are the specific configurations and settings for writing Change Data Capture (CDC) events to a MySQL database for the application \"dev15823\"?\n3. How is the data transformation and mapping defined for the Continuous Query (CQ) \"InsertWactions\" within the application \"dev15823\"?", "file_name": "addingCQ.tql"}
{"tql": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\n\nCREATE  APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE @srcName@ USING Global.DatabaseReader (\n  ConnectionURL: '@srcurl@', \n  Tables: '@srcschema@.@srctable@',\n  ReturnDateTimeAs: 'String', \n  FetchSize: 30000, \n  Username: '@srcusername@', \n  QuiesceOnILCompletion: true, \n  Password: '@srcpassword@', \n  DatabaseProviderType: 'DEFAULT' ) \nOUTPUT TO @outstreamname@;\n\nCREATE TARGET @tgtName@  USING Global.DatabaseWriter ( \n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', \n  CheckPointTable: 'CHKPOINT',\n  Username:'@tgtusername@', \n  Password:'@tgtpassword@', \n  CDDLAction: 'Process', \n  StatementCacheSize: '50', \n  CommitPolicy: 'EventCount:30000,Interval:60', \n  ConnectionURL:'@tgturl@',\n  DatabaseProviderType: 'Default', \n  PreserveSourceTransactionBoundary: 'false', \n  BatchPolicy: 'EventCount:30000,Interval:60', \n  Tables:'@srcschema@.@srctable@,@tgtschema@.@tgttable@', \n  adapterName: 'DatabaseWriter' ) \nINPUT FROM @instreamname@;\n\nEnd APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;", "generated_queries": "1. How can I set up a data pipeline to continuously read from a specific database table, process the data, and write it to another database table with specific configuration settings like fetch size, commit policy, and retry policy?\n2. What is the process to deploy and start an application named \"@AppName@\" which includes a source to read data from a database connection and a target to write processed data to another database connection?\n3. How can I ensure that when setting up an application, the pipeline is properly configured to stop gracefully during inter-language completion, retry connection attempts, and maintain transaction boundaries between the source and target databases?", "file_name": "oracletooracle.tql"}
{"tql": "--\n-- Recovery Test 1\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> WS\n--\n\nSTOP Recov1Tester.RecovTest1;\nUNDEPLOY APPLICATION Recov1Tester.RecovTest1;\nDROP APPLICATION Recov1Tester.RecovTest1 CASCADE;\nCREATE APPLICATION RecovTest1 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE WactionType (\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions\nINSERT INTO Wactions\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nEND APPLICATION RecovTest1;", "generated_queries": "1. What is the interval set for recovery in the \"RecovTest1\" application created by Nicholas Keene at WebAction, Inc.?\n2. Can you provide details of the source configuration for the \"CsvSource\" in the \"RecovTest1\" application by WebAction, Inc.?\n3. What types of events are stored in the WACTIONSTORE named \"Wactions\" in the \"RecovTest1\" application created by Nicholas Keene at WebAction, Inc.?", "file_name": "CRTest1.tql"}
{"tql": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@;\nCREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: '@ORACLE-URL@',\n  Tables: '@SOURCE-TABLES@',\n  Username: '@ORACLE-USERNAME@',\n  Password: '@ORACLE-PASSWORD@'\n) OUTPUT TO @APP_NAME@_Stream;\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt USING SnowflakeWriter\n\n(\n  ConnectionURL:'@SNOWFLAKE-URL@',\n  username:'@SNOWFLAKE-USERNAME@',\n  appendOnly:'false',\n  Tables:'@TARGET-TABLES@',\n  uploadpolicy:'eventcount:3,interval:10s',\n  externalStageType:'local'\n)\nINPUT FROM @APP_NAME@_Stream;\n\n\nEND APPLICATION @APP_NAME@;\nDEPLOY APPLICATION @APP_NAME@;\nSTART APPLICATION @APP_NAME@;", "generated_queries": "1. What are the steps involved in stopping, undeploying, dropping, creating, configuring and starting an application named @APP_NAME@ with Oracle as the source and Snowflake as the target?\n   \n2. How can an application named @APP_NAME@ be deployed and started after configuring an Oracle database as the source and Snowflake as the target in a data integration workflow?\n\n3. What actions are necessary to replace the source and target connections for an existing application named @APP_NAME@ with new connection details for Oracle and Snowflake databases?", "file_name": "OracleToSnowflake_Streaming.tql"}
{"tql": "--\n-- Canon Test W80\n-- Nicholas Keene, WebAction, Inc.\n--\n-- Basic test for a partitioned jumping attribute window\n--\n-- S -> JWa5p -> CQ -> WS\n--\n\n\nUNDEPLOY APPLICATION NameW80.W80;\nDROP APPLICATION NameW80.W80 CASCADE;\nCREATE APPLICATION W80 RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE FLOW DataAcquisitionW80;\n\nCREATE SOURCE CsvSourceW80 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'Canon1000.csv',\n  columndelimiter:',',\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStreamW80;\n\nEND FLOW DataAcquisitionW80;\n\n\n\nCREATE FLOW DataProcessingW80;\n\nCREATE TYPE DataTypeW80 (\n    eventID String,\n    word String KEY,\n    dateTime DateTime\n);\n\nCREATE STREAM DataStreamW80 OF DataTypeW80;\n\nCREATE CQ CSVStreamW80_to_DataStreamW80\nINSERT INTO DataStreamW80\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[2],'yyyyMMddHHmmss')\nFROM CsvStreamW80;\n\nCREATE JUMPING WINDOW JWa5pW80\nOVER DataStreamW80\nKEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY word;\n\nCREATE WACTIONSTORE WactionStoreW80 CONTEXT OF DataTypeW80\nEVENT TYPES ( DataTypeW80 KEY(word) )\n@PERSIST-TYPE@\n\nCREATE CQ JWa5pW80_to_WactionStoreW80\nINSERT INTO WactionStoreW80\nSELECT\n    FIRST(eventID),\n    FIRST(word),\n    FIRST(dateTime)\nFROM JWa5pW80\nGROUP BY word;\n\nEND FLOW DataProcessingW80;\n\n\n\nEND APPLICATION W80;", "generated_queries": "1. What is the process for acquiring and processing data from a CSV file with a specific column delimiter and date format within a 5-minute interval in a partitioned jumping attribute window using a specific application named W80?\n   \n2. How can a developer deploy, undeploy, and drop an application named W80 with a recovery interval of 5 seconds and configure data acquisition, processing, and storage of events based on key attributes in real-time using TQL commands?\n\n3. In the context of the Canon Test W80, how is data transformed from a CSV stream into a specific data type with event IDs, words, and timestamps, and then stored in a wActionStore based on unique word partitions within a 5-minute window?", "file_name": "W80.tql"}
{"tql": "STOP application AlterTester.DSV;\nundeploy application AlterTester.DSV;\ndrop application AlterTester.DSV cascade;\n\n\ncreate application DSV;\ncreate source CSVSource using FileReader (\ndirectory:'@TEST-DATA-PATH@',\nWildCard:'smallposdata.csv',\npositionByEOF:false,\ncharset:'UTF-8'\n)\nparse using DSVParser (\nheader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\n\nend application DSV;", "generated_queries": "1. How can I create an application in TQL that reads CSV data from a specific directory and parses it using a custom parser called DSVParser to generate a stream of typed data?\n2. What TQL commands do I need to execute in order to transform CSV data from a file named 'smallposdata.csv' into a structured format with fields such as merchantId, dateTime, hourValue, amount, and zip?\n3. Can you provide a step-by-step guide on how to undeploy and drop an existing application named AlterTester.DSV and then create a new application called DSV that processes CSV data according to a specific schema defined by the CSVType?", "file_name": "DSVAlterApp.tql"}
{"tql": "stop application ADW;\nundeploy application ADW;\ndrop application ADW cascade;\nCREATE APPLICATION ADW;\n\nCREATE  SOURCE OracleInitialLoad USING DatabaseReader  \n (\n Username:'src_username',\n Password:'src_password',\n ConnectionURL: 'src_url',\n Tables:'@SOURCE-TABLES@',\n FetchSize:2000\n) \nOUTPUT TO InitialLoadStream;\n\nCREATE TARGET AzureDWInitialLoad USING AzureSQLDWHWriter(\nConnectionURL: '@SQLDW-URL@',\n        username: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@',\n        uploadpolicy:'@EVENT-COUNT@'\n)\nINPUT FROM InitialLoadStream;\n\nEND APPLICATION ADW;\ndeploy application ADW;\nstart application ADW;", "generated_queries": "1. How can I undeploy and drop an application called ADW, and then create and deploy it again with initial data loading from an Oracle source to an Azure Data Warehouse using Talend?\n2. What are the source and target configurations for loading data into an Azure Data Warehouse application named ADW from an Oracle database using Talend?\n3. How can I stop, undeploy, drop, and then recreate and start an application named ADW in Talend, with an initial data load from Oracle to Azure Data Warehouse configured in the process?", "file_name": "OracleToSQLDWInitialLoad.tql"}
{"tql": "--\n-- Crash Recovery Test 3 on four node all server cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> JW -> CQ(aggregate) -> WS\n--\n\nSTOP APPLICATION N4S4CR3Tester.N4S4CRTest3;\nUNDEPLOY APPLICATION N4S4CR3Tester.N4S4CRTest3;\nDROP APPLICATION N4S4CR3Tester.N4S4CRTest3 CASCADE;\nCREATE APPLICATION N4S4CRTest3 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN4S4CRTest3;\n\nCREATE SOURCE CsvSourceN4S4CRTest3 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN4S4CRTest3;\n\nCREATE FLOW DataProcessingN4S4CRTest3;\n\nCREATE TYPE WactionTypeN4S4CRTest3 (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount int,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionTypeN4S4CRTest3;\n\nCREATE CQ CsvToDataN4S4CRTest3\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_INT(TO_DOUBLE(data[7])),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsN4S4CRTest3 CONTEXT OF WactionTypeN4S4CRTest3\nEVENT TYPES ( WactionTypeN4S4CRTest3 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsN4S4CRTest3\nINSERT INTO WactionsN4S4CRTest3\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.merchantId),\n    FIRST(p.dateTime),\n    SUM(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nEND FLOW DataProcessingN4S4CRTest3;\n\nEND APPLICATION N4S4CRTest3;", "generated_queries": "1. How can I stop, undeploy, and drop a specific application called N4S4CR3Tester.N4S4CRTest3 on a four-node all-server cluster?\n2. Can I create a new recovery application named N4S4CRTest3 that acquires data from a CSV file, processes it, and stores it in a custom Waction store, all within a 5-second interval?\n3. How can I configure a jumping window named DataStream5Minutes that keeps track of data within a 5-minute timeframe based on the dateTime field of a specific data stream called DataStream?", "file_name": "KStreamN4S4CRTest3.tql"}
{"tql": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@;\n\nCREATE OR REPLACE SOURCE CCBReader USING FileReader (\n  wildcard: '@WILDCARD@',\n  positionbyeof: false,\n  directory: '@TESTDIR@'\n  )\nPARSE USING Global.CobolCopybookParser (\n  copybookDialect: 'Mainframe',\n  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',\n  GroupPolicy: '',\n  TimeoutPolicy: '1s',\n  dataFileOrganization: 'FixedLength',\n  ProcessCopyBookFileAs: 'SingleEvent',\n  skipIndent: 0,\n  dataFileFont: 'utf8',\n  CopybookFileFormat: 'USE_STANDARD_COLUMNS',\n  copybookSplit: 'None',\n  copybookFileName: '@CCBFILE@'\n   )\nOUTPUT TO CCBStream;\n\nCREATE OR REPLACE TARGET JSONWriter USING FileWriter (\n  filename: '%@metadata(FileName)%',\n  directory: '@DIR@',\n  rolloverpolicy: 'EventCount:10000,Interval:30s',\n  flushpolicy: 'EventCount:1,Interval:30s'\n  )\nFORMAT USING JSONFormatter  ()\nINPUT FROM CCBStream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@ on all in default;\nstart application @APPNAME@;", "generated_queries": "1. How can I stop, undeploy, and drop a specific application in my TQL configuration?\n2. What is the process for creating and configuring an application in TQL, including defining sources, parsers, targets, and deployment settings?\n3. After configuring an application in TQL, how can I deploy and start it on all nodes in the default environment?", "file_name": "cobolSingleEvent.tql"}
{"tql": "stop OracleToKudu;\nundeploy application OracleToKudu;\ndrop application OracleToKudu cascade;\n\nCREATE APPLICATION OracleToKudu;\nCreate Source oracSource\n Using OracleReader\n(\n Username:'@LOGMINER-UNAME@',\n Password:'@LOGMINER-PASSWORD@',\n ConnectionURL:'@LOGMINER-URL@',\n Tables:'@SOURCE_TABLES@',\n OnlineCatalog:true,\n FetchSize:1\n) Output To DataStream;\nCREATE TARGET WriteintoKudu using KuduWriter (\nkuduclientconfig:'',\npkupdatehandlingmode:'@MODE@',\ntables: '@TARGET_TABLES@',\nbatchpolicy: 'EventCount:1,Interval:0')\nINPUT FROM DataStream;\n\nEND APPLICATION OracleToKudu;\ndeploy application OracleToKudu in default;\nstart OracleToKudu;", "generated_queries": "1. What are the steps required to deploy and start an application called OracleToKudu that reads from an Oracle database and writes to a Kudu database with specific table mappings and configuration settings?\n   \n2. How can I stop and undeploy an existing application named OracleToKudu that is transferring data from Oracle to Kudu, ensuring that all related components are gracefully deactivated and removed from the system?\n\n3. Can you provide guidance on dropping an application called OracleToKudu, including any associated dependencies, in such a way that the application and its resources are completely removed following a cascade deletion process?", "file_name": "OracleToKuduErrorHandling.tql"}
{"tql": "CREATE OR REPLACE EMBEDDINGGENERATOR @EMB_NAME@ USING @MODEL@ (\nmodelProvider: '@MODEL@',\nmodelName: '@MODEL_NAME@',\napiKey: '@API_KEY@'\n);", "generated_queries": "1. How can I create an embedding generator in my TextQL project using a specific pre-trained model?\n  \n2. What is the process for replacing an existing embedding generator with a new one in TextQL and specifying the model, model name, and API key to be used?\n\n3. How can I define an embedding generator in TextQL to utilize a particular model, set a custom model name, and provide an API key for access?", "file_name": "EmbeddingGenerator.tql"}
{"tql": "STOP WStester.WSusingApp;\nUNDEPLOY APPLICATION WStester.WSusingApp;\nDROP APPLICATION WStester.WSusingApp CASCADE;\n\nCREATE APPLICATION WSusingApp;\n\nCREATE TYPE Atm(\n  productID String KEY,\n  stateID String,\n  productWeight int,\n  quantity int,\n  size int);\n\nCREATE SOURCE liveSource using StreamReader(\n  OutputType: 'WStester.Atm',\n  noLimit: 'false',\n  maxRows: 20,\n  iterations: 0,\n  iterationDelay: 100,\n  StringSet: 'productID[001-002-003-004],stateID[AS-CA-WA-NY]',\n  NumberSet: 'productWeight[3-3]R,quantity[20-20]R,size[250-250]R'\n  )OUTPUT TO CsvStream;\n\n\nCREATE STREAM newStream OF Atm;\n\nCREATE CQ newCQ\nINSERT INTO newStream\nSELECT data[0], data[1], TO_INT(data[2]), TO_INT(data[3]), TO_INT(data[4]) FROM\nCsvStream;\n\ncreate target myOut8 using SysOut(name : testOut8) input from newStream;\n\nCREATE WACTIONSTORE streamActivityMEM CONTEXT OF Atm\nEVENT TYPES ( Atm )\nUSING MEMORY;\n\nCREATE WACTIONSTORE streamActivityES CONTEXT OF Atm\nEVENT TYPES ( Atm );\n\nCREATE WACTIONSTORE streamActivityES2 CONTEXT OF Atm\nEVENT TYPES ( Atm )\nUSING ( storageProvider: 'elasticsearch' );\n\nCREATE WACTIONSTORE streamActivityDerby CONTEXT OF Atm\nEVENT TYPES ( Atm )\nUSING (\nstorageProvider:'jdbc',\npersistence_interval:'10 sec',\nJDBC_DRIVER:'@WASTORE-DRIVER@',\nJDBC_URL:'@WASTORE-URL@',\nJDBC_USER:'@WASTORE-UNAME@',\nJDBC_PASSWORD:'@WASTORE-PASSWORD@',\nDDL_GENERATION:'drop-and-create-tables',\nLOGGING_LEVEL:'SEVERE');\n\nCREATE CQ newCQ2\nINSERT INTO streamActivityMEM\nSELECT * FROM newStream\nlink source event;\n\nCREATE CQ newCQ3\nINSERT INTO streamActivityES\nSELECT * FROM newStream\nlink source event;\n\nCREATE CQ newCQ4\nINSERT INTO streamActivityES2\nSELECT * FROM newStream\nlink source event;\n\nCREATE CQ newCQ5\nINSERT INTO streamActivityDerby\nSELECT * FROM newStream\nlink source event;\n\nEND APPLICATION WSusingApp;", "generated_queries": "1. How can I deploy a new application named WSusingApp and set up various components such as a source, stream, targets, and action stores using a TQL query?\n   \n2. What are the details and configurations for creating a new continuous query (CQ) that inserts data into different WActionStores based on specific conditions and with linking to a source event in the TQL query provided?\n\n3. Can you provide the TQL query syntax for setting up WActionStores for an Atom type, including configuring them with different storage providers like memory, Elasticsearch, and Derby, along with their corresponding event types and persistence settings?", "file_name": "WSusingApp.tql"}
{"tql": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');\n\nCREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.JsonNodeEvent PERSIST USING @APPNAME@KafkaPropset;\n\nCREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (\n  ProviderName: '',\n  Provider: '',\n  Ctx: '',\n  QueueName: '',\n  Topic:'',\n  UserName: '',\n  Password: '',\n  EnableTransaction: '',\n  transactionpolicy: ''\n )\nPARSE USING JSONParser ()\nOUTPUT TO @APPNAME@PersistStream@RANDOM@;\n\nCREATE OR REPLACE TARGET @APPNAME@_filetrgt USING FileWriter (\n  filename: '',\n  flushpolicy: '',\n  directory: '',\n  rolloverpolicy: '' )\nFORMAT USING JSONFormatter  (\n  members: 'data' )\nINPUT FROM @APPNAME@PersistStream@RANDOM@;\n\nEND APPLICATION @APPNAME@;", "generated_queries": "1. What is the configuration settings for the Kafka integration in the application named @APPNAME@?\n2. How is the JSON data from a JMS source parsed and persisted into a Kafka stream within the application @APPNAME@?\n3. Where does the FileWriter target within the @APPNAME@ application store the data and in what format is the data being saved?", "file_name": "JMSReaderJSON.tql"}
{"tql": "CREATE TARGET @TARGET_NAME@ using KuduWriter (\nkuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',\npkuPDaTehAnDliNgmOdE:'DELETEANDINSERT',\ntables: 'QATEST.ORCLALLDATATYPES,@TARGET_TABLE@ ColumnMap(Log_Id=LOG_ID,Description=Description,LogChar=LogChar,LogVarchar2=LogVarchar2,LogNumber=LogNumber,LogBinaryFloat=LogBinaryFloat,LogBinaryDouble=LogBinaryDouble,LogFloat=LogFloat,LogDate=LogDate,LogTimestamp=LogTimestamp,LogNchar=LogNchar,LogNvarchar2=LogNvarchar2,LogTimezone=LogTimezone,LogTimelocal=LogTimelocal,LogInterval=LogInterval,LogInterval2=LogInterval2,LogInteger=LogInteger)',\nbatchpolicy: 'EventCount:1,Interval:0')\nINPUT FROM @STREAM@;", "generated_queries": "1. What is the target name used by the KuduWriter in this TQL query and what configuration settings are specified for the Kudu client (such as socket read timeout and operation timeout)?\n2. Which tables are being targeted for data synchronization using the KuduWriter in this TQL query, and what are the column mappings specified for these tables?\n3. What is the batch policy defined for the KuduWriter in this TQL query, and how does it specify the batch processing of events for data synchronization from the input stream?", "file_name": "KuduWriter.tql"}
{"tql": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n--create flow AgentFlow;\nCREATE SOURCE @APPNAME@_S USING MySqlReader\n(\n  Compression: false,\n  FetchSize: 1,\n  SendBeforeImage: true,\n  ConnectionURL: 'mysql://localhost:3306',\n  DatabaseName: 'waction',\n  Tables: 'waction.test01',\n  Password: 'w@ct10n',\n  Password_encrypted: 'false',\n  Username: 'root',\n  connectionRetryPolicy:'retryInterval=1,maxRetries=3'\n)\nOUTPUT TO @APPNAME@_SS;\n--end flow AgentFlow;\n--create flow serverFlow;\n--create target sys_out using sysout (name:sysoutput) input from ss;\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'waction.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:0',\nStandardSQL:true\t\n) INPUT FROM @APPNAME@_ss;\n--end flow serverFlow;\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\n--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\nSTART APPLICATION @APPNAME@;", "generated_queries": "1. How can I deploy and start an application named @APPNAME@ that reads data from a MySQL database table and writes it to a BigQuery table with specific column mappings and retry policies?\n2. What is the process for undeploying and dropping an application named @APPNAME@ that utilizes a MySQL reader as a source and a BigQuery writer as a target in the data flow?\n3. Can you provide instructions on creating an application named @APPNAME@ that uses a MySQL database as a source and writes data to a BigQuery table with specified configuration settings such as service account key, encoding, and batch policy?", "file_name": "mysql_bq_quotes.tql"}
{"tql": "list servers;", "generated_queries": "1. \"Can you provide me with a list of all the servers that are currently active in our network?\"\n2. \"Which servers are included in the server inventory database?\"\n3. \"I need to know the names of all the servers that have been registered in our system, can you provide me with that information?\"", "file_name": "sampleTql.tql"}
{"tql": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@;\n\nCREATE OR REPLACE TYPE @appname@CQOUT1_Type (\n companyName java.lang.String,\n merchantId java.lang.String,\n dateTime org.joda.time.DateTime,\n hourValue java.lang.String,\n amount java.lang.String,\n zip java.lang.String,\n FileName java.lang.String);\n\nCREATE SOURCE @parquetsrc@ USING S3Reader (\n    bucketname:'',\n    objectnameprefix:''\n     )\nPARSE USING ParquetParser (\n)\nOUTPUT TO @appname@Stream;\n\nCREATE OR REPLACE STREAM @appname@CQOUT1 OF @appname@CQOUT1_Type;\nCREATE OR REPLACE CQ @appname@CQ_PQEvent\nINSERT INTO @appname@CQOUT1\n    Select\n    data.get(\"companyName\").toString(),\n    data.get(\"merchantId\").toString(),\n    TO_DATE(data.get(\"dateTime\").toString()),\n    data.get(\"hourValue\").toString(),\n    data.get(\"amount\").toString(),\n    data.get(\"zip\").toString(),\n    metadata.get(\"FileName\").toString()\n    FROM @appname@Stream p;\n\ncreate Target @s3target@ using S3Writer(\n    bucketname:'',\n    objectname:'',\n    uploadpolicy:'',\n    foldername:''\n)\nformat using AvroFormatter (\nschemaFileName: 'AvroS3Schema'\n)\ninput from @appname@CQOUT1;\n\ncreate Target @blobtarget@ using AzureBlobWriter(\n\taccountname:'',\n\taccountaccesskey:'',\n\tcontainername:'',\n    blobname:'',\n\tfoldername:'',\n\tuploadpolicy:'EventCount:10,interval:5s'\n)\nformat using JSONFormatter ()\nINPUT FROM @appname@CQOUT1;\n\n\nCREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (\n    bucketname:'',\n    objectname:'',\n    foldername:'',\n    projectId:'',\n    uploadPolicy:''\n)\nformat using JSONFormatter (\n)\nINPUT FROM @appname@CQOUT1;\n\nCREATE OR REPLACE TARGET @dbtarget@ USING DatabaseWriter (\n  Tables: '',\n  ConnectionURL:'',\n  Username:'',\n  Password:'',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  BatchPolicy:'EventCount:1,Interval:0'\n)\nINPUT FROM @appname@CQOUT1;\n\nCREATE TARGET @bqtarget@ USING BigQueryWriter (\n  Tables: '',\n  projectId:'',\n  BatchPolicy: 'eventCount:1, Interval:1',\n  ServiceAccountKey: '',\n   )\nINPUT FROM @appname@CQOUT1;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;", "generated_queries": "1. How can I create and deploy a streaming application in real-time to process and transform data from an S3 source into structured Avro, JSON, and JSONL formats, and then output the processed data to Azure Blob Storage, Google Cloud Storage, a database, and BigQuery?\n2. What are the steps involved in stopping, undeploying, dropping, creating, and deploying a streaming application named \"@appname@\" using various targets like S3, Azure Blob Storage, Google Cloud Storage, a database, and BigQuery?\n3. How can I set up a continuous query in a streaming application to extract specific fields from the incoming data, transform it into a custom data type, and output the processed data to different storage targets such as S3, Azure Blob Storage, Google Cloud Storage, a database, and BigQuery with specific upload policies in place?", "file_name": "parquetS3TypedStream.tql"}
{"tql": "create flow AgentFlow;\n\nCREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (\n  StartPosition: '@startPosition@',\n  ConnectionURL: '@sourceURL@',\n  Username: '@userName@',\n  Tables: '@tables@',\n  CheckColumn: '@checkColum@',\n  FetchSize: 1,\n  Password: '@password@',\n  DatabaseProviderType: 'Default',\n  ThreadPoolSize: 5,\n  pollingInterval: '2sec',\n  ConnectionPoolSize: 1 )\nOUTPUT TO @STREAM@;\n\nend flow AgentFlow;", "generated_queries": "1. How can I create a data flow named AgentFlow that reads data incrementally from a specified source using a batch reader with specific connection details like URL, username, and password?\n2. Which source and reader configuration settings are used in creating a data flow named AgentFlow for incrementally reading data from tables specified in a database with a custom polling interval of every 2 seconds?\n3. What is the TQL query syntax for setting up a data flow named AgentFlow that reads data from tables specified in a database, with a specified check column for incremental updates and a thread pool size of 5 for parallel processing?", "file_name": "IncrementalBatchReaderWithAgent.tql"}
{"tql": "STOP APPLICATION oraddl;\nUNDEPLOY APPLICATION oraddl;\nDROP APPLICATION oraddl CASCADE;\nCREATE APPLICATION oraddl recovery 5 second interval;\n \nCreate Source Ora Using OracleReader \n(\n Username:'@user-name@',\n Password:'@password@',\n ConnectionURL:'src_url',\n Tables:'QATEST.ORACLEDDL%',\n DictionaryMode:OfflineCatalog,\n DDLCaptureMode : 'All',\n FetchSize:1\n) Output To LogminerStream;\n\nCreate Target tgt using DatabaseWriter \n(\n Username:'@username@',\n Password:'@password@',\n ConnectionURL:'TGT_URL',\n BatchPolicy:'EventCount:1,Interval:1',\n CommitPolicy:'EventCount:1,Interval:1',\n IgnorableExceptionCode: '1,2290,942',\n Tables :'QATEST.ORACLEDDL%,QATEST2.%'\n) input from LogminerStream;\n\nCREATE TARGET cdcDump USING LogWriter(\nname:testOuput,\ndirectory:'/Users/abinandan/product/IntegrationTests/target/test-classes/testNG/AllTargetWriters/OracleDDLDatabaseWriter/logs',\nfilename:'oraclecdc.log',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING JSONFormatter ()\nINPUT FROM LogminerStream;\nend application oraddl;\ndeploy application oraddl;\nstart application oraddl;", "generated_queries": "1. What is the configuration details of the application named \"oraddl\", including its recovery interval and the target database tables it interacts with?\n   \n2. How can I monitor changes happening in tables that have names starting with \"QATEST.ORACLEDDL\" using LogminerStream and LogWriter within the \"oraddl\" application?\n   \n3. Can I find out the exception handling strategy set for the target database \"tgt\", specifically the ignorable exception codes defined for this application?", "file_name": "log.tql"}
{"tql": "stop FileReaderToKuduWriter;\nundeploy application FileReaderToKuduWriter;\ndrop application FileReaderToKuduWriter cascade;\n\nCREATE APPLICATION FileReaderToKuduWriter recovery 5 second interval ;;\n\nCREATE OR REPLACE SOURCE CSVPoller USING FileReader (\n        directory:'/Users/Striim/',\n        WildCard:'typetest.csv',\n        positionByEOF:false\n)\nparse using DSVParser (\n        header:'yes'\n)\nOUTPUT TO CsvStream;\n\nCREATE OR REPLACE TYPE CSVStream_Type  ( ID1 String KEY,\nID2 String\n);\n\nCREATE OR REPLACE STREAM CSVTypeStream OF CSVStream_Type;\n\nCREATE OR REPLACE CQ CQ1\nINSERT INTO CSVTypeStream\nSELECT TO_STRING(data[0]),TO_STRING(data[1])\nFROM CsvStream;\n\nCREATE TARGET WriteintoKudu using KuduWriter (\nKuduClientConfig:'master.addresses->192.168.56.101:7051;socketreadtimeout->10000;operationtimeout->30000',\nTables: 'INTEGRATIONTEST',\nBatchPolicy: 'EventCount:1,Interval:10') INPUT FROM CSVTypeStream;\n\nEND APPLICATION FileReaderToKuduWriter;\ndeploy application FileReaderToKuduWriter;\nstart FileReaderToKuduWriter;", "generated_queries": "1. What is the recovery interval set for the application named FileReaderToKuduWriter?\n\n2. How is the CSV data polled and parsed in the TQL query that involves the application FileReaderToKuduWriter?\n\n3. Which Kudu table is the data from the CSVStream being written into, and what are the specified batch and timeout configurations for this operation in the TQL query?", "file_name": "fileReaderToKudu.tql"}
{"tql": "CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (\n  ConnectionURL: @sourceURL@,\n  Username: '@userName',\n  Tables: '@tables@',\n  CheckColumn: '@checkColumn',\n  FetchSize: 1,\n  ReturnDateTimeAs: 'JODA',\n  Password: '@password',\n  Password_encrypted: 'false',\n  DatabaseProviderType: 'Default',\n  ThreadPoolSize: 5,\n  pollingInterval: '2sec',\n  ConnectionPoolSize: 1 )\nOUTPUT TO @STREAM@;", "generated_queries": "1. \"Can you provide me with the most up-to-date data from the specified tables in the database, with changes being monitored in real-time?\"\n2. \"How can I incrementally synchronize data from specific tables in a database with my streaming application, ensuring efficient resource utilization and a rapid polling interval of 2 seconds?\"\n3. \"Is it possible to configure a data source to use a thread pool size of 5 for parallel processing, with minimal connection pooling to the database to reduce resource overhead?\"", "file_name": "IncrementalBatchReader_NoStartPosition.tql"}
{"tql": "Stop Oracle_IRLogWriter;\nUndeploy application Oracle_IRLogWriter;\ndrop application Oracle_IRLogWriter cascade;\n\nCREATE APPLICATION Oracle_IRLogWriter WITH ENCRYPTION recovery 5 second interval;\n\nCREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( \n  FetchSize: 1,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.TDSOURCE',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.TEST01=ID;',\n  PollingInterval: '5sec',\n  ReturnDateTimeAs: 'String',\n  startPosition:'striim.test01=0'\n  )\n  OUTPUT TO data_stream;\n\n  CREATE OR REPLACE TARGET Oracle_IRSys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM data_stream;\n\nCREATE TARGET BinaryDump USING LogWriter(\n  name: 'TeraData',\n  filename:'TeraData.log'\n)INPUT FROM data_stream;\n\nEND APPLICATION Oracle_IRLogWriter;\n\ndeploy application Oracle_IRLogWriter in default;\n\nstart application Oracle_IRLogWriter;", "generated_queries": "1. What are the steps required to create and deploy an application named \"Oracle_IRLogWriter\" in a Teradata environment using Striim, with specific settings for source and target data processing?\n\n2. How can I configure data replication from a Teradata database to a target system using Striim's IncrementalBatchReader and LogWriter adapters, with a recovery interval of 5 seconds and a polling interval of 5 seconds?\n\n3. Can you provide the TQL commands needed to start, stop, undeploy, and drop the \"Oracle_IRLogWriter\" application in a Striim server environment interacting with a Teradata database?", "file_name": "OracleIR_LogWriter.tql"}
{"tql": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW ;\n\n \n\nCREATE OR REPLACE SOURCE DBSource USING DatabaseReader  ( \n  Username: '@SOURCE_USER@',\n  Password_encrypted: false,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'DatabaseReader',\n  FetchSize: 100,\n  Password: '@SOURCE_PASS@'\n ) \nOUTPUT TO Oracle_ChangeDataStream;\n\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1000,Interval:0',\n  CommitPolicy: 'EventCount:1000,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n ) \nINPUT FROM Oracle_ChangeDataStream;\n\ncreate Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\nEND APPLICATION DBRTOCW;\ndeploy application DBRTOCW in default;\nstart DBRTOCW;", "generated_queries": "1. What are the steps required to stop and undeploy the application named \"DBRTOCW\" and recreate it in a data integration platform?\n   \n2. How can I configure a source database connection using DatabaseReader to read data from specific tables in a relational database for the application \"DBRTOCW\"?\n   \n3. Can you provide details on setting up a target database connection, including batch and commit policies, to efficiently write data from the application \"DBRTOCW\" to a target database using a specified adapter?", "file_name": "OracToCassandra_InitLoad.tql"}
{"tql": "stop application @APPNAME1@;\nundeploy application @APPNAME1@;\nstop application @APPNAME2@;\nundeploy application @APPNAME2@;\ndrop application @APPNAME1@ cascade;\ndrop application @APPNAME2@ cascade;\n\n\nCREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\nCREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;\nCREATE OR REPLACE SOURCE @SourceName@ Using MysqlReader\n(\n   adapterName: MysqlReader,\n   CDDLAction: Process,\n   CDDLCapture: false,\n   Compression: false,\n   ConnectionURL: jdbc:mysql://localhost:3306/waction,\n   FilterTransactionBoundaries: true,\n   Password: ReaderPassword,\n   SendBeforeImage: true,\n   Tables: srcTable,\n   Username: ReaderUsername\n)OUTPUT TO @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME1@;\nDEPLOY APPLICATION @APPNAME1@;\nSTART APPLICATION @APPNAME1@;\n\nCREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  \n(\n  DatabaseProviderType:'Default',\n  CheckPointTable:'CHKPOINT',\n  PreserveSourceTransactionBoundary:'false',\n  Username:'qatest',\n  BatchPolicy:'EventCount:1',\n  CommitPolicy:'EventCount:1',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',\n  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',\n  Password:'qatest',\n  CDDLAction:'Process'\n) \nINPUT FROM @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME2@;\nDEPLOY APPLICATION @APPNAME2@;\nSTART APPLICATION @APPNAME2@;", "generated_queries": "1. What are the steps involved in stopping and undeploying two specific applications named APPNAME1 and APPNAME2, dropping them along with their dependencies, and then recreating and deploying them with recovery settings and stream and source configurations in a specific environment?\n   \n2. How can I set up a data pipeline where data from a MySQL database table named srcTable is read using a MysqlReader adapter with specific connection details and output to a Kafka stream called SRCINPUTSTREAM, which is then consumed by an application named APPNAME2 and written to Oracle tables using a DatabaseWriter target configuration?\n\n3. After stopping and undeploying two applications named APPNAME1 and APPNAME2, how do I recreate them with recovery settings, deploy them, and start them in order to maintain continuous data processing from a MySQL source to an Oracle target with specific data flow and transaction boundary settings?", "file_name": "MySQL_QuiesceCascade_SingReader.tql"}
{"tql": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.TABLE_TEST_%,@DATASET@.%',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:10',\nStandardSQL:true\t\n) INPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@2 USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.TABLE_TEST_%,@DATASET@.%',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:10',\nStandardSQL:true\t\n) INPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@3 USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.TABLE_TEST_%,@DATASET@.%',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:10',\nStandardSQL:true\t\n) INPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@4 USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.TABLE_TEST_%,@DATASET@.%',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:10',\nStandardSQL:true\t\n) INPUT FROM @STREAM@;\n\nCREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(\n  name: 'CDDL',\n  filename:'WAEventDump.log'\n)INPUT FROM @STREAM@;;\n\ncreate Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;", "generated_queries": "1. Which BigQuery tables are being written to by the targets specified in the TQL query?\n   \n2. Can you provide details about the configuration settings, such as encoding, data location, and batch policy, for the BigQueryWriter targets created in the TQL query?\n\n3. How many targets are being created in the TQL query using the BigQueryWriter and LogWriter components, and what are their respective configurations and input sources?", "file_name": "BigQueryWriter_multi.tql"}
{"tql": "stop PatternMatchingTimer.CSV;\nundeploy application PatternMatchingTimer.CSV;\ndrop application PatternMatchingTimer.CSV cascade;\n\ncreate application CSV;\n\ncreate source CSVSource using CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'ctest.csv',\n  columndelimiter:',',\n  positionByEOF:false\n)\nOUTPUT TO CsvStream;\n\nCREATE CQ ParseUserData\nINSERT INTO UserDataStream\nSELECT  TO_INT(data[0]) as UserId,\n\t    TO_INT(data[1]) as temp1,\n        TO_DOUBLE(data[2]) as temp2,\n\t    TO_STRING(data[3]) as temp3\nFROM CsvStream;\n\n-- scenario 1.1 check pattern using timer within 10 seconds and wait\nCREATE CQ TypeConversionTimerCQ1\nINSERT INTO TypedStream1\nSELECT UserId as typeduserid,\n\tA.temp1 as typedtemp1,\n\tB.temp2 as typedtemp2,\n\tC.temp3 as typedtemp3\nfrom UserDataStream\nMATCH_PATTERN T A (W | B | C)\ndefine T = timer(interval 10 second),\nA = UserDataStream(temp1 >= 20), B= UserDataStream(temp2 < 30.40), C= UserDataStream(temp3 = 'Bret'), W = wait(T)\nPARTITION BY UserId;\n\n-- scenario 1.2 check pattern using timer within 20 seconds\nCREATE CQ TypeConversionTimerCQ2\nINSERT INTO TypedStream2\nSELECT UserId as typeduserid,\n\tA.temp1 as typedtemp1,\n\tB.temp2 as typedtemp2,\n\tC.temp3 as typedtemp3\nfrom UserDataStream\nMATCH_PATTERN T A C\ndefine T = timer(interval 20 second), B= UserDataStream(temp2 < 30.40), C= UserDataStream(temp3 = 'zalak'),\nA = UserDataStream(temp1 >= 20)\nPARTITION BY UserId;\n\n-- scenario 1.3 check pattern using timer within 5 seconds with between values\nCREATE CQ TypeConversionTimerCQ3\nINSERT INTO TypedStream3\nSELECT UserId as typeduserid,\n\t   A.temp1 as typedtemp1\nfrom UserDataStream\nMATCH_PATTERN T A\ndefine T = timer(interval 5 second),\nA = UserDataStream(temp1 between 10 and 40)\nPARTITION BY UserId;\n\n-- scenario 1.4 check pattern using timer which match no events\nCREATE CQ TypeConversionTimerCQ4\nINSERT INTO TypedStream4\nSELECT UserId as typeduserid\nfrom UserDataStream\nMATCH_PATTERN T W\ndefine T = timer(interval 50 second), W = wait(T)\nPARTITION BY UserId;\n\n-- scenario 1.5 check pattern using stop timer\nCREATE CQ TypeConversionTimerCQ5\nINSERT INTO TypedStream5\nSELECT UserId as typeduserid,\n       A.temp1 as typedtemp1,\n       B.temp2 as typedtemp2\nfrom UserDataStream\nMATCH_PATTERN T A C T2 B\ndefine\nT = timer(interval 50 second),\nA = UserDataStream(temp1 between 10 and 40),\nC = stoptimer(T),\nT2 = timer(interval 30 second),\nB = UserDataStream(temp2 >= 20)\nPARTITION BY UserId;\n\nCREATE WACTIONSTORE UserActivityInfoTimer1\nCONTEXT OF TypedStream1_Type\nEVENT TYPES ( TypedStream1_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfoTimer2\nCONTEXT OF TypedStream2_Type\nEVENT TYPES ( TypedStream2_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfoTimer3\nCONTEXT OF TypedStream3_Type\nEVENT TYPES ( TypedStream3_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfoTimer4\nCONTEXT OF TypedStream4_Type\nEVENT TYPES ( TypedStream4_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfoTimer5\nCONTEXT OF TypedStream5_Type\nEVENT TYPES ( TypedStream5_Type )\n@PERSIST-TYPE@\n\n--get data from UserDataStream and place into wactionStore UserWaction\nCREATE CQ UserWaction1\nINSERT INTO UserActivityInfoTimer1\nSELECT * FROM TypedStream1\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction2\nINSERT INTO UserActivityInfoTimer2\nSELECT * FROM TypedStream2\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction3\nINSERT INTO UserActivityInfoTimer3\nSELECT * FROM TypedStream3\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction4\nINSERT INTO UserActivityInfoTimer4\nSELECT * FROM TypedStream4\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction5\nINSERT INTO UserActivityInfoTimer5\nSELECT * FROM TypedStream5\nLINK SOURCE EVENT;\n\nend application CSV;\ndeploy application csv;\nstart csv;", "generated_queries": "1. What is the user activity information for scenarios where a pattern match occurs within 10 seconds, waiting for specific conditions, and partitioned by user ID?\n   \n2. How does the system handle pattern matching within 20 seconds based on certain conditions, without waiting, and partitioned by user ID?\n\n3. Can you explain how the system identifies specific user actions based on defined patterns with timers, stops, and waiting conditions, and stores this information in different action stores as per given scenarios?", "file_name": "PatternTimer.tql"}
{"tql": "stop @appname@;\nundeploy application @appname@;\nDROP APPLICATION @appname@ CASCADE;\nCREATE APPLICATION @appname@;\n\nCREATE SOURCE @appname@_src USING databaseReader  (\n  Username: '@@',\n  Password: '@@',\n  ConnectionURL: '@@',\n  Tables: '@@',\n  FetchSize: '100'\n )\nOUTPUT TO @appname@_ss;\n\nCREATE JUMPING WINDOW @appname@_win OVER @appname@_ss KEEP @winsize@ ROWS;\n\nCREATE TYPE @appname@_MapType\n    (   \n       id INTEGER,\n        name STRING,\n        city  STRING\n    );\n    \nCREATE EXTERNAL CACHE @appname@_cach (\n  AdapterName: 'DatabaseReader',\n    ConnectionURL: '@url@',\n    UserName: '@uname@',\n    Password: '@pwd@',\n   Table: '@tablename@',\n  FetchSize: 100,\n  Columns: 'id,name,city',\n  trimquote: false,\n  KeyToMap: '@key@'\n )\n OF @appname@_MapType;\n \nCREATE TYPE @appname@_MapTypenew\n    (   id_t            INTEGER,\n        name_t           STRING,\n        city_t            STRING,\n        id_c            INTEGER,\n        name_c            STRING,\n        city_c            STRING\n    );\n    \nCREATE STREAM @appname@_JoinedData OF @appname@_MapTypenew;\n\nCREATE CQ @appname@_JoinDataCQ\nINSERT INTO @appname@_JoinedData\nSELECT  TO_INT(f.data[0]),\n        TO_STRING(f.data[1]),\n        TO_STRING(f.data[2]),\n        z.id,\n        z.name,\n        z.city\nFROM @appname@_win f, @appname@_cach z\nwhere TO_INT(f.data[0]) = z.id\n@Ex@;\n\nCREATE TARGET @appname@_tgt USING DatabaseWriter\n(\n  ConnectionURL:'@@',\n  Username:'@@',\n  Password:'@@',\n  BatchPolicy:'Eventcount:10000,Interval:1',\n  CommitPolicy:'Interval:1,Eventcount:10000',\n  Tables:'@@'\n) \nINPUT FROM @appname@_JoinedData;\n\nEND APPLICATION @appname@;\ndeploy application @appname@;\nstart @appname@;", "generated_queries": "1. What is the process for undeploying and then re-deploying an application named @appname@ that includes creating source data from a database, defining a jumping window, creating cache, joining data, and sending the result to a target database?\n\n2. How can I dynamically join streaming data from a jumping window with data from an external cache in TQL using a specified key and then insert the combined data into a target database table?\n\n3. What are the steps required to set up a real-time data processing pipeline with Apache Flink, involving creating source data from a database, defining a jumping window, creating an external cache, joining data from the window with cache data, and finally writing the joined data to a target database table?", "file_name": "excache.tql"}
{"tql": "stop application @APPNAME1@;\nundeploy application @APPNAME1@;\nstop application @APPNAME2@;\nundeploy application @APPNAME2@;\ndrop application @APPNAME1@ cascade;\ndrop application @APPNAME2@ cascade;\n\n\nCREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\nCREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;\nCREATE OR REPLACE SOURCE @SourceName@ Using MysqlReader\n(\n   adapterName: MysqlReader,\n   CDDLAction: Process,\n   CDDLCapture: false,\n   Compression: false,\n   ConnectionURL: jdbc:mysql://localhost:3306/waction,\n   FilterTransactionBoundaries: true,\n   Password: ReaderPassword,\n   SendBeforeImage: true,\n   Tables: waction.MultiMultiDownstream_src,\n   Username: ReaderUsername\n)OUTPUT TO @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME1@;\nDEPLOY APPLICATION @APPNAME1@;\nSTART APPLICATION @APPNAME1@;\n\nCREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  \n(\n  DatabaseProviderType:'Default',\n  CheckPointTable:'CHKPOINT',\n  PreserveSourceTransactionBoundary:'false',\n  Username:'qatest',\n  BatchPolicy:'EventCount:1',\n  CommitPolicy:'EventCount:1',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',\n  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',\n  Password:'qatest',\n  CDDLAction:'Process'\n) \nINPUT FROM @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME2@;\nDEPLOY APPLICATION @APPNAME2@;\nSTART APPLICATION @APPNAME2@;\n\nCREATE APPLICATION @APPNAME3@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE TARGET @targetName1@ USING DatabaseWriter  \n(\n  DatabaseProviderType:'Default',\n  CheckPointTable:'CHKPOINT',\n  PreserveSourceTransactionBoundary:'false',\n  Username:'qatest',\n  BatchPolicy:'EventCount:1',\n  CommitPolicy:'EventCount:1',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',\n  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',\n  Password:'qatest',\n  CDDLAction:'Process'\n) \nINPUT FROM @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME3@;\nDEPLOY APPLICATION @APPNAME3@;\nSTART APPLICATION @APPNAME3@;", "generated_queries": "1. How can I stop, undeploy, drop, create, deploy, and start multiple applications with specified configurations and recovery intervals in a streaming data processing environment?\n2. What are the steps to set up and configure sources and targets with specific properties such as adapter settings, connection URLs, and batch policies using a TQL query?\n3. Can I create and manage multiple applications with individual recovery intervals, input sources, and database targets by executing a single query with TQL commands?", "file_name": "MySQL_QuiesceCascade_MultiReader.tql"}
{"tql": "--spool on to '/Users/jeyaselvan/Product/spoolfile.log';\nselect * from Oracle12C_To_Oracle12CApp_ExceptionStore;\n--spool off;", "generated_queries": "1. What are all the entries in the Oracle 12C to Oracle 12C Application Exception Store?\n2. Can you provide me with the complete list of exceptions stored in the Oracle 12C to Oracle 12C Application Exception Store?\n3. How can I retrieve all the records from the Oracle 12C to Oracle 12C Application Exception Store?", "file_name": "ExceptionStore_Spool.tql"}
{"tql": "--\n-- Crash Recovery Test 1 on Four node all server cluster \n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> WS\n--\n\nSTOP N4S4CR1Tester.N4S4CRTest1;\nUNDEPLOY APPLICATION N4S4CR1Tester.N4S4CRTest1;\nDROP APPLICATION N4S4CR1Tester.N4S4CRTest1 CASCADE;\nCREATE APPLICATION N4S4CRTest1 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN4S4CRTest1;\n\nCREATE SOURCE CsvSourceN4S4CRTest1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN4S4CRTest1;\n\nCREATE FLOW DataProcessingN4S4CRTest1;\n\nCREATE TYPE WactionTypeN4S4CRTest1 (\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE WactionsN4S4CRTest1 CONTEXT OF WactionTypeN4S4CRTest1\nEVENT TYPES ( WactionTypeN4S4CRTest1 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsN4S4CRTest1\nINSERT INTO WactionsN4S4CRTest1\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nEND FLOW DataProcessingN4S4CRTest1;\n\nEND APPLICATION N4S4CRTest1;", "generated_queries": "1. How can I create a data processing application for crash recovery testing on a four-node all server cluster using TQL?\n   \n2. What are the steps to deploy a data acquisition flow for crash recovery testing on a four-node all server cluster in TQL?\n   \n3. Can you guide me through setting up a real-time data insertion process into a Waction store for crash recovery testing on a four-node all server cluster using TQL?", "file_name": "KStreamN4S4CRTest1.tql"}
{"tql": "create application KinesisTest \n RECOVERY 10 SECOND INTERVAL\n;\ncreate source CSVSource using FileReader (\n\tdirectory:'/home/dz/src/product/Samples/AppData',\n\tWildCard:'posdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  companyName String,\n  merID String,\n  primNum String,\n  posDataCode int,\n  dateTime String,\n  expDate String,\n  curCode String,\n  Amnt String,\n  termId String,\n  zip String,\n  city String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],data[2],TO_INT(data[3]),\n       data[4],data[5],data[6],\n       data[7],data[8],\n       data[9],data[10]\nFROM CsvStream;\n\ncreate or replace Target t using KinesisWriter (\n\tregionName:'TARGET_REGION',\n\tstreamName:'TARGET_STREAM'\n)\nformat using DSVFormatter (\n)\ninput from TypedCSVStream;\nend application KinesisTest;\ndeploy application KinesisTest in default;\nstart application KinesisTest;", "generated_queries": "1. What is the process for creating an application in TQL that reads data from a CSV file using FileReader, parses it using DSVParser, and writes the output to a typed stream called TypedCSVStream?\n\n2. How can I configure a continuous query in TQL that extracts specific fields from a CSV data stream and inserts them into a typed stream named TypedCSVStream?\n\n3. What steps are involved in deploying and starting an application named KinesisTest in TQL, which reads CSV data, processes it, and writes the output to a Kinesis stream using a KinesisWriter target?", "file_name": "KinesisTest_Random.tql"}
{"tql": "IMPORT static com.webaction.runtime.converters.DateConverter.*;\n\nUNDEPLOY APPLICATION admin.SQLMXReaderApp;\nDROP APPLICATION admin.SQLMXReaderApp cascade;\n\nCREATE APPLICATION SQLMXReaderApp;\ncreate source SQMXSource using HPNonStopSQLMXReader (\n\tportno:2020,\n\tipaddress:'10.10.196.122',\n\tName:'intg',\n\tAuditTrails:'parallel',\n\tAgentPortNo:8012,\n\tAgentIpAddress:'10.10.197.116', \n\tTables:'watest.wasch.sqlmxtest1;watest.wasch.sqlmxtest2') output to CDCStream,\n\tSQLMXMATStream MAP (table:'WATEST.WASCH.SQLMXTEST2');\n\n\nCREATE TYPE SQLMXTEST2Data(\n    C0 Integer,\n    C1 String,\n    C2 Short,\n    OPR String,\n    TABLENAME String,\n    AUXNAME String\n);\n\nCREATE STREAM SQLMXTEST2Stream OF SQLMXTEST2Data;\n\n\nCREATE JUMPING WINDOW SQLMXDataWindow\nOVER SQLMXTEST2Stream KEEP 4 ROWS\nPARTITION BY OPR;\n\n\nCREATE CQ ToSQLMXData\nINSERT INTO SQLMXTEST2Stream\nSELECT TO_INT(data[0]),\n\t   data[1],\n       TO_SHORT(data[2]),\n       META(x,\"OperationName\").toString(),\n       META(x, \"TableName\").toString(),\n       META(x,\"AuditTrailName\").toString()\nFROM CDCStream x\nWHERE not(META(x,\"OperationName\").toString() = \"BEGIN\") AND not(META(x,\"OperationName\").toString() = \"COMMIT\") AND not(META(x, \"TableName\").toString() is null) \nAND META(x, \"TableName\").toString() = \"WATEST.WASCH.SQLMXTEST1\" AND META(x, \"AuditTrailName\").toString() = \"MAT\";\n\n\n--CREATE TARGET SQLMXSYSOUT using SysOut(name:sqlmx) INPUT FROM CDCStream;\nCREATE TARGET SQLMXMAT USING LogWriter(\n  name:SQLMXReaderAppMAT,\nfilename:'@FEATURE-DIR@/logs/sqlmxmat.log'\n--  filename:'mat.log'\n) INPUT FROM SQLMXMATStream;\n\nCREATE TARGET SQLMXAUX01 USING LogWriter(\n  name:SQLMXReaderAppMAT1,\nfilename:'@FEATURE-DIR@/logs/sqlmxmat1.log'\n--  filename:'aux1.log'\n) INPUT FROM SQLMXTEST2Stream;\n\n\nEND APPLICATION SQLMXReaderApp;\ndeploy application SQLMXReaderApp in default;", "generated_queries": "1. What is the configuration of the SQLMXReader application in terms of data sources and data manipulation processes?\n2. How is data from tables 'watest.wasch.sqlmxtest1' and 'watest.wasch.sqlmxtest2' processed and transformed within the SQLMXReader application?\n3. Can you provide details on the audit trail settings and output destinations for data processing within the SQLMXReader application?", "file_name": "SQLMXReaderApp.tql"}
{"tql": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\nCREATE APPLICATION @APPNAME@ WITH ENCRYPTION @Recovery@ USE EXCEPTIONSTORE;\ncreate flow @APPNAME@_SourceFlow;\nCREATE SOURCE @APPNAME@_s USING FileReader(\n  directory:'Samples/AppData',\n  wildcard:'PO.JSON',\n  positionByEOF:false\n)\nparse using JSONParser (\n) OUTPUT TO @APPNAME@_ss1;\nend flow @APPNAME@_SourceFlow;\ncreate target @APPNAME@_t using sysout (name:ss1) input from @APPNAME@_ss1;\nend application @APPNAME@;\ndeploy application @APPNAME@ @DP@;\nstart @APPNAME@;", "generated_queries": "1. What is the process for stopping, undeploying, and dropping an application in a TQL environment with encryption and an exception store?\n2. How can I create a flow with a source reading from a directory in a TQL system, parsing JSON data, and outputting to a target named 'ss1'?\n3. In TQL, what are the steps to deploy and start an application named '@APPNAME@' with a specific data processing configuration?", "file_name": "LateCheckPointJson.tql"}
{"tql": "--\n-- Recovery Test 38 with two sources, two jumping time-count windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jc5t3W/p -> CQ1 -> WS\n--   S2 -> Jc6t4W/p -> CQ2 -> WS\n--\n\nSTOP Recov38Tester.RecovTest38;\nUNDEPLOY APPLICATION Recov38Tester.RecovTest38;\nDROP APPLICATION Recov38Tester.RecovTest38 CASCADE;\nCREATE APPLICATION RecovTest38 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Rows3Seconds\nOVER DataStream1 KEEP 5 ROWS WITHIN 3 SECOND\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Rows4Seconds\nOVER DataStream2 KEEP 6 ROWS WITHIN 4 SECOND\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataStream5Rows3Seconds\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Rows3Seconds p\nGROUP BY p.merchantId;\n\nCREATE CQ DataStream6Rows4Seconds\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Rows4Seconds p\nGROUP BY p.merchantId;\n\nEND APPLICATION RecovTest38;", "generated_queries": "1. How can I set up a recovery test scenario with two data sources, two jumping time-count windows, and a wactionstore all partitioned on the same key?\n  \n2. What is the process for mapping CSV data from two sources into corresponding streams and windows while ensuring they are partitioned by the merchant ID?\n\n3. How can I create continuous queries to calculate certain aggregated metrics like the total number of companies and the first merchant ID within specific time-count windows based on CSV data input?", "file_name": "RecovTest38.tql"}
{"tql": "create or replace propertyvariable TEAMSREFTOKEN = '@refershtoken@';\n create or replace propertyvariable TEAMSCLIENTSECRET = '@clientSecret@';\n create or replace propertyvariable TEAMSCHANNELURL = '@channelUrl@';\n create or replace propertyvariable TEAMSCLIENTID = '@clientID@';\n\nCREATE APPLICATION @AppName@ WITH ENCRYPTION EXCEPTIONHANDLER (AdapterException: 'IGNORE', ArithmeticException: 'IGNORE', ClassCastException: 'IGNORE', ConnectionException: 'IGNORE', InvalidDataException: 'IGNORE', NullPointerException: 'IGNORE', NumberFormatException: 'IGNORE', SystemException: 'IGNORE', UnExpectedDDLException: 'IGNORE', UnknownException: 'IGNORE')  USE EXCEPTIONSTORE TTL : '7d' ;\n\nCREATE OR REPLACE SOURCE OracleReader_AlertSource USING oraclereader (\n  ConnectionURL: '@ConnectionURl@',\n  Tables: '@table@',\n  Password: '@password@',\n  Username: '@username@' )\nOUTPUT TO AlertUpgradeStream;\n\nCREATE STREAM AlertStream OF Global.AlertEvent;\n\nCREATE STREAM SlackAndTeamsStream OF Global.SlackAlertEvent;\n\nCREATE OR REPLACE TARGET SmartAlertUpgradeSysOut USING SysOut (\n  name: 'SmartAlertUpgradeSysOut' )\nINPUT FROM AlertUpgradeStream;\n\nCREATE OR REPLACE CQ GenerateAlertsForWebAndEmail\nINSERT INTO AlertStream\nSELECT data[1],\n data[1],\n 'info',\n 'raise',\n'Welcome\\n to Striim'\nFROM AlertUpgradeStream;\n\n\n CREATE OR REPLACE CQ GenerateAlertsForSlackAndTeams\n INSERT INTO SlackAndTeamsStream\n SELECT data[1],\n  data[1],\n 'info',\n 'raise',\n 'Welcome\\n to Striim',\n 'TestChannel'\n FROM AlertUpgradeStream;\n\n\nCREATE TARGET TeamsAlertSender USING Global.TeamsAlertAdapter (\n  refreshToken: '$TEAMSREFTOKEN',\n  clientSecret: '$TEAMSCLIENTSECRET',\n  channelURL: '$TEAMSCHANNELURL',\n  clientID: '$TEAMSCLIENTID' )\nINPUT FROM SlackAndTeamsStream;\n\n\nCREATE SUBSCRIPTION WebAlertSender USING WebAlertAdapter (\n  isSubscription: 'true',\n  channelName: 'admin_PosAppWebAlert' )\nINPUT FROM AlertStream;\n\n\nCREATE TARGET slackalertSender USING Global.SlackAlertAdapter (\n  OauthToken: '@oauth@',\n  ChannelName: '@slackChannel@',\n  OauthToken_encrypted: 'false' )\nINPUT FROM SlackAndTeamsStream;\n\nCREATE SUBSCRIPTION EmailAlertsender USING EmailAdapter (\n  smtpurl: '@smtpUrl@',\n  starttls_enable: '@starttls@',\n  SMTPUSER: '@smtpuser@',\n  SMTPPASSWORD: '@stmpPwsd@',\n  emailList: '@emailList@',\n  subject: '@subject@',\n  senderEmail: '@senderEmail@',\n  SMTPPASSWORD_encrypted: 'false')\nINPUT FROM AlertStream;\n\nEND APPLICATION @AppName@;", "generated_queries": "1. How can I configure Striim to generate alerts for web and email based on data from an Oracle database table?\n2. What steps are involved in setting up alert generation for Slack and Microsoft Teams channels in Striim based on specific event data?\n3. How can I encrypt sensitive information such as connection URLs, passwords, and OAuth tokens while configuring alert handling and delivery within a Striim application named @AppName@?", "file_name": "UpgradeSmartAlert.tql"}
{"tql": "use admin;\ndrop namespace @namespace@ cascade;\ncreate namespace @namespace@;\nuse @namespace@;\nCREATE APPLICATION @appName@;\ncreate flow @flowName@;\nCREATE SOURCE @appName@_s1 USING Global.FileReader (\n  wildcard: 'posdata100.csv',\n  blocksize: 64,\n  directory: '@TestDataDir@',\n  positionByEOF:false)\nPARSE USING Global.DSVParser ()\nOUTPUT TO @appName@_st1;\nend flow srcFlow;\ncreate target @appName@_t1 using NullWriter() input from @appName@_st1;\nEND APPLICATION @appName@;", "generated_queries": "1. How can I create a new namespace and application in my database administration tool?\n  \n2. What is the process for setting up a source to read data from a CSV file in a specific directory and parsing it using a DSV parser within a flow in my application?\n  \n3. How do I configure a target to receive data input from a specific source within an application in my database system?", "file_name": "agentApp.tql"}
{"tql": "-- The PosApp sample application demonstrates how a credit card\n-- payment processor might use Striim to generate reports on current\n-- transaction activity by merchant and send alerts when transaction\n-- counts for a merchant are higher or lower than average for the time\n-- of day.\n\nstop admin.PosApp;\nundeploy application admin.PosApp;\ndrop application admin.PosApp cascade;\n\nCREATE APPLICATION PosApp;\n\n-- All CREATE statements between here and the END APPLICATION\n-- statement will create objects in the PosApp application.\n\n-- source CsvDataSource\n\nCREATE source CsvDataSource USING FileReader (\n  directory:'Samples/Customer/PosApp/appData',\n  wildcard:'$wildcard',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\n\n-- This is the primary data source for this application.\n-- In a real-world application, it would be real-time data. Here,\n-- the data comes from a comma-delimited file, posdata.csv. The first\n-- two lines of that file are:\n--\n-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY\n-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand\n--\n-- The \"header:Yes\" setting tells Striim that the first line contains\n-- field labels that should not be treated as data.\n--\n-- The \"positionByEOF:false\" setting tells Striim to start reading\n-- from the beginning of the file. (In a real-world application\n-- reading real log files, you would typically use the default \"true\"\n-- setting so that the application would read only new data.)\n--\n-- The OUTPUT TO clause automatically creates the stream\n-- CsvStream using the WAEvent type associated with the CSVReader\n-- adapater. The only field from WAEvent used by this application\n-- is \"data\", an array containing the delimited fields.\n\n\n-- CQ CsvToPosData\n\nCREATE CQ CsvToPosData\nINSERT INTO PosDataStream\nSELECT TO_STRING(data[1]) as merchantId,\n       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,\n       TO_DOUBLE(data[7]) as amount,\n       TO_STRING(data[9]) as zip\nFROM CsvStream;\n\n-- Here, \"data\" refers to the array mentioned above, and the number\n-- in brackets specifies a field from the array, counting from zero.\n-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH\n-- AMOUNT, and data[9] is ZIP.\n--\n-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as\n-- the types to be used in PosDataStream, which is created automatically.\n\n-- The DATETIME field from the source is converted to both a dateTime\n-- value, used as the event timestamp by the application, and (via the\n-- function) an integer hourValue, which is used to look up\n-- historical hourly averages from the HourlyAveLookup cache,\n-- discussed below.\n--\n-- The other six fields are discarded. Thus the first line of data\n-- from posdata.csv has at this point been reduced to five values:\n--\n-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)\n-- 20130312173210 (DateTime)\n-- 17 (hourValue)\n-- 2.20 (amount)\n-- 41363 (zip)\n\n\n-- CQ GenerateMerchantTxRateOnly\n--\n-- The PosData5Minutes window bounds the data so that the query can\n-- use the COUNT, FIRST, and SUM functions and join data from the\n-- HourlyAveLookup cache. (Aggregate functions cannot be used and\n-- joins cannot be performed on unbound real-time data.)\n--\n-- The HourlyAveLookup cache provides historical average sales\n-- amounts for the current hour for each merchant.\n\nCREATE JUMPING WINDOW PosData5Minutes\nOVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;\n\nCREATE TYPE MerchantHourlyAve(\n  merchantId String,\n  hourValue integer,\n  hourlyAve integer\n);\nCREATE CACHE HourlyAveLookup using FileReader (\n  directory: 'Samples/Customer/PosApp/appData',\n  wildcard: 'hourlyData.txt'\n)\nPARSE USING DSVParser (\n  header: Yes,\n  trimquote:false\n) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;\n\nCREATE TYPE MerchantTxRate(\n  merchantId String KEY,\n  zip String,\n  startingTime DateTime,\n  count integer,\n  totalAmount double,\n  hourlyAve integer,\n  upperLimit double,\n  lowerLimit double,\n  category String,\n  status String\n);\nCREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;\n\nCREATE CQ GenerateMerchantTxRateOnly\nINSERT INTO MerchantTxRateOnlyStream\nSELECT p.merchantId,\n       FIRST(p.zip),\n       FIRST(p.dateTime),\n       COUNT(p.merchantId),\n       SUM(p.amount),\n       FIRST(l.hourlyAve/12),\n       FIRST(l.hourlyAve/12 * CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       FIRST(l.hourlyAve/12 / CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       '<NOTSET>',\n       '<NOTSET>'\nFROM PosData5Minutes p, HourlyAveLookup l\nWHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue\nGROUP BY p.merchantId;\n\n-- This query aggregates five minutes' worth of data for each\n-- merchant, calculating the total transaction count and amount, and\n-- calculates the upperLimit and lowerLimit values based on the\n-- historical average transaction count for the current hour of the\n-- day from the HourlyAveLookup cache. The category and status fields\n-- are left unset to be populated by the next query.\n\n\n-- CQ GenerateMerchantTxRateWithStatus\n--\n-- This query sets the count values used by the Dashboard map and the\n-- status values used to trigger alerts.\n\nCREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;\n\nCREATE CQ GenerateMerchantTxRateWithStatus\nINSERT INTO MerchantTxRateWithStatusStream\nSELECT merchantId,\n       zip,\n       startingTime,\n       count,\n       totalAmount,\n       hourlyAve,\n       upperLimit,\n       lowerLimit,\n       CASE\n         WHEN count >10000 THEN 'HOT'\n         WHEN count > 800 THEN 'WARM'\n         WHEN count >200 THEN 'COOL'\n         ELSE 'COLD' END,\n       CASE\n         WHEN count > upperLimit THEN 'TOOHIGH'\n         WHEN count < lowerLimit THEN 'TOOLOW'\n         ELSE 'OK' END\nFROM MerchantTxRateOnlyStream;\n\n\n-- WAction store MerchantActivity\n--\n-- The following group of statements create and populate the MerchantActivity\n-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced\n-- with merchant details from NameLookup cache and with latitude and longitude\n-- values from the USAddressData cache.\n\nCREATE TYPE MerchantActivityContext(\n  MerchantId String KEY,\n  StartTime  DateTime,\n  CompanyName String,\n  Category String,\n  Status String,\n  Count integer,\n  HourlyAve integer,\n  UpperLimit double,\n  LowerLimit double,\n  Zip String,\n  City String,\n  State String,\n  LatVal double,\n  LongVal double\n);\n\nCREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext\nEVENT TYPES ( MerchantTxRate )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\n\nCREATE TYPE MerchantNameData(\n  merchantId String KEY,\n  companyName String\n);\n\nCREATE TYPE USAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\nCREATE CACHE NameLookup using FileReader (\n  directory:'Samples/Customer/PosApp/appData',\n  WildCard:'MerchantNames.csv',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:'yes',\n  trimquote:false\n)\nQUERY(keytomap:'merchantId') OF MerchantNameData;\n\nCREATE CACHE ZipLookup using FileReader (\n  directory: 'Samples/Customer/PosApp/appData',\n  wildcard: 'USAddresses.txt',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header: Yes,\n  columndelimiter: '\\t',\n  trimquote:false\n) QUERY (keytomap:'zip') OF USAddressData;\n\nCREATE CQ GenerateWactionContext\nINSERT INTO MerchantActivity\nSELECT  m.merchantId,\n        m.startingTime,\n        n.companyName,\n        m.category,\n        m.status,\n        m.count,\n        m.hourlyAve,\n        m.upperLimit,\n        m.lowerLimit,\n        m.zip,\n        z.city,\n        z.state,\n        z.latVal,\n        z.longVal\nFROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z\nWHERE m.merchantId = n.merchantId AND m.zip = z.zip\nLINK SOURCE EVENT;\n\n\n-- CQ GenerateAlerts\n--\n-- This CQ sends an alert when a merchant's status value changes to\n-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.\n\n\nCREATE STREAM AlertStream OF Global.AlertEvent;\n\nCREATE CQ GenerateAlerts\nINSERT INTO AlertStream\nSELECT n.CompanyName,\n       m.MerchantId,\n       CASE\n         WHEN m.Status = 'OK' THEN 'info'\n         ELSE 'warning' END,\n       CASE\n         WHEN m.Status = 'OK' THEN 'cancel'\n         ELSE 'raise' END,\n       CASE\n         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)\n         ELSE ''\n         END\nFROM MerchantTxRateWithStatusStream m, NameLookup n\nWHERE m.merchantId = n.merchantId;\n\n\n\n--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM AlertStream;\n\nCREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;\n\n\nEND APPLICATION PosApp;\n\n\nCREATE DASHBOARD USING \"Samples/Customer/PosApp/PosAppDashboard.json\";", "generated_queries": "1. \"Can you provide a report on the current transaction activity by merchant, including the total transaction count and amount, as well as any alerts triggered when a merchant's transaction count is higher or lower than the average for the time of day?\"\n  \n2. \"How does the PosApp application calculate the upper and lower transaction count limits for merchants, and what criteria determines the status of a merchant's transaction rate as HOT, WARM, COOL, or COLD?\"\n\n3. \"Can you explain how the PosApp application utilizes historical hourly averages from the HourlyAveLookup cache to monitor and generate alerts based on a merchant's transaction count exceeding predefined thresholds?\"", "file_name": "PropertyVariable.tql"}
{"tql": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.OJet (\n  PrimaryDatabasePassword: '@PRIMARY_PASSWORD@',\n  ConnectionURL: '@DOWNSTREAM_URL@',\n  PrimaryDatabaseUsername: '@PRIMARY_USER@',\n  Password: '@DOWNSTREAM_PASSWORD@',\n  DownstreamCaptureMode: 'REAL_TIME',\n  DownstreamCapture: true,\n  PrimaryDatabaseConnectionURL: '@PRIMARY_URL@',\n  Tables: '@SOURCE_TABLES@',\n  Username: '@DOWNSTREAM_USER@'\n  )\nOUTPUT TO @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_SYS@ USING Global.SysOut (\n  name: 'Out' )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter (\n  ConnectionURL: '@TARGET_URL@',\n  Username: '@TARGET_USER@',\n  Password: '@TARGET_PASSWORD@',\n  CheckPointTable: 'CHKPOINT',\n  CommitPolicy: 'EventCount:1',\n  Tables: '@TARGET_TABLES@',\n  BatchPolicy: 'EventCount:1' )\nINPUT FROM @STREAM@;\n\nEND APPLICATION @APPNAME@;", "generated_queries": "1. How can I stop, undeploy, and drop a specific application in my stream processing environment?\n2. What is the process for creating an application that includes a custom source, targets, and specific configurations in a stream processing framework?\n3. Can you provide a step-by-step guide on deploying and managing data flow within an application that involves capturing real-time data from a primary database and writing it to a target database?", "file_name": "PRPAllDataType_DW19C.tql"}
{"tql": "--\n-- Recovery Test 10 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> CW(p) -> CQ -> WS\n--\n\nSTOP Recov10Tester.RecovTest10;\nUNDEPLOY APPLICATION Recov10Tester.RecovTest10;\nDROP APPLICATION Recov10Tester.RecovTest10 CASCADE;\nCREATE APPLICATION RecovTest10 RECOVERY 1 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTest10Data.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE CsvData (\n  partKey String KEY,\n  serialNumber int\n);\n\nCREATE STREAM DataStream OF CsvData PARTITION BY partKey;\n\nCREATE CQ CsvToData\nINSERT INTO DataStream\nSELECT\n    data[0],\n    TO_INT(data[1])\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStreamTwoItems\nOVER DataStream KEEP 2 ROWS\nPARTITION BY partKey;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction\nINSERT INTO Wactions\nSELECT\n    first(partKey),\n    to_int(first(serialNumber))\nFROM DataStreamTwoItems\nGROUP BY partKey;\n\nEND APPLICATION RecovTest10;", "generated_queries": "1. \"What is the average of the serial numbers for each unique part key from the CsvSource data in the Recovery Test 10 application?\"\n2. \"How many distinct part keys are there in the DataStreamTwoItems jumping window within the RecovTest10 application?\"\n3. \"Can you provide a list of the latest two serial numbers for each unique part key from the CsvSource data processed by the DataToWaction continuous query in the Recovery Test 10 application?\"", "file_name": "RecovTest10.tql"}
{"tql": "--\n-- Crash Recovery Test 1 on two node cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> WS\n--\n\nSTOP APPLICATION N2S2CR1Tester.N2S2CRTest1;\nUNDEPLOY APPLICATION N2S2CR1Tester.N2S2CRTest1;\nDROP APPLICATION N2S2CR1Tester.N2S2CRTest1 CASCADE;\nCREATE APPLICATION N2S2CRTest1 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN2S2CRTest1;\n\nCREATE SOURCE CsvSourceN2S2CRTest1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN2S2CRTest1;\n\nCREATE FLOW DataProcessingN2S2CRTest1;\n\nCREATE TYPE WactionTypeN2S2CRTest1 (\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE WactionsN2S2CRTest1 CONTEXT OF WactionTypeN2S2CRTest1\nEVENT TYPES ( WactionTypeN2S2CRTest1 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsN2S2CRTest1\nINSERT INTO WactionsN2S2CRTest1\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nEND FLOW DataProcessingN2S2CRTest1;\n\nEND APPLICATION N2S2CRTest1;", "generated_queries": "1. How can I stop, undeploy, and drop a specific application named N2S2CRTest1 on a two-node cluster for crash recovery testing?\n2. What is the process for creating a flow named DataAcquisitionN2S2CRTest1 that involves reading CSV data from a specified directory and storing it in a stream called CsvStream?\n3. How can I define a custom WactionTypeN2S2CRTest1 with specific fields like merchantId, dateTime, amount, and city, and then insert data into a WactionStore named WactionsN2S2CRTest1 using a continuous query called InsertWactionsN2S2CRTest1?", "file_name": "N2S2CRTest1.tql"}
{"tql": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@ RECOVERY 1 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE @SourceName@ USING PostgreSQLReader  ( \n ReplicationSlotName: 'striim_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.postgrestopostgres_src'\n ) \nOUTPUT TO @SRCINPUTSTREAM@ ;\n\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy:'EventCount:1000,Interval:60',\nCommitPolicy:'EventCount:1000,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM @SRCINPUTSTREAM@;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;", "generated_queries": "1. How can I create an application in Striim that reads data from a PostgreSQL database and writes it to another PostgreSQL database using specific source and target configurations?\n\n2. What are the steps involved in stopping, undeploying, dropping, creating, configuring, and deploying an application named \"@APPNAME@\" in Striim that replicates data from a source PostgreSQL table to a target PostgreSQL table with specific connection details and batch/commit policies?\n\n3. Can you provide the TQL syntax for setting up continuous data replication from a PostgreSQL source table named \"public.postgrestopostgres_src\" to a PostgreSQL target table named \"public.postgrestopostgres_tgt\" in a Striim application named \"@APPNAME@\", with a recovery interval of 1 second and specific connection settings for both source and target configurations?", "file_name": "DynamicSwitch.tql"}
{"tql": "--\n-- Recovery Test 20 with two sources going to one wactionstore\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> CQ1 -> WS\n-- S2 -> CQ2 -> WS\n--\n\nSTOP KStreamRecov20Tester.KStreamRecovTest20;\nUNDEPLOY APPLICATION KStreamRecov20Tester.KStreamRecovTest20;\nDROP APPLICATION KStreamRecov20Tester.KStreamRecovTest20 CASCADE;\nDROP USER KStreamRecov20Tester;\nDROP NAMESPACE KStreamRecov20Tester CASCADE;\nCREATE USER KStreamRecov20Tester IDENTIFIED BY KStreamRecov20Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov20Tester;\nCONNECT KStreamRecov20Tester KStreamRecov20Tester;\n\nCREATE APPLICATION KStreamRecovTest20 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE WactionType (\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions1\nINSERT INTO Wactions\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream1;\n\nCREATE CQ InsertWactions2\nINSERT INTO Wactions\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream2;\n\nEND APPLICATION KStreamRecovTest20;", "generated_queries": "1. How can I set up a test scenario with two different CSV data sources feeding into a single Wactionstore in an event processing application?\n2. What are the steps required to create a recovery test scenario with a 5-second interval for processing data from two Kafka streams using a specified set of properties and configurations?\n3. How can I define the schema for the data being processed in the event processing application, including key fields, datetime formats, numeric values, and string fields for a specified data flow setup?", "file_name": "KStreamRecovTest20.tql"}
{"tql": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE OR REPLACE SOURCE DBSource USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//192.168.56.101:1521/orcl',\n  Tables: 'QATEST.oracle_200',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\n\ncreate stream xferredDataStream1 of Global.WAEvent;\n\nCREATE CQ CQ1\ninsert into xferredDataStream1\nselect  putuserdata (data1,'ID', data[0]) from Oracle_ChangeDataStream data1;\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:4,Interval:60',\n  CommitPolicy: 'EventCount:4,Interval:60',\n  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',\n  IgnorableExceptionCode:'PRIMARY KEY',\n  Tables: 'QATEST.oracle_200,test.cassandra_200 columnmap(field1=field1,field2=field2,field3=field3,field4=field4,field5=field5,field6=field6,field7=field7,field8=field8,field9=field9,field10=field10,field11=field11,field12=field12,field13=field13,field14=field14,field15=field15,field16=field16,field17=field17,field18=field18,field19=field19,field20=field20,field21=field21,field22=field22,field23=field23,field24=field24,field25=field25,field26=field26,field27=field27,field28=field28,field29=field29,field30=field30,field31=field31,field32=field32,field33=field33,field34=field34,field35=field35,field36=field36,field37=field37,field38=field38,field39=field39,field40=field40,field41=field41,field42=field42,field43=field43,field44=field44,field45=field45,field46=field46,field47=field47,field48=field48,field49=field49,field50=field50,field51=field51,field52=field52,field53=field53,field54=field54,field55=field55,field56=field56,field57=field57,field58=field58,field59=field59,field60=field60,field61=field61,field62=field62,field63=field63,field64=field64,field65=field65,field66=field66,field67=field67,field68=field68,field69=field69,field70=field70,field71=field71,field72=field72,field73=field73,field74=field74,field75=field75,field76=field76,field77=field77,field78=field78,field79=field79,field80=field80,field81=field81,field82=field82,field83=field83,field84=field84,field85=field85,field86=field86,field87=field87,field88=field88,field89=field89,field90=field90,field91=field91,field92=field92,field93=field93,field94=field94,field95=field95,field96=field96,field97=field97,field98=field98,field99=field99,field100=field100,field101=field101,field102=field102,field103=field103,field104=field104,field105=field105,field106=field106,field107=field107,field108=field108,field109=field109,field110=field110,field111=field111,field112=field112,field113=field113,field114=field114,field115=field115,field116=field116,field117=field117,field118=field118,field119=field119,field120=field120,field121=field121,field122=field122,field123=field123,field124=field124,field125=field125,field126=field126,field127=field127,field128=field128,field129=field129,field130=field130,field131=field131,field132=field132,field133=field133,field134=field134,field135=field135,field136=field136,field137=field137,field138=field138,field139=field139,field140=field140,field141=field141,field142=field142,field143=field143,field144=field144,field145=field145,field146=field146,field147=field147,field148=field148,field149=field149,field150=field150,field151=@METADATA(OperationName),field152=field1,field153=@METADATA(SQLRedoLength),field154=@METADATA(SEQUENCE),field155=@METADATA(SegmentName),field156=@METADATA(OperationType),field157=@METADATA(TxnUserID),field158=@METADATA(ThreadID),field159=@METADATA(TxnUserID),field160=@USERDATA(ID),field161=@USERDATA(ID),field162=@USERDATA(ID),field163=@USERDATA(ID),field164=field3,field165=field150,field166=field151)',\n  Password: 'cassandra',\n  Password_encrypted: false\n )\nINPUT FROM xferredDataStream1;\n\ncreate Target t2 using SysOut(name:Foo2) input from xferredDataStream1;\n\nEND APPLICATION DBRTOCW;\n\ndeploy application DBRTOCW in  default;\n\nstart application DBRTOCW;", "generated_queries": "1. How can I deploy and start an application named \"DBRTOCW\" in a default environment with specific configurations for data transfer between Oracle and Cassandra databases?\n2. What are the source and target configurations set up for the data replication application \"DBRTOCW\" to transfer data from an Oracle table named \"QATEST.oracle_200\" to a Cassandra table named \"test.cassandra_200\"?\n3. What is the interval set for recovery within the application \"DBRTOCW\" and what are the connection details for Oracle and Cassandra databases used in this application for data synchronization?", "file_name": "oracToCassandra_200map.tql"}
{"tql": "stop application ps1;\nstop application ps2;\nundeploy application ps1;\nundeploy application ps2;\ndrop application ps1 cascade;\ndrop application ps2 cascade;\nCREATE application ps1 recovery 5 second interval;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',\nacks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\ncreate type type1(\n  companyName String,\n  merchantId String,\n  city string\n);\n\nCREATE STREAM kps1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM kps2 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM kps3 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM kps4 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM kps5_typedStream OF type1 partition by city persist using KafkaPropset;\n\ncreate source s using FileReader (\n        directory:'/Users/saranyad/Product/IntegrationTests/TestData',\n        wildcard:'posdata.csv',\n        positionByEOF:false,\n        Charset:'UTF-8'\n)\nPARSE USING DSVParser (\n  columndelimiter:',',\n  ignoreemptycolumn:'Yes',\n  quoteset:'[]~\"',\n  separator:'~'\n)\nOUTPUT TO kps;\n\nCREATE CQ cq1\nINSERT INTO kps1\nSELECT *\nFROM kps;\n\nCREATE CQ cq2\nINSERT INTO kps2\nSELECT *\nFROM kps;\n\nCREATE CQ cq3\nINSERT INTO kps3\nSELECT *\nFROM kps;\n\nCREATE CQ cq4\nINSERT INTO kps4\nSELECT *\nFROM kps;\n\n\nCREATE CQ cq5\nINSERT INTO kps5_typedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\") as companyName,\nTO_STRING(data[1]) as merchantId,\nTO_STRING(data[10]) as city\nFROM kps;\n\nend application ps1;\n\nCREATE application ps2 recovery 5 second interval;\n\ncreate type type2(\n  companyName String,\n  merchantId String,\n  city string\n);\n\nCREATE STREAM kps1_typedStream OF type2 partition by city;\nCREATE STREAM kps2_typedStream OF type2 partition by city;\nCREATE STREAM kps3_typedStream OF type2 partition by city;\nCREATE STREAM kps4_typedStream OF type2 partition by city;\n\nCREATE CQ cq6\nINSERT INTO kps1_typedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\") as companyName,\nTO_STRING(data[1]) as merchantId,\nTO_STRING(data[10]) as city\nFROM kps1;\n\nCREATE CQ cq7\nINSERT INTO kps2_typedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\") as companyName,\nTO_STRING(data[1]) as merchantId,\nTO_STRING(data[10]) as city\nFROM kps2;\n\nCREATE CQ cq8\nINSERT INTO kps3_typedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\") as companyName,\nTO_STRING(data[1]) as merchantId,\nTO_STRING(data[10]) as city\nFROM kps3;\n\nCREATE CQ cq9\nINSERT INTO kps4_typedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\") as companyName,\nTO_STRING(data[1]) as merchantId,\nTO_STRING(data[10]) as city\nFROM kps4;\n\nCREATE TARGET target1 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'QATEST.KPS1'\n) INPUT FROM kps1_typedStream;\n\nCREATE TARGET target2 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'QATEST.KPS2'\n) INPUT FROM kps2_typedStream;\n\nCREATE TARGET target3 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'QATEST.KPS3'\n) INPUT FROM kps3_typedStream;\n\nCREATE TARGET target4 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'QATEST.KPS4'\n) INPUT FROM kps4_typedStream;\n\nCREATE TARGET target5 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'QATEST.KPS5'\n) INPUT FROM kps5_typedStream;\n\nend application ps2;\n--DEPLOY APPLICATION KafkaWriterApp with sourceFlow  ON ANY IN DEFAULT, targetFlow ON ALL IN DEFAULT;", "generated_queries": "1. How can I set up a data streaming pipeline to read data from CSV files, process it, and persist it using Kafka for real-time analytics?\n2. What is the configuration for creating and persisting different types of streams with custom properties through Kafka in an event processing application?\n3. How can I direct data from multiple streams with different schemas to specific tables in an Oracle database for further analysis and reporting?", "file_name": "AgentPSDBWriter_V11.tql"}
{"tql": "--\n-- Recovery Test 34 with two sources, two sliding time-count windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Sc5a9W/p  -> CQ1 -> WS\n-- S2 -> Sc6a11W/p -> CQ2 -> WS\n--\n\nSTOP KStreamRecov3Tester.KStreamRecovTest34;\nUNDEPLOY APPLICATION KStreamRecov3Tester.KStreamRecovTest34;\nDROP APPLICATION KStreamRecov34Tester.KStreamRecovTest34 CASCADE;\n\nDROP USER KStreamRecov34Tester;\nDROP NAMESPACE KStreamRecov34Tester CASCADE;\nCREATE USER KStreamRecov34Tester IDENTIFIED BY KStreamRecov34Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov34Tester;\nCONNECT KStreamRecov34Tester KStreamRecov34Tester;\n\nCREATE APPLICATION KStreamRecovTest34 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE\nPARTITION BY merchantId;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1\nGROUP BY merchantId;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2\nGROUP BY merchantId;\n\nEND APPLICATION KStreamRecovTest34;", "generated_queries": "1. How can I create a recovery test with two sources, two sliding time-count windows, and one wactionstore in TQL?\n\n2. What are the steps to deploy and configure a TQL application for processing CSV data streams with specific time window partitions and data transformations?\n\n3. How do I define and partition streaming data from CSV sources into 5-minute and 6-minute time-count windows based on a specific key using TQL?", "file_name": "KStreamRecovTest34.tql"}
{"tql": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSV1Source using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'RFC4180.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'no'\n)\nOUTPUT TO Csv1Stream;\n\ncreate Target t using FileWriter(\n  filename:'FileWriterStandardRFC4180',\n  directory:'@FEATURE-DIR@/logs/',\n  standard : 'RFC4180',\n  rolloverpolicy:'EventCount:10000,Interval:30s'\n)\nformat using DSVFormatter (\nmembers:'data'\n)\ninput from Csv1Stream;\n\nend application DSV;", "generated_queries": "1. What steps are involved in stopping and undeploying the RollOverTester.DSV application, as well as creating a new application named DSV with specific source and target configurations in a TQL script?\n   \n2. How can I set up a TQL script to create an application named DSV that reads data from a CSV file, processes it using a custom DSVParser, and writes the output to a FileWriter target with a specified rollover policy?\n   \n3. Can you show me a TQL script snippet that demonstrates how to define multiple components including a source, parser, target, and formatter within a single application configuration for data processing tasks?", "file_name": "FileWriterStandardRFC4180.tql"}
{"tql": "STOP APPLICATION @APPNAME@app1;\nSTOP APPLICATION @APPNAME@app2;\nSTOP APPLICATION @APPNAME@app3;\nSTOP APPLICATION @APPNAME@app4;\nSTOP APPLICATION @APPNAME@app5;\nUNDEPLOY APPLICATION @APPNAME@app1;\nUNDEPLOY APPLICATION @APPNAME@app2;\nUNDEPLOY APPLICATION @APPNAME@app3;\nUNDEPLOY APPLICATION @APPNAME@app4;\nUNDEPLOY APPLICATION @APPNAME@app5;\nDROP APPLICATION @APPNAME@app1 CASCADE;\nDROP APPLICATION @APPNAME@app2 CASCADE;\nDROP APPLICATION @APPNAME@app3 CASCADE;\nDROP APPLICATION @APPNAME@app4 CASCADE;\nDROP APPLICATION @APPNAME@app5 CASCADE;\n\nCREATE APPLICATION @APPNAME@app1 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;\n\ncreate flow @APPNAME@agentflowps;\nCREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',\nacks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\n\ncreate type @APPNAME@type1(\n  id String,\n  name String,\n  city string\n);\n\nCREATE STREAM @APPNAME@sourcestream OF Global.waevent persist using @APPNAME@KafkaPropset;\nCREATE STREAM @APPNAME@kps_typedStream OF @APPNAME@type1 partition by city persist using @APPNAME@KafkaPropset;\n\n\nCREATE OR REPLACE SOURCE @APPNAME@s USING oracleReader  ( \n  Username:'qatest',\n  Password:'qatest',\n  ConnectionURL:'localhost:1521/xe',\n  Tables:'QATEST.test01',\n  FetchSize:1\n ) \nOUTPUT TO @APPNAME@rawstream;\n\ncreate cq @APPNAME@cq1\nINSERT INTO @APPNAME@sourcestream\nSELECT * from @APPNAME@rawstream;\n\nCREATE CQ @APPNAME@cq2\nINSERT INTO @APPNAME@kps_typedStream\nSELECT TO_STRING(data[0]),\nTO_STRING(data[1]),\nTO_STRING(data[2])FROM @APPNAME@rawstream;\nend flow @APPNAME@agentflowps;\nend application @APPNAME@app1;\n--deploy application app1;\n--deploy application app1 with agentflowps on AGENTS;\n@DEPLOY@;\n\nCREATE APPLICATION @APPNAME@app2 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;\n\nCREATE TARGET @APPNAME@app2_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test01,QATEST.KPS1'\n) INPUT FROM @APPNAME@sourcestream;\n\n\nend application @APPNAME@app2;\ndeploy application @APPNAME@app2;\n\n\nCREATE APPLICATION @APPNAME@app3 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;\n\nCREATE TARGET @APPNAME@app3_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test01,QATEST.KPS2'\n) INPUT FROM @APPNAME@sourcestream;\n\nend application @APPNAME@app3;\n--deploy application app3;\n\n\nCREATE APPLICATION @APPNAME@app4 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;\n\nCREATE TARGET @APPNAME@app4_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test01,QATEST.KPS3'\n) INPUT FROM @APPNAME@sourcestream;\n\nend application @APPNAME@app4;\n--deploy application app4;\n\n\nCREATE APPLICATION @APPNAME@app5 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;\n\nCREATE TARGET @APPNAME@app5_target1 USING KafkaWriter VERSION '0.11.0'(\nbrokeraddress:'localhost:9092',\ntopic:'snappy1',\nKafkaConfig:'compression.type=snappy'\n) \nFORMAT USING DSVFormatter ()\nINPUT FROM @APPNAME@kps_typedStream;\n\nCREATE TARGET @APPNAME@app5_target2 USING KafkaWriter VERSION '0.11.0'(\nbrokeraddress:'localhost:9092',\ntopic:'gzip1',\nKafkaConfig:'compression.type=gzip'\n) \nFORMAT USING DSVFormatter ()\nINPUT FROM @APPNAME@sourcestream;\n\nCREATE TARGET @APPNAME@app5_target3 USING KafkaWriter VERSION '0.11.0'(\nbrokeraddress:'localhost:9092',\ntopic:'lz41',\nKafkaConfig:'compression.type=lz4'\n) \nFORMAT USING DSVFormatter ()\nINPUT FROM @APPNAME@sourcestream;\n\nend application @APPNAME@app5;\n--deploy application app5;", "generated_queries": "1. What are the specific actions and configurations needed to deploy multiple applications, each with different recovery intervals and target configurations to process and forward data streams to various endpoints like databases and Kafka topics?\n\n2. How can I set up a TQL query to create and configure different streams, property sets, sources, and continuous queries to process and transform incoming data from an Oracle source before forwarding it to various Kafka topics and database tables within a Talend platform?\n\n3. Can you provide a detailed TQL script for deploying multiple applications, each with unique recovery settings, input streams, targets, and data transformation operations such as inserting data into specific database tables and sending formatted data to different Kafka topics using various compression types?", "file_name": "QuiescePS.tql"}
{"tql": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCreate Source @SourceName@\n Using OracleReader\n(\nUsername:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@SourceConnectURL@',\n Tables:'@SourceTable@',\n _h_useClassic: false,\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@SourceConnectURL@',\nUsername:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;", "generated_queries": "1. How can I deploy and start an application named \"@APPNAME@\" that reads data from an Oracle database using OracleReader and writes to different database tables using DatabaseWriter, and also outputs data to the system out target \"Foo2\"?\n\n2. What are the steps involved in creating an application with the name \"@APPNAME@\", establishing a connection to an Oracle database to read data from a specific table, and then writing this data to specific database tables while also sending it to the system out target \"Foo2\"?\n\n3. Can you provide a detailed guide on the process of undeploying, dropping, and then recreating an application named \"@APPNAME@\" that involves reading data from an Oracle database and writing to database tables using DatabaseWriter, with an additional output to the system out target \"Foo2\"?", "file_name": "OracleALM.tql"}
{"tql": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE OR REPLACE APPLICATION @AppName@;\nCREATE SOURCE @srcName@ USING Global.OracleReader ( \n  Username: '@srcusername@',\n  Password: '@srcpassword@',\n  ConnectionURL: '@srcurl@',\n  Tables: '@srcschema@.@srctable@') \nOUTPUT TO @instreamname@;\n\nCREATE OR REPLACE TARGET @tgtName@ USING Global.ServiceNowWriter ( \n  MaxConnections: 20, \n  ClientSecret: '@clientsecret@', \n  ApplicationErrorCountThreshold: 0, \n  BatchPolicy: 'eventCount:10000, Interval:60', \n  ConnectionUrl: '@tgturl@', \n  Password: '@tgtpassword@', \n  BatchAPI: false,  \n  ConnectionTimeOut: 60, \n  Mode: 'APPENDONLY', \n  ClientID: '@clientid@', \n  Tables: '@srcschema@.@srctable@,@tgttable@ COLUMNMAP()', \n  ConnectionRetries: 3, \n  useConnectionProfile: false, \n  UserName: '@tgtusername@', \n  adapterName: 'ServiceNowWriter' ) \nINPUT FROM @instreamname@;\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;", "generated_queries": "1. How can I deploy and start a new application called \"AppName\" that reads data from an Oracle source named \"srcName\" and writes it to a ServiceNow target named \"tgtName\" using StreamSets?\n   \n2. What are the steps involved in creating a new application in StreamSets that involves extracting data from a specific schema and table in an Oracle database, and loading it into a ServiceNow instance with specific configurations such as batch size and connection settings?\n\n3. Can you provide a detailed TQL query example for setting up a data pipeline in StreamSets that includes creating sources and targets for Oracle and ServiceNow, respectively, and defining the required connection details and processing logic for each component in the pipeline?", "file_name": "oracletoservicenow.tql"}
{"tql": "STOP APPLICATION HW ;\nundeploy application HW ;\ndrop application HW cascade;\n\nCREATE APPLICATION HW Recovery 5 second interval;\n\nCREATE  SOURCE S USING OracleReader  ( \n  Username: 'miner',\n  Password: '@miner',\n  ConnectionURL: '@conn-url@',\n  Tables: '@src@',\n  FetchSize: 1) \nOUTPUT TO hivestream;\n\nCreate Target T using ClouderaHiveWriter (\n  ConnectionURL:'@hive-url@',\n  Username:'@uname@', \n            Password:'@pwd@',\n            hadoopurl:'hdfs://dockerhost:9000/',\n\t        Mode:'incremental',\n\t        mergepolicy: 'eventcount:5,interval:1s',\n            Tables:'@tgt-table@',\n            hadoopConfigurationPath:'/Users/saranyad/Documents/hello/'\n )\nINPUT FROM hivestream;\n\n\nEND APPLICATION HW;\ndeploy application HW on all in default;\n\nStart application HW;", "generated_queries": "1. What is the recovery interval set for the 'HW' application?\n2. What source and target configurations are defined for the 'HW' application?\n3. Can you provide details of the merge policy configured for the 'T' target in the 'HW' application?", "file_name": "ClouderaHiveColmap.tql"}
{"tql": "DROP APPLICATION ns1.OPExample cascade;\nDROP NAMESPACE ns1 cascade;\nCREATE OR REPLACE NAMESPACE ns1;\nUSE ns1;\nCREATE APPLICATION OPExample;\n\nCREATE source CsvDataSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'PosDataPreview.csv',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n)\nOUTPUT TO CsvStream;\n \nCREATE TYPE MerchantHourlyAve(\n  merchantId String,\n  hourValue integer,\n  hourlyAve integer\n);\n\nCREATE CACHE HourlyAveLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'hourlyData.txt'\n)\nPARSE USING DSVParser (\n  header: Yes,\n  trimquote:false,\n  trimwhitespace:true\n) \nQUERY (keytomap:'merchantId') \nOF MerchantHourlyAve;\n\nCREATE CQ CsvToPosData\nINSERT INTO PosDataStream partition by merchantId\nSELECT TO_STRING(data[1]) as merchantId,\n  TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\n  DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,\n  TO_DOUBLE(data[7]) as amount,\n  TO_INT(data[9]) as zip\nFROM CsvStream;\n \nCREATE CQ cq2\nINSERT INTO SendToOPStream\nSELECT makeList(dateTime) as dateTime,\n  makeList(zip) as zip\nFROM PosDataStream;\n \nCREATE TYPE ReturnFromOPStream_Type ( time DateTime , val Integer );\nCREATE STREAM ReturnFromOPStream OF ReturnFromOPStream_Type;\n\nCREATE TARGET OPExampleTarget \nUSING FileWriter (filename: 'OPExampleOut') \nFORMAT USING JSONFormatter() \nINPUT FROM ReturnFromOPStream;\n \nEND APPLICATION OPExample;", "generated_queries": "1. How can I create and configure a data pipeline in TQL to read CSV data from a specific directory and transform it into a structured format for further processing?\n2. Which TQL commands do I need to use in order to define a cache that maps hourly average data for different merchants based on their IDs, and then use this cache in my data processing pipeline?\n3. How can I set up continuous queries in TQL to stream processed data into an output target, such as writing JSON-formatted data to a file, as part of an overall application configuration?", "file_name": "op1.tql"}
{"tql": "create application access;\n\ncreate source AALAccessSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'access_log',\n  charset:'UTF-8',\n  positionByEOF:false\n) PARSE USING AALParser (\n  columndelimiter:' ',\n  IgnoreEmptyColumn:'Yes'\n) OUTPUT TO AalAccessStream;\n\ncreate Target AALAccessDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/logdata') input from AalAccessStream;\n\nend application access;", "generated_queries": "1. What is the process for creating an application access log from specific log files located in a designated directory with a particular character encoding using a TQL query?\n2. How can I parse log data from files with a column-delimited format and ignore empty columns to generate a stream of access data using a TQL query?\n3. How do I configure a target destination to store log data processed through a specific parser in a custom log file location using a TQL query within an application access setup?", "file_name": "AALAccess.tql"}
{"tql": "STOP APPLICATION testApp;\nUNDEPLOY APPLICATION testApp;\nDROP APPLICATION testApp CASCADE;\nCREATE APPLICATION testApp recovery 5 SECOND Interval;\n\n\n  CREATE OR REPLACE SOURCE testApp_Source Using PostgreSQLReader( \n  \n  ReplicationSlotName:'test_slot',\n  FilterTransactionBoundaries:'true',\n  Username:'waction',\n  Password_encrypted:false,\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  adapterName:'PostgreSQLReader',\n  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',\n  Password:'w@ct10n',\n  Tables:'public.sourceTable',\n  ExcludedTables:'public.chkpoint'\n ) OUTPUT TO PGtoBQ_Stream;\n\n\nCREATE OR REPLACE TARGET testApp_Target USING BigQueryWriter  (\n  ColumnDelimiter: '|', \n  NullMarker: 'NULL', \n  projectId:'striimqa-214712',\n  Encoding: 'UTF-8', \n  BatchPolicy: 'eventCount:5,Interval:120',\n  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', \n  AllowQuotedNewLines: 'false', \n  adapterName: 'BigQueryWriter', \n  optimizedMerge: 'true', \n  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', \n  StandardSQL: 'true', \n  QuoteCharacter: '\\\"', \n  Tables: 'public.sourceTable,BQAllpl.oratobqtgt',\n  Mode: 'MERGE',\n  StandardSQL: 'true',\n  QuoteCharacter: '\\\"'\n  ) INPUT FROM PGtoBQ_Stream;\n\nCREATE OR REPLACE TARGET testApp_SysOut USING Global.SysOut (name: 'wa') INPUT FROM PGtoBQ_Stream;\n\nEND APPLICATION testApp;\nDEPLOY APPLICATION testApp;\nSTART testApp;", "generated_queries": "1. How can I set up real-time data replication from a PostgreSQL source table to a BigQuery target table in a streaming fashion for near-instantaneous data synchronization?\n2. What configuration parameters should I define to ensure a recovery interval of 5 seconds for my streaming data application that replicates data from PostgreSQL to BigQuery?\n3. How can I deploy and start a data integration application named 'testApp' that streams data from a PostgreSQL source table to both a BigQuery target table and a SysOut destination using specific connection settings and configurations?", "file_name": "PGtoBQ.tql"}
{"tql": "STOP application AlterTester.DSV;\nundeploy application AlterTester.DSV;\ndrop application AlterTester.DSV cascade;\n\n\ncreate application DSV;\n\ncreate flow myFlowDSV;\ncreate source CSVSource using FileReader (\ndirectory:'@TEST-DATA-PATH@',\nWildCard:'smallposdata.csv',\npositionByEOF:false,\ncharset:'UTF-8'\n)\nparse using DSVParser (\nheader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\nend flow myFlowDSV;\nend application DSV;", "generated_queries": "1. How can I create an application in TQL to process CSV data from a specific directory and parse it using a custom parser to extract specific fields like merchant ID, date, amount, and zip code?\n2. What steps do I need to follow to deploy and start processing CSV data in real-time using a custom parser in TQL?\n3. Is it possible to undeploy and drop a specific application in TQL in order to stop processing CSV data and free up resources?", "file_name": "DSVAlterApp2.tql"}
{"tql": "Create Source @SOURCE_NAME@ Using OracleReader\n(\n Compression: false,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  --StartSCN: 'null',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n) Output To @STREAM@;", "generated_queries": "1. What configuration settings are being used to create a source connection to an Oracle database using the OracleReader adapter in a particular integration pipeline?\n2. How is the data replication mechanism from an Oracle database to a stream managed in terms of parameters such as compression, transaction handling, and connection properties?\n3. Can you explain the specifics of setting up real-time data capture from a specific table in an Oracle database to a target stream using GoldenGate Technology and the OracleReader adapter?", "file_name": "OracleReader.tql"}
{"tql": "STOP TestAlertsEmail.TestAlertsEmailApp;\nUNDEPLOY APPLICATION TestAlertsEmail.TestAlertsEmailApp;\nDROP APPLICATION TestAlertsEmail.TestAlertsEmailApp CASCADE;\n\nCREATE APPLICATION TestAlertsEmailApp;\n\nCREATE source rawSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:No,\n  wildcard:'alerts_csv.txt',\n  coldelimiter:' ',\n  positionByEOF:false\n) OUTPUT TO rawStream;\n\nCREATE STREAM MyAlertStream OF Global.AlertEvent;\nCREATE CQ GenerateMyAlerts\nINSERT INTO MyAlertStream (name, keyVal, severity, flag, message)\nSELECT \"Testing Alerts\", data[0], data[1], data[2], data[3]\nFROM rawStream s;\nCREATE TARGET output2 USING SysOut(name : alertsrecevied) input FROM MyAlertStream;\n\nCREATE SUBSCRIPTION alertSubscription USING EmailAdapter\n(SMTPUSER:'zalakalerts@gmail.com',\nSMTPPASSWORD:'paloalto',\nsmtpurl:'smtp.gmail.com',\nstarttls_enable:'true',\nsmtp_auth:'true',\nsubject:\"Test Email Alerts With Security Enabled\",\nemailList:\"siddhika@striim.com,s.henry@striim.com,saranya@striim.com,invalidmailid.@striim.com\",\nuserIds:'admin',\nthreadCount:\"5\",\nsenderEmail:\"doga@striim.com\"\n)\nINPUT FROM MyAlertStream;\n\nEND APPLICATION TestAlertsEmailApp;\nDEPLOY APPLICATION TestAlertsEmail.TestAlertsEmailApp;\nSTART TestAlertsEmail.TestAlertsEmailApp;", "generated_queries": "1. What is the process for deploying and starting the TestAlertsEmailApp application that sends test email alerts with security enabled using a CSV file as a data source?\n\n2. Can you provide details on how the TestAlertsEmailApp application is set up to generate alerts, process them, and send email notifications to specific email addresses using an EmailAdapter in Striim?\n\n3. How does the TestAlertsEmailApp application in Striim handle incoming data from a CSV file, transform it into AlertEvent objects, and subsequently trigger email alerts to a list of recipients with specified email configuration settings?", "file_name": "TestAlerts_Enable.tql"}
{"tql": "CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;\nCREATE OR REPLACE SOURCE @AppName@_Source USING FileReader (\ndirectory:'@dataDir@',\nwildcard:'data.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO @AppName@_rawstream;\n\nCREATE OR REPLACE STREAM @BuiltinFunc@_Stream OF Global.WAEVent;\nCREATE OR REPLACE STREAM CombineStream OF Global.WAEVent;\n\nCREATE OR REPLACE CQ cq1\nINSERT INTO @BuiltinFunc@_Stream\nSELECT\n@BuiltinFunc@(s1, 'city',data[5])\nFROM @AppName@_rawstream s1;\n\nCREATE OR REPLACE CQ cq2\nINSERT INTO CombineStream\nSelect *\nFROM @BuiltinFunc@_Stream s4;\n\nCREATE OR REPLACE CQ cq3\nINSERT INTO CombineStream\nselect *\nFROM @AppName@_rawstream s5;\n\nCREATE OR REPLACE TARGET @AppName@_FileTarget USING Global.FileWriter ( \n  flushpolicy: 'EventCount:10000,Interval:30s', \n  adapterName: 'FileWriter', \n  directory: '@logs@',\n  filename: '@BuiltinFunc@_Data', \n  rolloverpolicy: 'EventCount:10000,Interval:30s' ) \nFORMAT USING Global.JSONFormatter  ( \n  handler: 'com.webaction.proc.JSONFormatter', \n  jsonMemberDelimiter: '\\n', \n  EventsAsArrayOfJsonObjects: 'true', \n  formatterName: 'JSONFormatter', \n  jsonobjectdelimiter: '\\n' ) \nINPUT FROM CombineStream;\n\nEnd application @AppName@;\nDeploy application @AppName@; \nStart application @AppName@;", "generated_queries": "1. What is the configuration for the application named @AppName@ and how frequently does it perform recovery operations?\n  \n2. How are the data sources defined in the TQL query and what kind of parsing logic is applied to the raw data streams?\n\n3. Can you explain the processing logic outlined in the TQL query, specifically how the data is transformed and combined across different streams before being written to a file target with specific formatting and flushing policies?", "file_name": "putUserData.tql"}
{"tql": "use PosTester;\nDROP TYPE MerchantTxRate;", "generated_queries": "1. What type of data was stored in the MerchantTxRate type in the PosTester database before it was dropped?\n   \n2. Can you provide information on the structure and fields of the MerchantTxRate type that was previously defined in the PosTester database?\n\n3. What impact does dropping the MerchantTxRate type have on the overall schema and records in the PosTester database, and how does it affect data retrieval and manipulation operations?", "file_name": "dropType.tql"}
{"tql": "CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader\n(\nFetchSize:'@FETCHSIZE@',\nQueueSize:'@QUEUESIZE@',\nUsername:'@USERNAME@',\nPassword:'@PASSWORD@',\nConnectionURL:'@URL@',\n_h_useClassic: @CLASSICMODE@,\nTables:'@TABLES@',\nCommittedTransactions:'@COMMITEDT@'\nCompression:'@COMPRESSION@'\n)\nOUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)\nSELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[0]) != '2';\n\nCREATE TYPE LogType(\nNum_col String key,\nChar_col String,\nVarchar2_col String,\nlong_col String,\nTable String,\nOperation String\n);\n\nCREATE WINDOW CDCWindow\nOVER @STREAM@\nKEEP 1 ROWS;\n\nCREATE WACTIONSTORE CDCWS CONTEXT of LogType\nEVENT TYPES ( LogType )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ ToWactionStore\nINSERT INTO CDCWS\nSELECT * FROM CDCWindow\nLINK SOURCE EVENT;\n\nCREATE TARGET @SOURCE_NAME@_SYS USING SysOut (\n  name: '@SOURCE_NAME@_SYS' )\nINPUT FROM @STREAM@;", "generated_queries": "1. What are the changes (inserts, updates, deletes) made in specific Oracle tables that have been captured and stored in an Elasticsearch database?\n   \n2. Can you provide a log of all operations performed on the specified tables in the Oracle database, excluding any records with Num_col value equal to 2?\n   \n3. Which rows from the Oracle database stream have been processed and saved in the CDCWS window for immediate persistence using Elasticsearch as the storage provider?", "file_name": "OracleReaderSMF_AppNotEqual.tql"}
{"tql": "CREATE APPLICATION applicationApiSaas;\n\nCREATE FLOW SaasSourceFlow;\n\nCREATE SOURCE SaasLog4JSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'SaasMonitorLog4JOutput.xml',\n  positionByEOF:false\n)\nPARSE USING XMLParser(\n  rootnode:'/log4j:event',\n  columnlist:'log4j:event/@timestamp,log4j:event/@level,log4j:event/log4j:message,log4j:event/log4j:locationInfo/@method'\n)\nOUTPUT TO SaasRawXMLStream;\n\nCREATE TYPE SaasLog4JEvent (\n    ts DateTime,\n    level String,\n    message String,\n    method String\n);\nCREATE STREAM SaasLog4JStream OF SaasLog4JEvent;\n\nCREATE CQ SaasGetLog4JData\nINSERT INTO SaasLog4JStream\nSELECT TO_DATE(TO_LONG(data[0])), data[1], data[2], data[3]\nFROM SaasRawXMLStream;\n\nCREATE STREAM SaasErrorStream OF SaasLog4JEvent;\nCREATE STREAM SaasWarningStream OF SaasLog4JEvent;\nCREATE STREAM SaasInfoStream OF SaasLog4JEvent;\n\nCREATE CQ SaasGetErrors\nINSERT INTO SaasErrorStream\nSELECT log4j\nFROM SaasLog4JStream log4j WHERE log4j.level = 'ERROR';\n\nCREATE CQ SaasGetWarnings\nINSERT INTO SaasWarningStream\nSELECT log4j\nFROM SaasLog4JStream log4j WHERE log4j.level = 'WARN';\n\nCREATE CQ SaasGetInfo\nINSERT INTO SaasInfoStream\nSELECT log4j\nFROM SaasLog4JStream log4j WHERE log4j.level = 'INFO';\n\nEND FLOW SaasSourceFlow;\n\n\nCREATE FLOW SaasErrorFlow;\n\nCREATE STREAM SaasErrorAlertStream OF Global.AlertEvent;\n\nCREATE CQ SaasSendErrorAlerts\nINSERT INTO SaasErrorAlertStream\nSELECT 'ErrorAlert', ''+ts, 'error', 'raise', 'Error in log ' + message\nFROM SaasErrorStream;\n\nCREATE SUBSCRIPTION SaasErrorAlertSub USING WebAlertAdapter( ) INPUT FROM SaasErrorAlertStream;\n\nEND FLOW SaasErrorFlow;\n\n\nCREATE FLOW SaasWarningFlow;\n\nCREATE JUMPING WINDOW SaasWarningWindow\nOVER SaasWarningStream KEEP WITHIN 30 SECOND ON ts;\n\nCREATE STREAM SaasWarningAlertStream OF Global.AlertEvent;\n\nCREATE CQ SaasSendWarningAlerts\nINSERT INTO SaasWarningAlertStream\nSELECT 'WarningAlert', ''+ts, 'warning', 'raise',\n        COUNT(ts) + ' Warnings in log for api ' + method\nFROM SaasWarningWindow\nGROUP BY method\nHAVING count(ts) > 25;\nCREATE SUBSCRIPTION SaasWarningAlertSub USING WebAlertAdapter( ) INPUT FROM SaasWarningAlertStream;\n\nEND FLOW SaasWarningFlow;\n\n\nCREATE FLOW SaasInfoFlow;\n\nCREATE TYPE SaasApiCall (\n  userId String,\n  apiCall String,\n  sobject String,\n  ts DateTime,\n  userName String,\n  company String,\n  userZip String,\n  companyZip String\n);\nCREATE STREAM SaasApiStream OF SaasApiCall;\n\nCREATE CQ SaasGetApiDetails\nINSERT INTO SaasApiStream(userId,apiCall,sobject,ts)\nSELECT MATCH(i.message, '\\\\\\\\[user=([a-zA-Z0-9]*)\\\\\\\\]'),\n       MATCH(i.message, '\\\\\\\\[api=([a-zA-Z0-9]*)\\\\\\\\]'),\n       MATCH(i.message, '\\\\\\\\[sobject=([a-zA-Z0-9]*)\\\\\\\\]'),\n       i.ts\nFROM SaasInfoStream i;\n\nCREATE STREAM SaasApiEnrichedStream OF SaasApiCall;\n\nCREATE TYPE SaasUserInfo (\n  userId String,\n  userName String,\n  company String,\n  userZip String,\n  companyZip String\n);\nCREATE CACHE SaasUserLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'userLookup.txt',\n  header: Yes,\n  columndelimiter: ','\n) QUERY (keytomap:'userId') OF SaasUserInfo;\n\n\nCREATE CQ SaasGetUserDetails\nINSERT INTO SaasApiEnrichedStream\nSELECT a.userId, a.apiCall, a.sobject, a.ts, u.userName, u.company, u.userZip, u.companyZip\nFROM SaasApiStream a, SaasUserLookup u\nWHERE a.userId = u.userId;\n\nEND FLOW SaasInfoFlow;\n\n\nCREATE FLOW SaasUserApiFlow;\n\nCREATE JUMPING WINDOW SaasUserApiWindow\nOVER SaasApiEnrichedStream KEEP WITHIN 5 SECOND ON ts\nPARTITION BY userId;\n\nCREATE TYPE SaasUserApiUsage (\n  userId String key,\n  userName String,\n  userZip String,\n  userLat double,\n  userLong double,\n  company String,\n  apiCall String,\n  count integer,\n  state String,\n  topObject String,\n  ts DateTime\n);\nCREATE STREAM SaasUserApiUsageStream OF SaasUserApiUsage;\n\nCREATE TYPE SaasUSAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\nCREATE CACHE SaasZipLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'USAddresses.txt',\n  header: Yes,\n  columndelimiter: '\t'\n) QUERY (keytomap:'zip') OF SaasUSAddressData;\n\n\nCREATE CQ SaasGetUserApiUsage\nINSERT INTO SaasUserApiUsageStream\nSELECT a.userId, a.userName, a.userZip, z.latVal, z.longVal, a.company,\n       a.apiCall, COUNT(a.sobject),\n       CASE WHEN COUNT(a.sobject) > 175 THEN 'HOT'\n            ELSE 'COLD' END,\n       MAXOCCURS(a.sobject),\n       FIRST(a.ts)\nFROM SaasUserApiWindow a, SaasZipLookup z\nWHERE a.userZip = z.zip\nGROUP BY a.userId, a.apiCall\nHAVING FIRST(a.ts) IS NOT NULL;\n\nCREATE TYPE SaasUserApiContext (\n  userId String key,\n  userName String,\n  userZip String,\n  userLat double,\n  userLong double,\n  company String,\n  count integer,\n  state String,\n  topObject String,\n  ts DateTime\n);\nCREATE WACTIONSTORE SaasUserApiActivity\nCONTEXT OF SaasUserApiContext\nEVENT TYPES ( SaasUserApiUsage )\n@PERSIST-TYPE@\n\nCREATE JUMPING WINDOW SaasUserWindow\nOVER SaasUserApiUsageStream KEEP WITHIN 5 SECOND ON ts\nPARTITION BY userId;\n\nCREATE CQ SaasTrackUserApiSummary\nINSERT INTO SaasUserApiActivity\nSELECT a.userId, a.userName, a.userZip, a.userLat, a.userLong,\n       a.company, SUM(count),\n       CASE WHEN SUM(count) > 1000 THEN 'HOT'\n            ELSE 'COLD' END,\n       MAXOCCURS(a.topObject),\n       FIRST(a.ts)\nFROM SaasUserWindow a\nGROUP BY a.userId\nLINK SOURCE EVENT;\n\nCREATE STREAM SaasUserApiAlertStream OF Global.AlertEvent;\n\nCREATE CQ SaasSendUserApiAlerts\nINSERT INTO SaasUserApiAlertStream\nSELECT 'UserAPIAlert', ''+ts, 'warning', 'raise',\n       'User ' + userName + ' has used api ' + apiCall + ' ' + count + ' times for ' + topObject\nFROM SaasUserApiUsageStream\nWHERE state = 'HOT' AND count > 300;\n\nCREATE SUBSCRIPTION SaasUserApiAlertSub USING WebAlertAdapter( ) INPUT FROM SaasUserApiAlertStream;\n\nEND FLOW SaasUserApiFlow;\n\n\nCREATE FLOW SaasCompanyApiFlow;\n\nCREATE JUMPING WINDOW SaasCompanyApiWindow\nOVER SaasApiEnrichedStream KEEP WITHIN 5 SECOND ON ts\nPARTITION BY company;\n\nCREATE TYPE SaasCompanyApiUsage (\n  company String key,\n  companyZip String,\n  companyLat double,\n  companyLong double,\n  apiCall String,\n  count integer,\n  state String,\n  topObject String,\n  ts DateTime\n);\nCREATE STREAM SaasCompanyApiUsageStream OF SaasCompanyApiUsage;\n\nCREATE CQ SaasGetCompanyApiUsage\nINSERT INTO SaasCompanyApiUsageStream\nSELECT a.company, a.companyZip, z.latVal, z.longVal,\n       a.apiCall, COUNT(a.sobject),\n       CASE WHEN COUNT(a.sobject) > 3000 THEN 'HOT'\n            ELSE 'COLD' END,\n       MAXOCCURS(a.sobject),\n       FIRST(a.ts)\nFROM SaasCompanyApiWindow a, SaasZipLookup z\nWHERE a.companyZip = z.zip\nGROUP BY a.company, a.apiCall\nHAVING FIRST(a.ts) IS NOT NULL;\n\nCREATE TYPE SaasCompanyApiContext (\n  company String key,\n  companyZip String,\n  companyLat double,\n  companyLong double,\n  count integer,\n  state String,\n  topObject String,\n  ts DateTime\n);\nCREATE JUMPING WINDOW SaasCompanyWindow\nOVER SaasCompanyApiUsageStream KEEP WITHIN 5 SECOND ON ts\nPARTITION BY company;\n\nCREATE WACTIONSTORE SaasCompanyApiActivity\nCONTEXT OF SaasCompanyApiContext\nEVENT TYPES ( SaasCompanyApiUsage )\n@PERSIST-TYPE@\n\n\nCREATE CQ SaasTrackCompanyApiSummary\nINSERT INTO SaasCompanyApiActivity\nSELECT a.company, a.companyZip, a.companyLat, a.companyLong,\n       SUM(a.count),\n       CASE WHEN SUM(a.count) > 15000 THEN 'HOT'\n            ELSE 'COLD' END,\n       MAXOCCURS(a.topObject),\n       FIRST(a.ts)\nFROM SaasCompanyWindow a\nGROUP BY a.company\nLINK SOURCE EVENT;\n\nEND FLOW SaasCompanyApiFlow;\n\n\nCREATE FLOW SaasApiFlow;\n\nCREATE JUMPING WINDOW SaasApiWindow\nOVER SaasApiEnrichedStream KEEP WITHIN 5 SECOND ON ts\nPARTITION BY apiCall;\n\nCREATE TYPE SaasApiUsage (\n  apiCall String key,\n  sobject String,\n  count integer,\n  state String,\n  ts DateTime\n);\nCREATE STREAM SaasApiUsageStream OF SaasApiUsage;\n\nCREATE CQ SaasGetApiUsage\nINSERT INTO SaasApiUsageStream\nSELECT a.apiCall, a.sobject,\n       COUNT(a.userId),\n       CASE WHEN COUNT(a.userId) > 3000 THEN 'HOT'\n            ELSE 'COLD' END,\n       FIRST(a.ts)\nFROM SaasApiWindow a\nGROUP BY a.apiCall, a.sobject\nHAVING FIRST(a.ts) IS NOT NULL;\n\nCREATE TYPE SaasApiContext (\n  apiCall String key,\n  count integer,\n  state String,\n  topObject String,\n  ts DateTime\n);\n\nCREATE JUMPING WINDOW SaasApiAggWindow\nOVER SaasApiUsageStream KEEP WITHIN 5 SECOND ON ts\nPARTITION BY apiCall;\n\nCREATE WACTIONSTORE SaasApiActivity\nCONTEXT OF SaasApiContext\nEVENT TYPES ( SaasApiUsage )\n@PERSIST-TYPE@\n\nCREATE CQ SaasTrackApiSummary\nINSERT INTO SaasApiActivity\nSELECT a.apiCall,\n       SUM(a.count),\n       CASE WHEN SUM(a.count) > 20000 THEN 'HOT'\n            ELSE 'COLD' END,\n       MAXOCCURS(a.sobject),\n       first(a.ts)\nFROM SaasApiAggWindow a\nGROUP BY a.apiCall\nLINK SOURCE EVENT;\n\nEND FLOW SaasApiFlow;\n\nEND APPLICATION applicationApiSaas;", "generated_queries": "1. How many warnings have been logged in the Saas system in the last 30 seconds and which API methods have triggered these warnings?\n2. Which users have triggered error alerts in the Saas system and what specific error messages were generated for each alert?\n3. Is there any API call in the Saas system that has been used more than 3000 times, and if so, what specific objects were involved in these high-usage API calls and which users triggered them?", "file_name": "applicationApiSaas.tql"}
{"tql": "stop application GGTrailReaderApp;\nundeploy application GGTrailReaderApp;\ndrop application GGTrailReaderApp cascade;\n\ncreate application GGTrailReaderApp recovery 5 second interval;\n\ncreate source GGTrailSource using GGTrailReader (\ntRaildIrectory:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario15',\ntRAilfilepattern:'15*',\npositionByEOF:false,\nFilterTransactionBoundaries: true,\nDefinitionFile:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario15/Scn15_beforeddl.def',\ncaptureCDdl: true,\nCDDLAction:'Process',\n--CDDLAction:'ignore',\n--CDDLAction:'quiesce',\n--cddlAction:'Error',\nTrailByTeOrder:'LittleEndian',\nrecoveryInterval: 5,\nTABLES:'QATEST.INT2;QATEST.INT1'\n)\nOUTPUT TO GGTrailStream;\n\ncreate Target t2 using SysOut(name:Foo2) input from GGTrailStream;\n\nCREATE TARGET WriteCDCOracle1 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost/ORCL',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:1,Interval:1',\nCommitPolicy:'Eventcount:1,Interval:1',\nCheckpointtable:'RGRN_CHKPOINT',\nTables:'QATEST.GGDDL5,QATEST.GGDDL5_TGT'\n) INPUT FROM GGTrailStream;\n\n\nend application GGTrailReaderApp;\n\ndeploy application GGTrailReaderApp;\nstart application GGTrailReaderApp;", "generated_queries": "1. What is the configuration and setup process for the GGTrailReaderApp application in the scenario15 directory with specific table mappings and target destinations?\n2. How can we adjust the recovery interval for the GGTrailReaderApp application to 5 seconds and modify the CDC capture options for specific tables?\n3. What are the deployment and startup steps required to activate the GGTrailReaderApp application with defined sources, targets, and recovery policies in this TQL script?", "file_name": "t.tql"}
{"tql": "--\n-- Recovery Test 35 with two sources, two jumping count windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jc5W/p -> CQ1 -> WS\n--   S2 -> Jc6W/p -> CQ2 -> WS\n--\n\nSTOP KStreamRecov35Tester.KStreamRecovTest35;\nUNDEPLOY APPLICATION KStreamRecov35Tester.KStreamRecovTest35;\nDROP APPLICATION KStreamRecov35Tester.KStreamRecovTest35 CASCADE;\n\nDROP USER KStreamRecov35Tester;\nDROP NAMESPACE KStreamRecov35Tester CASCADE;\nCREATE USER KStreamRecov35Tester IDENTIFIED BY KStreamRecov35Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov35Tester;\nCONNECT KStreamRecov35Tester KStreamRecov35Tester;\n\nCREATE APPLICATION KStreamRecovTest35 RECOVERY 5 SECOND INTERVAL\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP 5 ROWS\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP 6 ROWS\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nEND APPLICATION KStreamRecovTest35;", "generated_queries": "1. What is the total number of unique merchants, the first company name, and the first merchant ID for each merchant in the data stream over a 5-minute jumping window partitioned by merchant ID?\n   \n2. How many companies and what is the first company name, date and time recorded, and merchant ID for a specific amount of data records in the data stream over a 6-minute jumping window partitioned by merchant ID?\n   \n3. Can you provide a breakdown of the company names, dates and times, total number of companies, and merchant IDs within the data streams from two separate sources, each divided into different jumping window time intervals, and all aggregated into a single wactionstore partitioned by their respective keys?", "file_name": "KStreamRecovTest35.tql"}
{"tql": "--\n-- Recovery Test 21 with two sources, two sliding count windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Sc5W -> CQ1 -> WS\n-- S2 -> Sc6W -> CQ2 -> WS\n--\n\nSTOP KStreamRecov21Tester.KStreamRecovTest21;\nUNDEPLOY APPLICATION KStreamRecov21Tester.KStreamRecovTest21;\nDROP APPLICATION KStreamRecov21Tester.KStreamRecovTest21 CASCADE;\nDROP USER KStreamRecov21Tester;\nDROP NAMESPACE KStreamRecov21Tester CASCADE;\nCREATE USER KStreamRecov21Tester IDENTIFIED BY KStreamRecov21Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov21Tester;\nCONNECT KStreamRecov21Tester KStreamRecov21Tester;\n\nCREATE APPLICATION KStreamRecovTest21 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP 5 ROWS;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP 6 ROWS;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2;\n\nEND APPLICATION KStreamRecovTest21;", "generated_queries": "1. What are the different sources configured in the recovery test \"KStreamRecovTest21\" with two sliding count windows and one wactionstore, all without partitioning?\n\n2. How many rows are stored in the window \"DataStream5Minutes1\" that is defined over \"DataStream1\" in the recovery test \"KStreamRecovTest21\"?\n\n3. What type of event data is stored in the wactionstore \"Wactions\" that is associated with the context of the CSV data in the recovery test \"KStreamRecovTest21\"?", "file_name": "KStreamRecovTest21.tql"}
{"tql": "Stop IR;\nUndeploy application IR;\ndrop application IR cascade;\nCREATE APPLICATION IR recovery 5 second interval;\nCREATE OR REPLACE SOURCE Teradata_source1 USING IncrementalBatchReader  ( \n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.test01=id',\n  startPosition: '%=0',\n  PollingInterval: '20sec'\n )\nOUTPUT TO data_stream1;\n\nCREATE OR REPLACE SOURCE Teradata_source2 USING IncrementalBatchReader  ( \n  FetchSize: 10000,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.test01=t1',\n  startPosition: '%=0',\n  PollingInterval: '20sec' )\nOUTPUT TO data_stream2;\n\nCREATE OR REPLACE SOURCE Teradata_source3 USING IncrementalBatchReader  ( \n  FetchSize: 10000,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.test01=id',\n  startPosition: '%=1',\n  PollingInterval: '20sec'\n )\nOUTPUT TO data_stream3;\n\nCREATE OR REPLACE SOURCE Teradata_source4 USING IncrementalBatchReader  ( \n  FetchSize: 10000,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.test01=t1',\n  startPosition: '%=1',\n  PollingInterval: '20sec' )\nOUTPUT TO data_stream4;\nCREATE OR REPLACE SOURCE Teradata_source5 USING IncrementalBatchReader  ( \n  FetchSize: 10000,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.test01=id',\n  startPosition: '%=1',\n  PollingInterval: '20sec'\n )\nOUTPUT TO data_stream5;\n\nCREATE OR REPLACE SOURCE Teradata_source6 USING IncrementalBatchReader  ( \n  FetchSize: 10000,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.test01=t1',\n  startPosition: '%=0',\n  PollingInterval: '20sec' )\nOUTPUT TO data_stream5;\n\n\ncreate target AzureSQLDWHTarget1 using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test4 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',\n        uploadpolicy:'eventcount:10000,interval:10s'\n) INPUT FROM data_stream1;\n\n\ncreate target AzureSQLDWHTarget2 using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test5',\n        uploadpolicy:'eventcount:10000,interval:10s'\n) INPUT FROM data_stream2;\n\ncreate target AzureSQLDWHTarget3 using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test5',\n        uploadpolicy:'eventcount:10000,interval:10s'\n) INPUT FROM data_stream3;\n\ncreate target AzureSQLDWHTarget4 using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test5',\n        uploadpolicy:'eventcount:10000,interval:10s'\n) INPUT FROM data_stream4;\n\ncreate target AzureSQLDWHTarget5 using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test5',\n        uploadpolicy:'eventcount:10000,interval:10s'\n) INPUT FROM data_stream5;\n\n\n\ncreate target AzureSQLDWHTarget6 using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test5',\n        uploadpolicy:'eventcount:10000,interval:10s'\n) INPUT FROM data_stream5;\n\n\nEND APPLICATION IR;\ndeploy application IR on all in default;\nstart application IR;", "generated_queries": "1. What are the steps involved in stopping, undeploying, dropping, creating, and deploying an application named \"IR\" that integrates data from Teradata sources to Azure SQL Data Warehouse targets?\n  \n2. How can I configure incremental batch reading from Teradata sources with specific parameters like CheckColumn and PollingInterval, and stream the data to multiple Azure SQL Data Warehouse targets using Striim's TQL?\n\n3. Can you provide a detailed breakdown of how to set up a Striim application named \"IR\" that processes data from various Teradata sources, including the configuration of input streams, intermediate processing, and output to Azure SQL Data Warehouse targets?", "file_name": "MultiSrcANDMultiTarget.tql"}
{"tql": "create flow serverFlow;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  --Password_encrypted: false\n )\nINPUT FROM @STREAM@;\n\nend flow serverFlow;", "generated_queries": "1. How can I set up a data flow in TQL to write streaming data into a specific database table using a DatabaseWriter target?\n2. What is the configuration required to connect to a database and write streaming data from a specific stream into a designated table in TQL?\n3. How do I define a TQL flow named \"serverFlow\" to transfer real-time data into a database using a DatabaseWriter target with specified connection parameters and table mapping?", "file_name": "DatabaseWriterWithAgent.tql"}
{"tql": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING Global.DeltaLakeWriter (\n  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',\n  hostname: 'adb-5292730997167687.7.azuredatabricks.net',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.testaswin',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:1,interval:10s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @STREAM@;", "generated_queries": "1. How can I configure a Delta Lake target using a personal access token for authentication and specifying a merge mode with event-based upload policies?\n2. Can I set up a Delta Lake target in Azure Databricks to connect to a specific JDBC URL for writing data from a streaming source?\n3. What configuration parameters should I use to write data to different tables (\"QATEST\".\"%\" and DEFAULT.testaswin) with a specific upload interval and optimized merge settings in Azure Databricks using TQL?", "file_name": "DeltaLakeWriter.tql"}
{"tql": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\ncreate flow @APPNAME@_agentflow;\n\nCreate Source @SOURCE@\n        Using OracleReader\n(\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: 'qatest.test77',\n  FetchSize:1,\n  QueueSize:2000,\n  CommittedTransactions:true,\n  Compression:false\n)\nOutput To @STREAM@;\n\nend flow @APPNAME@_agentflow;\n\ncreate flow @APPNAME@_serverflow;\n\ncreate Target @TARGET@ using GCSWriter(\n    bucketname:'@bucketname@',\n    objectname:'@objectname@',\n    projectId:'@project-id@',\n    uploadpolicy:'EventCount:7'\n    ServiceAccountKey:'@file-path@'\n)\nformat using DSVFormatter (\nmembers:'data'\n)\ninput from @STREAM@;\n\nend flow @APPNAME@_serverflow;\n\nend application @APPNAME@;", "generated_queries": "1. How can I create an application with a recovery interval of 5 seconds and define flows for processing data from an Oracle source to a Google Cloud Storage target with specific configurations like fetch size, queue size, and upload policy?\n   \n2. What steps are involved in stopping, undeploying, and dropping an application named @APPNAME@ along with its associated flows, sources, and targets in a TQL script that includes details like using an OracleReader for the source and a GCSWriter for the target?\n\n3. Can you provide a TQL query template for creating an application, defining flows for processing data from an Oracle table 'qatest.test77' to a Google Cloud Storage bucket, specifying parameters like recovery interval, fetch size, and data upload policy?", "file_name": "OracleRGCSWriter_Upgrade_Agent.tql"}
{"tql": "--\n-- Recovery Test 8\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> WS\n--\n\nSTOP Recov8Tester.RecovTest8;\nUNDEPLOY APPLICATION Recov8Tester.RecovTest8;\nDROP APPLICATION Recov8Tester.RecovTest8 CASCADE;\nCREATE APPLICATION RecovTest8 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE WactionType (\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions\nINSERT INTO Wactions\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nEND APPLICATION RecovTest8;", "generated_queries": "1. What is the recovery interval set for the \"RecovTest8\" application created by Nicholas Keene at WebAction, Inc.?\n2. Can you retrieve the details of the CSV source configuration used in the \"RecovTest8\" application created by Nicholas Keene at WebAction, Inc.?\n3. How are the data fields mapped to the WactionType in the \"RecovTest8\" application created by Nicholas Keene at WebAction, Inc.?", "file_name": "RecovTest8.tql"}
{"tql": "IMPORT static com.webaction.runtime.converters.DateConverter.*;\n\nDROP APPLICATION SampleCDCReaderApp cascade;\n\nCREATE APPLICATION SampleCDCReaderApp;\ncreate source CDCSource using SampleReader (\n\tAgentPortNo:'2012',\n\tAgentIpAddress:'127.0.0.1',\n\tportno:'2020',\n\tipaddress:'10.10.196.103',\n\tName:'testsession',\n    dataFile:'../conf/data.csv',\n    metaFile:'../conf/metadata.csv',\n\tTables:'POS;STU') output to dbstream,\n\tCheckUnSupportedTypeStream MAP (table:'STU');\n\nCREATE TYPE CDCPosData(\n    BUSINESSNAME String,\n    BUSINESSNAME_HEX String,\n    PRIMARYACCOUNTNUMBER String,\n    POSDATACODE Integer,\n    DATETIME org.joda.time.DateTime,\n    EXPDATE String,\n    CURRENCYCODE String,\n    AUTHAMOUNT Float,\n    TERMINALID String,\n    ZIP String,\n    CITY String,\n    OPR String,\n    TABLENAME String\n);\n\nCREATE STREAM CDCPosDataStream OF CDCPosData;\n\nCREATE JUMPING WINDOW CDCPosDataWindow\nOVER CDCPosDataStream KEEP 9 ROWS\nPARTITION BY OPR;\n\nCREATE CQ CDCCsvToPosData\nINSERT INTO CDCPosDataStream\nSELECT TO_STRING(data[0],\"UTF-8\"),\n\t   TO_HEX(data[0]),\n       data[2],\n       TO_INT(data[3]),\n\t   data[4],\n\t   data[5],\n       data[6],\n       TO_FLOAT(data[7]),\n\t   data[8],\n       data[9],\n\t   data[10],\n\tMETA(x,\"OperationName\").toString(),\n\tMETA(x, \"TableName\").toString()\nFROM dbstream x\nWHERE not(META(x,\"OperationName\").toString() = \"BEGIN\") AND not(META(x,\"OperationName\").toString() = \"COMMIT\") AND not(META(x, \"TableName\").toString() is null) AND META(x, \"TableName\").toString() = \"POS\";\n\nCREATE TYPE CDCSampleOperationData(\n    TableName String,\n    OperationName String,\n    Count Integer\n);\n\nCREATE STREAM CDCSampleOperationDataStream OF CDCSampleOperationData;\n-- PARTITION BY OperationName;\n\n\nCREATE CQ CDCSampleOperationCheck\nINSERT INTO CDCSampleOperationDataStream\nSELECT x.TABLENAME,\nCASE WHEN x.OPR = 'INSERT' THEN x.OPR\n     WHEN x.OPR = 'DELETE' THEN x.OPR\n     WHEN x.OPR = 'UPDATE' THEN x.OPR\n     ELSE 'UNSUPPORTED OPREATION' END,\nCASE WHEN x.OPR = 'INSERT' THEN COUNT(x.OPR)\n     WHEN x.OPR = 'DELETE' THEN COUNT(x.OPR)\n     WHEN x.OPR = 'UPDATE' THEN COUNT(x.OPR)\n     ELSE 0 END\nFROM CDCPosDataWindow x\nGROUP BY OPR;\n\nCREATE TARGET CDCOperationLog USING LogWriter(\n\tname:FILECDCP,\n\tfilename:'@FEATURE-DIR@/logs/SampleCDCReaderOperationCheck.log'\n--\tfilename:'a1.log'\n) INPUT FROM CDCSampleOperationDataStream;\n\nCREATE TARGET CDCLog USING LogWriter(\n  name:SampleCDCReaderApp,\nfilename:'@FEATURE-DIR@/logs/SampleCDCReaderApp.log'\n--  filename:'a.log'\n) INPUT FROM CDCPosDataStream;\n\nCREATE TARGET CDCLog1 USING LogWriter(\n  name:SampleCDCReaderApp,\n  filename:'@FEATURE-DIR@/logs/UnsupportedColumn.log'\n--  filename:'a2.log'\n) INPUT FROM CDCCheckUnSupportedTypeStream;\n\nEND APPLICATION SampleCDCReaderApp;\n\n--deploy application SampleCDCReaderApp.SampleCDCReaderApp in default;", "generated_queries": "1. How can I create an application for reading change data capture (CDC) from a source and outputting it to a database stream with specific configurations?\n2. What types of data are included in the CDCPosData and CDCSampleOperationData schemas for the SampleCDCReaderApp application?\n3. How can I monitor and log operations such as insertions, deletions, and updates on a specific table within the CDCPosDataStream using the CDCSampleOperationCheck continuous query in the SampleCDCReaderApp application?", "file_name": "SampleCDCReaderApp.tql"}
{"tql": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@ Using MSSqlReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n DatabaseName:'@DatabaseName@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'@SourceTable@',\n ConnectionPoolSize:1,\n StartPosition:'EOF'\n ) Output To @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;\n\ncreate Target @targetFile@ using FileWriter(\n  filename:'TestOut.log',\n  directory:'@FileDirectoryPath@',\n  rolloverpolicy:'eventcount:1000'\n)\nformat using DSVFormatter (\n\n)\ninput from @SRCINPUTSTREAM@;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;", "generated_queries": "1. How can I undeploy and drop a specific application with its dependencies in a TQL script before recreating it with a recovery interval of 5 seconds?\n2. Could you show me the TQL commands to create a source for reading data from a MSSQL database, as well as the targets for outputting the data to a syslog and a file in a specific directory with rollover policy in TQL syntax?\n3. In a TQL script, how can I stop, undeploy, drop, recreate, deploy, and start an application named '@APPNAME@', ensuring recovery with a 5-second interval, reading data from a MSSQL database, and outputting the data to a syslog and a file directory with a specific rollover policy?", "file_name": "MultipleTablesTest_mssql.tql"}
{"tql": "stop application SalesForceReaderTest;\nundeploy application SalesForceReaderTest;\ndrop application SalesForceReaderTest cascade;\n\nCREATE APPLICATION SalesForceReaderTest recovery 5 second interval;\n\nCREATE OR REPLACE SOURCE SFPoller USING SalesForceReader \n(\n  sObjects:'newobj__c',\n  --sObject:'Campaign',\n  pollingInterval:'1 min',\n  autoAuthTokenRenewal:'true',\n  Username:'siddhika@webaction.com',\n  Password:'webaction@1234',\n  securityToken:'qhNbKKmafFpanz8Y2oiM89UhR',\n  consumerKey:'3MVG9ZL0ppGP5UrBayz85eLnPg69gWLaE8pA3uzwcFCZ9s.J0mgE7AKvPCEhTaop4uYRbBaDnGXHjnLmngG6P',\n  consumerSecret:'2500119200751301808',\n  apiEndPoint:'https://ap2.salesforce.com',\n  mode:'InitialLoad',\n  startTimestamp:null\n)\nOUTPUT TO DataStream;\n\ncreate target tout using sysout(name : 'out')input from DataStream;\n\n/*\nCREATE TARGET dbtar USING DatabaseWriter( \n\tBatchPolicy:'EventCount:100,Interval:10',\n\tCommitPolicy:'EventCount:100,Interval:10',\n\tConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\n\tUsername:'qatest',\n\tPassword:'qatest',\n  Tables:'Position__c,QATEST.POS columnmap(Id=Id,numnum=numnum__c)'\n  -- Tables: 'newobj__c,QATEST.SFTABLE1 columnmap(Id=Id,CHECKBOOL=checkbool__c,CURR=curr__c,DT=dt__c,TIME1=time1__c,DtTime=DtTime__c,EMAILID=emailid__c,NUM=num__c,PERCNT=percnt__c,PHN=phn__c,TXTLONG=txtlong__c,URL1=url1__c,TXT=txt__c)'\n   ---,loc__latitude=loc__latitude__s,loc__longitude=loc__longitude__s)' \n) INPUT FROM DataStream;\n\n*/\nCREATE OR REPLACE TARGET Target2 using FileWriter\n(\n  filename:'Obj123.json',\n  directory:'/Users/siddhika/Product/',\n  rolloverpolicy:'EventCount:1'\n)\nFORMAT USING JSONFormatter ()\nINPUT FROM DataStream;\n\nEND APPLICATION SalesForceReaderTest;\nDEPLOY APPLICATION SalesForceReaderTest on any in default;\nSTART SalesForceReaderTest;", "generated_queries": "1. What is the frequency at which the SalesForceReaderTest application polls data from the Salesforce instance and what is its polling mode?\n2. What are the authentication credentials and configuration settings used by the SalesForceReaderTest application to connect to the Salesforce API?\n3. Where are the output data streams directed to by the SalesForceReaderTest application, and what are the specific targets set up to receive this data?", "file_name": "SFDB.tql"}
{"tql": "STOP Jumping1Tester.Jumping1;\nUNDEPLOY APPLICATION Jumping1Tester.Jumping1;\nDROP APPLICATION Jumping1Tester.Jumping1 CASCADE;\nCREATE APPLICATION Jumping1;\n\ncreate source CsvSource1 using FileReader\n(\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'WindowsTest.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n)\n parse using DSVParser\n(\n\theader:'yes',\n\tcolumndelimiter:','\n)\nOUTPUT TO CsvStream1;\n\nCREATE TYPE CsvData (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\nCREATE TYPE CsvData1 (\n  zip double\n);\n\nCREATE TYPE WactionData1 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData2 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData3 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData4 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData5 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData6 (\n  zip double\n);\nCREATE TYPE WactionData7 (\n  zip double\n);\nCREATE TYPE WactionData8 (\n  zip double\n);\n\nCREATE STREAM DataStream1 OF CsvData;\n\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY companyName;\n\nCREATE STREAM DataStream3 OF CsvData\nPARTITION BY city;\n\nCREATE STREAM DataStream4 OF CsvData;\nCREATE STREAM DataStream5 OF CsvData\nPARTITION BY city;\n\nCREATE STREAM DataStream6 OF CsvData1;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData3\nINSERT INTO DataStream3\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData4\nINSERT INTO DataStream4\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData5\nINSERT INTO DataStream5\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData6\nINSERT INTO DataStream6\nSELECT\n    TO_DOUBLE(data[8])\nFROM CsvStream1;\n\nCREATE CQ CsvToData7\nINSERT INTO DataStream7\nSELECT\n    TO_DOUBLE(data[8])\nFROM CsvStream1;\n\nCREATE CQ CsvToData8\nINSERT INTO DataStream8\nSELECT\n    TO_DOUBLE(data[8])\nFROM CsvStream1;\n\n-- Count based jumping window\nCREATE JUMPING WINDOW DataStreamCount\nOVER DataStream1 KEEP 5 ROWS;\n\n-- Time based jumping window\nCREATE JUMPING WINDOW DataStreamTime OVER DataStream2 KEEP\nwithin 40 second\nPARTITION BY companyName;\n\n-- Attribute based jumping window\nCREATE JUMPING WINDOW DataStreamAtrribute\nOVER DataStream3 KEEP\nrange 5 minute\nON dateTime\nPARTITION BY city;\n\n-- Count + time based jumping window\nCREATE JUMPING WINDOW DataStreamCountTime\nOVER DataStream4 KEEP\n5 rows\nwithin 8 minute;\n\n-- Attribute + time based jumping window\nCREATE JUMPING WINDOW DataStreamAttributeTime\nOVER DataStream5 KEEP\nrange 300 second\nON dateTime\nwithin 5 minute\nPARTITION BY city;\n\n-- Attribute + time based jumping window using COUNT\nCREATE JUMPING WINDOW DataStreamAttributeTime1\nOVER DataStream5 KEEP\nrange 300 second\nON dateTime\nwithin 5 minute;\n\nCREATE WACTIONSTORE Wactions1 CONTEXT OF WactionData1\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions2 CONTEXT OF WactionData2\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions3 CONTEXT OF WactionData3\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions4 CONTEXT OF WactionData4\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions5 CONTEXT OF WactionData5\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions6 CONTEXT OF WactionData6\nEVENT TYPES ( CsvData1 )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions7 CONTEXT OF WactionData7\nEVENT TYPES ( CsvData1 )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions8 CONTEXT OF WactionData8\nEVENT TYPES ( CsvData1 )\n@PERSIST-TYPE@\n\nCREATE CQ Data1ToWaction\nINSERT INTO Wactions1\nSELECT\n\tp.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamCount p;\n\nCREATE CQ Data2ToWaction\nINSERT INTO Wactions2\nSELECT\n    p.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamTime p\ngroup by companyName;\n\nCREATE CQ Data3ToWaction\nINSERT INTO Wactions3\nSELECT\n    p.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamAtrribute p;\n\nCREATE CQ Data4ToWaction\nINSERT INTO Wactions4\nSELECT\n    p.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamCountTime p;\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions5\nSELECT\n    p.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamAttributeTime p\ngroup by city;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions6\nSELECT\n    count(*)\nFROM DataStreamCount p;\n\nCREATE CQ Data7ToWaction\nINSERT INTO Wactions7\nSELECT\n    count(*)\nFROM DataStreamCountTime p;\n\nCREATE CQ Data8ToWaction\nINSERT INTO Wactions8\nSELECT\n    count(*)\nFROM DataStreamAttributeTime1 p;\n\n\nEND APPLICATION Jumping1;", "generated_queries": "1. How many unique companies have had data processed in the past 5 rows within the DataStreamCount jumping window?\n\n2. What is the total number of companies and the first city listed for each company within a 5-minute range in the DataStreamAtrribute jumping window partitioned by cities?\n\n3. How many rows of data have been processed in the 8-minute time frame within the DataStreamCountTime jumping window?", "file_name": "Jumping.tql"}
{"tql": "--\n-- Recovery Test 11\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS1\n-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS2\n--\n\nSTOP Recov11Tester.RecovTest11;\nUNDEPLOY APPLICATION Recov11Tester.RecovTest11;\nDROP APPLICATION Recov11Tester.RecovTest11 CASCADE;\nCREATE APPLICATION RecovTest11 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE WactionType (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionType;\n\nCREATE CQ CsvToWaction\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE Wactions1 CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions2 CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions1\nINSERT INTO Wactions1\nSELECT\n    *\nFROM DataStream5Minutes;\n\nCREATE CQ InsertWactions2\nINSERT INTO Wactions2\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND APPLICATION RecovTest11;", "generated_queries": "1. How many unique companies have transactions recorded in the WactionType data stream within the last 5 minutes?\n\n2. What is the total transaction amount recorded in Wactions1 and Wactions2 combined within the last 5 minutes?\n\n3. Which city had the highest total transaction amount recorded in WactionType data stream within the last 5 minutes?", "file_name": "RecovTest11.tql"}
{"tql": "stop DataGenSampleApp;\nundeploy application DataGenSampleApp;\ndrop application DataGenSampleApp cascade;\n\n\nCREATE APPLICATION DataGenSampleApp;\n\nCreate Source dataGenSrc using MySQLReader\n(\n Username:'root',\n Password:'w@ct10n',\n ConnectionURL: 'mysql://127.0.0.1:3306/waction',\n Tables:'@tableNames@'\n )\n Output To LCRStream;\n\ncreate Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;\n\nEND APPLICATION DataGenSampleApp;", "generated_queries": "1. What are the steps required to stop, undeploy, and drop the application named DataGenSampleApp, along with its associated components?\n   \n2. How can I create an application named DataGenSampleApp that reads data from a MySQL database located at 127.0.0.1:3306/waction using a specific username and password, and then outputs the data to an LCR stream?\n\n3. What is the configuration needed to create a target component named dataGenTgt that receives input from the LCRStream in the DataGenSampleApp application?", "file_name": "dataGenMysqlTqlTemplate.tql"}
{"tql": "--\n-- Canon Test W40\n-- Nicholas Keene, WebAction, Inc.\n--\n-- Basic test for an unpartitioned jumping attribute window\n--\n-- S -> JWa5u -> CQ -> WS\n--\n\n\nUNDEPLOY APPLICATION NameW40.W40;\nDROP APPLICATION NameW40.W40 CASCADE;\nCREATE APPLICATION W40 RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE FLOW DataAcquisitionW40;\n\n\nCREATE SOURCE CsvSourceW40 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'Canon1000.csv',\n  columndelimiter:',',\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStreamW40;\n\n\nEND FLOW DataAcquisitionW40;\n\n\n\n\nCREATE FLOW DataProcessingW40;\n\nCREATE TYPE DataTypeW40 (\n    eventID String,\n    word String KEY,\n    dateTime DateTime\n);\n\nCREATE STREAM DataStreamW40 OF DataTypeW40;\n\nCREATE CQ CSVStreamW40_to_DataStreamW40\nINSERT INTO DataStreamW40\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[2],'yyyyMMddHHmmss')\nFROM CsvStreamW40;\n\nCREATE JUMPING WINDOW JWa5uW40\nOVER DataStreamW40\nKEEP WITHIN 5 SECOND ON dateTime;\n\nCREATE WACTIONSTORE WactionStoreW40 CONTEXT OF DataTypeW40\nEVENT TYPES ( DataTypeW40 KEY(word) )\n@PERSIST-TYPE@\n\nCREATE CQ JWa5uW40_to_WactionStoreW40\nINSERT INTO WactionStoreW40\nSELECT\n    FIRST(eventID),\n    FIRST(word),\n    FIRST(dateTime)\nFROM JWa5uW40;\n\nEND FLOW DataProcessingW40;\n\n\n\nEND APPLICATION W40;", "generated_queries": "1. What is the data processing workflow for the W40 application, including data acquisition and processing steps?\n2. How is the jumping attribute window defined in the W40 application, and what data is stored in the associated WactionStore?\n3. Can you provide details on how the CSV data from a specific file is transformed and stored in the DataStream in the W40 application?", "file_name": "W40.tql"}
{"tql": "use PosTester;\nDROP STREAM MerchantTxRateOnlyStream;", "generated_queries": "1. Which database is currently active for performing queries in the PosTester environment?\n2. What action is being taken with the stream called MerchantTxRateOnlyStream in the PosTester framework?\n3. Can you provide details on the stream that has been dropped in the PosTester environment named MerchantTxRateOnlyStream?", "file_name": "dropStream.tql"}
{"tql": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE @SourceName@ USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --startLSN:'758199500005967269518578814280000',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.postgrestopostgres_src',\n  ExcludedTables:'public.postgres_2000target'\n ) \nOUTPUT TO @SRCINPUTSTREAM@ ;\n\n\nCREATE OR REPLACE TARGET @targetsys@ USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy: 'EventCount:1,Interval:60',\nCommitPolicy: 'EventCount:1,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM @SRCINPUTSTREAM@;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;", "generated_queries": "1. How can I create an application named \"@APPNAME@\" with a recovery interval of 5 seconds and stream data from a PostgreSQL database table \"public.postgrestopostgres_src\" to a target system named \"ora12_out\"?\n  \n2. What are the connection details and configurations required to set up a data stream from a PostgreSQL database running on localhost to a target system using the DatabaseWriter adapter with specific batch and commit policies?\n  \n3. Can you provide the sequence of commands needed to stop, undeploy, drop, create a new instance, set up data sources and targets, and start an application named \"@APPNAME@\" in a data integration environment?", "file_name": "DataType.tql"}
{"tql": "CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( \n  Username: '@SRC_USERNAME@',\n  Password: '@SRC_PASSWORD@',\n  ConnectionURL: '@CDC_URL@',\n  Tables: '@Source1Tables@',\n  FetchSize: 1) \nOUTPUT TO @APPNAME@cdcStream;\n\nCREATE OR REPLACE EXTERNAL CACHE @APPNAME@1 ( \n  AdapterName:'DatabaseReader',\n  Username: '@SRC_USERNAME@',\n  Password: '@SRC_PASSWORD@',\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  FetchSize: 20,\n  DatabaseProviderType: 'Default',\n  Table: '@Source3Tables@',\n  Columns: 'col1,col2,col3,col4,uniquecol',\n  keytomap: 'uniquecol')  \nOF @APPNAME@cachetype;\n\nCREATE OR REPLACE EXTERNAL CACHE @APPNAME@2 ( \n  AdapterName:'DatabaseReader',\n  Username: '@SRC_USERNAME@',\n  Password: '@SRC_PASSWORD@',\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  FetchSize: 10,\n  DatabaseProviderType: 'Default',\n  Table: '@Source2Tables@',\n  Columns: 'col1,col2,col3,col4,uniquecol',\n  keytomap: 'uniquecol')  \nOF @APPNAME@cachetype;", "generated_queries": "1. How can I set up change data capture (CDC) on my database to stream changes to a specific set of tables with a customized fetch size for each change?\n2. How can I create an external cache in my application that reads data from an Oracle database using a specific adapter and incorporates specific columns from a given table using a unique key mapping?\n3. What steps are required to configure external caches in different tables of my application's database from an Oracle source with varying fetch sizes and a defined set of columns for each cache?", "file_name": "ExternalCache_Alter_Oracle.tql"}
{"tql": "STOP APPLICATION orrs;\nUNDEPLOY APPLICATION orrs;\nDROP APPLICATION orrs CASCADE;\nCREATE APPLICATION orrs;\nCreate Source oraSource Using DatabaseReader\n(\n Username:'src_username',\n Password:'src_password',\n ConnectionURL: 'src_url',\n Tables:'QATEST.ORACLETOREDSHIFTIL1;QATEST.ORACLETOREDSHIFTIL2',\n FilterTransactionBoundaries:true,\n FetchSize:1000\n) Output To LCRStream;\n\nCREATE TARGET RSTarget USING RedshiftWriter\n\t(\n\t  ConnectionURL: 'tgt_url',\n\t  Username: 'tgt_username',\n\t  Password: 'tgt_pwrd',\n\t  bucketname: 'bucket_name',\n\t  --accesskeyId: 'access_key',\n\t  --secretaccesskey: 'secret_access',\n\t  S3IAMRole:'@IAMROLE@',\n\t  Tables: 'QATEST.ORACLETOREDSHIFT%,QATEST.ORACLETOREDSHIFT%',\n\t  uploadpolicy:'eventcount:1000,interval:1m'\n\t) INPUT FROM LCRStream;\n\t\nEND APPLICATION orrs;\nDEPLOY APPLICATION orrs;\nSTART APPLICATION orrs;", "generated_queries": "1. How can I configure an application named \"orrs\" to replicate data from two Oracle tables \"QATEST.ORACLETOREDSHIFTIL1\" and \"QATEST.ORACLETOREDSHIFTIL2\" to Amazon Redshift with batch processing enabled every minute?\n   \n2. What are the necessary steps to deploy the \"orrs\" application, which uses a Source using DatabaseReader and a Target using RedshiftWriter for data replication between an Oracle database and an Amazon Redshift instance?\n\n3. Can you provide a sequence of commands to stop, undeploy, drop, create, configure, deploy, and start an application called \"orrs\" that performs data replication tasks between an Oracle source and a Redshift target in a continuous streaming manner?", "file_name": "OraToRSInitialLoad.tql"}
{"tql": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL ;\n\n\nCREATE OR REPLACE SOURCE DBSource USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',\n  Tables: 'QATEST.OracToCql_alldatatypes1',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE SOURCE DBSource2 USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',\n  Tables: 'QATEST.OracToCql_alldatatypes2',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE SOURCE DBSource3 USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',\n  Tables: 'QATEST.OracToCql_alldatatypes3',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'TEST.user_chkpoint',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',\n  Tables: 'QATEST.OracToCql_alldatatypes1,test.oractocq_alldatatypes columnmap(NumericToBigint=NumericToBigint);QATEST.OracToCql_alldatatypes2,test.oractocq_alldatatypes columnmap(NumericToBigint=NumericToBigint);QATEST.OracToCql_alldatatypes3,test.oractocq_alldatatypes columnmap(NumericToBigint=NumericToBigint)',\n  IgnorableExceptionCode: 'PRIMARY KEY',\n  Password: 'cassandra',\n  Password_encrypted: false\n )\nINPUT FROM Oracle_ChangeDataStream;\n\ncreate Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\n\nEND APPLICATION DBRTOCW;\ndeploy application DBRTOCW in default;\nstart DBRTOCW;", "generated_queries": "1. What are the details of the Oracle sources configured in the application named \"DBRTOCW\" with LogMiner readers connecting to tables such as 'QATEST.OracToCql_alldatatypes1', 'QATEST.OracToCql_alldatatypes2', and 'QATEST.OracToCql_alldatatypes3'?\n\n2. How is the Oracle Change Data Capture stream being directed to a Cassandra target in the application \"DBRTOCW\", specifying the connection details and table mappings from Oracle to Cassandra?\n\n3. Can you provide the sequence of operations to create, deploy, and start an application named \"DBRTOCW\" with multiple Oracle sources feeding into Oracle Change Data Capture streams and a Cassandra target in TQL?", "file_name": "ManyCDCReadertoSingleDWWriter.tql"}
{"tql": "CREATE APPLICATION @APPNAME@;\n\nCREATE TYPE @APPNAME@type1 (\n companyName java.lang.String,\n merchantId java.lang.String,\n city java.lang.String);\n\nCREATE STREAM @APPNAME@TypedStream OF @APPNAME@type1 PARTITION BY city;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (\n  wildcard: '',\n  positionByEOF: false,\n  directory: ''\n  )\nPARSE USING DSVParser (\nheader:'true'\n)\nOUTPUT TO @APPNAME@Stream;\n\nCREATE OR REPLACE CQ @APPNAME@CQ\nINSERT INTO @APPNAME@TypedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\") as companyName,\nTO_STRING(data[1]) as merchantID,\nTO_STRING(data[10]) as city\nFROM @APPNAME@Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_jmstrgt1 USING JMSWriter (\n  QueueName: '',\n  UserName: '',\n  Password: '',\n  Ctx: '',\n  Provider: ''\n  )\nFORMAT USING DSVFormatter()\nINPUT FROM @APPNAME@TypedStream;\n\nCREATE OR REPLACE TARGET @APPNAME@_jmstrgt2 USING JMSWriter (\n  QueueName: '',\n  UserName: '',\n  Password: '',\n  Ctx: '',\n  Provider: ''\n  )\nFORMAT USING JSONFormatter()\nINPUT FROM @APPNAME@TypedStream;\n\nCREATE OR REPLACE TARGET @APPNAME@_jmstrgt3 USING JMSWriter (\n  QueueName: '',\n  UserName: '',\n  Password: '',\n  Ctx: '',\n  Provider: ''\n  )\nFORMAT USING DSVFormatter()\nINPUT FROM @APPNAME@TypedStream;\n\nEND APPLICATION @APPNAME@;", "generated_queries": "1. What types of data fields are being captured by the application for each transaction?\n2. How are the transactions partitioned within the application based on city information?\n3. What formatting and output options are configured for sending the processed data to JMS queues?", "file_name": "JMSRecoveryMultiAppWriter.tql"}
{"tql": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@;\nCREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream;\n\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.DeltaLakeWriter (\n  personalAccessToken: '',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.student',\n  adapterName: 'DeltaLakeWriter',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:1,interval:10s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @APP_NAME@_Stream;\n\nEND APPLICATION @APP_NAME@;\nDEPLOY APPLICATION @APP_NAME@;\nSTART APPLICATION @APP_NAME@;", "generated_queries": "1. How can I deploy and start a new application named @APP_NAME@ that reads data from an Oracle database table and writes it to Delta Lake using a MERGE mode in Azure Databricks?\n\n2. What is the process to stop, undeploy, drop, create, and then deploy an application called @APP_NAME@ that streams data from an Oracle table named 'QATEST.BQ1' and writes it to a Delta Lake table named 'student' in an Azure Databricks environment?\n\n3. Can you provide the detailed configuration steps required to set up an application in TQL that connects to an Oracle database hosted on localhost, fetches data from a specific table, and writes it to a Delta Lake table with a specific upload policy, all within an Azure Databricks setup?", "file_name": "OracleToDeltaLake.tql"}
{"tql": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@;\n\nCREATE OR REPLACE SOURCE @parquetsrc@ USING Global.HDFSReader (\n  wildcard: '',\n  directory: '',\n  hadoopurl: '',\n  hadoopconfigurationpath: '',\n  positionbyeof: false )\n  PARSE USING ParquetParser (\n   )\nOUTPUT TO @appname@Stream;\n\nCREATE CQ @appname@CQ\nINSERT INTO @appname@CqOut\n    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;\n\nCREATE OR REPLACE TARGET @filetarget@ USING FileWriter (\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:1,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING ParquetFormatter  (\nschemaFileName: 'ParquetFileSchema'\n)\nINPUT FROM @appname@CqOut;\n\ncreate Target @s3target@ using S3Writer(\n    bucketname:'',\n    objectname:'',\n    uploadpolicy:'',\n    foldername:''\n)\nformat using ParquetFormatter (\nschemaFileName: 'ParquetS3Schema'\n)\ninput from @appname@CqOut;\n\ncreate Target @blobtarget@ using AzureBlobWriter(\n\taccountname:'',\n\taccountaccesskey:'',\n\tcontainername:'',\n    blobname:'',\n\tfoldername:'',\n\tuploadpolicy:'EventCount:10,interval:5s'\n)\nformat using ParquetFormatter (\nschemaFileName: 'ParquetAzureSchema'\n)\nINPUT FROM @appname@CqOut;\n\nCREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (\n    bucketname:'',\n    objectname:'',\n    foldername:'',\n    projectId:'',\n    uploadPolicy:''\n)\nformat using ParquetFormatter (\nschemaFileName: 'ParquetGCSSchema'\n)\nINPUT FROM @appname@CqOut;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;", "generated_queries": "1. How can I deploy and run an application named \"@appname@\" that extracts data from a Parquet source, processes it, and writes the output to multiple targets including a file system, AWS S3, Azure Blob Storage, and Google Cloud Storage?\n\n2. Which process should be followed to create a data flow pipeline in TQL that reads data from a Parquet source, enriches it using user-defined functions, and extracts specific information to then route the processed data to different target destinations such as files, S3, Azure Blob, and Google Cloud Storage?\n\n3. Can you provide a step-by-step guide on how to stop, undeploy, drop, create, deploy, and start an application named \"@appname@\" that is responsible for transforming Parquet data and sending the resulting output to diverse target destinations like file systems, AWS S3, Azure Blob Storage, and Google Cloud Storage using TQL commands?", "file_name": "parquetHDFSReader.tql"}
{"tql": "UNDEPLOY APPLICATION admin.BadDeployGroup;\nDROP APPLICATION admin.BadDeployGroup cascade;\n\nCREATE APPLICATION BadDeployGroup;\n\n-- This sample application demonstrates how WebAction could be used\n-- by a retail chain to generate real-time reports on products and\n-- stores and to send alerts of unusual activity.\n\n\nCREATE FLOW BadSourceFlow;\n\n-- RetailDataSource is the primary data source for this application.\n--\n-- ParseOrderData discards the fields not needed by this application and puts the\n-- data into the appropriate Java types.\n--\n-- ParseOrderData outputs to RetailOrders stream, the start point of the\n-- RetailProductFlow and RetailStoreFlow flows.\n\nCREATE SOURCE RetailDataSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'retaildata2M.csv',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO Orders;\n\n-- A stream's type must be declared before the stream, and a CQ's\n-- output stream must be defined before the CQ. Hence type-stream-CQ\n-- sequences like the following are very common.\n\n-- output for ParseOrderData\nCREATE TYPE OrderType(\n  storeId      String,\n  orderId      String,\n  sku          String,\n  orderAmount  double,\n  dateTime     DateTime,\n  hourValue    int,\n  state        String,\n  city         String,\n  zip          String\n);\nCREATE STREAM RetailOrders Of OrderType;\n\nCREATE CQ ParseOrderData\nINSERT INTO RetailOrders\nSELECT  data[0],\n        data[6],\n        data[7],\n        TO_DOUBLE(SRIGHT(data[8],1)),\n        TO_DATE(data[9],'yyyyMMddHHmmss'),\n        DHOURS(TO_DATE(data[9],'yyyyMMddHHmmss')),\n        data[3],\n        data[2],\n        data[4]\nFROM Orders;\n\nEND FLOW BadSourceFlow;\n\n\nCREATE FLOW BadProductFlow;\n\n-- This flow populates the ProductActivity WAction store, which\n-- provides data for dashboard reports on sales by product.\n\n-- defines context for WAction store\nCREATE TYPE ProductActivityContext(\n  sku String  KEY,\n  OrderCount int,\n  SalesAmount double,\n  StartTime DateTime\n);\n\n-- defines event types for Waction store\nCREATE TYPE ProductTrackingType (\n  sku String KEY,\n  OrderCount int,\n  SalesAmount double,\n  StartTime DateTime\n);\n\nCREATE WACTIONSTORE ProductActivity\nCONTEXT OF ProductActivityContext\nEVENT TYPES ( ProductTrackingType )\nPERSIST NONE USING ( ) ;\n\n-- input for GetProductActivity\nCREATE JUMPING WINDOW ProductData_15MIN\nOVER RetailOrders\nKEEP WITHIN 15 MINUTE ON dateTime\nPARTITION BY sku;\n\nCREATE STREAM ProductTrackingStream OF ProductTrackingType;\n\n-- aggregates data and populates WAction store\nCREATE CQ GetProductActivity\nINSERT INTO ProductTrackingStream\nSELECT pd.sku, COUNT(*), SUM(pd.orderAmount), FIRST(pd.dateTime)\nFROM ProductData_15MIN pd\nGROUP BY pd.sku;\n\nCREATE CQ TrackProductActivity\nINSERT INTO ProductActivity\nSELECT sku, OrderCount, SalesAmount, StartTime\nFROM ProductTrackingStream\nLINK SOURCE EVENT;\n\nEND FLOW BadProductFlow;\n\n\nCREATE FLOW BadStoreFlow;\n\n-- This flow populates the StoreActivity WAction store, which provides\n-- data for dashboard reports on reports on stores, and sends alerts when\n-- sales volumes are higher or lower than expected.\n\n-- RetailStoreFlow part 1 - GetStoreActivity\n--\n-- The RetailData_5MIN window bounds the data so that the query can\n-- use the COUNT, FIRST, and SUM functions. (Aggregate functions cannot\n-- be used with unbound real-time data.)\n--\n-- The HourlyStoreSales_Cache cache provides historical averages for the\n-- current hour for each merchant\n\n-- input for GetStoreActivity\nCREATE JUMPING WINDOW RetailData_5MIN\n     OVER RetailOrders\n     KEEP WITHIN 5 MINUTE ON dateTime\n     PARTITION BY storeId;\n\n-- input for GetStoreActivity\nCREATE TYPE StoreHourlyAvg(\n  storeId String,\n  hourValue int,\n  hourlyAvg int,\n  hourlyItemCnt int\n);\nCREATE CACHE HourlyStoreSales_Cache using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'storehourlyData.txt',\n  header: Yes,\n  columndelimiter: ','\n) QUERY (keytomap:'storeId') OF StoreHourlyAvg;\n\n-- output for GetStoreActivity\nCREATE TYPE StoreOrdersTrackingType (\n  storeId String KEY,\n  state String,\n  city  String,\n  zip   String,\n  StartTime DateTime,\n  ordersCount int,\n  salesAmount double,\n  hourlyAvg int,\n  upperLimit double,\n  lowerLimit double,\n  category String,\n  status String\n);\nCREATE STREAM StoreOrdersTracking OF StoreOrdersTrackingType;\n\nCREATE CQ GetStoreActivity\nINSERT INTO StoreOrdersTracking\nSELECT rd.storeId, rd.state, rd.city, rd.zip, first(rd.dateTime),\n       COUNT(rd.storeId), SUM(rd.orderAmount), l.hourlyAvg/6,\n       l.hourlyAvg/6 + l.hourlyAvg/8,\n       l.hourlyAvg/6 - l.hourlyAvg/10,\n       '<NOTSET>', '<NOTSET>'\nFROM RetailData_5MIN rd, HourlyStoreSales_Cache l\nWHERE rd.storeId = l.storeId AND rd.hourValue = l.hourValue\nGROUP BY rd.storeId;\n-- This query aggregates five minutes' worth of data for each\n-- merchant, calculating the total transaction count and amount, and\n-- calculates the upperLimit and lowerLimit values based on the\n-- historical average transaction count for the current hour of the\n-- day from the HourlyStoreSales_Cache cache. The category and status fields\n-- are left unset to be populated by the next query.\n\n\n-- RetailStoreFlow part 2 - GetStoreStatus\n--\n-- This query sets the count values used by the dashboard map and the\n-- status values used to trigger alerts.\n\n-- uses type previously defined for StoreOrdersTracking\nCREATE STREAM StoreOrdersTracking_Status OF StoreOrdersTrackingType;\n\nCREATE CQ GetStoreStatus\nINSERT INTO StoreOrdersTracking_Status\nSELECT storeId, state, city, zip, StartTime,\n       ordersCount, salesAmount, hourlyAvg, upperLimit, lowerLimit,\n       CASE\n         WHEN salesAmount > (upperLimit + 2000) THEN 'HOT'\n         WHEN salesAmount > upperLimit THEN 'MEDIUM'\n         WHEN salesAmount < lowerLimit THEN 'COLD'\n         ELSE 'COOL' END,\n       CASE\n         WHEN salesAmount > upperLimit THEN 'TOOHIGH'\n         WHEN salesAmount < lowerLimit THEN 'TOOLOW'\n         ELSE 'OK' END\nFROM StoreOrdersTracking;\n\n\n-- RetailStoreFlow part 3 - create and populate the StoreActivity WAction store\n\n-- input for CQ TrackStoreActivity\nCREATE TYPE StoreNameData(\n  storeId       String KEY,\n  storeName     String\n);\nCREATE CACHE StoreNameLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'StoreNames.csv',\n  header: Yes,\n  columndelimiter: ','\n) QUERY(keytomap:'storeId') OF StoreNameData;\n\n-- input for CQ TrackStoreActivity\nCREATE TYPE RetailUSAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\nCREATE CACHE ZipCodeLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'USAddresses.txt',\n  header: Yes,\n  columndelimiter: '\t'\n) QUERY (keytomap:'zip') OF RetailUSAddressData;\n\n-- defines WAction store context\nCREATE TYPE StoreActivityContext(\n  storeId String KEY,\n  StartTime DateTime,\n  StoreName String,\n  Category String,\n  Status String,\n  OrderCount int,\n  salesamount double,\n  HourlyAvg int,\n  UpperLimit double,\n  LowerLimit double,\n  Zip String,\n  City String,\n  State String,\n  LatVal double,\n  LongVal double\n);\n\n-- StoreOrdersTrackingType previously defined for StoreOrdersTracking\nCREATE WACTIONSTORE StoreActivity\nCONTEXT OF StoreActivityContext\nEVENT TYPES (StoreOrdersTrackingType )\nPERSIST NONE USING ();\n\nCREATE CQ TrackStoreActivity\nINSERT INTO StoreActivity\nSELECT s.storeId,\n  s.StartTime,\n  n.storeName,\n  s.category,\n  s.status,\n  s.ordersCount,\n  s.salesAmount,\n  s.hourlyAvg,\n  s.upperLimit,\n  s.lowerLimit,\n  z.zip,\n  z.city,\n  s.state,\n  z.latVal,\n  z.longVal\nFROM StoreOrdersTracking_Status s, StoreNameLookup n, ZipCodeLookup z\nWHERE s.storeId = n.storeId AND s.zip = z.zip\nLINK SOURCE EVENT;\n\n\n-- RetailStoreFlow part 4 - send alerts\n\n\nCREATE STREAM RetailAlertStream OF Global.AlertEvent;\n\nCREATE CQ RetailRetailGenerateAlerts\nINSERT INTO RetailAlertStream\nSELECT n.storeName, s.storeId,\n        CASE\n          WHEN s.Status = 'OK' THEN 'info'\n          ELSE 'warning' END,\n        CASE\n          WHEN s.Status = 'OK' THEN 'cancel'\n          ELSE 'raise' END,\n        CASE\n          WHEN s.Status = 'OK' THEN 'Store ' + n.storeName + ' amount of $'+ s.salesAmount + ' is back between $' + s.lowerLimit + ' and $' +s.upperLimit\n          WHEN s.Status = 'TOOHIGH' THEN 'Store ' + n.storeName + ' amount of $'+ s.salesAmount + ' is above upper limit of $' + s.upperLimit\n          WHEN s.Status = 'TOOLOW' THEN 'Store ' + n.storeName + ' amount of $'+ s.salesAmount + ' is below lower limit of $' + s.lowerLimit\n          ELSE ''\n          END\nFROM StoreOrdersTracking_Status s, StoreNameLookup n\nWHERE s.storeId = n.storeId;\n\nEND FLOW BadStoreFlow;\n\n\n-- load dashboard visualization settings from file\n\n--CREATE VISUALIZATION BadDeployGroup \"Samples/Customer/RetailApp/RetailApp_visualization_settings.json\";\n\n-- The following statement defines the user and delivery method for alerts.\nCREATE SUBSCRIPTION BadAlertSub USING WebAlertAdapter( ) INPUT FROM RetailAlertStream;\n\n\nEND APPLICATION BadDeployGroup;", "generated_queries": "1. Can you provide real-time reports on products and stores for a retail chain using WebAction in the BadDeployGroup application?\n2. How does the BadStoreFlow in the BadDeployGroup application populate the StoreActivity WAction store and send alerts based on sales volumes?\n3. What alerts are generated by the RetailRetailGenerateAlerts CQ in the BadStoreFlow of the BadDeployGroup application for monitoring sales activity in retail stores?", "file_name": "BadDeployGroup.tql"}
{"tql": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@;\n\ncreate source @SourceName1@ USING IncrementalBatchReader\n(\n  FetchSize: 10,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: @checkColumn@,\n  startPosition: '@startPosition@',\n  PollingInterval: '20sec'\n)\nOUTPUT TO @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:@targetsys@) input from @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter(\n  ConnectionURL:'@READER-URL@',\n  Username:'@READER-UNAME@',\n  Password:'@READER-PASSWORD@',\n  BatchPolicy:'Eventcount:1,Interval:1',\n  CommitPolicy:'Eventcount:1,Interval:1',\n  Checkpointtable:'RGRN_CHKPOINT',\n  Tables:'@WATABLES@,@WATABLES@_target'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate source @SourceName2@ USING IncrementalBatchReader\n(\nFetchSize: 10,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: @checkColumn1@,\n  startPosition: '@startPosition1@',\n  PollingInterval: '20sec'\n)\nOUTPUT TO @SRCINPUTSTREAM1@;\n\ncreate Target @targetsys1@ using SysOut(name:@targetsys1@) input from @SRCINPUTSTREAM1@;\n\nCREATE TARGET @targetName1@ USING DatabaseWriter(\n  ConnectionURL:'@READER-URL@',\n  Username:'@READER-UNAME@',\n  Password:'@READER-PASSWORD@',\n  BatchPolicy:'Eventcount:1,Interval:1',\n  CommitPolicy:'Eventcount:1,Interval:1',\n  Checkpointtable:'RGRN_CHKPOINT',\n  Tables:'@WATABLES_target'\n) INPUT FROM @SRCINPUTSTREAM1@;\n\nEND APPLICATION @APPNAME@;\n\nDEPLOY APPLICATION @APPNAME@;\nstart application @APPNAME@;", "generated_queries": "1. How can I create an application with multiple sources and targets in Striim?\n2. What is the process for deploying and starting an application named @APPNAME@ in Striim?\n3. Can you show me how to set up incremental data replication from a MariaDB database to a target system using Striim?", "file_name": "MultiIBR_MultiDBR_IBR.tql"}
{"tql": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()\nPARSE USING XMLParser (\n  rootnode: ''\n)\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()\nFORMAT USING JSONFormatter ()\nINPUT FROM @APPNAME@_Stream;\n\nEND APPLICATION @APPNAME@;", "generated_queries": "1. What process is being used to parse the data in the source system before it is sent to the streaming application @APPNAME@?\n  \n2. How is the data formatted before being input into the target system in the application @APPNAME@?\n\n3. Can you provide details on the streaming application @APPNAME@, including the source and target systems being utilized and the data processing steps involved in the data flow?", "file_name": "XmlToJson.tql"}
{"tql": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@;\n\n\nCREATE SOURCE @APPNAME@_Source USING MySqlReader\n(\n  Compression: false,\n  FetchSize: 1,\n  SendBeforeImage: true,\n  ConnectionURL: 'mysql://localhost:3306',\n  DatabaseName: 'waction',\n  Tables: 'waction.test01',\n  Password: 'w@ct10n',\n  Password_encrypted: 'false',\n  Username: 'root'\n)\nOUTPUT TO @APPNAME@_Stream;\n\n\nCREATE TARGET @APPNAME@_Target USING SpannerWriter (\n\tTables: 'QATEST.%,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tBatchPolicy: 'EventCount: 1; Interval: 1s',\n\tinstanceId: 'qatest'\n) INPUT FROM @APPNAME@_stream;\n\ncreate Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_stream;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;", "generated_queries": "1. What are the steps involved in deploying and running an application named \"@APPNAME@\" that reads data from a MySQL database table \"waction.test01\" and writes to a Spanner table \"QATEST\" in instance \"qatest\" with a batch policy of 1 event per second?\n   \n2. How can I configure an application, named \"@APPNAME@\", to continuously stream data from a MySQL database located at \"mysql://localhost:3306\" and write it to a Spanner table in a Cloud Spanner instance \"qatest\" using SpannerWriter target with a specified Service Account Key file path?\n\n3. What are the commands required to stop, undeploy, drop, create, configure sources and targets, and finally deploy and start an application named \"@APPNAME@\" for data streaming between a MySQL database and a Spanner table?", "file_name": "MysqlCDCSpannerDBWriter.tql"}
{"tql": "CREATE APPLICATION jsonapp;\n\nCREATE TYPE JSONAccessLogEntry (\n    srcIp String KEY,\n    userId String,\n    sessionId String,\n    accessTime long,\n    request String,\n    code int,\n    size int,\n    referrer String,\n    userAgent String,\n    responseTime int\n);\nCREATE STREAM JSONSourceStream OF JSONAccessLogEntry;\n\nCREATE SOURCE JSONAccessLogSource USING FileReader(\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'FileWithJSONTest.json',\n  positionbyeof:false\n)\nparse using JSONParser (\n  eventType:'admin.JSONAccessLogEntry'\n) OUTPUT TO JSONSourceStream;\n\ncreate Target JSONDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/jsondata') input FROM JSONSourceStream;\n\nEND APPLICATION jsonapp;", "generated_queries": "1. How can I analyze JSON access log data to track the time of user requests, response codes, and the size of server responses within a specific application?\n2. What are the most frequently accessed user sessions for a particular source IP address in the JSON access logs of an application?\n3. Can I retrieve a list of all unique user agents that have accessed the application, along with the corresponding response times and referrer URLs from the JSON access log entries?", "file_name": "FileReaderWithJSONParser.tql"}
{"tql": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE CCBReader USING FileReader (\n  wildcard: '@WILDCARD@',\n  positionbyeof: false,\n  directory: '@TESTDIR@'\n  )\nPARSE USING Global.CobolCopybookParser (\n  copybookDialect: 'Mainframe',\n  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',\n  GroupPolicy: '@GRPPOLICY@',\n  TimeoutPolicy: '1s',\n  dataFileOrganization: 'FixedLength',\n  ProcessCopyBookFileAs: 'MultipleEvents',\n  skipIndent: 0,\n  dataFileFont: 'utf8',\n  CopybookFileFormat: 'USE_STANDARD_COLUMNS',\n  copybookSplit: 'None',\n  copybookFileName: '@CCBFILE@'\n   )\nOUTPUT TO CCBStream;\n\ncreate Target KafkaTarget using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'',\nTopic:'',\nMode: 'Sync',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;linger.ms=30000'\n)\nFORMAT USING JSONFormatter (\nmembers:'data')\ninput from CCBStream;\n\ncreate source KafkaSource using KafkaReader VERSION '2.1.0'(\nbrokerAddress:'',\n\tTopic:''\n)\nparse using JSONParser ()\noutput to KafkaStream;\n\nCREATE OR REPLACE TARGET JSONWriter USING FileWriter (\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:1,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING JSONFormatter ()\nINPUT FROM KafkaStream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@ on all in default;\nstart application @APPNAME@;", "generated_queries": "1. How can I create an application named '@APPNAME@' with a recovery interval of 5 seconds and set up various sources and targets such as FileReader, KafkaWriter, KafkaReader, and FileWriter within this application configuration?\n\n2. What are the specific configurations for creating a KafkaTarget using a KafkaWriter with certain Kafka configurations like request timeout, session timeout, and linger time, all associated with the application '@APPNAME@'?\n\n3. How can I deploy and start the application '@APPNAME@' with all its sources and targets being configured through TQL commands like stop, undeploy, drop, create, end, deploy, and start?", "file_name": "cobolParserToKafkaWriter.tql"}
{"tql": "stop application @APPNAME1@;\nundeploy application @APPNAME1@;\nstop application @APPNAME2@;\nundeploy application @APPNAME2@;\nstop application @APPNAME3@;\nundeploy application @APPNAME3@;\ndrop application @APPNAME1@ cascade;\ndrop application @APPNAME2@ cascade;\ndrop application @APPNAME3@ cascade;\n\nCREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\nCREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;\nCREATE OR REPLACE SOURCE @SourceName@ Using OracleReader\n(\n  Compression:true,\n  StartTimestamp:'null',\n  CommittedTransactions:true,\n  FilterTransactionBoundaries:true,\n  Password_encrypted:'false',\n  SendBeforeImage:true,\n  XstreamTimeOut:600,\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Tables:'qatest.oraMultiDownstream_src',\n  adapterName:'OracleReader',\n  Password:'qatest',\n  DictionaryMode:'OfflineCatalog',\n  FilterTransactionState:true,\n  connectionRetryPolicy:'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType:'LogMiner',\n  FetchSize: 1,\n  Username:'qatest',\n  OutboundServerProcessName:'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic:true,\n  CDDLAction:'Quiesce_Cascade',\n  CDDLCapture:'true'\n)OUTPUT TO @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME1@;\nDEPLOY APPLICATION @APPNAME1@;\nSTART APPLICATION @APPNAME1@;\n\nCREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  \n(\n  DatabaseProviderType:'Default',\n  CheckPointTable:'CHKPOINT',\n  PreserveSourceTransactionBoundary:'false',\n  Username:'qatest',\n  BatchPolicy:'EventCount:1',\n  CommitPolicy:'EventCount:1',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',\n  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',\n  Password:'qatest',\n  CDDLAction:'Process'\n) \nINPUT FROM @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME2@;\nDEPLOY APPLICATION @APPNAME2@;\nSTART APPLICATION @APPNAME2@;\n\n\nCREATE APPLICATION @APPNAME3@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE TARGET @targetName1@ USING DatabaseWriter  \n(\n  DatabaseProviderType:'Default',\n  CheckPointTable:'CHKPOINT',\n  PreserveSourceTransactionBoundary:'false',\n  Username:'qatest',\n  BatchPolicy:'EventCount:1',\n  CommitPolicy:'EventCount:1',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',\n  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',\n  Password:'qatest',\n  CDDLAction:'Process'\n) \nINPUT FROM @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME3@;\nDEPLOY APPLICATION @APPNAME3@;\nSTART APPLICATION @APPNAME3@;", "generated_queries": "1. Can you provide details on the applications that have been undeployed and then re-deployed with specific recovery settings and target configurations in a TQL script?\n\n2. Which source and target configurations were created and linked to applications that have been stopped, undeployed, and then started again using a certain TQL script?\n\n3. How can I ensure that specific applications are stopped, undeployed, and then redeployed with the defined recovery intervals and database writer settings all in one TQL query?", "file_name": "QuiesceCascade_MultiDownstream.tql"}
{"tql": "STOP APPLICATION KafkaWPTester.KWApp;\nSTOP APPLICATION KafkaWPTester.KRApp;\nUNDEPLOY APPLICATION KafkaWPTester.KWApp;\nUNDEPLOY APPLICATION KafkaWPTester.KRApp;\nDROP APPLICATION KafkaWPTester.KWApp CASCADE;\nDROP APPLICATION KafkaWPTester.KRApp CASCADE;\n\nCREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;\nCONNECT KafkaWPTester KafkaWPTester;\n\n\nCREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE SOURCE CSVSource USING FileReader (\n\tdirectory:'/Users/saranyad/Product/IntegrationTests/TestData/kafka_tmp',\n    WildCard:'mybanks*',\n\tpositionByEOF:false,\n        Charset:'UTF-8'\n)\nPARSE USING DSVParser (\n  columndelimiter:',',\n  ignoreemptycolumn:'Yes',\n  quoteset:'[]~\"',\n  separator:'~'\n)\nOUTPUT TO FileStream;\n\n\nCREATE TYPE AccessLogType(\nCol1 String,\nCol2 String\n);\n\nCREATE STREAM TypedAccessLogStream1 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream2 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream3 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream4 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream5 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream6 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream7 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream8 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream9 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream10 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream11 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream12 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream13 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream14 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream15 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream16 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream17 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream18 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream19 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream20 OF AccessLogType;\n\nCREATE CQ AcceeslogCQ\nINSERT INTO TypedAccessLogStream\nSELECT \nTO_STRING(data[0]) as Col1,\nTO_STRING(data[1]) as Col2\nFROM FileStream ; \n\nCREATE CQ AcceeslogCQ1\nINSERT INTO TypedAccessLogStream1\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS WHERE FS.Col1 = '1'; \n\n\ncreate Target KW1 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test01',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream1;\n\nCREATE CQ AcceeslogCQ2\nINSERT INTO TypedAccessLogStream2\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '2'; \n\n\ncreate Target KW2 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test02',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream2;\n\nCREATE CQ AcceeslogCQ3\nINSERT INTO TypedAccessLogStream3\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '3'; \n\n\ncreate Target KW3 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test03',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream3;\n\nCREATE CQ AcceeslogCQ4\nINSERT INTO TypedAccessLogStream4\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '4'; \n\n\ncreate Target KW4 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test04',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream4;\n\nCREATE CQ AcceeslogCQ5\nINSERT INTO TypedAccessLogStream5\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '5'; \n\n\ncreate Target KW5 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test05',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream5;\n\nCREATE CQ AcceeslogCQ6\nINSERT INTO TypedAccessLogStream6\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '6'; \n\n\ncreate Target KW6 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test06',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream6;\n\nCREATE CQ AcceeslogCQ7\nINSERT INTO TypedAccessLogStream7\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '7'; \n\n\ncreate Target KW7 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test07',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream7;\n\nCREATE CQ AcceeslogCQ8\nINSERT INTO TypedAccessLogStream8\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '8'; \n\n\ncreate Target KW8 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test08',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream8;\n\nCREATE CQ AcceeslogCQ9\nINSERT INTO TypedAccessLogStream9\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '9'; \n\n\ncreate Target KW9 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test09',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream9;\n\nCREATE CQ AcceeslogCQ10\nINSERT INTO TypedAccessLogStream10\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '10'; \n\n\ncreate Target KW10 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test10',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream10;\n\nCREATE CQ AcceeslogCQ11\nINSERT INTO TypedAccessLogStream11\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '11'; \n\n\ncreate Target KW11 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test11',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream11;\n\nCREATE CQ AcceeslogCQ12\nINSERT INTO TypedAccessLogStream12\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '12'; \n\n\ncreate Target KW12 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test12',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream12;\n\nCREATE CQ AcceeslogCQ13\nINSERT INTO TypedAccessLogStream13\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '13'; \n\n\ncreate Target KW13 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test13',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream13;\n\nCREATE CQ AcceeslogCQ14\nINSERT INTO TypedAccessLogStream14\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '14'; \n\n\ncreate Target KW14 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test14',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream14;\n\nCREATE CQ AcceeslogCQ15\nINSERT INTO TypedAccessLogStream15\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '15'; \n\n\ncreate Target KW15 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test15',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream15;\n\nCREATE CQ AcceeslogCQ16\nINSERT INTO TypedAccessLogStream16\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '16'; \n\n\ncreate Target KW16 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test16',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream16;\n\nCREATE CQ AcceeslogCQ17\nINSERT INTO TypedAccessLogStream17\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '17'; \n\n\ncreate Target KW17 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test17',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream17;\n\nCREATE CQ AcceeslogCQ18\nINSERT INTO TypedAccessLogStream18\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '18'; \n\n\ncreate Target KW18 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test18',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream18;\n\nCREATE CQ AcceeslogCQ19\nINSERT INTO TypedAccessLogStream19\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '19'; \n\n\ncreate Target KW19 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test19',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream19;\n\nCREATE CQ AcceeslogCQ20\nINSERT INTO TypedAccessLogStream20\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '20'; \n\n\ncreate Target KW20 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test20',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream20;\n\n\nEND APPLICATION KWApp;\n\nDEPLOY APPLICATION KWApp on any in default;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-- Kafka Reader Apps\n\nCREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;\n\nCREATE SOURCE KR1 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test01',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream1;\n\n\n\nCREATE SOURCE KR2 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test02',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream2;\n\n\nCREATE SOURCE KR3 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test03',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream3;\n\n\nCREATE SOURCE KR4 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test04',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream4;\n\n\nCREATE SOURCE KR5 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test05',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream5;\n\n\nCREATE SOURCE KR6 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test06',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream6;\n\n\nCREATE SOURCE KR7 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test07',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream7;\n\n\nCREATE SOURCE KR8 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test08',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream8;\n\n\nCREATE SOURCE KR9 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test09',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream9;\n\n\nCREATE SOURCE KR10 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test10',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream10;\n\n\nCREATE SOURCE KR11 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test11',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream11;\n\n\n\nCREATE SOURCE KR12 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test12',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream12;\n\n\nCREATE SOURCE KR13 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test13',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream13;\n\n\nCREATE SOURCE KR14 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test14',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream14;\n\n\nCREATE SOURCE KR15 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test15',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream15;\n\n\nCREATE SOURCE KR16 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test16',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream16;\n\n\nCREATE SOURCE KR17 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test17',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream17;\n\n\nCREATE SOURCE KR18 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test18',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream18;\n\n\nCREATE SOURCE KR19 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test19',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream19;\n\n\nCREATE SOURCE KR20 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test20',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream20;\n\n\n\n\nCREATE TARGET DumpKafkaReaderStream1 USING FileWriter(\n  name:KafkaROuput1,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_1',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream1;\n\n\nCREATE TARGET DumpKafkaReaderStream2 USING FileWriter(\n  name:KafkaROuput2,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_2',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream2;\n\n\nCREATE TARGET DumpKafkaReaderStream3 USING FileWriter(\n  name:KafkaROuput3,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_3',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream3;\n\n\nCREATE TARGET DumpKafkaReaderStream4 USING FileWriter(\n  name:KafkaROuput4,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_4',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream4;\n\n\nCREATE TARGET DumpKafkaReaderStream5 USING FileWriter(\n  name:KafkaROuput5,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_5',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream5;\n\n\nCREATE TARGET DumpKafkaReaderStream6 USING FileWriter(\n  name:KafkaROuput6,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_6',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream6;\n\n\nCREATE TARGET DumpKafkaReaderStream7 USING FileWriter(\n  name:KafkaROuput7,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_7',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream7;\n\n\nCREATE TARGET DumpKafkaReaderStream8 USING FileWriter(\n  name:KafkaROuput8,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_8',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream8;\n\n\nCREATE TARGET DumpKafkaReaderStream9 USING FileWriter(\n  name:KafkaROuput9,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_9',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream9;\n\n\nCREATE TARGET DumpKafkaReaderStream10 USING FileWriter(\n  name:KafkaROuput10,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_10',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream10;\n\n\nCREATE TARGET DumpKafkaReaderStream11 USING FileWriter(\n  name:KafkaROuput11,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_11',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream11;\n\n\nCREATE TARGET DumpKafkaReaderStream12 USING FileWriter(\n  name:KafkaROuput12,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_12',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream12;\n\n\nCREATE TARGET DumpKafkaReaderStream13 USING FileWriter(\n  name:KafkaROuput13,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_13',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream13;\n\n\nCREATE TARGET DumpKafkaReaderStream14 USING FileWriter(\n  name:KafkaROuput14,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_14',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream14;\n\n\nCREATE TARGET DumpKafkaReaderStream15 USING FileWriter(\n  name:KafkaROuput15,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_15',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream15;\n\n\nCREATE TARGET DumpKafkaReaderStream16 USING FileWriter(\n  name:KafkaROuput16,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_16',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream16;\n\n\nCREATE TARGET DumpKafkaReaderStream17 USING FileWriter(\n  name:KafkaROuput17,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_17',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream17;\n\n\nCREATE TARGET DumpKafkaReaderStream18 USING FileWriter(\n  name:KafkaROuput18,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_18',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream18;\n\n\nCREATE TARGET DumpKafkaReaderStream19 USING FileWriter(\n  name:KafkaROuput19,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_19',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream19;\n\n\nCREATE TARGET DumpKafkaReaderStream20 USING FileWriter(\n  name:KafkaROuput20,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_20',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream20;\n\nEND APPLICATION KRApp;\nDEPLOY APPLICATION KRApp on any in default;", "generated_queries": "1. What is the configuration for deploying and managing applications in a Kafka environment?\n2. How can I create Kafka streaming applications to process data from CSV files and Kafka topics?\n3. How do you set up Kafka readers to read from different topics and write the data to files in a Kafka Writer sync mode?", "file_name": "startHang.tql"}
{"tql": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\ncreate flow @APPNAME@_agentflow;\n\nCreate Source @SOURCE@\n        Using OracleReader\n(\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: 'qatest.test77',\n  FetchSize:1,\n  QueueSize:2000,\n  CommittedTransactions:true,\n  Compression:false\n)\nOutput To @STREAM@;\n\nend flow @APPNAME@_agentflow;\n\ncreate flow @APPNAME@_serverflow;\n\ncreate Target @TARGET@ using AzureblobWriter(\n    accountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:7'\n)\nformat using DSVFormatter (\n)\ninput from @STREAM@;\n\nend flow @APPNAME@_serverflow;\n\nend application @APPNAME@;", "generated_queries": "1. How can I undeploy, stop, and drop an application named @APPNAME@ and create a new application with a recovery interval of 5 seconds?\n2. What is the process for creating a flow named @APPNAME@_agentflow and setting up a source using an Oracle Reader to pull data from a specific table in a database?\n3. How do I create a server flow named @APPNAME@_serverflow, establish a target using an Azure Blob Writer with specific account credentials and settings, and link it to the source stream @STREAM@ for data processing?", "file_name": "OracleRBlobWriter_Upgrade_Agent.tql"}
{"tql": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  --Password_encrypted: false\n ) \nINPUT FROM @STREAM@;", "generated_queries": "1. How can I create a target in my data processing pipeline that writes data to a specific database table using a certain set of credentials?\n2. What is the configuration required to set up a target in my streaming application that writes events to a database table with a specified batch and commit policy?\n3. How do I ensure that the data from a stream is securely written to a database table while controlling the batch and commit policies for the write operation?", "file_name": "DatabaseWriter.tql"}
{"tql": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCreate Source @APPNAME@_src Using OracleReader\n(\n Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n   _h_ReturnDateTimeAs:'ZonedDateTime'\n) Output To @APPNAME@_stream;\n\ncreate Target @APPNAME@_tgt using FileWriter(\n  filename:'CompressedMerchant.gz',\n  directory:'/logs/',\n  rolloverpolicy:'EventCount:10000'\n)\nformat using ParquetFormatter (\n\tschemaFileName:'@FILENAME@',\n\tFormatAs:'@FORMATAS@'\n)\ninput from @APPNAME@_stream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;", "generated_queries": "1. How can I stop, undeploy, drop, and then recreate an application named @APPNAME@ with a recovery interval of 5 seconds?\n2. How can I create a source named '@APPNAME@_src' using an Oracle Reader, with specific configuration settings like compression, queue size, and connection details such as URL, tables, username, and password?\n3. How can I set up a target named '@APPNAME@_tgt' to write data to a file in a specified directory and with a rollover policy, using the Parquet format and a schema file, with input coming from the source created earlier named '@APPNAME@_stream'?", "file_name": "ParquetFormatterRecovery.tql"}
{"tql": "stop application @APPNAME1@;\nundeploy application @APPNAME1@;\nstop application @APPNAME2@;\nundeploy application @APPNAME2@;\ndrop application @APPNAME1@ cascade;\ndrop application @APPNAME2@ cascade;\n\n\nCREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\nCREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;\nCREATE OR REPLACE SOURCE @SourceName@ Using OracleReader\n(\n  Compression:true,\n  StartTimestamp:'null',\n  FetchSize:1,\n  CommittedTransactions:true,\n  QueueSize:2048,\n  FilterTransactionBoundaries:true,\n  Password_encrypted:'false',\n  SendBeforeImage:true,\n  XstreamTimeOut:600,\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',\n  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE',\n  adapterName:'OracleReader',\n  Password:'qatest',\n  DictionaryMode:'OfflineCatalog',\n  FilterTransactionState:true,\n  connectionRetryPolicy:'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType:'LogMiner',\n  Username:'qatest',\n  OutboundServerProcessName:'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic:true,\n  CDDLAction:'Quiesce_Cascade',\n  CDDLCapture:'true'\n)OUTPUT TO @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME1@;\nDEPLOY APPLICATION @APPNAME1@;\nSTART APPLICATION @APPNAME1@;\n\nCREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE TARGET @targetName@ USING BigqueryWriter  \n(\n  serviceAccountKey:'/Users/hariharasudhan/Downloads/google-gcs.json',\n  projectId:'striimqa-214712',\n  datalocation:'US',\n  Tables:'public.dbr_pg234567890123456789source1,public.dbr_pg234567890123456789Target1;public.dbr_pg234567890123456789source2,public.dbr_pg234567890123456789Target2',\n  BatchPolicy:\"eventCount:1,Interval:90\",\n  CDDLAction:'Process'\n) \nINPUT FROM @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME2@;\nDEPLOY APPLICATION @APPNAME2@;\nSTART APPLICATION @APPNAME2@;", "generated_queries": "1. What applications are currently deployed and running in the system?\n2. What sources and targets are configured for the applications named @APPNAME1@ and @APPNAME2@?\n3. How are the OracleReader and BigqueryWriter sources and targets configured in the system for data ingestion and processing?", "file_name": "QuiesceCascade_BQSingReader.tql"}
{"tql": "stop CSVToHBase;\nundeploy application CSVToHBase;\ndrop application CSVToHBase cascade;\n\nCREATE APPLICATION CSVToHBase;\n\nCREATE OR REPLACE SOURCE CSVPoller USING FileReader ( \n\tdirectory:'/Users/ravipathak/webactionrepo/Product',\n\tWildCard:'smallpos.csv',\n\tpositionByEOF:false\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCREATE OR REPLACE TYPE CSVStream_Type  ( BUSINESS_NAME java.lang.String KEY, \nMERCHANT_ID java.lang.String, \nPRIMARY_ACCOUNT_NUMBER java.lang.String  \n ) ;\n\nCREATE OR REPLACE STREAM CSVTypeStream OF CSVStream_Type;\n\nCREATE OR REPLACE CQ CQ1 \nINSERT INTO CSVTypeStream\nSELECT data[0],data[1],data[2]\nFROM CsvStream;\n\nCREATE OR REPLACE TARGET Target1 USING SysOut ( \n  name: \"dstream\"\n ) \nINPUT FROM CsvStream;\n\nCREATE OR REPLACE TARGET Target2 using HBaseWriter(\n HBaseConfigurationPath:\"/Users/ravipathak/soft/hbase-1.1.5/conf/hbase-site.xml\",\n  Tables: \"maprtest.maprdata\",\n  --FamilyNames: \"maprdata\",\n  BatchPolicy: \"eventCount:1\")\nINPUT FROM CSVTypeStream;\n\nEND APPLICATION CSVToHBase;\n\ndeploy application CSVToHBase;\nstart CSVToHBase;", "generated_queries": "1. How can I deploy an application named CSVToHBase that processes CSV files located in a specific directory?\n2. What are the steps to create a continuous query (CQ) in TQL that extracts data from a CSV file and inserts it into a target stream named CSVTypeStream?\n3. Can you provide instructions on setting up a target in TQL that writes data from a CSVTypeStream into an HBase table using the HBaseWriter component?", "file_name": "CSVToHBase.tql"}
{"tql": "stop application @AppName@;\nUndeploy application @AppName@;\nAlter application @AppName@;\n\nCREATE FLOW @AppName@_Agent_flow;\n\nCREATE OR REPLACE SOURCE @AppName@_FileReaderSource USING FileReader (\nwildcard: 'posdata.csv', \n  positionByEOF: false, \n  blocksize: 10100,\n  rolloverpolicy: 'EventCount:100,Interval:30s', \n  directory: '@dir@' ) \nPARSE USING DSVParser ( \n  trimquote: false, \n  header: 'Yes' ) \nOUTPUT TO @AppName@_CsvStream;\n\nEND FLOW @AppName@_Agent_flow;\n\nalter application @AppName@ recompile;\nDEPLOY APPLICATION @AppName@ with @AppName@_Agent_flow on any in AGENTS;\nstart application @AppName@;", "generated_queries": "1. Can you provide the steps to stop, undeploy, alter, create, and deploy an application named @AppName@?\n2. How can I recompile and start an application named @AppName@ along with a specific flow called @AppName@_Agent_flow using Text Query Language (TQL)?\n3. What are the configuration details for a source named @AppName@_FileReaderSource, and how is it related to a stream named @AppName@_CsvStream in a TQL query for application deployment?", "file_name": "Alter_AliasCheck.tql"}
{"tql": "STOP ModifyBeforeDataTester.ModifyBeforeData;\nUNDEPLOY APPLICATION ModifyBeforeDataTester.ModifyBeforeData;\nDROP APPLICATION ModifyBeforeDataTester.ModifyBeforeData CASCADE;\n\nCREATE APPLICATION ModifyBeforeData;\n\ncreate source GGTrailSource using FileReader (\n    directory:'@TEST-DATA-PATH@/OGG/oracle_alltypes_122',\n    WildCard:'ld*',\n    positionByEOF:false\n) parse using GGTrailParser (\n    FilterTransactionBoundaries: true,\n    metadata:'@TEST-DATA-PATH@/OGG/oracle_alltypes_122/def_oracle_alltypes122.def',\n    compression:false\n)\nOUTPUT TO SourceStream;\n\nCREATE STREAM ModifiedBeforeStream OF Global.WAEvent;\nCREATE STREAM ModifiedDataStream OF Global.WAEvent;\n\nCREATE OR REPLACE CQ ModifierBeforeCQ\nINSERT INTO ModifiedBeforeStream\nSELECT * FROM SourceStream\nwhere not(before is null)\nmodify(before[1] = data[1].toString().replaceAll(\".\", \"B\"));\n\nCREATE OR REPLACE CQ ModifierDataCQ\nINSERT INTO ModifiedDataStream\nSELECT * FROM SourceStream\nwhere not(before is null)\nmodify(data[1] = before[1].toString().replaceAll(\".\", \"D\"));\n\n-- Generate the final filtered stream to write to the waction store.\nCREATE OR REPLACE CQ CopyBeforeCQ\nINSERT INTO FilteredBeforeStream\nSELECT *\nFROM (SELECT TO_STRING(before[1]) as tcolBefore\n      FROM ModifiedBeforeStream) as src\nwhere src.tcolBefore like 'BBBBBB%';\n\nCREATE OR REPLACE CQ CopyDataCQ\nINSERT INTO FilteredDataStream\nSELECT *\nFROM (SELECT TO_STRING(data[1]) as tcolData\n      FROM ModifiedDataStream) as src\nwhere src.tcolData like \"DDDDDD%\";\n\n-- Need duplicate types due to a limitation of Waction Stores: two Waction Stores\n-- should not share the same type for CONTEXT OF.\nCREATE TYPE WactionType1 (\n  colTest String KEY\n);\n\nCREATE TYPE WactionType2 (\n  colTest String KEY\n);\n\nCREATE WACTIONSTORE WactionsBefore CONTEXT OF WactionType1\nEVENT TYPES ( WactionType1 )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE WactionsData CONTEXT OF WactionType2\nEVENT TYPES ( WactionType2 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions1\nINSERT INTO WactionsBefore\nSELECT tcolBefore as colTest\nFROM FilteredBeforeStream;\n\nCREATE CQ InsertWactions2\nINSERT INTO WactionsData\nSELECT tcolData as colTest\nFROM FilteredDataStream;\n\nEND APPLICATION ModifyBeforeData;", "generated_queries": "1. What is the process for modifying before and after data values in a stream using a Complex Event Processing (CEP) application?\n2. How can I set up a data processing pipeline to replace certain characters in before and after data values before storing them in a Waction Store using SQL queries?\n3. Can you provide a step-by-step guide on creating and deploying an application that modifies, filters, and stores specific data values in a Waction Store based on predefined criteria in the input stream?", "file_name": "ModifyBeforeData.tql"}
{"tql": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  --Password_encrypted: false\n )\nINPUT FROM @STREAM@;", "generated_queries": "1. What is the target database and table where the data from the specified stream is being written to?\n2. What are the connection details for the database where the data is being written from the specified stream?\n3. Is the source transaction boundary being preserved when writing data from the specified stream to the target database and table?", "file_name": "DatabaseWriterMultiApp1.tql"}
{"tql": "STOP APPLICATION ORACLETOBIGQUERY;\nUNDEPLOY APPLICATION ORACLETOBIGQUERY;\nDROP APPLICATION ORACLETOBIGQUERY CASCADE;\n\n--create application \nCREATE APPLICATION OracleToBigquery RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE OracleSource USING OracleReader (\n ConnectionURL: '192.168.123.30:1521:ORCL',\n Tables: 'QATEST.E1PLOADTEST',\n Username: 'qatest',\n Password: 'qatest',\n FetchSize:100,\n OnlineCatalog:true,\n QueueSize:2000,\n CommittedTransactions:true,\n Compression:false\n) OUTPUT TO CDCStream;\n\nCREATE OR REPLACE TARGET bqtables using BigqueryWriter(\n BQServiceAccountConfigurationPath:\"/Users/ravipathak/Downloads/bqtest-e287bcb47998.json\",\n projectId:\"bqtest-158706\",\n Tables: \"QATEST.E1PLOADTEST,issues.DEV11070\",\n BatchPolicy: \"eventCount:100,Interval:1\")\nINPUT FROM CDCStream;\n\nCREATE OR REPLACE TARGET T1 using SysOut(\nname : \"some text\"\n)\nINPUT FROM CDCStream;\n\nEND APPLICATION OracleToBigquery;\n\nDEPLOY APPLICATION OracleToBigquery;\nSTART APPLICATION OracleToBigquery;", "generated_queries": "1. Can you provide the steps to deploy and start an application named OracleToBigquery that transfers data from an Oracle source to Bigquery and also streams data to a SysOut target?\n   \n2. How do I configure an application called OracleToBigquery with recovery set at a 5-second interval, using an Oracle source to read data from a specific table and output it to a CDCStream, while writing to BigQuery tables and also streaming to a SysOut target?\n\n3. What TQL commands should I use to stop, undeploy, and drop the application OracleToBigquery, while also creating it with specified configurations for recovery interval and source/target settings before deploying and starting it?", "file_name": "ORAToBQ.tql"}
{"tql": "CREATE OR REPLACE APPLICATION @AppName@ USE EXCEPTIONSTORE TTL : '7d';\n\nCreate Source @AppName@_source Using OracleReader(\n  Username:'@username@',\n  Password:'@password@',\n  ConnectionURL:'@url@',\n  Tables:'@srctableName@',\n  Fetchsize:1\n)\nOutput To @AppName@_Stream;\nCREATE OR REPLACE TARGET @AppName@_Target USING Global.SnowflakeWriter (\n  streamingUpload: 'false',\n  useConnectionProfile:'true',\n  connectionProfileName: '@CP@',\n  CDDLAction: 'Process',\n  optimizedMerge: 'false',\n  columnDelimiter: '|',\n  tables: '@srctableName@,@trgtableName@',\n  appendOnly: 'false',\n  uploadPolicy: 'eventcount:1000,interval:60s',\n  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',\n  externalStageType: 'Local',\n  adapterName: 'SnowflakeWriter',\n  fileFormatOptions: 'null_if = \\\"\\\"' )\nINPUT FROM @AppName@_Stream;\nEND APPLICATION @AppName@;\nDeploy application @AppName@;\nStart application @AppName@;", "generated_queries": "1. What is the TTL (time-to-live) set for the application named @AppName@?\n2. Which source type and connection details are defined for the source in the application named @AppName@?\n3. What is the upload policy and UUID specified for the target in the application named @AppName@?", "file_name": "SF_OAuth.tql"}
{"tql": "stop application APP_HEARTBEATS;\nundeploy application APP_HEARTBEATS;\ndrop application APP_HEARTBEATS cascade;\n\nCREATE APPLICATION APP_HEARTBEATS;\n\nCREATE OR REPLACE CQ CQ_HB_90SEC \nINSERT INTO STREAM_CQ_HB_90SEC \nselect makeWAEvent(dnow()) as dummy from heartbeat(interval 90 second) h;\n\nCREATE OR REPLACE CQ CQ_HB_10SEC \nINSERT INTO STREAM_CQ_HB_10SEC \nselect makeWAEvent(dnow()) as dummy from heartbeat(interval 10 second) h;\n\nCREATE OR REPLACE CQ CQ_HB_1MIN \nINSERT INTO STREAM_CQ_HB_1MIN \nselect makeWAEvent(dnow()) as dummy from heartbeat(interval 1 minute) h;\n\nCREATE OR REPLACE CQ CQ_HB_30SEC \nINSERT INTO STREAM_CQ_HB_30SEC \nselect makeWAEvent(dnow()) as dummy from heartbeat(interval 30 second) h;\n\nCREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_90SEC OVER STREAM_CQ_HB_90SEC \nKEEP 1 ROWS;\n\nCREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_10SEC OVER STREAM_CQ_HB_10SEC \nKEEP 1 ROWS;\n\nCREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_1MIN OVER STREAM_CQ_HB_1MIN \nKEEP 1 ROWS;\n\nCREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_30SEC OVER STREAM_CQ_HB_30SEC \nKEEP 1 ROWS;\n\nEND APPLICATION APP_HEARTBEATS;", "generated_queries": "1. How can I create and manage different continuous queries related to heartbeat data with specified intervals in an application named \"APP_HEARTBEATS\"?\n   \n2. What are the steps to stop, undeploy, and drop an application named \"APP_HEARTBEATS\" along with its associated continuous queries and jumping windows in TQL?\n   \n3. Can you explain the process of defining jumping windows over specified continuous queries that capture heartbeat events at different intervals within the context of the application \"APP_HEARTBEATS\" using TQL?", "file_name": "APP_HEARTBEATS.tql"}
{"tql": "stop application AzureApp;\nundeploy application AzureApp;\ndrop application AzureApp cascade;\n\ncreate application AzureApp\nRECOVERY 10 second interval;\ncreate source CSVSource using FileReader (\n\tdirectory:'@DIR@',\n\tWildCard:'@WILDCARD@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  curr String,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       data[6],\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target BlobT using AzureBlobWriter(\n\taccountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:50,interval:5s'\n)\nformat using JSONFormatter (\n)\ninput from TypedCSVStream;\nend application AzureApp;\ndeploy application AzureApp in default;\nstart application AzureApp;", "generated_queries": "1. What is the process for deploying and starting an application named AzureApp that reads data from CSV files, parses it using a DSVParser, and writes the formatted data to an Azure Blob Storage account using an AzureBlobWriter?\n  \n2. How can I configure a continuous query (CQ) named CsvToPosData within the AzureApp application to transform data fields from a CSV file into a custom format before inserting them into a stream called TypedCSVStream?\n\n3. Can I set a specific recovery interval for the AzureApp application and specify upload policies for writing data to an Azure Blob Storage container using the BlobT target with an AzureBlobWriter?", "file_name": "FileWAzurewithJson.tql"}
{"tql": "--\n-- Crash Recovery Test 4 on four node all server cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS\n-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS\n--\n\nSTOP APPLICATION N4S4CR4Tester.N4S4CRTest4;\nUNDEPLOY APPLICATION N4S4CR4Tester.N4S4CRTest4;\nDROP APPLICATION N4S4CR4Tester.N4S4CRTest4 CASCADE;\nCREATE APPLICATION N4S4CRTest4 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN4S4CRTest4;\n\nCREATE SOURCE CsvSourceN4S4CRTest4 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN4S4CRTest4;\n\nCREATE FLOW DataProcessingN4S4CRTest4;\n\nCREATE TYPE CsvDataN4S4CRTest4 (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE TYPE WactionDataN4S4CRTest4 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\n\nCREATE STREAM DataStream OF CsvDataN4S4CRTest4;\n\nCREATE CQ CsvToDataN4S4CRTest4\nINSERT INTO DataStream\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsN4S4CRTest4 CONTEXT OF WactionDataN4S4CRTest4\nEVENT TYPES ( CsvDataN4S4CRTest4 )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO WactionsN4S4CRTest4\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nCREATE CQ Data6ToWaction\nINSERT INTO WactionsN4S4CRTest4\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream6Minutes p;\n\nEND FLOW DataProcessingN4S4CRTest4;\n\nEND APPLICATION N4S4CRTest4;", "generated_queries": "1. What is the process for crash recovery testing on a four-node all-server cluster using Test 4 with Bert Hashemi from WebAction, Inc.?\n   \n2. How can I stop, undeploy, and drop the application N4S4CR4Tester.N4S4CRTest4, and then create a new application N4S4CRTest4 with a recovery interval of 5 seconds for a specific flow named DataAcquisitionN4S4CRTest4?\n\n3. What are the steps involved in converting CSV data into Waction data on a cluster with multiple nodes for crash recovery testing, specifically focusing on the aggregation of data in 5-minute and 6-minute windows?", "file_name": "KStreamN4S4CRTest4.tql"}
{"tql": "create or replace type @STREAM@details(\nC_CUSTKEY int,\nC_MKTSEGMENT String,\nC_NATIONKEY int,\nC_NAME String,\nC_ADDRESS String,\nC_PHONE String,\nC_ACCTBAL int,\nC_COMMENT String\n);\n\ncreate or replace stream @STREAM@_TYPED of @STREAM@details;\n\nCreate or replace CQ @STREAM@detailsCQ\ninsert into @STREAM@_TYPED\nselect \nto_int(data[0]),data[1],to_int(data[2]),data[3],data[4],data[5],to_int(data[6]),data[7]\nfrom @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  --Password_encrypted: false\n ) \nINPUT FROM @STREAM@_TYPED;", "generated_queries": "1. What are the customer details including their account balance and contact information stored in the target database?\n2. How many customers from each market segment are being captured and stored in the specified target table?\n3. Can I retrieve the national key and address of a specific customer from the streaming data source?", "file_name": "DatabaseWriter_Typed.tql"}
{"tql": "stop application @APPNAME@1;\nundeploy application @APPNAME@1;\nstop application @APPNAME@2;\nundeploy application @APPNAME@2;\n\nDROP STREAM @APPNAME@_STREAM;\nDROP APPLICATION @APPNAME@1 CASCADE;\nDROP APPLICATION @APPNAME@2 CASCADE;\n\ndrop propertyset PGtoPGPlatfm_App_KafkaPropset;\ndrop stream  PGToPGPlatfm_Stream CASCADE;\n\n\nCREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\nCREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;\n\t\t\t\t\t\nCREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';\ncreate application @APPNAME@1 recovery 5 SECOND INTERVAL;\n\ncreate or replace stream @STREAM@2 of Global.waevent;\n\nCREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using PostgreSQLReader(\n  ReplicationSlotName: 'slotname',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: '$table1',\n  ExcludedTables:'public.chkpoint'\n ) OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_AGENTFLOW1;\n\nCREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using PostgreSQLReader( \n  ReplicationSlotName: 'slotname',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: '@TABLENAME@2',\n  ExcludedTables:'public.chkpoint'\n ) OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_SERVERFLOW1;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n ) \nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;\nCREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x\n WHERE META(x,'TableName').toString() == 'WACTION.PGToPGPLATFM_SOURCE4';\n\nCREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n ) \nINPUT FROM @STREAM@2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(\n  name: 'CDDL',\n  filename:'WAEventDump.log'\n)INPUT FROM @STREAM@;\n\nCREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;\n\nEND APPLICATION @APPNAME@1;\n\n\n\nCREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';\nCREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;\n\ncreate or replace stream @STREAM@3 of Global.waevent;\n\nCREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using PostgreSQLReader( \n  ReplicationSlotName: 'slotname',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: '$table2',\n  ExcludedTables:'public.chkpoint'\n ) OUTPUT TO @STREAM@;\n\nEND FLOW @APPNAME@_AGENTFLOW2;\n\nCREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using PostgreSQLReader(\n  ReplicationSlotName: 'slotname',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: '@TABLENAME@4',  \n  ExcludedTables:'public.chkpoint'\n ) OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_SERVERFLOW2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n ) \nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;\nCREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y\n WHERE META(y,'TableName').toString() == 'WACTION.PGToPGPLATFM_SOURCE2';\n\nCREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n ) \nINPUT FROM @STREAM@3;\n\nCREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;\n\nEND APPLICATION @APPNAME@2;", "generated_queries": "1. How can I stop and undeploy multiple applications named \"@APPNAME@1\" and \"@APPNAME@2\" in a TQL environment?\n   \n2. What is the process for creating and replacing various property sets, streams, applications, flows, sources, targets, windows, and continuous queries with specific configurations in a TQL query using placeholders like \"@APPNAME@\", \"@STREAM@\", \"@TABLENAME@\", etc.?\n\n3. Can you provide me with the TQL query syntax for setting up a data processing pipeline that involves PostgreSQLReader as a source, DatabaseWriter as a target, and defining jumping windows and continuous queries based on table names in a TQL environment?", "file_name": "PGToPGPlatfm.tql"}
{"tql": "CREATE APPLICATION @APPNAME@ USE EXCEPTIONSTORE TTL : '7d' ;\n\nCREATE SOURCE @APPNAME@_Source USING Global.OracleReader (\n  Username: '@srcusername@',\n  Password: '@srcpassword@',\n  ConnectionURL: '@srcurl@', )\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_Target1 USING Global.SnowflakeWriter (\n  connectionUrl: '@tgturl@',\n    tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',\n    password: '@tgtpassword@',\n    username: '@tgtusername@',\n    appendOnly: 'true',\n    uploadPolicy: 'eventcount:1,interval:5m',\n    externalStageType: 'Local',\n    adapterName: 'SnowflakeWriter' )\nINPUT FROM @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_Target2 USING Global.SnowflakeWriter (\n  connectionUrl: '@tgturl@',\n    tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',\n    password: '@tgtpassword@',\n    username: '@tgtusername@',\n    appendOnly: 'false',\n    optimizedMerge: 'false',\n    uploadPolicy: 'eventcount:1,interval:5m',\n    externalStageType: 'Local',\n    adapterName: 'SnowflakeWriter' )\nINPUT FROM @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_Target3 USING Global.SnowflakeWriter (\n  connectionUrl: '@tgturl@',\n    tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',\n    password: '@tgtpassword@',\n    username: '@tgtusername@',\n    optimizedMerge: 'true',\n    uploadPolicy: 'eventcount:1,interval:5m',\n    externalStageType: 'Local',\n    adapterName: 'SnowflakeWriter' )\nINPUT FROM @APPNAME@_Stream;\n\nEND APPLICATION @APPNAME@;", "generated_queries": "1. What is the configuration setup for creating a data pipeline application named '@APPNAME@' that reads data from an Oracle database source, processes it, and writes to Snowflake targets with different configurations like append-only, optimized merging, and table uploads?\n  \n2. How can I design an application in the data pipeline system that utilizes an Oracle reader as the data source, has multiple Snowflake targets with varying upload policies and merging options while ensuring data persists for only seven days in the exception store TTL?\n\n3. What is the step-by-step process to create a data pipeline application named '@APPNAME@' that involves capturing data from an Oracle database, streaming it through different Snowflake targets with unique configurations, such as append-only, optimized merging, and table uploads, before finally ending the application configuration with a TTL of 7 days for the exception store?", "file_name": "OracleToSnowflakeMultiTarget.tql"}
{"tql": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet\n(\n  ConnectionURL: '@CONNECTION_URL@',\n  CommittedTransactions: true,\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OJet',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  Username: '@SOURCE_USER@'\n)OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)\nSELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]);\n\nCREATE TYPE LogType(\nNum_col String key,\nChar_col String,\nVarchar2_col String,\nlong_col String,\nTable String,\nOperation String\n);\n\nCREATE WINDOW CDCWindow\nOVER @STREAM@\nKEEP 1 ROWS;\n\nCREATE WACTIONSTORE CDCWS CONTEXT of LogType\nEVENT TYPES ( LogType )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ ToWactionStore\nINSERT INTO CDCWS\nSELECT * FROM CDCWindow\nLINK SOURCE EVENT;\n\nCREATE TARGET @SOURCE_NAME@_SYS USING SysOut (\n  name: '@SOURCE_NAME@_SYS' )\nINPUT FROM @STREAM@;", "generated_queries": "1. What are the latest changes made to the specified table in the database, including the table name, operation type, and affected columns?\n\n2. Can you show me a real-time stream of data changes occurring in a specific table, including the column values before and after the operation was executed?\n\n3. How can I store and track committed transactions for a particular database table, including detailed information such as operation type and affected columns, using OJet as the adapter and Elasticsearch as the storage provider?", "file_name": "OjetSMF.tql"}
{"tql": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@;\nCREATE OR REPLACE SOURCE @APP_NAME@_src1 USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream1;\n\nCREATE OR REPLACE CQ @APP_NAME@_CQ1\nINSERT INTO @APP_NAME@_Stream6\nSELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream1 d;\n\nCREATE OR REPLACE SOURCE @APP_NAME@_src2 USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream2;\n\nCREATE OR REPLACE CQ @APP_NAME@_CQ2\nINSERT INTO @APP_NAME@_Stream6\nSELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream2 d;\n\nCREATE OR REPLACE SOURCE @APP_NAME@_src3 USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream3;\n\nCREATE OR REPLACE CQ @APP_NAME@_CQ3\nINSERT INTO @APP_NAME@_Stream6\nSELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream3 d;\n\nCREATE OR REPLACE SOURCE @APP_NAME@_src4 USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream4;\n\nCREATE OR REPLACE CQ @APP_NAME@_CQ4\nINSERT INTO @APP_NAME@_Stream6\nSELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream4 d;\n\nCREATE OR REPLACE SOURCE @APP_NAME@_src5 USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream5;\n\nCREATE OR REPLACE CQ @APP_NAME@_CQ5\nINSERT INTO @APP_NAME@_Stream6\nSELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream5 d;\n\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt1 USING Global.DeltaLakeWriter (\n  personalAccessToken: '',\n  hostname: 'adb-5292730997167687.7.azuredatabricks.net',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.student',\n  adapterName: 'DeltaLakeWriter',\n  personalAccessToken_encrypted: 'false',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:1,interval:10s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @APP_NAME@_Stream6;\n\nEND APPLICATION @APP_NAME@;\nDEPLOY APPLICATION @APP_NAME@;\nSTART APPLICATION @APP_NAME@;", "generated_queries": "1. How can I create a new stream processing application named \"APP_NAME\" that reads data from an Oracle database table and writes the transformed data to a Delta Lake table in Azure Databricks with specified upload policies?\n   \n2. What steps are involved in deploying a stream processing application called \"APP_NAME\" which consists of multiple sources, continuous queries, and a Delta Lake target in Azure Databricks, all connected in a dataflow pipeline?\n\n3. How can I stop, undeploy, drop, create, configure sources, queries, and targets, and then restart a complex stream processing application named \"APP_NAME\" that integrates Oracle database reads and Delta Lake writes in an Azure Databricks environment using TQL commands?", "file_name": "OracleToDeltaLake_MultipleAdapters2.tql"}
{"tql": "CREATE TARGET @TARGET_NAME@ USING RedshiftWriter\n\t(\n\t  ConnectionURL: '@CONNECTION_URL@',\n\t  Username: 'tgt_username',\n\t  Password: 'tgt_pwrd',\n\t  bucketname: 'bucket_name',\n\t  accesskeyId: 'access_key',\n\t  secretaccesskey: 'secret_access',\n\t  Tables: 'tgt_table',\n\t  uploadpolicy:'eventcount:10,interval:1m'\n\t) INPUT FROM @STREAM@;", "generated_queries": "1. How can data from a specific stream be continuously ingested into a Redshift database table with a target name, bucket name, access key, and secret key defined?\n   \n2. Can you provide guidance on configuring a RedshiftWriter in TreasureData to upload data from a designated stream to a Redshift database table using specific upload policies such as event count and interval?\n\n3. What are the necessary steps to set up a data pipeline in TreasureData that utilizes a RedshiftWriter to connect to a Redshift database, ensuring continuous ingestion of data based on specified upload policies like event count and interval?", "file_name": "RedshiftWriter.tql"}
{"tql": "--\n-- Canon Test W50\n-- Nicholas Keene, WebAction, Inc.\n--\n-- Basic test for a partitioned sliding count window\n--\n-- S -> SWc5p -> CQ -> WS\n--\n\n\nUNDEPLOY APPLICATION NameW50.W50;\nDROP APPLICATION NameW50.W50 CASCADE;\nCREATE APPLICATION W50 RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE FLOW DataAcquisitionW50;\n\nCREATE SOURCE CsvSourceW50 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'Canon1000.csv',\n  columndelimiter:',',\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStreamW50;\n\nEND FLOW DataAcquisitionW50;\n\n\nCREATE FLOW DataProcessingW50;\n\nCREATE TYPE DataTypeW50 (\n    eventID String,\n    word String KEY,\n    dateTime DateTime\n);\n\nCREATE STREAM DataStreamW50 OF DataTypeW50;\n\nCREATE CQ CSVStreamW50_to_DataStreamW50\nINSERT INTO DataStreamW50\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[2],'yyyyMMddHHmmss')\nFROM CsvStreamW50;\n\nCREATE WINDOW SWc5pW50\nOVER DataStreamW50\nKEEP 5 ROWS\nPARTITION BY word;\n\nCREATE WACTIONSTORE WactionStoreW50 CONTEXT OF DataTypeW50\nEVENT TYPES ( DataTypeW50 KEY(word) )\n@PERSIST-TYPE@\n\nCREATE CQ SWc5pW50_to_WactionStoreW50\nINSERT INTO WactionStoreW50\nSELECT\n    FIRST(eventID),\n    FIRST(word),\n    FIRST(dateTime)\nFROM SWc5pW50\nGROUP BY word;\n\nEND FLOW DataProcessingW50;\n\n\n\nEND APPLICATION W50;", "generated_queries": "1. How can I create a data flow process in my application that involves acquiring data from a CSV file, processing it, and storing it in a partitioned sliding count window along with specific event details?\n2. Can you show me the TQL query for creating a window that keeps only the last 5 rows of data in a stream, partitioned by a specific word identifier?\n3. What TQL commands should I use to deploy, drop, and recover an application named \"W50\" that includes data acquisition, processing, and storage components for handling CSV data streams?", "file_name": "W50.tql"}
{"tql": "STOP APPLICATION @AppName@_App1;\nUNDEPLOY APPLICATION @AppName@_App1;\nDROP APPLICATION @AppName@_App1 CASCADE;\nCREATE APPLICATION @AppName@_App1 recovery 1 second interval;\n\n\nCREATE SOURCE @AppName@_FileSource USING FileReader (\n  directory:'@TestdataDir@',\n    WildCard:'banks*',\n  positionByEOF:false\n  )\nPARSE USING DSVParser (\n  header:no\n)OUTPUT TO FileStream;\n\n\nCREATE OR REPLACE ROUTER filerouter1 INPUT FROM FileStream s CASE\nWHEN meta(s,\"FileName\").toString()='banks1.csv' THEN ROUTE TO stream1,\nWHEN meta(s,\"FileName\").toString()='banks2.csv' THEN ROUTE TO stream2,\nWHEN meta(s,\"FileName\").toString()='banks3.csv' THEN ROUTE TO stream3,\nWHEN meta(s,\"FileName\").toString()='banks4.csv' THEN ROUTE TO stream4,\nELSE ROUTE TO ss_else;\n\nCREATE TYPE banks1(\n  id int,\n  name String ,\nFilename String\n);\n\nCreate stream cdctypestream1 of banks1;\n\nCREATE CQ cdcstreamcq1\nINSERT INTO cdctypestream1\nSELECT TO_INT(p.data[0]), \n       TO_STRING(p.data[1]), TO_STRING(META(p,'FileName'))\nFROM stream1 p;\n\n\nCREATE OR REPLACE TARGET @AppName@_DataBaset1 USING DatabaseWriter  ( \nConnectionURL:'@url@',\nUsername:'@userName@',\nPassword:'@password@',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.NEWBANKS1') \nINPUT FROM cdctypestream1;\n\nEnd application @AppName@_App1;\nDeploy application @AppName@_App1;\nstart application @AppName@_App1;\n\nstop application @AppName@_App2;\nundeploy application @AppName@_App2;\ndrop application @AppName@_App2 CASCADE;\ncreate application @AppName@_App2 recovery 1 second interval;\n\nCREATE OR REPLACE TARGET @AppName@_DataBaset2 USING DatabaseWriter  ( \nConnectionURL:'@url@',\nUsername:'@userName@',\nPassword:'@password@',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.BANKS1') \nINPUT FROM cdctypestream1;\n\nend application @AppName@_App2;\ndeploy application @AppName@_App2;\nstart application @AppName@_App2;", "generated_queries": "1. What are the steps involved in creating, deploying, and starting applications @AppName@_App1 and @AppName@_App2, including the definition of sources, routers, types, streams, CQs, and database targets for each application?\n  \n2. Which databases are being written to by the applications @AppName@_App1 and @AppName@_App2, and what data transformation and routing logic is applied to the input streams before writing to these databases?\n\n3. How can I manage the lifecycle of applications @AppName@_App1 and @AppName@_App2, including stopping, undeploying, dropping, creating, deploying, and starting them, as well as defining database connections and tables for writing data from input streams?", "file_name": "LateCheckpoint.tql"}
{"tql": "--\n-- Crash Recovery Test 4 on two node cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS\n-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS\n--\n\nSTOP APPLICATION N2S2CR4Tester.N2S2CRTest4;\nUNDEPLOY APPLICATION N2S2CR4Tester.N2S2CRTest4;\nDROP APPLICATION N2S2CR4Tester.N2S2CRTest4 CASCADE;\nCREATE APPLICATION N2S2CRTest4 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN2S2CRTest4;\n\nCREATE SOURCE CsvSourceN2S2CRTest4 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN2S2CRTest4;\n\nCREATE FLOW DataProcessingN2S2CRTest4;\n\nCREATE TYPE CsvDataN2S2CRTest4 (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE TYPE WactionTypeN2S2CRTest4 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\n\nCREATE STREAM DataStream OF CsvDataN2S2CRTest4;\n\nCREATE CQ CsvToDataN2S2CRTest4\nINSERT INTO DataStream\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsN2S2CRTest4 CONTEXT OF WactionTypeN2S2CRTest4\nEVENT TYPES ( CsvDataN2S2CRTest4 )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO WactionsN2S2CRTest4\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nCREATE CQ Data6ToWaction\nINSERT INTO WactionsN2S2CRTest4\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream6Minutes p;\n\nEND FLOW DataProcessingN2S2CRTest4;\n\nEND APPLICATION N2S2CRTest4;", "generated_queries": "1. How can I stop, undeploy, and drop a specific application named N2S2CR4Tester.N2S2CRTest4 on a two-node cluster in a crash recovery scenario?\n   \n2. How do I create a data acquisition flow named DataAcquisitionN2S2CRTest4 that reads CSV data from a specified directory, processes it based on certain configurations, and outputs it to a stream named CsvStream?\n\n3. Can you show me how to design a data processing flow named DataProcessingN2S2CRTest4 that involves creating jumping windows over a stream of CSV data, aggregating data within specific time intervals, and storing the processed data in a WactionStore named WactionsN2S2CRTest4 for further analysis and reporting purposes?", "file_name": "N2S2CRTest4.tql"}
{"tql": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@;\n\n\nCREATE OR REPLACE SOURCE @APPNAME@CDC_Source1 USING MySQLReader( \n  Compression: true,\n  FetchSize: 1,\n  SendBeforeImage: true,\n  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',\n  Tables: 'waction.crash_type',\n  Password: 'w@ct10n',\n  Password_encrypted: 'false',\n  Username: 'root'\n ) \nOUTPUT TO @APPNAME@AppStream1;\n\n\nCREATE OR REPLACE TARGET @APPNAME@sap_target USING DatabaseWriter( \n  DatabaseProviderType:'SAPHANA',\n  ConnectionRetryPolicy: 'retryInterval=30,maxRetries=3',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'SYSTEM',\n  Password_encrypted: 'false',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:sap://10.77.21.116:39013/?databaseName=striim&currentSchema=QA',\n  Tables: 'waction.crash_type,QA.CRASH_TYPES',\n  adapterName: 'DatabaseWriter',\n  --IgnorableExceptionCode: '',\n  Password: 'Striim_SAP@123'\n ) \nINPUT FROM @APPNAME@AppStream1;\n\n\ncreate or replace target @APPNAME@sys_tgt using sysout(\nname:Foo2\n)input from @APPNAME@AppStream1;\n\nEND APPLICATION @APPNAME@;\n\ndeploy application @APPNAME@;\nstart application @APPNAME@;", "generated_queries": "1. What are the steps to create and deploy a real-time application in a streaming data processing platform that reads data from a MySQL database table \"crash_type\" and writes to tables in both a SAP HANA database and a sysout target named \"Foo2\"?\n\n2. How can I set up a data pipeline within a streaming platform to extract incremental changes from a MySQL database table \"crash_type\" to feed into a SAP HANA database and a sysout target concurrently, with specific configurations such as compression, fetch size, and connection details?\n\n3. Can you provide a script example for deploying a streaming application that involves reading data from a MySQL source, performing CDC on the \"crash_type\" table, and then writing the processed data to both a SAP HANA database and a sysout target named \"Foo2\"?", "file_name": "MysqlToHanaDB_DatabaseWriter.tql"}
{"tql": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet\n(\n  ConnectionURL: '@CONNECTION_URL@',\n  CommittedTransactions: true,\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OJet',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  Username: '@SOURCE_USER@'\n)OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)\nSELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[0]) = '1' ;\n\nCREATE TYPE LogType(\nNum_col String key,\nChar_col String,\nVarchar2_col String,\nlong_col String,\nTable String,\nOperation String\n);\n\nCREATE WINDOW CDCWindow\nOVER @STREAM@\nKEEP 1 ROWS;\n\nCREATE WACTIONSTORE CDCWS CONTEXT of LogType\nEVENT TYPES ( LogType )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ ToWactionStore\nINSERT INTO CDCWS\nSELECT * FROM CDCWindow\nLINK SOURCE EVENT;\n\nCREATE TARGET @SOURCE_NAME@_SYS USING SysOut (\n  name: '@SOURCE_NAME@_SYS' )\nINPUT FROM @STREAM@;", "generated_queries": "1. What are the changes made to table '@SOURCE_TABLE@' where the value in the first column is '1'?\n2. How many rows have been committed in the system that involve table '@SOURCE_TABLE@' and meet the condition of having '1' in the first column?\n3. Can you provide a log of all operations performed on table '@SOURCE_TABLE@' where the value in the first column is '1' within the specified time frame?", "file_name": "OjetSMF_SimpleEqual.tql"}
{"tql": "STOP UpdatableCacher.UpdatableCache;\nUNDEPLOY APPLICATION UpdatableCacher.UpdatableCache;\nDROP APPLICATION UpdatableCacher.UpdatableCache CASCADE;\nCREATE APPLICATION UpdatableCacher.UpdatableCache;\n\nCREATE TYPE MerchantHourlyAve(\n  merchantId String KEY,\n  hourlyAve Integer,\n  theDate DateTime,\n  dVal Double\n);\n\n\nCREATE source CsvDataSource USING FileReader (\n      directory:'@TEST-DATA-PATH@',\n      columndelimiter: ',',\n      wildcard:'ucData.csv',\n      blocksize: 10240,\n      positionByEOF:false\n)\nPARSE USING DSVParser (\n      header:No,\n      trimquote:false\n) OUTPUT TO CsvStream;\n\n\nCREATE STREAM S1 OF MerchantHourlyAve;\n\nCREATE CQ cq1\n\tinsert into S1\n\t\tSELECT data[0],\n\t\t\t\tTO_INT(data[1]),\n\t\t\t\tTO_DATE(data[2]),\n\t\t\t\tTO_DOUBLE(data[3])\n\t\tFROM CsvStream;\n\n\nCREATE EVENTTABLE ET1 using STREAM (\n  NAME: 'S1'\n) QUERY (keytomap:'dVal', persistPolicy: 'true' ) OF MerchantHourlyAve;\n\n\nCREATE EVENTTABLE ET2 using STREAM (\n  NAME: 'S1'\n) QUERY (keytomap:'merchantId' ) OF MerchantHourlyAve;\n\n\n\nEND APPLICATION UpdatableCache;", "generated_queries": "1. How can I set up a real-time data processing pipeline to ingest and transform data from a CSV file into a stream of hourly average values for different merchants?\n  \n2. What is the process to create event tables that query the streaming data for specific metrics, such as the average hourly value or merchant ID, using the TQL language?\n\n3. Can you explain the steps required to deploy an application called \"UpdatableCacher.UpdatableCache\" which involves creating a stream of MerchantHourlyAve data type, parsing CSV data, and setting up continuous queries to populate the stream?", "file_name": "UpdatableCache.tql"}
{"tql": "stop ORAToBigquery;\nundeploy application ORAToBigquery;\ndrop application ORAToBigquery cascade;\nCREATE APPLICATION ORAToBigquery RECOVERY 5 SECOND INTERVAL;\nCREATE OR REPLACE SOURCE Rac11g USING OracleReader ( \n  SupportPDB: false,\n  SendBeforeImage: true,\n  ReaderType: 'LogMiner',\n  CommittedTransactions: false,\n  FetchSize: 1,\n  Password: 'manager',\n  DDLTracking: false,\n  StartTimestamp: 'null',\n  OutboundServerProcessName: 'WebActionXStream',\n  OnlineCatalog: true,\n  ConnectionURL: '192.168.33.10:1521/XE',\n  SkipOpenTransactions: false,\n  Compression: false,\n  QueueSize: 40000,\n  RedoLogfiles: 'null',\n  Tables: 'SYSTEM.GGAUTHORIZATIONS',\n  Username: 'system',\n  FilterTransactionBoundaries: true,\n  adapterName: 'OracleReader',\n  XstreamTimeOut: 600,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=60, maxRetries=3'\n ) \nOUTPUT TO DataStream;\nCREATE OR REPLACE TARGET Target1 USING SysOut ( \n  name: \"dstream\"\n ) \nINPUT FROM DataStream;\nCREATE OR REPLACE TARGET Target2 USING BigqueryWriter  ( \n  BQServiceAccountConfigurationPath: '/Users/ravipathak/Downloads/abc.json',\n  projectId: 'big-querytest',\n  Tables: 'SYSTEM.GGAUTHORIZATIONS,testing1.ggauthorisation',\n  parallelismCount: 2,\n  BatchPolicy: 'eventCount:100000,Interval:0'\n ) \nINPUT FROM DataStream;\nEND APPLICATION ORAToBigquery;\ndeploy application ORAToBigquery;\nstart ORAToBigquery;", "generated_queries": "1. How do I transfer data from an Oracle database to BigQuery with real-time synchronization while ensuring high availability and resilience in case of failures?\n2. What configuration settings are used to specify the source, target, and data processing parameters for transferring data from an Oracle source to BigQuery using a data streaming application named ORAToBigquery?\n3. Can you provide the detailed steps involved in creating and deploying a data streaming application named ORAToBigquery to continuously replicate data from an Oracle database to a BigQuery destination, including the setup of source and target connections and the associated recovery and deployment processes?", "file_name": "tartemp.tql"}
{"tql": "use PosTester;\nalter application PosApp;\n\nCREATE source CsvDataSource USING CSVReader (\n  directory:'Samples/Customer/PosApp/appData',\n  header:Yes,\n  wildcard:'posdata.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nend application PosApp;\n\nalter application PosApp recompile;", "generated_queries": "1. How can I set up a CSV data source in the PosApp application to import customer POS data located in the \"Samples/Customer/PosApp/appData\" directory from a file named \"posdata.csv\" with comma as the column delimiter?\n2. What is the TQL query syntax to configure a CSVReader source in the PosApp application with specific parameters like block size, header presence, position by EOF, and quote trimming settings for handling POS data?\n3. How do I recompile the PosApp application after making changes to the CSVDataSource configuration using TQL commands in the PosTester environment?", "file_name": "createSource.tql"}
{"tql": "UNDEPLOY APPLICATION FileToKWriterUpgrade;\n\nDROP APPLICATION FileToKWriterUpgrade cascade;\n\nCREATE APPLICATION FileToKWriterUpgrade;\n\nCREATE TYPE Type1\n(\n City String,\n Col2 String,\n Col3 String,\n Col4 String\n);\n\n-- Create a stream of type Type1\n\nCREATE STREAM TypedStream OF Type1;\n\n-- Create a source using FileReader\n\nCREATE SOURCE KafkaCSVSource USING FILEREADER\n(\n directory:'@DIRECTORY@',\n WildCard:'city*.dsv',\n positionByEOF:false,\n charset:'UTF-8'\n)\nPARSE USING DSVPARSER\n(\n columndelimiter:',',\n ignoreemptycolumn:'Yes'\n)\nOUTPUT TO FileStream;\n\n-- Read from raw stream to typed stream using CQ\n\nCREATE CQ RawStreamCQ\nINSERT INTO TypedStream\nSELECT \n data[0],\n data[1],\n data[2],\n data[3] \nFROM FileStream;\n\n-- Load the KafkaWriter from TypedStream\n\nCREATE TARGET KWriter USING KAFKAWRITER VERSION '0.9.0'\n(\n brokerAddress:'localhost:9092',\n Topic:'@TOPIC@',\n KafkaMessageFormatVersion:v2\n)\nFORMAT USING DSVFORMATTER()\nINPUT FROM TypedStream;\n\n\nCREATE SOURCE KReader USING KAFKAREADER VERSION '0.9.0'\n(\n brokerAddress:'localhost:9092',\n Topic:'@TOPIC@',\n charset : 'UTF-8',\n KafkaConfig:'retry.backoff.ms=5000',\n startOffset:0\n)\nPARSE USING DSVParser (\n)\n\nOUTPUT TO KafkaReaderStream;\n\nCREATE TARGET LogKafkaReaderStream USING LOGWRITER\n(\n name:KafkaLOuput,\n filename:'@LOGFILENAME@',\n flushpolicy : 'flushcount:1',\n rolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nINPUT FROM KafkaReaderStream;\n\n\nEND APPLICATION FileToKWriterUpgrade;\n\nDEPLOY APPLICATION FileToKWriterUpgrade;\n\nSTART APPLICATION FileToKWriterUpgrade;", "generated_queries": "1. How can I deploy an application called FileToKWriterUpgrade using TQL and set up Kafka integration for data processing?\n2. What is the TQL query syntax to create a stream of a specific data type (Type1) in the FileToKWriterUpgrade application?\n3. How do I configure a Kafka source and target within the FileToKWriterUpgrade application using TQL for seamless data transfer?", "file_name": "FileReaderToKafkaWriterSelfUpgrade.tql"}
{"tql": "--\n-- Recovery Test 25 with two sources, two jumping count windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jc5W -> CQ1 -> WS\n--   S2 -> Jc6W -> CQ2 -> WS\n--\n\nSTOP KStreamRecov25Tester.KStreamRecovTest25;\nUNDEPLOY APPLICATION KStreamRecov25Tester.KStreamRecovTest25;\nDROP APPLICATION KStreamRecov25Tester.KStreamRecovTest25 CASCADE;\nDROP USER KStreamRecov25Tester;\nDROP NAMESPACE KStreamRecov25Tester CASCADE;\nCREATE USER KStreamRecov25Tester IDENTIFIED BY KStreamRecov25Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov25Tester;\nCONNECT KStreamRecov25Tester KStreamRecov25Tester;\n\nCREATE APPLICATION KStreamRecovTest25 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP 5 ROWS;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP 6 ROWS;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream6Minutes p;\n\nEND APPLICATION KStreamRecovTest25;", "generated_queries": "1. What are the companies and their corresponding total transaction amounts in the last 5 minutes captured by the jumping window on the first data stream from the provided CSV files?\n\n2. How many rows of data are retained in the jumping window over the second data stream from the CSV files for a duration of 6 minutes?\n\n3. Which city had the highest number of transactions among all companies in the last 5 minutes as extracted from the CSV data in the first stream, as processed by the defined continuous query (CQ) in the TQL query?", "file_name": "KStreamRecovTest25.tql"}
{"tql": "--\n-- Recovery Test 31 with two sources, two sliding count windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Sc5W/p -> CQ1 -> WS\n-- S2 -> Sc6W/p -> CQ2 -> WS\n--\n\nSTOP KStreamRecov31Tester.KStreamRecovTest31;\nUNDEPLOY APPLICATION KStreamRecov31Tester.KStreamRecovTest31;\nDROP APPLICATION KStreamRecov31Tester.KStreamRecovTest31 CASCADE;\n\nDROP USER KStreamRecov31Tester;\nDROP NAMESPACE KStreamRecov31Tester CASCADE;\nCREATE USER KStreamRecov31Tester IDENTIFIED BY KStreamRecov31Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov31Tester;\nCONNECT KStreamRecov31Tester KStreamRecov31Tester;\n\nCREATE APPLICATION KStreamRecovTest31 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP 5 ROWS\nPARTITION BY merchantId;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP 6 ROWS\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1\nGROUP BY merchantId;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2\nGROUP BY merchantId;\n\nEND APPLICATION KStreamRecovTest31;", "generated_queries": "1. How can I set up a test scenario with two sources, two sliding count windows, and one Wactionstore, all partitioned on the same key, using Kafka streams and CSV data?\n2. What are the configuration details for setting up a recovery test with a 5-second interval, using two CSV sources reading data from files in a specified directory and persisting them in Kafka streams?\n3. How can I create sliding windows for processing data from two different streams based on a common key, and then aggregate the data into a Wactionstore using a TQL query in a specified application context?", "file_name": "KStreamRecovTest31.tql"}
{"tql": "STOP @Appname@;\nUNDEPLOY APPLICATION @Appname@;\nDROP APPLICATION @Appname@ CASCADE;\n\nCREATE APPLICATION @Appname@ @Recovery@ use exceptionstore;\n\nCREATE SOURCE @Appname@_S USING OracleReader\n(\n\tUsername: 'qatest',\n\tPassword: 'qatest',\n\tConnectionURL: 'dockerhost:1521:orcl',\n\tTables: 'QATEST.TABLE_TEST_%',\n\tFetchSize: '1'\n)\nOUTPUT TO @Appname@_SS;\n\n\nCREATE or replace TARGET @Appname@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.TABLE_TEST_%,@DATASET@.%',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1000,Interval:10',\nStandardSQL:true\t\n) INPUT FROM @Appname@_ss;\n\nEND APPLICATION @Appname@;\nDEPLOY APPLICATION @Appname@;\nSTART APPLICATION @Appname@;", "generated_queries": "1. How can I stop, undeploy, drop, create, and deploy an application named \"Appname\" using an exception store for recovery with OracleReader as the source and BigQueryWriter as the target, connecting to a database at 'dockerhost:1521:orcl' and writing to BigQuery tables named 'QATEST.TABLE_TEST_%' and '@DATASET@.%', with specific data processing parameters and retry policies?\n\n2. What is the process to set up an application named \"Appname\" that reads data from Oracle tables matching the pattern 'QATEST.TABLE_TEST_%' and writes to both BigQuery tables and tables in a specific dataset, using a service account key for authentication and defining transformation and loading policies?\n\n3. Can you explain how to deploy and start an application named \"Appname\" that fetches data from Oracle tables, writes it to BigQuery tables in the US data location, and follows certain data manipulation rules and performance optimization strategies?", "file_name": "bq.tql"}
{"tql": "stop application AzureApp;\nundeploy application AzureApp;\ndrop application AzureApp cascade;\n\ncreate application AzureApp\nRECOVERY 10 second interval;\ncreate source CS using FileReader (\n\tdirectory:'@DIR@',\n\tWildCard:'@WILDCARD@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target T using AzureBlobWriter(\n\taccountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:50'\n)\nformat using DSVFormatter (\n)\ninput from TypedCSVStream;\nend application AzureApp;\ndeploy application AzureApp in default;\nstart application AzureApp;", "generated_queries": "1. How can I create an application in TIBCO StreamBase named AzureApp that reads CSV files from a specified directory, parses the data using DSVParser, and outputs the processed data to a CSV stream?\n\n2. What data transformation operations are performed by the Continuous Query named CsvToPosData within the TIBCO StreamBase application AzureApp? \n\n3. How can I configure TIBCO StreamBase to upload processed data from a CSV stream to a specific Azure Blob Storage account using AzureBlobWriter with the specified upload policy?", "file_name": "FileWAzure3.tql"}
{"tql": "create application HPTippingLog;\ncreate source DHCPLogSource using FileReader (\n\tdirectory:'@TEST-DATA-PATH@',\n\tWildCard:'hp*',\n\tcharset:'UTF-8',\n\tpositionByEOF:false \n) PARSE USING HPTippingPointLogParser (\n) OUTPUT TO HPLogStream;\ncreate Target HPDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/hp_log') input from HPLogStream;\nend application HPTippingLog;", "generated_queries": "1. What application was created to process and analyze HP Tipping Point logs?\n   \n2. How can I set up a source to read HP Tipping Point logs from a specific directory using a FileReader with certain configurations?\n   \n3. Which target is being used to write the processed HP Tipping Point logs into a log file named 'hp_log' in a designated feature directory?", "file_name": "HPTippingPointLogReader.tql"}
{"tql": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@ @RECOVERY@;\ncreate source @SOURCE@ using FileReader (\n        directory:'./Samples/AppData/',\n        WildCard:'dynamicdirectory.csv',\n        positionByEOF:false,\n        charset:'UTF-8'\n)\nparse using DSVParser (\n        header:'no'\n)\nOUTPUT TO @STREAM@;\n\ncreate Target @TARGET@ using ADLSGen2Writer(\n    accountname:'',\n\tsastoken:'',\n\tfilesystemname:'',\n\tfilename:'',\n\tdirectory:'',\n\tuploadpolicy:'eventcount:5000'\n)format using DSVFormatter (\n members: 'data'\n)\ninput from @STREAM@;\n\nend application @APPNAME@;\n\ndeploy application @APPNAME@;\nstart application @APPNAME@;", "generated_queries": "1. How can I stop, undeploy, and drop a specific application in my TQL environment?\n2. What steps are involved in creating a new application with data ingestion from a file reader and output to a specified target storage location in TQL?\n3. Can you show me the TQL commands needed to deploy and start a specific application after configuring its input and output sources?", "file_name": "AzureDLS_Gen2Rec.tql"}
{"tql": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@ recovery 5 SECOND Interval;\nCREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream;\n\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.DeltaLakeWriter (\n  personalAccessToken: '',\n  hostname: 'adb-5292730997167687.7.azuredatabricks.net',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.student',\n  adapterName: 'DeltaLakeWriter',\n  personalAccessToken_encrypted: 'false',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:1,interval:30s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @APP_NAME@_Stream;\n\nEND APPLICATION @APP_NAME@;\nDEPLOY APPLICATION @APP_NAME@;\nSTART APPLICATION @APP_NAME@;", "generated_queries": "1. How can I deploy and start an application named @APP_NAME@ which reads data from an Oracle database table and writes it to a Delta Lake table using Azure Databricks?\n  \n2. What steps are involved in creating an application with a recovery interval of 5 seconds, using an Oracle reader to ingest data from the QATEST.BQ1 table, and writing the data to a Delta Lake table on Azure Databricks with a merge mode?\n\n3. How can I stop, undeploy, drop, and then recreate an application named @APP_NAME@ which processes streaming data and uses a Delta Lake writer to write the data to a specified table in the QATEST database?", "file_name": "OracleToDeltaLake_Recovery.tql"}
{"tql": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCREATE SOURCE @SourceName@ USING MySqlReader  ( \nTransactionSupport: false, \n  FetchTransactionMetadata: false, \n  Compression: false, \n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', \n  Password_encrypted: 'false', \n  ConnectionURL: '@ConnectionURL@', \n  Fetchsize: 0, \n  ConnectionPoolSize: 10, \n  Username: '@UN@', \n  cdcRoleName: 'STRIIM_READER', \n  Password: '@PWD@', \n  Tables: 'qatest.%', \n  FilterTransactionBoundaries: true, \n  SendBeforeImage: true, \n  AutoDisableTableCDC: false ) \nOUTPUT TO @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\n\nCREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'), 'OpTime',META(x, 'TimeStamp'))) FROM @SRCINPUTSTREAM@ x; ;\n\nCREATE TARGET @targetName@ USING DatabaseWriter ( \nConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', \n  ParallelThreads: '', \n  CheckPointTable: 'CHKPOINT', \n  Password_encrypted: 'false', \n  ConnectionURL: '@ConnectionURL@', \n  CommitPolicy: 'EventCount:1,Interval:60', \n  StatementCacheSize: '50', \n  DatabaseProviderType: 'Default', \n  Username: '@UN@', \n  Password: '@PWD@', \n  PreserveSourceTransactionBoundary: 'false', \n  BatchPolicy: 'EventCount:1,Interval:60', \n  Tables: 'qatest.%, dbo.% columnmap(opt_type=@USERDATA(OpType),opt_time=@USERDATA(OpTime));' ) \nINPUT FROM admin.sqlreader_cq_out;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;", "generated_queries": "1. How can I deploy and start an application named \"@APPNAME@\", which includes reading data from a MySQL source with specific connection details and writing the transformed data to a database table using a custom change data capture logic?\n   \n2. What are the steps involved in creating and configuring an application that reads data from a MySQL source, enriches the data using custom logic, and writes it to a database target with specific connection parameters and data handling policies?\n\n3. Can you provide a detailed guide on setting up a data pipeline that streams real-time data from a MySQL database, processes it to include additional metadata, and loads the transformed data into a separate database table while ensuring fault tolerance and transaction consistency?", "file_name": "colMapDBWwithWildCard_MySQL.tql"}
{"tql": "use PosTester;\nDROP WACTIONSTORE MerchantActivity;", "generated_queries": "1. Which workspace is being used to test the Point of Sale system?\n2. What action is being performed on the Merchant Activity waction store?\n3. Why is the Merchant Activity waction store being dropped in the Point of Sale testing environment?", "file_name": "dropWactionStore.tql"}
{"tql": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n\nCREATE SOURCE @APPNAME@_S USING MSSqlReader\n(\n  Compression: false,\n  cdcRoleName: 'STRIIM_READER',\n  DatabaseName: 'QATEST',\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ConnectionPoolSize: 1,\n  FetchTransactionMetadata: false,\n  StartPosition: 'EOF',\n  Username: 'qatest',\n  SendBeforeImage: true,\n  AutoDisableTableCDC: true,\n  ConnectionURL: 'localhost:1433',\n  Tables: 'qatest.test01',\n  adapterName: 'MSSqlReader',\n  Password: 'w3b@ct10n'\n)\nOUTPUT TO @APPNAME@_SS;\n\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(id,name) COLUMNMAP(id=id,col1=col11,col2=col12)',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:10',\nStandardSQL:true\t\n) INPUT FROM @APPNAME@_ss;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\nSTART APPLICATION @APPNAME@;", "generated_queries": "1. How can I stop, undeploy, drop, create, and deploy a data application named @APPNAME@ in my system?\n   \n2. What are the source and target configurations for reading data from MSSQL server and writing it to BigQuery in a data application named @APPNAME@?\n   \n3. What steps are involved in starting and ending the execution of a data application named @APPNAME@ that reads from a MSSQL table and writes to a BigQuery dataset?", "file_name": "mssql_bq.tql"}
{"tql": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING DatabaseReader  (\n  Username: '@READER-UNAME@',\n  Password: '@READER-PASSWORD@',\n  ConnectionURL: '@CDC-READER-URL@',\n  Tables: @SOURCE_TABLE@,\n  sendBeforeImage:'true',\n  FilterTransactionBoundaries: 'true'\n )\nOUTPUT TO @STREAM@;\n\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@2 USING DatabaseReader  (\n  Username: '@READER-UNAME@',\n  Password: '@READER-PASSWORD@',\n  ConnectionURL: '@CDC-READER-URL@',\n  Tables: @SOURCE_TABLE@,\n  sendBeforeImage:'true',\n  FilterTransactionBoundaries: 'true'\n )\nOUTPUT TO @STREAM@;", "generated_queries": "1. What are the details of the data source connections configured for @SOURCE_NAME@ and @SOURCE_NAME@2 in the current database environment?\n2. Which tables are being tracked for changes by the DatabaseReader sources @SOURCE_NAME@ and @SOURCE_NAME@2, and are these sources set to capture before images of the data?\n3. How are the transaction boundaries being filtered for the DatabaseReader sources @SOURCE_NAME@ and @SOURCE_NAME@2 in the data streaming setup?", "file_name": "DBReader_Multi.tql"}
{"tql": "CREATE OR REPLACE PROPERTYVARIABLE RetryPolicy='timeOut=00,retryInterval=1,maxRetries=3';\nCREATE OR REPLACE PROPERTYVARIABLE KafkaBrokerAddress='localhost:9099';\nCREATE OR REPLACE PROPERTYVARIABLE KafkaConfig='request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;';\n\nSTOP APPLICATION @Appname@;\nUNDEPLOY APPLICATION @Appname@;\nDROP APPLICATION @Appname@ CASCADE;\nCREATE APPLICATION @Appname@ @Recovery@;\nCREATE FLOW @Appname@AgentFlow;\n--Partitioning source stream with meta (tablename)\nCreate Source @Appname@s1 Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL: '@Connectionurl@',\n Tables:'qatest.oracle_PartitionStream_test%',\n FetchSize:1\n)\nOutput To @Appname@ss1 partition by meta(@Appname@ss1,'TableName');\n-- (id integer,name1 string,name2 string) partition by sleft(name1,1)\n-- select TO_INT(data[0]),TO_STRING(data[1]),TO_STRING(data[2]);\n\n\n--Partitioning source stream using meta (STARTSCN)\nCreate Source @Appname@s2 Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL: '@Connectionurl@',\nTables:'qatest.oracle_PartitionStream_test%',\n FetchSize:1\n)\nOutput To @Appname@ss2 partition by meta(@Appname@ss2,'STARTSCN');\n\n\n\nCREATE TYPE @Appname@OpTableDataType(\n  TableName String,\n  STARTSCN String,\n  data java.util.HashMap\n);\n\n--Partitioning typed stream inline while creation using expr\nCREATE STREAM @Appname@OracleTypedStream OF @Appname@OpTableDataType partition by sright(STARTSCN,2);\nCreate Source @Appname@s3 Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL: '@Connectionurl@',\nTables:'qatest.oracle_PartitionStream_test%',\n FetchSize:1\n)\nOutput To @Appname@ss3 partition by meta(@Appname@ss3,'STARTSCN');\n\nCREATE CQ @Appname@ParseOracleRawStream\n  INSERT INTO @Appname@OracleTypedStream\n  SELECT META(@Appname@ss3, 'TableName').toString(),META(@Appname@ss3, 'STARTSCN').toString(),\n    DATA(@Appname@ss3)\n  FROM @Appname@ss3;\n\n--Partitioning source stream with types defined inline using meta expr\nCreate Source @Appname@s4 Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL: '@Connectionurl@',\nTables:'qatest.oracle_PartitionStream_test%',\n FetchSize:1\n)\nOutput To @Appname@ss4 (id integer,NAME string,COL1 string)\n-- partition by meta(ss4,'STARTSCN')\nselect TO_INT(data[0]),TO_STRING(data[2]),TO_STRING(data[3]);\n\nCREATE CQ @Appname@cqss4\n  INSERT INTO @Appname@ss4new Partition by sright(id,1)\n  SELECT * FROM @Appname@ss4;\n\n--Partitioning source stream -(waevent-s5) with field that has null value\n\nCREATE STREAM @Appname@s5 of Global.WAEvent PARTITION BY meta(@Appname@s5, 'TransactionName');\n\nCreate Source @Appname@src5 Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL: '@Connectionurl@',\nTables:'qatest.oracle_PartitionStream_test%',\n FetchSize:1\n)\nOutput To @Appname@ss5;\n\nEND FLOW @Appname@AgentFlow;\n\n\nCREATE FLOW @Appname@ServerFlow;\n--CREATE STREAM modifyStream OF Global.WAEvent partition by meta(modifyStream,'STARTSCN');\nCREATE STREAM @Appname@modifyStream OF Global.WAEvent partition by sleft(data[0],1);\nCREATE CQ @Appname@modifycq INSERT INTO @Appname@modifyStream\nSELECT * FROM @Appname@ss5\nMODIFY\n(\ndata[0] = (TO_INT(data[0]) * 10000) / 100\n);\n\n\n--Partitioning source stream -(waevent) with modified fields\nCreate Source @Appname@s6 Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL: '@Connectionurl@',\nTables:'qatest.oracle_PartitionStream_test%',\n FetchSize:1\n)\nOutput To @Appname@ss6;\n\nCREATE CQ @Appname@putUserDatacq1\nINSERT INTO @Appname@newss6\nPARTITION BY userdata(@Appname@newss6, 'Modified_Date')\nSELECT\nputUserData(x,'Modified_Date',TO_STRING(DNOW(),'yyyy-MM-dd HH:mm:ss'))\nFROM @Appname@ss6 x;\n\n--Partitioning stream with userdata\nCREATE STREAM @Appname@newss7 of Global.WAEvent PARTITION BY userdata(@Appname@newss7, 'Modified_Date');\n\nCreate Source @Appname@s7 Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL: '@Connectionurl@',\nTables:'qatest.oracle_PartitionStream_test%',\n FetchSize:1\n)\nOutput To @Appname@ss7;\n\nCREATE CQ @Appname@putUserDatacq2\nINSERT INTO @Appname@newss7\nSELECT\nputUserData(x,'Modified_Date',TO_STRING(DNOW(),'yyyy-MM-dd HH:mm:ss'))\nFROM @Appname@ss7 x;\n\n\ncreate Target @Appname@KW1 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'oracle_Expr01',\nMode:'Async',\nKafkaConfig: '$KafkaConfig'\n)\nFORMAT USING jsonFormatter ()\ninput from @Appname@ss1;\n\ncreate Target @Appname@KW2 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'oracle_Expr02',\nMode:'Async',\nKafkaConfig: '$KafkaConfig'\n        )\nFORMAT USING jsonFormatter ()\ninput from @Appname@ss2;\n\ncreate Target @Appname@KW3 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'oracle_Expr03',\nMode:'Async',\nKafkaConfig: '$KafkaConfig'\n        )\nFORMAT USING jsonFormatter ()\ninput from @Appname@OracleTypedStream;\n\ncreate Target @Appname@KW4 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'oracle_Expr04',\nMode:'sync',\nKafkaConfig: '$KafkaConfig'\n        )\nFORMAT USING jsonFormatter ()\ninput from @Appname@ss4new;\n\ncreate Target @Appname@KW5 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'oracle_Expr05',\nMode:'sync',\nKafkaConfig: '$KafkaConfig'\n        )\nFORMAT USING jsonFormatter ()\ninput from @Appname@modifyStream;\n\ncreate Target @Appname@KW6 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'oracle_Expr06',\nMode:'sync',\nKafkaConfig: '$KafkaConfig'\n        )\nFORMAT USING jsonFormatter ()\ninput from @Appname@newss6;\n\ncreate Target @Appname@KW7 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'oracle_Expr07',\nMode:'sync',\nKafkaConfig: '$KafkaConfig'\n        )\nFORMAT USING jsonFormatter ()\ninput from @Appname@newss7;\nEND FLOW @Appname@ServerFlow;\nend application @Appname@;\n--deploy application @Appname@ with @Appname@AgentFlow in Agents, @Appname@ServerFlow in default;\ndeploy application @Appname@;\nstart @Appname@;\n\nstop application @Appname@KR;\nundeploy application @Appname@KR;\ndrop application @Appname@KR cascade;\ncreate application @Appname@KR;\nCREATE STREAM @Appname@KafkaStream of Global.jsonnodeEvent;\nalter stream @Appname@KafkaStream PARTITION BY meta(@Appname@KafkaStream, 'PartitionID');\nCREATE SOURCE @Appname@KafkaSource1 USING KafkaReader Version '2.1.0'\n(\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'oracle_Expr01',\nstartOffset:0\n)\nPARSE USING jsonParser ()\nOUTPUT TO @Appname@KafkaStream;\n\ncreate Target @Appname@t2 using filewriter(\n\tfilename:'%n%',\n\trolloverpolicy:'EventCount:5000000',\n    directory:'FEATURE-DIR/logs/%@metadata(TopicName)%/%@metadata(PartitionID)%'\n)\nformat using jsonFormatter (\n)\ninput from @Appname@KafkaStream;\n\nend application @Appname@KR;\ndeploy application @Appname@KR;\n--start @Appname@KR;\n\nSTOP APPLICATION @Appname@FR;\nUNDEPLOY APPLICATION @Appname@FR;\nDROP APPLICATION @Appname@FR CASCADE;\nCREATE APPLICATION @Appname@FR;\nCREATE SOURCE @Appname@FS0 USING FileReader (\n    directory:'Product/IntegrationTests/TestData/',\n    WildCard:'0',\n\tpositionByEOF:false\n\t)\nPARSE USING jsonParser (\n)OUTPUT TO @Appname@FR_SS0;\n\nCREATE SOURCE @Appname@FS1 USING FileReader (\n    directory:'Product/IntegrationTests/TestData/',\n    WildCard:'0',\n\tpositionByEOF:false\n\t)\nPARSE USING jsonParser (\n)OUTPUT TO @Appname@FR_SS1;\n\nCREATE SOURCE @Appname@FS2 USING FileReader (\n    directory:'Product/IntegrationTests/TestData/',\n    WildCard:'0',\n\tpositionByEOF:false\n\t)\nPARSE USING jsonParser (\n)OUTPUT TO @Appname@FR_SS2;\n\nCREATE SOURCE @Appname@FS3 USING FileReader (\n    directory:'Product/IntegrationTests/TestData/',\n    WildCard:'0',\n\tpositionByEOF:false\n\t)\nPARSE USING jsonParser (\n)OUTPUT TO @Appname@FR_SS3;\n\nCREATE SOURCE @Appname@FS4 USING FileReader (\n    directory:'Product/IntegrationTests/TestData/',\n    WildCard:'0',\n\tpositionByEOF:false\n\t)\nPARSE USING jsonParser (\n)OUTPUT TO @Appname@FR_SS4;\n\nCREATE SOURCE @Appname@FS5 USING FileReader (\n    directory:'Product/IntegrationTests/TestData/',\n    WildCard:'0',\n\tpositionByEOF:false\n\t)\nPARSE USING jsonParser (\n)OUTPUT TO @Appname@FR_SS5;\n\nCREATE SOURCE @Appname@FS6 USING FileReader (\n    directory:'Product/IntegrationTests/TestData/',\n    WildCard:'0',\n\tpositionByEOF:false\n\t)\nPARSE USING jsonParser (\n)OUTPUT TO @Appname@FR_SS6;\nend application @Appname@FR;\ndeploy application @Appname@FR;", "generated_queries": "1. How can I configure retry policies, Kafka broker addresses, and Kafka configurations for a specific application deployment within a TQL script?\n\n2. What are the steps involved in partitioning source streams and creating flows using OracleReader in a TQL query for a particular application?\n\n3. How can I set up Kafka writers with specific configurations, such as broker addresses, topics, and modes, and link them to specific input streams in a TQL script for a designated application deployment?", "file_name": "PartitionByExpr.tql"}
{"tql": "CREATE FLOW @STREAM@_SourceFlow;\n\nCREATE SOURCE @SOURCE_NAME@ Using OracleReader\n(\n Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n   _h_ReturnDateTimeAs:'ZonedDateTime'\n) OUTPUT TO @STREAM@;\n\nEND FLOW @STREAM@_SourceFlow;", "generated_queries": "1. What are the configuration settings for the OracleReader source named \"@SOURCE_NAME@\" that is connected to the database with the connection URL \"@CONNECTION_URL@\" and accesses the table \"@SOURCE_TABLE@\"?\n  \n2. How does the OracleReader source named \"@SOURCE_NAME@\" handle data compression, transaction boundaries filtering, and password encryption? \n  \n3. Can you provide details on the OracleReader source configuration settings, such as the queue size, Xstream timeout, and connection retry policy, for the source named \"@SOURCE_NAME@\" that reads data from the table \"@SOURCE_TABLE@\"?", "file_name": "OracleReader_Flow.tql"}
{"tql": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\nCREATE SOURCE @APPNAME@_S USING OracleReader\n(\n\tUsername: 'qatest',\n\tPassword: 'qatest',\n\tConnectionURL: 'dockerhost:1521:xe',\n\tTables: 'qatest.test01',\n\tFetchSize: '1',\n\tconnectionRetryPolicy:'timeOut=00,retryInterval=1,maxRetries=3'\n)\nOUTPUT TO @APPNAME@_SS;\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(id,name) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:0',\nStandardSQL:true\t\t\n) INPUT FROM @APPNAME@_ss;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\n--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\nSTART APPLICATION @APPNAME@;", "generated_queries": "1. How can I deploy and start an application named @APPNAME@ with specific configurations such as using an OracleReader source and a BigQueryWriter target?\n\n2. What is the process for creating an application named @APPNAME@ with a recovery mechanism, an OracleReader source connected to a database on 'dockerhost:1521:xe' and a BigQueryWriter target writing data to a table in a BigQuery dataset?\n\n3. Can you explain the steps involved in stopping, undeploying, and dropping an application named @APPNAME@, along with creating a new application with recovery settings, setting up an OracleReader source, and deploying the application again with additional configurations?", "file_name": "orcl_bq_quotes.tql"}
{"tql": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW ;\n\nCREATE OR REPLACE SOURCE DBSource USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: 'QATEST.orac_1000COL',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:1,Interval:60',\n  CommitPolicy: 'EventCount:1,Interval:60',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: 'QATEST.orac_1000COL,test.cassandra_1500col columnmap(field1=f1,field2=f2,field3=f3,field4=f4,field5=f5,field6=f6,field7=f7,field8=f8,field9=f9,field10=f10,field11=f11,field12=f12,field13=f13,field14=f14,field15=f15,field16=f16,field17=f17,field18=f18,field19=f19,field20=f20,field21=f21,field22=f22,field23=f23,field24=f24,field25=f25,field26=f26,field27=f27,field28=f28,field29=f29,field30=f30,field31=f31,field32=f32,field33=f33,field34=f34,field35=f35,field36=f36,field37=f37,field38=f38,field39=f39,field40=f40,field41=f41,field42=f42,field43=f43,field44=f44,field45=f45,field46=f46,field47=f47,field48=f48,field49=f49,field50=f50,field51=f51,field52=f52,field53=f53,field54=f54,field55=f55,field56=f56,field57=f57,field58=f58,field59=f59,field60=f60,field61=f61,field62=f62,field63=f63,field64=f64,field65=f65,field66=f66,field67=f67,field68=f68,field69=f69,field70=f70,field71=f71,field72=f72,field73=f73,field74=f74,field75=f75,field76=f76,field77=f77,field78=f78,field79=f79,field80=f80,field81=f81,field82=f82,field83=f83,field84=f84,field85=f85,field86=f86,field87=f87,field88=f88,field89=f89,field90=f90,field91=f91,field92=f92,field93=f93,field94=f94,field95=f95,field96=f96,field97=f97,field98=f98,field99=f99,field100=f100,field101=f101,field102=f102,field103=f103,field104=f104,field105=f105,field106=f106,field107=f107,field108=f108,field109=f109,field110=f110,field111=f111,field112=f112,field113=f113,field114=f114,field115=f115,field116=f116,field117=f117,field118=f118,field119=f119,field120=f120,field121=f121,field122=f122,field123=f123,field124=f124,field125=f125,field126=f126,field127=f127,field128=f128,field129=f129,field130=f130,field131=f131,field132=f132,field133=f133,field134=f134,field135=f135,field136=f136,field137=f137,field138=f138,field139=f139,field140=f140,field141=f141,field142=f142,field143=f143,field144=f144,field145=f145,field146=f146,field147=f147,field148=f148,field149=f149,field150=f150,field151=f151,field152=f152,field153=f153,field154=f154,field155=f155,field156=f156,field157=f157,field158=f158,field159=f159,field160=f160,field161=f161,field162=f162,field163=f163,field164=f164,field165=f165,field166=f166,field167=f167,field168=f168,field169=f169,field170=f170,field171=f171,field172=f172,field173=f173,field174=f174,field175=f175,field176=f176,field177=f177,field178=f178,field179=f179,field180=f180,field181=f181,field182=f182,field183=f183,field184=f184,field185=f185,field186=f186,field187=f187,field188=f188,field189=f189,field190=f190,field191=f191,field192=f192,field193=f193,field194=f194,field195=f195,field196=f196,field197=f197,field198=f198,field199=f199,field200=f200,field201=f201,field202=f202,field203=f203,field204=f204,field205=f205,field206=f206,field207=f207,field208=f208,field209=f209,field210=f210,field211=f211,field212=f212,field213=f213,field214=f214,field215=f215,field216=f216,field217=f217,field218=f218,field219=f219,field220=f220,field221=f221,field222=f222,field223=f223,field224=f224,field225=f225,field226=f226,field227=f227,field228=f228,field229=f229,field230=f230,field231=f231,field232=f232,field233=f233,field234=f234,field235=f235,field236=f236,field237=f237,field238=f238,field239=f239,field240=f240,field241=f241,field242=f242,field243=f243,field244=f244,field245=f245,field246=f246,field247=f247,field248=f248,field249=f249,field250=f250,field251=f251,field252=f252,field253=f253,field254=f254,field255=f255,field256=f256,field257=f257,field258=f258,field259=f259,field260=f260,field261=f261,field262=f262,field263=f263,field264=f264,field265=f265,field266=f266,field267=f267,field268=f268,field269=f269,field270=f270,field271=f271,field272=f272,field273=f273,field274=f274,field275=f275,field276=f276,field277=f277,field278=f278,field279=f279,field280=f280,field281=f281,field282=f282,field283=f283,field284=f284,field285=f285,field286=f286,field287=f287,field288=f288,field289=f289,field290=f290,field291=f291,field292=f292,field293=f293,field294=f294,field295=f295,field296=f296,field297=f297,field298=f298,field299=f299,field300=f300,field301=f301,field302=f302,field303=f303,field304=f304,field305=f305,field306=f306,field307=f307,field308=f308,field309=f309,field310=f310,field311=f311,field312=f312,field313=f313,field314=f314,field315=f315,field316=f316,field317=f317,field318=f318,field319=f319,field320=f320,field321=f321,field322=f322,field323=f323,field324=f324,field325=f325,field326=f326,field327=f327,field328=f328,field329=f329,field330=f330,field331=f331,field332=f332,field333=f333,field334=f334,field335=f335,field336=f336,field337=f337,field338=f338,field339=f339,field340=f340,field341=f341,field342=f342,field343=f343,field344=f344,field345=f345,field346=f346,field347=f347,field348=f348,field349=f349,field350=f350,field351=f351,field352=f352,field353=f353,field354=f354,field355=f355,field356=f356,field357=f357,field358=f358,field359=f359,field360=f360,field361=f361,field362=f362,field363=f363,field364=f364,field365=f365,field366=f366,field367=f367,field368=f368,field369=f369,field370=f370,field371=f371,field372=f372,field373=f373,field374=f374,field375=f375,field376=f376,field377=f377,field378=f378,field379=f379,field380=f380,field381=f381,field382=f382,field383=f383,field384=f384,field385=f385,field386=f386,field387=f387,field388=f388,field389=f389,field390=f390,field391=f391,field392=f392,field393=f393,field394=f394,field395=f395,field396=f396,field397=f397,field398=f398,field399=f399,field400=f400,field401=f401,field402=f402,field403=f403,field404=f404,field405=f405,field406=f406,field407=f407,field408=f408,field409=f409,field410=f410,field411=f411,field412=f412,field413=f413,field414=f414,field415=f415,field416=f416,field417=f417,field418=f418,field419=f419,field420=f420,field421=f421,field422=f422,field423=f423,field424=f424,field425=f425,field426=f426,field427=f427,field428=f428,field429=f429,field430=f430,field431=f431,field432=f432,field433=f433,field434=f434,field435=f435,field436=f436,field437=f437,field438=f438,field439=f439,field440=f440,field441=f441,field442=f442,field443=f443,field444=f444,field445=f445,field446=f446,field447=f447,field448=f448,field449=f449,field450=f450,field451=f451,field452=f452,field453=f453,field454=f454,field455=f455,field456=f456,field457=f457,field458=f458,field459=f459,field460=f460,field461=f461,field462=f462,field463=f463,field464=f464,field465=f465,field466=f466,field467=f467,field468=f468,field469=f469,field470=f470,field471=f471,field472=f472,field473=f473,field474=f474,field475=f475,field476=f476,field477=f477,field478=f478,field479=f479,field480=f480,field481=f481,field482=f482,field483=f483,field484=f484,field485=f485,field486=f486,field487=f487,field488=f488,field489=f489,field490=f490,field491=f491,field492=f492,field493=f493,field494=f494,field495=f495,field496=f496,field497=f497,field498=f498,field499=f499,field500=f500,field501=f501,field502=f502,field503=f503,field504=f504,field505=f505,field506=f506,field507=f507,field508=f508,field509=f509,field510=f510,field511=f511,field512=f512,field513=f513,field514=f514,field515=f515,field516=f516,field517=f517,field518=f518,field519=f519,field520=f520,field521=f521,field522=f522,field523=f523,field524=f524,field525=f525,field526=f526,field527=f527,field528=f528,field529=f529,field530=f530,field531=f531,field532=f532,field533=f533,field534=f534,field535=f535,field536=f536,field537=f537,field538=f538,field539=f539,field540=f540,field541=f541,field542=f542,field543=f543,field544=f544,field545=f545,field546=f546,field547=f547,field548=f548,field549=f549,field550=f550,field551=f551,field552=f552,field553=f553,field554=f554,field555=f555,field556=f556,field557=f557,field558=f558,field559=f559,field560=f560,field561=f561,field562=f562,field563=f563,field564=f564,field565=f565,field566=f566,field567=f567,field568=f568,field569=f569,field570=f570,field571=f571,field572=f572,field573=f573,field574=f574,field575=f575,field576=f576,field577=f577,field578=f578,field579=f579,field580=f580,field581=f581,field582=f582,field583=f583,field584=f584,field585=f585,field586=f586,field587=f587,field588=f588,field589=f589,field590=f590,field591=f591,field592=f592,field593=f593,field594=f594,field595=f595,field596=f596,field597=f597,field598=f598,field599=f599,field600=f600,field601=f601,field602=f602,field603=f603,field604=f604,field605=f605,field606=f606,field607=f607,field608=f608,field609=f609,field610=f610,field611=f611,field612=f612,field613=f613,field614=f614,field615=f615,field616=f616,field617=f617,field618=f618,field619=f619,field620=f620,field621=f621,field622=f622,field623=f623,field624=f624,field625=f625,field626=f626,field627=f627,field628=f628,field629=f629,field630=f630,field631=f631,field632=f632,field633=f633,field634=f634,field635=f635,field636=f636,field637=f637,field638=f638,field639=f639,field640=f640,field641=f641,field642=f642,field643=f643,field644=f644,field645=f645,field646=f646,field647=f647,field648=f648,field649=f649,field650=f650,field651=f651,field652=f652,field653=f653,field654=f654,field655=f655,field656=f656,field657=f657,field658=f658,field659=f659,field660=f660,field661=f661,field662=f662,field663=f663,field664=f664,field665=f665,field666=f666,field667=f667,field668=f668,field669=f669,field670=f670,field671=f671,field672=f672,field673=f673,field674=f674,field675=f675,field676=f676,field677=f677,field678=f678,field679=f679,field680=f680,field681=f681,field682=f682,field683=f683,field684=f684,field685=f685,field686=f686,field687=f687,field688=f688,field689=f689,field690=f690,field691=f691,field692=f692,field693=f693,field694=f694,field695=f695,field696=f696,field697=f697,field698=f698,field699=f699,field700=f700,field701=f701,field702=f702,field703=f703,field704=f704,field705=f705,field706=f706,field707=f707,field708=f708,field709=f709,field710=f710,field711=f711,field712=f712,field713=f713,field714=f714,field715=f715,field716=f716,field717=f717,field718=f718,field719=f719,field720=f720,field721=f721,field722=f722,field723=f723,field724=f724,field725=f725,field726=f726,field727=f727,field728=f728,field729=f729,field730=f730,field731=f731,field732=f732,field733=f733,field734=f734,field735=f735,field736=f736,field737=f737,field738=f738,field739=f739,field740=f740,field741=f741,field742=f742,field743=f743,field744=f744,field745=f745,field746=f746,field747=f747,field748=f748,field749=f749,field750=f750,field751=f751,field752=f752,field753=f753,field754=f754,field755=f755,field756=f756,field757=f757,field758=f758,field759=f759,field760=f760,field761=f761,field762=f762,field763=f763,field764=f764,field765=f765,field766=f766,field767=f767,field768=f768,field769=f769,field770=f770,field771=f771,field772=f772,field773=f773,field774=f774,field775=f775,field776=f776,field777=f777,field778=f778,field779=f779,field780=f780,field781=f781,field782=f782,field783=f783,field784=f784,field785=f785,field786=f786,field787=f787,field788=f788,field789=f789,field790=f790,field791=f791,field792=f792,field793=f793,field794=f794,field795=f795,field796=f796,field797=f797,field798=f798,field799=f799,field800=f800,field801=f801,field802=f802,field803=f803,field804=f804,field805=f805,field806=f806,field807=f807,field808=f808,field809=f809,field810=f810,field811=f811,field812=f812,field813=f813,field814=f814,field815=f815,field816=f816,field817=f817,field818=f818,field819=f819,field820=f820,field821=f821,field822=f822,field823=f823,field824=f824,field825=f825,field826=f826,field827=f827,field828=f828,field829=f829,field830=f830,field831=f831,field832=f832,field833=f833,field834=f834,field835=f835,field836=f836,field837=f837,field838=f838,field839=f839,field840=f840,field841=f841,field842=f842,field843=f843,field844=f844,field845=f845,field846=f846,field847=f847,field848=f848,field849=f849,field850=f850,field851=f851,field852=f852,field853=f853,field854=f854,field855=f855,field856=f856,field857=f857,field858=f858,field859=f859,field860=f860,field861=f861,field862=f862,field863=f863,field864=f864,field865=f865,field866=f866,field867=f867,field868=f868,field869=f869,field870=f870,field871=f871,field872=f872,field873=f873,field874=f874,field875=f875,field876=f876,field877=f877,field878=f878,field879=f879,field880=f880,field881=f881,field882=f882,field883=f883,field884=f884,field885=f885,field886=f886,field887=f887,field888=f888,field889=f889,field890=f890,field891=f891,field892=f892,field893=f893,field894=f894,field895=f895,field896=f896,field897=f897,field898=f898,field899=f899,field900=f900,field901=f901,field902=f902,field903=f903,field904=f904,field905=f905,field906=f906,field907=f907,field908=f908,field909=f909,field910=f910,field911=f911,field912=f912,field913=f913,field914=f914,field915=f915,field916=f916,field917=f917,field918=f918,field919=f919,field920=f920,field921=f921,field922=f922,field923=f923,field924=f924,field925=f925,field926=f926,field927=f927,field928=f928,field929=f929,field930=f930,field931=f931,field932=f932,field933=f933,field934=f934,field935=f935,field936=f936,field937=f937,field938=f938,field939=f939,field940=f940,field941=f941,field942=f942,field943=f943,field944=f944,field945=f945,field946=f946,field947=f947,field948=f948,field949=f949,field950=f950,field951=f951,field952=f952,field953=f953,field954=f954,field955=f955,field956=f956,field957=f957,field958=f958,field959=f959,field960=f960,field961=f961,field962=f962,field963=f963,field964=f964,field965=f965,field966=f966,field967=f967,field968=f968,field969=f969,field970=f970,field971=f971,field972=f972,field973=f973,field974=f974,field975=f975,field976=f976,field977=f977,field978=f978,field979=f979,field980=f980,field981=f981,field982=f982,field983=f983,field984=f984,field985=f985,field986=f986,field987=f987,field988=f988,field989=f989,field990=f990,field991=f991,field992=f992,field993=f993,field994=f994,field995=f995,field996=f996,field997=f997,field998=f998,field999=f999,field1000=f1000,field1001=f501,field1002=f2,field1003=f3,field1004=f4,field1005=f5,field1006=f6,field1007=f7,field1008=f8,field1009=f9,field1010=f10,field1011=f11,field1012=f12,field1013=f13,field1014=f14,field1015=f15,field1016=f16,field1017=f17,field1018=f18,field1019=f19,field1020=f20,field1021=f21,field1022=f22,field1023=f23,field1024=f24,field1025=f25,field1026=f26,field1027=f27,field1028=f28,field1029=f29,field1030=f30,field1031=f31,field1032=f32,field1033=f33,field1034=f34,field1035=f35,field1036=f36,field1037=f37,field1038=f38,field1039=f39,field1040=f40,field1041=f41,field1042=f42,field1043=f43,field1044=f44,field1045=f45,field1046=f46,field1047=f47,field1048=f48,field1049=f49,field1050=f50,field1051=f51,field1052=f52,field1053=f53,field1054=f54,field1055=f55,field1056=f56,field1057=f57,field1058=f58,field1059=f59,field1060=f60,field1061=f61,field1062=f62,field1063=f63,field1064=f64,field1065=f65,field1066=f66,field1067=f67,field1068=f68,field1069=f69,field1070=f70,field1071=f71,field1072=f72,field1073=f73,field1074=f74,field1075=f75,field1076=f76,field1077=f77,field1078=f78,field1079=f79,field1080=f80,field1081=f81,field1082=f82,field1083=f83,field1084=f84,field1085=f85,field1086=f86,field1087=f87,field1088=f88,field1089=f89,field1090=f90,field1091=f91,field1092=f92,field1093=f93,field1094=f94,field1095=f95,field1096=f96,field1097=f97,field1098=f98,field1099=f99,field1100=f100,field1101=f101,field1102=f102,field1103=f103,field1104=f104,field1105=f105,field1106=f106,field1107=f107,field1108=f108,field1109=f109,field1110=f110,field1111=f111,field1112=f112,field1113=f113,field1114=f114,field1115=f115,field1116=f116,field1117=f117,field1118=f118,field1119=f119,field1120=f120,field1121=f121,field1122=f122,field1123=f123,field1124=f124,field1125=f125,field1126=f126,field1127=f127,field1128=f128,field1129=f129,field1130=f130,field1131=f131,field1132=f132,field1133=f133,field1134=f134,field1135=f135,field1136=f136,field1137=f137,field1138=f138,field1139=f139,field1140=f140,field1141=f141,field1142=f142,field1143=f143,field1144=f144,field1145=f145,field1146=f146,field1147=f147,field1148=f148,field1149=f149,field1150=f150,field1151=f151,field1152=f152,field1153=f153,field1154=f154,field1155=f155,field1156=f156,field1157=f157,field1158=f158,field1159=f159,field1160=f160,field1161=f161,field1162=f162,field1163=f163,field1164=f164,field1165=f165,field1166=f166,field1167=f167,field1168=f168,field1169=f169,field1170=f170,field1171=f171,field1172=f172,field1173=f173,field1174=f174,field1175=f175,field1176=f176,field1177=f177,field1178=f178,field1179=f179,field1180=f180,field1181=f181,field1182=f182,field1183=f183,field1184=f184,field1185=f185,field1186=f186,field1187=f187,field1188=f188,field1189=f189,field1190=f190,field1191=f191,field1192=f192,field1193=f193,field1194=f194,field1195=f195,field1196=f196,field1197=f197,field1198=f198,field1199=f199,field1200=f200,field1201=f201,field1202=f202,field1203=f203,field1204=f204,field1205=f205,field1206=f206,field1207=f207,field1208=f208,field1209=f209,field1210=f210,field1211=f211,field1212=f212,field1213=f213,field1214=f214,field1215=f215,field1216=f216,field1217=f217,field1218=f218,field1219=f219,field1220=f220,field1221=f221,field1222=f222,field1223=f223,field1224=f224,field1225=f225,field1226=f226,field1227=f227,field1228=f228,field1229=f229,field1230=f230,field1231=f231,field1232=f232,field1233=f233,field1234=f234,field1235=f235,field1236=f236,field1237=f237,field1238=f238,field1239=f239,field1240=f240,field1241=f241,field1242=f242,field1243=f243,field1244=f244,field1245=f245,field1246=f246,field1247=f247,field1248=f248,field1249=f249,field1250=f250,field1251=f251,field1252=f252,field1253=f253,field1254=f254,field1255=f255,field1256=f256,field1257=f257,field1258=f258,field1259=f259,field1260=f260,field1261=f261,field1262=f262,field1263=f263,field1264=f264,field1265=f265,field1266=f266,field1267=f267,field1268=f268,field1269=f269,field1270=f270,field1271=f271,field1272=f272,field1273=f273,field1274=f274,field1275=f275,field1276=f276,field1277=f277,field1278=f278,field1279=f279,field1280=f280,field1281=f281,field1282=f282,field1283=f283,field1284=f284,field1285=f285,field1286=f286,field1287=f287,field1288=f288,field1289=f289,field1290=f290,field1291=f291,field1292=f292,field1293=f293,field1294=f294,field1295=f295,field1296=f296,field1297=f297,field1298=f298,field1299=f299,field1300=f300,field1301=f301,field1302=f302,field1303=f303,field1304=f304,field1305=f305,field1306=f306,field1307=f307,field1308=f308,field1309=f309,field1310=f310,field1311=f311,field1312=f312,field1313=f313,field1314=f314,field1315=f315,field1316=f316,field1317=f317,field1318=f318,field1319=f319,field1320=f320,field1321=f321,field1322=f322,field1323=f323,field1324=f324,field1325=f325,field1326=f326,field1327=f327,field1328=f328,field1329=f329,field1330=f330,field1331=f331,field1332=f332,field1333=f333,field1334=f334,field1335=f335,field1336=f336,field1337=f337,field1338=f338,field1339=f339,field1340=f340,field1341=f341,field1342=f342,field1343=f343,field1344=f344,field1345=f345,field1346=f346,field1347=f347,field1348=f348,field1349=f349,field1350=f350,field1351=f351,field1352=f352,field1353=f353,field1354=f354,field1355=f355,field1356=f356,field1357=f357,field1358=f358,field1359=f359,field1360=f360,field1361=f361,field1362=f362,field1363=f363,field1364=f364,field1365=f365,field1366=f366,field1367=f367,field1368=f368,field1369=f369,field1370=f370,field1371=f371,field1372=f372,field1373=f373,field1374=f374,field1375=f375,field1376=f376,field1377=f377,field1378=f378,field1379=f379,field1380=f380,field1381=f381,field1382=f382,field1383=f383,field1384=f384,field1385=f385,field1386=f386,field1387=f387,field1388=f388,field1389=f389,field1390=f390,field1391=f391,field1392=f392,field1393=f393,field1394=f394,field1395=f395,field1396=f396,field1397=f397,field1398=f398,field1399=f399,field1400=f400,field1401=f401,field1402=f402,field1403=f403,field1404=f404,field1405=f405,field1406=f406,field1407=f407,field1408=f408,field1409=f409,field1410=f410,field1411=f411,field1412=f412,field1413=f413,field1414=f414,field1415=f415,field1416=f416,field1417=f417,field1418=f418,field1419=f419,field1420=f420,field1421=f421,field1422=f422,field1423=f423,field1424=f424,field1425=f425,field1426=f426,field1427=f427,field1428=f428,field1429=f429,field1430=f430,field1431=f431,field1432=f432,field1433=f433,field1434=f434,field1435=f435,field1436=f436,field1437=f437,field1438=f438,field1439=f439,field1440=f440,field1441=f441,field1442=f442,field1443=f443,field1444=f444,field1445=f445,field1446=f446,field1447=f447,field1448=f448,field1449=f449,field1450=f450,field1451=f451,field1452=f452,field1453=f453,field1454=f454,field1455=f455,field1456=f456,field1457=f457,field1458=f458,field1459=f459,field1460=f460,field1461=f461,field1462=f462,field1463=f463,field1464=f464,field1465=f465,field1466=f466,field1467=f467,field1468=f468,field1469=f469,field1470=f470,field1471=f471,field1472=f472,field1473=f473,field1474=f474,field1475=f475,field1476=f476,field1477=f477,field1478=f478,field1479=f479,field1480=f480,field1481=f481,field1482=f482,field1483=f483,field1484=f484,field1485=f485,field1486=f486,field1487=f487,field1488=f488,field1489=f489,field1490=f490,field1491=f491,field1492=f492,field1493=f493,field1494=f494,field1495=f495,field1496=f496,field1497=f497,field1498=f498,field1499=f499,field1500=f500)',\n  Password: 'cassandra',\n  Password_encrypted: false\n )\nINPUT FROM Oracle_ChangeDataStream;\n\ncreate Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\nEND APPLICATION DBRTOCW;\ndeploy application DBRTOCW in default;\nstart DBRTOCW;", "generated_queries": "1. What are the configurations and settings used for capturing data from the Oracle table 'QATEST.orac_1000COL' and replicating it to a Cassandra table 'test.cassandra_1500col' in a real-time data integration process?\n  \n2. How can I undeploy and then redeploy an application named 'DBRTOCW' along with its corresponding sources, targets, and any associated streams or processes using TQL commands?\n\n3. How can I stop, undeploy, drop, create, deploy, and start an application named 'DBRTOCW' that is responsible for real-time data replication between an Oracle source and a Cassandra target in a data integration scenario?", "file_name": "OractoCassandra1500col.tql"}
{"tql": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\nCREATE OR REPLACE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCREATE FLOW @APPNAME@_Agent_flow;\n\nCREATE OR REPLACE SOURCE @SourceName@1 USING Global.MSJet (\n  Tables: 'dbo.compsrc',\n    username: 'qatest',\n    DatabaseName: 'qatest',\n    FetchTransactionMetadata: true,\n    filterTransactionBoundaries: true,\n    compression: false,\n    ConnectionURL: '10.211.55.3:1433',\n    CommittedTransactions: true,\n    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',\n    SendBeforeImage: true,\n    password: 'w3b@ct10n' )\nOUTPUT TO @SRCINPUTSTREAM@;\n\nEND FLOW @APPNAME@_Agent_flow;\n\nCREATE FLOW @APPNAME@_Agent_flow1;\n\nCREATE OR REPLACE SOURCE @SourceName@2 USING Global.MSJet (\n  Tables: 'dbo.compsrc',\n    username: 'qatest',\n    DatabaseName: 'qatest',\n    FetchTransactionMetadata: true,\n    filterTransactionBoundaries: true,\n    compression: false,\n    ConnectionURL: '10.211.55.3:1433',\n    CommittedTransactions: true,\n    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',\n    SendBeforeImage: true,\n    password: 'w3b@ct10n' )\nOUTPUT TO @SRCINPUTSTREAM@1;\n\nEND FLOW @APPNAME@_Agent_flow1;\n\nCREATE FLOW @APPNAME@_server_flow;\n\nCREATE OR REPLACE TARGET @targetName@1 USING Global.DatabaseWriter\n(\n  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',\n  BatchPolicy:'EventCount:10,Interval:60',\n  CommitPolicy:'EventCount:10,Interval:60',\n  ParallelThreads:'',\n  CheckPointTable:'CHKPOINT',\n  Password_encrypted:'false',\n  Tables:'qatest.MSJEtsrc1,qatest.MSJEtar1;qatest.MSJEtsrc2,qatest.MSJEtar2;',\n  CDDLAction:'Process',\n  Password:'w3b@ct10n',\n  StatementCacheSize:'50',\n  ConnectionURL:'jdbc:sqlserver://10.211.55.3:1433;databaseName=qatest',\n  DatabaseProviderType:'Default',\n  Username:'qatest',\n  PreserveSourceTransactionBoundary:'false',\n  adapterName:'DatabaseWriter'\n)\nINPUT FROM @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@2 USING Global.DatabaseWriter\n(\n  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',\n  BatchPolicy:'EventCount:10,Interval:60',\n  CommitPolicy:'EventCount:10,Interval:60',\n  ParallelThreads:'',\n  CheckPointTable:'CHKPOINT',\n  Password_encrypted:'false',\n  Tables:'qatest.MSJEtsrc1,qatest.MSJEtar1;qatest.MSJEtsrc2,qatest.MSJEtar2;',\n  CDDLAction:'Process',\n  Password:'w3b@ct10n',\n  StatementCacheSize:'50',\n  ConnectionURL:'jdbc:sqlserver://10.211.55.3:1433;databaseName=qatest',\n  DatabaseProviderType:'Default',\n  Username:'qatest',\n  PreserveSourceTransactionBoundary:'false',\n  adapterName:'DatabaseWriter'\n)\nINPUT FROM @SRCINPUTSTREAM@1;\n\nCREATE TARGET @targetsys@ USING Global.SysOut (\n  name: '@targetsys@' )\nINPUT FROM @SRCINPUTSTREAM@;\n\nEND FLOW @APPNAME@_server_flow;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@ with @APPNAME@_Agent_flow in AGENTS, @APPNAME@_Agent_flow1 in AGENTS ,@APPNAME@_server_flow on any in default;\nSTART APPLICATION @APPNAME@;", "generated_queries": "1. What are the steps involved in deploying and starting an application named '@APPNAME@' with multiple flows in the Talend platform?\n2. How can I configure sources, targets, and flows for an application named '@APPNAME@' in Talend using TQL?\n3. Can you provide a detailed explanation of the data flow setup for an application named '@APPNAME@' including sources, targets, and deployment in Talend?", "file_name": "Mul_src_Multar_temp.tql"}
{"tql": "stop application ADW1;\nundeploy application ADW1;\ndrop application ADW1 cascade;\nCREATE APPLICATION ADW1;\n\nCREATE  SOURCE SqlServerInitialLoad1 USING DatabaseReader  \n (\n Username:'src_username',\n Password:'src_password',\n ConnectionURL: 'src_url',\n Tables:'@SOURCE-TABLES@',\n FetchSize:2000\n) \nOUTPUT TO InitialLoadStream1;\n\nCREATE TARGET AzureDWInitialLoad1 USING AzureSQLDWHWriter(\nConnectionURL: '@SQLDW-URL@',\n        username: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@',\n        uploadpolicy:'@EVENT-COUNT@'\n)\nINPUT FROM InitialLoadStream1;\n\nEND APPLICATION ADW1;\ndeploy application ADW1;\nstart application ADW1;", "generated_queries": "1. Can you stop, undeploy, and drop a data warehouse application named ADW1, and then create it again with a specified initial load configuration using a source being a SQL Server and a target being an Azure Data Warehouse?\n  \n2. How can I set up a real-time data integration process between a SQL Server database and an Azure Data Warehouse named ADW1 with specific configurations for data fetching, table mappings, and upload policies using TQL commands?\n\n3. What are the steps involved in deploying and starting an Azure Data Warehouse application named ADW1 after configuring the initial data load settings from a SQL Server source to an Azure Data Warehouse target in a data pipeline setup using TQL commands?", "file_name": "SqlServerToAzureInitialLoad1.tql"}
{"tql": "STOP APPLICATION app1;\nSTOP APPLICATION app2;\nSTOP APPLICATION app3;\nSTOP APPLICATION app4;\nSTOP APPLICATION app5;\nSTOP APPLICATION app6;\nSTOP APPLICATION app7;\nUNDEPLOY APPLICATION app1;\nUNDEPLOY APPLICATION app2;\nUNDEPLOY APPLICATION app3;\nUNDEPLOY APPLICATION app4;\nUNDEPLOY APPLICATION app5;\nUNDEPLOY APPLICATION app6;\nUNDEPLOY APPLICATION app7;\nDROP APPLICATION app1 CASCADE;\nDROP APPLICATION app2 CASCADE;\nDROP APPLICATION app3 CASCADE;\nDROP APPLICATION app4 CASCADE;\nDROP APPLICATION app5 CASCADE;\nDROP APPLICATION app6 CASCADE;\nDROP APPLICATION app7 CASCADE;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',\nacks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\nCREATE APPLICATION app1 RECOVERY 1 SECOND INTERVAL;\n\ncreate flow sourceflow;\n\ncreate type type1(\n  id String,\n  name String,\n  city string\n);\n\nCREATE STREAM rawstream OF Global.waevent persist using KafkaPropset;\nCREATE STREAM kps_typedStream OF type1 partition by city persist using KafkaPropset;\nCREATE STREAM sourcestream of Global.waevent;\n\nCREATE OR REPLACE SOURCE s USING oracleReader  (\n  Username:'qatest',\n  Password:'qatest',\n  ConnectionURL:'localhost:1521/xe',\n  Tables:'QATEST.test%',\n  FetchSize:1\n )\nOUTPUT TO rawstream;\n\nend flow sourceflow;\ncreate flow targetflow;\ncreate cq cq1\nINSERT INTO sourcestream\nSELECT * from rawstream;\n\nCREATE CQ cq2\nINSERT INTO kps_typedStream\nSELECT TO_STRING(data[0]),\nTO_STRING(data[1]),\nTO_STRING(data[2])FROM rawstream;\nend flow targetflow;\n\nend application app1;\n-- deploy application app1;\n-- deploy application app1 with sourceflow in AGENTS, targetflow on any in default;\n\nCREATE APPLICATION app2 RECOVERY 1 SECOND INTERVAL;\n\nCREATE TARGET app2_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test01,QATEST.KPS1'\n) INPUT FROM rawstream;\n\n\nend application app2;\ndeploy application app2;\n\n\nCREATE APPLICATION app3 RECOVERY 1 SECOND INTERVAL;\n\nCREATE TARGET app3_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test02,QATEST.KPS2'\n) INPUT FROM rawstream;\n\nend application app3;\ndeploy application app3;\n\n\nCREATE APPLICATION app4 RECOVERY 1 SECOND INTERVAL;\n\nCREATE TARGET app4_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test03,QATEST.KPS3'\n) INPUT FROM rawstream;\n\nend application app4;\ndeploy application app4;\n\n\nCREATE APPLICATION app5 RECOVERY 1 SECOND INTERVAL;\n\nCREATE TARGET app5_target1 USING KafkaWriter VERSION '0.11.0'(\nbrokeraddress:'localhost:9092',\ntopic:'snappy',\nKafkaConfig:'compression.type=snappy'\n)\nFORMAT USING DSVFormatter ()\nINPUT FROM kps_typedStream;\n\nCREATE TARGET app5_target2 USING KafkaWriter VERSION '0.11.0'(\nbrokeraddress:'localhost:9092',\ntopic:'gzip',\nKafkaConfig:'compression.type=gzip'\n)\nFORMAT USING DSVFormatter ()\nINPUT FROM rawstream;\n\nCREATE TARGET app5_target3 USING KafkaWriter VERSION '0.11.0'(\nbrokeraddress:'localhost:9092',\ntopic:'lz4',\nKafkaConfig:'compression.type=lz4'\n)\nFORMAT USING DSVFormatter ()\nINPUT FROM rawstream;\n\n\nend application app5;\ndeploy application app5;\n\nCREATE APPLICATION app6 RECOVERY 1 SECOND INTERVAL;\n\nCREATE TARGET app6_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test03,QATEST.KPS4'\n) INPUT FROM rawstream;\n\nend application app6;\ndeploy application app6;\n\nCREATE APPLICATION app7 RECOVERY 1 SECOND INTERVAL;\n\nCREATE TARGET app7_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test01,QATEST.KPS_NEW1;qatest.test02,QATEST.KPS_NEW2;qatest.test03,QATEST.KPS_NEW3;'\n) INPUT FROM rawstream;\n\nend application app7;\ndeploy application app7;", "generated_queries": "1. What are the database tables and Kafka topics being written to by each of the deployed applications in the system?\n2. How many applications are currently stopped in the system and what are their individual recovery intervals set to?\n3. Which application is configured to write to multiple database tables with different schemas, and what are the schemas of those tables?", "file_name": "AgentPSDBWriter_5Apps_V11.tql"}
{"tql": "/* *******************************************************************\n Ibasis App for monitoring network with pings to different locations \n****************************************************************** */\n\nuse global;\ndrop namespace PingMonitorBatchApp cascade;\ncreate namespace PingMonitorBatchApp;\nuse PingMonitorBatchApp;\n\nCREATE APPLICATION PingMonitorBatchApp;\n\ncreate\tflow PingMonitorBatchAppFlow;\t\n\nCREATE TYPE PingDateEntry(\n    MaxTestDateTime  DateTime KEY\n );\n\n\nCREATE TYPE PINGEVENT(\n        SERVICE_NAME  String,\n        TEST_DATETIME DateTime,\n\tSOURCE_VERIFIER_ALIAS String,\n\tTARGET_VERIFIER_ALIAS  String,\n        DELAY_AVG float,\n\tPACKET_LOST float,\n        JITTER_AVG float,\n        SEQ_NO     long\n);\n\n\nCREATE CACHE PINGEVENTCACHE  using DatabaseReader (\n   ConnectionURL:'jdbc:oracle:thin:@server1204v.ivanet.net:1960:prtld',\n   Username:'xtract',\n   Password:'xtract123',\n   Query:'select SERVICE_NAME,TEST_DATETIME,SOURCE_VERIFIER_ALIAS,TARGET_VERIFIER_ALIAS,NVL(DELAY_AVG,0),NVL(PACKET_LOST,0),NVL(JITTER_AVG,0),SEQ_NO  from ir34_ping_site where TEST_DATETIME > (select NVL(to_date(\\'1970-01-01\\', \\'YYYY-MM-DD\\') + (MAX(MaxTestDateTime)/ 86400000), sysdate -10/24) from IR34SiteLastDate)order by TEST_DATETIME ASC'\n ) QUERY (keytomap:'SOURCE_VERIFIER_ALIAS', refreshinterval: '60 second', publishonrefresh: 'true') OF PINGEVENT;\n\n\ncreate Target trace0 using CSVWriter (filename:'logs/PINGEVENTCACHE.csv') input from PINGEVENTCACHE;\n\nCREATE WACTIONSTORE IR34ElasticSearchWS  \nCONTEXT OF   PingDateEntry \nEVENT TYPES ( PingDateEntry  )\nPERSIST IMMEDIATE\nUSING ( storageProvider: \"elasticsearch\" );\n\n\nCREATE WACTIONSTORE IR34SiteLastDateWS CONTEXT OF PingDateEntry\n \t\tEVENT TYPES ( PingDateEntry )\n \t    PERSIST EVERY 50 second USING (\n \t    JDBC_DRIVER:'oracle.jdbc.driver.OracleDriver',\n \t    JDBC_URL:'jdbc:oracle:thin:@server1204v.ivanet.net:1960:prtld',\n \t    JDBC_USER:'xtract',\n \t    JDBC_PASSWORD:'xtract123',\n \t    DDL_GENERATION:'create-or-extend-tables',\n            CONTEXT_TABLE:'IR34SiteLastDate'\n \t   );\t\n\t   \n\nCREATE CQ populateMaxDate\n INSERT INTO IR34SiteLastDateWS\n select TO_DATE(MAX(TO_LONG(TEST_DATETIME)))\n from  PINGEVENTCACHE\n , heartbeat(interval '20' second, 1 , 1) hb;\n\n\n \n/* create Target trace1 using CSVWriter (filename:'logs/IR34SiteLastDateWS.csv') input from IR34SiteLastDateWS; */\n\n\nCREATE TYPE DELAYTHRESHTYPE(\n        SITE_KEY  STRING,\n        SOURCE_SITE STRING,\n        TARGET_SITE STRING,\n        DELAY_THRESH FLOAT,\n        PROFILE_ID   INT\n);\n\n\nCREATE CACHE DELAYTHRESHCACHE  USING DATABASEREADER (\n   ConnectionURL:'jdbc:oracle:thin:@timstsnap05.ivanet.net:1521:timsrdg2',\n   Username:'network_cfg',\n   Password:'network_cfg',\n   QUERY: 'select src.site_abbreviation || \\'-\\' || tgt.site_abbreviation as SITE_KEY, src.site_abbreviation as SOURCE_SITE, tgt.site_abbreviation as TARGET_SITE,D.DELAY as DELAY_THRESH, d.PROFILE_ID from site_attributes src, geography gsrc, threshold_delay d, site_attributes tgt, geography gtgt where  Src.GEO_ABBREVIATION = Gsrc.GEO_ABBREVIATION  and src.GEO_ABBREVIATION = D.SOURCE_VERIFIER_GEO  and tgt.GEO_ABBREVIATION = D.TARGET_VERIFIER_GEO  and tgt.GEO_ABBREVIATION = Gtgt.GEO_ABBREVIATION  and tgt.PROFILE_ID = D.PROFILE_ID  and d.profile_id = 1'\n ) QUERY (KEYTOMAP:'SITE_KEY', REFRESHINTERVAL: '60 SECOND', publishonrefresh: 'true') OF DELAYTHRESHTYPE;\n\n\nCREATE TARGET TRACE2 USING CSVWRITER (FILENAME:'logs/DELAYTHRESHCACHE.CSV') INPUT FROM DELAYTHRESHCACHE;\n\n\nCREATE TYPE ALERTENTRY(\n\tALERTKEY STRING KEY,\n \tTOLOC STRING,\n \tFROMLOC STRING,\t\n        STATUS STRING,\n\tINTSTATUS INT,\n\tACTUAL_DELAY INT,\n\tDELAY_THRESHOLD INT,\n        TESTTIME DATETIME\n );\n\nCREATE STREAM PingStreamRaw OF PINGEVENT;\n\nCREATE CQ PROCESSSortCQ\n INSERT INTO PingStreamRaw\n select  SERVICE_NAME  ,\n        TEST_DATETIME ,\n        SOURCE_VERIFIER_ALIAS ,\n        TARGET_VERIFIER_ALIAS  ,\n        DELAY_AVG ,\n        PACKET_LOST ,\n        JITTER_AVG ,\n        SEQ_NO     \nfrom PINGEVENTCACHE,  heartbeat(interval '30' second) hb;\n\nCREATE SORTER PingSorterStream OVER\nPingStreamRaw ON TEST_DATETIME OUTPUT TO SortedPingStream\nWITHIN 70 second  \nOUTPUT ERRORS TO fooErrorStream;\n\n\n CREATE STREAM ALERTENTRYSTREAM OF ALERTENTRY;\n\n CREATE CQ PROCESSCQ1\n INSERT INTO ALERTENTRYSTREAM\n SELECT DW.SOURCE_VERIFIER_ALIAS + \" - \" + DW.TARGET_VERIFIER_ALIAS,\n        DW.TARGET_VERIFIER_ALIAS, \n        DW.SOURCE_VERIFIER_ALIAS,         \n        CASE WHEN DW.DELAY_AVG > DT.DELAY_THRESH THEN \"WARNING\" \n             ELSE \"NORMAL\" \n        END,\n        CASE WHEN DW.DELAY_AVG > DT.DELAY_THRESH  THEN 2\n             ELSE 1\n        END,\n        DW.DELAY_AVG,\n        DT.DELAY_THRESH,\n        DW.TEST_DATETIME\n FROM SortedPingStream DW  , DELAYTHRESHCACHE DT \n WHERE DT.SOURCE_SITE = DW.SOURCE_VERIFIER_ALIAS\n   AND DT.TARGET_SITE = DW.TARGET_VERIFIER_ALIAS;\n\nCREATE TARGET TRACE4 USING CSVWRITER (FILENAME:'logs/ALERTENTRIES.CSV') INPUT FROM ALERTENTRYSTREAM;\n\nCREATE WINDOW DELAYWARNWINDOW OVER  ALERTENTRYSTREAM KEEP WITHIN 20 minute ON TESTTIME;\n\nCREATE TYPE ALERTAGGENTRY(\n        ALERTKEY STRING KEY,\n        ALERT_STATUS STRING,\n        NUMBER_OF_VIOLATIONS INT,\n        DELAY_THRESHOLD INT,\n        FIRSTTIME DATETIME\n );\n\n\n\nCREATE STREAM ALERTAGGSTREAM OF ALERTAGGENTRY;\n\n\nCREATE CQ PROCAGGCQ1 \n \tINSERT INTO ALERTAGGSTREAM\n\t\tSELECT  ALERTKEY,\n                        CASE WHEN COUNT(*) > 5 THEN 'CRITICAL'\n                             WHEN COUNT(*) >= 3 AND COUNT(*) <= 5 THEN 'MAJOR'\n                             WHEN COUNT(*) >=1 AND COUNT(*) < 3 THEN 'MINOR'\n                        ELSE 'NORMAL'\n                        END ,\n                        COUNT(*),\n                        DELAY_THRESHOLD,\n                        FIRST(TESTTIME)\n\t\tFROM DELAYWARNWINDOW   \n                WHERE STATUS = 'WARNING'\n\t\tGROUP BY ALERTKEY, STATUS,DELAY_THRESHOLD;\n\n\n-- new waction stores\n\nCREATE WACTIONSTORE PING_DETAILS\n  CONTEXT OF ALERTENTRY\n  EVENT TYPES (ALERTENTRY )\n  PERSIST EVERY 60 second USING (\n  JDBC_DRIVER:'oracle.jdbc.driver.OracleDriver',\n  JDBC_URL:'jdbc:oracle:thin:@timstsnap05.ivanet.net:1521:timsrdg2',\n  JDBC_USER:'network_cfg', JDBC_PASSWORD:'network_cfg',\n  DDL_GENERATION:'create-or-extend-tables',\n--  DDL_GENERATION:'drop-and-create-tables',\n  CONTEXT_TABLE:'PING_DETAILS'\n  );\n\nCREATE CQ STOREDETAILSCQ\n       INSERT INTO PING_DETAILS\n              SELECT * FROM DELAYWARNWINDOW\n              WHERE STATUS <> 'NORMAL';\n\nCREATE WACTIONSTORE PingStore CONTEXT OF ALERTAGGENTRY\n        EVENT TYPES( ALERTAGGENTRY )\n        PERSIST NONE USING ( ) ;\n\nCREATE CQ LoadPingStoreCQ\n                INSERT INTO PingStore\n                SELECT *\n                FROM ALERTAGGSTREAM s;\n\n-- end of stores \n\n\n\nCREATE STREAM AlertStream OF Global.AlertEvent;\n\n\nCREATE CQ GenerateAlerts\nINSERT INTO AlertStream\nSELECT 'FROM TO SITES',      \n       ALERTKEY, \nCASE\nWHEN ALERT_STATUS != 'NORMAL' THEN 'warning'\nELSE 'info' END,\nCASE\nWHEN  (DNOW()  > DADD(FIRSTTIME, DMINS(20)) OR ALERT_STATUS = 'NORMAL')   THEN 'cancel'\nELSE 'raise' END,\nCASE\nWHEN ALERT_STATUS  != 'NORMAL' THEN ALERT_STATUS + ' ALERT FOR ' + ALERTKEY + ' has delay of over threshold ' + TO_STRING(DELAY_THRESHOLD) + ' has ' + TO_STRING(NUMBER_OF_VIOLATIONS) + '  violations over limit over the hour ' + TO_STRING(FIRSTTIME,'MM/dd/yyyy HH:mm:ss')  ELSE ''\nEND\n    FROM ALERTAGGSTREAM \nWHERE NUMBER_OF_VIOLATIONS > 1;\n--    FROM RESULTSSTORE; \n\n\nCREATE TARGET TRACE3 USING CSVWRITER (FILENAME:'logs/ALERTS.CSV') INPUT FROM AlertStream;\n\nCREATE SUBSCRIPTION PingAppEmailAlert\nUSING EmailAdapter (\nsmtpurl:'smtp.nyc.ibasis.net:25',\nsmtp_auth:'false',\nstarttls_enable:'false',\nsubject:\"IR34 Alert from Webaction\", emailList:\"pgopal@ibasis.net,ed@webaction.com\",senderEmail:\"webactionsupport@ibasis.net\"\n)\nINPUT FROM AlertStream;\n\nCREATE SUBSCRIPTION PosAppWebAlert\nUSING WebAlertAdapter( )\nINPUT FROM AlertStream;\n\n\nEND flow PingMonitorBatchAppFlow;\t\n\n/*\t\nCREATE TYPE AlarmThreshold(\n\tprofile int,\n\tthreshholdValue int\n);\n\nCREATE CACHE AlarmThresholdCache using DatabaseReader (\n   ConnectionURL:'jdbc:oracle:thin:@10.1.110.128:1521:orcl',\n   Username:'scott',\n   Password:'tiger',\n   Query:'select * from scott.ALARM_THRESHOLD'\n ) QUERY (keytomap:'profile', refreshinterval: '60000000') OF AlarmThreshold; \n \n\nCREATE TYPE pingEvent(\n\ttoLoc String,\n\tfromLoc String,\t\n        delay float,\n        jitter float,\n\tpackloss float,\n        dateTime DateTime\n);\n\nCREATE STREAM dataStream OF pingEvent;\n\n Create Target trace1 Using Sysout (name:'rawping') input From LCRStream;\n\n \n CREATE CQ renderEvents\n INSERT INTO dataStream\n SELECT\n \tx.TOLOCATION, x.FROMLOCACTION,  TO_FLOAT(x.DELAYAMT), TO_FLOAT(x.JITTERAMT), TO_FLOAT(x.PACKLOSSAMT), DNOW()\n FROM LCRStream1 x;\n \nCREATE  jumping WINDOW  dataWindow OVER dataStream KEEP WITHIN 30 second; \n \n CREATE TYPE AlertEntry(\n\talertKey String KEY,\n \ttoLoc String,\n \tfromLoc String,\t\n    status String,\n\tintStatus int,\n    dateTime DateTime\n );\n\n CREATE STREAM AlertEntryStream OF AlertEntry;\n\n CREATE CQ processCQ1\n INSERT INTO AlertEntryStream\n select dw.toLoc + dw.fromLoc,\n        dw.toLoc, \n        dw.fromLoc,         \n        CASE WHEN dw.delay > c.threshholdValue THEN \"WARNING\" \n             ELSE \"NORMAL\" \n        END,\n        CASE WHEN dw.delay > c.threshholdValue THEN 2\n             ELSE 1\n        END,\n        dw.dateTime\n from dataWindow dw, AlarmThresholdCache c;\n\t  \t \n Create Target trace2 Using Sysout (name:'alert') input From AlertEntryStream;\n*/ \n \nEND APPLICATION PingMonitorBatchApp;\n -- create dashboard using \"ClientApps/Overstock/ProdIndexDash.json\";", "generated_queries": "1. \"What are the average delay, packet loss, and jitter values between specific network verifiers over the last 24 hours in the Ibasis network monitoring app?\"\n2. \"Which network sites have delay thresholds that have been exceeded, triggering warning alerts within the last hour in the PingMonitorBatchApp?\"\n3. \"How many critical, major, and minor alerts have been generated due to delay threshold violations between network sites within the Ibasis monitoring system over the past day?\"", "file_name": "PingMonitorBatchApp-2.tql"}
{"tql": "CREATE APPLICATION @APPNAME@;\n\nCREATE TYPE @APPNAME@type1 (\n companyName java.lang.String,\n merchantId java.lang.String,\n city java.lang.String);\n\nCREATE STREAM @APPNAME@TypedStream OF @APPNAME@type1 PARTITION BY city;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (\n  wildcard: '',\n  positionByEOF: false,\n  directory: ''\n  )\nPARSE USING DSVParser (\nheader:'true'\n)\nOUTPUT TO @APPNAME@Stream;\n\nCREATE OR REPLACE CQ @APPNAME@CQ\nINSERT INTO @APPNAME@TypedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\") as companyName,\nTO_STRING(data[1]) as merchantID,\nTO_STRING(data[10]) as city\nFROM @APPNAME@Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_jmstrgt USING JMSWriter (\n  QueueName: '',\n  UserName: '',\n  Password: '',\n  Ctx: '',\n  Provider: ''\n  )\nFORMAT USING DSVFormatter()\nINPUT FROM @APPNAME@TypedStream;\n\nEND APPLICATION @APPNAME@;", "generated_queries": "1. What is the process for creating an application with a specific name and defining a custom data type within that application, along with specifying a stream partitioning strategy based on a specific attribute?\n2. How can I set up a real-time data source using a FileReader, parse the incoming data using a DSVParser with a specified configuration, and output the parsed data to a stream within the application?\n3. What steps are involved in creating a continuous query (CQ) within an application that transforms the data from a source stream, such as converting data types and manipulating string values, and then inserts the processed data into a typed stream for further processing or analysis?", "file_name": "DSVtoJMSWriter.tql"}
{"tql": "STOP APPLICATION @APPNAME@_app;\nUNDEPLOY APPLICATION @APPNAME@_app;\nDROP APPLICATION @APPNAME@_app CASCADE;\n-- DROP EXCEPTIONSTORE @APPNAME@_exceptionstore;\n\nCREATE APPLICATION @APPNAME@_app RECOVERY 120 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE @APPNAME@_Source USING @SOURCE_ADAPTER@  (\n  Username:'qatest',\n  Password:'qatest',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: '',\n  ) OUTPUT TO @APPNAME@_Stream  ;\n\nCREATE OR REPLACE TARGET @APPNAME@_Target USING BigQueryWriter  (\n  Tables                        : '',\n  projectId                    : '',\n  ServiceAccountKey            : '',\n  Mode                         : 'APPENDONLY',\n  BatchPolicy                  : 'EventCount:1, Interval:60',\n  ) INPUT FROM @APPNAME@_Stream;\n\n-- CREATE OR REPLACE TARGET @APPNAME@_SysOut USING Global.SysOut (name: 'wa') INPUT FROM @APPNAME@_Stream;\n\nEND APPLICATION @APPNAME@_app;\nDEPLOY APPLICATION @APPNAME@_app;\nSTART APPLICATION @APPNAME@_app;", "generated_queries": "1. How can I stop, undeploy, and drop a specific application, along with its associated resources, in my TQL environment?\n2. What steps are involved in creating an application with a recovery interval of 120 seconds, setting up a source connected to an Oracle database, and configuring a BigQueryWriter target with specific settings in TQL?\n3. Can you explain the process of ending an application, deploying it again, and starting it after making necessary changes to its configuration in TQL?", "file_name": "DataBasetoBqGenric.tql"}
{"tql": "stop application Postgres_To_PostgresApp;\nundeploy application Postgres_To_PostgresApp;\ndrop application Postgres_To_PostgresApp cascade;\nCREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;\ncreate type pkFlag_type\n(\nTableName String,\nPK_UPDATE String,\nOperationName String\n);\nCREATE STREAM Postgres_TypedStream of pkFlag_type;\nCREATE OR REPLACE SOURCE Postgres_Src USING PostgreSQLReader  ( \n  ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: true,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.postgres_src'\n ) \nOUTPUT TO Postgres_Change_Data_Stream;\ncreate CQ Cqfilter \ninsert into Postgres_TypedStream\nselect \nMETA(u,'TableName').toString(),\nMETA(u,'PK_UPDATE').toString(),\nMETA(u,'OperationName').toString()\nfrom Postgres_Change_Data_Stream u;\nCREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( \n  name: 'postgres_PK_Out'\n ) INPUT FROM Postgres_TypedStream;\nCREATE  TARGET Postgres_FW USING FileWriter  ( \n  filename: 'Postgres_PKOut.log',\n  directory: '/Users/jenniffer/Product2/IntegrationTests/target/test-classes/testNG/PostgreSQLReader/logs'\n ) \nFORMAT USING DSVFormatter  (  ) \nINPUT FROM Postgres_TypedStream;\nend application Postgres_To_PostgresApp;\ndeploy application Postgres_To_PostgresApp;\nstart Postgres_To_PostgresApp;", "generated_queries": "1. How can I set up real-time data replication from a PostgreSQL database to another PostgreSQL database with specific filtering and transformation requirements?\n2. What are the steps to create a continuous query that extracts specific metadata fields from a streaming data source and outputs them to a typed stream for further processing?\n3. How can I configure targets to output the data from a typed stream to both the system console and a log file in a specific directory using TIBCO StreamBase?", "file_name": "Postgres_PKFlag.tql"}
{"tql": "create source @SOURCE_NAME@ USING MySQLReader \n(\nUsername: '@READER-UNAME@',\nPassword: '@READER-PASSWORD@',\nConnectionURL: 'jdbc:mysql://127.0.0.1:3306/@DBName@',\nTables: @WATABLES@,\nsendBeforeImage:'true',\nFilterTransactionBoundaries: 'true'\n) \nOUTPUT TO @STREAM@;", "generated_queries": "1. Which tables from my MySQL database are being read by the data source named \"@SOURCE_NAME@\"?\n2. Is the data source named \"@SOURCE_NAME@\" configured to capture before images of records from the MySQL database?\n3. What is the connection URL used by the data source named \"@SOURCE_NAME@\" to access the MySQL database on host 127.0.0.1 and port 3306?", "file_name": "MYSQLReader.tql"}
{"tql": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@ Using MSSqlReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n DatabaseName:'qatest',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'@SourceTable@',\n ConnectionPoolSize:1,\n StartPosition:'EOF'\n ) Output To @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;\n\n\n CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@SourceTable@,@TargetTable@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;", "generated_queries": "1. What is the recovery interval set for application @APPNAME@?\n2. How many tables are being replicated from the MSSQL source database to the target database in application @APPNAME@?\n3. What are the batch policies defined for writing data from the source table to the target table in application @APPNAME@?", "file_name": "NoOp_MSSQLServer2MSSQLServer.tql"}
{"tql": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE OR REPLACE SOURCE DBSource USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'MINER.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@(\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:100,Interval:120',\n  CommitPolicy: 'EventCount:100,Interval:120',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n )\nINPUT FROM Oracle_ChangeDataStream;\n\n\ncreate Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\nEND APPLICATION DBRTOCW;\n\ndeploy application DBRTOCW on ANY in default;\n\nstart application DBRTOCW;", "generated_queries": "1. What is the interval for recovery set for application \"DBRTOCW\"?\n2. Which source database and target database are being used in application \"DBRTOCW\"?\n3. How many targets are configured in application \"DBRTOCW\" and what are their names and types?", "file_name": "OracToCassandraQuiesce.tql"}
{"tql": "STOP IS2noder.IS2Node;\nUNDEPLOY APPLICATION IS2noder.IS2Node;\nDROP APPLICATION IS2noder.IS2Node CASCADE;\n\nCREATE APPLICATION IS2Node;\n\nCREATE FLOW ISFLOW1;\n----------------------------------------------------\nCREATE source implicitSOurce USING FileReader (\n      directory:'@TEST-DATA-PATH@',\n      columndelimiter: ',',\n      wildcard:'ISdata.csv',\n      blocksize: 10240,\n      positionByEOF:false\n)\nPARSE USING DSVParser (\n      header:False,\n      trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE Atm(\n  productID String KEY,\n  stateID String,\n  productWeight int,\n  quantity double,\n  size long,\n  currentDate DateTime);\n\nEND FLOW ISFLOW1;\n----------------------------------------------------\n\nCREATE FLOW ISFLOW2;\n\nCREATE CACHE cache1 USING CsvReader(\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'ISdata.csv',\n  header: false,\n  columndelimiter: ',',\n  trimquote: false\n  ) QUERY (keytomap:'productID') OF Atm;\n\n\nCREATE STREAM newStream OF Atm;\n\n\nCREATE CQ newCQ\nINSERT INTO newStream\nSELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]), TO_DATE(data[5]) FROM\nCsvStream;\n\nCREATE WINDOW win1\nOVER newStream\nKEEP 50 rows;\n\n\nCREATE CQ newCQ2\nINSERT INTO newStream2\nSELECT productID as A , stateID AS B, productWeight AS C, quantity AS D, size AS E, currentDate AS F FROM\nnewStream;\n\n\nCREATE CQ newCQ3\nINSERT INTO newStream3 PARTITION BY A\nSELECT A,B,C,D,E,F FROM newStream2 order by C,D\nlink source event;\n\nCREATE CQ newCQ4\nINSERT INTO newStream4\nSELECT count(productID),currentDate FROM newStream ORDER BY currentDate\nlink source event;\n\nCREATE CQ newCQ5\nINSERT INTO newStream5\nSELECT x.*, y.* from cache1 x, newStream y WHERE x.productweight > 6 ORDER BY x.currentDate;\n\n\nCREATE WACTIONSTORE WS1 CONTEXT OF Atm\nEVENT TYPES(Atm );\n\nCREATE CQ newCQ6\nINSERT INTO WS1\nSELECT * FROM newStream WHERE productID = '001';\n\nCREATE CQ newCQ7\nINSERT INTO newStream6\nSELECT aa.productID FROM WS1 [push] aa, cache1 bb;\n\n\nCREATE CQ newCQ8\nINSERT INTO newStream7\nSELECT Sum(X.size) FROM (Select size from win1 where productweight > 5) X;\n\nEND FLOW ISFLOW2;\n----------------------------------------------------\n\nEND APPLICATION IS2Node;", "generated_queries": "1. What are the total quantity of products with a weight greater than 6 in the dataset, grouped by their current date of entry?\n   \n2. How many unique product IDs have been stored in the system and what is the latest date of entry for each product?\n\n3. Can you provide a summary of the sizes of products that exceed a weight of 5 in a rolling window of the last 50 product entries, grouped by their respective product weights?", "file_name": "IS2Node.tql"}
{"tql": "CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader\n(\nFetchSize:'@FETCHSIZE@',\nQueueSize:'@QUEUESIZE@',\nUsername:'@USERNAME@',\nPassword:'@PASSWORD@',\nConnectionURL:'@URL@',\n_h_useClassic: @CLASSICMODE@,\nTables:'@TABLES@',\nCommittedTransactions:'@COMMITEDT@'\nCompression:'@COMPRESSION@'\n)\nOUTPUT TO @STREAM@ MAP (table: '@SOURCE_SCHEMA@.@SOURCE_TABLE@1')\nSELECT NUM_COL,CHAR_COL,VARCHAR2_COL,LONG_COL,DATE_COL,TIMESTAMP_COL where TO_INT(NUM_COL) > 1;\n\nCREATE TYPE LogType(\nNum_col String key,\nChar_col String,\nVarchar2_col String,\nlong_col String,\nTable String,\nOperation String\n);\n\nCREATE WINDOW CDCWindow\nOVER @STREAM@\nKEEP 1 ROWS;\n\nCREATE WACTIONSTORE CDCWS CONTEXT of LogType\nEVENT TYPES ( LogType )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ ToWactionStore\nINSERT INTO CDCWS\nSELECT * FROM CDCWindow\nLINK SOURCE EVENT;\n\nCREATE TARGET @SOURCE_NAME@_SYS USING SysOut (\n  name: '@SOURCE_NAME@_SYS' )\nINPUT FROM @STREAM@;", "generated_queries": "1. How many records in the Oracle database have a value greater than 1 in the NUM_COL column, and what are the values of the CHAR_COL, VARCHAR2_COL, LONG_COL, DATE_COL, and TIMESTAMP_COL columns for those records?\n\n2. What operations have been performed on the specified tables within the specified schema in the Oracle database, and what are the corresponding values in the NUM_COL, CHAR_COL, VARCHAR2_COL, LONG_COL, and DATE_COL columns for those operations?\n\n3. Can you provide me with real-time updates on any changes occurring in the specified Oracle database tables, such as inserts, updates, or deletes, and store this information in an Elasticsearch storage for further analysis?", "file_name": "OracleReaderSMF_Map.tql"}
