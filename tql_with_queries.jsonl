{"tql": "stop DataGenSampleApp;\nundeploy application DataGenSampleApp;\ndrop application DataGenSampleApp cascade;\n\n\nCREATE APPLICATION DataGenSampleApp;\n\nCreate Source dataGenSrc USING MariaDBReader  ( \n  Username:'qatest',\n  Password:'w3b@ct10n',\n  ConnectionURL:'jdbc:mariadb://10.77.21.53:3306/qatest',\n  Tables: '@tableNames@',\n  ClusterSupport: 'Galera'\n ) \n Output To LCRStream;\n\ncreate Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;\n\nEND APPLICATION DataGenSampleApp;", "generated_queries": "\"What are the steps to stop, undeploy, and then recreate the DataGenSampleApp application using a MariaDB source and outputting to a LCRStream?\"", "file_name": "dataGenMariaDBTqlTemplate.tql"}
{"tql": "stop DBRTOCW;\n undeploy application DBRTOCW;\n drop application DBRTOCW cascade;\n CREATE APPLICATION DBRTOCW;\n\n Create Source MSSQLSource Using MSSqlReader\n(\nUsername:'qatest',\nPassword:'w@ct10n',\nDatabaseName:'qatest',\nConnectionURL:'10.77.61.30:1433',\nTables:'qatest.MssqlTocql_Alldatatypes',\nConnectionPoolSize:1,\nCompression:'true'\n)\nOUTPUT TO Oracle_ChangeDataStream;\n\n CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n ) \nINPUT FROM Oracle_ChangeDataStream;\n\n create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\n END APPLICATION DBRTOCW;\n\n deploy application DBRTOCW in default;\n\n start DBRTOCW;", "generated_queries": "What steps are involved in setting up and deploying the DBRTOCW application to facilitate data streaming from an MSSQL source to an Oracle target?", "file_name": "MssqlToCassandra.tql"}
{"tql": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY;\n\nCreate Source @APPNAME@_src Using OracleReader\n(\n Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1000,\n -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n   _h_ReturnDateTimeAs:'ZonedDateTime'\n) Output To @APPNAME@_stream;\n\ncreate Target @APPNAME@_tgt using FileWriter(\n  filename:'CompressedMerchant.gz',\n  directory:'/logs/',\n  rolloverpolicy:'EventCount:10000'\n)\nformat using ParquetFormatter (\n\tschemaFileName:'@FILENAME@',\n\tFormatAs:'@FORMATAS@'\n)\ninput from @APPNAME@_stream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;", "generated_queries": "How can I set up a new data application to read from an Oracle database and write the output in a compressed Parquet format?", "file_name": "ParquetFormatterNoRecovery.tql"}
{"tql": "CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (\n  Tables: '',\n  ConnectionURL: '',\n  Password: '',\n  Username: ''\n  )\nOUTPUT TO @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING AvroFormatter (\nschemaFileName: '@SCHEMAFILE@'\n)\nINPUT FROM @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING DSVFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nEND APPLICATION @APPNAME@;", "generated_queries": "What are the components and configurations of the application that streams data from an Oracle database to multiple S3 targets in different formats?", "file_name": "OracleCDCToS3MutliTargetsAvro.tql"}
{"tql": "CREATE APPLICATION KafkaReader;\n\nCREATE OR REPLACE TYPE KafkaSourceStr2_Type  ( seq java.lang.Integer\n );\n\nCREATE OR REPLACE STREAM KafkaSourceStr2 OF KafkaSourceStr2_Type;\n\nCREATE  JUMPING WINDOW GetTargData OVER KafkaSourceStr2 KEEP 1000000 ROWS;\n\nCREATE OR REPLACE SOURCE KafkaSource USING KafkaReader VERSION '0.11.0' (\n  KafkaConfigPropertySeparator: ';',\n  startOffset: 0,\n  adapterName: 'KafkaReader',\n  Topic: 'kafkaTopic7',\n  AutoMapPartition: true,\n  brokerAddress: 'localhost:9092',\n  KafkaConfigValueSeparator: '=',\n  KafkaConfig: 'max.partition.fetch.bytes=10485760;fetch.min.bytes=1048576;fetch.max.wait.ms=1000;receive.buffer.bytes=2000000;poll.timeout.ms=10000;request.timeout.ms=60001;session.timeout.ms=60000'\n )\n PARSE USING DSVParser  (\n  charset: 'UTF-8',\n  handler: 'com.webaction.proc.DSVParser_1_0',\n  linenumber: 0,\n  nocolumndelimiter: false,\n  trimwhitespace: false,\n  columndelimiter: ',',\n  columndelimittill: '-1',\n  ignoremultiplerecordbegin: 'true',\n  ignorerowdelimiterinquote: false,\n  parserName: 'DSVParser',\n  separator: ':',\n  blockascompleterecord: false,\n  ignoreemptycolumn: false,\n  rowdelimiter: '\\n',\n  header: false,\n  headerlineno: 0,\n  quoteset: '\\\"',\n  trimquote: true\n )\nOUTPUT TO KafkaSourceStr1 ;\n\nCREATE OR REPLACE CQ GetKafkaDataQuery\nINSERT INTO KafkaSourceStr2\nSELECT TO_INT(data[1]) as seq\nFROM KafkaSourceStr1;\n\nCREATE  TYPE KafkaSourceStr3_Type  ( SUMKafkaSourceStr2seq java.lang.Long\n );\n\nCREATE STREAM KafkaSourceStr3 OF KafkaSourceStr3_Type;\n\nCREATE OR REPLACE CQ GetTheSum\nINSERT INTO KafkaSourceStr3\nSELECT SUM(GetTargData .seq)\nFROM GetTargData;\n\nCREATE OR REPLACE TARGET KafkaFile USING FileWriter  (\n  filename: 'TargetResults',\n  rolloveronddl: 'true',\n  flushpolicy: 'eventcount:10000,interval:30',\n  adapterName: 'FileWriter',\n  directory: '@FEATURE-DIR@/logs',\n  rolloverpolicy: 'eventcount:10000,interval:30s'\n )\nFORMAT USING DSVFormatter  (   nullvalue: 'NULL',\n  standard: 'none',\n  handler: 'com.webaction.proc.DSVFormatter',\n  formatterName: 'DSVFormatter',\n  usequotes: 'false',\n  rowdelimiter: '\\n',\n  quotecharacter: '\\\"',\n  header: 'false',\n  columndelimiter: ','\n )\nINPUT FROM KafkaSourceStr3;\n\nEND APPLICATION KafkaReader;", "generated_queries": "What is the sum of the sequence numbers produced from messages in the Kafka topic 'kafkaTopic7' after processing them through the application?", "file_name": "KafkaReader.tql"}
{"tql": "STOP APPLICATION @WRITERAPPNAME@;\nUNDEPLOY APPLICATION @WRITERAPPNAME@;\nDROP APPLICATION @WRITERAPPNAME@ CASCADE;\n\nCREATE APPLICATION @WRITERAPPNAME@ RECOVERY 1 SECOND INTERVAL;\n\ncreate flow @APPNAME@_agentflow;\n\nCREATE SOURCE @SOURCE@ USING OracleReader\n(\nFetchSize:1,\nUsername:'@SOURCE_USER@',\nPassword:'85d7qFnwTW8=',\nConnectionURL:'@CONNECTION_URL@',\nTables:'@SOURCE_TABLE@',\npassword_encrypted: 'true'\n)\nOUTPUT TO @STREAM1@;\n\n\nend flow @APPNAME@_agentflow;\n\ncreate flow @APPNAME@_serverflow;\n\nCREATE OR REPLACE TYPE @TYPE@( \ndatae java.util.HashMap , \nTABLE_NAME java.lang.String , \nOPS_NAME java.lang.String , \nDB_TIMESTAMP java.lang.String  ,\nCOMMITSCN java.lang.String ,\nSCN java.lang.String ,\nREC_INS_TIME java.lang.String );\n\nCREATE CQ @CQ1@\nINSERT INTO @STREAM2@\nSELECT  \nCASE WHEN (META(c,\"OperationName\").toString() == \"DELETE\")\nTHEN putUserData(c, 'isDelete', 'true') \nELSE\nputUserData(c,'isDelete', 'false')\nEND\nFROM @STREAM1@ c;\n\nCREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;\n\nCREATE OR REPLACE CQ @CQ2@ \nINSERT INTO @STREAM3@\nSELECT \ndata(e),\nMETA(e,\"TableName\").toString() as TABLE_NAME,\nMETA(e, \"OperationName\").toString() as OPS_NAME,\nMETA(e, \"TimeStamp\").toString() as DB_TIMESTAMP,\nMETA(e,\"COMMITSCN\").toString() as COMMITSCN ,\nMETA(e,\"SCN\").toString() as  SCN ,\nDNOW().toString() as REC_INS_TIME\nFROM @STREAM1@ e;\n\ncreate Target @TARGET1@ using KafkaWriter VERSION @kafkaAdpVersion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC1',\nParallelThreads:'',\nPartitionKey:'@metadata(TableName)',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING dsvFormatter ()\ninput from @STREAM1@;\n\n\ncreate Target @TARGET2@ using KafkaWriter VERSION @kafkaAdpVersion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC2',\nParallelThreads:'2',\nPartitionKey:'TABLE_NAME',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING jsonFormatter ()\ninput from @STREAM3@;\n\ncreate Target @TARGET3@ using KafkaWriter VERSION @kafkaAdpVersion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC3',\nParallelThreads:'',\nPartitionKey:'@userdata(isDelete)',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING avroFormatter (\nschemaFileName:'KafkaAvroTest.avsc')\ninput from @STREAM2@;\n\nend application @WRITERAPPNAME@;\ndeploy application @WRITERAPPNAME@;\nstart @WRITERAPPNAME@;\nstop application @READERAPPNAME@;\nundeploy application @READERAPPNAME@;\ndrop application @READERAPPNAME@ cascade;\nCREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafkaAdpVersion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC1',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream1;\n\n\nCREATE TARGET kafkaDumpDSV USING FileWriter(\nname:kafkaOuputDSV,\nfilename:'@READERAPPNAME@_RT_DSV')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaReaderStream1;\n\nCREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafkaAdpVersion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC2',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO KafkaReaderStream2;\n\n\nCREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafkaAdpVersion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC3',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'KafkaAvroTest.avsc'\n)\nOUTPUT TO KafkaReaderStream3;\n\nend flow @APPNAME@_serverflow;\n\nend application @READERAPPNAME@;\ndeploy application @READERAPPNAME@;", "generated_queries": "What are the steps to create and deploy an application that reads data from Kafka topics and writes it to different output formats, using a combination of DSV, JSON, and Avro formats?", "file_name": "orcl_kw_11_agent.tql"}
{"tql": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\nCREATE APPLICATION @APPNAME@ RECOVERY 10 SECOND INTERVAL;\n\n create flow Mysqlflow;\nCREATE SOURCE MysqlToDBRoutersource USING MysqlReader\n(\n  Username: '',\n  Password: '',\n  Tables: '',\n  ConnectionURL: '',\n  Password_encrypted: 'false',\n  connectionRetryPolicy: 'retryInterval=30, maxRetries=3'\n)\nOUTPUT TO RouterTestMasterStream;\n\nend flow Mysqlflow;\n\nCREATE OR REPLACE ROUTER RouterTestRs1 INPUT FROM RouterTestMasterStream s CASE\nWHEN meta(s,\"TableName\").toString()='waction.source1' THEN ROUTE TO RouterTestTyped1,\nWHEN meta(s,\"TableName\").toString()='waction.source2' THEN ROUTE TO RouterTestTyped2,\nELSE ROUTE TO RouterTestTypedElse;\n\nCREATE TARGET MysqlToDBRoutertarget1 USING DatabaseWriter(\n   Username: '',\n   Password: '',\n   Tables: '',\n   ConnectionURL: '',\n   Password_encrypted: 'false',\n   connectionRetryPolicy: 'retryInterval=30, maxRetries=3'\n) INPUT FROM RouterTestTyped1;\n\n\nCREATE TARGET MysqlToDBRoutertarget2 USING DatabaseWriter(\n    Username: '',\n    Password: '',\n    Tables: '',\n    ConnectionURL: '',\n    Password_encrypted: 'false',\n    connectionRetryPolicy: 'retryInterval=30, maxRetries=3'\n) INPUT FROM RouterTestTyped2;\n\n\nend application @APPNAME@;\ndeploy application @APPNAME@ with Mysqlflow in AGENTs;\nstart application @APPNAME@;", "generated_queries": "What steps do I need to follow to deploy the application named @APPNAME@ with a MySQL data flow that routes data from specific source tables to different database targets?", "file_name": "routerMySQL.tql"}
{"tql": "--\n-- Crash Recovery Test 2 on four node all server cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS\n--\n\nSTOP APPLICATION N4S4CR2Tester.N4S4CRTest2;\nUNDEPLOY APPLICATION N4S4CR2Tester.N4S4CRTest2;\nDROP APPLICATION N4S4CR2Tester.N4S4CRTest2 CASCADE;\nCREATE APPLICATION N4S4CRTest2 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN4S4CRTest2;\n\nCREATE SOURCE CsvSourceN4S4CRTest2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN4S4CRTest2;\n\nCREATE FLOW DataProcessingN4S4CRTest2;\n\nCREATE TYPE WactionTypeN4S4CRTest2 (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionTypeN4S4CRTest2;\n\nCREATE CQ CsvToWaction\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsN4S4CRTest2 CONTEXT OF WactionTypeN4S4CRTest2\nEVENT TYPES ( WactionTypeN4S4CRTest2 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsN4S4CRTest2\nINSERT INTO WactionsN4S4CRTest2\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND FLOW DataProcessingN4S4CRTest2;\n\nEND APPLICATION N4S4CRTest2;", "generated_queries": "What steps are involved in setting up and executing the crash recovery test for the N4S4CR2Tester application, including data acquisition and processing?", "file_name": "KStreamN4S4CRTest2.tql"}
{"tql": "stop OracleToKudu;\nundeploy application OracleToKudu;\ndrop application OracleToKudu cascade;\n\nCREATE APPLICATION OracleToKudu RECOVERY 5 SECOND INTERVAL;\nCreate Source oracSource\n Using OracleReader\n(\n Username:'@LOGMINER-UNAME@',\n Password:'@LOGMINER-PASSWORD@',\n ConnectionURL:'@LOGMINER-URL@',\n Tables:'@SOURCE_TABLES@',\n OnlineCatalog:true,\n FetchSize:1\n) Output To DataStream;\nCREATE TARGET WriteintoKudu using KuduWriter (\nkuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',\npkupdatehandlingmode:'@MODE@',\ntables: '@TARGET_TABLES@',\nConnectionRetryPolicy: 'retryInterval=40,maxRetries=7',\nbatchpolicy: 'EventCount:20,Interval:60')\nINPUT FROM DataStream;\n\nEND APPLICATION OracleToKudu;\ndeploy application OracleToKudu in default;\nstart OracleToKudu;", "generated_queries": "1. How can I transfer data from an Oracle database to a Kudu table using a streaming application?", "file_name": "OracleToKuduRecovery.tql"}
{"tql": "STOP APPLICATION EH;\nUNDEPLOY APPLICATION EH;\nDROP APPLICATION EH CASCADE;\nCREATE APPLICATION EH recovery 5 second interval;\nCREATE Source s USING PostgreSQLReader  ( \n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Tables: 'public.tablename1000%') \nOUTPUT TO ss ;\n\nCREATE OR REPLACE TYPE jsontype( \ndatae java.util.HashMap , \nTABLE_NAME java.lang.String , \nOPS_NAME java.lang.String , \nDB_TIMESTAMP java.lang.String );\n\nCREATE STREAM cq_json_out OF jsontype PARTITION BY TABLE_NAME;\n\nCREATE OR REPLACE CQ cq_json \nINSERT INTO cq_json_out\nSELECT \ndata(e),\nMETA(e,\"TableName\").toString() as TABLE_NAME,\nMETA(e, \"OperationName\").toString() as OPS_NAME,\nMETA(e, \"TimeStamp\").toString() as DB_TIMESTAMP\nFROM ss e;\n\nCREATE CQ cq1\nINSERT INTO TypedAccessLogStream1\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000101'; \n\ncreate Target t1 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_101',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_101',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream1;\n\nCREATE CQ cq2\nINSERT INTO TypedAccessLogStream2\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000102'; \n\ncreate Target t2 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_102',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_102',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream2;\n\nCREATE CQ cq3\nINSERT INTO TypedAccessLogStream3\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000103'; \n\ncreate Target t3 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_103',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_103',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream3;\n\nCREATE CQ cq4\nINSERT INTO TypedAccessLogStream4\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000104'; \n\ncreate Target t4 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_104',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_104',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream4;\n\nCREATE CQ cq5\nINSERT INTO TypedAccessLogStream5\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000105'; \n\ncreate Target t5 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_105',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_105',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream5;\n\nCREATE CQ cq6\nINSERT INTO TypedAccessLogStream6\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000106'; \n\ncreate Target t6 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_106',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_106',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream6;\n\nCREATE CQ cq7\nINSERT INTO TypedAccessLogStream7\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000107'; \n\ncreate Target t7 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_107',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_107',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream7;\n\nCREATE CQ cq8\nINSERT INTO TypedAccessLogStream8\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000108'; \n\ncreate Target t8 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_108',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_108',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream8;\n\nCREATE CQ cq9\nINSERT INTO TypedAccessLogStream9\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000109'; \n\ncreate Target t9 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_109',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_109',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream9;\n\n-- CREATE CQ cq10\n-- INSERT INTO TypedAccessLogStream10\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000110'; \n-- \n-- create Target t10 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_110',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_110',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream10;\n\n\n-- CREATE CQ cq11\n-- INSERT INTO TypedAccessLogStream11\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000111'; \n-- \n-- create Target t11 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_111',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_111',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream11;\n-- \n-- CREATE CQ cq12\n-- INSERT INTO TypedAccessLogStream12\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000112'; \n-- \n-- create Target t12 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_112',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_112',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream12;\n-- \n-- CREATE CQ cq13\n-- INSERT INTO TypedAccessLogStream13\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000113'; \n-- \n-- create Target t13 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_113',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_113',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream13;\n-- \n-- CREATE CQ cq14\n-- INSERT INTO TypedAccessLogStream14\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000114'; \n-- \n-- create Target t14 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_114',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_114',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream14;\n-- \n-- CREATE CQ cq15\n-- INSERT INTO TypedAccessLogStream15\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000115'; \n-- \n-- create Target t15 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_115',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_115',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream15;\n-- \n-- CREATE CQ cq16\n-- INSERT INTO TypedAccessLogStream16\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000116'; \n-- \n-- create Target t16 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_116',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_116',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream16;\n-- \n-- CREATE CQ cq17\n-- INSERT INTO TypedAccessLogStream17\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000117'; \n-- \n-- create Target t17 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_117',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_117',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream17;\n-- \n-- CREATE CQ cq18\n-- INSERT INTO TypedAccessLogStream18\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000118'; \n-- \n-- create Target t18 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_118',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_118',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream18;\n-- \n-- CREATE CQ cq19\n-- INSERT INTO TypedAccessLogStream19\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000119'; \n-- \n-- create Target t19 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_119',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_119',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream19;\n-- \n-- CREATE CQ cq20\n-- INSERT INTO TypedAccessLogStream20\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000120'; \n-- \n-- create Target t20 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_120',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_120',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream20;\n\n\nEND APPLICATION EH;\nDEPLOY APPLICATION EH;\nstart application EH;", "generated_queries": "What are the specific operations being logged from the PostgreSQL table \"public.tablename1000%\" and how are those logs being processed and sent to Azure Event Hub?", "file_name": "postgrescdc_eh.tql"}
{"tql": "create application CSVToJSON;\ncreate source CSVSource using FileReader (\n\tdirectory:'Samples/AppData',\n\tWildCard:'posdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\n\tfilename:'posdata_JSON',\n\trolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'\n)\nformat using JSONFormatter (\n\tmembers:'merchantname,merchantid,dateTime,hourValue,amount,zip'\n)\n\ninput from TypedCSVStream;\nend application CSVToJSON;\n\ndeploy application CSVToJSON;\nstart application CSVToJSON;", "generated_queries": "How can I convert a CSV file containing payment data into a JSON format for further analysis?", "file_name": "FileWriterWithJSONFormatter.tql"}
{"tql": "stop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@;\n\nCREATE SOURCE @SourceName@ USING PostgreSQLReader  ( \nReaderType: 'LogMiner', \n  Password_encrypted: 'false', \n  SupportPDB: false, \n  ReplicationSlotName: 'test_slot',\n  QuiesceMarkerTable: 'QUIESCEMARKER', \n  QueueSize: 2048, \n  CommittedTransactions: true, \n  Username: '@UserName@', \n  TransactionBufferType: 'Memory', \n  TransactionBufferDiskLocation: '.striim/LargeBuffer', \n  OutboundServerProcessName: 'WebActionXStream', \n  Password: '@Password@', \n  DDLCaptureMode: 'All', \n  Compression: false, \n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', \n  FetchSize: 1, \n  Tables: '@SourceTables@', \n  DictionaryMode: 'OnlineCatalog', \n  XstreamTimeOut: 600, \n  TransactionBufferSpilloverSize: '1MB', \n  FilterTransactionBoundaries: true, \n  ConnectionURL: '@ConnectionURL@', \n  SendBeforeImage: true ) \nOUTPUT TO @AppStream@  ;\n\nCREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream SELECT data[2], to_string(to_date(data[2]), \"dd-MMM-yy hh.mm.ss\") FROM @AppStream@ o ;\n\nCREATE  TARGET @targetsys@ USING Global.SysOut  ( \nname: 'ora1_sys' ) \nINPUT FROM admin.ZDT_cq_stream;\n\ncreate Target @TargetFile@ using FileWriter(\n  filename:'toStringOut.log',\n  directory:'@FilePath@',\n  rolloverpolicy:'eventcount:1000'\n)\nformat using DSVFormatter (\n\n)\ninput from admin.ZDT_cq_stream;\n\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;", "generated_queries": "How do I reset, recreate, and deploy an application using the latest changes from a PostgreSQL source, and set up real-time data capture with specific configurations?", "file_name": "toStringZonedDateTime_postgres.tql"}
{"tql": "--\n-- Recovery Test 13 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same compound key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> CW(p#p) -> CQ -> WS\n--\n\nSTOP Recov13Tester.RecovTest13;\nUNDEPLOY APPLICATION Recov13Tester.RecovTest13;\nDROP APPLICATION Recov13Tester.RecovTest13 CASCADE;\nCREATE APPLICATION RecovTest13 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTest10Data.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE CsvData (\n  partKey String KEY,\n  serialNumber int,\n  partKey2 String KEY\n);\n\nCREATE TYPE WactionData (\n  partKey String KEY,\n  serialNumber int\n);\n\nCREATE STREAM DataStream OF CsvData PARTITION BY partKey, partKey2;\n\nCREATE CQ CsvToData\nINSERT INTO DataStream\nSELECT\n    data[0],\n    TO_INT(data[1]),\n    data[0]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStreamTwoItems\nOVER DataStream KEEP 2 ROWS\nPARTITION BY partKey, partKey2;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction\nINSERT INTO Wactions\nSELECT\n    first(partKey),\n    to_int(first(serialNumber))\nFROM DataStreamTwoItems\nGROUP BY partKey, partKey2;\n\nEND APPLICATION RecovTest13;", "generated_queries": "How is the Recov13Tester application processing data from the CSV source, and what are the key actions stored in the Wactionstore for different partitions?", "file_name": "RecovTest13.tql"}
{"tql": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()\nPARSE USING XMLParserV2 ()\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE OR REPLACE CQ @APPNAME@_CQ\nINSERT INTO @APPNAME@_CQOut\nSELECT\ndata.attributeValue(\"merchantid\") as merchantID,\ndata.getText() as companyName\nFROM @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()\nFORMAT USING JSONFormatter ()\nINPUT FROM @APPNAME@_CQOut;\n\nEND APPLICATION @APPNAME@;", "generated_queries": "What is the process for creating a new application that extracts merchant IDs and company names from XML data and outputs them in JSON format?", "file_name": "XmlV2ToJson.tql"}
{"tql": "use PosTester;\nalter application PosApp;\n\nCREATE TYPE MerchantTxRate(\n  merchantId String KEY,\n  zip String,\n  startTime DateTime,\n  count int,\n  totalAmount double,\n  hourlyAve int,\n  upperLimit double,\n  lowerLimit double,\n  category String,\n  status String\n);\n\nend application PosApp;\nalter application PosApp recompile;", "generated_queries": "What new merchant transaction rate data structure has been created for the PosApp application, and what attributes does it include?", "file_name": "createType.tql"}
{"tql": "STOP APPLICATION ORACLETOBIGQUERY;\nUNDEPLOY APPLICATION ORACLETOBIGQUERY;\nDROP APPLICATION ORACLETOBIGQUERY CASCADE;\n\n--create application \nCREATE APPLICATION ORACLETOBIGQUERY RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE OR REPLACE SOURCE OracleSource USING OracleReader (\n ConnectionURL: '192.168.123.12:1521/ORCL',\n Tables: 'QATEST.ORATOBIGQALLDATATYPE',\n Username: 'qatest',\n Password: 'qatest',\n FetchSize:1\n) OUTPUT TO CDCStream;\n\nCREATE OR REPLACE TARGET bqtables using BigqueryWriter(\n BQServiceAccountConfigurationPath:\"/Users/karthikmurugan/Downloads/bqtest-540227c31980.json\",\n projectId:\"bqtest-158706\",\n Tables: \"QATEST.ORATOBIGQALLDATATYPE,QATEST.ORATOBIGQALLDATATYPE\",\n BatchPolicy: \"eventCount:1,Interval:90\")\nINPUT FROM CDCStream;\n\n\nCREATE OR REPLACE TARGET T1 using SysOut(name : \"some text\") INPUT FROM CDCStream;\n\nEND APPLICATION ORACLETOBIGQUERY;\n\nDEPLOY APPLICATION ORACLETOBIGQUERY;\nSTART APPLICATION ORACLETOBIGQUERY;", "generated_queries": "What steps are involved in setting up and deploying the OracleToBigQuery application for data streaming from Oracle to BigQuery?", "file_name": "OracleToBigQueryAlldatatypes.tql"}
{"tql": "stop application reconnect;\nundeploy application reconnect;\ndrop application reconnect cascade;\nCREATE APPLICATION reconnect recovery 1 second interval;\n\nCREATE  SOURCE mssqlsource USING MssqlReader  ( \n  Username: '@USERNAME@',\n  Password: '@PASSWORD@',\n  ConnectionURL: '@URL@',\n  Tables: '@TABLE@',\n  FetchSize: 1\n ) \nOUTPUT TO sqlstream;\n\nCREATE TARGET dbtarget USING CassandraWriter(\n  ConnectionURL:'@URL@',\n  Username:'@USERNAME@',\n  Password:'@PASSWORD@',\n  ConnectionRetryPolicy: 'retryInterval=15s,maxRetries=2',\n  BatchPolicy:'EventCount:5,Interval:30',\n  CommitPolicy:'EventCount:5,Interval:30',\n  Tables: '@TABLES@'\n ) INPUT FROM sqlstream;\n\n create Target tSysOut using Sysout(name:OrgData) input from sqlstream;\n end application reconnect;\n deploy application reconnect;\n start application reconnect;", "generated_queries": "What steps are involved in configuring the \"reconnect\" application to read from a SQL Server source and write to a Cassandra target while also outputting to the system console?", "file_name": "oracle_CassandraReconnect.tql"}
{"tql": "STOP APPLICATION App1;\nUNDEPLOY APPLICATION App1;\nDROP APPLICATION App1 CASCADE;\nCREATE APPLICATION App1;\nCREATE FLOW AgentFlow;\nCREATE OR REPLACE SOURCE App1_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App1_SampleStream;\nEND FLOW AgentFlow;\nCREATE FLOW ServerFlow;\nCREATE OR REPLACE TARGET App1_NullTarget using NullWriter()\nINPUT FROM App1_SampleStream;\nEND FLOW ServerFlow;\nEND APPLICATION App1;\ndeploy application App1 on any in ServerDG1 with AgentFlow on any in Agents, ServerFlow on any in ServerDG1;\nSTART APPLICATION App1;\n\nSTOP APPLICATION App2;\nUNDEPLOY APPLICATION App2;\nDROP APPLICATION App2 CASCADE;\nCREATE APPLICATION App2;\nCREATE FLOW AgentFlow2;\nCREATE OR REPLACE SOURCE App2_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App2_SampleStream;\nEND FLOW AgentFlow2;\nCREATE FLOW ServerFlow2;\nCREATE OR REPLACE TARGET App2_NullTarget using NullWriter()\nINPUT FROM App2_SampleStream;\nEND FLOW ServerFlow2;\nEND APPLICATION App2;\ndeploy application App2 on any in ServerDG1 with AgentFlow2 on any in Agents, ServerFlow2 on any in ServerDG1;\nSTART APPLICATION App2;\n\nSTOP APPLICATION App3;\nUNDEPLOY APPLICATION App3;\nDROP APPLICATION App3 CASCADE;\nCREATE APPLICATION App3;\nCREATE OR REPLACE SOURCE App3_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App3_SampleStream;\nCREATE OR REPLACE TARGET App3_NullTarget using NullWriter()\nINPUT FROM App3_SampleStream;\nEND APPLICATION App3;\nDEPLOY APPLICATION App3;\nSTART APPLICATION App3;\n\nSTOP APPLICATION App4;\nUNDEPLOY APPLICATION App4;\nDROP APPLICATION App4 CASCADE;\nCREATE APPLICATION App4;\nCREATE OR REPLACE SOURCE App4_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App4_SampleStream;\nCREATE OR REPLACE TARGET App4_NullTarget using NullWriter()\nINPUT FROM App4_SampleStream;\nEND APPLICATION App4;\nDEPLOY APPLICATION App4 ON ONE IN ServerDG1;\nSTART APPLICATION App4;\n\nSTOP APPLICATION App5;\nUNDEPLOY APPLICATION App5;\nDROP APPLICATION App5 CASCADE;\nCREATE APPLICATION App5;\nCREATE OR REPLACE SOURCE App5_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App5_SampleStream;\nCREATE OR REPLACE TARGET App5_NullTarget using NullWriter()\nINPUT FROM App5_SampleStream;\nEND APPLICATION App5;\nDEPLOY APPLICATION App5 ON ONE IN ServerDG1;\nSTART APPLICATION App5;\n\nSTOP APPLICATION App6;\nUNDEPLOY APPLICATION App6;\nDROP APPLICATION App6 CASCADE;\nCREATE APPLICATION App6;\nCREATE OR REPLACE SOURCE App6_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App6_SampleStream;\nCREATE OR REPLACE TARGET App6_NullTarget using NullWriter()\nINPUT FROM App6_SampleStream;\nEND APPLICATION App6;\nDEPLOY APPLICATION App6 ON ONE IN ServerDG1;\nSTART APPLICATION App6;\n\nSTOP APPLICATION App7;\nUNDEPLOY APPLICATION App7;\nDROP APPLICATION App7 CASCADE;\nCREATE APPLICATION App7;\nCREATE OR REPLACE SOURCE App7_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App7_SampleStream;\nCREATE OR REPLACE TARGET App7_NullTarget using NullWriter()\nINPUT FROM App7_SampleStream;\nEND APPLICATION App7;\nDEPLOY APPLICATION App7 ON ONE IN ServerDG1;\nSTART APPLICATION App7;\n\nSTOP APPLICATION App8;\nUNDEPLOY APPLICATION App8;\nDROP APPLICATION App8 CASCADE;\nCREATE APPLICATION App8;\nCREATE OR REPLACE SOURCE App8_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App8_SampleStream;\nCREATE OR REPLACE TARGET App8_NullTarget using NullWriter()\nINPUT FROM App8_SampleStream;\nEND APPLICATION App8;\nDEPLOY APPLICATION App8 ON ONE IN ServerDG1;\nSTART APPLICATION App8;\n\n\nSTOP APPLICATION App9;\nUNDEPLOY APPLICATION App9;\nDROP APPLICATION App9 CASCADE;\nCREATE APPLICATION App9;\nCREATE OR REPLACE SOURCE App9_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App9_SampleStream;\nCREATE OR REPLACE TARGET App9_NullTarget using NullWriter()\nINPUT FROM App9_SampleStream;\nEND APPLICATION App9;\nDEPLOY APPLICATION App9;\nSTART APPLICATION App9;\n\nSTOP APPLICATION App10;\nUNDEPLOY APPLICATION App10;\nDROP APPLICATION App10 CASCADE;\nCREATE APPLICATION App10;\nCREATE OR REPLACE SOURCE App10_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App10_SampleStream;\nCREATE OR REPLACE TARGET App10_NullTarget using NullWriter()\nINPUT FROM App10_SampleStream;\nEND APPLICATION App10;\nDEPLOY APPLICATION App10;\nSTART APPLICATION App10;\n\nSTOP APPLICATION App11;\nUNDEPLOY APPLICATION App11;\nDROP APPLICATION App11 CASCADE;\nCREATE APPLICATION App11;\nCREATE OR REPLACE SOURCE App11_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App11_SampleStream;\nCREATE OR REPLACE TARGET App11_NullTarget using NullWriter()\nINPUT FROM App11_SampleStream;\nEND APPLICATION App11;\nDEPLOY APPLICATION App11;\nSTART APPLICATION App11;\n\nSTOP APPLICATION App12;\nUNDEPLOY APPLICATION App12;\nDROP APPLICATION App12 CASCADE;\nCREATE APPLICATION App12;\nCREATE OR REPLACE SOURCE App12_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App12_SampleStream;\nCREATE OR REPLACE TARGET App12_NullTarget using NullWriter()\nINPUT FROM App12_SampleStream;\nEND APPLICATION App12;\nDEPLOY APPLICATION App12;\nSTART APPLICATION App12;\n\nSTOP APPLICATION App13;\nUNDEPLOY APPLICATION App13;\nDROP APPLICATION App13 CASCADE;\nCREATE APPLICATION App13;\nCREATE FLOW AgentFlow13;\nCREATE OR REPLACE SOURCE App13_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App13_SampleStream;\nEND FLOW AgentFlow13;\nCREATE FLOW ServerFlow13;\nCREATE OR REPLACE TARGET App13_NullTarget using NullWriter()\nINPUT FROM App13_SampleStream;\nEND FLOW ServerFlow13;\nEND APPLICATION App13;\ndeploy application App13 on any in ServerDG1 with AgentFlow13 on any in Agents, ServerFlow13 on any in ServerDG1;\nSTART APPLICATION App13;", "generated_queries": "What are the steps taken to create, deploy, and start applications App1 through App13, and how are they configured to read from a specific file with a wildcard pattern?", "file_name": "DGAgent.tql"}
{"tql": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n\nCREATE SOURCE @APPNAME@_S USING MSSqlReader\n(\n  Compression: false,\n  cdcRoleName: 'STRIIM_READER',\n  DatabaseName: 'QATEST',\n  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',\n  ConnectionPoolSize: 1,\n  FetchTransactionMetadata: false,\n  StartPosition: 'EOF',\n  Username: 'qatest',\n  SendBeforeImage: true,\n  AutoDisableTableCDC: true,\n  ConnectionURL: 'localhost:1433',\n  Tables: 'qatest.test01',\n  adapterName: 'MSSqlReader',\n  Password: 'w3b@ct10n'\n)\nOUTPUT TO @APPNAME@_SS;\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'NULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:0',\nStandardSQL:true\t,\noptimizedMerge:true\t\n) INPUT FROM @APPNAME@_ss;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\n--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\nSTART APPLICATION @APPNAME@;", "generated_queries": "How can I set up and deploy a new application that reads data from an MSSQL database and writes it to BigQuery with specific configurations?", "file_name": "mssql_bq_optimizedMerge_diff_PK.tql"}
{"tql": "--\n-- Crash Recovery Test 2 on two node cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS\n--\n\nSTOP APPLICATION N2S2CR2Tester.N2S2CRTest2;\nUNDEPLOY APPLICATION N2S2CR2Tester.N2S2CRTest2;\nDROP APPLICATION N2S2CR2Tester.N2S2CRTest2 CASCADE;\nCREATE APPLICATION N2S2CRTest2 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN2S2CRTest2;\n\nCREATE SOURCE CsvSourceN2S2CRTest2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN2S2CRTest2;\n\nCREATE FLOW DataProcessingN2S2CRTest2;\n\nCREATE TYPE WactionTypeN2S2CRTest2 (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionTypeN2S2CRTest2;\n\nCREATE CQ CsvToWaction\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsN2S2CRTest2 CONTEXT OF WactionTypeN2S2CRTest2\nEVENT TYPES ( WactionTypeN2S2CRTest2 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsN2S2CRTest2\nINSERT INTO WactionsN2S2CRTest2\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND FLOW DataProcessingN2S2CRTest2;\n\nEND APPLICATION N2S2CRTest2;", "generated_queries": "What are the steps taken to set up and configure the N2S2CRTest2 application for crash recovery testing, including the data source and processing flow used?", "file_name": "N2S2CRTest2.tql"}
{"tql": "stop application AzureApp;\nundeploy application AzureApp;\ndrop application AzureApp cascade;\n\ncreate application AzureApp\nRECOVERY 10 second interval;\ncreate source CSVSource using FileReader (\n\tdirectory:'@DIR@',\n\tWildCard:'@WILDCARD@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  curr String,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       data[6],\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target BlobT using AzureBlobWriter(\n\taccountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:30,interval:5s'\n)\nformat using AvroFormatter (\n)\ninput from TypedCSVStream;\nend application AzureApp;\ndeploy application AzureApp in default;\nstart application AzureApp;", "generated_queries": "How can I set up an application in Azure to process CSV data and store the results as Avro files in Azure Blob Storage?", "file_name": "FileWAzurewithAvro.tql"}
{"tql": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ recovery 1 second interval;\n\nCREATE SOURCE @SOURCENAME@ USING OracleReader\n(\n    Username: '@SRCUSERNAME@',\n    Password: '@SRCPASSWORD@',\n    ConnectionURL: '@SRCURL',\n    Tables: '@SRCTABLE',\n    FetchSize: '@FETCHSIZE@',\n    CommittedTransactions: true\n)\n\nOUTPUT TO @STREAM@ ;\n\nCREATE TARGET @TARGETNAME@ using DatabaseWriter\n(\n    ConnectionURL: '@TARGETURL',\n    username: '@TARGETUSERNAME@',\n    Password: '@TARGETPASSWORD@',\n    Tables: '@TARGETTABLE@',\n    BatchPolicy:'EventCount:1,Interval:1',\n    CommitPolicy:'EventCount:1,Interval:1'\n)\nINPUT FROM @STREAM@;\n\nEND APPLICATION @APPNAME@;\n\nDEPLOY APPLICATION @APPNAME@;\nSTART APPLICATION @APPNAME@;", "generated_queries": "- How can I stop, undeploy, and then completely remove an application before creating a new one with a recovery interval, as well as set up a source and target for data transfer from an Oracle database?", "file_name": "OracleTemplate.tql"}
{"tql": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@ recovery 5 second Interval;\ncreate source @srcName@ USING MySQLReader\n(\n  Username:'@srcusername@',\n  Password:'@srcpassword@',\n  ConnectionURL:'@srcurl@',\n  Tables:'@srcschema@.@srctable@',\n  sendBeforeImage:'true',\n  FilterTransactionBoundaries:'true'\n) \nOUTPUT TO @outstreamname@;\n\nCREATE OR REPLACE TARGET @tgtName@ USING DatabaseWriter\n(\n  CheckPointTable:'CHKPOINT',\n  Username:'@tgtusername@',\n  Password:'@tgtpassword@',\n  BatchPolicy:'EventCount:1,Interval:0',\n  CommitPolicy:'EventCount:1,Interval:0',\n  ConnectionURL:'@tgturl@',\n  Tables:'@srcschema@.@srctable@,@tgtschema@.@tgttable@'\n) \nINPUT FROM @instreamname@;\n\n\nEnd APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;", "generated_queries": "What steps do I need to follow to set up a data pipeline that reads from a MySQL database and writes to another database using a specific application configuration?", "file_name": "mysqltomysql.tql"}
{"tql": "CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( \n  Username: '@SRC_USERNAME@',\n  Password: '@SRC_PASSWORD@',\n  ConnectionURL: '@CDC_URL@',\n  Tables: '@Source1Tables@',\n  FetchSize: 1) \nOUTPUT TO @APPNAME@cdcStream;", "generated_queries": "What are the configuration details for setting up a CDC (Change Data Capture) source connection, including username, password, connection URL, tables to monitor, and fetch size?", "file_name": "DBWriter_Alter_Oracle.tql"}
{"tql": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@ RECOVERY 1 SECOND INTERVAL;\nCREATE SOURCE @srcName@ USING OracleReader (\n Username: '@srcusername@',\n  Password: '@srcpassword@',\n  ConnectionURL: '@srcurl@',\n  Tables: '@srcschema@.@srctable@'\n)\nOUTPUT TO @outstreamname@;\n\nCREATE OR REPLACE TARGET @tgtName@ USING DatabaseWriter\n(\n  DatabaseProviderType:'Default',\n  CheckPointTable:'CHKPOINT',\n  PreserveSourceTransactionBoundary:'false',\n  Username:'@tgtusername@',\n  BatchPolicy:'EventCount:1,Interval:0',\n  CommitPolicy:'EventCount:1,Interval:0',\n  ConnectionURL:'@tgturl@',\n  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',\n  Password:'@tgtpassword@'\n)\nINPUT FROM @instreamname@;\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;", "generated_queries": "How can I create a new application in the test namespace that pulls data from an Oracle database and writes it to a different database, ensuring recovery and checkpointing are set up correctly?", "file_name": "oracletosqlserver.tql"}
{"tql": "CREATE TARGET @TARGET_NAME@ USING SpannerWriter (\n\tTables: 'QATEST.%,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tinstanceId: 'qatest'\n) INPUT FROM @STREAM@;", "generated_queries": "What steps do I need to take to set up a target in Spanner for writing data from a specific stream, and which tables are involved in this process?", "file_name": "SpannerWriter.tql"}
{"tql": "--\n-- Recovery Test 37 with two sources, two jumping time windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jt1W/p -> CQ1 -> WS\n--   S2 -> Jt2W/p -> CQ2 -> WS\n--\n\nSTOP KStreamRecov37Tester.KStreamRecovTest37;\nUNDEPLOY APPLICATION KStreamRecov37Tester.KStreamRecovTest37;\nDROP APPLICATION KStreamRecov37Tester.KStreamRecovTest37 CASCADE;\n\nDROP USER KStreamRecov37Tester;\nDROP NAMESPACE KStreamRecov37Tester CASCADE;\nCREATE USER KStreamRecov37Tester IDENTIFIED BY KStreamRecov37Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov37Tester;\nCONNECT KStreamRecov37Tester KStreamRecov37Tester;\n\nCREATE APPLICATION KStreamRecovTest37 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP WITHIN 1 SECOND\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP WITHIN 2 SECOND\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nEND APPLICATION KStreamRecovTest37;", "generated_queries": "What are the results from the recovery test concerning the total number of companies and the first merchant ID grouped by merchant ID over different time intervals using the CSV data streams?", "file_name": "KStreamRecovTest37.tql"}
{"tql": "--\n-- Recovery Test 23 with two sources, two sliding time windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> St1W -> CQ1 -> WS\n-- S2 -> St2W -> CQ2 -> WS\n--\n\nSTOP KStreamRecov23Tester.KStreamRecovTest23;\nUNDEPLOY APPLICATION KStreamRecov23Tester.KStreamRecovTest23;\nDROP APPLICATION KStreamRecov23Tester.KStreamRecovTest23 CASCADE;\nDROP USER KStreamRecov23Tester;\nDROP NAMESPACE KStreamRecov23Tester CASCADE;\nCREATE USER KStreamRecov23Tester IDENTIFIED BY KStreamRecov23Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov23Tester;\nCONNECT KStreamRecov23Tester KStreamRecov23Tester;\n\nCREATE APPLICATION KStreamRecovTest23 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP WITHIN 1 SECOND;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP WITHIN 2 SECOND;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2;\n\nEND APPLICATION KStreamRecovTest23;", "generated_queries": "What steps are involved in setting up a recovery test application with multiple data sources, and how is the data processed and stored in the system?", "file_name": "KStreamRecovTest23.tql"}
{"tql": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@ recovery 1 second interval;\n\ncreate source @APPNAME@_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using CobolCopybookParser (\ncopybookFileName : '@TD@/@PROP1@',\n  dataFileFont: '@PROP2@',\n  copybookSplit: '@PROP3@',\n  dataFileOrganization: '@PROP4@',\n  copybookDialect: '@PROP5@', \n  skipIndent:'@PROP6@',\n  DatahandlingScheme:'@PROP7@'\n  --recordSelector: '@PROP8@'\n)\nOUTPUT TO @APPNAME@Stream;\n\ncreate Target @APPNAME@Target using FileWriter(\n    filename :'@FILE@',\n    directory : '@FOLDER@'\n)\nformat using JsonFormatter (\n)\ninput from @APPNAME@Stream;\n\nCREATE TYPE test_type (\n account_no com.fasterxml.jackson.databind.JsonNode,\n first_name com.fasterxml.jackson.databind.JsonNode,\n last_name com.fasterxml.jackson.databind.JsonNode,\n addr1 com.fasterxml.jackson.databind.JsonNode,\nAddr2 com.fasterxml.jackson.databind.JsonNode,\nCity com.fasterxml.jackson.databind.JsonNode,\nState com.fasterxml.jackson.databind.JsonNode,\nZip com.fasterxml.jackson.databind.JsonNode\n);\n\nCreate stream cqAsJSONNodeStream of test_type;\n\nCREATE CQ GetPOAsJsonNodes\nINSERT into cqAsJSONNodeStream\n    select \n    data.get('ACCTS-RECORD').get('ACCOUNT-NO'),\ndata.get('ACCTS-RECORD').get('NAME').get('FIRST-NAME'),\ndata.get('ACCTS-RECORD').get('NAME').get('LAST-NAME'),\ndata.get('ACCTS-RECORD').get('ADDRESS1'),\ndata.get('ACCTS-RECORD').get('ADDRESS2'),\ndata.get('ACCTS-RECORD').get('ADDRESS3').get('CITY'),\ndata.get('ACCTS-RECORD').get('ADDRESS3').get('STATE'),\ndata.get('ACCTS-RECORD').get('ADDRESS3').get('ZIP-CODE')\nfrom @APPNAME@Stream js;\n\ncreate type finaldtype(\n      ACCOUNT_NO String, \n      FIRST_NAME String,\n      LAST_NAME String,\n      ADDRESS1 String,\n      ADDRESS2 String,\n      CITY String,\n      STATE String,\n      ZIP_CODE String\n);\n\nCREATE CQ getdata\nINSERT into getdataStream\n    select account_no.toString(),\n    first_name.toString(),\n    last_name.toString(),\n    addr1.toString(),\n    Addr2.toString(),\n    City.toString(),\n    State.toString(),\n    Zip.toString()\nfrom cqAsJSONNodeStream x;\n\ncreate Target @APPNAME@DBTarget using DatabaseWriter(\n  Username: 'qatest',\n  Password: 'qatest',\n  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',\n  BatchPolicy: 'EventCount:10000',\n  CommitPolicy: 'EventCount:10000',\n  Tables: 'QATEST.@APPNAME@'\n)\ninput from getdataStream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@ on all in default;\nstart application @APPNAME@;", "generated_queries": "How can I deploy an application that reads data from a file using a Cobol copybook, processes it into JSON format, and then stores the results in an Oracle database?", "file_name": "cobolparserACCT.tql"}
{"tql": "stop application AzureDLSGen1_sanity;\nundeploy application AzureDLSGen1_sanity;\ndrop application AzureDLSGen1_sanity cascade;\n\n\ncreate application AzureDLSGen1_sanity recovery 5 Second interval;\ncreate source CSVSource using FileReader (\n        directory:'./Samples/AppData/',\n        WildCard:'dynamicdirectory.csv',\n        positionByEOF:false,\n        charset:'UTF-8'\n)\nparse using DSVParser (\n        header:'no'\n)\nOUTPUT TO CsvStream;\n\ncreate Target WriteToADLSGen1 using ADLSGen1Writer(\n        filename:'',\n        directory:'',\n        datalakestorename:'',\n        clientid:'',\n        authtokenendpoint:'',\n        clientkey:'',\n\t\trolloverpolicy:'eventcount:100000'\n)\nformat using DSVFormatter (\n)\ninput from CsvStream; \n\nend application AzureDLSGen1_sanity;\n\ndeploy application AzureDLSGen1_sanity;\nstart application AzureDLSGen1_sanity;", "generated_queries": "What steps are involved in setting up and deploying the AzureDLSGen1_sanity application to read data from a CSV file and write it to Azure Data Lake Storage Gen1?", "file_name": "AzureDLS_GenRecovery.tql"}
{"tql": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\ncreate application @appname@ recovery 1 second interval;\n\nCREATE SOURCE @parquetsrc@ USING FileReader (\n  directory: '',\n  positionByEOF: false,\n  WildCard: '' )\nPARSE USING ParquetParser (\n )\nOUTPUT TO @appname@Streams;\n\nCREATE OR REPLACE CQ @appname@CQOrder3\nINSERT INTO @appname@Stream3\nSELECT\nPUTUSERDATA(s,'schemaName',s.data.getSchema().getName())\nFROM @appname@Streams s;\n\nCREATE TARGET @adlstarget@ USING Global.ADLSGen2Writer (\n    accountname:'',\n  \tsastoken:'',\n  \tfilesystemname:'',\n  \tfilename:'',\n  \tdirectory:'',\n  \tuploadpolicy:'eventcount:10' )\n\nformat using AvroFormatter (\n)\nINPUT FROM @appname@Stream3;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;", "generated_queries": "What steps are involved in creating, deploying, and starting a new streaming application that processes Parquet files and writes the output to ADLS Gen2 using Avro format?", "file_name": "FileParquetToGen2Avro.tql"}
{"tql": "stop ORAToBigquery;\nundeploy application ORAToBigquery;\ndrop application ORAToBigquery cascade;\n\nCREATE APPLICATION ORAToBigquery RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE Rac11g USING OracleReader ( \n  SupportPDB: false,\n  SendBeforeImage: true,\n  ReaderType: 'LogMiner',\n  CommittedTransactions: false,\n  FetchSize: 1,\n  Password: 'manager',\n  DDLTracking: false,\n  StartTimestamp: 'null',\n  OutboundServerProcessName: 'WebActionXStream',\n  OnlineCatalog: true,\n  ConnectionURL: '192.168.33.10:1521/XE',\n  SkipOpenTransactions: false,\n  Compression: false,\n  QueueSize: 40000,\n  RedoLogfiles: 'null',\n  Tables: 'SYSTEM.GGAUTHORIZATIONS',\n  Username: 'system',\n  FilterTransactionBoundaries: true,\n  adapterName: 'OracleReader',\n  XstreamTimeOut: 600,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=60, maxRetries=3'\n ) \nOUTPUT TO DataStream;\n\nCREATE OR REPLACE TARGET Target1 USING SysOut ( \n  name: \"dstream\"\n ) \nINPUT FROM DataStream;\n\nCREATE OR REPLACE TARGET Target2 using BigqueryWriter(\n  BQServiceAccountConfigurationPath:\"/Users/ravipathak/Downloads/big-querytest-1963ae421e90.json\",\n  projectId:\"big-querytest\",\n  Tables: \"SYSTEM.GGAUTHORIZATIONS,testing1.ggauthorisation\",\n  parallelismCount: 2,\n  BatchPolicy: \"eventCount:100000,Interval:0\")\nINPUT FROM DataStream;\n\nEND APPLICATION ORAToBigquery;\n\ndeploy application ORAToBigquery;\nstart ORAToBigquery;", "generated_queries": "What are the steps involved in deploying and configuring the ORAToBigquery application to transfer data from an Oracle source to BigQuery?", "file_name": "template.tql"}
{"tql": "CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING DatabaseReader (\n  Tables: '',\n  ConnectionURL: '',\n  Password: '',\n  Username: ''\n  )\nOUTPUT TO @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nEND APPLICATION @APPNAME@;", "generated_queries": "What is the process to create a new data application that reads from a database and outputs the data to an S3 bucket in JSON format?", "file_name": "OracleILToS3.tql"}
{"tql": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@ Using DatabaseReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'qatest.@SourceTable@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;", "generated_queries": "\"What steps are involved in stopping, undeploying, and recreating the application @APPNAME@ with a recovery interval, including details about the source and target database configurations?\"", "file_name": "PostgresDBR.tql"}
{"tql": "stop ADW;\nundeploy application ADW;\nDROP APPLICATION ADW CASCADE;\nCREATE APPLICATION ADW recovery 5 second interval;\nCreate Source OracleSource Using OracleReader\n(\n Username:'@ORACLE-USERNAME',\n Password:'@ORACLE-PASSWORD',\n ConnectionURL: '@ORACLE-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'@FETCH-SIZE@'\n) \nOutput To str;\n\n\ncreate target AzureTarget using AzureSQLDWHWriter (\n\t\tConnectionURL: '@SQLDW-URL@',\n        username: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@',\n        uploadpolicy:'@EVENT-COUNT@'\n) INPUT FROM str;\n\ncreate Target t2 using SysOut(name:Foo2) input from str;\n\nEND APPLICATION ADW;\ndeploy application ADW;\nstart application ADW;", "generated_queries": "What steps are involved in setting up and deploying the ADW application to transfer data from an Oracle database to an Azure SQL Data Warehouse?", "file_name": "OracleToAzure.tql"}
{"tql": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCreate Source @SourceName1@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM1@;\nCreate Source @SourceName2@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM2@;\nCreate Source @SourceName3@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM3@;\nCreate Source @SourceName4@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM4@;\nCreate Source @SourceName5@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM5@;\nCreate Source @SourceName6@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM6@;\nCreate Source @SourceName7@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM7@;\nCreate Source @SourceName8@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM8@;\nCreate Source @SourceName9@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM9@;\nCreate Source @SourceName10@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM10@;\n\nCREATE TARGET @targetName1@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM1@;\nCREATE TARGET @targetName2@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM2@;\nCREATE TARGET @targetName3@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM3@;\nCREATE TARGET @targetName4@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM4@;\nCREATE TARGET @targetName5@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM5@;\nCREATE TARGET @targetName6@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM6@;\nCREATE TARGET @targetName7@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM7@;\nCREATE TARGET @targetName8@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM8@;\nCREATE TARGET @targetName9@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM9@;\nCREATE TARGET @targetName10@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM10@;\n\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;", "generated_queries": "How can I create and deploy an application that connects to multiple sources using Ojet while writing to an Oracle database?", "file_name": "OjetALMMultipleReader.tql"}
{"tql": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@ Using PostgreSQLReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'qatest.@SourceTable@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;", "generated_queries": "How do I create, deploy, and start an application that reads from a PostgreSQL source and writes to both a database and the system output?", "file_name": "PostgresCDC.tql"}
{"tql": "--\n-- Recovery Test T20\n-- Nicholas Keene, WebAction, Inc.\n--\n-- Snum -> CQ -> WS\n--\n\n\nUNDEPLOY APPLICATION NameT20.T20;\nDROP APPLICATION NameT20.T20 CASCADE;\nCREATE APPLICATION T20;\n\n\n\n\nCREATE FLOW DataAcquisitionT20;\n\n\nCREATE SOURCE CsvSourceT20 USING NumberSource ( \n  lowValue: '1',\n  highValue: '1003',\n  delayMillis: '10',\n  delayNanos: '0',\n  repeat: 'false'\n ) \nOUTPUT TO OutputStreamT20;\n\n\nEND FLOW DataAcquisitionT20;\n\n\n\n\nCREATE FLOW DataProcessingT20;\n\n\nCreate Target OutputTargetT20\nUsing Sysout (name: 'OutputTargetT20')\nInput From OutputStreamT20;\n\n\nEND FLOW DataProcessingT20;\n\n\n\nEND APPLICATION T20;", "generated_queries": "What steps are involved in the creation and configuration of the T20 application, including data acquisition and processing?", "file_name": "T20.tql"}
{"tql": "stop application GGTrailReaderApp;\nundeploy application GGTrailReaderApp;\ndrop application GGTrailReaderApp cascade;\n\ncreate application GGTrailReaderApp recovery 5 second interval;\n\ncreate source GGTrailSource using GGTrailReader (\ntRaildIrectory:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario1',\ntRAilfilepattern:'n1*',\npositionByEOF:false,\nFilterTransactionBoundaries: true,\nDefinitionFile:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario1/Scn1_beforeddl.def',\ncaptureCDdl: true,\nCDDLAction:'Process',\n--CDDLAction:'Ignore',\nTrailByTeOrder:'LittleEndian',\nrecoveryInterval: 5\n)\nOUTPUT TO GGTrailStream;\n\ncreate Target t2 using SysOut(name:Foo2) input from GGTrailStream;\n\nCREATE TARGET WriteCDCOracle1 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost/orcl',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:1,Interval:1',\nCommitPolicy:'Eventcount:1,Interval:1',\nCheckpointtable:'RGRN_CHKPOINT',\nTables:'QATEST.GGDDL1,QATEST.GGDDL1_TGT'\n) INPUT FROM GGTrailStream1;\n\n\nend application GGTrailReaderApp;\n\ndeploy application GGTrailReaderApp;\nstart application GGTrailReaderApp;", "generated_queries": "What steps are needed to stop, undeploy, drop, and then recreate the GGTrailReaderApp application with a specific source and target configuration in order to process trail files from a specified directory?", "file_name": "scn1.tql"}
{"tql": "DROP APPLICATION ns1.OPExample cascade;\nDROP NAMESPACE ns1 cascade;\nCREATE OR REPLACE NAMESPACE ns1;\nUSE ns1;\nCREATE APPLICATION OPExample;\n\nCREATE source CsvDataSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'PosDataPreview.csv',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n)\nOUTPUT TO CsvStream;\n \nCREATE TYPE MerchantHourlyAve(\n  merchantId String,\n  hourValue integer,\n  hourlyAve integer\n);\n\nCREATE CACHE HourlyAveLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'hourlyData.txt'\n)\nPARSE USING DSVParser (\n  header: Yes,\n  trimquote:false,\n  trimwhitespace:true\n) \nQUERY (keytomap:'merchantId') \nOF MerchantHourlyAve;\n\nCREATE CQ CsvToPosData\nINSERT INTO PosDataStream partition by merchantId\nSELECT TO_STRING(data[1]) as merchantId,\n  TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\n  DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,\n  TO_DOUBLE(data[7]) as amount,\n  TO_INT(data[9]) as zip\nFROM CsvStream;\n \nCREATE CQ cq2\nINSERT INTO SendToOPStream\nSELECT makeList(dateTime) as dateTime,\n  makeList(zip) as zip\nFROM PosDataStream;\n \nCREATE TYPE ReturnFromOPStream_Type ( time DateTime , val Integer );\nCREATE STREAM ReturnFromOPStream OF ReturnFromOPStream_Type;\n\nCREATE TARGET OPExampleTarget \nUSING FileWriter (filename: 'OPExampleOut') \nFORMAT USING JSONFormatter() \nINPUT FROM ReturnFromOPStream;\n\nCREATE OPEN PROCESSOR testOp USING Global.TupleConverter ( lastItemSeen: 0, ahead: 1 )\nINSERT INTO ReturnFromOPStream FROM SendToOPStream ENRICH WITH HourlyAveLookup;\n \nEND APPLICATION OPExample;", "generated_queries": "How can I set up a data processing application to read CSV data, enrich it with hourly averages from a lookup file, and output the results in JSON format?", "file_name": "op2.tql"}
{"tql": "create application FileXML;\ncreate source XMLSource using FileReader (\n\tDirectory:'@TEST-DATA-PATH@',\n\tWildCard:'books.xml',\n\tpositionByEOF:false\n)\nparse using XMLParser (\n\tRootNode:'/catalog/book'\n)\nOUTPUT TO XmlStream;\n\n-- Below Sysout is added to test DEV-23437.  Not directly validated in the test except the App should not crash with sysout target\nCREATE TARGET XMLEventSYSout USING sysout  (\nname: 'XMLEventSYSoutOut' )\nINPUT FROM XmlStream;\n\ncreate Target XMLDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/xmldata') input from XmlStream;\nend application FileXML;", "generated_queries": "What operations are being performed in the FileXML application, and how is the XML data being processed and outputted?", "file_name": "FileReaderWithXMLParser.tql"}
{"tql": "CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE OR REPLACE SOURCE @APPNAME@DataSrc USING OracleReader (\n  Tables: '',\n  ConnectionURL: '',\n  Password: '',\n  Username: ''\n)\nOUTPUT TO @APPNAME@DataStream;\n\nCREATE OR REPLACE TARGET @APPNAME@DataTrgt USING MongoDBWriter (\n  ConnectionURL: '',\n  Username: '',\n  Password: '',\n  collections: ''\n  AuthDB: '',\n  batchpolicy: 'EventCount:1000, Interval:30',\n )\nINPUT FROM @APPNAME@DataStream;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING MongoDBReader (\n  ConnectionURL: '',\n  Username: '',\n  password: '',\n  authDB: '',\n  collections: '',\n  mode: 'Incremental'\n  )\nOUTPUT TO @APPNAME@stream;\n\nCREATE CQ @APPNAME@CQ\nINSERT INTO @APPNAME@CQSTREAM\nSELECT data.get(\"NUM_COL\").toString() AS NUM_COL,\n  data.get(\"CHAR_COL\").toString() AS CHAR_COL,\n  data.get(\"VARCHAR2_COL\").toString() AS VARCHAR2_COL,\n  data.get(\"FLOAT_COL\").toString() AS FLOAT_COL,\n  data.get(\"BINARY_FLOAT_COL\").toString() AS BINARY_FLOAT_COL,\n  data.get(\"BINARY_DOUBLE_COL\").toString() AS BINARY_DOUBLE_COL,\n  data.get(\"DATE_COL\").toString() AS DATE_COL,\n  data.get(\"TIMESTAMP_COL\").toString() AS TIMESTAMP_COL\nFROM @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING AvroFormatter (\nschemaFileName: '@SCHEMAFILE@'\n)\nINPUT FROM @APPNAME@CQSTREAM;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING DSVFormatter ()\nINPUT FROM @APPNAME@CQSTREAM;\n\nEND APPLICATION @APPNAME@;", "generated_queries": "How can I set up a data integration process that reads from an Oracle database, processes the data, and then writes the results to an S3 bucket in multiple formats including Avro, JSON, and DSV?", "file_name": "MongoToS3MutliTargetsAvro.tql"}
{"tql": "stop application AzureApp;\nundeploy application AzureApp;\ndrop application AzureApp cascade;\n\ncreate application AzureApp\nRECOVERY 10 second interval;\nCREATE SOURCE OracleSource USING OracleReader\n(\n    Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n Tables:'@TABLES@',\n    FetchSize: 1\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  tablename String,\n  data java.util.HashMap  \n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT TO_LOWER(META(s, \"TableName\").toString()) as tablename,\n       DATA(s) as data\nFROM CsvStream s;\n\ncreate Target BlobT using AzureBlobWriter(\n\taccountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:5,interval:5s'\n)\nformat using AvroFormatter (\n)\ninput from TypedCSVStream;\nend application AzureApp;\ndeploy application AzureApp in default;\nstart application AzureApp;", "generated_queries": "What are the steps involved in setting up an application to read data from an Oracle source, transform it to a CSV format, and then write it to an Azure Blob storage?", "file_name": "OracleAzurewithAvroLoad.tql"}
{"tql": "STOP APPLICATION @appname@routerApp;\nUNDEPLOY APPLICATION @appname@routerApp;\nDROP APPLICATION @appname@routerApp CASCADE;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',\n  acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\nCREATE APPLICATION @appname@routerApp RECOVERY 10 SECOND INTERVAL;\n\nCREATE  SOURCE @appname@OraSource USING OracleReader  (\nUsername: 'qatest',\nPassword: 'qatest',\nConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',\nTables: 'QATEST.TGT_T%',\n FetchSize:'100'\n)\nOUTPUT TO @appname@MasterStream1;\n\n-- CREATE STREAM @appname@ss1 OF Global.waevent persist using Global.DefaultKafkaProperties;\n-- CREATE STREAM @appname@ss2 OF Global.waevent persist using Global.DefaultKafkaProperties;\n-- CREATE STREAM @appname@ss3 OF Global.waevent persist using Global.DefaultKafkaProperties;\n\nCREATE STREAM @appname@ss1 OF Global.waevent PERSIST USING KafkaPropset;\nCREATE STREAM @appname@ss2 OF Global.waevent PERSIST USING KafkaPropset;\nCREATE STREAM @appname@ss3 OF Global.waevent PERSIST USING KafkaPropset;\n\nCREATE OR REPLACE ROUTER @appname@tablerouter1 INPUT FROM @appname@MasterStream1 s CASE\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T1' THEN ROUTE TO @appname@ss1,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T2' THEN ROUTE TO @appname@ss2,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T3' THEN ROUTE TO @appname@ss3,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T4' THEN ROUTE TO @appname@ss4,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T5' THEN ROUTE TO @appname@ss5,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T6' THEN ROUTE TO @appname@ss6,\nELSE ROUTE TO @appname@ss_else;\n\ncreate Target @appname@FileTarget_1 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n)\nFORMAT USING dsvFormatter ()\ninput from @appname@ss1;\n\ncreate Target @appname@FileTarget_2 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n)\nFORMAT USING dsvFormatter ()\ninput from @appname@ss2;\n\ncreate Target @appname@FileTarget_3 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n\n)\nFORMAT USING dsvFormatter ()\ninput from @appname@ss3;\n\nCREATE OR REPLACE TARGET @appname@KafkaTarget_4 USING KafkaWriter VERSION '0.11.0' (\n  brokerAddress: 'localhost:9092',\n  Topic: 'target4'\n )\nFORMAT USING JSONFormatter  (\n )\nINPUT FROM @appname@ss4;\n\nCREATE OR REPLACE TARGET @appname@KafkaTarget_5 USING KafkaWriter VERSION '0.11.0' (\n  brokerAddress: 'localhost:9092',\n  Topic: 'target5'\n )\nFORMAT USING JSONFormatter  (\n )\nINPUT FROM @appname@ss5;\n\nCREATE OR REPLACE TARGET @appname@KafkaTarget_6 USING KafkaWriter VERSION '0.11.0' (\n  brokerAddress: 'localhost:9092',\n  Topic: 'target6'\n )\nFORMAT USING JSONFormatter  (\n )\nINPUT FROM @appname@ss6;\n\n\n\n\nend application @appname@routerApp;\ndeploy application @appname@routerApp;\nstart @appname@routerApp;", "generated_queries": "What steps were taken to set up the router application, including data sources, streaming configurations, and output targets?", "file_name": "routerToKafka.tql"}
{"tql": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;\n\nCreate Source @SourceName@ Using Ojet\n\n(\n  Username:'c##qatest',\n  Password:'qatest',\n  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',\n  Tables:'CDB$ROOT.\"C##QATEST\".ojet_src;ORCLPDB.QATEST.ojet_src',\n  _h_useClassic:false,\n  Fetchsize:1,\n  Compression: true,\n  SupportPDB:true,\n  ReplicationSlotName:'null'\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter\n(\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',\n  Username:'c##qatest',\n  Password:'qatest',\n  BatchPolicy:'EventCount:1,Interval:1',\n  Tables:'CDB$ROOT.\"C##QATEST\".ojet_src,CDB$ROOT.\"C##QATEST\".ojet_tgt'\n) INPUT FROM @SRCINPUTSTREAM@;\n\n\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;", "generated_queries": "How can I set up an application in Oracle that deploys a data replication process from a source database to a target database using Oracle GoldenGate?", "file_name": "Ojet_DBWPDBCDB1.tql"}
{"tql": "CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING DataBaseReader (\n  Tables: '',\n  ConnectionURL: '',\n  Password: '',\n  Username: ''\n  )\nOUTPUT TO @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nEND APPLICATION @APPNAME@;", "generated_queries": "How can I set up an application that reads data from a database, processes it, and writes the output in JSON format to two different S3 buckets?", "file_name": "OracleILToS3_2Targets.tql"}
{"tql": "create application KinesisTest;\ncreate source CSVSource using FileReader (\n\tdirectory:'/home/dz/src/product/Samples/AppData',\n\tWildCard:'posdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  companyName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0], data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate or replace Target t using KinesisWriter (\n\tregionName:'TARGET_REGION',\n\tstreamName:'TARGET_STREAM'\n)\nformat using DSVFormatter (\n)\ninput from TypedCSVStream;\nend application KinesisTest;\ndeploy application KinesisTest in default;\nstart application KinesisTest;", "generated_queries": "What data is being processed and how is it transformed before sending to the Kinesis stream in the KinesisTest application?", "file_name": "KinesisTest.tql"}
{"tql": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE application @APPNAME@ @Recovery@ AUTORESUME MAXRETRIES 2 RETRYINTERVAL 10;\n\ncreate type @APPNAME@type1(\n  companyName String,\n  merchantId String,\n  city string\n);\n\ncreate type @APPNAME@type2(\n  c1 integer,\n  c2 String,\n  c3 string\n);\n\ncreate type @APPNAME@type3(\nc1 integer\n);\n\ncreate type @APPNAME@type4(\nc1 integer,\nc2 integer\n);\n\ncreate stream @APPNAME@in_memory_typedStream of @APPNAME@type1 partition by city;\ncreate stream @APPNAME@in_memory_typedStream_num of @APPNAME@type2;\ncreate stream @APPNAME@in_memory_typedStream_num1 of @APPNAME@type2;\ncreate stream @APPNAME@in_memory_typedStream_num2 of @APPNAME@type2;\ncreate stream @APPNAME@in_memory_typedStream_num3 of @APPNAME@type2;\ncreate stream @APPNAME@in_memory_typedStream_num4 of @APPNAME@type2;\ncreate stream @APPNAME@in_memory_typedStream_num5 of @APPNAME@type2;\ncreate stream @APPNAME@finalstream6 of @APPNAME@type4;\n\ncreate source @APPNAME@s using FileReader (\n        directory:'Product/IntegrationTests/TestData/',\n        wildcard:'posdata5L.csv',\n        positionByEOF:false,\n        Charset:'UTF-8'\n)\nPARSE USING DSVParser (\n  columndelimiter:',',\n  ignoreemptycolumn:'Yes',\n  quoteset:'[]~\"',\n  header: true,\n  separator:'~'\n\n)\nOUTPUT TO @APPNAME@in_memory_rawStream;\n\n\ncreate CQ @APPNAME@cq1\nINSERT INTO @APPNAME@kps_waevent\nSELECT *\nFROM @APPNAME@in_memory_rawStream  ;\n\ncreate CQ @APPNAME@cq2\nINSERT INTO @APPNAME@in_memory_typedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\"),\nTO_STRING(data[1]),\nTO_STRING(data[10])\nFROM @APPNAME@kps_waevent ;\n\ncreate CQ @APPNAME@cq3\nINSERT INTO @APPNAME@in_memory_typedStream_num1\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\nFROM @APPNAME@in_memory_typedStream;\n-- order by c3;\n\ncreate CQ @APPNAME@cq4\nINSERT INTO @APPNAME@in_memory_typedStream_num2\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\nFROM @APPNAME@in_memory_typedStream;\n\ncreate CQ @APPNAME@cq5\nINSERT INTO @APPNAME@in_memory_typedStream_num3\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\nFROM @APPNAME@in_memory_typedStream;\n\ncreate CQ @APPNAME@cq6\nINSERT INTO @APPNAME@in_memory_typedStream_num4\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\nFROM @APPNAME@in_memory_typedStream;\n\ncreate CQ @APPNAME@cq7\nINSERT INTO @APPNAME@in_memory_typedStream_num5\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\nFROM @APPNAME@in_memory_typedStream;\n\nCREATE CQ @APPNAME@cq8\nINSERT INTO @APPNAME@in_memory_typedStream_num6\nSELECT TO_INT(companyName) as c1\nFROM @APPNAME@in_memory_typedStream;\n\n\nCREATE JUMPING WINDOW @APPNAME@DataStream1_100000Rows\nOVER @APPNAME@in_memory_typedStream_num1 KEEP 100000 ROWS;\n\n\nCREATE JUMPING WINDOW @APPNAME@DataStream2_100000Rows\nOVER @APPNAME@in_memory_typedStream_num2 KEEP 100000 ROWS;\n\n\nCREATE JUMPING WINDOW @APPNAME@DataStream3_100000Rows\nOVER @APPNAME@in_memory_typedStream_num3 KEEP 100000 ROWS;\n\n\nCREATE JUMPING WINDOW @APPNAME@DataStream4_100000Rows\nOVER @APPNAME@in_memory_typedStream_num4 KEEP 100000 ROWS;\n\n\nCREATE JUMPING WINDOW @APPNAME@DataStream5_100000Rows\nOVER @APPNAME@in_memory_typedStream_num5 KEEP 100000 ROWS;\n\nCREATE JUMPING WINDOW @APPNAME@DataStream6_100000Rows\nOVER @APPNAME@in_memory_typedStream_num6 KEEP 100000 ROWS;\n\ncreate CQ @APPNAME@cq9\nINSERT INTO @APPNAME@finalstream1\nSELECT c1 FROM @APPNAME@DataStream1_100000Rows sample by c1;\n\ncreate CQ @APPNAME@cq10\nINSERT INTO @APPNAME@finalstream2\nSELECT c1 FROM @APPNAME@DataStream2_100000Rows sample by c1 selectivity 0.1;\n\ncreate CQ @APPNAME@cq11\nINSERT INTO @APPNAME@finalstream3\nSELECT c1 FROM @APPNAME@DataStream3_100000Rows sample by c1 selectivity 0.25;\n\n\ncreate CQ @APPNAME@cq12\nINSERT INTO @APPNAME@finalstream4\nSELECT c1 FROM @APPNAME@DataStream4_100000Rows sample by c1 selectivity 0.05;\n--SELECT count(*) FROM @APPNAME@DataStream4Rows10000Seconds sample by c1 selectivity 0.05;\n\ncreate CQ @APPNAME@cq13\nINSERT INTO @APPNAME@finalstream5\nSELECT c1 FROM @APPNAME@DataStream5_100000Rows sample by c1 selectivity 0.01;\n\ncreate CQ @APPNAME@cq14\nINSERT INTO @APPNAME@finalstream6\nSELECT c1,c1 as c2 FROM @APPNAME@DataStream6_100000Rows sample by c1,c2 selectivity 0.01;\n\ncreate target @APPNAME@target1 using filewriter (\nfilename:'FEATURE-DIR/logs/@APPNAME@target1.log',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000,sequence:00'\n)\nformat using dsvFormatter()\ninput from @APPNAME@finalstream1;\n\ncreate target @APPNAME@target2 using filewriter (\nfilename:'FEATURE-DIR/logs/@APPNAME@target2.log',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000,sequence:00'\n)\nformat using dsvFormatter()\ninput from @APPNAME@finalstream2;\n\ncreate target @APPNAME@target3 using filewriter (\nfilename:'FEATURE-DIR/logs/@APPNAME@target3.log',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000,sequence:00'\n)\nformat using dsvFormatter()\ninput from @APPNAME@finalstream3;\n\ncreate target @APPNAME@target4 using filewriter (\nfilename:'FEATURE-DIR/logs/@APPNAME@target4.log',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000,sequence:00'\n)\nformat using dsvFormatter()\ninput from @APPNAME@finalstream4;\n\ncreate target @APPNAME@target5 using filewriter (\nfilename:'FEATURE-DIR/logs/@APPNAME@target5.log',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000,sequence:00'\n)\nformat using dsvFormatter()\ninput from @APPNAME@finalstream5;\n\nCREATE WACTIONSTORE @APPNAME@Wactions1 CONTEXT OF @APPNAME@type3\nEVENT TYPES ( @APPNAME@type2 )\nUSING ( storageProvider:'elasticsearch' );\n\nCREATE WACTIONSTORE @APPNAME@Wactions2 CONTEXT OF @APPNAME@type3\nEVENT TYPES ( @APPNAME@type2 )\nUSING ( storageProvider:'elasticsearch' );\n\nCREATE WACTIONSTORE @APPNAME@Wactions3 CONTEXT OF @APPNAME@type3\nEVENT TYPES ( @APPNAME@type2 )\nUSING ( storageProvider:'elasticsearch' );\n\nCREATE WACTIONSTORE @APPNAME@Wactions4 CONTEXT OF @APPNAME@type4\nEVENT TYPES ( @APPNAME@type4 )\nUSING ( storageProvider:'elasticsearch' );\n\nCREATE WACTIONSTORE @APPNAME@Wactions5 CONTEXT OF @APPNAME@type4\nEVENT TYPES ( @APPNAME@type4 )\nUSING ( storageProvider:'elasticsearch' );\n\n--sampling twice: one in finalstream1 and another in select query.\nCREATE CQ @APPNAME@cq15\nINSERT INTO @APPNAME@Wactions1\nSELECT FIRST(p.c1) FROM @APPNAME@finalstream1 p GROUP BY p.c1 sample by p.c1 ;\n\n--sampling once: results will be same as target2 and target1.\nCREATE CQ @APPNAME@cq16\nINSERT INTO @APPNAME@Wactions2\nSELECT * from @APPNAME@finalstream1 order by c1 desc limit 10000 ;\n\n--sampling twice: one in finalstream1 and another in select query.\nCREATE CQ @APPNAME@cq17\nINSERT INTO @APPNAME@Wactions3\nSELECT * from @APPNAME@finalstream1 order by c1 sample by c1;\n\n--sampling using 2 fields, 2800 for single field and 332 for 2 field\nCREATE CQ @APPNAME@cq18\nINSERT INTO @APPNAME@Wactions4\nSELECT c1,c1 from @APPNAME@finalstream6 order by c1 sample by c1;\n\n--same as Wactions4 - here selectivity alone varies, so output is 8\nCREATE CQ @APPNAME@cq19\nINSERT INTO @APPNAME@Wactions5\nSELECT c1,c1 from @APPNAME@finalstream6 order by c1 sample by c1 selectivity 0.0001;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\n--start @APPNAME@;", "generated_queries": "What steps do I need to follow to undeploy and recreate the application with new data streams and types in the specified target directory?", "file_name": "SampleByClause.tql"}
{"tql": "--\n-- Recovery Test 22 with two sources, two sliding attribute windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Sa5W -> CQ1 -> WS\n-- S2 -> Sa6W -> CQ2 -> WS\n--\n\nSTOP KStreamRecov22Tester.KStreamRecovTest22;\nUNDEPLOY APPLICATION KStreamRecov22Tester.KStreamRecovTest22;\nDROP APPLICATION KStreamRecov22Tester.KStreamRecovTest22 CASCADE;\nDROP USER KStreamRecov22Tester;\nDROP NAMESPACE KStreamRecov22Tester CASCADE;\nCREATE USER KStreamRecov22Tester IDENTIFIED BY KStreamRecov22Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov22Tester;\nCONNECT KStreamRecov22Tester KStreamRecov22Tester;\n\nCREATE APPLICATION KStreamRecovTest22 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2;\n\nEND APPLICATION KStreamRecovTest22;", "generated_queries": "What are the steps taken to create a recovery application that processes CSV data from two sources and stores results in a wactionstore, with specific configurations for windowing and user permissions?", "file_name": "KStreamRecovTest22.tql"}
{"tql": "--\n-- Recovery Test 36 with two sources, two jumping attribute windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Ja5W/p -> CQ1 -> WS\n-- S2 -> Ja6W/p -> CQ2 -> WS\n--\n\nSTOP Recov36Tester.RecovTest36;\nUNDEPLOY APPLICATION Recov36Tester.RecovTest36;\nDROP APPLICATION Recov36Tester.RecovTest36 CASCADE;\n\nDROP USER KStreamRecov36Tester;\nDROP NAMESPACE KStreamRecov36Tester CASCADE;\nCREATE USER KStreamRecov36Tester IDENTIFIED BY KStreamRecov36Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov36Tester;\nCONNECT KStreamRecov36Tester KStreamRecov36Tester;\n\nCREATE APPLICATION KStreamRecovTest36 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nEND APPLICATION KStreamRecovTest36;", "generated_queries": "What is the process for creating a recovery test application that processes CSV data from two different sources and performs aggregations using jumping windows on a specified key?", "file_name": "KStreamRecovTest36.tql"}
