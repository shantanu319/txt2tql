{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdbbbc71-f7a3-4ca9-a7ee-2773fe64b986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (1.55.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from openai) (0.28.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from openai) (0.8.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from openai) (2.10.2)\n",
      "Requirement already satisfied: sniffio in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from openai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (0.3.19)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain) (0.3.40)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain) (2.10.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain) (2.0.38)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain) (3.11.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.4 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: anyio in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: jellyfish in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (1.1.3)\n",
      "Requirement already satisfied: python-Levenshtein in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (0.26.1)\n",
      "Requirement already satisfied: fuzzywuzzy in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (0.18.0)\n",
      "Requirement already satisfied: rapidfuzz in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (3.12.1)\n",
      "Requirement already satisfied: Levenshtein==0.26.1 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from python-Levenshtein) (0.26.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: pandas in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Shantanu/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai\n",
    "%pip install langchain\n",
    "%pip install jellyfish python-Levenshtein fuzzywuzzy rapidfuzz\n",
    "%pip install seaborn pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b6b5ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d51513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Lines file is valid.\n"
     ]
    }
   ],
   "source": [
    "# Builds all the data into a single JSONL file\n",
    "def collectTQLFiles(input_directory, output_file):\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant, returning correct TQL content.\"\n",
    "    }\n",
    "\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Please share the TQL content.\"\n",
    "    }\n",
    "\n",
    "    tql_paths = glob.glob(os.path.join(input_directory, \"*.tql\"))\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for tql_file in tql_paths:\n",
    "            with open(tql_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                tql_content = f.read()\n",
    "\n",
    "            messages = [\n",
    "                system_message,\n",
    "                user_message,\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": tql_content\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            record = {\n",
    "               \n",
    "                \"messages\": messages\n",
    "            }\n",
    "            out_f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "input_dir = \"/Users/Shantanu/downloads/txt2tql\"\n",
    "output_file = \"tql_data.json\"\n",
    "\n",
    "collectTQLFiles(input_dir, output_file)\n",
    "\n",
    "def validate_jsonl(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                json.loads(line)\n",
    "        print(\"JSON Lines file is valid.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Invalid JSON in line: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "validate_jsonl('tql_data.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "950c5533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More usefully, generates training, validation, and test splits\n",
    "def combine_files(input_dir):\n",
    "    \"\"\"Reads all .tql files in input_dir and returns a list of file contents.\"\"\"\n",
    "    file_pattern = os.path.join(input_dir, \"*.tql\")\n",
    "    files = glob.glob(file_pattern)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .tql files found in directory: {input_dir}\")\n",
    "    \n",
    "    corpus = []\n",
    "    for file in files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read().strip()\n",
    "            if content:\n",
    "                corpus.append(content)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def split_corpus(corpus, train_ratio, val_ratio, test_ratio):\n",
    "    \"\"\"Shuffles and splits the corpus into training, validation, and test sets.\"\"\"\n",
    "    total = len(corpus)\n",
    "    random.shuffle(corpus)\n",
    "    \n",
    "    train_end = int(total * train_ratio)\n",
    "    val_end = train_end + int(total * val_ratio)\n",
    "    \n",
    "    train_set = corpus[:train_end]\n",
    "    val_set = corpus[train_end:val_end]\n",
    "    test_set = corpus[val_end:]\n",
    "    \n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "def write_split(output_dir, filename, data):\n",
    "    \"\"\"Writes the list of samples to a file in the output directory.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        # Join samples with two newlines (adjust if needed)\n",
    "        f.write(\"\\n\\n\".join(data))\n",
    "\n",
    "corpus = combine_files(input_dir)\n",
    "train_set, val_set, test_set = split_corpus(corpus, 0.6, 0.2, 0.2)\n",
    "write_split(\"/Users/Shantanu/Documents/GitHub/txt2tql\", \"train.tql\", train_set)\n",
    "write_split(\"/Users/Shantanu/Documents/GitHub/txt2tql\", \"val.tql\", val_set)\n",
    "write_split(\"/Users/Shantanu/Documents/GitHub/txt2tql\", \"test.tql\", test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6c5610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore this chunk: this is just to test the API\n",
    "client = OpenAI(\n",
    "  api_key=\"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\"\n",
    ")\n",
    "\n",
    "# completion = client.chat.completions.create(\n",
    "#   model=\"gpt-4o-mini\",\n",
    "#   store=True,\n",
    "#   messages=[\n",
    "#     {\"role\": \"user\", \"content\": \"write a haiku about ai\"}\n",
    "#   ]\n",
    "# )\n",
    "# print(completion.choices[0].message);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86b4d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tql_queries(client, tql_content, num_queries=1):\n",
    "    \"\"\"Generate natural language queries that would map to the given TQL.\"\"\"\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert at generating natural language queries that would map to TQL queries. Generate realistic user questions that would require this TQL to answer them.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Given this TQL query:\\n\\n{tql_content}\\n\\nGenerate {num_queries} natural language question(s) that this TQL query would answer.\"}\n",
    "            ]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating queries: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_tql_files_with_queries(input_dir, output_file, client, max_files, queries_per_tql=1):\n",
    "    \"\"\"Process all TQL files and generate corresponding queries.\"\"\"\n",
    "    tql_paths = glob.glob(os.path.join(input_dir, \"*.tql\"))\n",
    "    tql_paths = tql_paths[:max_files]\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for tql_file in tql_paths:\n",
    "            with open(tql_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                tql_content = f.read().strip()\n",
    "            \n",
    "            # Generate queries for this TQL\n",
    "            queries = generate_tql_queries(client, tql_content, queries_per_tql)\n",
    "            \n",
    "            if queries:\n",
    "                record = {\n",
    "                    \"tql\": tql_content,\n",
    "                    \"generated_queries\": queries,\n",
    "                    \"file_name\": os.path.basename(tql_file)\n",
    "                }\n",
    "                out_f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def generate_tql_from_query(client, query):\n",
    "    \"\"\"Generate TQL code from a natural language query.\"\"\"\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert at converting natural language queries into TQL code. Generate only the TQL code without any additional explanation.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Convert this question into a TQL query:\\n\\n{query}\"}\n",
    "            ]\n",
    "        )\n",
    "        return completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating TQL: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_query_tql_pairs(input_file, output_file, client):\n",
    "    \"\"\"Create pairs of queries and their corresponding TQL code.\"\"\"\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "                for line in f:\n",
    "                    record = json.loads(line)\n",
    "                    queries = record['generated_queries'].split('\\n')\n",
    "                    \n",
    "                    for query in queries:\n",
    "                        query = query.strip()\n",
    "                        if query:  # Skip empty lines\n",
    "                            # Remove numbering if present (e.g., \"1.\", \"2.\")\n",
    "                            query = ' '.join(query.split()[1:]) if query[0].isdigit() else query\n",
    "                            \n",
    "                            # Generate TQL for this query\n",
    "                            generated_tql = generate_tql_from_query(client, query)\n",
    "                            \n",
    "                            if generated_tql:\n",
    "                                pair = {\n",
    "                                    \"query\": query,\n",
    "                                    \"original_tql\": record['tql'],\n",
    "                                    \"generated_tql\": generated_tql,\n",
    "                                    \"file_name\": record['file_name']\n",
    "                                }\n",
    "                                out_f.write(json.dumps(pair, ensure_ascii=False) + '\\n')\n",
    "                                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing files: {e}\")\n",
    "\n",
    "\n",
    "input_dir = \"/Users/Shantanu/downloads/txt2tql\"\n",
    "output_file = \"tql_with_queries.jsonl\"\n",
    "\n",
    "process_tql_files_with_queries(input_dir, output_file, client, 50, queries_per_tql=1)\n",
    "\n",
    "input_file = \"tql_with_queries.jsonl\"\n",
    "output_file = \"query_tql_pairs.jsonl\"\n",
    "create_query_tql_pairs(input_file, output_file, client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ffce2a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================== Entry 1 ==================================================\n",
      "{\n",
      "  \"query\": \"\\\"What are the steps to stop, undeploy, and then recreate the DataGenSampleApp application using a MariaDB source and outputting to a LCRStream?\\\"\",\n",
      "  \"original_tql\": \"    stop DataGenSampleApp;\\n    undeploy application DataGenSampleApp;\\n    drop application DataGenSampleApp cascade;\\n    CREATE APPLICATION DataGenSampleApp;\\n    Create Source dataGenSrc USING MariaDBReader  (\\n    Username:'qatest',\\n    Password:'w3b@ct10n',\\n    ConnectionURL:'jdbc:mariadb://10.77.21.53:3306/qatest',\\n    Tables: '@tableNames@',\\n    ClusterSupport: 'Galera'\\n    )\\n    Output To LCRStream;\\n    create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;\\n    END APPLICATION DataGenSampleApp;\",\n",
      "  \"generated_tql\": \"    ```\\n    STOP Application DataGenSampleApp;\\n    UNDEPLOY Application DataGenSampleApp;\\n    RECREATE Application DataGenSampleApp\\n    SOURCE MariaDB\\n    OUTPUT LCRStream;\\n    ```\",\n",
      "  \"file_name\": \"dataGenMariaDBTqlTemplate.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 2 ==================================================\n",
      "{\n",
      "  \"query\": \"What steps are involved in setting up and deploying the DBRTOCW application to facilitate data streaming from an MSSQL source to an Oracle target?\",\n",
      "  \"original_tql\": \"    stop DBRTOCW;\\n    undeploy application DBRTOCW;\\n    drop application DBRTOCW cascade;\\n    CREATE APPLICATION DBRTOCW;\\n    Create Source MSSQLSource Using MSSqlReader\\n    (\\n    Username:'qatest',\\n    Password:'w@ct10n',\\n    DatabaseName:'qatest',\\n    ConnectionURL:'10.77.61.30:1433',\\n    Tables:'qatest.MssqlTocql_Alldatatypes',\\n    ConnectionPoolSize:1,\\n    Compression:'true'\\n    )\\n    OUTPUT TO Oracle_ChangeDataStream;\\n    CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (\\n    DatabaseProviderType: 'Default',\\n    CheckPointTable: 'CHKPOINT',\\n    PreserveSourceTransactionBoundary: 'false',\\n    Username: '@TARGET_USER@',\\n    BatchPolicy: 'EventCount:1,Interval:0',\\n    CommitPolicy: 'EventCount:1,Interval:0',\\n    ConnectionURL: '@TARGET_URL@',\\n    Tables: '@TARGET_TABLE@',\\n    Password: '@TARGET_PASS@',\\n    Password_encrypted: false\\n    )\\n    INPUT\\nFROM Oracle_ChangeDataStream;\\n    create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\\n    END APPLICATION DBRTOCW;\\n    deploy application DBRTOCW in default;\\n    start DBRTOCW;\",\n",
      "  \"generated_tql\": \"    ```\\nSELECT steps\\nFROM deployment_process\\n    WHERE application = 'DBRTOCW'\\n    AND source = 'MSSQL'\\n    AND target = 'Oracle';\\n    ```\",\n",
      "  \"file_name\": \"MssqlToCassandra.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 3 ==================================================\n",
      "{\n",
      "  \"query\": \"How can I set up a new data application to read from an Oracle database and write the output in a compressed Parquet format?\",\n",
      "  \"original_tql\": \"    stop application @APPNAME@;\\n    undeploy application @APPNAME@;\\n    drop application @APPNAME@ cascade;\\n    create application @APPNAME@ RECOVERY;\\n    Create Source @APPNAME@_src Using OracleReader\\n    (\\n    Compression: true,\\n    StartTimestamp: 'null',\\n    SupportPDB: false,\\n    FetchSize: 1000,\\n    -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\\n    CommittedTransactions: true,\\n    QueueSize: 2048,\\n    FilterTransactionBoundaries: true,\\n    Password_encrypted: true,\\n    SendBeforeImage: true,\\n    XstreamTimeOut: 600,\\n    ConnectionURL: '@CONNECTION_URL@',\\n    Tables: '@SOURCE_TABLE@',\\n    adapterName: 'OracleReader',\\n    Password: '@SOURCE_PASS@',\\n    Password_encrypted: 'false',\\n    DictionaryMode: 'OnlineCatalog',\\n    FilterTransactionState: true,\\n    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\\n    ReaderType: 'LogMiner',\\n    Username: '@SOURCE_USER@',\\n    OutboundServerProcessName: 'WebActionXStream',\\n    _h_ReturnDateTimeAs:'ZonedDateTime'\\n    ) Output To @APPNAME@_stream;\\n    create Target @APPNAME@_tgt using FileWriter(\\n    filename:'CompressedMerchant.gz',\\n    directory:'/logs/',\\n    rolloverpolicy:'EventCount:10000'\\n    )\\n    format using ParquetFormatter (\\n    schemaFileName:'@FILENAME@',\\n    FormatAs:'@FORMATAS@'\\n    )\\n    input from @APPNAME@_stream;\\n    end application @APPNAME@;\\n    deploy application @APPNAME@;\\n    start application @APPNAME@;\",\n",
      "  \"generated_tql\": \"    ```tql\\n    CREATE APPLICATION new_data_application\\n    SOURCE ORACLE_DATABASE\\n    TARGET PARQUET_COMPRESSION\\n    SETTINGS {\\n    \\\"compression\\\": \\\"gzip\\\"\\n    };\\n    ```\",\n",
      "  \"file_name\": \"ParquetFormatterNoRecovery.tql\"\n",
      "}\n",
      "\n",
      "... (more entries exist)\n",
      "\n",
      "================================================== Entry 1 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop DataGenSampleApp;\\n    undeploy application DataGenSampleApp;\\n    drop application DataGenSampleApp cascade;\\n    CREATE APPLICATION DataGenSampleApp;\\n    Create Source dataGenSrc USING MariaDBReader  (\\n    Username:'qatest',\\n    Password:'w3b@ct10n',\\n    ConnectionURL:'jdbc:mariadb://10.77.21.53:3306/qatest',\\n    Tables: '@tableNames@',\\n    ClusterSupport: 'Galera'\\n    )\\n    Output To LCRStream;\\n    create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;\\n    END APPLICATION DataGenSampleApp;\",\n",
      "  \"generated_queries\": \"\\\"What are the steps to stop, undeploy, and then recreate the DataGenSampleApp application using a MariaDB source and outputting to a LCRStream?\\\"\",\n",
      "  \"file_name\": \"dataGenMariaDBTqlTemplate.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 2 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop DBRTOCW;\\n    undeploy application DBRTOCW;\\n    drop application DBRTOCW cascade;\\n    CREATE APPLICATION DBRTOCW;\\n    Create Source MSSQLSource Using MSSqlReader\\n    (\\n    Username:'qatest',\\n    Password:'w@ct10n',\\n    DatabaseName:'qatest',\\n    ConnectionURL:'10.77.61.30:1433',\\n    Tables:'qatest.MssqlTocql_Alldatatypes',\\n    ConnectionPoolSize:1,\\n    Compression:'true'\\n    )\\n    OUTPUT TO Oracle_ChangeDataStream;\\n    CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (\\n    DatabaseProviderType: 'Default',\\n    CheckPointTable: 'CHKPOINT',\\n    PreserveSourceTransactionBoundary: 'false',\\n    Username: '@TARGET_USER@',\\n    BatchPolicy: 'EventCount:1,Interval:0',\\n    CommitPolicy: 'EventCount:1,Interval:0',\\n    ConnectionURL: '@TARGET_URL@',\\n    Tables: '@TARGET_TABLE@',\\n    Password: '@TARGET_PASS@',\\n    Password_encrypted: false\\n    )\\n    INPUT\\nFROM Oracle_ChangeDataStream;\\n    create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\\n    END APPLICATION DBRTOCW;\\n    deploy application DBRTOCW in default;\\n    start DBRTOCW;\",\n",
      "  \"generated_queries\": \"What steps are involved in setting up and deploying the DBRTOCW application to facilitate data streaming from an MSSQL source to an Oracle target?\",\n",
      "  \"file_name\": \"MssqlToCassandra.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 3 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop application @APPNAME@;\\n    undeploy application @APPNAME@;\\n    drop application @APPNAME@ cascade;\\n    create application @APPNAME@ RECOVERY;\\n    Create Source @APPNAME@_src Using OracleReader\\n    (\\n    Compression: true,\\n    StartTimestamp: 'null',\\n    SupportPDB: false,\\n    FetchSize: 1000,\\n    -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\\n    CommittedTransactions: true,\\n    QueueSize: 2048,\\n    FilterTransactionBoundaries: true,\\n    Password_encrypted: true,\\n    SendBeforeImage: true,\\n    XstreamTimeOut: 600,\\n    ConnectionURL: '@CONNECTION_URL@',\\n    Tables: '@SOURCE_TABLE@',\\n    adapterName: 'OracleReader',\\n    Password: '@SOURCE_PASS@',\\n    Password_encrypted: 'false',\\n    DictionaryMode: 'OnlineCatalog',\\n    FilterTransactionState: true,\\n    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\\n    ReaderType: 'LogMiner',\\n    Username: '@SOURCE_USER@',\\n    OutboundServerProcessName: 'WebActionXStream',\\n    _h_ReturnDateTimeAs:'ZonedDateTime'\\n    ) Output To @APPNAME@_stream;\\n    create Target @APPNAME@_tgt using FileWriter(\\n    filename:'CompressedMerchant.gz',\\n    directory:'/logs/',\\n    rolloverpolicy:'EventCount:10000'\\n    )\\n    format using ParquetFormatter (\\n    schemaFileName:'@FILENAME@',\\n    FormatAs:'@FORMATAS@'\\n    )\\n    input from @APPNAME@_stream;\\n    end application @APPNAME@;\\n    deploy application @APPNAME@;\\n    start application @APPNAME@;\",\n",
      "  \"generated_queries\": \"How can I set up a new data application to read from an Oracle database and write the output in a compressed Parquet format?\",\n",
      "  \"file_name\": \"ParquetFormatterNoRecovery.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 4 ==================================================\n",
      "{\n",
      "  \"tql\": \"    CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\\n    CREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (\\n    Tables: '',\\n    ConnectionURL: '',\\n    Password: '',\\n    Username: ''\\n    )\\n    OUTPUT TO @APPNAME@stream;\\n    CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (\\n    bucketname: '',\\n    uploadpolicy: '',\\n    UploadConfiguration: '',\\n    objectname: '' )\\n    FORMAT USING AvroFormatter (\\n    schemaFileName: '@SCHEMAFILE@'\\n    )\\n    INPUT\\nFROM @APPNAME@stream;\\n    CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (\\n    bucketname: '',\\n    uploadpolicy: '',\\n    UploadConfiguration: '',\\n    objectname: '' )\\n    FORMAT USING JSONFormatter (\\n    members:'data'\\n    )\\n    INPUT\\nFROM @APPNAME@stream;\\n    CREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (\\n    bucketname: '',\\n    uploadpolicy: '',\\n    UploadConfiguration: '',\\n    objectname: '' )\\n    FORMAT USING DSVFormatter (\\n    members:'data'\\n    )\\n    INPUT\\nFROM @APPNAME@stream;\\n    END APPLICATION @APPNAME@;\",\n",
      "  \"generated_queries\": \"What are the components and configurations of the application that streams data from an Oracle database to multiple S3 targets in different formats?\",\n",
      "  \"file_name\": \"OracleCDCToS3MutliTargetsAvro.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 5 ==================================================\n",
      "{\n",
      "  \"tql\": \"    CREATE APPLICATION KafkaReader;\\n    CREATE OR REPLACE TYPE KafkaSourceStr2_Type  ( seq java.lang.Integer\\n    );\\n    CREATE OR REPLACE STREAM KafkaSourceStr2 OF KafkaSourceStr2_Type;\\n    CREATE  JUMPING WINDOW GetTargData OVER KafkaSourceStr2 KEEP 1000000 ROWS;\\n    CREATE OR REPLACE SOURCE KafkaSource USING KafkaReader VERSION '0.11.0' (\\n    KafkaConfigPropertySeparator: ';',\\n    startOffset: 0,\\n    adapterName: 'KafkaReader',\\n    Topic: 'kafkaTopic7',\\n    AutoMapPartition: true,\\n    brokerAddress: 'localhost:9092',\\n    KafkaConfigValueSeparator: '=',\\n    KafkaConfig: 'max.partition.fetch.bytes=10485760;fetch.min.bytes=1048576;fetch.max.wait.ms=1000;receive.buffer.bytes=2000000;poll.timeout.ms=10000;request.timeout.ms=60001;session.timeout.ms=60000'\\n    )\\n    PARSE USING DSVParser  (\\n    charset: 'UTF-8',\\n    handler: 'com.webaction.proc.DSVParser_1_0',\\n    linenumber: 0,\\n    nocolumndelimiter: false,\\n    trimwhitespace: false,\\n    columndelimiter: ',',\\n    columndelimittill: '-1',\\n    ignoremultiplerecordbegin: 'true',\\n    ignorerowdelimiterinquote: false,\\n    parserName: 'DSVParser',\\n    separator: ':',\\n    blockascompleterecord: false,\\n    ignoreemptycolumn: false,\\n    rowdelimiter: '\\\\n',\\n    header: false,\\n    headerlineno: 0,\\n    quoteset: '\\\\\\\"',\\n    trimquote: true\\n    )\\n    OUTPUT TO KafkaSourceStr1 ;\\n    CREATE OR REPLACE CQ GetKafkaDataQuery\\n    INSERT INTO KafkaSourceStr2\\nSELECT TO_INT(data[1]) as seq\\nFROM KafkaSourceStr1;\\n    CREATE  TYPE KafkaSourceStr3_Type  ( SUMKafkaSourceStr2seq java.lang.Long\\n    );\\n    CREATE STREAM KafkaSourceStr3 OF KafkaSourceStr3_Type;\\n    CREATE OR REPLACE CQ GetTheSum\\n    INSERT INTO KafkaSourceStr3\\nSELECT SUM(GetTargData .seq)\\nFROM GetTargData;\\n    CREATE OR REPLACE TARGET KafkaFile USING FileWriter  (\\n    filename: 'TargetResults',\\n    rolloveronddl: 'true',\\n    flushpolicy: 'eventcount:10000,interval:30',\\n    adapterName: 'FileWriter',\\n    directory: '@FEATURE-DIR@/logs',\\n    rolloverpolicy: 'eventcount:10000,interval:30s'\\n    )\\n    FORMAT USING DSVFormatter  (   nullvalue: 'NULL',\\n    standard: 'none',\\n    handler: 'com.webaction.proc.DSVFormatter',\\n    formatterName: 'DSVFormatter',\\n    usequotes: 'false',\\n    rowdelimiter: '\\\\n',\\n    quotecharacter: '\\\\\\\"',\\n    header: 'false',\\n    columndelimiter: ','\\n    )\\n    INPUT\\nFROM KafkaSourceStr3;\\n    END APPLICATION KafkaReader;\",\n",
      "  \"generated_queries\": \"What is the sum of the sequence numbers produced from messages in the Kafka topic 'kafkaTopic7' after processing them through the application?\",\n",
      "  \"file_name\": \"KafkaReader.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 6 ==================================================\n",
      "{\n",
      "  \"tql\": \"    STOP APPLICATION @WRITERAPPNAME@;\\n    UNDEPLOY APPLICATION @WRITERAPPNAME@;\\n    DROP APPLICATION @WRITERAPPNAME@ CASCADE;\\n    CREATE APPLICATION @WRITERAPPNAME@ RECOVERY 1 SECOND INTERVAL;\\n    create flow @APPNAME@_agentflow;\\n    CREATE SOURCE @SOURCE@ USING OracleReader\\n    (\\n    FetchSize:1,\\n    Username:'@SOURCE_USER@',\\n    Password:'85d7qFnwTW8=',\\n    ConnectionURL:'@CONNECTION_URL@',\\n    Tables:'@SOURCE_TABLE@',\\n    password_encrypted: 'true'\\n    )\\n    OUTPUT TO @STREAM1@;\\n    end flow @APPNAME@_agentflow;\\n    create flow @APPNAME@_serverflow;\\n    CREATE OR REPLACE TYPE @TYPE@(\\n    datae java.util.HashMap ,\\n    TABLE_NAME java.lang.String ,\\n    OPS_NAME java.lang.String ,\\n    DB_TIMESTAMP java.lang.String  ,\\n    COMMITSCN java.lang.String ,\\n    SCN java.lang.String ,\\n    REC_INS_TIME java.lang.String );\\n    CREATE CQ @CQ1@\\n    INSERT INTO @STREAM2@\\nSELECT\\n    CASE WHEN (META(c,\\\"OperationName\\\").toString() == \\\"DELETE\\\")\\n    THEN putUserData(c, 'isDelete', 'true')\\n    ELSE\\n    putUserData(c,'isDelete', 'false')\\n    END\\nFROM @STREAM1@ c;\\n    CREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;\\n    CREATE OR REPLACE CQ @CQ2@\\n    INSERT INTO @STREAM3@\\nSELECT\\n    data(e),\\n    META(e,\\\"TableName\\\").toString() as TABLE_NAME,\\n    META(e, \\\"OperationName\\\").toString() as OPS_NAME,\\n    META(e, \\\"TimeStamp\\\").toString() as DB_TIMESTAMP,\\n    META(e,\\\"COMMITSCN\\\").toString() as COMMITSCN ,\\n    META(e,\\\"SCN\\\").toString() as  SCN ,\\n    DNOW().toString() as REC_INS_TIME\\nFROM @STREAM1@ e;\\n    create Target @TARGET1@ using KafkaWriter VERSION @kafkaAdpVersion@ (\\n    brokerAddress:'localhost:9099',\\n    Topic:'@WRITERAPPNAME@_TOPIC1',\\n    ParallelThreads:'',\\n    PartitionKey:'@metadata(TableName)',\\n    KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\\n    FORMAT USING dsvFormatter ()\\n    input from @STREAM1@;\\n    create Target @TARGET2@ using KafkaWriter VERSION @kafkaAdpVersion@ (\\n    brokerAddress:'localhost:9099',\\n    Topic:'@WRITERAPPNAME@_TOPIC2',\\n    ParallelThreads:'2',\\n    PartitionKey:'TABLE_NAME',\\n    KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\\n    FORMAT USING jsonFormatter ()\\n    input from @STREAM3@;\\n    create Target @TARGET3@ using KafkaWriter VERSION @kafkaAdpVersion@ (\\n    brokerAddress:'localhost:9099',\\n    Topic:'@WRITERAPPNAME@_TOPIC3',\\n    ParallelThreads:'',\\n    PartitionKey:'@userdata(isDelete)',\\n    KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\\n    FORMAT USING avroFormatter (\\n    schemaFileName:'KafkaAvroTest.avsc')\\n    input from @STREAM2@;\\n    end application @WRITERAPPNAME@;\\n    deploy application @WRITERAPPNAME@;\\n    start @WRITERAPPNAME@;\\n    stop application @READERAPPNAME@;\\n    undeploy application @READERAPPNAME@;\\n    drop application @READERAPPNAME@ cascade;\\n    CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;\\n    CREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafkaAdpVersion@ (\\n    brokerAddress:'localhost:9099',\\n    Topic:'@WRITERAPPNAME@_TOPIC1',\\n    startOffset:0\\n    )\\n    PARSE USING DSVParser (\\n    )\\n    OUTPUT TO KafkaReaderStream1;\\n    CREATE TARGET kafkaDumpDSV USING FileWriter(\\n    name:kafkaOuputDSV,\\n    filename:'@READERAPPNAME@_RT_DSV')\\n    FORMAT USING DSVFormatter()\\n    INPUT\\nFROM KafkaReaderStream1;\\n    CREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafkaAdpVersion@ (\\n    brokerAddress:'localhost:9099',\\n    Topic:'@WRITERAPPNAME@_TOPIC2',\\n    startOffset:0\\n    )\\n    PARSE USING JSONParser (\\n    eventType:''\\n    )\\n    OUTPUT TO KafkaReaderStream2;\\n    CREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafkaAdpVersion@ (\\n    brokerAddress:'localhost:9099',\\n    Topic:'@WRITERAPPNAME@_TOPIC3',\\n    startOffset:0\\n    )\\n    PARSE USING AvroParser (\\n    schemaFileName:'KafkaAvroTest.avsc'\\n    )\\n    OUTPUT TO KafkaReaderStream3;\\n    end flow @APPNAME@_serverflow;\\n    end application @READERAPPNAME@;\\n    deploy application @READERAPPNAME@;\",\n",
      "  \"generated_queries\": \"What are the steps to create and deploy an application that reads data from Kafka topics and writes it to different output formats, using a combination of DSV, JSON, and Avro formats?\",\n",
      "  \"file_name\": \"orcl_kw_11_agent.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 7 ==================================================\n",
      "{\n",
      "  \"tql\": \"    STOP APPLICATION @APPNAME@;\\n    UNDEPLOY APPLICATION @APPNAME@;\\n    DROP APPLICATION @APPNAME@ CASCADE;\\n    CREATE APPLICATION @APPNAME@ RECOVERY 10 SECOND INTERVAL;\\n    create flow Mysqlflow;\\n    CREATE SOURCE MysqlToDBRoutersource USING MysqlReader\\n    (\\n    Username: '',\\n    Password: '',\\n    Tables: '',\\n    ConnectionURL: '',\\n    Password_encrypted: 'false',\\n    connectionRetryPolicy: 'retryInterval=30, maxRetries=3'\\n    )\\n    OUTPUT TO RouterTestMasterStream;\\n    end flow Mysqlflow;\\n    CREATE OR REPLACE ROUTER RouterTestRs1 INPUT\\nFROM RouterTestMasterStream s CASE\\n    WHEN meta(s,\\\"TableName\\\").toString()='waction.source1' THEN ROUTE TO RouterTestTyped1,\\n    WHEN meta(s,\\\"TableName\\\").toString()='waction.source2' THEN ROUTE TO RouterTestTyped2,\\n    ELSE ROUTE TO RouterTestTypedElse;\\n    CREATE TARGET MysqlToDBRoutertarget1 USING DatabaseWriter(\\n    Username: '',\\n    Password: '',\\n    Tables: '',\\n    ConnectionURL: '',\\n    Password_encrypted: 'false',\\n    connectionRetryPolicy: 'retryInterval=30, maxRetries=3'\\n    ) INPUT\\nFROM RouterTestTyped1;\\n    CREATE TARGET MysqlToDBRoutertarget2 USING DatabaseWriter(\\n    Username: '',\\n    Password: '',\\n    Tables: '',\\n    ConnectionURL: '',\\n    Password_encrypted: 'false',\\n    connectionRetryPolicy: 'retryInterval=30, maxRetries=3'\\n    ) INPUT\\nFROM RouterTestTyped2;\\n    end application @APPNAME@;\\n    deploy application @APPNAME@ with Mysqlflow in AGENTs;\\n    start application @APPNAME@;\",\n",
      "  \"generated_queries\": \"What steps do I need to follow to deploy the application named @APPNAME@ with a MySQL data flow that routes data from specific source tables to different database targets?\",\n",
      "  \"file_name\": \"routerMySQL.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 8 ==================================================\n",
      "{\n",
      "  \"tql\": \"    --\\n    -- Crash Recovery Test 2 on four node all server cluster\\n    -- Bert Hashemi, WebAction, Inc.\\n    --\\n    -- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS\\n    --\\n    STOP APPLICATION N4S4CR2Tester.N4S4CRTest2;\\n    UNDEPLOY APPLICATION N4S4CR2Tester.N4S4CRTest2;\\n    DROP APPLICATION N4S4CR2Tester.N4S4CRTest2 CASCADE;\\n    CREATE APPLICATION N4S4CRTest2 RECOVERY 5 SECOND INTERVAL;\\n    CREATE FLOW DataAcquisitionN4S4CRTest2;\\n    CREATE SOURCE CsvSourceN4S4CRTest2 USING CSVReader (\\n    directory:'@TEST-DATA-PATH@',\\n    header:Yes,\\n    wildcard:'RecovTestData.csv',\\n    columndelimiter:',',\\n    blocksize: 10240,\\n    positionByEOF:false,\\n    trimquote:false\\n    ) OUTPUT TO CsvStream;\\n    END FLOW DataAcquisitionN4S4CRTest2;\\n    CREATE FLOW DataProcessingN4S4CRTest2;\\n    CREATE TYPE WactionTypeN4S4CRTest2 (\\n    companyName String,\\n    merchantId String KEY,\\n    dateTime DateTime,\\n    amount double,\\n    city String\\n    );\\n    CREATE STREAM DataStream OF WactionTypeN4S4CRTest2;\\n    CREATE CQ CsvToWaction\\n    INSERT INTO DataStream\\nSELECT\\n    data[0],\\n    data[1],\\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\\n    TO_DOUBLE(data[7]),\\n    data[10]\\nFROM CsvStream;\\n    CREATE JUMPING WINDOW DataStream5Minutes\\n    OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\\n    CREATE WACTIONSTORE WactionsN4S4CRTest2 CONTEXT OF WactionTypeN4S4CRTest2\\n    EVENT TYPES ( WactionTypeN4S4CRTest2 )\\n    @PERSIST-TYPE@\\n    CREATE CQ InsertWactionsN4S4CRTest2\\n    INSERT INTO WactionsN4S4CRTest2\\nSELECT\\n    *\\nFROM DataStream5Minutes;\\n    END FLOW DataProcessingN4S4CRTest2;\\n    END APPLICATION N4S4CRTest2;\",\n",
      "  \"generated_queries\": \"What steps are involved in setting up and executing the crash recovery test for the N4S4CR2Tester application, including data acquisition and processing?\",\n",
      "  \"file_name\": \"KStreamN4S4CRTest2.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 9 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop OracleToKudu;\\n    undeploy application OracleToKudu;\\n    drop application OracleToKudu cascade;\\n    CREATE APPLICATION OracleToKudu RECOVERY 5 SECOND INTERVAL;\\n    Create Source oracSource\\n    Using OracleReader\\n    (\\n    Username:'@LOGMINER-UNAME@',\\n    Password:'@LOGMINER-PASSWORD@',\\n    ConnectionURL:'@LOGMINER-URL@',\\n    Tables:'@SOURCE_TABLES@',\\n    OnlineCatalog:true,\\n    FetchSize:1\\n    ) Output To DataStream;\\n    CREATE TARGET WriteintoKudu using KuduWriter (\\n    kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',\\n    pkupdatehandlingmode:'@MODE@',\\n    tables: '@TARGET_TABLES@',\\n    ConnectionRetryPolicy: 'retryInterval=40,maxRetries=7',\\n    batchpolicy: 'EventCount:20,Interval:60')\\n    INPUT\\nFROM DataStream;\\n    END APPLICATION OracleToKudu;\\n    deploy application OracleToKudu in default;\\n    start OracleToKudu;\",\n",
      "  \"generated_queries\": \"1. How can I transfer data from an Oracle database to a Kudu table using a streaming application?\",\n",
      "  \"file_name\": \"OracleToKuduRecovery.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 10 ==================================================\n",
      "{\n",
      "  \"tql\": \"    STOP APPLICATION EH;\\n    UNDEPLOY APPLICATION EH;\\n    DROP APPLICATION EH CASCADE;\\n    CREATE APPLICATION EH recovery 5 second interval;\\n    CREATE Source s USING PostgreSQLReader  (\\n    FilterTransactionBoundaries: 'true',\\n    Username: 'waction',\\n    ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\\n    adapterName: 'PostgreSQLReader',\\n    ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\\n    Tables: 'public.tablename1000%')\\n    OUTPUT TO ss ;\\n    CREATE OR REPLACE TYPE jsontype(\\n    datae java.util.HashMap ,\\n    TABLE_NAME java.lang.String ,\\n    OPS_NAME java.lang.String ,\\n    DB_TIMESTAMP java.lang.String );\\n    CREATE STREAM cq_json_out OF jsontype PARTITION BY TABLE_NAME;\\n    CREATE OR REPLACE CQ cq_json\\n    INSERT INTO cq_json_out\\nSELECT\\n    data(e),\\n    META(e,\\\"TableName\\\").toString() as TABLE_NAME,\\n    META(e, \\\"OperationName\\\").toString() as OPS_NAME,\\n    META(e, \\\"TimeStamp\\\").toString() as DB_TIMESTAMP\\nFROM ss e;\\n    CREATE CQ cq1\\n    INSERT INTO TypedAccessLogStream1\\nSELECT *\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000101';\\n    create Target t1 using AzureEventHubWriter (\\n    EventHubNamespace:'EventHubWriterTest',\\n    EventHubName:'test_101',\\n    SASPolicyName:'RootManageSharedAccessKey',\\n    SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    ConsumerGroup:'test_cg_101',\\n    E1P:'true',\\n    OperationTimeoutMS:'200000',\\n    BatchPolicy:'Size:256000,interval:30s',\\n    ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\\n    )\\n    format using jsonFormatter(\\n    )\\n    input from TypedAccessLogStream1;\\n    CREATE CQ cq2\\n    INSERT INTO TypedAccessLogStream2\\nSELECT *\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000102';\\n    create Target t2 using AzureEventHubWriter (\\n    EventHubNamespace:'EventHubWriterTest',\\n    EventHubName:'test_102',\\n    SASPolicyName:'RootManageSharedAccessKey',\\n    SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    ConsumerGroup:'test_cg_102',\\n    E1P:'true',\\n    OperationTimeoutMS:'200000',\\n    BatchPolicy:'Size:256000,interval:30s',\\n    ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\\n    )\\n    format using jsonFormatter(\\n    )\\n    input from TypedAccessLogStream2;\\n    CREATE CQ cq3\\n    INSERT INTO TypedAccessLogStream3\\nSELECT *\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000103';\\n    create Target t3 using AzureEventHubWriter (\\n    EventHubNamespace:'EventHubWriterTest',\\n    EventHubName:'test_103',\\n    SASPolicyName:'RootManageSharedAccessKey',\\n    SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    ConsumerGroup:'test_cg_103',\\n    E1P:'true',\\n    OperationTimeoutMS:'200000',\\n    BatchPolicy:'Size:256000,interval:30s',\\n    ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\\n    )\\n    format using jsonFormatter(\\n    )\\n    input from TypedAccessLogStream3;\\n    CREATE CQ cq4\\n    INSERT INTO TypedAccessLogStream4\\nSELECT *\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000104';\\n    create Target t4 using AzureEventHubWriter (\\n    EventHubNamespace:'EventHubWriterTest',\\n    EventHubName:'test_104',\\n    SASPolicyName:'RootManageSharedAccessKey',\\n    SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    ConsumerGroup:'test_cg_104',\\n    E1P:'true',\\n    OperationTimeoutMS:'200000',\\n    BatchPolicy:'Size:256000,interval:30s',\\n    ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\\n    )\\n    format using jsonFormatter(\\n    )\\n    input from TypedAccessLogStream4;\\n    CREATE CQ cq5\\n    INSERT INTO TypedAccessLogStream5\\nSELECT *\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000105';\\n    create Target t5 using AzureEventHubWriter (\\n    EventHubNamespace:'EventHubWriterTest',\\n    EventHubName:'test_105',\\n    SASPolicyName:'RootManageSharedAccessKey',\\n    SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    ConsumerGroup:'test_cg_105',\\n    E1P:'true',\\n    OperationTimeoutMS:'200000',\\n    BatchPolicy:'Size:256000,interval:30s',\\n    ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\\n    )\\n    format using jsonFormatter(\\n    )\\n    input from TypedAccessLogStream5;\\n    CREATE CQ cq6\\n    INSERT INTO TypedAccessLogStream6\\nSELECT *\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000106';\\n    create Target t6 using AzureEventHubWriter (\\n    EventHubNamespace:'EventHubWriterTest',\\n    EventHubName:'test_106',\\n    SASPolicyName:'RootManageSharedAccessKey',\\n    SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    ConsumerGroup:'test_cg_106',\\n    E1P:'true',\\n    OperationTimeoutMS:'200000',\\n    BatchPolicy:'Size:256000,interval:30s',\\n    ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\\n    )\\n    format using jsonFormatter(\\n    )\\n    input from TypedAccessLogStream6;\\n    CREATE CQ cq7\\n    INSERT INTO TypedAccessLogStream7\\nSELECT *\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000107';\\n    create Target t7 using AzureEventHubWriter (\\n    EventHubNamespace:'EventHubWriterTest',\\n    EventHubName:'test_107',\\n    SASPolicyName:'RootManageSharedAccessKey',\\n    SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    ConsumerGroup:'test_cg_107',\\n    E1P:'true',\\n    OperationTimeoutMS:'200000',\\n    BatchPolicy:'Size:256000,interval:30s',\\n    ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\\n    )\\n    format using jsonFormatter(\\n    )\\n    input from TypedAccessLogStream7;\\n    CREATE CQ cq8\\n    INSERT INTO TypedAccessLogStream8\\nSELECT *\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000108';\\n    create Target t8 using AzureEventHubWriter (\\n    EventHubNamespace:'EventHubWriterTest',\\n    EventHubName:'test_108',\\n    SASPolicyName:'RootManageSharedAccessKey',\\n    SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    ConsumerGroup:'test_cg_108',\\n    E1P:'true',\\n    OperationTimeoutMS:'200000',\\n    BatchPolicy:'Size:256000,interval:30s',\\n    ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\\n    )\\n    format using jsonFormatter(\\n    )\\n    input from TypedAccessLogStream8;\\n    CREATE CQ cq9\\n    INSERT INTO TypedAccessLogStream9\\nSELECT *\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000109';\\n    create Target t9 using AzureEventHubWriter (\\n    EventHubNamespace:'EventHubWriterTest',\\n    EventHubName:'test_109',\\n    SASPolicyName:'RootManageSharedAccessKey',\\n    SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    ConsumerGroup:'test_cg_109',\\n    E1P:'true',\\n    OperationTimeoutMS:'200000',\\n    BatchPolicy:'Size:256000,interval:30s',\\n    ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\\n    )\\n    format using jsonFormatter(\\n    )\\n    input from TypedAccessLogStream9;\\n    -- CREATE CQ cq10\\n    -- INSERT INTO TypedAccessLogStream10\\n    --\\nSELECT *\\n    --\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000110';\\n    --\\n    -- create Target t10 using AzureEventHubWriter (\\n    -- \\tEventHubNamespace:'EventHubWriterTest',\\n    -- \\tEventHubName:'test_110',\\n    -- \\tSASPolicyName:'RootManageSharedAccessKey',\\n    -- \\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    -- \\tConsumerGroup:'test_cg_110',\\n    -- \\tE1P:'true',\\n    -- \\tOperationTimeoutMS:'200000',\\n    -- \\tBatchPolicy:'Size:256000,interval:30s'\\n    -- )\\n    -- format using jsonFormatter(\\n    -- )\\n    -- input from TypedAccessLogStream10;\\n    -- CREATE CQ cq11\\n    -- INSERT INTO TypedAccessLogStream11\\n    --\\nSELECT *\\n    --\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000111';\\n    --\\n    -- create Target t11 using AzureEventHubWriter (\\n    -- \\tEventHubNamespace:'EventHubWriterTest',\\n    -- \\tEventHubName:'test_111',\\n    -- \\tSASPolicyName:'RootManageSharedAccessKey',\\n    -- \\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    -- \\tConsumerGroup:'test_cg_111',\\n    -- \\tE1P:'true',\\n    -- \\tOperationTimeoutMS:'200000',\\n    -- \\tBatchPolicy:'Size:256000,interval:30s'\\n    -- )\\n    -- format using jsonFormatter(\\n    -- )\\n    -- input from TypedAccessLogStream11;\\n    --\\n    -- CREATE CQ cq12\\n    -- INSERT INTO TypedAccessLogStream12\\n    --\\nSELECT *\\n    --\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000112';\\n    --\\n    -- create Target t12 using AzureEventHubWriter (\\n    -- \\tEventHubNamespace:'EventHubWriterTest',\\n    -- \\tEventHubName:'test_112',\\n    -- \\tSASPolicyName:'RootManageSharedAccessKey',\\n    -- \\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    -- \\tConsumerGroup:'test_cg_112',\\n    -- \\tE1P:'true',\\n    -- \\tOperationTimeoutMS:'200000',\\n    -- \\tBatchPolicy:'Size:256000,interval:30s'\\n    -- )\\n    -- format using jsonFormatter(\\n    -- )\\n    -- input from TypedAccessLogStream12;\\n    --\\n    -- CREATE CQ cq13\\n    -- INSERT INTO TypedAccessLogStream13\\n    --\\nSELECT *\\n    --\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000113';\\n    --\\n    -- create Target t13 using AzureEventHubWriter (\\n    -- \\tEventHubNamespace:'EventHubWriterTest',\\n    -- \\tEventHubName:'test_113',\\n    -- \\tSASPolicyName:'RootManageSharedAccessKey',\\n    -- \\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    -- \\tConsumerGroup:'test_cg_113',\\n    -- \\tE1P:'true',\\n    -- \\tOperationTimeoutMS:'200000',\\n    -- \\tBatchPolicy:'Size:256000,interval:30s'\\n    -- )\\n    -- format using jsonFormatter(\\n    -- )\\n    -- input from TypedAccessLogStream13;\\n    --\\n    -- CREATE CQ cq14\\n    -- INSERT INTO TypedAccessLogStream14\\n    --\\nSELECT *\\n    --\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000114';\\n    --\\n    -- create Target t14 using AzureEventHubWriter (\\n    -- \\tEventHubNamespace:'EventHubWriterTest',\\n    -- \\tEventHubName:'test_114',\\n    -- \\tSASPolicyName:'RootManageSharedAccessKey',\\n    -- \\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    -- \\tConsumerGroup:'test_cg_114',\\n    -- \\tE1P:'true',\\n    -- \\tOperationTimeoutMS:'200000',\\n    -- \\tBatchPolicy:'Size:256000,interval:30s'\\n    -- )\\n    -- format using jsonFormatter(\\n    -- )\\n    -- input from TypedAccessLogStream14;\\n    --\\n    -- CREATE CQ cq15\\n    -- INSERT INTO TypedAccessLogStream15\\n    --\\nSELECT *\\n    --\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000115';\\n    --\\n    -- create Target t15 using AzureEventHubWriter (\\n    -- \\tEventHubNamespace:'EventHubWriterTest',\\n    -- \\tEventHubName:'test_115',\\n    -- \\tSASPolicyName:'RootManageSharedAccessKey',\\n    -- \\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    -- \\tConsumerGroup:'test_cg_115',\\n    -- \\tE1P:'true',\\n    -- \\tOperationTimeoutMS:'200000',\\n    -- \\tBatchPolicy:'Size:256000,interval:30s'\\n    -- )\\n    -- format using jsonFormatter(\\n    -- )\\n    -- input from TypedAccessLogStream15;\\n    --\\n    -- CREATE CQ cq16\\n    -- INSERT INTO TypedAccessLogStream16\\n    --\\nSELECT *\\n    --\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000116';\\n    --\\n    -- create Target t16 using AzureEventHubWriter (\\n    -- \\tEventHubNamespace:'EventHubWriterTest',\\n    -- \\tEventHubName:'test_116',\\n    -- \\tSASPolicyName:'RootManageSharedAccessKey',\\n    -- \\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    -- \\tConsumerGroup:'test_cg_116',\\n    -- \\tE1P:'true',\\n    -- \\tOperationTimeoutMS:'200000',\\n    -- \\tBatchPolicy:'Size:256000,interval:30s'\\n    -- )\\n    -- format using jsonFormatter(\\n    -- )\\n    -- input from TypedAccessLogStream16;\\n    --\\n    -- CREATE CQ cq17\\n    -- INSERT INTO TypedAccessLogStream17\\n    --\\nSELECT *\\n    --\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000117';\\n    --\\n    -- create Target t17 using AzureEventHubWriter (\\n    -- \\tEventHubNamespace:'EventHubWriterTest',\\n    -- \\tEventHubName:'test_117',\\n    -- \\tSASPolicyName:'RootManageSharedAccessKey',\\n    -- \\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    -- \\tConsumerGroup:'test_cg_117',\\n    -- \\tE1P:'true',\\n    -- \\tOperationTimeoutMS:'200000',\\n    -- \\tBatchPolicy:'Size:256000,interval:30s'\\n    -- )\\n    -- format using jsonFormatter(\\n    -- )\\n    -- input from TypedAccessLogStream17;\\n    --\\n    -- CREATE CQ cq18\\n    -- INSERT INTO TypedAccessLogStream18\\n    --\\nSELECT *\\n    --\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000118';\\n    --\\n    -- create Target t18 using AzureEventHubWriter (\\n    -- \\tEventHubNamespace:'EventHubWriterTest',\\n    -- \\tEventHubName:'test_118',\\n    -- \\tSASPolicyName:'RootManageSharedAccessKey',\\n    -- \\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    -- \\tConsumerGroup:'test_cg_118',\\n    -- \\tE1P:'true',\\n    -- \\tOperationTimeoutMS:'200000',\\n    -- \\tBatchPolicy:'Size:256000,interval:30s'\\n    -- )\\n    -- format using jsonFormatter(\\n    -- )\\n    -- input from TypedAccessLogStream18;\\n    --\\n    -- CREATE CQ cq19\\n    -- INSERT INTO TypedAccessLogStream19\\n    --\\nSELECT *\\n    --\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000119';\\n    --\\n    -- create Target t19 using AzureEventHubWriter (\\n    -- \\tEventHubNamespace:'EventHubWriterTest',\\n    -- \\tEventHubName:'test_119',\\n    -- \\tSASPolicyName:'RootManageSharedAccessKey',\\n    -- \\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    -- \\tConsumerGroup:'test_cg_119',\\n    -- \\tE1P:'true',\\n    -- \\tOperationTimeoutMS:'200000',\\n    -- \\tBatchPolicy:'Size:256000,interval:30s'\\n    -- )\\n    -- format using jsonFormatter(\\n    -- )\\n    -- input from TypedAccessLogStream19;\\n    --\\n    -- CREATE CQ cq20\\n    -- INSERT INTO TypedAccessLogStream20\\n    --\\nSELECT *\\n    --\\nFROM cq_json_out PS\\n    WHERE PS.Table_Name = 'public.tablename1000120';\\n    --\\n    -- create Target t20 using AzureEventHubWriter (\\n    -- \\tEventHubNamespace:'EventHubWriterTest',\\n    -- \\tEventHubName:'test_120',\\n    -- \\tSASPolicyName:'RootManageSharedAccessKey',\\n    -- \\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\\n    -- \\tConsumerGroup:'test_cg_120',\\n    -- \\tE1P:'true',\\n    -- \\tOperationTimeoutMS:'200000',\\n    -- \\tBatchPolicy:'Size:256000,interval:30s'\\n    -- )\\n    -- format using jsonFormatter(\\n    -- )\\n    -- input from TypedAccessLogStream20;\\n    END APPLICATION EH;\\n    DEPLOY APPLICATION EH;\\n    start application EH;\",\n",
      "  \"generated_queries\": \"What are the specific operations being logged from the PostgreSQL table \\\"public.tablename1000%\\\" and how are those logs being processed and sent to Azure Event Hub?\",\n",
      "  \"file_name\": \"postgrescdc_eh.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 11 ==================================================\n",
      "{\n",
      "  \"tql\": \"    create application CSVToJSON;\\n    create source CSVSource using FileReader (\\n    directory:'Samples/AppData',\\n    WildCard:'posdata.csv',\\n    positionByEOF:false,\\n    charset:'UTF-8'\\n    )\\n    parse using DSVParser (\\n    header:'yes'\\n    )\\n    OUTPUT TO CsvStream;\\n    Create Type CSVType (\\n    merchantName String,\\n    merchantId String,\\n    dateTime DateTime,\\n    hourValue int,\\n    amount double,\\n    zip String\\n    );\\n    Create Stream TypedCSVStream of CSVType;\\n    CREATE CQ CsvToPosData\\n    INSERT INTO TypedCSVStream\\nSELECT data[0],data[1],\\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\\n    DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\\n    TO_DOUBLE(data[7]),\\n    data[9]\\nFROM CsvStream;\\n    create Target t using FileWriter(\\n    filename:'posdata_JSON',\\n    rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'\\n    )\\n    format using JSONFormatter (\\n    members:'merchantname,merchantid,dateTime,hourValue,amount,zip'\\n    )\\n    input from TypedCSVStream;\\n    end application CSVToJSON;\\n    deploy application CSVToJSON;\\n    start application CSVToJSON;\",\n",
      "  \"generated_queries\": \"How can I convert a CSV file containing payment data into a JSON format for further analysis?\",\n",
      "  \"file_name\": \"FileWriterWithJSONFormatter.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 12 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop @AppName@;\\n    undeploy application @AppName@;\\n    drop application @AppName@ cascade;\\n    CREATE APPLICATION @AppName@;\\n    CREATE SOURCE @SourceName@ USING PostgreSQLReader  (\\n    ReaderType: 'LogMiner',\\n    Password_encrypted: 'false',\\n    SupportPDB: false,\\n    ReplicationSlotName: 'test_slot',\\n    QuiesceMarkerTable: 'QUIESCEMARKER',\\n    QueueSize: 2048,\\n    CommittedTransactions: true,\\n    Username: '@UserName@',\\n    TransactionBufferType: 'Memory',\\n    TransactionBufferDiskLocation: '.striim/LargeBuffer',\\n    OutboundServerProcessName: 'WebActionXStream',\\n    Password: '@Password@',\\n    DDLCaptureMode: 'All',\\n    Compression: false,\\n    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\\n    FetchSize: 1,\\n    Tables: '@SourceTables@',\\n    DictionaryMode: 'OnlineCatalog',\\n    XstreamTimeOut: 600,\\n    TransactionBufferSpilloverSize: '1MB',\\n    FilterTransactionBoundaries: true,\\n    ConnectionURL: '@ConnectionURL@',\\n    SendBeforeImage: true )\\n    OUTPUT TO @AppStream@  ;\\n    CREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream\\nSELECT data[2], to_string(to_date(data[2]), \\\"dd-MMM-yy hh.mm.ss\\\")\\nFROM @AppStream@ o ;\\n    CREATE  TARGET @targetsys@ USING Global.SysOut  (\\n    name: 'ora1_sys' )\\n    INPUT\\nFROM admin.ZDT_cq_stream;\\n    create Target @TargetFile@ using FileWriter(\\n    filename:'toStringOut.log',\\n    directory:'@FilePath@',\\n    rolloverpolicy:'eventcount:1000'\\n    )\\n    format using DSVFormatter (\\n    )\\n    input from admin.ZDT_cq_stream;\\n    END APPLICATION @AppName@;\\n    deploy application @AppName@;\\n    start @AppName@;\",\n",
      "  \"generated_queries\": \"How do I reset, recreate, and deploy an application using the latest changes from a PostgreSQL source, and set up real-time data capture with specific configurations?\",\n",
      "  \"file_name\": \"toStringZonedDateTime_postgres.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 13 ==================================================\n",
      "{\n",
      "  \"tql\": \"    --\\n    -- Recovery Test 13 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same compound key\\n    -- Nicholas Keene WebAction, Inc.\\n    --\\n    -- S1 -> CW(p#p) -> CQ -> WS\\n    --\\n    STOP Recov13Tester.RecovTest13;\\n    UNDEPLOY APPLICATION Recov13Tester.RecovTest13;\\n    DROP APPLICATION Recov13Tester.RecovTest13 CASCADE;\\n    CREATE APPLICATION RecovTest13 RECOVERY 5 SECOND INTERVAL;\\n    CREATE SOURCE CsvSource USING CSVReader (\\n    directory:'@TEST-DATA-PATH@',\\n    header:Yes,\\n    wildcard:'RecovTest10Data.csv',\\n    columndelimiter:',',\\n    blocksize: 10240,\\n    positionByEOF:false,\\n    trimquote:false\\n    ) OUTPUT TO CsvStream;\\n    CREATE TYPE CsvData (\\n    partKey String KEY,\\n    serialNumber int,\\n    partKey2 String KEY\\n    );\\n    CREATE TYPE WactionData (\\n    partKey String KEY,\\n    serialNumber int\\n    );\\n    CREATE STREAM DataStream OF CsvData PARTITION BY partKey, partKey2;\\n    CREATE CQ CsvToData\\n    INSERT INTO DataStream\\nSELECT\\n    data[0],\\n    TO_INT(data[1]),\\n    data[0]\\nFROM CsvStream;\\n    CREATE JUMPING WINDOW DataStreamTwoItems\\n    OVER DataStream KEEP 2 ROWS\\n    PARTITION BY partKey, partKey2;\\n    CREATE WACTIONSTORE Wactions CONTEXT OF WactionData\\n    EVENT TYPES ( CsvData )\\n    @PERSIST-TYPE@\\n    CREATE CQ DataToWaction\\n    INSERT INTO Wactions\\nSELECT\\n    first(partKey),\\n    to_int(first(serialNumber))\\nFROM DataStreamTwoItems\\n    GROUP BY partKey, partKey2;\\n    END APPLICATION RecovTest13;\",\n",
      "  \"generated_queries\": \"How is the Recov13Tester application processing data from the CSV source, and what are the key actions stored in the Wactionstore for different partitions?\",\n",
      "  \"file_name\": \"RecovTest13.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 14 ==================================================\n",
      "{\n",
      "  \"tql\": \"    CREATE APPLICATION @APPNAME@;\\n    CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()\\n    PARSE USING XMLParserV2 ()\\n    OUTPUT TO @APPNAME@_Stream;\\n    CREATE OR REPLACE CQ @APPNAME@_CQ\\n    INSERT INTO @APPNAME@_CQOut\\nSELECT\\n    data.attributeValue(\\\"merchantid\\\") as merchantID,\\n    data.getText() as companyName\\nFROM @APPNAME@_Stream;\\n    CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()\\n    FORMAT USING JSONFormatter ()\\n    INPUT\\nFROM @APPNAME@_CQOut;\\n    END APPLICATION @APPNAME@;\",\n",
      "  \"generated_queries\": \"What is the process for creating a new application that extracts merchant IDs and company names from XML data and outputs them in JSON format?\",\n",
      "  \"file_name\": \"XmlV2ToJson.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 15 ==================================================\n",
      "{\n",
      "  \"tql\": \"    use PosTester;\\n    alter application PosApp;\\n    CREATE TYPE MerchantTxRate(\\n    merchantId String KEY,\\n    zip String,\\n    startTime DateTime,\\n    count int,\\n    totalAmount double,\\n    hourlyAve int,\\n    upperLimit double,\\n    lowerLimit double,\\n    category String,\\n    status String\\n    );\\n    end application PosApp;\\n    alter application PosApp recompile;\",\n",
      "  \"generated_queries\": \"What new merchant transaction rate data structure has been created for the PosApp application, and what attributes does it include?\",\n",
      "  \"file_name\": \"createType.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 16 ==================================================\n",
      "{\n",
      "  \"tql\": \"    STOP APPLICATION ORACLETOBIGQUERY;\\n    UNDEPLOY APPLICATION ORACLETOBIGQUERY;\\n    DROP APPLICATION ORACLETOBIGQUERY CASCADE;\\n    --create application\\n    CREATE APPLICATION ORACLETOBIGQUERY RECOVERY 5 SECOND INTERVAL;\\n    CREATE OR REPLACE SOURCE OracleSource USING OracleReader (\\n    ConnectionURL: '192.168.123.12:1521/ORCL',\\n    Tables: 'QATEST.ORATOBIGQALLDATATYPE',\\n    Username: 'qatest',\\n    Password: 'qatest',\\n    FetchSize:1\\n    ) OUTPUT TO CDCStream;\\n    CREATE OR REPLACE TARGET bqtables using BigqueryWriter(\\n    BQServiceAccountConfigurationPath:\\\"/Users/karthikmurugan/Downloads/bqtest-540227c31980.json\\\",\\n    projectId:\\\"bqtest-158706\\\",\\n    Tables: \\\"QATEST.ORATOBIGQALLDATATYPE,QATEST.ORATOBIGQALLDATATYPE\\\",\\n    BatchPolicy: \\\"eventCount:1,Interval:90\\\")\\n    INPUT\\nFROM CDCStream;\\n    CREATE OR REPLACE TARGET T1 using SysOut(name : \\\"some text\\\") INPUT\\nFROM CDCStream;\\n    END APPLICATION ORACLETOBIGQUERY;\\n    DEPLOY APPLICATION ORACLETOBIGQUERY;\\n    START APPLICATION ORACLETOBIGQUERY;\",\n",
      "  \"generated_queries\": \"What steps are involved in setting up and deploying the OracleToBigQuery application for data streaming from Oracle to BigQuery?\",\n",
      "  \"file_name\": \"OracleToBigQueryAlldatatypes.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 17 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop application reconnect;\\n    undeploy application reconnect;\\n    drop application reconnect cascade;\\n    CREATE APPLICATION reconnect recovery 1 second interval;\\n    CREATE  SOURCE mssqlsource USING MssqlReader  (\\n    Username: '@USERNAME@',\\n    Password: '@PASSWORD@',\\n    ConnectionURL: '@URL@',\\n    Tables: '@TABLE@',\\n    FetchSize: 1\\n    )\\n    OUTPUT TO sqlstream;\\n    CREATE TARGET dbtarget USING CassandraWriter(\\n    ConnectionURL:'@URL@',\\n    Username:'@USERNAME@',\\n    Password:'@PASSWORD@',\\n    ConnectionRetryPolicy: 'retryInterval=15s,maxRetries=2',\\n    BatchPolicy:'EventCount:5,Interval:30',\\n    CommitPolicy:'EventCount:5,Interval:30',\\n    Tables: '@TABLES@'\\n    ) INPUT\\nFROM sqlstream;\\n    create Target tSysOut using Sysout(name:OrgData) input from sqlstream;\\n    end application reconnect;\\n    deploy application reconnect;\\n    start application reconnect;\",\n",
      "  \"generated_queries\": \"What steps are involved in configuring the \\\"reconnect\\\" application to read from a SQL Server source and write to a Cassandra target while also outputting to the system console?\",\n",
      "  \"file_name\": \"oracle_CassandraReconnect.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 18 ==================================================\n",
      "{\n",
      "  \"tql\": \"    STOP APPLICATION App1;\\n    UNDEPLOY APPLICATION App1;\\n    DROP APPLICATION App1 CASCADE;\\n    CREATE APPLICATION App1;\\n    CREATE FLOW AgentFlow;\\n    CREATE OR REPLACE SOURCE App1_FileSource USING FileReader (\\n    directory:'@dirPath@',\\n    wildcard:'posdata.csv',\\n    positionByEOF:false\\n    )\\n    PARSE USING DSVParser (\\n    ) OUTPUT TO App1_SampleStream;\\n    END FLOW AgentFlow;\\n    CREATE FLOW ServerFlow;\\n    CREATE OR REPLACE TARGET App1_NullTarget using NullWriter()\\n    INPUT\\nFROM App1_SampleStream;\\n    END FLOW ServerFlow;\\n    END APPLICATION App1;\\n    deploy application App1 on any in ServerDG1 with AgentFlow on any in Agents, ServerFlow on any in ServerDG1;\\n    START APPLICATION App1;\\n    STOP APPLICATION App2;\\n    UNDEPLOY APPLICATION App2;\\n    DROP APPLICATION App2 CASCADE;\\n    CREATE APPLICATION App2;\\n    CREATE FLOW AgentFlow2;\\n    CREATE OR REPLACE SOURCE App2_FileSource USING FileReader (\\n    directory:'@dirPath@',\\n    wildcard:'posdata.csv',\\n    positionByEOF:false\\n    )\\n    PARSE USING DSVParser (\\n    ) OUTPUT TO App2_SampleStream;\\n    END FLOW AgentFlow2;\\n    CREATE FLOW ServerFlow2;\\n    CREATE OR REPLACE TARGET App2_NullTarget using NullWriter()\\n    INPUT\\nFROM App2_SampleStream;\\n    END FLOW ServerFlow2;\\n    END APPLICATION App2;\\n    deploy application App2 on any in ServerDG1 with AgentFlow2 on any in Agents, ServerFlow2 on any in ServerDG1;\\n    START APPLICATION App2;\\n    STOP APPLICATION App3;\\n    UNDEPLOY APPLICATION App3;\\n    DROP APPLICATION App3 CASCADE;\\n    CREATE APPLICATION App3;\\n    CREATE OR REPLACE SOURCE App3_FileSource USING FileReader (\\n    directory:'@dirPath@',\\n    wildcard:'posdata.csv',\\n    positionByEOF:false\\n    )\\n    PARSE USING DSVParser (\\n    ) OUTPUT TO App3_SampleStream;\\n    CREATE OR REPLACE TARGET App3_NullTarget using NullWriter()\\n    INPUT\\nFROM App3_SampleStream;\\n    END APPLICATION App3;\\n    DEPLOY APPLICATION App3;\\n    START APPLICATION App3;\\n    STOP APPLICATION App4;\\n    UNDEPLOY APPLICATION App4;\\n    DROP APPLICATION App4 CASCADE;\\n    CREATE APPLICATION App4;\\n    CREATE OR REPLACE SOURCE App4_FileSource USING FileReader (\\n    directory:'@dirPath@',\\n    wildcard:'posdata.csv',\\n    positionByEOF:false\\n    )\\n    PARSE USING DSVParser (\\n    ) OUTPUT TO App4_SampleStream;\\n    CREATE OR REPLACE TARGET App4_NullTarget using NullWriter()\\n    INPUT\\nFROM App4_SampleStream;\\n    END APPLICATION App4;\\n    DEPLOY APPLICATION App4 ON ONE IN ServerDG1;\\n    START APPLICATION App4;\\n    STOP APPLICATION App5;\\n    UNDEPLOY APPLICATION App5;\\n    DROP APPLICATION App5 CASCADE;\\n    CREATE APPLICATION App5;\\n    CREATE OR REPLACE SOURCE App5_FileSource USING FileReader (\\n    directory:'@dirPath@',\\n    wildcard:'posdata.csv',\\n    positionByEOF:false\\n    )\\n    PARSE USING DSVParser (\\n    ) OUTPUT TO App5_SampleStream;\\n    CREATE OR REPLACE TARGET App5_NullTarget using NullWriter()\\n    INPUT\\nFROM App5_SampleStream;\\n    END APPLICATION App5;\\n    DEPLOY APPLICATION App5 ON ONE IN ServerDG1;\\n    START APPLICATION App5;\\n    STOP APPLICATION App6;\\n    UNDEPLOY APPLICATION App6;\\n    DROP APPLICATION App6 CASCADE;\\n    CREATE APPLICATION App6;\\n    CREATE OR REPLACE SOURCE App6_FileSource USING FileReader (\\n    directory:'@dirPath@',\\n    wildcard:'posdata.csv',\\n    positionByEOF:false\\n    )\\n    PARSE USING DSVParser (\\n    ) OUTPUT TO App6_SampleStream;\\n    CREATE OR REPLACE TARGET App6_NullTarget using NullWriter()\\n    INPUT\\nFROM App6_SampleStream;\\n    END APPLICATION App6;\\n    DEPLOY APPLICATION App6 ON ONE IN ServerDG1;\\n    START APPLICATION App6;\\n    STOP APPLICATION App7;\\n    UNDEPLOY APPLICATION App7;\\n    DROP APPLICATION App7 CASCADE;\\n    CREATE APPLICATION App7;\\n    CREATE OR REPLACE SOURCE App7_FileSource USING FileReader (\\n    directory:'@dirPath@',\\n    wildcard:'posdata.csv',\\n    positionByEOF:false\\n    )\\n    PARSE USING DSVParser (\\n    ) OUTPUT TO App7_SampleStream;\\n    CREATE OR REPLACE TARGET App7_NullTarget using NullWriter()\\n    INPUT\\nFROM App7_SampleStream;\\n    END APPLICATION App7;\\n    DEPLOY APPLICATION App7 ON ONE IN ServerDG1;\\n    START APPLICATION App7;\\n    STOP APPLICATION App8;\\n    UNDEPLOY APPLICATION App8;\\n    DROP APPLICATION App8 CASCADE;\\n    CREATE APPLICATION App8;\\n    CREATE OR REPLACE SOURCE App8_FileSource USING FileReader (\\n    directory:'@dirPath@',\\n    wildcard:'posdata.csv',\\n    positionByEOF:false\\n    )\\n    PARSE USING DSVParser (\\n    ) OUTPUT TO App8_SampleStream;\\n    CREATE OR REPLACE TARGET App8_NullTarget using NullWriter()\\n    INPUT\\nFROM App8_SampleStream;\\n    END APPLICATION App8;\\n    DEPLOY APPLICATION App8 ON ONE IN ServerDG1;\\n    START APPLICATION App8;\\n    STOP APPLICATION App9;\\n    UNDEPLOY APPLICATION App9;\\n    DROP APPLICATION App9 CASCADE;\\n    CREATE APPLICATION App9;\\n    CREATE OR REPLACE SOURCE App9_FileSource USING FileReader (\\n    directory:'@dirPath@',\\n    wildcard:'posdata.csv',\\n    positionByEOF:false\\n    )\\n    PARSE USING DSVParser (\\n    ) OUTPUT TO App9_SampleStream;\\n    CREATE OR REPLACE TARGET App9_NullTarget using NullWriter()\\n    INPUT\\nFROM App9_SampleStream;\\n    END APPLICATION App9;\\n    DEPLOY APPLICATION App9;\\n    START APPLICATION App9;\\n    STOP APPLICATION App10;\\n    UNDEPLOY APPLICATION App10;\\n    DROP APPLICATION App10 CASCADE;\\n    CREATE APPLICATION App10;\\n    CREATE OR REPLACE SOURCE App10_FileSource USING FileReader (\\n    directory:'@dirPath@',\\n    wildcard:'posdata.csv',\\n    positionByEOF:false\\n    )\\n    PARSE USING DSVParser (\\n    ) OUTPUT TO App10_SampleStream;\\n    CREATE OR REPLACE TARGET App10_NullTarget using NullWriter()\\n    INPUT\\nFROM App10_SampleStream;\\n    END APPLICATION App10;\\n    DEPLOY APPLICATION App10;\\n    START APPLICATION App10;\\n    STOP APPLICATION App11;\\n    UNDEPLOY APPLICATION App11;\\n    DROP APPLICATION App11 CASCADE;\\n    CREATE APPLICATION App11;\\n    CREATE OR REPLACE SOURCE App11_FileSource USING FileReader (\\n    directory:'@dirPath@',\\n    wildcard:'posdata.csv',\\n    positionByEOF:false\\n    )\\n    PARSE USING DSVParser (\\n    ) OUTPUT TO App11_SampleStream;\\n    CREATE OR REPLACE TARGET App11_NullTarget using NullWriter()\\n    INPUT\\nFROM App11_SampleStream;\\n    END APPLICATION App11;\\n    DEPLOY APPLICATION App11;\\n    START APPLICATION App11;\\n    STOP APPLICATION App12;\\n    UNDEPLOY APPLICATION App12;\\n    DROP APPLICATION App12 CASCADE;\\n    CREATE APPLICATION App12;\\n    CREATE OR REPLACE SOURCE App12_FileSource USING FileReader (\\n    directory:'@dirPath@',\\n    wildcard:'posdata.csv',\\n    positionByEOF:false\\n    )\\n    PARSE USING DSVParser (\\n    ) OUTPUT TO App12_SampleStream;\\n    CREATE OR REPLACE TARGET App12_NullTarget using NullWriter()\\n    INPUT\\nFROM App12_SampleStream;\\n    END APPLICATION App12;\\n    DEPLOY APPLICATION App12;\\n    START APPLICATION App12;\\n    STOP APPLICATION App13;\\n    UNDEPLOY APPLICATION App13;\\n    DROP APPLICATION App13 CASCADE;\\n    CREATE APPLICATION App13;\\n    CREATE FLOW AgentFlow13;\\n    CREATE OR REPLACE SOURCE App13_FileSource USING FileReader (\\n    directory:'@dirPath@',\\n    wildcard:'posdata.csv',\\n    positionByEOF:false\\n    )\\n    PARSE USING DSVParser (\\n    ) OUTPUT TO App13_SampleStream;\\n    END FLOW AgentFlow13;\\n    CREATE FLOW ServerFlow13;\\n    CREATE OR REPLACE TARGET App13_NullTarget using NullWriter()\\n    INPUT\\nFROM App13_SampleStream;\\n    END FLOW ServerFlow13;\\n    END APPLICATION App13;\\n    deploy application App13 on any in ServerDG1 with AgentFlow13 on any in Agents, ServerFlow13 on any in ServerDG1;\\n    START APPLICATION App13;\",\n",
      "  \"generated_queries\": \"What are the steps taken to create, deploy, and start applications App1 through App13, and how are they configured to read from a specific file with a wildcard pattern?\",\n",
      "  \"file_name\": \"DGAgent.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 19 ==================================================\n",
      "{\n",
      "  \"tql\": \"    STOP @APPNAME@;\\n    UNDEPLOY APPLICATION @APPNAME@;\\n    DROP APPLICATION @APPNAME@ CASCADE;\\n    CREATE APPLICATION @APPNAME@ @Recovery@;\\n    CREATE SOURCE @APPNAME@_S USING MSSqlReader\\n    (\\n    Compression: false,\\n    cdcRoleName: 'STRIIM_READER',\\n    DatabaseName: 'QATEST',\\n    connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',\\n    ConnectionPoolSize: 1,\\n    FetchTransactionMetadata: false,\\n    StartPosition: 'EOF',\\n    Username: 'qatest',\\n    SendBeforeImage: true,\\n    AutoDisableTableCDC: true,\\n    ConnectionURL: 'localhost:1433',\\n    Tables: 'qatest.test01',\\n    adapterName: 'MSSqlReader',\\n    Password: 'w3b@ct10n'\\n    )\\n    OUTPUT TO @APPNAME@_SS;\\n    CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\\n    AllowQuotedNewlines:False,\\n    ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\\n    serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\\n    Encoding:'UTF-8',\\n    projectId: 'bigquerywritertest',\\n    Tables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\\n    Mode:'merge',\\n    datalocation: 'US',\\n    nullmarker: 'NULL',\\n    columnDelimiter: '|',\\n    BatchPolicy: 'eventCount:1,Interval:0',\\n    StandardSQL:true\\t,\\n    optimizedMerge:true\\n    ) INPUT\\nFROM @APPNAME@_ss;\\n    END APPLICATION @APPNAME@;\\n    DEPLOY APPLICATION @APPNAME@;\\n    --deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\\n    START APPLICATION @APPNAME@;\",\n",
      "  \"generated_queries\": \"How can I set up and deploy a new application that reads data from an MSSQL database and writes it to BigQuery with specific configurations?\",\n",
      "  \"file_name\": \"mssql_bq_optimizedMerge_diff_PK.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 20 ==================================================\n",
      "{\n",
      "  \"tql\": \"    --\\n    -- Crash Recovery Test 2 on two node cluster\\n    -- Bert Hashemi, WebAction, Inc.\\n    --\\n    -- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS\\n    --\\n    STOP APPLICATION N2S2CR2Tester.N2S2CRTest2;\\n    UNDEPLOY APPLICATION N2S2CR2Tester.N2S2CRTest2;\\n    DROP APPLICATION N2S2CR2Tester.N2S2CRTest2 CASCADE;\\n    CREATE APPLICATION N2S2CRTest2 RECOVERY 5 SECOND INTERVAL;\\n    CREATE FLOW DataAcquisitionN2S2CRTest2;\\n    CREATE SOURCE CsvSourceN2S2CRTest2 USING CSVReader (\\n    directory:'@TEST-DATA-PATH@',\\n    header:Yes,\\n    wildcard:'RecovTestData.csv',\\n    columndelimiter:',',\\n    blocksize: 10240,\\n    positionByEOF:false,\\n    trimquote:false\\n    ) OUTPUT TO CsvStream;\\n    END FLOW DataAcquisitionN2S2CRTest2;\\n    CREATE FLOW DataProcessingN2S2CRTest2;\\n    CREATE TYPE WactionTypeN2S2CRTest2 (\\n    companyName String,\\n    merchantId String KEY,\\n    dateTime DateTime,\\n    amount double,\\n    city String\\n    );\\n    CREATE STREAM DataStream OF WactionTypeN2S2CRTest2;\\n    CREATE CQ CsvToWaction\\n    INSERT INTO DataStream\\nSELECT\\n    data[0],\\n    data[1],\\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\\n    TO_DOUBLE(data[7]),\\n    data[10]\\nFROM CsvStream;\\n    CREATE JUMPING WINDOW DataStream5Minutes\\n    OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\\n    CREATE WACTIONSTORE WactionsN2S2CRTest2 CONTEXT OF WactionTypeN2S2CRTest2\\n    EVENT TYPES ( WactionTypeN2S2CRTest2 )\\n    @PERSIST-TYPE@\\n    CREATE CQ InsertWactionsN2S2CRTest2\\n    INSERT INTO WactionsN2S2CRTest2\\nSELECT\\n    *\\nFROM DataStream5Minutes;\\n    END FLOW DataProcessingN2S2CRTest2;\\n    END APPLICATION N2S2CRTest2;\",\n",
      "  \"generated_queries\": \"What are the steps taken to set up and configure the N2S2CRTest2 application for crash recovery testing, including the data source and processing flow used?\",\n",
      "  \"file_name\": \"N2S2CRTest2.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 21 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop application AzureApp;\\n    undeploy application AzureApp;\\n    drop application AzureApp cascade;\\n    create application AzureApp\\n    RECOVERY 10 second interval;\\n    create source CSVSource using FileReader (\\n    directory:'@DIR@',\\n    WildCard:'@WILDCARD@',\\n    positionByEOF:false,\\n    charset:'UTF-8'\\n    )\\n    parse using DSVParser (\\n    header:'yes'\\n    )\\n    OUTPUT TO CsvStream;\\n    Create Type CSVType (\\n    merchantId String,\\n    dateTime DateTime,\\n    hourValue int,\\n    curr String,\\n    amount double,\\n    zip String\\n    );\\n    Create Stream TypedCSVStream of CSVType;\\n    CREATE CQ CsvToPosData\\n    INSERT INTO TypedCSVStream\\nSELECT data[1],\\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\\n    DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\\n    data[6],\\n    TO_DOUBLE(data[7]),\\n    data[9]\\nFROM CsvStream;\\n    create Target BlobT using AzureBlobWriter(\\n    accountname:'@ACCNAME@',\\n    accountaccesskey:'@ACCKEY@',\\n    containername:'@CONT@',\\n    blobname:'@BLOB@',\\n    foldername:'@FOLDER@',\\n    uploadpolicy:'EventCount:30,interval:5s'\\n    )\\n    format using AvroFormatter (\\n    )\\n    input from TypedCSVStream;\\n    end application AzureApp;\\n    deploy application AzureApp in default;\\n    start application AzureApp;\",\n",
      "  \"generated_queries\": \"How can I set up an application in Azure to process CSV data and store the results as Avro files in Azure Blob Storage?\",\n",
      "  \"file_name\": \"FileWAzurewithAvro.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 22 ==================================================\n",
      "{\n",
      "  \"tql\": \"    STOP APPLICATION @APPNAME@;\\n    UNDEPLOY APPLICATION @APPNAME@;\\n    DROP APPLICATION @APPNAME@ CASCADE;\\n    CREATE APPLICATION @APPNAME@ recovery 1 second interval;\\n    CREATE SOURCE @SOURCENAME@ USING OracleReader\\n    (\\n    Username: '@SRCUSERNAME@',\\n    Password: '@SRCPASSWORD@',\\n    ConnectionURL: '@SRCURL',\\n    Tables: '@SRCTABLE',\\n    FetchSize: '@FETCHSIZE@',\\n    CommittedTransactions: true\\n    )\\n    OUTPUT TO @STREAM@ ;\\n    CREATE TARGET @TARGETNAME@ using DatabaseWriter\\n    (\\n    ConnectionURL: '@TARGETURL',\\n    username: '@TARGETUSERNAME@',\\n    Password: '@TARGETPASSWORD@',\\n    Tables: '@TARGETTABLE@',\\n    BatchPolicy:'EventCount:1,Interval:1',\\n    CommitPolicy:'EventCount:1,Interval:1'\\n    )\\n    INPUT\\nFROM @STREAM@;\\n    END APPLICATION @APPNAME@;\\n    DEPLOY APPLICATION @APPNAME@;\\n    START APPLICATION @APPNAME@;\",\n",
      "  \"generated_queries\": \"- How can I stop, undeploy, and then completely remove an application before creating a new one with a recovery interval, as well as set up a source and target for data transfer from an Oracle database?\",\n",
      "  \"file_name\": \"OracleTemplate.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 23 ==================================================\n",
      "{\n",
      "  \"tql\": \"    drop namespace test cascade force;\\n    create namespace test;\\n    use test;\\n    stop @AppName@;\\n    undeploy application @AppName@;\\n    drop application @AppName@ cascade;\\n    CREATE APPLICATION @AppName@ recovery 5 second Interval;\\n    create source @srcName@ USING MySQLReader\\n    (\\n    Username:'@srcusername@',\\n    Password:'@srcpassword@',\\n    ConnectionURL:'@srcurl@',\\n    Tables:'@srcschema@.@srctable@',\\n    sendBeforeImage:'true',\\n    FilterTransactionBoundaries:'true'\\n    )\\n    OUTPUT TO @outstreamname@;\\n    CREATE OR REPLACE TARGET @tgtName@ USING DatabaseWriter\\n    (\\n    CheckPointTable:'CHKPOINT',\\n    Username:'@tgtusername@',\\n    Password:'@tgtpassword@',\\n    BatchPolicy:'EventCount:1,Interval:0',\\n    CommitPolicy:'EventCount:1,Interval:0',\\n    ConnectionURL:'@tgturl@',\\n    Tables:'@srcschema@.@srctable@,@tgtschema@.@tgttable@'\\n    )\\n    INPUT\\nFROM @instreamname@;\\n    End APPLICATION @AppName@;\\n    deploy application @AppName@;\\n    start @AppName@;\",\n",
      "  \"generated_queries\": \"What steps do I need to follow to set up a data pipeline that reads from a MySQL database and writes to another database using a specific application configuration?\",\n",
      "  \"file_name\": \"mysqltomysql.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 24 ==================================================\n",
      "{\n",
      "  \"tql\": \"    CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  (\\n    Username: '@SRC_USERNAME@',\\n    Password: '@SRC_PASSWORD@',\\n    ConnectionURL: '@CDC_URL@',\\n    Tables: '@Source1Tables@',\\n    FetchSize: 1)\\n    OUTPUT TO @APPNAME@cdcStream;\",\n",
      "  \"generated_queries\": \"What are the configuration details for setting up a CDC (Change Data Capture) source connection, including username, password, connection URL, tables to monitor, and fetch size?\",\n",
      "  \"file_name\": \"DBWriter_Alter_Oracle.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 25 ==================================================\n",
      "{\n",
      "  \"tql\": \"    drop namespace test cascade force;\\n    create namespace test;\\n    use test;\\n    stop @AppName@;\\n    undeploy application @AppName@;\\n    drop application @AppName@ cascade;\\n    CREATE APPLICATION @AppName@ RECOVERY 1 SECOND INTERVAL;\\n    CREATE SOURCE @srcName@ USING OracleReader (\\n    Username: '@srcusername@',\\n    Password: '@srcpassword@',\\n    ConnectionURL: '@srcurl@',\\n    Tables: '@srcschema@.@srctable@'\\n    )\\n    OUTPUT TO @outstreamname@;\\n    CREATE OR REPLACE TARGET @tgtName@ USING DatabaseWriter\\n    (\\n    DatabaseProviderType:'Default',\\n    CheckPointTable:'CHKPOINT',\\n    PreserveSourceTransactionBoundary:'false',\\n    Username:'@tgtusername@',\\n    BatchPolicy:'EventCount:1,Interval:0',\\n    CommitPolicy:'EventCount:1,Interval:0',\\n    ConnectionURL:'@tgturl@',\\n    Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',\\n    Password:'@tgtpassword@'\\n    )\\n    INPUT\\nFROM @instreamname@;\\n    END APPLICATION @AppName@;\\n    deploy application @AppName@;\\n    start @AppName@;\",\n",
      "  \"generated_queries\": \"How can I create a new application in the test namespace that pulls data from an Oracle database and writes it to a different database, ensuring recovery and checkpointing are set up correctly?\",\n",
      "  \"file_name\": \"oracletosqlserver.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 26 ==================================================\n",
      "{\n",
      "  \"tql\": \"    CREATE TARGET @TARGET_NAME@ USING SpannerWriter (\\n    Tables: 'QATEST.%,testdb.test',\\n    ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\\n    instanceId: 'qatest'\\n    ) INPUT\\nFROM @STREAM@;\",\n",
      "  \"generated_queries\": \"What steps do I need to take to set up a target in Spanner for writing data from a specific stream, and which tables are involved in this process?\",\n",
      "  \"file_name\": \"SpannerWriter.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 27 ==================================================\n",
      "{\n",
      "  \"tql\": \"    --\\n    -- Recovery Test 37 with two sources, two jumping time windows, and one wactionstore -- all partitioned on the same key\\n    -- Nicholas Keene WebAction, Inc.\\n    --\\n    --   S1 -> Jt1W/p -> CQ1 -> WS\\n    --   S2 -> Jt2W/p -> CQ2 -> WS\\n    --\\n    STOP KStreamRecov37Tester.KStreamRecovTest37;\\n    UNDEPLOY APPLICATION KStreamRecov37Tester.KStreamRecovTest37;\\n    DROP APPLICATION KStreamRecov37Tester.KStreamRecovTest37 CASCADE;\\n    DROP USER KStreamRecov37Tester;\\n    DROP NAMESPACE KStreamRecov37Tester CASCADE;\\n    CREATE USER KStreamRecov37Tester IDENTIFIED BY KStreamRecov37Tester;\\n    GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov37Tester;\\n    CONNECT KStreamRecov37Tester KStreamRecov37Tester;\\n    CREATE APPLICATION KStreamRecovTest37 RECOVERY 5 SECOND INTERVAL;\\n    CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\\n    CREATE STREAM KafkaCsvStream1 OF Global.waevent using KafkaPropset;\\n    CREATE STREAM KafkaCsvStream2 OF Global.waevent using KafkaPropset;\\n    CREATE SOURCE CsvSource1 USING CSVReader (\\n    directory:'@TEST-DATA-PATH@',\\n    header:Yes,\\n    wildcard:'RecovTestDataLong.csv',\\n    columndelimiter:',',\\n    blocksize: 10240,\\n    positionByEOF:false,\\n    trimquote:false\\n    ) OUTPUT TO KafkaCsvStream1;\\n    CREATE SOURCE CsvSource2 USING CSVReader (\\n    directory:'@TEST-DATA-PATH@',\\n    header:Yes,\\n    wildcard:'RecovTestDataLong.csv',\\n    columndelimiter:',',\\n    blocksize: 10240,\\n    positionByEOF:false,\\n    trimquote:false\\n    ) OUTPUT TO KafkaCsvStream2;\\n    CREATE TYPE CsvData (\\n    merchantId String KEY,\\n    companyName String,\\n    dateTime DateTime,\\n    amount double\\n    );\\n    CREATE TYPE WactionData (\\n    firstCompanyName String KEY,\\n    dateTime DateTime,\\n    totalCompanies int,\\n    firstMerchantId String\\n    );\\n    CREATE STREAM DataStream1 OF CsvData\\n    PARTITION BY merchantId;\\n    CREATE STREAM DataStream2 OF CsvData\\n    PARTITION BY merchantId;\\n    CREATE CQ CsvToData1\\n    INSERT INTO DataStream1\\nSELECT\\n    data[1],\\n    data[0],\\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\\n    TO_DOUBLE(data[7])\\nFROM KafkaCsvStream1;\\n    CREATE CQ CsvToData2\\n    INSERT INTO DataStream2\\nSELECT\\n    data[1],\\n    data[0],\\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\\n    TO_DOUBLE(data[7])\\nFROM KafkaCsvStream2;\\n    CREATE JUMPING WINDOW DataStream5Minutes\\n    OVER DataStream1 KEEP WITHIN 1 SECOND\\n    PARTITION BY merchantId;\\n    CREATE JUMPING WINDOW DataStream6Minutes\\n    OVER DataStream2 KEEP WITHIN 2 SECOND\\n    PARTITION BY merchantId;\\n    CREATE WACTIONSTORE Wactions CONTEXT OF WactionData\\n    EVENT TYPES ( CsvData )\\n    @PERSIST-TYPE@\\n    CREATE CQ Data5ToWaction\\n    INSERT INTO Wactions\\nSELECT\\n    FIRST(p.companyName),\\n    FIRST(p.dateTime),\\n    COUNT(p.amount),\\n    FIRST(p.merchantId)\\nFROM DataStream5Minutes p\\n    GROUP BY p.merchantId;\\n    CREATE CQ Data6ToWaction\\n    INSERT INTO Wactions\\nSELECT\\n    FIRST(p.companyName),\\n    FIRST(p.dateTime),\\n    COUNT(p.amount),\\n    FIRST(p.merchantId)\\nFROM DataStream6Minutes p\\n    GROUP BY p.merchantId;\\n    END APPLICATION KStreamRecovTest37;\",\n",
      "  \"generated_queries\": \"What are the results from the recovery test concerning the total number of companies and the first merchant ID grouped by merchant ID over different time intervals using the CSV data streams?\",\n",
      "  \"file_name\": \"KStreamRecovTest37.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 28 ==================================================\n",
      "{\n",
      "  \"tql\": \"    --\\n    -- Recovery Test 23 with two sources, two sliding time windows, and one wactionstore -- all with no partitioning\\n    -- Nicholas Keene WebAction, Inc.\\n    --\\n    -- S1 -> St1W -> CQ1 -> WS\\n    -- S2 -> St2W -> CQ2 -> WS\\n    --\\n    STOP KStreamRecov23Tester.KStreamRecovTest23;\\n    UNDEPLOY APPLICATION KStreamRecov23Tester.KStreamRecovTest23;\\n    DROP APPLICATION KStreamRecov23Tester.KStreamRecovTest23 CASCADE;\\n    DROP USER KStreamRecov23Tester;\\n    DROP NAMESPACE KStreamRecov23Tester CASCADE;\\n    CREATE USER KStreamRecov23Tester IDENTIFIED BY KStreamRecov23Tester;\\n    GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov23Tester;\\n    CONNECT KStreamRecov23Tester KStreamRecov23Tester;\\n    CREATE APPLICATION KStreamRecovTest23 RECOVERY 5 SECOND INTERVAL;\\n    CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\\n    CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\\n    CREATE SOURCE CsvSource1 USING CSVReader (\\n    directory:'@TEST-DATA-PATH@',\\n    header:Yes,\\n    wildcard:'RecovTestDataLong.csv',\\n    columndelimiter:',',\\n    blocksize: 10240,\\n    positionByEOF:false,\\n    trimquote:false\\n    ) OUTPUT TO KafkaCsvStream;\\n    CREATE SOURCE CsvSource2 USING CSVReader (\\n    directory:'@TEST-DATA-PATH@',\\n    header:Yes,\\n    wildcard:'RecovTestDataLong.csv',\\n    columndelimiter:',',\\n    blocksize: 10240,\\n    positionByEOF:false,\\n    trimquote:false\\n    ) OUTPUT TO KafkaCsvStream;\\n    CREATE TYPE CsvData (\\n    merchantId String KEY,\\n    companyName String,\\n    dateTime DateTime,\\n    amount double\\n    );\\n    CREATE STREAM DataStream1 OF CsvData;\\n    CREATE STREAM DataStream2 OF CsvData;\\n    CREATE CQ CsvToData1\\n    INSERT INTO DataStream1\\nSELECT\\n    data[1],\\n    data[0],\\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\\n    TO_DOUBLE(data[7])\\nFROM KafkaCsvStream;\\n    CREATE CQ CsvToData2\\n    INSERT INTO DataStream2\\nSELECT\\n    data[1],\\n    data[0],\\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\\n    TO_DOUBLE(data[7])\\nFROM KafkaCsvStream;\\n    CREATE WINDOW DataStream5Minutes1\\n    OVER DataStream1 KEEP WITHIN 1 SECOND;\\n    CREATE WINDOW DataStream5Minutes2\\n    OVER DataStream2 KEEP WITHIN 2 SECOND;\\n    CREATE WACTIONSTORE Wactions CONTEXT OF CsvData\\n    EVENT TYPES ( CsvData )\\n    @PERSIST-TYPE@\\n    CREATE CQ DataToWaction1\\n    INSERT INTO Wactions\\nSELECT\\n    *\\nFROM DataStream5Minutes1;\\n    CREATE CQ DataToWaction2\\n    INSERT INTO Wactions\\nSELECT\\n    *\\nFROM DataStream5Minutes2;\\n    END APPLICATION KStreamRecovTest23;\",\n",
      "  \"generated_queries\": \"What steps are involved in setting up a recovery test application with multiple data sources, and how is the data processed and stored in the system?\",\n",
      "  \"file_name\": \"KStreamRecovTest23.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 29 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop application @APPNAME@;\\n    undeploy application @APPNAME@;\\n    drop application @APPNAME@ cascade;\\n    create application @APPNAME@ recovery 1 second interval;\\n    create source @APPNAME@_SRC Using FileReader(\\n    directory:'@DIRECTORY@',\\n    WildCard:'@FILENAME@',\\n    positionByEOF:false,\\n    charset:'UTF-8'\\n    )\\n    parse using CobolCopybookParser (\\n    copybookFileName : '@TD@/@PROP1@',\\n    dataFileFont: '@PROP2@',\\n    copybookSplit: '@PROP3@',\\n    dataFileOrganization: '@PROP4@',\\n    copybookDialect: '@PROP5@',\\n    skipIndent:'@PROP6@',\\n    DatahandlingScheme:'@PROP7@'\\n    --recordSelector: '@PROP8@'\\n    )\\n    OUTPUT TO @APPNAME@Stream;\\n    create Target @APPNAME@Target using FileWriter(\\n    filename :'@FILE@',\\n    directory : '@FOLDER@'\\n    )\\n    format using JsonFormatter (\\n    )\\n    input from @APPNAME@Stream;\\n    CREATE TYPE test_type (\\n    account_no com.fasterxml.jackson.databind.JsonNode,\\n    first_name com.fasterxml.jackson.databind.JsonNode,\\n    last_name com.fasterxml.jackson.databind.JsonNode,\\n    addr1 com.fasterxml.jackson.databind.JsonNode,\\n    Addr2 com.fasterxml.jackson.databind.JsonNode,\\n    City com.fasterxml.jackson.databind.JsonNode,\\n    State com.fasterxml.jackson.databind.JsonNode,\\n    Zip com.fasterxml.jackson.databind.JsonNode\\n    );\\n    Create stream cqAsJSONNodeStream of test_type;\\n    CREATE CQ GetPOAsJsonNodes\\n    INSERT into cqAsJSONNodeStream\\n    select\\n    data.get('ACCTS-RECORD').get('ACCOUNT-NO'),\\n    data.get('ACCTS-RECORD').get('NAME').get('FIRST-NAME'),\\n    data.get('ACCTS-RECORD').get('NAME').get('LAST-NAME'),\\n    data.get('ACCTS-RECORD').get('ADDRESS1'),\\n    data.get('ACCTS-RECORD').get('ADDRESS2'),\\n    data.get('ACCTS-RECORD').get('ADDRESS3').get('CITY'),\\n    data.get('ACCTS-RECORD').get('ADDRESS3').get('STATE'),\\n    data.get('ACCTS-RECORD').get('ADDRESS3').get('ZIP-CODE')\\n    from @APPNAME@Stream js;\\n    create type finaldtype(\\n    ACCOUNT_NO String,\\n    FIRST_NAME String,\\n    LAST_NAME String,\\n    ADDRESS1 String,\\n    ADDRESS2 String,\\n    CITY String,\\n    STATE String,\\n    ZIP_CODE String\\n    );\\n    CREATE CQ getdata\\n    INSERT into getdataStream\\n    select account_no.toString(),\\n    first_name.toString(),\\n    last_name.toString(),\\n    addr1.toString(),\\n    Addr2.toString(),\\n    City.toString(),\\n    State.toString(),\\n    Zip.toString()\\n    from cqAsJSONNodeStream x;\\n    create Target @APPNAME@DBTarget using DatabaseWriter(\\n    Username: 'qatest',\\n    Password: 'qatest',\\n    ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',\\n    BatchPolicy: 'EventCount:10000',\\n    CommitPolicy: 'EventCount:10000',\\n    Tables: 'QATEST.@APPNAME@'\\n    )\\n    input from getdataStream;\\n    end application @APPNAME@;\\n    deploy application @APPNAME@ on all in default;\\n    start application @APPNAME@;\",\n",
      "  \"generated_queries\": \"How can I deploy an application that reads data from a file using a Cobol copybook, processes it into JSON format, and then stores the results in an Oracle database?\",\n",
      "  \"file_name\": \"cobolparserACCT.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 30 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop application AzureDLSGen1_sanity;\\n    undeploy application AzureDLSGen1_sanity;\\n    drop application AzureDLSGen1_sanity cascade;\\n    create application AzureDLSGen1_sanity recovery 5 Second interval;\\n    create source CSVSource using FileReader (\\n    directory:'./Samples/AppData/',\\n    WildCard:'dynamicdirectory.csv',\\n    positionByEOF:false,\\n    charset:'UTF-8'\\n    )\\n    parse using DSVParser (\\n    header:'no'\\n    )\\n    OUTPUT TO CsvStream;\\n    create Target WriteToADLSGen1 using ADLSGen1Writer(\\n    filename:'',\\n    directory:'',\\n    datalakestorename:'',\\n    clientid:'',\\n    authtokenendpoint:'',\\n    clientkey:'',\\n    rolloverpolicy:'eventcount:100000'\\n    )\\n    format using DSVFormatter (\\n    )\\n    input from CsvStream;\\n    end application AzureDLSGen1_sanity;\\n    deploy application AzureDLSGen1_sanity;\\n    start application AzureDLSGen1_sanity;\",\n",
      "  \"generated_queries\": \"What steps are involved in setting up and deploying the AzureDLSGen1_sanity application to read data from a CSV file and write it to Azure Data Lake Storage Gen1?\",\n",
      "  \"file_name\": \"AzureDLS_GenRecovery.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 31 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop application @appname@;\\n    undeploy application @appname@;\\n    drop application @appname@ cascade;\\n    create application @appname@ recovery 1 second interval;\\n    CREATE SOURCE @parquetsrc@ USING FileReader (\\n    directory: '',\\n    positionByEOF: false,\\n    WildCard: '' )\\n    PARSE USING ParquetParser (\\n    )\\n    OUTPUT TO @appname@Streams;\\n    CREATE OR REPLACE CQ @appname@CQOrder3\\n    INSERT INTO @appname@Stream3\\nSELECT\\n    PUTUSERDATA(s,'schemaName',s.data.getSchema().getName())\\nFROM @appname@Streams s;\\n    CREATE TARGET @adlstarget@ USING Global.ADLSGen2Writer (\\n    accountname:'',\\n    sastoken:'',\\n    filesystemname:'',\\n    filename:'',\\n    directory:'',\\n    uploadpolicy:'eventcount:10' )\\n    format using AvroFormatter (\\n    )\\n    INPUT\\nFROM @appname@Stream3;\\n    END APPLICATION @appname@;\\n    deploy application @appname@ on all in default;\\n    start application @appname@;\",\n",
      "  \"generated_queries\": \"What steps are involved in creating, deploying, and starting a new streaming application that processes Parquet files and writes the output to ADLS Gen2 using Avro format?\",\n",
      "  \"file_name\": \"FileParquetToGen2Avro.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 32 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop ORAToBigquery;\\n    undeploy application ORAToBigquery;\\n    drop application ORAToBigquery cascade;\\n    CREATE APPLICATION ORAToBigquery RECOVERY 5 SECOND INTERVAL;\\n    CREATE OR REPLACE SOURCE Rac11g USING OracleReader (\\n    SupportPDB: false,\\n    SendBeforeImage: true,\\n    ReaderType: 'LogMiner',\\n    CommittedTransactions: false,\\n    FetchSize: 1,\\n    Password: 'manager',\\n    DDLTracking: false,\\n    StartTimestamp: 'null',\\n    OutboundServerProcessName: 'WebActionXStream',\\n    OnlineCatalog: true,\\n    ConnectionURL: '192.168.33.10:1521/XE',\\n    SkipOpenTransactions: false,\\n    Compression: false,\\n    QueueSize: 40000,\\n    RedoLogfiles: 'null',\\n    Tables: 'SYSTEM.GGAUTHORIZATIONS',\\n    Username: 'system',\\n    FilterTransactionBoundaries: true,\\n    adapterName: 'OracleReader',\\n    XstreamTimeOut: 600,\\n    connectionRetryPolicy: 'timeOut=30, retryInterval=60, maxRetries=3'\\n    )\\n    OUTPUT TO DataStream;\\n    CREATE OR REPLACE TARGET Target1 USING SysOut (\\n    name: \\\"dstream\\\"\\n    )\\n    INPUT\\nFROM DataStream;\\n    CREATE OR REPLACE TARGET Target2 using BigqueryWriter(\\n    BQServiceAccountConfigurationPath:\\\"/Users/ravipathak/Downloads/big-querytest-1963ae421e90.json\\\",\\n    projectId:\\\"big-querytest\\\",\\n    Tables: \\\"SYSTEM.GGAUTHORIZATIONS,testing1.ggauthorisation\\\",\\n    parallelismCount: 2,\\n    BatchPolicy: \\\"eventCount:100000,Interval:0\\\")\\n    INPUT\\nFROM DataStream;\\n    END APPLICATION ORAToBigquery;\\n    deploy application ORAToBigquery;\\n    start ORAToBigquery;\",\n",
      "  \"generated_queries\": \"What are the steps involved in deploying and configuring the ORAToBigquery application to transfer data from an Oracle source to BigQuery?\",\n",
      "  \"file_name\": \"template.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 33 ==================================================\n",
      "{\n",
      "  \"tql\": \"    CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\\n    CREATE OR REPLACE SOURCE @APPNAME@_src USING DatabaseReader (\\n    Tables: '',\\n    ConnectionURL: '',\\n    Password: '',\\n    Username: ''\\n    )\\n    OUTPUT TO @APPNAME@stream;\\n    CREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (\\n    bucketname: '',\\n    uploadpolicy: '',\\n    UploadConfiguration: '',\\n    objectname: '' )\\n    FORMAT USING JSONFormatter (\\n    members:'data'\\n    )\\n    INPUT\\nFROM @APPNAME@stream;\\n    END APPLICATION @APPNAME@;\",\n",
      "  \"generated_queries\": \"What is the process to create a new data application that reads from a database and outputs the data to an S3 bucket in JSON format?\",\n",
      "  \"file_name\": \"OracleILToS3.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 34 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop @APPNAME@;\\n    undeploy application @APPNAME@;\\n    drop application @APPNAME@ cascade;\\n    CREATE APPLICATION @APPNAME@ recovery 5 second interval;\\n    Create Source @SourceName@ Using DatabaseReader\\n    (\\n    Username:'@UserName@',\\n    Password:'@Password@',\\n    ConnectionURL:'@SourceConnectionURL@',\\n    Tables:'qatest.@SourceTable@',\\n    Fetchsize:1\\n    )\\n    Output To @SRCINPUTSTREAM@;\\n    CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\\n    ConnectionURL:'@TargetConnectionURL@',\\n    Username:'@UserName@',\\n    Password:'@Password@',\\n    BatchPolicy:'EventCount:1,Interval:1',\\n    Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\\n    ) INPUT\\nFROM @SRCINPUTSTREAM@;\\n    create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\\n    END APPLICATION @APPNAME@;\\n    deploy application @APPNAME@;\\n    start @APPNAME@;\",\n",
      "  \"generated_queries\": \"\\\"What steps are involved in stopping, undeploying, and recreating the application @APPNAME@ with a recovery interval, including details about the source and target database configurations?\\\"\",\n",
      "  \"file_name\": \"PostgresDBR.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 35 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop ADW;\\n    undeploy application ADW;\\n    DROP APPLICATION ADW CASCADE;\\n    CREATE APPLICATION ADW recovery 5 second interval;\\n    Create Source OracleSource Using OracleReader\\n    (\\n    Username:'@ORACLE-USERNAME',\\n    Password:'@ORACLE-PASSWORD',\\n    ConnectionURL: '@ORACLE-IP@',\\n    Tables: '@SOURCE-TABLES@',\\n    FetchSize:'@FETCH-SIZE@'\\n    )\\n    Output To str;\\n    create target AzureTarget using AzureSQLDWHWriter (\\n    ConnectionURL: '@SQLDW-URL@',\\n    username: '@SQLDW-USERNAME@',\\n    password: '@SQLDW-PASSWORD@',\\n    AccountName: '@STORAGEACCOUNT@',\\n    AccountAccessKey: '@ACCESSKEY@',\\n    Tables: '@TARGET-TABLES@',\\n    uploadpolicy:'@EVENT-COUNT@'\\n    ) INPUT\\nFROM str;\\n    create Target t2 using SysOut(name:Foo2) input from str;\\n    END APPLICATION ADW;\\n    deploy application ADW;\\n    start application ADW;\",\n",
      "  \"generated_queries\": \"What steps are involved in setting up and deploying the ADW application to transfer data from an Oracle database to an Azure SQL Data Warehouse?\",\n",
      "  \"file_name\": \"OracleToAzure.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 36 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop @APPNAME@;\\n    undeploy application @APPNAME@;\\n    drop application @APPNAME@ cascade;\\n    CREATE APPLICATION @APPNAME@;\\n    Create Source @SourceName1@ Using Ojet\\n    (\\n    Username:'@OJET-UNAME@',\\n    Password:'@OJET-PASSWORD@',\\n    ConnectionURL:'@OCI-URL@',\\n    Tables:'@SourceTable@'\\n    )\\n    Output To @SRCINPUTSTREAM1@;\\n    Create Source @SourceName2@ Using Ojet\\n    (\\n    Username:'@OJET-UNAME@',\\n    Password:'@OJET-PASSWORD@',\\n    ConnectionURL:'@OCI-URL@',\\n    Tables:'@SourceTable@'\\n    )\\n    Output To @SRCINPUTSTREAM2@;\\n    Create Source @SourceName3@ Using Ojet\\n    (\\n    Username:'@OJET-UNAME@',\\n    Password:'@OJET-PASSWORD@',\\n    ConnectionURL:'@OCI-URL@',\\n    Tables:'@SourceTable@'\\n    )\\n    Output To @SRCINPUTSTREAM3@;\\n    Create Source @SourceName4@ Using Ojet\\n    (\\n    Username:'@OJET-UNAME@',\\n    Password:'@OJET-PASSWORD@',\\n    ConnectionURL:'@OCI-URL@',\\n    Tables:'@SourceTable@'\\n    )\\n    Output To @SRCINPUTSTREAM4@;\\n    Create Source @SourceName5@ Using Ojet\\n    (\\n    Username:'@OJET-UNAME@',\\n    Password:'@OJET-PASSWORD@',\\n    ConnectionURL:'@OCI-URL@',\\n    Tables:'@SourceTable@'\\n    )\\n    Output To @SRCINPUTSTREAM5@;\\n    Create Source @SourceName6@ Using Ojet\\n    (\\n    Username:'@OJET-UNAME@',\\n    Password:'@OJET-PASSWORD@',\\n    ConnectionURL:'@OCI-URL@',\\n    Tables:'@SourceTable@'\\n    )\\n    Output To @SRCINPUTSTREAM6@;\\n    Create Source @SourceName7@ Using Ojet\\n    (\\n    Username:'@OJET-UNAME@',\\n    Password:'@OJET-PASSWORD@',\\n    ConnectionURL:'@OCI-URL@',\\n    Tables:'@SourceTable@'\\n    )\\n    Output To @SRCINPUTSTREAM7@;\\n    Create Source @SourceName8@ Using Ojet\\n    (\\n    Username:'@OJET-UNAME@',\\n    Password:'@OJET-PASSWORD@',\\n    ConnectionURL:'@OCI-URL@',\\n    Tables:'@SourceTable@'\\n    )\\n    Output To @SRCINPUTSTREAM8@;\\n    Create Source @SourceName9@ Using Ojet\\n    (\\n    Username:'@OJET-UNAME@',\\n    Password:'@OJET-PASSWORD@',\\n    ConnectionURL:'@OCI-URL@',\\n    Tables:'@SourceTable@'\\n    )\\n    Output To @SRCINPUTSTREAM9@;\\n    Create Source @SourceName10@ Using Ojet\\n    (\\n    Username:'@OJET-UNAME@',\\n    Password:'@OJET-PASSWORD@',\\n    ConnectionURL:'@OCI-URL@',\\n    Tables:'@SourceTable@'\\n    )\\n    Output To @SRCINPUTSTREAM10@;\\n    CREATE TARGET @targetName1@ USING DatabaseWriter(\\n    ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\\n    Username:'@UN@',\\n    Password:'@PWD@',\\n    BatchPolicy:'EventCount:1,Interval:1',\\n    Tables: '@Tablemapping@'\\n    ) INPUT\\nFROM @SRCINPUTSTREAM1@;\\n    CREATE TARGET @targetName2@ USING DatabaseWriter(\\n    ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\\n    Username:'@UN@',\\n    Password:'@PWD@',\\n    BatchPolicy:'EventCount:1,Interval:1',\\n    Tables: '@Tablemapping@'\\n    ) INPUT\\nFROM @SRCINPUTSTREAM2@;\\n    CREATE TARGET @targetName3@ USING DatabaseWriter(\\n    ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\\n    Username:'@UN@',\\n    Password:'@PWD@',\\n    BatchPolicy:'EventCount:1,Interval:1',\\n    Tables: '@Tablemapping@'\\n    ) INPUT\\nFROM @SRCINPUTSTREAM3@;\\n    CREATE TARGET @targetName4@ USING DatabaseWriter(\\n    ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\\n    Username:'@UN@',\\n    Password:'@PWD@',\\n    BatchPolicy:'EventCount:1,Interval:1',\\n    Tables: '@Tablemapping@'\\n    ) INPUT\\nFROM @SRCINPUTSTREAM4@;\\n    CREATE TARGET @targetName5@ USING DatabaseWriter(\\n    ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\\n    Username:'@UN@',\\n    Password:'@PWD@',\\n    BatchPolicy:'EventCount:1,Interval:1',\\n    Tables: '@Tablemapping@'\\n    ) INPUT\\nFROM @SRCINPUTSTREAM5@;\\n    CREATE TARGET @targetName6@ USING DatabaseWriter(\\n    ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\\n    Username:'@UN@',\\n    Password:'@PWD@',\\n    BatchPolicy:'EventCount:1,Interval:1',\\n    Tables: '@Tablemapping@'\\n    ) INPUT\\nFROM @SRCINPUTSTREAM6@;\\n    CREATE TARGET @targetName7@ USING DatabaseWriter(\\n    ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\\n    Username:'@UN@',\\n    Password:'@PWD@',\\n    BatchPolicy:'EventCount:1,Interval:1',\\n    Tables: '@Tablemapping@'\\n    ) INPUT\\nFROM @SRCINPUTSTREAM7@;\\n    CREATE TARGET @targetName8@ USING DatabaseWriter(\\n    ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\\n    Username:'@UN@',\\n    Password:'@PWD@',\\n    BatchPolicy:'EventCount:1,Interval:1',\\n    Tables: '@Tablemapping@'\\n    ) INPUT\\nFROM @SRCINPUTSTREAM8@;\\n    CREATE TARGET @targetName9@ USING DatabaseWriter(\\n    ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\\n    Username:'@UN@',\\n    Password:'@PWD@',\\n    BatchPolicy:'EventCount:1,Interval:1',\\n    Tables: '@Tablemapping@'\\n    ) INPUT\\nFROM @SRCINPUTSTREAM9@;\\n    CREATE TARGET @targetName10@ USING DatabaseWriter(\\n    ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\\n    Username:'@UN@',\\n    Password:'@PWD@',\\n    BatchPolicy:'EventCount:1,Interval:1',\\n    Tables: '@Tablemapping@'\\n    ) INPUT\\nFROM @SRCINPUTSTREAM10@;\\n    END APPLICATION @APPNAME@;\\n    deploy application @APPNAME@ in default;\\n    start @APPNAME@;\",\n",
      "  \"generated_queries\": \"How can I create and deploy an application that connects to multiple sources using Ojet while writing to an Oracle database?\",\n",
      "  \"file_name\": \"OjetALMMultipleReader.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 37 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop @APPNAME@;\\n    undeploy application @APPNAME@;\\n    drop application @APPNAME@ cascade;\\n    CREATE APPLICATION @APPNAME@ recovery 5 second interval;\\n    Create Source @SourceName@ Using PostgreSQLReader\\n    (\\n    Username:'@UserName@',\\n    Password:'@Password@',\\n    ConnectionURL:'@SourceConnectionURL@',\\n    Tables:'qatest.@SourceTable@',\\n    Fetchsize:1\\n    )\\n    Output To @SRCINPUTSTREAM@;\\n    CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\\n    ConnectionURL:'@TargetConnectionURL@',\\n    Username:'@UserName@',\\n    Password:'@Password@',\\n    BatchPolicy:'EventCount:1,Interval:1',\\n    Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\\n    ) INPUT\\nFROM @SRCINPUTSTREAM@;\\n    create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\\n    END APPLICATION @APPNAME@;\\n    deploy application @APPNAME@;\\n    start @APPNAME@;\",\n",
      "  \"generated_queries\": \"How do I create, deploy, and start an application that reads from a PostgreSQL source and writes to both a database and the system output?\",\n",
      "  \"file_name\": \"PostgresCDC.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 38 ==================================================\n",
      "{\n",
      "  \"tql\": \"    --\\n    -- Recovery Test T20\\n    -- Nicholas Keene, WebAction, Inc.\\n    --\\n    -- Snum -> CQ -> WS\\n    --\\n    UNDEPLOY APPLICATION NameT20.T20;\\n    DROP APPLICATION NameT20.T20 CASCADE;\\n    CREATE APPLICATION T20;\\n    CREATE FLOW DataAcquisitionT20;\\n    CREATE SOURCE CsvSourceT20 USING NumberSource (\\n    lowValue: '1',\\n    highValue: '1003',\\n    delayMillis: '10',\\n    delayNanos: '0',\\n    repeat: 'false'\\n    )\\n    OUTPUT TO OutputStreamT20;\\n    END FLOW DataAcquisitionT20;\\n    CREATE FLOW DataProcessingT20;\\n    Create Target OutputTargetT20\\n    Using Sysout (name: 'OutputTargetT20')\\n    Input From OutputStreamT20;\\n    END FLOW DataProcessingT20;\\n    END APPLICATION T20;\",\n",
      "  \"generated_queries\": \"What steps are involved in the creation and configuration of the T20 application, including data acquisition and processing?\",\n",
      "  \"file_name\": \"T20.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 39 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop application GGTrailReaderApp;\\n    undeploy application GGTrailReaderApp;\\n    drop application GGTrailReaderApp cascade;\\n    create application GGTrailReaderApp recovery 5 second interval;\\n    create source GGTrailSource using GGTrailReader (\\n    tRaildIrectory:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario1',\\n    tRAilfilepattern:'n1*',\\n    positionByEOF:false,\\n    FilterTransactionBoundaries: true,\\n    DefinitionFile:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario1/Scn1_beforeddl.def',\\n    captureCDdl: true,\\n    CDDLAction:'Process',\\n    --CDDLAction:'Ignore',\\n    TrailByTeOrder:'LittleEndian',\\n    recoveryInterval: 5\\n    )\\n    OUTPUT TO GGTrailStream;\\n    create Target t2 using SysOut(name:Foo2) input from GGTrailStream;\\n    CREATE TARGET WriteCDCOracle1 USING DatabaseWriter(\\n    ConnectionURL:'jdbc:oracle:thin:@//localhost/orcl',\\n    Username:'qatest',\\n    Password:'qatest',\\n    BatchPolicy:'Eventcount:1,Interval:1',\\n    CommitPolicy:'Eventcount:1,Interval:1',\\n    Checkpointtable:'RGRN_CHKPOINT',\\n    Tables:'QATEST.GGDDL1,QATEST.GGDDL1_TGT'\\n    ) INPUT\\nFROM GGTrailStream1;\\n    end application GGTrailReaderApp;\\n    deploy application GGTrailReaderApp;\\n    start application GGTrailReaderApp;\",\n",
      "  \"generated_queries\": \"What steps are needed to stop, undeploy, drop, and then recreate the GGTrailReaderApp application with a specific source and target configuration in order to process trail files from a specified directory?\",\n",
      "  \"file_name\": \"scn1.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 40 ==================================================\n",
      "{\n",
      "  \"tql\": \"    DROP APPLICATION ns1.OPExample cascade;\\n    DROP NAMESPACE ns1 cascade;\\n    CREATE OR REPLACE NAMESPACE ns1;\\n    USE ns1;\\n    CREATE APPLICATION OPExample;\\n    CREATE source CsvDataSource USING FileReader (\\n    directory:'@TEST-DATA-PATH@',\\n    wildcard:'PosDataPreview.csv',\\n    positionByEOF:false\\n    )\\n    PARSE USING DSVParser (\\n    header:Yes,\\n    trimquote:false\\n    )\\n    OUTPUT TO CsvStream;\\n    CREATE TYPE MerchantHourlyAve(\\n    merchantId String,\\n    hourValue integer,\\n    hourlyAve integer\\n    );\\n    CREATE CACHE HourlyAveLookup using FileReader (\\n    directory: '@TEST-DATA-PATH@',\\n    wildcard: 'hourlyData.txt'\\n    )\\n    PARSE USING DSVParser (\\n    header: Yes,\\n    trimquote:false,\\n    trimwhitespace:true\\n    )\\n    QUERY (keytomap:'merchantId')\\n    OF MerchantHourlyAve;\\n    CREATE CQ CsvToPosData\\n    INSERT INTO PosDataStream partition by merchantId\\nSELECT TO_STRING(data[1]) as merchantId,\\n    TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\\n    DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,\\n    TO_DOUBLE(data[7]) as amount,\\n    TO_INT(data[9]) as zip\\nFROM CsvStream;\\n    CREATE CQ cq2\\n    INSERT INTO SendToOPStream\\nSELECT makeList(dateTime) as dateTime,\\n    makeList(zip) as zip\\nFROM PosDataStream;\\n    CREATE TYPE ReturnFromOPStream_Type ( time DateTime , val Integer );\\n    CREATE STREAM ReturnFromOPStream OF ReturnFromOPStream_Type;\\n    CREATE TARGET OPExampleTarget\\n    USING FileWriter (filename: 'OPExampleOut')\\n    FORMAT USING JSONFormatter()\\n    INPUT\\nFROM ReturnFromOPStream;\\n    CREATE OPEN PROCESSOR testOp USING Global.TupleConverter ( lastItemSeen: 0, ahead: 1 )\\n    INSERT INTO ReturnFromOPStream\\nFROM SendToOPStream ENRICH WITH HourlyAveLookup;\\n    END APPLICATION OPExample;\",\n",
      "  \"generated_queries\": \"How can I set up a data processing application to read CSV data, enrich it with hourly averages from a lookup file, and output the results in JSON format?\",\n",
      "  \"file_name\": \"op2.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 41 ==================================================\n",
      "{\n",
      "  \"tql\": \"    create application FileXML;\\n    create source XMLSource using FileReader (\\n    Directory:'@TEST-DATA-PATH@',\\n    WildCard:'books.xml',\\n    positionByEOF:false\\n    )\\n    parse using XMLParser (\\n    RootNode:'/catalog/book'\\n    )\\n    OUTPUT TO XmlStream;\\n    -- Below Sysout is added to test DEV-23437.  Not directly validated in the test except the App should not crash with sysout target\\n    CREATE TARGET XMLEventSYSout USING sysout  (\\n    name: 'XMLEventSYSoutOut' )\\n    INPUT\\nFROM XmlStream;\\n    create Target XMLDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/xmldata') input from XmlStream;\\n    end application FileXML;\",\n",
      "  \"generated_queries\": \"What operations are being performed in the FileXML application, and how is the XML data being processed and outputted?\",\n",
      "  \"file_name\": \"FileReaderWithXMLParser.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 42 ==================================================\n",
      "{\n",
      "  \"tql\": \"    CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\\n    CREATE OR REPLACE SOURCE @APPNAME@DataSrc USING OracleReader (\\n    Tables: '',\\n    ConnectionURL: '',\\n    Password: '',\\n    Username: ''\\n    )\\n    OUTPUT TO @APPNAME@DataStream;\\n    CREATE OR REPLACE TARGET @APPNAME@DataTrgt USING MongoDBWriter (\\n    ConnectionURL: '',\\n    Username: '',\\n    Password: '',\\n    collections: ''\\n    AuthDB: '',\\n    batchpolicy: 'EventCount:1000, Interval:30',\\n    )\\n    INPUT\\nFROM @APPNAME@DataStream;\\n    CREATE OR REPLACE SOURCE @APPNAME@_src USING MongoDBReader (\\n    ConnectionURL: '',\\n    Username: '',\\n    password: '',\\n    authDB: '',\\n    collections: '',\\n    mode: 'Incremental'\\n    )\\n    OUTPUT TO @APPNAME@stream;\\n    CREATE CQ @APPNAME@CQ\\n    INSERT INTO @APPNAME@CQSTREAM\\nSELECT data.get(\\\"NUM_COL\\\").toString() AS NUM_COL,\\n    data.get(\\\"CHAR_COL\\\").toString() AS CHAR_COL,\\n    data.get(\\\"VARCHAR2_COL\\\").toString() AS VARCHAR2_COL,\\n    data.get(\\\"FLOAT_COL\\\").toString() AS FLOAT_COL,\\n    data.get(\\\"BINARY_FLOAT_COL\\\").toString() AS BINARY_FLOAT_COL,\\n    data.get(\\\"BINARY_DOUBLE_COL\\\").toString() AS BINARY_DOUBLE_COL,\\n    data.get(\\\"DATE_COL\\\").toString() AS DATE_COL,\\n    data.get(\\\"TIMESTAMP_COL\\\").toString() AS TIMESTAMP_COL\\nFROM @APPNAME@stream;\\n    CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (\\n    bucketname: '',\\n    uploadpolicy: '',\\n    UploadConfiguration: '',\\n    objectname: '' )\\n    FORMAT USING AvroFormatter (\\n    schemaFileName: '@SCHEMAFILE@'\\n    )\\n    INPUT\\nFROM @APPNAME@CQSTREAM;\\n    CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (\\n    bucketname: '',\\n    uploadpolicy: '',\\n    UploadConfiguration: '',\\n    objectname: '' )\\n    FORMAT USING JSONFormatter (\\n    members:'data'\\n    )\\n    INPUT\\nFROM @APPNAME@stream;\\n    CREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (\\n    bucketname: '',\\n    uploadpolicy: '',\\n    UploadConfiguration: '',\\n    objectname: '' )\\n    FORMAT USING DSVFormatter ()\\n    INPUT\\nFROM @APPNAME@CQSTREAM;\\n    END APPLICATION @APPNAME@;\",\n",
      "  \"generated_queries\": \"How can I set up a data integration process that reads from an Oracle database, processes the data, and then writes the results to an S3 bucket in multiple formats including Avro, JSON, and DSV?\",\n",
      "  \"file_name\": \"MongoToS3MutliTargetsAvro.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 43 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop application AzureApp;\\n    undeploy application AzureApp;\\n    drop application AzureApp cascade;\\n    create application AzureApp\\n    RECOVERY 10 second interval;\\n    CREATE SOURCE OracleSource USING OracleReader\\n    (\\n    Username: '@LOGMINER-UNAME@',\\n    Password: '@LOGMINER-PASSWORD@',\\n    ConnectionURL: '@LOGMINER-URL@',\\n    Tables:'@TABLES@',\\n    FetchSize: 1\\n    )\\n    OUTPUT TO CsvStream;\\n    Create Type CSVType (\\n    tablename String,\\n    data java.util.HashMap\\n    );\\n    Create Stream TypedCSVStream of CSVType;\\n    CREATE CQ CsvToPosData\\n    INSERT INTO TypedCSVStream\\nSELECT TO_LOWER(META(s, \\\"TableName\\\").toString()) as tablename,\\n    DATA(s) as data\\nFROM CsvStream s;\\n    create Target BlobT using AzureBlobWriter(\\n    accountname:'@ACCNAME@',\\n    accountaccesskey:'@ACCKEY@',\\n    containername:'@CONT@',\\n    blobname:'@BLOB@',\\n    foldername:'@FOLDER@',\\n    uploadpolicy:'EventCount:5,interval:5s'\\n    )\\n    format using AvroFormatter (\\n    )\\n    input from TypedCSVStream;\\n    end application AzureApp;\\n    deploy application AzureApp in default;\\n    start application AzureApp;\",\n",
      "  \"generated_queries\": \"What are the steps involved in setting up an application to read data from an Oracle source, transform it to a CSV format, and then write it to an Azure Blob storage?\",\n",
      "  \"file_name\": \"OracleAzurewithAvroLoad.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 44 ==================================================\n",
      "{\n",
      "  \"tql\": \"    STOP APPLICATION @appname@routerApp;\\n    UNDEPLOY APPLICATION @appname@routerApp;\\n    DROP APPLICATION @appname@routerApp CASCADE;\\n    CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',\\n    acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\\n    CREATE APPLICATION @appname@routerApp RECOVERY 10 SECOND INTERVAL;\\n    CREATE  SOURCE @appname@OraSource USING OracleReader  (\\n    Username: 'qatest',\\n    Password: 'qatest',\\n    ConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',\\n    Tables: 'QATEST.TGT_T%',\\n    FetchSize:'100'\\n    )\\n    OUTPUT TO @appname@MasterStream1;\\n    -- CREATE STREAM @appname@ss1 OF Global.waevent persist using Global.DefaultKafkaProperties;\\n    -- CREATE STREAM @appname@ss2 OF Global.waevent persist using Global.DefaultKafkaProperties;\\n    -- CREATE STREAM @appname@ss3 OF Global.waevent persist using Global.DefaultKafkaProperties;\\n    CREATE STREAM @appname@ss1 OF Global.waevent PERSIST USING KafkaPropset;\\n    CREATE STREAM @appname@ss2 OF Global.waevent PERSIST USING KafkaPropset;\\n    CREATE STREAM @appname@ss3 OF Global.waevent PERSIST USING KafkaPropset;\\n    CREATE OR REPLACE ROUTER @appname@tablerouter1 INPUT\\nFROM @appname@MasterStream1 s CASE\\n    WHEN meta(s,\\\"TableName\\\").toString()='QATEST.TGT_T1' THEN ROUTE TO @appname@ss1,\\n    WHEN meta(s,\\\"TableName\\\").toString()='QATEST.TGT_T2' THEN ROUTE TO @appname@ss2,\\n    WHEN meta(s,\\\"TableName\\\").toString()='QATEST.TGT_T3' THEN ROUTE TO @appname@ss3,\\n    WHEN meta(s,\\\"TableName\\\").toString()='QATEST.TGT_T4' THEN ROUTE TO @appname@ss4,\\n    WHEN meta(s,\\\"TableName\\\").toString()='QATEST.TGT_T5' THEN ROUTE TO @appname@ss5,\\n    WHEN meta(s,\\\"TableName\\\").toString()='QATEST.TGT_T6' THEN ROUTE TO @appname@ss6,\\n    ELSE ROUTE TO @appname@ss_else;\\n    create Target @appname@FileTarget_1 using FileWriter\\n    (\\n    directory: 'testSep17',\\n    filename:'%@metadata(TableName)%'\\n    )\\n    FORMAT USING dsvFormatter ()\\n    input from @appname@ss1;\\n    create Target @appname@FileTarget_2 using FileWriter\\n    (\\n    directory: 'testSep17',\\n    filename:'%@metadata(TableName)%'\\n    )\\n    FORMAT USING dsvFormatter ()\\n    input from @appname@ss2;\\n    create Target @appname@FileTarget_3 using FileWriter\\n    (\\n    directory: 'testSep17',\\n    filename:'%@metadata(TableName)%'\\n    )\\n    FORMAT USING dsvFormatter ()\\n    input from @appname@ss3;\\n    CREATE OR REPLACE TARGET @appname@KafkaTarget_4 USING KafkaWriter VERSION '0.11.0' (\\n    brokerAddress: 'localhost:9092',\\n    Topic: 'target4'\\n    )\\n    FORMAT USING JSONFormatter  (\\n    )\\n    INPUT\\nFROM @appname@ss4;\\n    CREATE OR REPLACE TARGET @appname@KafkaTarget_5 USING KafkaWriter VERSION '0.11.0' (\\n    brokerAddress: 'localhost:9092',\\n    Topic: 'target5'\\n    )\\n    FORMAT USING JSONFormatter  (\\n    )\\n    INPUT\\nFROM @appname@ss5;\\n    CREATE OR REPLACE TARGET @appname@KafkaTarget_6 USING KafkaWriter VERSION '0.11.0' (\\n    brokerAddress: 'localhost:9092',\\n    Topic: 'target6'\\n    )\\n    FORMAT USING JSONFormatter  (\\n    )\\n    INPUT\\nFROM @appname@ss6;\\n    end application @appname@routerApp;\\n    deploy application @appname@routerApp;\\n    start @appname@routerApp;\",\n",
      "  \"generated_queries\": \"What steps were taken to set up the router application, including data sources, streaming configurations, and output targets?\",\n",
      "  \"file_name\": \"routerToKafka.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 45 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop @APPNAME@;\\n    undeploy application @APPNAME@;\\n    drop application @APPNAME@ cascade;\\n    CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;\\n    Create Source @SourceName@ Using Ojet\\n    (\\n    Username:'c##qatest',\\n    Password:'qatest',\\n    ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',\\n    Tables:'CDB$ROOT.\\\"C##QATEST\\\".ojet_src;ORCLPDB.QATEST.ojet_src',\\n    _h_useClassic:false,\\n    Fetchsize:1,\\n    Compression: true,\\n    SupportPDB:true,\\n    ReplicationSlotName:'null'\\n    )\\n    Output To @SRCINPUTSTREAM@;\\n    CREATE TARGET @targetName@ USING DatabaseWriter\\n    (\\n    ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',\\n    Username:'c##qatest',\\n    Password:'qatest',\\n    BatchPolicy:'EventCount:1,Interval:1',\\n    Tables:'CDB$ROOT.\\\"C##QATEST\\\".ojet_src,CDB$ROOT.\\\"C##QATEST\\\".ojet_tgt'\\n    ) INPUT\\nFROM @SRCINPUTSTREAM@;\\n    create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\\n    END APPLICATION @APPNAME@;\\n    deploy application @APPNAME@ in default;\\n    start @APPNAME@;\",\n",
      "  \"generated_queries\": \"How can I set up an application in Oracle that deploys a data replication process from a source database to a target database using Oracle GoldenGate?\",\n",
      "  \"file_name\": \"Ojet_DBWPDBCDB1.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 46 ==================================================\n",
      "{\n",
      "  \"tql\": \"    CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\\n    CREATE OR REPLACE SOURCE @APPNAME@_src USING DataBaseReader (\\n    Tables: '',\\n    ConnectionURL: '',\\n    Password: '',\\n    Username: ''\\n    )\\n    OUTPUT TO @APPNAME@stream;\\n    CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (\\n    bucketname: '',\\n    uploadpolicy: '',\\n    UploadConfiguration: '',\\n    objectname: '' )\\n    FORMAT USING JSONFormatter (\\n    members:'data'\\n    )\\n    INPUT\\nFROM @APPNAME@stream;\\n    CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (\\n    bucketname: '',\\n    uploadpolicy: '',\\n    UploadConfiguration: '',\\n    objectname: '' )\\n    FORMAT USING JSONFormatter (\\n    members:'data'\\n    )\\n    INPUT\\nFROM @APPNAME@stream;\\n    END APPLICATION @APPNAME@;\",\n",
      "  \"generated_queries\": \"How can I set up an application that reads data from a database, processes it, and writes the output in JSON format to two different S3 buckets?\",\n",
      "  \"file_name\": \"OracleILToS3_2Targets.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 47 ==================================================\n",
      "{\n",
      "  \"tql\": \"    create application KinesisTest;\\n    create source CSVSource using FileReader (\\n    directory:'/home/dz/src/product/Samples/AppData',\\n    WildCard:'posdata.csv',\\n    positionByEOF:false,\\n    charset:'UTF-8'\\n    )\\n    parse using DSVParser (\\n    header:'yes'\\n    )\\n    OUTPUT TO CsvStream;\\n    Create Type CSVType (\\n    companyName String,\\n    merchantId String,\\n    dateTime DateTime,\\n    hourValue int,\\n    amount double,\\n    zip String\\n    );\\n    Create Stream TypedCSVStream of CSVType;\\n    CREATE CQ CsvToPosData\\n    INSERT INTO TypedCSVStream\\nSELECT data[0], data[1],\\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\\n    DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\\n    TO_DOUBLE(data[7]),\\n    data[9]\\nFROM CsvStream;\\n    create or replace Target t using KinesisWriter (\\n    regionName:'TARGET_REGION',\\n    streamName:'TARGET_STREAM'\\n    )\\n    format using DSVFormatter (\\n    )\\n    input from TypedCSVStream;\\n    end application KinesisTest;\\n    deploy application KinesisTest in default;\\n    start application KinesisTest;\",\n",
      "  \"generated_queries\": \"What data is being processed and how is it transformed before sending to the Kinesis stream in the KinesisTest application?\",\n",
      "  \"file_name\": \"KinesisTest.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 48 ==================================================\n",
      "{\n",
      "  \"tql\": \"    stop application @APPNAME@;\\n    undeploy application @APPNAME@;\\n    drop application @APPNAME@ cascade;\\n    CREATE application @APPNAME@ @Recovery@ AUTORESUME MAXRETRIES 2 RETRYINTERVAL 10;\\n    create type @APPNAME@type1(\\n    companyName String,\\n    merchantId String,\\n    city string\\n    );\\n    create type @APPNAME@type2(\\n    c1 integer,\\n    c2 String,\\n    c3 string\\n    );\\n    create type @APPNAME@type3(\\n    c1 integer\\n    );\\n    create type @APPNAME@type4(\\n    c1 integer,\\n    c2 integer\\n    );\\n    create stream @APPNAME@in_memory_typedStream of @APPNAME@type1 partition by city;\\n    create stream @APPNAME@in_memory_typedStream_num of @APPNAME@type2;\\n    create stream @APPNAME@in_memory_typedStream_num1 of @APPNAME@type2;\\n    create stream @APPNAME@in_memory_typedStream_num2 of @APPNAME@type2;\\n    create stream @APPNAME@in_memory_typedStream_num3 of @APPNAME@type2;\\n    create stream @APPNAME@in_memory_typedStream_num4 of @APPNAME@type2;\\n    create stream @APPNAME@in_memory_typedStream_num5 of @APPNAME@type2;\\n    create stream @APPNAME@finalstream6 of @APPNAME@type4;\\n    create source @APPNAME@s using FileReader (\\n    directory:'Product/IntegrationTests/TestData/',\\n    wildcard:'posdata5L.csv',\\n    positionByEOF:false,\\n    Charset:'UTF-8'\\n    )\\n    PARSE USING DSVParser (\\n    columndelimiter:',',\\n    ignoreemptycolumn:'Yes',\\n    quoteset:'[]~\\\"',\\n    header: true,\\n    separator:'~'\\n    )\\n    OUTPUT TO @APPNAME@in_memory_rawStream;\\n    create CQ @APPNAME@cq1\\n    INSERT INTO @APPNAME@kps_waevent\\nSELECT *\\nFROM @APPNAME@in_memory_rawStream  ;\\n    create CQ @APPNAME@cq2\\n    INSERT INTO @APPNAME@in_memory_typedStream\\nSELECT TO_STRING(data[0]).replaceAll(\\\"COMPANY \\\", \\\"\\\"),\\n    TO_STRING(data[1]),\\n    TO_STRING(data[10])\\nFROM @APPNAME@kps_waevent ;\\n    create CQ @APPNAME@cq3\\n    INSERT INTO @APPNAME@in_memory_typedStream_num1\\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\\nFROM @APPNAME@in_memory_typedStream;\\n    -- order by c3;\\n    create CQ @APPNAME@cq4\\n    INSERT INTO @APPNAME@in_memory_typedStream_num2\\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\\nFROM @APPNAME@in_memory_typedStream;\\n    create CQ @APPNAME@cq5\\n    INSERT INTO @APPNAME@in_memory_typedStream_num3\\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\\nFROM @APPNAME@in_memory_typedStream;\\n    create CQ @APPNAME@cq6\\n    INSERT INTO @APPNAME@in_memory_typedStream_num4\\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\\nFROM @APPNAME@in_memory_typedStream;\\n    create CQ @APPNAME@cq7\\n    INSERT INTO @APPNAME@in_memory_typedStream_num5\\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\\nFROM @APPNAME@in_memory_typedStream;\\n    CREATE CQ @APPNAME@cq8\\n    INSERT INTO @APPNAME@in_memory_typedStream_num6\\nSELECT TO_INT(companyName) as c1\\nFROM @APPNAME@in_memory_typedStream;\\n    CREATE JUMPING WINDOW @APPNAME@DataStream1_100000Rows\\n    OVER @APPNAME@in_memory_typedStream_num1 KEEP 100000 ROWS;\\n    CREATE JUMPING WINDOW @APPNAME@DataStream2_100000Rows\\n    OVER @APPNAME@in_memory_typedStream_num2 KEEP 100000 ROWS;\\n    CREATE JUMPING WINDOW @APPNAME@DataStream3_100000Rows\\n    OVER @APPNAME@in_memory_typedStream_num3 KEEP 100000 ROWS;\\n    CREATE JUMPING WINDOW @APPNAME@DataStream4_100000Rows\\n    OVER @APPNAME@in_memory_typedStream_num4 KEEP 100000 ROWS;\\n    CREATE JUMPING WINDOW @APPNAME@DataStream5_100000Rows\\n    OVER @APPNAME@in_memory_typedStream_num5 KEEP 100000 ROWS;\\n    CREATE JUMPING WINDOW @APPNAME@DataStream6_100000Rows\\n    OVER @APPNAME@in_memory_typedStream_num6 KEEP 100000 ROWS;\\n    create CQ @APPNAME@cq9\\n    INSERT INTO @APPNAME@finalstream1\\nSELECT c1\\nFROM @APPNAME@DataStream1_100000Rows sample by c1;\\n    create CQ @APPNAME@cq10\\n    INSERT INTO @APPNAME@finalstream2\\nSELECT c1\\nFROM @APPNAME@DataStream2_100000Rows sample by c1 selectivity 0.1;\\n    create CQ @APPNAME@cq11\\n    INSERT INTO @APPNAME@finalstream3\\nSELECT c1\\nFROM @APPNAME@DataStream3_100000Rows sample by c1 selectivity 0.25;\\n    create CQ @APPNAME@cq12\\n    INSERT INTO @APPNAME@finalstream4\\nSELECT c1\\nFROM @APPNAME@DataStream4_100000Rows sample by c1 selectivity 0.05;\\n    --\\nSELECT count(*)\\nFROM @APPNAME@DataStream4Rows10000Seconds sample by c1 selectivity 0.05;\\n    create CQ @APPNAME@cq13\\n    INSERT INTO @APPNAME@finalstream5\\nSELECT c1\\nFROM @APPNAME@DataStream5_100000Rows sample by c1 selectivity 0.01;\\n    create CQ @APPNAME@cq14\\n    INSERT INTO @APPNAME@finalstream6\\nSELECT c1,c1 as c2\\nFROM @APPNAME@DataStream6_100000Rows sample by c1,c2 selectivity 0.01;\\n    create target @APPNAME@target1 using filewriter (\\n    filename:'FEATURE-DIR/logs/@APPNAME@target1.log',\\n    flushpolicy:'eventcount:1',\\n    rolloverpolicy:'eventcount:5000000,sequence:00'\\n    )\\n    format using dsvFormatter()\\n    input from @APPNAME@finalstream1;\\n    create target @APPNAME@target2 using filewriter (\\n    filename:'FEATURE-DIR/logs/@APPNAME@target2.log',\\n    flushpolicy:'eventcount:1',\\n    rolloverpolicy:'eventcount:5000000,sequence:00'\\n    )\\n    format using dsvFormatter()\\n    input from @APPNAME@finalstream2;\\n    create target @APPNAME@target3 using filewriter (\\n    filename:'FEATURE-DIR/logs/@APPNAME@target3.log',\\n    flushpolicy:'eventcount:1',\\n    rolloverpolicy:'eventcount:5000000,sequence:00'\\n    )\\n    format using dsvFormatter()\\n    input from @APPNAME@finalstream3;\\n    create target @APPNAME@target4 using filewriter (\\n    filename:'FEATURE-DIR/logs/@APPNAME@target4.log',\\n    flushpolicy:'eventcount:1',\\n    rolloverpolicy:'eventcount:5000000,sequence:00'\\n    )\\n    format using dsvFormatter()\\n    input from @APPNAME@finalstream4;\\n    create target @APPNAME@target5 using filewriter (\\n    filename:'FEATURE-DIR/logs/@APPNAME@target5.log',\\n    flushpolicy:'eventcount:1',\\n    rolloverpolicy:'eventcount:5000000,sequence:00'\\n    )\\n    format using dsvFormatter()\\n    input from @APPNAME@finalstream5;\\n    CREATE WACTIONSTORE @APPNAME@Wactions1 CONTEXT OF @APPNAME@type3\\n    EVENT TYPES ( @APPNAME@type2 )\\n    USING ( storageProvider:'elasticsearch' );\\n    CREATE WACTIONSTORE @APPNAME@Wactions2 CONTEXT OF @APPNAME@type3\\n    EVENT TYPES ( @APPNAME@type2 )\\n    USING ( storageProvider:'elasticsearch' );\\n    CREATE WACTIONSTORE @APPNAME@Wactions3 CONTEXT OF @APPNAME@type3\\n    EVENT TYPES ( @APPNAME@type2 )\\n    USING ( storageProvider:'elasticsearch' );\\n    CREATE WACTIONSTORE @APPNAME@Wactions4 CONTEXT OF @APPNAME@type4\\n    EVENT TYPES ( @APPNAME@type4 )\\n    USING ( storageProvider:'elasticsearch' );\\n    CREATE WACTIONSTORE @APPNAME@Wactions5 CONTEXT OF @APPNAME@type4\\n    EVENT TYPES ( @APPNAME@type4 )\\n    USING ( storageProvider:'elasticsearch' );\\n    --sampling twice: one in finalstream1 and another in select query.\\n    CREATE CQ @APPNAME@cq15\\n    INSERT INTO @APPNAME@Wactions1\\nSELECT FIRST(p.c1)\\nFROM @APPNAME@finalstream1 p\\n    GROUP BY p.c1 sample by p.c1 ;\\n    --sampling once: results will be same as target2 and target1.\\n    CREATE CQ @APPNAME@cq16\\n    INSERT INTO @APPNAME@Wactions2\\nSELECT * from @APPNAME@finalstream1 order by c1 desc limit 10000 ;\\n    --sampling twice: one in finalstream1 and another in select query.\\n    CREATE CQ @APPNAME@cq17\\n    INSERT INTO @APPNAME@Wactions3\\nSELECT * from @APPNAME@finalstream1 order by c1 sample by c1;\\n    --sampling using 2 fields, 2800 for single field and 332 for 2 field\\n    CREATE CQ @APPNAME@cq18\\n    INSERT INTO @APPNAME@Wactions4\\nSELECT c1,c1 from @APPNAME@finalstream6 order by c1 sample by c1;\\n    --same as Wactions4 - here selectivity alone varies, so output is 8\\n    CREATE CQ @APPNAME@cq19\\n    INSERT INTO @APPNAME@Wactions5\\nSELECT c1,c1 from @APPNAME@finalstream6 order by c1 sample by c1 selectivity 0.0001;\\n    end application @APPNAME@;\\n    deploy application @APPNAME@;\\n    --start @APPNAME@;\",\n",
      "  \"generated_queries\": \"What steps do I need to follow to undeploy and recreate the application with new data streams and types in the specified target directory?\",\n",
      "  \"file_name\": \"SampleByClause.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 49 ==================================================\n",
      "{\n",
      "  \"tql\": \"    --\\n    -- Recovery Test 22 with two sources, two sliding attribute windows, and one wactionstore -- all with no partitioning\\n    -- Nicholas Keene WebAction, Inc.\\n    --\\n    -- S1 -> Sa5W -> CQ1 -> WS\\n    -- S2 -> Sa6W -> CQ2 -> WS\\n    --\\n    STOP KStreamRecov22Tester.KStreamRecovTest22;\\n    UNDEPLOY APPLICATION KStreamRecov22Tester.KStreamRecovTest22;\\n    DROP APPLICATION KStreamRecov22Tester.KStreamRecovTest22 CASCADE;\\n    DROP USER KStreamRecov22Tester;\\n    DROP NAMESPACE KStreamRecov22Tester CASCADE;\\n    CREATE USER KStreamRecov22Tester IDENTIFIED BY KStreamRecov22Tester;\\n    GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov22Tester;\\n    CONNECT KStreamRecov22Tester KStreamRecov22Tester;\\n    CREATE APPLICATION KStreamRecovTest22 RECOVERY 5 SECOND INTERVAL;\\n    CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\\n    CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\\n    CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\\n    CREATE SOURCE CsvSource1 USING CSVReader (\\n    directory:'@TEST-DATA-PATH@',\\n    header:Yes,\\n    wildcard:'RecovTestDataLong.csv',\\n    columndelimiter:',',\\n    blocksize: 10240,\\n    positionByEOF:false,\\n    trimquote:false\\n    ) OUTPUT TO KafkaCsvStream1;\\n    CREATE SOURCE CsvSource2 USING CSVReader (\\n    directory:'@TEST-DATA-PATH@',\\n    header:Yes,\\n    wildcard:'RecovTestDataLong.csv',\\n    columndelimiter:',',\\n    blocksize: 10240,\\n    positionByEOF:false,\\n    trimquote:false\\n    ) OUTPUT TO KafkaCsvStream2;\\n    CREATE TYPE CsvData (\\n    merchantId String KEY,\\n    companyName String,\\n    dateTime DateTime,\\n    amount double\\n    );\\n    CREATE STREAM DataStream1 OF CsvData;\\n    CREATE STREAM DataStream2 OF CsvData;\\n    CREATE CQ CsvToData1\\n    INSERT INTO DataStream1\\nSELECT\\n    data[1],\\n    data[0],\\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\\n    TO_DOUBLE(data[7])\\nFROM KafkaCsvStream1;\\n    CREATE CQ CsvToData2\\n    INSERT INTO DataStream2\\nSELECT\\n    data[1],\\n    data[0],\\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\\n    TO_DOUBLE(data[7])\\nFROM KafkaCsvStream2;\\n    CREATE WINDOW DataStream5Minutes1\\n    OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;\\n    CREATE WINDOW DataStream5Minutes2\\n    OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;\\n    CREATE WACTIONSTORE Wactions CONTEXT OF CsvData\\n    EVENT TYPES ( CsvData )\\n    @PERSIST-TYPE@\\n    CREATE CQ DataToWaction1\\n    INSERT INTO Wactions\\nSELECT\\n    *\\nFROM DataStream5Minutes1;\\n    CREATE CQ DataToWaction2\\n    INSERT INTO Wactions\\nSELECT\\n    *\\nFROM DataStream5Minutes2;\\n    END APPLICATION KStreamRecovTest22;\",\n",
      "  \"generated_queries\": \"What are the steps taken to create a recovery application that processes CSV data from two sources and stores results in a wactionstore, with specific configurations for windowing and user permissions?\",\n",
      "  \"file_name\": \"KStreamRecovTest22.tql\"\n",
      "}\n",
      "\n",
      "================================================== Entry 50 ==================================================\n",
      "{\n",
      "  \"tql\": \"    --\\n    -- Recovery Test 36 with two sources, two jumping attribute windows, and one wactionstore -- all partitioned on the same key\\n    -- Nicholas Keene WebAction, Inc.\\n    --\\n    -- S1 -> Ja5W/p -> CQ1 -> WS\\n    -- S2 -> Ja6W/p -> CQ2 -> WS\\n    --\\n    STOP Recov36Tester.RecovTest36;\\n    UNDEPLOY APPLICATION Recov36Tester.RecovTest36;\\n    DROP APPLICATION Recov36Tester.RecovTest36 CASCADE;\\n    DROP USER KStreamRecov36Tester;\\n    DROP NAMESPACE KStreamRecov36Tester CASCADE;\\n    CREATE USER KStreamRecov36Tester IDENTIFIED BY KStreamRecov36Tester;\\n    GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov36Tester;\\n    CONNECT KStreamRecov36Tester KStreamRecov36Tester;\\n    CREATE APPLICATION KStreamRecovTest36 RECOVERY 5 SECOND INTERVAL;\\n    CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\\n    CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\\n    CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\\n    CREATE SOURCE CsvSource1 USING CSVReader (\\n    directory:'@TEST-DATA-PATH@',\\n    header:Yes,\\n    wildcard:'RecovTestDataLong.csv',\\n    columndelimiter:',',\\n    blocksize: 10240,\\n    positionByEOF:false,\\n    trimquote:false\\n    ) OUTPUT TO KafkaCsvStream1;\\n    CREATE SOURCE CsvSource2 USING CSVReader (\\n    directory:'@TEST-DATA-PATH@',\\n    header:Yes,\\n    wildcard:'RecovTestDataLong.csv',\\n    columndelimiter:',',\\n    blocksize: 10240,\\n    positionByEOF:false,\\n    trimquote:false\\n    ) OUTPUT TO KafkaCsvStream2;\\n    CREATE TYPE CsvData (\\n    merchantId String KEY,\\n    companyName String,\\n    dateTime DateTime,\\n    amount double\\n    );\\n    CREATE TYPE WactionData (\\n    firstCompanyName String KEY,\\n    dateTime DateTime,\\n    totalCompanies int,\\n    firstMerchantId String\\n    );\\n    CREATE STREAM DataStream1 OF CsvData\\n    PARTITION BY merchantId;\\n    CREATE STREAM DataStream2 OF CsvData\\n    PARTITION BY merchantId;\\n    CREATE CQ CsvToData1\\n    INSERT INTO DataStream1\\nSELECT\\n    data[1],\\n    data[0],\\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\\n    TO_DOUBLE(data[7])\\nFROM KafkaCsvStream1;\\n    CREATE CQ CsvToData2\\n    INSERT INTO DataStream2\\nSELECT\\n    data[1],\\n    data[0],\\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\\n    TO_DOUBLE(data[7])\\nFROM KafkaCsvStream2;\\n    CREATE JUMPING WINDOW DataStream5Minutes\\n    OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime\\n    PARTITION BY merchantId;\\n    CREATE JUMPING WINDOW DataStream6Minutes\\n    OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime\\n    PARTITION BY merchantId;\\n    CREATE WACTIONSTORE Wactions CONTEXT OF WactionData\\n    EVENT TYPES ( CsvData )\\n    @PERSIST-TYPE@\\n    CREATE CQ Data5ToWaction\\n    INSERT INTO Wactions\\nSELECT\\n    FIRST(p.companyName),\\n    FIRST(p.dateTime),\\n    COUNT(p.amount),\\n    FIRST(p.merchantId)\\nFROM DataStream5Minutes p\\n    GROUP BY p.merchantId;\\n    CREATE CQ Data6ToWaction\\n    INSERT INTO Wactions\\nSELECT\\n    FIRST(p.companyName),\\n    FIRST(p.dateTime),\\n    COUNT(p.amount),\\n    FIRST(p.merchantId)\\nFROM DataStream6Minutes p\\n    GROUP BY p.merchantId;\\n    END APPLICATION KStreamRecovTest36;\",\n",
      "  \"generated_queries\": \"What is the process for creating a recovery test application that processes CSV data from two different sources and performs aggregations using jumping windows on a specified key?\",\n",
      "  \"file_name\": \"KStreamRecovTest36.tql\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def format_tql(tql_string):\n",
    "    \"\"\"\n",
    "    Format TQL string with proper indentation and line breaks.\n",
    "    \"\"\"\n",
    "    # Common TQL keywords that should start on new lines\n",
    "    keywords = ['SELECT', 'FROM', 'WHERE', 'GROUP BY', 'ORDER BY', 'HAVING', 'JOIN', 'LEFT JOIN', 'RIGHT JOIN', 'INNER JOIN']\n",
    "    \n",
    "    # Initial formatting\n",
    "    formatted = tql_string.strip()\n",
    "    \n",
    "    # Add newlines before keywords\n",
    "    for keyword in keywords:\n",
    "        formatted = formatted.replace(keyword, f'\\n{keyword}')\n",
    "    \n",
    "    # Split into lines and indent\n",
    "    lines = formatted.split('\\n')\n",
    "    indented_lines = []\n",
    "    base_indent = '    '  # 4 spaces\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            # Determine indentation level\n",
    "            if any(line.startswith(keyword) for keyword in ['SELECT', 'FROM']):\n",
    "                indented_lines.append(line)  # No indent for main clauses\n",
    "            else:\n",
    "                indented_lines.append(base_indent + line)  # Indent other clauses\n",
    "    \n",
    "    return '\\n'.join(indented_lines)\n",
    "\n",
    "def print_jsonl(file_path, max_entries=None):\n",
    "    \"\"\"\n",
    "    Prints the contents of a JSONL file in a readable format with special handling for TQL.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSONL file\n",
    "        max_entries (int, optional): Maximum number of entries to print. Defaults to None (all entries).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            count = 0\n",
    "            for line in file:\n",
    "                if max_entries and count >= max_entries:\n",
    "                    print(\"\\n... (more entries exist)\")\n",
    "                    break\n",
    "                    \n",
    "                data = json.loads(line)\n",
    "                print(\"\\n\" + \"=\"*50 + f\" Entry {count+1} \" + \"=\"*50)\n",
    "                \n",
    "                # Create a formatted copy of the data\n",
    "                formatted_data = data.copy()\n",
    "                \n",
    "                # Format TQL fields if they exist\n",
    "                if 'tql' in formatted_data:\n",
    "                    formatted_data['tql'] = format_tql(formatted_data['tql'])\n",
    "                if 'original_tql' in formatted_data:\n",
    "                    formatted_data['original_tql'] = format_tql(formatted_data['original_tql'])\n",
    "                if 'generated_tql' in formatted_data:\n",
    "                    formatted_data['generated_tql'] = format_tql(formatted_data['generated_tql'])\n",
    "                \n",
    "                print(json.dumps(formatted_data, indent=2, ensure_ascii=False))\n",
    "                count += 1\n",
    "                \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Invalid JSON in line: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "print_jsonl('query_tql_pairs.jsonl', max_entries=3)  # Print first 3 entries\n",
    "print_jsonl('tql_with_queries.jsonl')  # Print all entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a3fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
