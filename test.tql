STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'TargetPosDataXmlTI',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:merchantid:text=merchantname'
)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlTI_actual.log') input from TypedCSVStream;

end application DSV;

drop user dtest;
drop namespace dtest cascade;

create user dtest identified by test;
grant Global.appuser to user dtest;
connect dtest test;
create cq permissioncq select * from banker.oneWS where ( bankID = :bI);

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
filename:'',
directory:'@FEATURE-DIR@/logs/',
    sequence:'00',
rolloverpolicy:'eventcount:200,sequence:00'
)
format using DSVFormatter (

)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetEventCount_actual.log') input from TypedCSVStream;

end application DSV;

use PosTester;
alter application PosApp;

CREATE CACHE HourlyAveLookup using CSVReader (
  directory: 'Samples/Customer/PosApp/appData',
  wildcard: 'hourlyData.txt',
  header: Yes,
  columndelimiter: ',',
  trimquote:false
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

end application PosApp;

alter application PosApp recompile;

--
-- Recovery Test 2
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS
--

STOP Recov2Tester.RecovTest2;
UNDEPLOY APPLICATION Recov2Tester.RecovTest2;
DROP APPLICATION Recov2Tester.RecovTest2 CASCADE;
CREATE APPLICATION RecovTest2 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION RecovTest2;

Stop application @externalCacheName@;
undeploy application @externalCacheName@;
drop application @externalCacheName@ cascade;
drop EXCEPTIONSTORE @externalCacheName@_Exceptionstore;

drop application @ApplicationName@ cascade;

CREATE APPLICATION @ApplicationName@ RECOVERY 5 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;

CREATE OR REPLACE SOURCE @SourceName@ USING Global.DatabaseReader
(
  Username: '@Username@',
  Password: '@Password@',
  ConnectionURL: '@ConnectionURL@',
  Tables: '@SourceTableName@',
  FetchSize: 1
)
Output To @SourceName@_st;

CREATE TYPE @TYPEName@_type (
 id Integer,
 email STRING,
 address STRING
 );

CREATE EXTERNAL CACHE @externalCacheName@ (
  keytomap: 'id',
  DatabaseProviderType: 'Default',
  Table: '@LookUpTableName@',
  AdapterName: 'DatabaseReader',
  ConnectionURL: '@ConnectionURL@',
  FetchSize: 1,
  Password_encrypted: 'false',
  Columns: 'id,email,address',
  Password: '@Password@',
  connectionRetryPolicy: 'timeOut=5, retryInterval=5, maxRetries=5',
  Username: '@Username@' )
OF @TYPEName@_type;


CREATE CQ @CQName@
INSERT INTO @TargetName@_st
SELECT t1.data[0] as ID,t1.data[1] as Name,t1.data[2] as Deptarment,t2.email as Email,t2.address as address
FROM @SourceName@_st t1 left outer join @externalCacheName@ t2
on TO_INT(t1.data[0]) = t2.id;


CREATE OR REPLACE TARGET @TargetName@ USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  BatchPolicy: 'EventCount:1,Interval:10',
  CheckPointTable: 'CHKPOINT',
  ConnectionURL: '@ConnectionURL@',
  Password_encrypted: 'false',
  CDDLAction: 'Process',
  Password: '@Password@',
  CommitPolicy: 'EventCount:1,Interval:30',
  StatementCacheSize: '50',
  DatabaseProviderType: 'Default',
  PreserveSourceTransactionBoundary: 'false',
  Tables: '@TargetTableName@',
  Username: '@Username@',
  adapterName: 'DatabaseWriter' )
INPUT FROM @TargetName@_st;

END APPLICATION @ApplicationName@;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: '@ORACLE-URL@',
  Tables: '@SOURCE-TABLES@',
  Username: '@ORACLE-USERNAME@',
  Password: '@ORACLE-PASSWORD@'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt USING SnowflakeWriter

(
  ConnectionURL:'@SNOWFLAKE-URL@',
  username:'@SNOWFLAKE-USERNAME@',
  appendOnly:'false',
  Tables:'@TARGET-TABLES@',
  uploadpolicy:'eventcount:3,interval:10s',
  externalStageType:'local'
)
INPUT FROM @APP_NAME@_Stream;


END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;
Create Source @APPNAME@_s Using DatabaseReader
(
 Username:'@UNAME@',
 Password:'@PASSWORD@',
 ConnectionURL:'@URL@',
 --Query: "SELECT * FROM QATEST.AUTHORIZATIONS",
 Tables:'QATEST.HDFS_IL_%',
 FetchSize:1,
 QuiesceOnILCompletion: true
)
Output To @APPNAME@_ss;


create Target @APPNAME@_t using HDFSWriter(
	hadoopurl:'hdfs://localhost:9000/',
	directory: '%@METADATA(TableName)%',
	filename:'%@METADATA(TableName)%',
    rolloverpolicy: 'filesize:500M',
    flushpolicy:'@FLUSHPOLICY@',
    hadoopConfigurationPath:'@CONF@'
	)
format using DSVFormatter (
)input from @APPNAME@_ss;

--create Target t2 using SysOut(name:Foo2) input from DataStream;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;

Create Source @SourceName@ Using OracleReader

(
  Username:'c##qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',
  Tables:'CDB$ROOT."C##QATEST".ojet_src;ORCLPDB.QATEST.ojet_src',
  _h_useClassic:false,
  Fetchsize:1,
  Compression: true,
  SupportPDB:true,
  ReplicationSlotName:'null'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',
  Username:'c##qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'CDB$ROOT."C##QATEST".ojet_src,CDB$ROOT."C##QATEST".ojet_tgt'
) INPUT FROM @SRCINPUTSTREAM@;

CREATE TARGET @targetName1@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orclpdb',
  Username:'qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'ORCLPDB.QATEST.ojet_src,ORCLPDB.QATEST.ojet_tgt'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

CREATE OR REPLACE PROPERTYVARIABLE Mode='sync';
CREATE OR REPLACE PROPERTYVARIABLE BatchPolicy='Size:900000,Interval:1';
create application KinesisTest;
create source CSVSource using FileReader (
	directory:'/home/dz/src/product/Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  companyName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0], data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	BatchPolicy: '$BatchPolicy',
    Mode: '$Mode'	
)
format using AvroFormatter (
	schemaFileName:'/Users/shikhar_nahar/Product/testwaevent.avsc'
)
input from TypedCSVStream;
end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 1 second interval;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@'
  --recordSelector: '@PROP8@'
)
OUTPUT TO @APPNAME@Stream;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;

CREATE TYPE test_type (
 account_no com.fasterxml.jackson.databind.JsonNode,
 first_name com.fasterxml.jackson.databind.JsonNode,
 last_name com.fasterxml.jackson.databind.JsonNode,
 addr1 com.fasterxml.jackson.databind.JsonNode,
Addr2 com.fasterxml.jackson.databind.JsonNode,
City com.fasterxml.jackson.databind.JsonNode,
State com.fasterxml.jackson.databind.JsonNode,
Zip com.fasterxml.jackson.databind.JsonNode
);

Create stream cqAsJSONNodeStream of test_type;

CREATE CQ GetPOAsJsonNodes
INSERT into cqAsJSONNodeStream
    select 
    data.get('ACCTS-RECORD').get('ACCOUNT-NO'),
data.get('ACCTS-RECORD').get('NAME').get('FIRST-NAME'),
data.get('ACCTS-RECORD').get('NAME').get('LAST-NAME'),
data.get('ACCTS-RECORD').get('ADDRESS1'),
data.get('ACCTS-RECORD').get('ADDRESS2'),
data.get('ACCTS-RECORD').get('ADDRESS3').get('CITY'),
data.get('ACCTS-RECORD').get('ADDRESS3').get('STATE'),
data.get('ACCTS-RECORD').get('ADDRESS3').get('ZIP-CODE')
from @APPNAME@Stream js;

create type finaldtype(
      ACCOUNT_NO String, 
      FIRST_NAME String,
      LAST_NAME String,
      ADDRESS1 String,
      ADDRESS2 String,
      CITY String,
      STATE String,
      ZIP_CODE String
);

CREATE CQ getdata
INSERT into getdataStream
    select account_no.toString(),
    first_name.toString(),
    last_name.toString(),
    addr1.toString(),
    Addr2.toString(),
    City.toString(),
    State.toString(),
    Zip.toString()
from cqAsJSONNodeStream x;

create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:10000',
  CommitPolicy: 'EventCount:10000',
  Tables: 'QATEST.@APPNAME@'
)
input from getdataStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

STOP liveStream;
UNDEPLOY APPLICATION liveStream;
DROP APPLICATION liveStream CASCADE;

CREATE APPLICATION liveStream;

CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity int,
  size int);

--SOURCE DESCRIPTION
---------------------------------------------
--OutputType: name of the corresponding TYPE
--noLimit: produce infinite data
--maxRows: (if noLimit = false) produces specified rows
--iterations (optional): 1 iteration = populating Type attributes once.
--iterationDelay (ms)(optional): delay between iterations (0 if none)

-- ** either maxRows or iterations must be 0 **

--StringSet: columns of type String. column values within brackets (seperate values by dash), seperate columns by comma
--NumberSet: columns of type Int, Double, Long. supply a range between brackets, followed by G (Gaussian) or R (Random) distribution.
---------------------------------------------

CREATE SOURCE liveSource using StreamReader(
  OutputType: 'eventLister.Atm',
  noLimit: 'false',
  maxRows: 20,
  iterations: 0,
  iterationDelay: 100,
  StringSet: 'productID[001-002-003-004],stateID[AS-CA-WA-NY]',
  NumberSet: 'productWeight[3-3]R,quantity[20-20]R,size[250-250]R'
  )OUTPUT TO CsvStream;

CREATE STREAM newStream OF Atm;


CREATE CQ newCQ
INSERT INTO newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_INT(data[3]), TO_INT(data[4]) FROM
CsvStream;

CREATE WACTIONSTORE streamActivity CONTEXT OF Atm
EVENT TYPES ( Atm )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ newCQ2
INSERT INTO streamActivity
SELECT * FROM newStream
link source event;

END APPLICATION liveStream;

--
-- Recovery Test 23 with two sources, two sliding time windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> St1W -> CQ1 -> WS
-- S2 -> St2W -> CQ2 -> WS
--

STOP Recov23Tester.RecovTest23;
UNDEPLOY APPLICATION Recov23Tester.RecovTest23;
DROP APPLICATION Recov23Tester.RecovTest23 CASCADE;
CREATE APPLICATION RecovTest23 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 1 SECOND;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 2 SECOND;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION RecovTest23;

-----------------------------------
stop application SourceAgentApp;
undeploy application SourceAgentApp;

stop application TargetServerApp1;
undeploy application TargetServerApp1;

stop application TargetServerApp2;
undeploy application TargetServerApp2;

stop application TargetServerApp3;
undeploy application TargetServerApp3;

drop application SourceAgentApp cascade;
drop application TargetServerApp1 cascade;
drop application TargetServerApp2 cascade;
drop application TargetServerApp3 cascade;


CREATE APPLICATION SourceAgentApp;

create flow flow1;
create source CSVSource using FileReader (
directory: '@TEST-DATA-PATH@/tmp',
WildCard:'mybanks*',
positionByEOF: true,
charset:'UTF-8'
) parse using DSVParser (header:'no')
OUTPUT TO CsvStream;
end flow flow1;

--CREATE TARGET T USING Sysout(name:'sysout1') INPUT FROM CsvStream;

END APPLICATION SourceAgentApp;

DEPLOY APPLICATION SourceAgentApp with flow1 in AGENTS;

-- Fisrt app consuming from app running in agent
CREATE APPLICATION TargetServerApp1;
create flow flow2;

CREATE TARGET T2 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp1_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')
FORMAT USING JSONFormatter ()
INPUT FROM CsvStream;
end flow flow2;

END APPLICATION TargetServerApp1;
deploy application TargetServerApp1 with flow2 in default;


-- another app consuming from app running in agent

CREATE APPLICATION TargetServerApp2;
create flow flow3;

CREATE TARGET T3 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp2_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')
FORMAT USING JSONFormatter ()
INPUT FROM CsvStream;
end flow flow3;

END APPLICATION TargetServerApp2;
deploy application TargetServerApp2 with flow3 in default;

-- another app consuming from app running in agent. This app will be deployed and started after other apps started.

CREATE APPLICATION TargetServerApp3;
create flow flow4;

CREATE TARGET T4 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp3_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')
 FORMAT USING JSONFormatter ()
 INPUT FROM CsvStream;
end flow flow4;

END APPLICATION TargetServerApp3;

STOP banker.bankApp;
UNDEPLOY APPLICATION banker.bankApp;
DROP APPLICATION banker.bankApp cascade;

CREATE APPLICATION bankApp;


CREATE source wsSource USING FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;


CREATE TYPE wsData
(
bankID Integer KEY,
bankName String
);


CREATE WACTIONSTORE oneWS
CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@

CREATE CQ oneWSCQ
INSERT INTO oneWS
SELECT TO_INT(data[0]),data[1] FROM QaStream
LINK SOURCE EVENT;

END APPLICATION bankApp;
deploy application bankapp;
start application bankapp;

stop application JMSWriter.JMS;
undeploy application JMSWriter.JMS;
drop application JMSWriter.JMS cascade;

create application JMS;
create source JMSCSVSource using FileReader (
        directory:'@TEST-DATA-PATH@',
        WildCard:'AdhocQueryData2.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target JmsTarget  using JMSWriter (
		Provider:'@JMSWRITERPROVIDER@',
		Ctx:'@JMSWRITERCONTEXT@',
		messagetype: @MESSAGETYPE@,
		UserName:'@JMSWRITERUSERNAME@',
		Password:'@JMSWRITERPASSWORD@',
		@DESTINATIONTYPE@)
format using @JMSTARGETFORMATTERTYPE@ (
@JMSTARGETFORMATTERMEMBERS@
)
input from TypedCSVStream;

--SECOND TARGET--

Create Type SecondType (
  zip String,
  city String
);

Create Stream SecondStream of SecondType;

CREATE CQ SecondCQ
INSERT INTO SecondStream
SELECT data[9],data[10]
FROM CsvStream;

create Target SecondTarget using JMSWriter (
		Provider:'@JMSWRITERPROVIDER@',
		Ctx:'@JMSWRITERCONTEXT@',
		messagetype: @MESSAGETYPE@,
		UserName:'@JMSWRITERUSERNAME@',
		Password:'@JMSWRITERPASSWORD@',
		@DESTINATIONTYPE@)
format using @SECONDTARGETFORMATTERTYPE@ (
@SECONDTARGETFORMATTERMEMBERS@
)
input from SecondStream;

end Application Jms;

stop application app2PS;
undeploy application app2PS;
drop application app2PS cascade;

create application app2PS;

create target File_TargerPS2 using FileWriter
(
directory : '',
filename : ''
)
format using DSVFormatter()
input from KPSRss2;

end application app2PS;

deploy application app2PS;
start application app2PS;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

Create Source dataGenSrc USING MariaDBReader  ( 
  Username:'qatest',
  Password:'w3b@ct10n',
  ConnectionURL:'jdbc:mariadb://10.77.21.53:3306/qatest',
  Tables: '@tableNames@',
  ClusterSupport: 'Galera'
 ) 
 Output To LCRStream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;

END APPLICATION DataGenSampleApp;

Stop Oracle_LogWriter;
Undeploy application Oracle_LogWriter;
drop application Oracle_LogWriter cascade;

CREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;

CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  (

  FetchSize: 1000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=t1',
 startPosition: '%=-1',
  PollingInterval: '2sec',
  ConnectionPoolSize: 5,
  ThreadPoolSize: 5
  )
  OUTPUT TO data_stream;
create target AzureSQLDWHTarget using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:1000,interval:300s'
) INPUT FROM data_stream;
  CREATE OR REPLACE TARGET TeraSys USING SysOut  (
  name: 'ora12_out'
 ) INPUT FROM data_stream;

END APPLICATION Oracle_IRLogWriter;
deploy application Oracle_IRLogWriter;
start Oracle_IRLogWriter;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ @RECOVERY@;
create source @SOURCE@ using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'dynamicdirectory.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO @STREAM@;

create Target @TARGET@ using ADLSGen2Writer(
    accountname:'',
	sastoken:'',
	filesystemname:'',
	filename:'',
	directory:'',
	uploadpolicy:'eventcount:5000'
)format using DSVFormatter (
 members: 'data'
)
input from @STREAM@;

end application @APPNAME@;

deploy application @APPNAME@;
start application @APPNAME@;

UNDEPLOY APPLICATION admin.BasicAppNoFlow;
DROP APPLICATION admin.BasicAppNoFlow cascade;

CREATE APPLICATION BasicAppNoFlow;

CREATE SOURCE CsvDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt',
  header: Yes,
  columndelimiter: ',',
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE MerchantHourlyAve(
  merchantId	String,
  hourValue int,
  hourlyAve int
);

CREATE STREAM MerchantHourlyStream OF MerchantHourlyAve PARTITION BY merchantId;

CREATE CQ CsvToPosData
INSERT INTO MerchantHourlyStream
SELECT data[1], TO_INT(data[2]),
       TO_INT(data[3])
FROM CsvStream;


END APPLICATION BasicAppNoFlow;

-- The PosApp sample application demonstrates how a credit card
-- payment processor might use Striim to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.

stop admin.PosApp;
undeploy application admin.PosApp;
drop application admin.PosApp cascade;

CREATE APPLICATION PosApp;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING FileReader (
  directory:'Samples/Customer/PosApp/appData',
  wildcard:'$wildcard',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells Striim that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells Striim to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvToPosData

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP.
--
-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as
-- the types to be used in PosDataStream, which is created automatically.

-- The DATETIME field from the source is converted to both a dateTime
-- value, used as the event timestamp by the application, and (via the
-- function) an integer hourValue, which is used to look up
-- historical hourly averages from the HourlyAveLookup cache,
-- discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)
-- 17 (hourValue)
-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateOnly
--
-- The PosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: 'Samples/Customer/PosApp/appData',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startingTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;


-- WAction store MerchantActivity
--
-- The following group of statements create and populate the MerchantActivity
-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced
-- with merchant details from NameLookup cache and with latitude and longitude
-- values from the USAddressData cache.

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );


CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'Samples/Customer/PosApp/appData',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false
)
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: 'Samples/Customer/PosApp/appData',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;


-- CQ GenerateAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;



--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM AlertStream;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;


END APPLICATION PosApp;


CREATE DASHBOARD USING "Samples/Customer/PosApp/PosAppDashboard.json";

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

CREATE  SOURCE @QuerySource@1 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@1;
create Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;
 CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@1;

CREATE  SOURCE @QuerySource@2 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@2;
create Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;
 CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@2;

CREATE  SOURCE @QuerySource@3 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@3;
create Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;
 CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@3;

CREATE  SOURCE @QuerySource@4 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@4;
create Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;
 CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@4;

CREATE  SOURCE @QuerySource@5 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@5;
create Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;
 CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@5;

CREATE  SOURCE @QuerySource@6 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@6;
create Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;
 CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@6;

CREATE  SOURCE @QuerySource@7 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@7;
create Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;
 CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@7;

CREATE  SOURCE @QuerySource@8 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@8;
create Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;
 CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@8;

CREATE  SOURCE @QuerySource@9 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@9;
create Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;
 CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@9;

CREATE  SOURCE @QuerySource@10 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@10;
create Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;
 CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@10;
end application @APPNAME@;

CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (
  StartPosition: '@startPosition@',
  ConnectionURL: '@sourceURL@',
  Username: '@userName@',
  Tables: '@tables@',
  CheckColumn: '@checkColum@',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  Password: '@password@',
  Password_encrypted: 'false',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

STOP AdhocTester.ws_one;
UNDEPLOY APPLICATION AdhocTester.ws_one;
DROP APPLICATION AdhocTester.ws_one cascade;

CREATE APPLICATION ws_one;


CREATE SOURCE wsSource USING CSVReader
(
directory:'@TEST-DATA-PATH@',
header: Yes,
wildcard:'AdhocQueryData2.csv',
columndelimiter:',',
blocksize: 10240,
positionByEOF:false
) OUTPUT TO QaStream;

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE ZipLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'AdhocQueryData.csv',
  header: Yes,
  columndelimiter: '	',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

Create TYPE wsData(
	CompanyNum String,
	CompanyName String KEY,
	CompanyCode int,
	Zip String
);


CREATE STREAM wsStream OF wsData;

--Select data from QaStream and insert into wsStream

CREATE CQ csvTowsData
INSERT INTO wsStream
SELECT  data[0],
    data[1],
    TO_INT(data[3]),
    data[9]
 FROM QaStream;




CREATE WACTIONSTORE oneWS CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@


CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT * FROM wsStream
LINK SOURCE EVENT;

END APPLICATION ws_one;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.WAEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (
  ProviderName: '',
  Provider: '',
  Ctx: '',
  QueueName: '',
  Topic:'',
  UserName: '',
  Password: '',
  EnableTransaction: '',
  transactionpolicy: ''
 )
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING DSVFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream@RANDOM@;

END APPLICATION @APPNAME@;

stop application @APPNAME@app4;
undeploy application @APPNAME@app4;
alter application @APPNAME@app4;
CREATE or replace TARGET @APPNAME@app4_targetNew USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS4_Alter'
) INPUT FROM @APPNAME@sourcestream;
alter application @APPNAME@app4 recompile;
deploy application @APPNAME@app4;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;


create Target DBRTOCW_t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;

deploy application DBRTOCW on ANY in default;

start application DBRTOCW;

stop application MySQLToOracle;
undeploy application MySQLToOracle;
drop application MySQLToOracle cascade;

CREATE APPLICATION MySQLToOracle recovery 5 second interval;

create source Src1ReadFromMySQL USING MySQLReader (
Username: 'root',
  Password: 'w@ct10n',
  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',
--Tables: 'waction.mytable1;waction.mytable2;qatest.mysqlmarker',
Tables: 'waction.Parent%;waction.Child%;',
--Tables: 'waction.mytable%',
BiDirectionalMarkerTable: 'waction.mysqlmarker',
compression:'false',
sendBeforeImage:True
) OUTPUT TO App1Stream;

/*
CREATE TARGET MySQLReaderOutput
USING SysOut(name:MySQLReaderOutput)
INPUT FROM App1Stream;
*/

CREATE TARGET MySQLReaderOutput using FileWriter(
  filename:'MySQLReaderOutput.log',
  flushpolicy: 'EventCount:1, Interval:30s',
  rolloverpolicy: 'Eventcount:1000, rolloverpolicy:3000s')
FORMAT USING JsonFormatter ()

INPUT FROM App1Stream;


CREATE TARGET WriteToOracle USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'EventCount:10,Interval:5',
CommitPolicy:'EventCount:15,Interval:10',
BiDirectionalMarkerTable: 'qatest.mysqlmarker',
--Tables: 'waction.mytable1,qatest.mytable1;'
--Tables: 'waction.table_1_1,qatest.table_1_1;waction.table_2_1,qatest.table_2_1;waction.table_1_2,qatest.table_1_2;waction.table_2_2,qatest.table_2_2'
--Tables: 'waction.mytable%,qatest.%'
Tables: 'waction.Parent%,qatest.%;waction.Child%,qatest.%'

)
INPUT FROM App1Stream;



--Orcle to MySQL

Create Source SrcReadFromOracle
Using OracleReader
(
Username:'qatest',
Password:'qatest',
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
--Tables:'qatest.mytable1;qatest.mytable2;qatest.mysqlmarker;qatest.mysqlmarker2',
Tables:'QATEST.PARENT%;QATEST.CHILD%',
--Tables:'qatest.mytable%',
BiDirectionalMarkerTable: 'QATEST.MYSQLMARKER',
FilterTransactionBoundaries: true,
compression:'false',
FetchSize: 1

)
Output To App2Stream;

/*
CREATE TARGET MSSqlReaderOutput
USING SysOut(name:MSSqlReaderOutput)
INPUT FROM App2Stream;
*/

CREATE TARGET OracleReaderOutput using FileWriter(
  filename:'OracleReaderOutput.log',
    flushpolicy: 'EventCount:1, Interval:30s',
  rolloverpolicy: 'Eventcount:1000, rolloverpolicy:3000s')
FORMAT USING JsonFormatter ()

INPUT FROM App2Stream;



CREATE TARGET WriteToMySQL1 USING DatabaseWriter(
ConnectionURL:'jdbc:mysql://localhost:3306/waction',
Username:'root',
Password:'w@ct10n',
BatchPolicy:'EventCount:10,Interval:10',
CommitPolicy: 'EventCount:15,Interval:12',
BiDirectionalMarkerTable: 'waction.mysqlmarker',
--Tables: 'qatest.mytable1,waction.mytable1;qatest.mytable2,waction.mytable3;'
Tables: 'QATEST.PARENT_1,waction.Parent_1;QATEST.PARENT_2,waction.Parent_2;QATEST.CHILD_1,waction.Child_1;QATEST.CHILD_2,waction.Child_2;'
--Tables: 'qatest.mytable%,waction.%'
)
INPUT FROM App2Stream;




END APPLICATION MySQLToOracle;
deploy application MySQLToOracle;
start application MySQLToOracle;

STOP APPLICATION tpcc;
UNDEPLOY APPLICATION tpcc;
DROP APPLICATION tpcc CASCADE;

CREATE APPLICATION tpcc RECOVERY 5 SECOND Interval;

CREATE SOURCE Ojet_Source USING Ojet
(
    Username: '@Username@',
    Password: '@Password@',
    ConnectionURL: '@ConnectionURL@',
    Tables: '@Tables@',
)

OUTPUT TO SourceStream ;

create Target Ojet_FileWriter using FileWriter(
  filename:'qatar.csv',
  directory:'',
  flushpolicy: 'EventCount:10000,Interval:60s',
  rolloverpolicy: 'EventCount:10000,Interval:60s'
)
format using DSVFormatter (

)
input from SourceStream;

create Target t2 using SysOut(name:Foo2) input from SourceStream;

END APPLICATION tpcc;

DEPLOY APPLICATION tpcc;
START APPLICATION tpcc;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

CREATE  SOURCE @QuerySource@1 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@1;
create Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;
 CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@1;

CREATE  SOURCE @QuerySource@2 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@2;
create Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;
 CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@2;

CREATE  SOURCE @QuerySource@3 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@3;
create Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;
 CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@3;

CREATE  SOURCE @QuerySource@4 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@4;
create Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;
 CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@4;

CREATE  SOURCE @QuerySource@5 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@5;
create Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;
 CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@5;

CREATE  SOURCE @QuerySource@6 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@6;
create Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;
 CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@6;

CREATE  SOURCE @QuerySource@7 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@7;
create Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;
 CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@7;

CREATE  SOURCE @QuerySource@8 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@8;
create Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;
 CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@8;

CREATE  SOURCE @QuerySource@9 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@9;
create Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;
 CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@9;

CREATE  SOURCE @QuerySource@10 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@10;
create Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;
 CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@10;
end application @APPNAME@;

alter APPLICATION DBRTOCW;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes1',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource2 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes2',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource3 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes3',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget2 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget3 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:60',
  CommitPolicy: 'EventCount:1,Interval:60',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget4 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:120',
  CommitPolicy: 'EventCount:1,Interval:120',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget5 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:120',
  CommitPolicy: 'EventCount:1,Interval:120',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  ExcludedTables:'QATEST.ORACTOCQL_ALLDATATYPES',
  Password: '+hbb060plSWQwscvI105cg==',
  Password_encrypted: true
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET FWTarget USING FileWriter(
	name:CassandraOuput,
	filename:'OracToFw.log',
	flushpolicy : 'interval:120,eventcount:11',
	rolloverpolicy : 'interval:300s'
)
FORMAT USING DSVFormatter()
INPUT FROM Oracle_ChangeDataStream;
create  or replace Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;


END APPLICATION DBRTOCW;
ALTER APPLICATION DBRTOCW RECOMPILE;
deploy application DBRTOCW in default;
start DBRTOCW;

--
-- Recovery Test 35 with two sources, two jumping count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5W/p -> CQ1 -> WS
--   S2 -> Jc6W/p -> CQ2 -> WS
--

STOP KStreamRecov35Tester.KStreamRecovTest35;
UNDEPLOY APPLICATION KStreamRecov35Tester.KStreamRecovTest35;
DROP APPLICATION KStreamRecov35Tester.KStreamRecovTest35 CASCADE;

DROP USER KStreamRecov35Tester;
DROP NAMESPACE KStreamRecov35Tester CASCADE;
CREATE USER KStreamRecov35Tester IDENTIFIED BY KStreamRecov35Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov35Tester;
CONNECT KStreamRecov35Tester KStreamRecov35Tester;

CREATE APPLICATION KStreamRecovTest35 RECOVERY 5 SECOND INTERVAL

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION KStreamRecovTest35;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ recovery 5 second Interval;

CREATE SOURCE @srcName@ USING Global.S3Reader ( 
 secretaccesskey: '@secretaccesskey@', 
  foldername: '@sourcefoldername@', 
  blocksize: 64, 
  bucketname: '@bucketname@', 
  objectnameprefix: '@sourcefilename@', 
  accesskeyid: '@accesskeyid@' ) 
PARSE USING Global.DSVParser ( 
  trimwhitespace: false, 
  commentcharacter: '', 
  linenumber: '-1', 
  columndelimiter: ',', 
  trimquote: true, 
  columndelimittill: '-1', 
  ignoreemptycolumn: false, 
  separator: ':', 
  quoteset: '\"', 
  charset: 'UTF-8', 
  ignoremultiplerecordbegin: 'true', 
  ignorerowdelimiterinquote: false, 
  header: false, 
  blockascompleterecord: false, 
  rowdelimiter: '\n', 
  nocolumndelimiter: false, 
  headerlineno: 0 ) 
OUTPUT TO @outstreamname@;

CREATE OR REPLACE TARGET @tgtName@ USING Global.S3Writer ( 
  UploadConfigValueSeparator: '=', 
  UploadConfigPropertySeparator: ',', 
  objectname: '@targetfilename@', 
  secretaccesskey: '@secretaccesskey@', 
  ParallelThreads: '', 
  rolloveronddl: 'true', 
  bucketname: '@bucketname@', 
  foldername: '@targetfoldername@', 
  uploadpolicy: 'eventcount:5,interval:60s', 
  accesskeyid: '@accesskeyid@' ) 
FORMAT USING Global.DSVFormatter  ( 
  quotecharacter: '\"', 
  columndelimiter: ',', 
  members: 'data', 
  nullvalue: 'NULL', 
  usequotes: 'false', 
  rowdelimiter: '\n', 
  standard: 'none', 
  header: 'false' ) 
INPUT FROM @instreamname@;
End APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

use admin;
drop application ns1.testgroupapp1 cascade;
drop application ns1.testgroupapp2 cascade;
drop application ns2.testgroupapp1 cascade;
drop application ns2.testgroupapp2 cascade;
drop namespace ns1 CASCADE;
drop namespace ns2 CASCADE;

stop application AzureDLSGen1_sanity;
undeploy application AzureDLSGen1_sanity;
drop application AzureDLSGen1_sanity cascade;


create application AzureDLSGen1_sanity recovery 5 Second interval;
create source CSVSource using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'dynamicdirectory.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO CsvStream;

create Target WriteToADLSGen1 using ADLSGen1Writer(
        filename:'',
        directory:'',
        datalakestorename:'',
        clientid:'',
        authtokenendpoint:'',
        clientkey:'',
		rolloverpolicy:'eventcount:100000'
)
format using DSVFormatter (
)
input from CsvStream; 

end application AzureDLSGen1_sanity;

deploy application AzureDLSGen1_sanity;
start application AzureDLSGen1_sanity;

STOP APPLICATION SystemTimeTester.SystemTimeWindows;
UNDEPLOY APPLICATION SystemTimeTester.SystemTimeWindows;
DROP APPLICATION SystemTimeTester.SystemTimeWindows cascade;

CREATE APPLICATION SystemTimeWindows;

CREATE TYPE RandomData(
bankNumber int KEY,
bankName String
);


CREATE SOURCE ranDataSource using StreamReader(
OutputType: 'SystemTimeTester.RandomData',
noLimit: 'false',
isSeeded: 'true',
maxRows: 0,
iterations: 30,
iterationDelay: 1000,
StringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',
NumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]R'
)OUTPUT TO CSVDataStream;

CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT (data[0]), data[1]
FROM CSVDataStream;

CREATE @WINDOWTYPE@ WINDOW tierone OVER RandomDataStream keep within 20 second;

CREATE STREAM onetwostream OF RandomData;

CREATE CQ onetwocq
INSERT INTO onetwostream
SELECT bankNumber,bankName
FROM tierone
where bankName ='bofa'
order by bankNumber;

CREATE @WINDOWTYPE@ WINDOW tiertwo OVER onetwostream keep within 40 second;

CREATE STREAM twothreestream OF RandomData;

CREATE CQ twothreecq
INSERT INTO twothreestream
SELECT bankNumber,bankName
FROM tierTwo
where bankName ='bofa'
order by bankNumber;

CREATE @WINDOWTYPE@ WINDOW tierthree OVER twothreestream keep within 1 minute;

CREATE WACTIONSTORE MyDataActivity
CONTEXT OF RandomData
EVENT TYPES(RandomData )
@PERSIST-TYPE@

Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
SELECT bankNumber,bankName from @FROMSTREAM@
where bankName ='bofa'
order by bankNumber
LINK SOURCE EVENT;


END APPLICATION SystemTimeWindows;
deploy application SystemTimeWindows;
start application SystemTimeWindows;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.AvroEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING Global.JMSReader (
  ProviderName: '',
  Provider: '',
  Ctx: '',
  QueueName: '',
  Topic:'',
  UserName: '',
  Password: '',
  EnableTransaction: '',
  transactionpolicy: ''
  )
PARSE USING Global.ParquetParser ()
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

END APPLICATION @APPNAME@;

Stop Oracle_IRLogWriter;
Undeploy application Oracle_IRLogWriter;
drop application Oracle_IRLogWriter cascade;

CREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;

CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( 
 
  FetchSize: 5000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=t1',
 startPosition: '%=-1',
  PollingInterval: '20sec'
  )
  OUTPUT TO data_stream;
create target AzureSQLDWHTarget using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10000,interval:300s'
) INPUT FROM data_stream;
  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

END APPLICATION Oracle_IRLogWriter;
deploy application Oracle_IRLogWriter;
start Oracle_IRLogWriter;

Stop Oracle_IRLogWriter2;
Undeploy application Oracle_IRLogWriter2;
drop application Oracle_IRLogWriter2 cascade;

CREATE APPLICATION Oracle_IRLogWriter2 recovery 5 second interval;

CREATE OR REPLACE SOURCE Oracle_IRSource2 USING IncrementalBatchReader  ( 
 
  FetchSize: 5000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=t1',
 startPosition: '%=-1',
  PollingInterval: '20sec'
  )
  OUTPUT TO data_stream2;
create target AzureSQLDWHTarget2 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10000,interval:300s'
) INPUT FROM data_stream2;
  CREATE OR REPLACE TARGET TeraSys2 USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream2;

END APPLICATION Oracle_IRLogWriter2;
deploy application Oracle_IRLogWriter2;
start Oracle_IRLogWriter2;

create Target @TARGET_NAME@ using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'%@metadata(TableName)%',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
		rolloverpolicy:'filesize:10M',
		compressiontype: 'false'
)
format using DSVFormatter (
)
input from @STREAM@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;

Create Source @SourceName@ Using OracleReader

(
  Username:'c##qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',
  Tables:'CDB$ROOT."C##QATEST".ojet_src;ORCLPDB.QATEST.ojet_src',
  _h_useClassic:false,
  Fetchsize:1,
  Compression: true,
  SupportPDB:true,
  ReplicationSlotName:'null'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',
  Username:'c##qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'CDB$ROOT."C##QATEST".ojet_src,CDB$ROOT."C##QATEST".ojet_tgt'
) INPUT FROM @SRCINPUTSTREAM@;



create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop APPLICATION OrcToDWH;
undeploy APPLICATION OrcToDWH;
DROP APPLICATION OrcToDWH CASCADE;
CREATE APPLICATION OrcToDWH recovery 5 second interval;
Create Source OracleSource Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;


create target AzureTarget using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
        _h_TransportOptions:'connectionTimeout=30s, readTimeout=12s',
) INPUT FROM str;

END APPLICATION OrcToDWH;
deploy APPLICATION OrcToDWH;
start APPLICATION OrcToDWH;

STOP application admin.SampleApp;
undeploy application admin.SampleApp;
drop application admin.SampleApp cascade;


CREATE APPLICATION SampleApp RECOVERY 10 SECOND INTERVAL;

CREATE SOURCE Oracle_Src USING Global.OracleReader (
  Tables: 'QATEST.TEST01',
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  OutboundServerProcessName: 'WebActionXStream',
  Password: 'qatest',
  Compression: false,
  ReaderType: 'LogMiner',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  FetchSize: 1,
  Password_encrypted: 'false',
  SupportPDB: false,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CDDLAction: 'Process',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  DictionaryMode: 'OnlineCatalog',
  QueueSize: 2048,
  CommittedTransactions: true,
  XstreamTimeOut: 600,
  CDDLCapture: false,
  TransactionBufferType: 'Disk',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '100MB',
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  DatabaseRole: 'Primary' )
OUTPUT TO Striim_Buffer;

CREATE TARGET Oracle_tgt USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'qatest',
  ParallelThreads: '',
  DatabaseProviderType: 'Oracle',
  CheckPointTable: 'CHKPOINT',
  Password_encrypted: 'false',
  CDDLAction: 'Process',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.TEST01,QATEST.TEST02',
  CommitPolicy: 'EventCount:1,Interval:10',
  StatementCacheSize: '50',
  Username: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:10',
  PreserveSourceTransactionBoundary: 'false' )
INPUT FROM Striim_Buffer;

END APPLICATION SampleApp;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using googlepubsubwriter(
    ServiceAccountKey:'@SAS-KEY@',
ProjectId:'@PROJECTID@',
topic:'@topic@',
BatchPolicy:'@BATCHPOLICY@'
)
format using DSVFormatter (
)
input from @STREAM@;

end application @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@ Using Ojet
(
Username:'@OJET-UNAME@',
Password:'@OJET-PASSWORD@',
ConnectionURL:'@OCI-URL@',
Tables:'@SourceTable@',
)
Output To @SRCINPUTSTREAM@;
CREATE OR REPLACE STREAM Rstream1 OF Global.WAEvent;
CREATE OR REPLACE OPEN PROCESSOR Open_Processor1 USING PartialRecordPolicy
(
Username: 'qatest_ojet',
Password: 'qatest_ojet',
ConnectionURL: 'jdbc:oracle:oci:@//localhost:1525/orcl',
Tables: 'QATEST.Ojetsrc(ROWIDCOL)'
)
INSERT INTO Rstream1
FROM @SRCINPUTSTREAM@;
CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
Username:'@UN@',
Password:'@PWD@',
BatchPolicy:'EventCount:1,Interval:1',
Tables: '@Tablemapping@'
)INPUT FROM Rstream1;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING Global.DeltaLakeWriter (
  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.testaswin',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @STREAM@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE TYPE @APPNAME@oracleType (
 companyName java.lang.String,
 merchantId java.lang.String);

CREATE SOURCE @APPNAME@_Orcl USING OracleReader (
  ConnectionURL: '',
  Password: '',
  Tables: '',
  Username: ''
  )
OUTPUT TO @APPNAME@OracleOut;

CREATE OR REPLACE STREAM @APPNAME@TypedStream OF @APPNAME@oracleType;

CREATE OR REPLACE CQ @APPNAME@CQOut
INSERT INTO @APPNAME@TypedStream
SELECT
TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantID
FROM @APPNAME@OracleOut o;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@TypedStream;

END APPLICATION @APPNAME@;

CREATE OR REPLACE APPLICATION @AppName@;

Create Source @AppName@_source Using OracleReader(
  Username:'@username@',
  Password:'@password@',
  ConnectionURL:'@url@',
  Tables:'@srctableName@',
  Fetchsize:1
)
Output To @AppName@_Stream;
CREATE OR REPLACE TARGET @AppName@_SF_Target USING Global.SnowflakeWriter (
  streamingUpload: 'false',
  useConnectionProfile:'true',
  connectionProfileName: 'admin.@SFCP@',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  columnDelimiter: '|',
  tables: '@srctableName@,@trgtableName@',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:1,interval:5m',
  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',
  externalStageType: 'Local',
  adapterName: 'SnowflakeWriter',
  fileFormatOptions: 'null_if = \"\"' )
INPUT FROM @AppName@_Stream;

CREATE OR REPLACE TARGET @AppName@_SF_Target2 USING Global.SnowflakeWriter (
  streamingUpload: 'false',
  useConnectionProfile:'true',
  connectionProfileName: 'admin.@SFCP@',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  columnDelimiter: '|',
  tables: '@srctableName@,@trgtableName@',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:1,interval:5m',
  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',
  externalStageType: 'Local',
  adapterName: 'SnowflakeWriter',
  fileFormatOptions: 'null_if = \"\"' )
INPUT FROM @AppName@_Stream;

END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_SourceFlow;
CREATE OR REPLACE SOURCE @SOURCE@ USING Ojet  (
  FilterTransactionBoundaries: true,
  ConnectionURL: '@OCI-URL@',
  Tables: '@SOURCE_TABLE@',
  Password: '@OJET-PASSWORD@',
  fetchsize: 1,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Username: '@OJET-UNAME@'
 )
OUTPUT TO @STREAM@;
end flow @APPNAME@_SourceFlow;
create flow @APPNAME@_TargetFlow;
CREATE OR REPLACE TARGET @TARGET@1 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'false',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@2 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'false',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'true',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@3 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'true',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;
end flow @APPNAME@_TargetFlow;
END APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName1@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM1@;
Create Source @SourceName2@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM2@;
Create Source @SourceName3@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM3@;
Create Source @SourceName4@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM4@;
Create Source @SourceName5@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM5@;
Create Source @SourceName6@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM6@;
Create Source @SourceName7@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM7@;
Create Source @SourceName8@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM8@;
Create Source @SourceName9@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM9@;
Create Source @SourceName10@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM10@;

CREATE TARGET @targetName1@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM1@;
CREATE TARGET @targetName2@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM2@;
CREATE TARGET @targetName3@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM3@;
CREATE TARGET @targetName4@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM4@;
CREATE TARGET @targetName5@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM5@;
CREATE TARGET @targetName6@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM6@;
CREATE TARGET @targetName7@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM7@;
CREATE TARGET @targetName8@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM8@;
CREATE TARGET @targetName9@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM9@;
CREATE TARGET @targetName10@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM10@;


END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;
Create Source OracleSource Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;


create target AzureTarget using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

create Target t2 using SysOut(name:Foo2) input from str;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

-------------------------
-- The tql checks the auto conversion feature for multiple tables.
-- It covers all the data types supported by sqlmp (except "interval").
-------------------------

IMPORT static com.webaction.runtime.converters.DateConverter.*;

UNDEPLOY APPLICATION admin.SQLMPReaderApp;
DROP APPLICATION admin.SQLMPReaderApp cascade;

CREATE APPLICATION SQLMPReaderApp;
create source SQLMPSource using HPNonStopSQLMPReader (
    portno:2020,
	ipaddress:'10.10.196.103',
	Name:'intg',
	AuditTrails:'parallel',
	AgentPortNo:8012,
	AgentIpAddress:'10.10.197.150', 
    Tables:'$DATA06.MAHA.ESA;$DATA06.MAHA.ESB;$DATA06.MAHA.ESC'
) OUTPUT TO CDCStream,
ESAStream MAP (table:'\\RPC4.$DATA06.MAHA.ESA'),
ESBStream MAP (table:'\\RPC4.$DATA06.MAHA.ESB');


CREATE TYPE ESCStreamType(
C0 Short,
C1 Long,
C2 Long,
C3 String,
C4 String,
C5 Integer,
C6 Long,
C7 String,
C8 String,
C9 Double,
C10 Double,
OPR String,
TABLENAME String
);

CREATE STREAM ESCStream OF ESCStreamType;


CREATE JUMPING WINDOW SQLMPDataWindow
OVER ESCStream KEEP 5 ROWS
PARTITION BY OPR;

CREATE CQ ESCStreamCq
INSERT INTO ESCStream
SELECT TO_SHORT(data[0]),
    TO_LONG(data[1]),
    TO_LONG(data[2]),
    data[3],
    data[4],
    TO_INT(data[5]),
       TO_LONG(data[6]),
    data[7],
    data[8],
    TO_DOUBLE(data[9]),
    TO_DOUBLE(data[10]),
    META(x,"OperationName").toString(),
    META(x, "TableName").toString()
FROM CDCStream x
WHERE not(META(x,"OperationName").toString() = "BEGIN") AND not(META(x,"OperationName").toString() = "COMMIT") AND not(META(x, "TableName").toString() is null) AND META(x, "TableName").toString() = "\\\\RPC4.$DATA06.MAHA.ESC";


CREATE TYPE SQLMPOperationData(
    TableName String,
    OperationName String,
    Count Integer
);

CREATE STREAM SQLMPOperationDataStream OF SQLMPOperationData;

CREATE CQ SQLMPOperationCheck
INSERT INTO SQLMPOperationDataStream
SELECT x.TABLENAME,
CASE WHEN x.OPR = 'INSERT' THEN x.OPR
     WHEN x.OPR = 'DELETE' THEN x.OPR
     WHEN x.OPR = 'UPDATE' THEN x.OPR
     ELSE 'UNSUPPORTED OPREATION' END,
CASE WHEN x.OPR = 'INSERT' THEN COUNT(x.OPR)
     WHEN x.OPR = 'DELETE' THEN COUNT(x.OPR)
     WHEN x.OPR = 'UPDATE' THEN COUNT(x.OPR)
     ELSE 0 END
FROM SQLMPDataWindow x
GROUP BY OPR;

CREATE TARGET Log USING LogWriter(
  name:SQLMPReaderAppESA,
-- filename:'@FEATURE-DIR@/logs/SQLMPReaderAppESA.log'
  filename:'mp.log'
) INPUT FROM ESAStream;


CREATE TARGET Log1 USING LogWriter(
  name:SQLMPReaderAppESB,
--  filename:'@FEATURE-DIR@/logs/SQLMPReaderAppESB.log'
  filename:'mp1.log'
) INPUT FROM ESBStream;


CREATE TARGET OperationLog USING LogWriter(
  name:SQLMPReaderAppESC,
--  filename:'@FEATURE-DIR@/logs/SQLMPReaderOperationCheck.log'
  filename:'mp2.log'
) INPUT FROM SQLMPOperationDataStream;

END APPLICATION SQLMPReaderApp;

STOP APPLICATION bq;
UNDEPLOY APPLICATION bq;
DROP APPLICATION bq CASCADE;
CREATE APPLICATION bq;

CREATE SOURCE s USING FileReader
(
  directory:'/Users/sujith_syk/MyTasks/Analyse_BQtableScan_on_partitioned_table_merge_query',
  WildCard:'testdata1.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO FileStream;

CREATE TYPE cdctype(
    STORE_ID  String,
    NAME  String,
    CITY  String,
    STATE  String,
    ZIP  String,
    CUSTOMER_ACCOUNT_NUMBER  String,
    ORDER_ID  String,
    SKU  String,
    ORDER_AMOUNT  String,
    DATETIME  String
);

CREATE STREAM cdctypestream OF cdctype;

CREATE CQ cdcstreamcq
INSERT INTO cdctypestream
SELECT TO_STRING(p.data[0]),
       TO_STRING(p.data[1]),
       TO_STRING(p.data[2]),
       TO_STRING(p.data[3]),
       TO_STRING(p.data[4]),
       TO_STRING(p.data[5]),
       TO_STRING(p.data[6]),
       TO_STRING(p.data[7]),
       TO_STRING(p.data[8]),
       TO_STRING(p.data[9])
FROM FileStream p;


CREATE TARGET t1 USING BigQueryWriter
(
  ServiceAccountKey:'/Users/sujith_syk/Documents/striimdev-creds.json',
  projectId:'striimdev',
  Tables:'qatest.FILETOBQHEAPTEST1',
  datalocation:'US',
  nullmarker:'NOTNULL',
  columnDelimiter:'|',
  BatchPolicy:'eventCount:100, Interval:180',
  Mode: 'MERGE'
) INPUT FROM cdctypestream;

CREATE TARGET t2 USING BigQueryWriter
(
  ServiceAccountKey:'/Users/sujith_syk/Documents/striimdev-creds.json',
  projectId:'striimdev',
  Tables:'qatest.FILETOBQHEAPTEST2',
  datalocation:'US',
  nullmarker:'NOTNULL',
  columnDelimiter:'|',
  BatchPolicy:'eventCount:100, Interval:180',
  Mode: 'MERGE'
) INPUT FROM cdctypestream;

CREATE TARGET t3 USING BigQueryWriter
(
  ServiceAccountKey:'/Users/sujith_syk/Documents/striimdev-creds.json',
  projectId:'striimdev',
  Tables:'qatest.FILETOBQHEAPTEST3',
  datalocation:'US',
  nullmarker:'NOTNULL',
  columnDelimiter:'|',
  BatchPolicy:'eventCount:100, Interval:180',
  Mode: 'MERGE'
) INPUT FROM cdctypestream;

END APPLICATION bq;
DEPLOY APPLICATION bq;
START APPLICATION bq;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using googlepubsubwriter(
    ServiceAccountKey:'@SAS-KEY@',
ProjectId:'@PROJECTID@',
topic:'@topic@',
BatchPolicy:'@BATCHPOLICY@'
)
format using DSVFormatter (
)
input from @STREAM@;


end flow @APPNAME@_serverflow;

end application @APPNAME@;

create or replace Target EH_TARGET using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_01',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_01_cg',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from EH_SS;

Create Target @TARGET_NAME@ using HiveWriter
(
  ConnectionURL:'jdbc:hive2://dockerhost:10000/default',
  Username:'cloudera',
  Password:'cloudera',
  hadoopurl:'hdfs://dockerhost:9000',
  Mode:'initialload',
  mergepolicy:'eventcount:5,interval:1s',
  Tables:'QATEST.HIVE_EMP,default.hive_emp KEYCOLUMNS(id)',
  hadoopConfigurationPath:'/home/ubuntu/Product/IntegrationTests/TestData/hdfsconf/'
)
INPUT FROM @STREAM@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '@GRPPOLICY@',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'Text',
  ProcessCopyBookFileAs: 'MultipleEvents',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
  filename: '%@metadata(FileName)%',
  directory: '@DIR@',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter  ()
INPUT FROM CCBStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;;

Create Source OracleSource Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;


create target AzureTarget1 using AzureSQLDWHWriter (
		CoNNectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',  
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;


create target AzureTarget2 using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        UserName: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@', 
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;


create target AzureTarget3 using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        UserName: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@', 
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

create target AzureTarget4 using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        UserName: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@', 
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@ MAP (table: 'QATEST.SMFTEST6')
SELECT NUM_COL,CHAR_COL,VARCHAR2_COL,LONG_COL,DATE_COL,TIMESTAMP_COL where TO_INT(NUM_COL) > 1;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

stop application @APPNAME@Apps2;
undeploy application @APPNAME@Apps2;
drop application @APPNAME@Apps2 cascade;


stop application @APPNAME@Apps3;
undeploy application @APPNAME@Apps3;
drop application @APPNAME@Apps3 cascade;



stop application @APPNAME@Apps4;
undeploy application @APPNAME@Apps4;
drop application @APPNAME@Apps4 cascade;



stop application @APPNAME@Apps1;
undeploy application @APPNAME@Apps1;
drop application @APPNAME@Apps1 cascade;

CREATE OR REPLACE PROPERTYSET @APPNAME@Apps_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9099', kafkaversion:'0.11');

CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream1 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;
CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream3 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;
CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream4 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;


--**********************Application 1*******************
-- with 2 source flow 
-- <sourceflow1>source1->PS1<sourceflow1>
-- <sourceflow2>Source2->Inmemory1<sourceflow2>
-- Inmemomry1->cq1->PS1

CREATE APPLICATION @APPNAME@Apps1 RECOVERY 5 SECOND INTERVAL;
create flow @APPNAME@serverflow;
CREATE OR REPLACE SOURCE @APPNAME@AppsDBSource1 USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@1',
  adapterName: 'OracleReader',
  Password: '@PASSWORD@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@USERNAME@',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@Apps_PS_Stream1;
end flow @APPNAME@serverflow;

create flow @APPNAME@serverflow2;
CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream2 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;
CREATE OR REPLACE SOURCE @APPNAME@AppsDBSource2 USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@2;@SOURCE_TABLE@3',
  adapterName: 'OracleReader',
  Password: '@PASSWORD@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@USERNAME@',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@Apps_PS_InmemoryStream1;
end flow @APPNAME@serverflow2;

CREATE CQ @APPNAME@cq_Inmemory1
INSERT INTO @APPNAME@Apps_PS_Stream2
SELECT *
FROM @APPNAME@Apps_PS_InmemoryStream1;

end application @APPNAME@Apps1;

-- ********************Application 2***********************
-- PS1->Target1

CREATE APPLICATION @APPNAME@Apps2 RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE TARGET @APPNAME@Apps2DBTarget1 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:5,Interval:0',
  CommitPolicy: 'EventCount:5,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@1,@TARGET_TABLE@1',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream1;
END APPLICATION @APPNAME@Apps2;

-- ********************Application 3**************************
--WITH 3 TARGET FLOW
-- 1. <targetflow1> cq->ps3->target2 <targetflow1>
-- 2. cq->ps4 <targetflow2>ps4->target3 <targetflow2>
-- 3. <targetflow3> ps2 -> target4 <targetflow3>

CREATE APPLICATION @APPNAME@Apps3 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @APPNAME@TARGETFLOW1;
CREATE CQ @APPNAME@cq_ps1
INSERT INTO @APPNAME@Apps_PS_Stream3
SELECT *
FROM @APPNAME@Apps_PS_Stream2
WHERE META(@APPNAME@Apps_PS_Stream2,'TableName').toString() == '@SOURCE_TABLE@2';
CREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget2 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@2,@TARGET_TABLE@2',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream3;
END FLOW @APPNAME@TARGETFLOW1;

CREATE CQ @APPNAME@cq_ps2
INSERT INTO @APPNAME@Apps_PS_Stream4
SELECT *
FROM @APPNAME@Apps_PS_Stream2
WHERE META(@APPNAME@Apps_PS_Stream2,'TableName').toString() == '@SOURCE_TABLE@3';

CREATE FLOW @APPNAME@TARGETFLOW2;
CREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget3 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:1000,Interval:0',
  CommitPolicy: 'EventCount:1000,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@3,@TARGET_TABLE@3',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream4;
END FLOW @APPNAME@TARGETFLOW2;

CREATE FLOW @APPNAME@TARGETFLOW3;
CREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget4 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:50,Interval:0',
  CommitPolicy: 'EventCount:50,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@2,@TARGET_TABLE@4;@SOURCE_TABLE@3,@TARGET_TABLE@4',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream2;
END FLOW @APPNAME@TARGETFLOW3;

END APPLICATION @APPNAME@Apps3;


--********************Application 4************************
-- PS2->cq->Inmemory2 <targetflow4> Inmemory2->target5<targetflow4>
-- PS3->Target6

CREATE APPLICATION @APPNAME@Apps4 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE STREAM @APPNAME@Apps_PS_InmemoryStream2 OF Global.waevent;

CREATE CQ @APPNAME@cq_Ps_Inmemory1
INSERT INTO @APPNAME@Apps_PS_InmemoryStream2
SELECT *
FROM @APPNAME@Apps_PS_Stream2
WHERE META(@APPNAME@Apps_PS_Stream2,'TableName').toString() == '@SOURCE_TABLE@3';

create flow @APPNAME@targetflow4;
CREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget5 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:10,Interval:0',
  CommitPolicy: 'EventCount:10,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@3,@TARGET_TABLE@5',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_InmemoryStream2;
end flow @APPNAME@targetflow4;

CREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget6 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@2,@TARGET_TABLE@6',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream3;

END APPLICATION @APPNAME@Apps4;

deploy application @APPNAME@Apps4 on ANY in Appsdg with @APPNAME@Targetflow4 in Targetdg;
start application @APPNAME@Apps4;

deploy application @APPNAME@Apps3 on ANY in Appsdg with @APPNAME@Targetflow3 in Targetdg,@APPNAME@Targetflow2 in Targetdg,@APPNAME@Targetflow1 in Targetdg;
start application @APPNAME@Apps3;

deploy application @APPNAME@Apps2 on ANY in default;
start application @APPNAME@Apps2;

deploy application @APPNAME@Apps1 in Appsdg with @APPNAME@serverflow in sourcedg, @APPNAME@serverflow2 in sourcedg2;
start application @APPNAME@Apps1;

stop @APPNAME@;
undeploy application @APPNAME@;
--drop exceptionstore admin.MySQL_To_MySQLApp_ExceptionStore;
drop application @APPNAME@ cascade;
create application @APPNAME@ use exceptionstore;

create source @SourceName@ using MySQLReader
  (ConnectionURL: '@SourceConnectionURL@',
   Username:'@UserName@',
   Password:'@Password@',
   Tables: '@SourceTableName@'
)
output to @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
   ConnectionURL:'@TargetConnectionURL@',
   Username:'@UserName@',
   Password:'@Password@',
   BatchPolicy:'EventCount:1,Interval:0',
   Tables: '@SourceTableName@,@TargetTableName@',
   CommitPolicy: 'Interval:5'
 ) INPUT FROM @SRCINPUTSTREAM@;

create or replace cq @cq@
insert into @finalstream@
select exceptionType,action,appName,entityType,entityName,className,message,relatedActivity from @APPNAME@_ExceptionStore;

Create target @targetfile@ using filewriter (
filename:'@APPNAME@_file.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using jsonFormatter()
input from @finalstream@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop @APPNAME@_ExpStore;
undeploy application @APPNAME@_ExpStore;
drop application @APPNAME@_ExpStore cascade;
CREATE APPLICATION @APPNAME@_ExpStore;

CREATE TYPE @APPNAME@_ExpStore_CDCStreams_Type  (
  evtlist java.util.List  
 );

CREATE STREAM @APPNAME@_ExpStore_CDCStreams OF @APPNAME@_ExpStore_CDCStreams_Type;

CREATE CQ @APPNAME@_ReadFromExpStore 
INSERT INTO @APPNAME@_ExpStore_CDCStreams
select to_waevent(s.relatedObjects) as evtlist from admin.@APPNAME@_ExceptionStore [jumping 5 second] s;

CREATE STREAM @APPNAME@_ExpStore_CDCEventStream OF Global.WAEvent;

CREATE CQ @APPNAME@_ExpStore_GetCDCEvent 
INSERT INTO @APPNAME@_ExpStore_CDCEventStream
SELECT com.webaction.proc.events.WAEvent.makecopy(cdcevent) FROM @APPNAME@_ExpStore_CDCStreams a, iterator(a.evtlist) cdcevent;

CREATE CQ @APPNAME@_ExpStore_JoinDataCQ
INSERT INTO @APPNAME@_ExpStore_JoinedDataStream
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1])
        from @APPNAME@_ExpStore_CDCEventStream f;
        
CREATE OR REPLACE TARGET @APPNAME@_ExpStore_WriteToFileAsJSON USING FileWriter  ( 
  filename: '@APPNAME@_file',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:1,interval:30',
  adapterName: 'FileWriter',
  directory: 'ExpStore_logs',
  rolloverpolicy: 'eventcount:6,interval:30s'
 ) 
FORMAT USING JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter',
  formatterName: 'JSONFormatter',
  jsonMemberDelimiter: '\n',
  jsonobjectdelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true'
 ) 
INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;
        
CREATE TARGET @APPNAME@_ExpStore_dbtarget USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1,Interval:1',
CommitPolicy:'Interval:1,Eventcount:1',
Tables:'@TargetTable@'
) INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;

END APPLICATION @APPNAME@_ExpStore;

deploy application @APPNAME@_ExpStore;
start @APPNAME@_ExpStore;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 10 second interval;
CREATE SOURCE OracleSource USING OracleReader
(
    Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
 Tables:'@TABLES@',
    FetchSize: 1
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId int,
  curr String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT TO_INT(data[0]),
       data[1]
FROM CsvStream;

create Target BlobT using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:5,interval:5s'
)
format using AvroFormatter (
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using S3Writer(
    bucketname:'@BUCKET@',
   objectname:'upgradeData.csv',
   foldername:'upgradefolder',
  uploadpolicy:'EventCount : 10000,Interval :1m '
)
format using DSVFormatter (
)
input from @STREAM@;

end flow @APPNAME@_serverflow;

end application @APPNAME@;

STOP UpdatableCacher.UpdatableCache;
UNDEPLOY APPLICATION UpdatableCacher.UpdatableCache;
DROP APPLICATION UpdatableCacher.UpdatableCache CASCADE;
CREATE APPLICATION UpdatableCacher.UpdatableCache;

CREATE TYPE MerchantHourlyAve(
  merchantId String KEY,
  hourlyAve Integer,
  theDate DateTime,
  price Double
);


CREATE source CsvDataSource USING FileReader (
      directory:'@TEST-DATA-PATH@',
      columndelimiter: ',',
      wildcard:'ucData.csv',
      blocksize: 10240,
      positionByEOF:false
)
PARSE USING DSVParser (
      header:No,
      trimquote:false
) OUTPUT TO CsvStream;


CREATE STREAM S1 OF MerchantHourlyAve;

CREATE CQ cq1
	insert into S1
		SELECT data[0],
				TO_INT(data[1]),
				TO_DATE(data[2]),
				TO_DOUBLE(data[3])
		FROM CsvStream;


CREATE EVENTTABLE ET1 using STREAM (
  NAME: 'S1'
) QUERY (keytomap:'price', persistPolicy: 'true' ) OF MerchantHourlyAve;


CREATE EVENTTABLE ET2 using STREAM (
  NAME: 'S1'
) QUERY (keytomap:'merchantId' ) OF MerchantHourlyAve;
--) QUERY (keytomap:'merchantId', uniquekey:'price' ) OF MerchantHourlyAve;



END APPLICATION UpdatableCache;

Stop @APPNAME@_App;
undeploy application @APPNAME@_App;
drop application @APPNAME@_App cascade;

CREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;
CREATE SOURCE @APPNAME@_Src USING Global.OracleReader(
  FetchSize:'1',
  Username:'@SOURCE_USER@',
  Password:'@SOURCE_PASSWORD@',
  ConnectionURL:'@SOURCE_URL@',
  Tables:'@SOURCE_TABLES@',
  ConnectionRetryPolicy:'@AUTO_CONNECTION_RETRY@'
)
OUTPUT TO @APPNAME@_Stream;

CREATE TARGET @APPNAME@_SYS USING SysOut (
  name: '@APPNAME@_SYS' )
INPUT FROM @APPNAME@_Stream;

End APPLICATION @APPNAME@_App;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  directory:'@FEATURE-DIR@/logs/',
  filename:'RoundUPPosData',
  rolloverpolicy:'FileSizeRollingPolicy,filesize:2.5M,sequence:00'
)
format using DSVFormatter (

)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizeRoundUp_actual.log') input from TypedCSVStream;

end application DSV;

stop application recoveryTestAgent.CSV;
undeploy application recoveryTestAgent.CSV;
drop application recoveryTestAgent.CSV cascade;

create application CSV
RECOVERY 5 SECOND INTERVAL;

CREATE FLOW AgentFlow;
create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails-recovery.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStream;
END FLOW AgentFlow;

CREATE FLOW ServerFlow;
CREATE TYPE UserDataType
(
  UserId String KEY,
  UserName String
);

CREATE STREAM UserDataStream OF UserDataType PARTITION BY UserId;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  data[0],
        data[1]
FROM CsvStream;


CREATE WACTIONSTORE UserActivityInfo
CONTEXT OF UserDataType
EVENT TYPES ( UserDataType )
@PERSIST-TYPE@

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction
INSERT INTO UserActivityInfo
SELECT * FROM UserDataStream
LINK SOURCE EVENT;
END FLOW ServerFlow;

END APPLICATION CSV;
DEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;
START CSV;

CREATE STREAM PosDataStream OF PosData;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;


CREATE TARGET DumpFileStream USING LogWriter(
name:testOuput1,
filename:'@FEATURE-DIR@/logs/TQLwithinTQL-console-out-RT.log'
) INPUT FROM PosDataStream;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.OJet (
  PrimaryDatabasePassword: '@PRIMARY_PASSWORD@',
  ConnectionURL: '@DOWNSTREAM_URL@',
  PrimaryDatabaseUsername: '@PRIMARY_USER@',
  Password: '@DOWNSTREAM_PASSWORD@',
  DownstreamCaptureMode: 'REAL_TIME',
  DownstreamCapture: true,
  PrimaryDatabaseConnectionURL: '@PRIMARY_URL@',
  Tables: '@SOURCE_TABLES@',
  Username: '@DOWNSTREAM_USER@'
  )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET_SYS@ USING Global.SysOut (
  name: 'Out' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter (
  ConnectionURL: '@TARGET_URL@',
  Username: '@TARGET_USER@',
  Password: '@TARGET_PASSWORD@',
  CheckPointTable: 'CHKPOINT',
  CommitPolicy: 'EventCount:1',
  Tables: '@TARGET_TABLES@',
  BatchPolicy: 'EventCount:1' )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;
Create Source oracSource
 Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;

CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

CREATE APPLICATION FtoKaf RECOVERY 2 SECOND INTERVAL;

CREATE OR REPLACE SOURCE FIletoRead USING FileReader  (
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '@TEST-DATA-PATH@/Validate-Striim',
  skipbom: true,
  wildcard: 'FiletoRead.txt'
 )
 PARSE USING DSVParser  (
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: '-1',
  nocolumndelimiter: true,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: false,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 )
OUTPUT TO FtoK1 ;

CREATE  TYPE FtoK2_Type  ( seq java.lang.Integer
 );

CREATE STREAM FtoK2 OF FtoK2_Type;

CREATE OR REPLACE CQ GetData
INSERT INTO FtoK2
SELECT TO_INT(data[0]) as seq
FROM FtoK1;

CREATE OR REPLACE TARGET KafkatoWrite USING KafkaWriter VERSION '0.11.0' (
  KafkaConfigPropertySeparator: ';',
  Mode: 'Sync',
  adapterName: 'KafkaWriter',
  Topic: 'kafkaTopic7',
  brokerAddress: 'localhost:9092',
  KafkaConfigValueSeparator: '=',
  KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000'
 )
FORMAT USING DSVFormatter  (   nullvalue: 'NULL',
  standard: 'none',
  handler: 'com.webaction.proc.DSVFormatter',
  formatterName: 'DSVFormatter',
  usequotes: 'false',
  rowdelimiter: '\n',
  quotecharacter: '\"',
  header: 'false',
  columndelimiter: ','
 )
INPUT FROM FtoK2;

END APPLICATION FtoKaf;

STOP BuiltInTester.AnomalyBoundTest;
UNDEPLOY APPLICATION BuiltInTester.AnomalyBoundTest;
DROP APPLICATION BuiltInTester.AnomalyBoundTest CASCADE;

CREATE APPLICATION AnomalyBoundTest;

CREATE OR REPLACE TYPE s1_Type  ( mString java.lang.String , 
mDouble java.lang.Double , 
mInt java.lang.Integer , 
mShort java.lang.Short , 
mLong java.lang.Long , 
mFloat java.lang.Float  
 );

CREATE OR REPLACE STREAM s1 OF s1_Type;

CREATE  WINDOW anomalyWin OVER s1 KEEP 4 ROWS;

CREATE OR REPLACE TYPE S3_Type  ( val java.lang.Integer , 
upperInt java.lang.Double , 
lowerInt java.lang.Double , 
upperDouble java.lang.Double , 
lowerDouble java.lang.Double , 
upperFloat java.lang.Double , 
lowerFloat java.lang.Double , 
upperLong java.lang.Double , 
lowerLong java.lang.Double , 
upperShort java.lang.Double , 
lowerShort java.lang.Double  
 ) ;

CREATE WACTIONSTORE anomalyWS  CONTEXT OF S3_Type
EVENT TYPES(S3_Type )
@PERSIST-TYPE@


CREATE OR REPLACE STREAM S3 OF S3_Type;

CREATE OR REPLACE CQ wsCQ 
INSERT INTO anomalyWS
SELECT * FROM S3 s;
;

CREATE OR REPLACE CQ anomalyCQ 
INSERT INTO S3
SELECT last(mInt) as val, 
round_double(distributionUpperBound(mInt), 2) as upperInt,  round_double(distributionLowerBound(mInt), 2) as lowerInt, round_double(distributionUpperBound(mDouble), 2) as upperDouble,  round_double(distributionLowerBound(mDouble), 2) as lowerDouble, round_double(distributionUpperBound(mFloat), 2) as upperFloat, round_double(distributionLowerBound(mFloat), 2) as lowerFloat, round_double(distributionUpperBound(mLong), 2) as upperLong, round_double(distributionLowerBound(mLong), 2) as lowerLong, round_double(distributionUpperBound(mShort), 2) as upperShort, round_double(distributionLowerBound(mShort), 2) as lowerShort 
FROM anomalyWin a
;

CREATE OR REPLACE TYPE hpstream3_Type  ( hpTime org.joda.time.DateTime , 
mDouble java.lang.Double , 
mInt java.lang.Integer , 
mShort java.lang.Short , 
mLong java.lang.Long  
 );

CREATE OR REPLACE SOURCE housePowerCSV USING FileReader  ( 
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '@TEST-DATA-PATH@',
  skipbom: true,
  wildcard: 'anomalyBound.csv'
 ) 
 PARSE USING DSVParser  ( 
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: '-1',
  nocolumndelimiter: false,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: true,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 ) 
OUTPUT TO housePowerStream ;

CREATE OR REPLACE CQ HPparseCQ 
INSERT INTO s1
SELECT to_string(data[0]) as mString, to_double(data[0]) as mDouble,  to_int(data[0]) as mInt, to_short(data[0]) as mShort, to_long(data[0]) as mLong, to_float(data[0]) as mFloat 
FROM housePowerStream s;

CREATE OR REPLACE TYPE hpstream_Type  ( hpTime org.joda.time.DateTime , 
mDouble java.lang.Double , 
mInt java.lang.Integer , 
mShort java.lang.Short , 
mString java.lang.String  
 );

CREATE OR REPLACE TYPE hqType  ( hpTime org.joda.time.DateTime , 
m1 java.lang.Double , 
m2 java.lang.Double , 
m3 java.lang.Double , 
m4 java.lang.Double  
 );

CREATE OR REPLACE TYPE s2_Type  ( val java.lang.Integer , 
upper java.lang.Double , 
lower java.lang.Double , 
isAnomaly java.lang.Boolean  
 );

CREATE OR REPLACE TYPE hpStream2_Type  ( hpTime org.joda.time.DateTime , 
m1 java.lang.Double , 
m2 java.lang.Double , 
m3 java.lang.Double , 
m4 java.lang.Double  
 );

END APPLICATION AnomalyBoundTest;

STOP APPLICATION @appname@routerApp;
UNDEPLOY APPLICATION @appname@routerApp;
DROP APPLICATION @appname@routerApp CASCADE;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',
  acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

CREATE APPLICATION @appname@routerApp;

CREATE  SOURCE @appname@OraSource USING OracleReader  (
Username: 'qatest',
Password: 'qatest',
ConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',
Tables: 'QATEST.TGT_T%',
 FetchSize:'100'
)
OUTPUT TO @appname@MasterStream1;

-- CREATE STREAM @appname@ss1 OF Global.waevent persist using Global.DefaultKafkaProperties;
-- CREATE STREAM @appname@ss2 OF Global.waevent persist using Global.DefaultKafkaProperties;
-- CREATE STREAM @appname@ss3 OF Global.waevent persist using Global.DefaultKafkaProperties;

CREATE STREAM @appname@ss1 OF Global.waevent PERSIST USING KafkaPropset;
CREATE STREAM @appname@ss2 OF Global.waevent PERSIST USING KafkaPropset;
CREATE STREAM @appname@ss3 OF Global.waevent PERSIST USING KafkaPropset;

CREATE OR REPLACE ROUTER @appname@tablerouter1 INPUT FROM @appname@MasterStream1 s CASE
WHEN meta(s,"TableName").toString()='QATEST.TGT_T1' THEN ROUTE TO @appname@ss1,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T2' THEN ROUTE TO @appname@ss2,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T3' THEN ROUTE TO @appname@ss3,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T4' THEN ROUTE TO @appname@ss4,
ELSE ROUTE TO @appname@ss_else;

create Target @appname@FileTarget_1 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'
)
FORMAT USING dsvFormatter ()
input from @appname@ss1;

create Target @appname@FileTarget_2 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'
)
FORMAT USING dsvFormatter ()
input from @appname@ss2;

create Target @appname@FileTarget_3 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from @appname@ss3;

create Target @appname@FileTarget_4 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from @appname@ss4;


create Target @appname@FileTarget_5 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from @appname@ss_else;


end application @appname@routerApp;
deploy application @appname@routerApp;
start @appname@routerApp;

stop application MSSQLTransactionSupportCompression;
undeploy application MSSQLTransactionSupportCompression;
drop application MSSQLTransactionSupportCompression cascade;

CREATE APPLICATION MSSQLTransactionSupportCompression recovery 1 second interval;

Create Source ReadFromMSSQL7
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
FetchTransactionMetadata:'true',
FilterTransactionBoundaries: true,
Compression:'true',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportCompressionStream;


CREATE TARGET WriteToMySQL7 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC@,@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportCompressionStream;

CREATE TARGET MSSqlReaderOutput7 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportCompressionStream; 


CREATE OR REPLACE TARGET MSSQLFileOut7 USING FileWriter  ( 
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  directory:'@FEATURE-DIR@/logs/',
  filename: 'TransactionSupportMSSQLToMySQLCompressionOn.txt'
 ) 
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 ) 
INPUT FROM MSSQLTransactionSupportCompressionStream;

END APPLICATION MSSQLTransactionSupportCompression;
deploy application MSSQLTransactionSupportCompression;
start application MSSQLTransactionSupportCompression;

create application KinesisTest;
CREATE OR REPLACE SOURCE OS USING OracleReader (
  Username: 'miner',
  Password: 'miner',
  ConnectionURL: '192.168.1.113:1521:ORCL',
  TABLES: 'QATEST.H_REGION;QATEST.H_NATION;QATEST.H_CUSTOMER',
  FetchSize: '1'
 )
OUTPUT TO DDLCDCStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	accesskeyid:'ACCESS_KEY',
	secretaccesskey:'SECRET_KEY'
)
format using JSONFormatter (
)
input from DDLCDCStream;

end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE MongoDB_Source USING Global.MongoDBReader(
collections: '@table@',
  ConnectionURL: '@connectionUrl@',
  mode: 'Incremental',
  Username: '@userName@',
  Password: '@Password@' )
Output To mongoDBRaw_Stream;

CREATE CQ JSON2StriimType
INSERT INTO jsonNodeEventStream
SELECT data.get("_id").toString() AS ID,
  data.get("firstname").toString() AS FirstName,
  data.get("lastname").toString() AS LastName,
  data.get("age").toString() AS Age
FROM mongoDBRaw_Stream;

CREATE TARGET MongoJson_FileDSV_Target USING Global.FileWriter ( 
  flushpolicy: 'EventCount:10000,Interval:30s',  
  directory: '@logs@',
  filename: '@BuiltinFunc@_Data', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING Global.DSVFormatter  ( 
   ) 
INPUT FROM jsonNodeEventStream;

End Application @AppName@;
Deploy application @AppName@;
Start Application @AppName@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@ (a String, b String ,c String,g String)

SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]) where TO_String(data[0]) > '1' ;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

-- The PosAppAgent sample application demonstrates how a credit card
-- payment processor might use Striim to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.


CREATE APPLICATION PosAppAgent;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosAppAgent application.

-- source CsvAgentDataSource

CREATE FLOW AgentFlow;

CREATE source CsvAgentDataSource USING FileReader (
  directory:'/opt/striim/Samples/PosApp/appData',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvAgentStream;

END FLOW AgentFlow;

-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells Striim that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells Striim to start reading
-- from the beginning of the file. (In a real-world application 
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvAgentStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvAgentToPosDataCq

CREATE FLOW ProcessFlow;

CREATE CQ CsvAgentToPosDataCq
INSERT INTO PosDataAgentStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvAgentStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP.
--
-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as 
-- the types to be used in PosDataAgentStream, which is created automatically.

-- The DATETIME field from the source is converted to both a dateTime 
-- value, used as the event timestamp by the application, and (via the 
-- function) an integer hourValue, which is used to look up 
-- historical hourly averages from the HourlyAgentAveLookup cache, 
-- discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)
-- 17 (hourValue)
-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateAgentOnly
--
-- The AgentPosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAgentAveLookup cache. (Aggregate functions cannot be used and 
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAgentAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW AgentPosData5Minutes
OVER PosDataAgentStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantAgentHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAgentAveLookup using FileReader (
  directory: 'Samples/PosApp/appData',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'merchantId') OF MerchantAgentHourlyAve;

CREATE TYPE MerchantTxRateAgent(
  merchantId String KEY,
  zip String,
  startTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateAgentOnlyStream OF MerchantTxRateAgent PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateAgentOnly
INSERT INTO MerchantTxRateAgentOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM AgentPosData5Minutes p, HourlyAgentAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAgentAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateAgentWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE STREAM MerchantTxRateAgentWithStatusStream OF MerchantTxRateAgent;

CREATE CQ GenerateMerchantTxRateAgentWithStatus
INSERT INTO MerchantTxRateAgentWithStatusStream
SELECT merchantId,
       zip,
       startTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateAgentOnlyStream;


-- WAction store MerchantActivityAgent
--
-- The following group of statements create and populate the MerchantActivityAgent
-- WAction store. Data from the MerchantTxRateAgentWithStatusStream is enhanced
-- with merchant details from NameLookupAgent cache and with latitude and longitude
-- values from the USAddressDataAgent cache.

CREATE TYPE MerchantActivityAgentContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivityAgent CONTEXT OF MerchantActivityAgentContext
EVENT TYPES ( MerchantTxRateAgent )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );


CREATE TYPE MerchantAgentNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressDataAgent(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookupAgent using FileReader (
  directory:'Samples/PosApp/appData',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false,
  trimwhitespace:true
) 
QUERY(keytomap:'merchantId') OF MerchantAgentNameData;

CREATE CACHE ZipLookupAgent using FileReader (
  directory: 'Samples/PosApp/appData',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'zip') OF USAddressDataAgent;


CREATE CQ GenerateWactionAgentContext
INSERT INTO MerchantActivityAgent
SELECT  m.merchantId,
        m.startTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateAgentWithStatusStream m, NameLookupAgent n, ZipLookupAgent z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

-- CQ GenerateAgentAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertAgentStream OF Global.AlertEvent;

CREATE CQ GenerateAgentAlerts
INSERT INTO AlertAgentStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateAgentWithStatusStream m, NameLookupAgent n
WHERE m.merchantId = n.merchantId;

CREATE SUBSCRIPTION AgentAlertSub USING WebAlertAdapter( ) INPUT FROM AlertAgentStream;

END FLOW ProcessFlow;

END APPLICATION PosAppAgent;

DEPLOY APPLICATION PosAppAgent with AgentFlow in AGENTS, ProcessFlow in default;

-- CREATE DASHBOARD USING "Samples/PosApp/PosAppDashboard.json";

STOP application consoletest.noApp;
undeploy application consoletest.noApp;
drop application consoletest.noApp cascade;

create application noApp;
create source CSVSource using FileReader (
  directory:'Wrong/Dir/Path',
  WildCard:'posdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'Foo',
        sequence:'00',
  rolloverpolicy:'eventcount:200'
)
format using DSVFormatter (

)
input from TypedCSVStream;
end application noApp;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using S3Writer(
    bucketname:'@BUCKET@',
   objectname:'upgradeData.csv',
   foldername:'upgradefolder',
  uploadpolicy:'EventCount : 10000,Interval :1m '
)
format using DSVFormatter (
)
input from @STREAM@;

end application @APPNAME@;

drop application ConsoleApplication cascade;
create application ConsoleApplication;

stop application APP_KAFKASOURCE_AGG;
undeploy application APP_KAFKASOURCE_AGG;
alter application APP_KAFKASOURCE_AGG;

CREATE OR REPLACE CQ CQ_CALCULATE_HOURLY_TOTAL
INSERT INTO STREAM_CQ_CALCULATE_HOURLY_TOTAL
SELECT f.topic as topic, sum(f.rawdatacount) as TotalLast24hour, B.rawdatacount as TotalLast1hour FROM JUMP_WND_1EVT_1MIN h
       join SLIDE_WND_HOURLYTOTALS_KAFKADATA_FILE f on 1=1
       join (SELECT rawdatacount, topic,timerange from ET_HOURLYTOTALS_KAFKADATA_FILE,JUMP_WND_1EVT_30SEC where timerange = DHOURS(DNOW())-1) B on B.topic=f.topic
       Group by f.topic;

alter application APP_KAFKASOURCE_AGG recompile;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes1',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource2 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes2',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource3 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',
  Tables: 'QATEST.OracToCql_alldatatypes3',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget2 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget3 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:10,Interval:60',
  CommitPolicy: 'EventCount:10,Interval:60',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget4 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:11,Interval:120',
  CommitPolicy: 'EventCount:11,Interval:120',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget5 USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:3,Interval:120',
  CommitPolicy: 'EventCount:3,Interval:120',
  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',
  Tables: '@Tables@',
  IgnorableExceptionCode: 'PRIMARY KEY',
  ExcludedTables:'QATEST.ORACTOCQL_ALLDATATYPES',
  Password: '+hbb060plSWQwscvI105cg==',
  Password_encrypted: true
 )
INPUT FROM Oracle_ChangeDataStream;

CREATE TARGET FWTarget USING FileWriter(
	name:CassandraOuput,
	filename:'OracToFw.log',
	flushpolicy : 'interval:120,eventcount:3',
	rolloverpolicy : 'interval:300s'
)
FORMAT USING DSVFormatter()
INPUT FROM Oracle_ChangeDataStream;
create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;


END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

CREATE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src1 USING Global.GCSReader (
  _h_GCSQueryCoolingTime: '1',
  ServiceAccountKey: '',
  ProjectId: '',
  DownloadPolicy: '',
  BucketName: '',
  compressiontype: '',
  PollingInterval: ,
  FolderName: '',
  UseStreaming: false,
  StartTimestamp: '',
  ObjectFilter: '',
  ObjectDetectionMode: '',
  IncludeSubfolders: false )
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream1;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (
  objectname: '',
  foldername: '',
  bucketname: '',
  uploadpolicy: '' )
FORMAT USING DSVFormatter (
members:'data')
INPUT FROM @APPNAME@_Stream1;

CREATE OR REPLACE SOURCE @APPNAME@_src2 USING Global.GCSReader (
  _h_GCSQueryCoolingTime: '1',
  ServiceAccountKey: '',
  ProjectId: '',
  DownloadPolicy: '',
  BucketName: '',
  compressiontype: '',
  PollingInterval: ,
  FolderName: '',
  UseStreaming: false,
  StartTimestamp: '',
  ObjectFilter: '',
  ObjectDetectionMode: '',
  IncludeSubfolders: false )
PARSE USING Global.JSONParser ()
OUTPUT TO @APPNAME@_Stream2;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (
  objectname: '',
  foldername: '',
  bucketname: '',
  uploadpolicy: '' )
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_Stream2;

CREATE OR REPLACE SOURCE @APPNAME@_src3 USING GCSReader (
  _h_GCSQueryCoolingTime: '1',
  ServiceAccountKey: '',
  ProjectId: '',
  DownloadPolicy: '',
  BucketName: '',
  compressiontype: '',
  PollingInterval: ,
  FolderName: '',
  UseStreaming: false,
  StartTimestamp: '',
  ObjectFilter: '',
  ObjectDetectionMode: '',
  IncludeSubfolders: false )
PARSE USING AvroParser ()
OUTPUT TO @APPNAME@_Stream3;

CREATE CQ @APPNAME@_CQ3
INSERT INTO @APPNAME@_CQOut3
SELECT AvroToJson(data,false) FROM @APPNAME@_Stream3;

CREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (
  objectname: '',
  foldername: '',
  bucketname: '',
  uploadpolicy: '' )
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_CQOut3;

CREATE OR REPLACE SOURCE @APPNAME@_src4 USING Global.GCSReader (
  _h_GCSQueryCoolingTime: '1',
  ServiceAccountKey: '',
  ProjectId: '',
  DownloadPolicy: '',
  BucketName: '',
  compressiontype: '',
  PollingInterval: ,
  FolderName: '',
  UseStreaming: false,
  StartTimestamp: '',
  ObjectFilter: '',
  ObjectDetectionMode: '',
  IncludeSubfolders: false )
PARSE USING Global.XMLParser (
  rootnode: ''
)
OUTPUT TO @APPNAME@_Stream4;

CREATE OR REPLACE TARGET @APPNAME@_trgt4 USING S3Writer (
  objectname: '',
  foldername: '',
  bucketname: '',
  uploadpolicy: '' )
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_Stream4;

END APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( 
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: '@CDC_URL@',
  Tables: '@Source1Tables@',
  FetchSize: 1) 
OUTPUT TO @APPNAME@cdcStream;

create Target @TARGET_NAME@ using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (

)
input from @STREAM@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING HL7v2Parser (
  EnableMessageValidation: false,
  MLLPDelimited: false
  )
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING XMLFormatter (
    rootelement:"document",
    charset: "UTF-8"
    )
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING SnowflakeWriter (
  ConnectionURL: '@SNOWFLAKE-URL@',
  username: '@SNOWFLAKE-USERNAME@',
  password: '@SNOWFLAKE-PASSWORD@',
  appendOnly:'false',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  Tables: '@TARGET-TABLES@',
  uploadpolicy:'eventcount:10,interval:5s'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING SnowflakeWriter (
  ConnectionURL: '@SNOWFLAKE-URL@',
  username: '@SNOWFLAKE-USERNAME@',
  password: '@SNOWFLAKE-PASSWORD@',
  appendOnly:'false',
  BatchPolicy: 'Interval:10',
  CommitPolicy: 'Interval:10',
  Tables: '@TARGET-TABLES@',
  uploadpolicy:'eventcount:10,interval:5s'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING SnowflakeWriter (
  ConnectionURL: '@SNOWFLAKE-URL@',
  username: '@SNOWFLAKE-USERNAME@',
  password: '@SNOWFLAKE-PASSWORD@',
  appendOnly:'false',
  BatchPolicy: 'eventCount:100000,Interval:20',
  CommitPolicy: 'eventCount:100000,Interval:20',
  Tables: '@TARGET-TABLES@',
  uploadpolicy:'eventcount:10,interval:5s'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING SnowflakeWriter (
  ConnectionURL: '@SNOWFLAKE-URL@',
  username: '@SNOWFLAKE-USERNAME@',
  password: '@SNOWFLAKE-PASSWORD@',
  appendOnly:'false',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  Tables: '@TARGET-TABLES@',
  uploadpolicy:'eventcount:10,interval:5s'
 )
INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)
INPUT FROM @STREAM@;;

create Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;

STOP OuterJoinTester.OJApp;
UNDEPLOY APPLICATION OuterJoinTester.OJApp;
DROP APPLICATION OuterJoinTester.OJApp cascade;

CREATE APPLICATION OJApp;


CREATE TYPE bankData
(
ucID Integer,
ucLong long,
ucDate DateTime,
ucDouble Double KEY
);


CREATE STREAM wsStream OF bankData;

CREATE CACHE cache1 USING CsvReader(
directory: '@TEST-DATA-PATH@',
wildcard: 'ucData.csv',
header: No,
columndelimiter: ',',
trimquote: false
) QUERY (keytomap:'ucDouble') OF bankData;



END APPLICATION OJApp;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

CREATE  SOURCE @QuerySource@1 USING DatabaseReader  (
  ReplicationSlotName:'@slotname1@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@1;
create Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;
 CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@1;

CREATE  SOURCE @QuerySource@2 USING DatabaseReader  (
  ReplicationSlotName:'@slotname2@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@2;
create Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;
 CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@2;

CREATE  SOURCE @QuerySource@3 USING DatabaseReader  (
  ReplicationSlotName:'@slotname3@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@3;
create Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;
 CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@3;

CREATE  SOURCE @QuerySource@4 USING DatabaseReader  (
  ReplicationSlotName:'@slotname4@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@4;
create Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;
 CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@4;

CREATE  SOURCE @QuerySource@5 USING DatabaseReader  (
  ReplicationSlotName:'@slotname5@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@5;
create Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;
 CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@5;

CREATE  SOURCE @QuerySource@6 USING DatabaseReader  (
  ReplicationSlotName:'@slotname6@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@6;
create Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;
 CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@6;

CREATE  SOURCE @QuerySource@7 USING DatabaseReader  (
  ReplicationSlotName:'@slotname7@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@7;
create Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;
 CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@7;

CREATE  SOURCE @QuerySource@8 USING DatabaseReader  (
  ReplicationSlotName:'@slotname8@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@8;
create Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;
 CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@8;

CREATE  SOURCE @QuerySource@9 USING DatabaseReader  (
  ReplicationSlotName:'@slotname9@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@9;
create Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;
 CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@9;

CREATE  SOURCE @QuerySource@10 USING DatabaseReader  (
  ReplicationSlotName:'@slotname10@',
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@10;
create Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;
 CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  Username:'waction',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,public.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@10;
end application @APPNAME@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'JsonTargetEC',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'eventcount:2000,sequence:00'
)
format using JSONFormatter (
  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)

input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetJsonECBig_actual.log') input from TypedCSVStream;

end application DSV;

import com.webaction.proc.events.StringArrayEvent;
import com.webaction.proc.events.WAEvent;

STOP FunctionTester.BuiltInFunctionApp;
UNDEPLOY APPLICATION FunctionTester.BuiltInFunctionApp;
DROP APPLICATION FunctionTester.BuiltInFunctionApp cascade;

CREATE APPLICATION BuiltInFunctionApp;


CREATE source rawSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'csvdata.txt',
  positionByEOF:false
)
PARSE USING DSVParser
(
   header:False,
   columndelimiter:','
) OUTPUT TO rawStream;


CREATE TYPE rawDataType(
  level String,
  floatnumber float,
  doublenumber double,
  intvalue  int,
  ourEvent  WAEvent
);


CREATE STREAM testStream OF rawDataType ;


-- Create a WAEvent out of the first to items, using makeWAEvent makeWAEvent(data[0], data[1])
CREATE CQ testCQ
  INSERT INTO testStream
  SELECT data[0], ROUND_FLOAT(data[1],4), ROUND_DOUBLE(data[2],0), TO_INT(data[3]), makeWAEvent(data[0], data[1])
FROM rawStream;

CREATE OR REPLACE Target BuiltFuncTarget using LogWriter(name:BuiltFunc,filename:'@FEATURE-DIR@/logs/BuiltInFuncResults') input from testStream;


CREATE TYPE waEventDataType
(
  level String,
  floatnumber float
);

CREATE STREAM waEventStream OF waEventDataType;

-- Parse the WAEvent created by the CQ testCQ and store the values
CREATE CQ otherCQ
  INSERT INTO waEventStream
  SELECT TO_STRING(VALUE(ourEvent,0)), ROUND_FLOAT(VALUE(ourEvent,1),4)
FROM testStream;


CREATE OR REPLACE Target BuiltFuncWAEventTarget using LogWriter(name:BuiltFunc,filename:'@FEATURE-DIR@/logs/BuiltInFuncResultsWAEvent') input from waEventStream;



CREATE TYPE stringArrayEventDataType
(
  level String,
  floatnumber float,
  doublenumber double,
  intvalue  int,
  stringArrayEvent StringArrayEvent
);

CREATE STREAM stringArrayEventStream OF stringArrayEventDataType;

-- Create a StringArrayEvent from the raw stream
-- We can't use the target below to verify, as it's generating a timestamp field, based on system generated time
CREATE CQ stringArrayEventCQ
  INSERT INTO stringArrayEventStream
  SELECT data[0], ROUND_FLOAT(data[1],4), ROUND_DOUBLE(data[2],0), TO_INT(data[3]), makeStringArrayEvent(data[0], data[2], data[3])
FROM rawStream;


CREATE OR REPLACE Target BuiltFuncSimpleEventTarget using LogWriter(name:BuiltFunc,filename:'@FEATURE-DIR@/logs/BuiltInFuncResultsStringArrayEvent') input from stringArrayEventStream;


END APPLICATION BuiltInFunctionApp;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

create application HTTPtest;
create source DSVCSVSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'customerdetails.csv',
	charset: 'UTF-8',
	positionByEOF:false
)
parse using DSVParser (
	header:'no'
)
OUTPUT TO DSVCsvStream;
create Target DSVDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/customerdetails') input from DSVCsvStream;
end application HTTPtest;

stop Quiesce_IL;
undeploy application Quiesce_IL;
drop application Quiesce_IL cascade;
CREATE APPLICATION Quiesce_IL USE EXCEPTIONSTORE TTL : '7d' ;
CREATE FLOW Quiesce_IL_flow;
Create Source Quiesce_IL_Oraclesrc Using databasereader(
 Username:'@USERNAME@',
 Password:'@PASSWORD@',
 ConnectionURL:'@CONNECTION_URL@',
 Tables:'QATEST.QUIESCE_TABLE1;QATEST.QUIESCE_TABLE2',
 QuiesceOnILCompletion: 'true',
 _h_fetchexactrowcount: 'true'
)
Output To Quiesce_IL_OrcStrm;
END FLOW Quiesce_IL_flow;

CREATE TARGET Quiesce_IL_BigQueryTrg USING BigQueryWriter (
  serviceAccountKey: '@SERVICEACCOUNTKEY@',
  projectId:'@PROJECTID@',
  Tables:'QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE1;QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE2'
)

INPUT FROM Quiesce_IL_OrcStrm;
END APPLICATION Quiesce_IL;
DEPLOY APPLICATION Quiesce_IL;
start application Quiesce_IL;

stop Quiesce_CDC;
undeploy application Quiesce_CDC;
drop application Quiesce_CDC cascade;
CREATE APPLICATION Quiesce_CDC recovery 1 second interval USE EXCEPTIONSTORE TTL : '7d' ;
CREATE stream Quiesce_CDC_OrcStrm of global.waevent persist using Global.DefaultKafkaProperties;
CREATE FLOW Quiesce_CDC_flow;
Create Source Quiesce_CDC_Oraclesrc Using oraclereader(
 Username:'@USERNAME@',
 Password:'@PASSWORD@',
 ConnectionURL:'@CONNECTION_URL@',
 Tables:'QATEST.QUIESCE_TABLE1;QATEST.QUIESCE_TABLE2',
 _h_fetchexactrowcount: 'true'
)
Output To Quiesce_CDC_OrcStrm;
END FLOW Quiesce_CDC_flow;
END APPLICATION Quiesce_CDC;
DEPLOY APPLICATION Quiesce_CDC;
start application Quiesce_CDC;

stop Quiesce_CDC_BQ_TARGET;
undeploy application Quiesce_CDC_BQ_TARGET;
drop application Quiesce_CDC_BQ_TARGET cascade;
CREATE APPLICATION Quiesce_CDC_BQ_TARGET recovery 1 second interval USE EXCEPTIONSTORE TTL : '7d' ;
CREATE TARGET Quiesce_CDC_BigQueryTrg USING BigQueryWriter (
  serviceAccountKey: '@SERVICEACCOUNTKEY@',
  projectId:'@PROJECTID@',
  BatchPolicy:'Interval:10',
  _h_maxParallelStreamingRequests: '10',
  Tables:'QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE1;QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE2'
)
INPUT FROM Quiesce_CDC_OrcStrm;
END APPLICATION Quiesce_CDC_BQ_TARGET;
DEPLOY APPLICATION Quiesce_CDC_BQ_TARGET;
start application Quiesce_CDC_BQ_TARGET;

use RetailTester;
alter application RetailApp;

CREATE FLOW RetailSourceFlow;

-- RetailDataSource is the primary data source for this application.
--
-- ParseOrderData discards the fields not needed by this application and puts the
-- data into the appropriate Java types.
--
-- ParseOrderData outputs to RetailOrders stream, the start point of the
-- RetailProductFlow and RetailStoreFlow flows.

CREATE SOURCE RetailDataSource USING CSVReader (
  directory:'Samples/Customer/RetailApp/appData',
  header:Yes,
  wildcard:'retaildata2M.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO Orders;

-- A stream's type must be declared before the stream, and a CQ's
-- output stream must be defined before the CQ. Hence type-stream-CQ
-- sequences like the following are very common.

-- output for ParseOrderData
CREATE TYPE OrderType(
  storeId      String,
  orderId      String,
  sku          String,
  orderAmount  double,
  dateTime     DateTime,
  hourValue    int,
  state        String,
  city         String,
  zip          String
);
CREATE STREAM RetailOrders Of OrderType;

CREATE CQ ParseOrderData
INSERT INTO RetailOrders
SELECT  data[0],
        data[6],
        data[7],
        TO_DOUBLE(SRIGHT(data[8],1)),
        TO_DATE(data[9],'yyyyMMddHHmmss'),
        DHOURS(TO_DATE(data[9],'yyyyMMddHHmmss')),
        data[3],
        data[2],
        data[4]
FROM Orders;

END FLOW RetailSourceFlow;

end application RetailApp;

alter application RetailApp recompile;

stop application @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;

CREATE OR REPLACE APPLICATION @AppName@;

CREATE SOURCE @AppName@_MssqlSource USING MSSqlReader
(
   Username:'@userName@',
  Tables: '@tableName@', 
  Password:'@password@',
 DatabaseName:'qatest',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
 AutoDisableTableCDC:false,
 connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5'
)
OUTPUT TO @AppName@_Stream;

CREATE OR REPLACE CQ @AppName@_Cq
INSERT INTO InputData
select userdata, metadata,
CASE WHEN meta(h,'OperationName').toString() = 'DELETE' THEN data(h) ELSE before(h) END AS before , 
CASE WHEN meta(h,'OperationName').toString() = 'DELETE' THEN NULL ELSE data(h) END AS data,
CASE WHEN meta(h,'OperationName').toString() = 'UPDATE' THEN data(h) ELSE before(h) END AS before ,
CASE WHEN meta(h,'OperationName').toString() = 'INSERT' THEN data(h) ELSE before(h) END AS before
from  @AppName@_Stream  h where meta(h,'TableName').tostring() = 'qatest.MetaDataTable';;


CREATE OR REPLACE TARGET @AppName@_FileWriterTarget USING Global.FileWriter ( 
  flushpolicy: 'EventCount:10000,Interval:30s', 
  adapterName: 'FileWriter', 
  directory: '@logsDir@',
  filename: '@FileName@', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING Global.JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter', 
  jsonMemberDelimiter: '\n', 
  EventsAsArrayOfJsonObjects: 'true', 
  formatterName: 'JSONFormatter', 
  jsonobjectdelimiter: '\n' ) 
INPUT FROM InputData;

END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

--
-- Kafka Stream with KryoParser and Kafka Reader Recovery Test 1
-- Bert Hashemi and Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> KS -> WS
-- S -> K -> CQ -> WS

STOP KStreamKryoParser1Tester.KStreamKryoParserTest1;
UNDEPLOY APPLICATION KStreamKryoParser1Tester.KStreamKryoParserTest1;
DROP APPLICATION KStreamKryoParser1Tester.KStreamKryoParserTest1 CASCADE;
DROP USER KStreamKryoParser1Tester;
DROP NAMESPACE KStreamKryoParser1Tester CASCADE;
CREATE USER KStreamKryoParser1Tester IDENTIFIED BY KStreamKryoParser1Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamKryoParser1Tester;
CONNECT KStreamKryoParser1Tester KStreamKryoParser1Tester;

CREATE APPLICATION KStreamKryoParserTest1 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'1');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE or REPLACE TYPE KafkaCsvStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF KafkaCsvStreamType 
EVENT TYPES ( KafkaCsvStreamType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE or REPLACE TYPE KafkaStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

--CREATE STREAM KafkaTypedStream OF KafkaStreamType;

CREATE STREAM KafkaStream OF Global.waevent;

CREATE SOURCE KafkaSource USING KafkaReader Version '0.8.0'
(
        brokerAddress:'localhost:9092',
        Topic:'KStreamKryoParser1Tester_KafkaCsvStream',
        PartitionIDList:'0',
        startOffset:0
)
PARSE USING StriimParser ()
OUTPUT TO KafkaStream;

CREATE WACTIONSTORE KRWactions CONTEXT OF KafkaStreamType
EVENT TYPES ( KafkaStreamType )
@PERSIST-TYPE@

CREATE CQ KRInsertWactions
INSERT INTO KRWactions
SELECT TO_STRING(data[1]) as merchantId,
    TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
    TO_DOUBLE(data[7]) as amount,
    TO_STRING(data[10]) as city 
FROM KafkaStream;

/*
CREATE CQ CQ2KafkaTypedStream
INSERT INTO KafkaTypedStream
SELECT TO_STRING(data[1]) as merchantId,
    TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
    TO_DOUBLE(data[7]) as amount,
    TO_STRING(data[10]) as city 
FROM KafkaStream;
*/

END APPLICATION KStreamKryoParserTest1;

create flow AgentFlow;

CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (
  StartPosition: '@startPosition@',
  ConnectionURL: '@sourceURL@',
  Username: '@userName@',
  Tables: '@tables@',
  CheckColumn: '@checkColum@',
  FetchSize: 1,
  Password: '@password@',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

end flow AgentFlow;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;
CREATE  SOURCE @SourceName@ USING MySqlReader  ( 
  Username: '@Username@',
  Password: '@Password@',
  DatabaseName: 'qatest',
  connectionRetryPolicy: @ConnectionRetryPolicy@,
  ConnectionURL: '@ConnectionURL@',
  Tables: '@SourceTables@',
  ConnectionPoolSize: 1,
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @SRCINPUTSTREAM@;
create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (
  StartPosition: '@startPosition@',
  ConnectionURL: '@sourceURL@',
  Username: '@userName@',
  Tables: '@tables@',
  CheckColumn: '@checkColum@',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  Password: '@password@',
  Password_encrypted: 'false',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

CREATE APPLICATION SourceRetailApp;

CREATE source RetailDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:no,
  wildcard:'retaildata2M.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO RetailOrders;

CREATE TARGET RetailSourceDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceRetailAppData') input from RetailOrders;

CREATE Source RetailHourlyStoreSales using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'storehourlyData.txt',
  header: no,
  columndelimiter: ',',
  positionByEOF:false
) OUTPUT TO RetailCacheSource1;

CREATE TARGET RetailCacheDump1 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceRetailCacheData1') input from RetailCacheSource1;

CREATE Source RetailStoreNameLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'StoreNames.csv',
  header: no,
  columndelimiter: ',',
  positionByEOF:false
) OUTPUT TO RetailCacheSource2;

CREATE TARGET RetailCacheDump2 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceRetailCacheData2') input from RetailCacheSource2;

CREATE Source RetailZipCodeLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  header: no,
  columndelimiter: '	',
  positionByEOF:false
) OUTPUT TO  RetailCacheSource3;

CREATE TARGET RetailCacheDump3 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceRetailCacheData3') input from RetailCacheSource3;


END APPLICATION SourceRetailApp;

CREATE OR REPLACE APPLICATION @AppName@;

Create Source @AppName@_source Using OracleReader(
  Username:'@username@',
  Password:'@password@',
  ConnectionURL:'@url@',
  Tables:'@srctableName@',
  Fetchsize:1
)
Output To @AppName@_Stream;

CREATE OR REPLACE TARGET @AppName@_DB_Target USING Global.DeltaLakeWriter (
connectionProfileName: 'admin.@DBCP@',
   useConnectionProfile: 'true',
  Tables: '@srctableName@,@trgtableName@',
  uploadPolicy: 'eventcount:100000,interval:60s'
)

INPUT FROM @AppName@_Stream;

CREATE OR REPLACE TARGET @AppName@_DB_Target2 USING Global.DeltaLakeWriter (
connectionProfileName: 'admin.@DBCP@',
   useConnectionProfile: 'true',
  Tables: '@srctableName@,@trgtableName@',
  uploadPolicy: 'eventcount:100000,interval:60s'
)

INPUT FROM @AppName@_Stream;

END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  IgnorableExceptionCode: '1,TABLE_NOT_FOUND'
 )
INPUT FROM @STREAM@;

--
-- Recovery Test 26 with two sources, two jumping attribute windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W -> CQ1 -> WS
-- S2 -> Ja6W -> CQ2 -> WS
--

STOP KStreamRecov26Tester.KStreamRecovTest26;
UNDEPLOY APPLICATION KStreamRecov26Tester.KStreamRecovTest26;
DROP APPLICATION KStreamRecov26Tester.KStreamRecovTest26 CASCADE;
DROP USER KStreamRecov26Tester;
DROP NAMESPACE KStreamRecov26Tester CASCADE;
CREATE USER KStreamRecov26Tester IDENTIFIED BY KStreamRecov26Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov26Tester;
CONNECT KStreamRecov26Tester KStreamRecov26Tester;

CREATE APPLICATION KStreamRecovTest26 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION KStreamRecovTest26;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src1 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;


CREATE OR REPLACE TARGET @APP_NAME@_tgt1 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt2 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt3 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt4 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt5 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

--
-- Recovery Test 23 with two sources, two sliding time windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> St1W -> CQ1 -> WS
-- S2 -> St2W -> CQ2 -> WS
--

STOP KStreamRecov23Tester.KStreamRecovTest23;
UNDEPLOY APPLICATION KStreamRecov23Tester.KStreamRecovTest23;
DROP APPLICATION KStreamRecov23Tester.KStreamRecovTest23 CASCADE;
DROP USER KStreamRecov23Tester;
DROP NAMESPACE KStreamRecov23Tester CASCADE;
CREATE USER KStreamRecov23Tester IDENTIFIED BY KStreamRecov23Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov23Tester;
CONNECT KStreamRecov23Tester KStreamRecov23Tester;

CREATE APPLICATION KStreamRecovTest23 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 1 SECOND;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 2 SECOND;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION KStreamRecovTest23;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE OR REPLACE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;

CREATE OR REPLACE SOURCE @SOURCE@ USING SalesForceReader (
  autoAuthTokenRenewal: 'true',
  Username: '@userName@',
  securityToken: '@securityToken@',
  sObjects: '@SourceObj@',
  pollingInterval: '1 min',
  Password_encrypted: 'false',
  securityToken_encrypted: 'false',
  customObjects: 'False',
  consumerKey: '@consumerKey@',
  startTimestamp: '',
  apiEndPoint: 'https://ap2.salesforce.com',
  mode: 'Incremental',
  consumerSecret: '@consumerSecert@',
  consumerSecret_encrypted: 'false',
  Password: '@Password@' )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@ USING SnowflakeWriter (
  connectionUrl: '@tgtConnectionUrl@',
  optimizedMerge: 'true',
  password: '@TgtPassword@',
  username: '@TgtUserName@',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10,interval:1m',
  tables: '@SourceObj@,@TargetTableName@ columnmap(ID=ID,checkbool__c=checkbool__c,dt__c=dt__c,percnt__c=percnt__c,phn__c=phn__c,txtlong__c=txtlong__c,url1__c=url1__c)'
 )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

Stop @APPNAME@_App;
undeploy application @APPNAME@_App;
drop application @APPNAME@_App cascade;

CREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;
CREATE SOURCE @APPNAME@_Src USING Global.OracleReader(
  FetchSize:'1',
  Username:'@SOURCE_USER@',
  Password:'@SOURCE_PASSWORD@',
  ConnectionURL:'@SOURCE_URL@',
  Tables:'@SOURCE_TABLES@',
  _h_useClassic:'true',
  ConnectionRetryPolicy:'@AUTO_CONNECTION_RETRY@'
)
OUTPUT TO @APPNAME@_Stream;

CREATE TARGET @APPNAME@_SYS USING SysOut (
  name: '@APPNAME@_SYS' )
INPUT FROM @APPNAME@_Stream;

End APPLICATION @APPNAME@_App;

--
-- Canon Test W80
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for a partitioned jumping attribute window
--
-- S -> JWa5p -> CQ -> WS
--


UNDEPLOY APPLICATION NameW80.W80;
DROP APPLICATION NameW80.W80 CASCADE;
CREATE APPLICATION W80 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW80;

CREATE SOURCE CsvSourceW80 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW80;

END FLOW DataAcquisitionW80;



CREATE FLOW DataProcessingW80;

CREATE TYPE DataTypeW80 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW80 OF DataTypeW80;

CREATE CQ CSVStreamW80_to_DataStreamW80
INSERT INTO DataStreamW80
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW80;

CREATE JUMPING WINDOW JWa5pW80
OVER DataStreamW80
KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY word;

CREATE WACTIONSTORE WactionStoreW80 CONTEXT OF DataTypeW80
EVENT TYPES ( DataTypeW80 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWa5pW80_to_WactionStoreW80
INSERT INTO WactionStoreW80
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWa5pW80
GROUP BY word;

END FLOW DataProcessingW80;



END APPLICATION W80;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade; 

create application @APPNAME@;

create or replace type @APPNAME@emp_type(
Sno integer,
Empname string,
Doj string,
Country string,
CompanyName string
);

CREATE OR REPLACE SOURCE @APPNAME@File_SOURCE1 using Filereader(
	directory:'@DIRECTORY@',
  wildcard:'File_empdata.csv',
  positionByEOF:false
)parse using dsvParser(
    header:'yes'
)
OUTPUT TO @APPNAME@File_Stream1,
OUTPUT TO @APPNAME@File_Stream1_automap MAP(filename:'File_empdata.csv');

CREATE OR REPLACE SOURCE @APPNAME@Init_Source1 USING DatabaseReader  ( 
  Username: '@SRC-USER@',
  Password_encrypted: false,
  ConnectionURL: '@SRC-URL@',
  Tables: 'QATEST.EMP_INIT',
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SRC-PASS@'
 ) 
OUTPUT TO 	@APPNAME@InitialLoad_Stream1;

CREATE OR REPLACE SOURCE @APPNAME@CDC_Source1 USING OracleReader  ( 
  StartTimestamp: 'null',
  SupportPDB: false,
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  FetchSize: 1,
  DDLCaptureMode: 'All',
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@SRC-URL@',
  Tables: 'QATEST.EMP_CDC',
  adapterName: 'OracleReader',
  Password: '@SRC-USER@',
  TransactionBufferType: 'Memory',
  DictionaryMode: 'OnlineCatalog',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SRC-USER@',
  TransactionBufferSpilloverSize: '1MB',
  compression: true,
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@CDC_Stream1 ;

create or replace stream @APPNAME@FileSource_cdc_init_TypedStream of @APPNAME@emp_type;
create or replace cq @APPNAME@file_typed_streamcq 
insert into @APPNAME@FileSource_cdc_init_TypedStream 
SELECT to_int(data[0]),
data[1],
data[2],
data[3],
data[4]
from @APPNAME@File_Stream1;

create or replace cq @APPNAME@cdc_typed_streamcq 
insert into @APPNAME@FileSource_cdc_init_TypedStream 
SELECT to_int(data[0]),
data[1],
data[2],
data[3],
data[4]
from @APPNAME@CDC_Stream1;

create or replace cq @APPNAME@init_typed_streamcq 
insert into @APPNAME@FileSource_cdc_init_TypedStream 
SELECT to_int(data[0]),
data[1],
data[2],
data[3],
data[4]
from @APPNAME@InitialLoad_Stream1;

CREATE OR REPLACE TARGET @APPNAME@sap_target1 USING DatabaseWriter  ( 
  DatabaseProviderType:'SAPHANA',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT-USER@',
  Password_encrypted: 'false',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TGT-URL@',
  Tables: 'QA.FILE_EMP',
  adapterName: 'DatabaseWriter',
  IgnorableExceptionCode: '301',
  Password: '@TGT-PASS@'
 ) 
INPUT FROM @APPNAME@File_Stream1_automap;

CREATE OR REPLACE TARGET @APPNAME@sap_target2 USING DatabaseWriter  ( 
  DatabaseProviderType:'SAPHANA',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT-USER@',
  Password_encrypted: 'false',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TGT-URL@',
  Tables: 'QATEST.EMP,QA.CDC_EMP',
  adapterName: 'DatabaseWriter',
  --IgnorableExceptionCode: '301',
  Password: '@TGT-PASS@'
 ) 
INPUT FROM @APPNAME@InitialLoad_Stream1;

CREATE OR REPLACE TARGET @APPNAME@sap_target3 USING DatabaseWriter  ( 
  DatabaseProviderType:'SAPHANA',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT-USER@',
  Password_encrypted: 'flase',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TGT-URL@',
  Tables: 'QATEST.EMP_INIT,QA.INITIALLOAD_EMP',
  adapterName: 'DatabaseWriter',
  --IgnorableExceptionCode: '301',
  Password: '@TGT-PASS@'
 ) 
INPUT FROM @APPNAME@CDC_Stream1;


CREATE OR REPLACE TARGET @APPNAME@sap_target4 USING DatabaseWriter  ( 
  DatabaseProviderType:'SAPHANA',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT-USER@',
  Password_encrypted: 'false',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TGT-URL@',
  Tables: 'QA.FILE_INIT_CDC_EMP',
  adapterName: 'DatabaseWriter',
  IgnorableExceptionCode: '301',
  Password: '@TGT-PASS@'
 ) 
INPUT FROM @APPNAME@FileSource_cdc_init_TypedStream;

create or replace target @APPNAME@sys_file_tgt using sysout(
name:'foo_file'
)input from @APPNAME@FileSource_Stream1;

create or replace target @APPNAME@sys_cdc_tgt using sysout(
name:'foo_cdc'
)input from @APPNAME@CDC_Stream1;

create or replace target @APPNAME@sys_init_tgt using sysout(
name:'foo_init'
)input from @APPNAME@InitialLoad_Stream1;

End Application @APPNAME@;


deploy application @APPNAME@;
start application @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 1 second interval;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@',
  CopybookFileFormat:'USE_COLS_6_TO_80'
  --recordSelector: '@PROP8@'
)
OUTPUT TO @APPNAME@Stream;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;

/*
create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:1,Interval:5',
  CommitPolicy: 'EventCount:1,Interval:5',
  Tables: 'QATEST.@table@'
)
input from @APPNAME@Stream;*/
end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop application @APPNAME@Apps2;
undeploy application @APPNAME@Apps2;
drop application @APPNAME@Apps2 cascade;


stop application @APPNAME@Apps3;
undeploy application @APPNAME@Apps3;
drop application @APPNAME@Apps3 cascade;



stop application @APPNAME@Apps4;
undeploy application @APPNAME@Apps4;
drop application @APPNAME@Apps4 cascade;



stop application @APPNAME@Apps1;
undeploy application @APPNAME@Apps1;
drop application @APPNAME@Apps1 cascade;

CREATE OR REPLACE PROPERTYSET @APPNAME@Apps_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9099', kafkaversion:'0.11');

CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream1 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;
CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream2 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;
CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream3 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;
CREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream4 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;


--**********************Application 1*******************
-- with 2 agent flow 
-- <agentflow1>source1->PS1<agentflow1>
-- <agentflow2>Source2->Inmemory1->cq1->PS2<agentflow2>

CREATE APPLICATION @APPNAME@Apps1 RECOVERY 5 SECOND INTERVAL;
create flow @APPNAME@agentflow;
CREATE OR REPLACE SOURCE @APPNAME@AppsDBSource1 USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@1',
  adapterName: 'OracleReader',
  Password: '@PASSWORD@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@USERNAME@',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@Apps_PS_Stream1;
end flow @APPNAME@agentflow;

create flow @APPNAME@agentflow2;
CREATE OR REPLACE SOURCE @APPNAME@AppsDBSource2 USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@2',
  adapterName: 'OracleReader',
  Password: '@PASSWORD@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@USERNAME@',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@Apps_PS_InmemoryStream1;

CREATE CQ @APPNAME@cq_Inmemory1
INSERT INTO @APPNAME@Apps_PS_Stream2
SELECT *
FROM @APPNAME@Apps_PS_InmemoryStream1;
end flow @APPNAME@agentflow2;

end application @APPNAME@Apps1;

-- ********************Application 2***********************
-- with 2 agent flow 
-- <agentflow3>source3->inmemory2<agentflow3>inmemory->cq2->ps3
-- <agentflow4>Source2->Inmemory3<agentflow4><serverflow1>inmemory->cq3->ps4<serverflow1>

CREATE APPLICATION @APPNAME@Apps2 RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@agentflow3;
CREATE OR REPLACE SOURCE @APPNAME@AppsDBSource3 USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@3',
  adapterName: 'OracleReader',
  Password: '@PASSWORD@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@USERNAME@',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@Apps_PS_InmemoryStream2;

CREATE CQ @APPNAME@cq_Inmemory2
INSERT INTO @APPNAME@Apps_PS_Stream3
SELECT *
FROM @APPNAME@Apps_PS_InmemoryStream2;

end flow @APPNAME@agentflow3;

create flow @APPNAME@agentflow4;
CREATE OR REPLACE SOURCE @APPNAME@AppsDBSource4 USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@4',
  adapterName: 'OracleReader',
  Password: '@PASSWORD@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@USERNAME@',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@Apps_PS_InmemoryStream3;

end flow @APPNAME@agentflow4;

create  flow @APPNAME@serverflow;
CREATE CQ @APPNAME@cq_Inmemory3
INSERT INTO @APPNAME@Apps_PS_Stream4
SELECT *
FROM @APPNAME@Apps_PS_InmemoryStream3;
end flow @APPNAME@serverflow;
END APPLICATION @APPNAME@Apps2;

-- ********************Application 3**************************
--WITH 3 TARGET FLOW
-- 1. <targetflow1> ps1->target1 <targetflow1>
-- 2. <targetflow2>ps2->target2 <targetflow2>
-- 3. <targetflow3> ps1,ps2->cq4 -> target3 <targetflow3>

CREATE APPLICATION @APPNAME@Apps3 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @APPNAME@TARGETFLOW1;
CREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget1 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@1,@TARGET_TABLE@1',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream1;
END FLOW @APPNAME@TARGETFLOW1;

CREATE FLOW @APPNAME@TARGETFLOW2;
CREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget2 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@2,@TARGET_TABLE@2',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream2;
END FLOW @APPNAME@TARGETFLOW2;

CREATE OR REPLACE STREAM @APPNAME@Apps_PS_InmemoryStream4 OF Global.waevent;

CREATE CQ @APPNAME@cq_ps1
INSERT INTO @APPNAME@Apps_PS_InmemoryStream4
SELECT *
FROM @APPNAME@Apps_PS_Stream1;

CREATE CQ @APPNAME@cq_ps2
INSERT INTO @APPNAME@Apps_PS_InmemoryStream4
SELECT *
FROM @APPNAME@Apps_PS_Stream2;

CREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget2 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@1,@TARGET_TABLE@5;@SOURCE_TABLE@2,@TARGET_TABLE@5',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_InmemoryStream4;

END APPLICATION @APPNAME@Apps3;


--********************Application 4************************
--WITH 3 TARGET FLOW
-- 1. <targetflow1> ps3->target4 <targetflow1>
-- 2. <targetflow2>ps4->target5 <targetflow2>
-- 3. <targetflow3> ps3,ps4->cq5 -> target6 <targetflow3>

CREATE APPLICATION @APPNAME@Apps4 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @APPNAME@TARGETFLOW3;
CREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget4 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@3,@TARGET_TABLE@3',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream3;
END FLOW @APPNAME@TARGETFLOW3;

CREATE FLOW @APPNAME@TARGETFLOW4;
CREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget5 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@4,@TARGET_TABLE@4',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_Stream4;
END FLOW @APPNAME@TARGETFLOW4;

CREATE OR REPLACE STREAM @APPNAME@Apps_PS_InmemoryStream5 OF Global.waevent;

CREATE CQ @APPNAME@cq_ps3
INSERT INTO @APPNAME@Apps_PS_InmemoryStream5
SELECT *
FROM @APPNAME@Apps_PS_Stream3;

CREATE CQ @APPNAME@cq_ps4
INSERT INTO @APPNAME@Apps_PS_InmemoryStream5
SELECT *
FROM @APPNAME@Apps_PS_Stream4;


CREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget6 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT_USERNAME@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@CONNECT_URL@',
  Tables: '@SOURCE_TABLE@3,@TARGET_TABLE@6;@SOURCE_TABLE@4,@TARGET_TABLE@6',
  Password: '@TGT_PASSWORD@',
  Password_encrypted: false
 ) 
INPUT FROM @APPNAME@Apps_PS_InmemoryStream5;

END APPLICATION @APPNAME@Apps4;

deploy application @APPNAME@Apps4 on ANY in Appsdg with @APPNAME@Targetflow3 in Targetdg1,@APPNAME@Targetflow4 in Targetdg;
start application @APPNAME@Apps4;

deploy application @APPNAME@Apps3 on ANY in Appsdg with @APPNAME@Targetflow2 in Targetdg1,@APPNAME@Targetflow1 in Targetdg;
start application @APPNAME@Apps3;

deploy application @APPNAME@Apps2 in Appsdg with @APPNAME@agentflow3 in agents, @APPNAME@agentflow4 in agents2, @APPNAME@serverflow in default;
start application @APPNAME@Apps2;

deploy application @APPNAME@Apps1 in Appsdg with @APPNAME@agentflow in agents, @APPNAME@agentflow2 in agents2;
start application @APPNAME@Apps1;

STOP APPLICATION SystemTimeTester.SystemTimeWindows;
UNDEPLOY APPLICATION SystemTimeTester.SystemTimeWindows;
DROP APPLICATION SystemTimeTester.SystemTimeWindows cascade;


CREATE APPLICATION SystemTimeWindows;

CREATE TYPE RandomData(
bankNumber int KEY,
bankName String
);

CREATE  SOURCE ranDataSource USING StreamReader (
  OutputType: 'SystemTimeTester.RandomData',
  noLimit: 'false',
  isSeeded: 'true',
  maxRows: 0,
  iterations: 30,
  iterationDelay: 1000,
  StringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',
  NumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]R'
 )
OUTPUT TO CSVDataStream;

CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT data[0], data[1]
FROM CSVDataStream;

CREATE @WINDOWTYPE@ WINDOW tierone OVER RandomDataStream keep within 20 second;

CREATE STREAM onetwostream OF RandomData;

CREATE CQ onetwocq
INSERT INTO onetwostream
SELECT bankNumber,bankName
FROM tierone
where  bankName LIKE 'bofa'
order by bankName;

CREATE WACTIONSTORE MyDataActivity  CONTEXT OF RandomData
EVENT TYPES ( RandomData  )
@PERSIST-TYPE@

Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
SELECT bankNumber,bankName from @FROMSTREAM@
where  bankName LIKE '%fa%'
order by bankName
LINK SOURCE EVENT;

END APPLICATION SystemTimeWindows;
deploy application SystemTimeWindows;
start application SystemTimeWindows;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create or replace propertyvariable ms_pass='@SOURCE_PASS@';
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE @SOURCE@ USING MSSQLReader  ( 
  FilterTransactionBoundaries: true,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Password: '$ms_pass',
  Password_encrypted: 'true',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Username: '@SOURCE_USER@',
  DatabaseName: 'qatest',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@ USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;


END APPLICATION @APPNAME@;

deploy application @APPNAME@ on ANY in default;

start application @APPNAME@;

Stop Oracle_IRLogWriter;
Undeploy application Oracle_IRLogWriter;
drop application Oracle_IRLogWriter cascade;

CREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;

CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( 
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.TDSOURCE',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.TEST01=ID;',
  PollingInterval: '5sec',
  ReturnDateTimeAs: 'String',
  startPosition:'striim.test01=0'
  )
  OUTPUT TO data_stream;

  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

CREATE TARGET BinaryDump USING LogWriter(
  name: 'TeraData',
  filename:'TeraData.log',
  flushpolicy:'EventCount:100,Interval:30s'
)INPUT FROM data_stream;

END APPLICATION Oracle_IRLogWriter;

deploy application Oracle_IRLogWriter in default;

start application Oracle_IRLogWriter;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW ;

 

CREATE OR REPLACE SOURCE DBSource USING DatabaseReader  ( 
  Username: '@SOURCE_USER@',
  Password_encrypted: false,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Query: "SELECT * FROM waction.MysqlToCql_alldatatypes",
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SOURCE_PASS@'
 ) 
OUTPUT TO Oracle_ChangeDataStream;


CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

STOP application consoletest.noApp;
undeploy application consoletest.noApp;
drop application consoletest.noApp cascade;

DROP USER consoletest;
DROP NAMESPACE consoletest CASCADE;
CREATE USER consoletest IDENTIFIED BY consoletest;
REVOKE consoletest.admin from user consoletest;
GRANT create,drop ON deploymentgroup Global.consoletest To user consoletest;
GRANT all ON namespace Global.consoletest To user consoletest;
GRANT all ON Application consoletest.noApp To user consoletest;
REVOKE drop ON Application consoletest.noApp from user consoletest;
CONNECT consoletest consoletest;

create application noApp;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'posdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
directory:'@FEATURE-DIR@/logs/',
filename:'PermissionTarget',
sequence:'00',
rolloverpolicy:'eventcount:1222001'
)
format using DSVFormatter (

)
input from TypedCSVStream;
end application noApp;

stop application SQLtoRedshift;
undeploy application SQLtoRedshift;
drop application SQLtoRedshift cascade;
CREATE APPLICATION SQLtoRedshift RECOVERY 1 SECOND INTERVAL;

CREATE  SOURCE SQLSource USING MSSqlReader  ( 
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  DatabaseName: '@DATABASE-NAME@',
  ConnectionURL: '@URL@',
  Tables: '@SOURCE-TABLES@'
 ) 
OUTPUT TO sqlstream ;

--CREATE  TARGET t2 USING SysOut  ( name: 'sqltors') INPUT FROM sqlstream;

CREATE TARGET RedshiftTarget USING RedshiftWriter
	(
	  ConnectionURL: '@TARGET-URL@',
	  Username: '@TARGET-UNAME@',
	  Password: '@TARGET-PASSWORD@',
	  bucketname: '@BUCKETNAME@',
	  --accesskeyId: '@ACCESS-KEY-ID@',
	  --secretaccesskey: '@SECRET-ACCESS-KEY@',
	  S3IAMRole:'@IAMROLE@',
	  Tables: '@TARGET-TABLES@',
	  uploadpolicy:'eventcount:1,interval:5s',
	  Mode:'incremental'
	) INPUT FROM sqlstream;

END APPLICATION SQLtoRedshift;
deploy application SQLtoRedshift;
start application SQLtoRedshift;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

stop application @appname@Out;
undeploy application @appname@Out;
drop application @appname@Out cascade;

drop stream @appname@PSStream;
CREATE OR REPLACE PROPERTYSET @appname@KafkaPropset (zk.address:'localhost:@keeperport@', bootstrap.brokers:'localhost:@brokerport@', partitions:'50');
CREATE STREAM @appname@PSStream@rand@ OF Global.JSONNodeEvent PERSIST USING @appname@KafkaPropset;

CREATE APPLICATION @appname@ recovery 5 second interval;;

CREATE OR REPLACE SOURCE @cobolsrc@ USING FileReader (
  wildcard: '',
  positionbyeof: false,
  directory: ''
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: 'ProcessRecordAsEvent',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'SingleEvent',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'Level01',
  copybookFileName: ''
   )
OUTPUT TO @appname@PSStream@rand@;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE APPLICATION @appname@Out recovery 5 second interval;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
  filename: '',
  directory: '',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter  (
  members: 'data',
  EventsAsArrayOfJsonObjects: 'true'
 )
INPUT FROM @appname@PSStream@rand@;

END APPLICATION @appname@Out;
deploy application @appname@Out on all in default;

list servers;

stop OracleTOFileWriterApp;
undeploy application OracleTOFileWriterApp;
drop application OracleTOFileWriterApp cascade;

CREATE APPLICATION OracleTOFileWriterApp recovery 5 second interval;

Create Source ReadFromOracle
Using OracleReader
(
Username:'qatest',
Password:'qatest',
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:xe',
Tables:'QATEST.VARRAY_AS_VARCHAR',
OnlineCatalog:true,
CommittedTransactions:true
)
Output To DataStream;


CREATE OR REPLACE TARGET FileWriter1 USING FileWriter  (
filename: 'OutFile.csv',
  directory:'@DIR@/%@metadata(TableName)%',
  rolloverpolicy: 'EventCount:6,Interval:60s'
)
format using DSVFormatter (
members: 'data'
)
input from DataStream;

create target tout using sysout(name:'out') input from DataStream;

CREATE OR REPLACE TARGET FileWriter2 USING FileWriter  (
filename: 'OutFile.json',
  directory:'@DIR@/%@metadata(TableName)%',
  rolloverpolicy: 'EventCount:6,Interval:60s'
)
format using JSONFormatter (
)
input from DataStream;


CREATE OR REPLACE TARGET FileWriter3 USING FileWriter  (
filename: 'OutFile.xml',
  directory:'@DIR@/%@metadata(TableName)%',
  rolloverpolicy: 'EventCount:6,Interval:60s'
)
format using XMLFormatter (
rootelement : 'data',
FormatColumnValueAS:'xmlelement')
input from DataStream;


CREATE OR REPLACE TARGET FileWriter4 USING FileWriter  (
  filename: 'OutFile.parquet',
  directory:'@DIR@/%@metadata(TableName)%',
  rolloverpolicy: 'EventCount:6,Interval:60s' 
  )
FORMAT USING ParquetFormatter  
( 
blocksize: '128000000',
  formatAs: 'Default',
  schemaFileName: '@DIR@/parquetSchema'
  )
INPUT FROM DataStream;

CREATE OR REPLACE TARGET FileWriter5 USING FileWriter  (
  filename: 'OutFile.avro',
  directory:'@DIR@/%@metadata(TableName)%',
  rolloverpolicy: 'EventCount:6,Interval:60s' 
  )
FORMAT USING AvroFormatter  
( 
schemaFileName : '@DIR@/varray.avsc'
  )
INPUT FROM DataStream;

END APPLICATION OracleTOFileWriterApp;
deploy application OracleTOFileWriterApp in default;
start OracleTOFileWriterApp;

stop application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
	directory:'@TEST-DATA-PATH@',
        WildCard:'banks.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'no'
)
OUTPUT TO CsvStream;

CREATE TARGET DBDump USING FileWriter(
filename:'@FEATURE-DIR@/logs/JsonRaw_RT.log', rolloverpolicy:'EventCount:10000,Interval:30s' )Format using JSONFormatter()
INPUT FROM CsvStream;
end application DSV;

--
-- Crash Recovery Test 1 on Four node all server cluster 
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP N4S4CR1Tester.N4S4CRTest1;
UNDEPLOY APPLICATION N4S4CR1Tester.N4S4CRTest1;
DROP APPLICATION N4S4CR1Tester.N4S4CRTest1 CASCADE;
CREATE APPLICATION N4S4CRTest1 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest1;

CREATE SOURCE CsvSourceN4S4CRTest1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest1;

CREATE FLOW DataProcessingN4S4CRTest1;

CREATE TYPE WactionTypeN4S4CRTest1 (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE WactionsN4S4CRTest1 CONTEXT OF WactionTypeN4S4CRTest1
EVENT TYPES ( WactionTypeN4S4CRTest1 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN4S4CRTest1
INSERT INTO WactionsN4S4CRTest1
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END FLOW DataProcessingN4S4CRTest1;

END APPLICATION N4S4CRTest1;

STOP APPLICATION KafkaWPTester.KWApp;
STOP APPLICATION KafkaWPTester.KRApp;
UNDEPLOY APPLICATION KafkaWPTester.KWApp;
UNDEPLOY APPLICATION KafkaWPTester.KRApp;
DROP APPLICATION KafkaWPTester.KWApp CASCADE;
DROP APPLICATION KafkaWPTester.KRApp CASCADE;

CREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;
GRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;
CONNECT KafkaWPTester KafkaWPTester;


CREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE CSVSource USING FileReader (
	directory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',
    WildCard:'access_log_half',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;

CREATE TYPE AccessLogType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;

CREATE CQ AceeslogCQ
INSERT INTO TypedAccessLogStream
SELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]
FROM FileStream;

create Target DsvWriter using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V10dsvExistTopic',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream;


create Target JsonWriter using KafkaWriter VERSION '0.10.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V10jsonExistTopic',
	Mode:'Sync'
    )
 
FORMAT USING JSONFormatter ()
input from TypedAccessLogStream;


create Target AvroWriter using KafkaWriter VERSION '0.10.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V10avroExistTopic',
    Mode:'Sync'
    )
 
FORMAT USING AvroFormatter (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')
input from TypedAccessLogStream;


END APPLICATION KWApp;

DEPLOY APPLICATION KWApp on any in default;


-- Kafka Reader Apps

CREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;


CREATE TYPE AccessLogType2(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE Stream KafkaDSVReaderStream of AccessLogType2;
CREATE Stream KafkaJsonReaderStream of AccessLogType2;
CREATE Stream KafkaAvroReaderStream of AccessLogType2;


CREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V10dsvExistTopic',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;




CREATE CQ DSVtoKafkaDSVReaderStream
INSERT INTO KafkaDSVReaderStream
SELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]
FROM KafkaReaderStream1;

CREATE TARGET DSVDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V10_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaDSVReaderStream;

CREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V10jsonExistTopic',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;




CREATE CQ DSVtoKafkaJsonReaderStream
INSERT INTO KafkaJsonReaderStream
SELECT 
data.get('Col1').textValue(),
data.get('Col2').textValue(),
data.get('Col3').textValue(),
data.get('Col4').textValue(),
data.get('Col5').textValue(),
data.get('Col6').textValue(),
data.get('Col7').textValue(),
data.get('Col8').textValue(),
data.get('Col9').textValue(),
data.get('Col10').textValue()
FROM KafkaReaderStream2;

CREATE TARGET JSONDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V10_RT_JSON')
FORMAT USING DSVFormatter()
INPUT FROM KafkaJsonReaderStream;

CREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V10avroExistTopic',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'
)
OUTPUT TO KafkaReaderStream3;



CREATE Stream KRTypedStream of AccessLogType;

CREATE CQ DSVtoKafkaAvroReaderStream
INSERT INTO KafkaAvroReaderStream
SELECT    
data.get("Col1").toString(), 
data.get("Col2").toString(), 
data.get("Col3").toString(), 
data.get("Col4").toString(),
data.get("Col5").toString(), 
data.get("Col6").toString(), 
data.get("Col7").toString(), 
data.get("Col8").toString(),
data.get("Col9").toString(), 
data.get("Col10").toString()
FROM KafkaReaderStream3;

CREATE TARGET AVRODump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V10_RT_AVRO')
FORMAT USING DSVFormatter()
INPUT FROM KafkaAvroReaderStream;

END APPLICATION KRApp;
DEPLOY APPLICATION KRApp on any in default;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;


CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using PostgreSQLReader
(
   adapterName: PostgreSQLReader,
   CDDLAction: Quiesce_Cascade,
   CDDLCapture: true,
   CDDLTrackingTable:'striim.ddlcapturetable',
   ConnectionURL: jdbc:postgresql://localhost:5432/qatest,
   FilterTransactionBoundaries: true,
   Password: w@ct10n,
   ReplicationSlotName:'test_slot',
   Tables: public.PGMultiDownstream_src,
   Username: sa,
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;

CREATE APPLICATION @APPNAME3@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName1@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME3@;
DEPLOY APPLICATION @APPNAME3@;
START APPLICATION @APPNAME3@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[0]) = '1' ;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

--
-- Recovery Test 1
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--


UNDEPLOY APPLICATION NameT00.T00;
DROP APPLICATION NameT00.T00 CASCADE;
CREATE APPLICATION T00 RECOVERY 5 SECOND INTERVAL;




CREATE FLOW DataAcquisitionT00;


CREATE SOURCE CsvSourceT00 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO OutputStreamT00;


END FLOW DataAcquisitionT00;




CREATE FLOW DataProcessingT00;


Create Target OutputTargetT00
Using Sysout (name: 'OutputTargetT00')
Input From OutputStreamT00;


END FLOW DataProcessingT00;



END APPLICATION T00;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

--CREATE APPLICATION @APPNAME@;
create application @APPNAME@ Recovery 5 second Interval;

--create or replace flow @APPNAME@_agentflow;

CREATE OR REPLACE SOURCE @APPNAME@_source USING OracleReader  (
  Compression: false,
  StartTimestamp: 'null',
  SupportPDB: false,
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  FetchSize: 1,
  DDLCaptureMode: 'All',
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'localhost:1521:xe',
 Tables: 'QATEST.Source1',
-- Tables: 'QATEST.OracToCql_alldatatypes',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  TransactionBufferType: 'Memory',
  DictionaryMode: 'OnlineCatalog',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '1MB'
 )
OUTPUT TO @APPNAME@_stream ;

--end flow @APPNAME@_@APPNAME@_agentflow;

CREATE OR REPLACE TARGET @APPNAME@_target USING CassandraCosmosDBWriter  (
  AccountEndpoint: 'cassandracosmostest.cassandra.cosmos.azure.com',
  AccountKey: 'pqDZvVgbdSCg7VzIzD77dAhPG2odGRZPLhAQA1qnZbAKoIDk6RuQX5r2phbRQFnR1l54qxOcvBXNdz8DeijYIg==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  Tables: 'QATEST.Source1,test.target1',
  adapterName: 'CassandraCosmosDBWriter'
 )
 INPUT FROM @APPNAME@_stream;

 END APPLICATION @APPNAME@;

deploy application @APPNAME@;
 --deploy application @APPNAME@ with agentflow in agents;
 start application @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

CREATE  SOURCE @QuerySource@1 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@1;
create Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;
 CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@1;

CREATE  SOURCE @QuerySource@2 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@2;
create Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;
 CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@2;

CREATE  SOURCE @QuerySource@3 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@3;
create Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;
 CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@3;

CREATE  SOURCE @QuerySource@4 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@4;
create Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;
 CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@4;

CREATE  SOURCE @QuerySource@5 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@5;
create Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;
 CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@5;

CREATE  SOURCE @QuerySource@6 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@6;
create Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;
 CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@6;

CREATE  SOURCE @QuerySource@7 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@7;
create Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;
 CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@7;

CREATE  SOURCE @QuerySource@8 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@8;
create Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;
 CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@8;

CREATE  SOURCE @QuerySource@9 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@9;
create Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;
 CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@9;

CREATE  SOURCE @QuerySource@10 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@10;
create Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;
 CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@10;
end application @APPNAME@;

-- The PosAppAgent sample application demonstrates how a credit card
-- payment processor might use Striim to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.


CREATE APPLICATION PosAppAgent;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosAppAgent application.

-- source CsvAgentDataSource

CREATE FLOW AgentFlow;

CREATE source CsvAgentDataSource USING FileReader (
  directory: 'C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvAgentStream;

END FLOW AgentFlow;

-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells Striim that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells Striim to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvAgentStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvAgentToPosDataCq

CREATE FLOW ProcessFlow;

CREATE CQ CsvAgentToPosDataCq
INSERT INTO PosDataAgentStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvAgentStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP.
--
-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as
-- the types to be used in PosDataAgentStream, which is created automatically.

-- The DATETIME field from the source is converted to both a dateTime
-- value, used as the event timestamp by the application, and (via the
-- function) an integer hourValue, which is used to look up
-- historical hourly averages from the HourlyAgentAveLookup cache,
-- discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)
-- 17 (hourValue)
-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateAgentOnly
--
-- The AgentPosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAgentAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAgentAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW AgentPosData5Minutes
OVER PosDataAgentStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantAgentHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAgentAveLookup using FileReader (
  directory: 'C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'merchantId') OF MerchantAgentHourlyAve;

CREATE TYPE MerchantTxRateAgent(
  merchantId String KEY,
  zip String,
  startTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateAgentOnlyStream OF MerchantTxRateAgent PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateAgentOnly
INSERT INTO MerchantTxRateAgentOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM AgentPosData5Minutes p, HourlyAgentAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAgentAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateAgentWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE STREAM MerchantTxRateAgentWithStatusStream OF MerchantTxRateAgent;

CREATE CQ GenerateMerchantTxRateAgentWithStatus
INSERT INTO MerchantTxRateAgentWithStatusStream
SELECT merchantId,
       zip,
       startTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateAgentOnlyStream;


-- WAction store MerchantActivityAgent
--
-- The following group of statements create and populate the MerchantActivityAgent
-- WAction store. Data from the MerchantTxRateAgentWithStatusStream is enhanced
-- with merchant details from NameLookupAgent cache and with latitude and longitude
-- values from the USAddressDataAgent cache.

CREATE TYPE MerchantActivityAgentContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivityAgent CONTEXT OF MerchantActivityAgentContext
EVENT TYPES ( MerchantTxRateAgent )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );


CREATE TYPE MerchantAgentNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressDataAgent(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookupAgent using FileReader (
  directory:'C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false,
  trimwhitespace:true
)
QUERY(keytomap:'merchantId') OF MerchantAgentNameData;

CREATE CACHE ZipLookupAgent using FileReader (
  directory: 'C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'zip') OF USAddressDataAgent;


CREATE CQ GenerateWactionAgentContext
INSERT INTO MerchantActivityAgent
SELECT  m.merchantId,
        m.startTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateAgentWithStatusStream m, NameLookupAgent n, ZipLookupAgent z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

-- CQ GenerateAgentAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertAgentStream OF Global.AlertEvent;

CREATE CQ GenerateAgentAlerts
INSERT INTO AlertAgentStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateAgentWithStatusStream m, NameLookupAgent n
WHERE m.merchantId = n.merchantId;

CREATE SUBSCRIPTION AgentAlertSub USING WebAlertAdapter( ) INPUT FROM AlertAgentStream;

END FLOW ProcessFlow;

END APPLICATION PosAppAgent;

DEPLOY APPLICATION PosAppAgent with AgentFlow in AGENTS, ProcessFlow in default;

-- CREATE DASHBOARD USING "C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData/PosAppDashboard.json";

undeploy application GCSWriterTest;
alter application GCSWriterTest;

CREATE OR REPLACE SOURCE OracleSource USING OracleReader  (
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: '@SOURCE_TABLES@',
  FetchSize: 1
 ) Output To OracleStream;

 create or replace Target OracleGCSTarget using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@',
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from OracleStream;
end application GCSWriterTest;
alter application GCSWriterTest recompile;
deploy application GCSWriterTest;
start application GCSWriterTest;

-- stop application Metadata_Application;
-- undeploy application Metadata_Application;
drop application Metadata_Application force;
CREATE APPLICATION Metadata_Application;

CREATE OR REPLACE TYPE app1_AccessSource_Type (
 timestamp org.joda.time.DateTime,
 src_ip java.lang.String,
 dest_ip java.lang.String,
 http_code java.lang.String);


CREATE OR REPLACE SOURCE app1_AccessSource USING ContinuousGenerator ( 
  OutputType: 'admin.TYPE.app1_AccessSource_Type', 
  Throughput: 'Unrestricted' ) 
OUTPUT TO app1_OutputStream;


CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

END APPLICATION Metadata_Application;



-- stop application app2;
-- undeploy application Metadata_Application2;
drop application Metadata_Application2 force;

CREATE APPLICATION Metadata_Application2;

CREATE OR REPLACE CQ app2_Query 
INSERT INTO app2_AccessStream 
SELECT *
FROM app1_AccessStream;

END APPLICATION Metadata_Application2;

Stop bq;
Undeploy application bq;
alter application bq;
--CREATE CQ cq1
--INSERT INTO OpsStream
--SELECT  
--CASE WHEN (META(a,"OperationName").toString() == "INSERT")
--THEN putUserData(a, 'ops_name', 'I') 
--Else putUserData(a, 'ops_name', 'NOT_INSERT') 
--END
--FROM ss a;
CREATE OR REPLACE TARGET T USING BigQueryWriter  ( 
  serviceAccountKey: '/Users/saranyad/Product/IntegrationTests/TestData/google-gcs.json',
  projectId: 'bigquerywritertest',
  Tables: '@TABLE@',
  datalocation: 'US',
  nullmarker: 'null',
  columnDelimiter: '|',
  Mode: 'Merge',
  _h_throwExceptionOnTableNotFound:'false',
  IgnorableExceptioncode:'TABLE_NOT_FOUND',
  BatchPolicy: 'eventcount:100,interval:10'
 ) INPUT FROM ss;
alter application bq recompile;
deploy application bq;
Start bq;

--
-- Recovery Test 2
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS
--


STOP KStreamRecov2Tester.KStreamRecovTest2;
UNDEPLOY APPLICATION KStreamRecov2Tester.KStreamRecovTest2;
DROP APPLICATION KStreamRecov2Tester.KStreamRecovTest2 CASCADE;
DROP USER KStreamRecov2Tester;
DROP NAMESPACE KStreamRecov2Tester CASCADE;
CREATE USER KStreamRecov2Tester IDENTIFIED BY KStreamRecov2Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov2Tester;
CONNECT KStreamRecov2Tester KStreamRecov2Tester;

CREATE APPLICATION KStreamRecovTest2 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION KStreamRecovTest2;

Stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 1 second interval;
CREATE TYPE @APPNAME@_Test_1_type
( id java.lang.Integer KEY , 
status java.lang.String  
 );

CREATE OR REPLACE CACHE @APPNAME@_Test_1 USING DatabaseReader ( 
  --ConnectionURL: 'jdbc:sqlserver://@MSSQLIP@:@MSSQLPORT@',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433',
  DatabaseProviderType: 'Default',
  FetchSize: 1,
  Password:'@Password@',
  Query: 'Select id,status from [QATEST].[QATEST].[Test_1]',
  Username: '@Username@'
 ) 
QUERY ( 
  keytomap: 'id',
  skipinvalid: 'false'
 ) 
 OF @APPNAME@_Test_1_type;

CREATE TYPE @APPNAME@_Test_3_type
( id java.lang.Integer KEY , 
status java.lang.String  
 );

CREATE TYPE @APPNAME@_Test_2_type
( id java.lang.Integer KEY , 
status java.lang.String  
 );

CREATE OR REPLACE CACHE @APPNAME@_Test_3 USING DatabaseReader ( 
  --ConnectionURL: 'jdbc:sqlserver://@MSSQLIP@:@MSSQLPORT@',
    ConnectionURL: 'jdbc:sqlserver://localhost:1433',
  DatabaseProviderType: 'Default',
  FetchSize: 1,
  Password:'@Password@',
  Query: 'Select id,status from [QATEST].[QATEST].[Test_3]',
  Username: '@Username@'
 ) 
QUERY ( 
  keytomap: 'id',
  skipinvalid: 'false'
 ) 
 OF @APPNAME@_Test_3_type;
 
 CREATE OR REPLACE CACHE @APPNAME@_Test_2 USING DatabaseReader ( 
  --ConnectionURL: 'jdbc:sqlserver://@MSSQLIP@:@MSSQLPORT@',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433',
  DatabaseProviderType: 'Default',
  FetchSize: 1,
  Password:'@Password@',
  Query: 'Select id,status from [QATEST].[QATEST].[Test_2]',
  Username: '@Username@'
 ) 
QUERY ( 
  keytomap: 'id',
  skipinvalid: 'false',
  refreshinterval: '5 SECOND'
 ) 
 OF @APPNAME@_Test_2_type;
 

CREATE OR REPLACE SOURCE @APPNAME@_s1 USING MSSqlReader  ( 
  Compression: true,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.Test_Master',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
) 
OUTPUT TO @APPNAME@_ss1;

CREATE OR REPLACE SOURCE @APPNAME@_s2 USING MSSqlReader  ( 
  Compression: true,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qates%.Test_%',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
) 
OUTPUT TO @APPNAME@_ss2;

CREATE TYPE @APPNAME@_ExtractedFields_Type  ( 
id java.lang.Long , 
name java.lang.String , 
city java.lang.String , 
Test_1_id java.lang.Long , 
Test_3_id java.lang.Long ,
Test_2_id java.lang.Long 
 );

CREATE OR REPLACE STREAM @APPNAME@_ExtractedFields OF @APPNAME@_ExtractedFields_Type;


CREATE OR REPLACE CQ @APPNAME@_ExtractedFields_cq
INSERT INTO @APPNAME@_ExtractedFields
SELECT 
   b.data[0] as id,
   b.data[1] as name,
   b.data[2] as city,
   b.data[3] as Test_1_ID,
   b.data[4] as Test_3_ID,
   b.data[5] as Test_2_ID
FROM
@APPNAME@_ss1 b LEFT OUTER JOIN @APPNAME@_Test_2 s ON TO_INT(b.data[0]) = s.id
 LEFT OUTER JOIN 
@APPNAME@_Test_1 p ON TO_INT(b.data[3]) = p.id
 LEFT OUTER JOIN
@APPNAME@_Test_3 r ON TO_INT(b.data[4]) = r.id
WHERE (TO_STRING(META(b, "OperationName")) = "UPDATE" or TO_STRING(META(b, "OperationName")) = "INSERT")
limit 1;

CREATE  TARGET @APPNAME@_t1_dsv USING AzureEventHubWriter  ( 
  EventHubNamespace: 'EventHubWriterTest',
  EventHubName: 'test_01',
  E1P: 'true',
  SASPolicyName: 'RootManageSharedAccessKey',
  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
  PartitionKey: 'id',
  ConsumerGroup: 'test_01_cg',
  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
  --ParallelThreads: '2'
 ) 
FORMAT USING DSVFormatter  (  ) 
INPUT FROM @APPNAME@_ExtractedFields;

CREATE  TARGET @APPNAME@_t2_json USING AzureEventHubWriter  ( 
  EventHubNamespace: 'EventHubWriterTest',
  EventHubName: 'test_02',
  E1P: 'true',
  SASPolicyName: 'RootManageSharedAccessKey',
  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
  PartitionKey: 'id',
  ConsumerGroup: 'test_02_cg',
  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
  --ParallelThreads: '2'
 ) 
FORMAT USING JSONFormatter  (  ) 
INPUT FROM @APPNAME@_ExtractedFields;
CREATE  TARGET @APPNAME@_t3_avro USING AzureEventHubWriter  ( 
  EventHubNamespace: 'EventHubWriterTest',
  EventHubName: 'test_03',
  E1P: 'true',
  SASPolicyName: 'RootManageSharedAccessKey',
  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
  PartitionKey: 'id',
  ConsumerGroup: 'test_03_cg',
  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
  --ParallelThreads: '2'
 ) 
FORMAT USING AvroFormatter  (   schemaFileName: 'kafkaAvroTest_multipleReader.avsc'
 ) 
INPUT FROM @APPNAME@_ExtractedFields;

CREATE  TARGET @APPNAME@_t1_dsv_rawstream USING AzureEventHubWriter  ( 
  EventHubNamespace: 'EventHubWriterTest',
  EventHubName: 'test_04',
  E1P: 'true',
  SASPolicyName: 'RootManageSharedAccessKey',
  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
  PartitionKey: '@metadata(TableName)',
  ConsumerGroup: 'test_04_cg',
  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
  --ParallelThreads: '2'
 ) 
FORMAT USING DSVFormatter  (  ) 
INPUT FROM @APPNAME@_ss2;

CREATE  TARGET @APPNAME@_t2_json_rawstream USING AzureEventHubWriter  ( 
  EventHubNamespace: 'EventHubWriterTest',
  EventHubName: 'test_05',
  E1P: 'true',
  SASPolicyName: 'RootManageSharedAccessKey',
  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
  PartitionKey: '@metadata(TableName)',
  ConsumerGroup: 'test_05_cg',
  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
  --ParallelThreads: '2'
 ) 
FORMAT USING JSONFormatter  (  ) 
INPUT FROM @APPNAME@_ss2;

CREATE  TARGET @APPNAME@_t3_avro_rawstream USING AzureEventHubWriter  ( 
  EventHubNamespace: 'EventHubWriterTest',
  EventHubName: 'test_06',
  E1P: 'true',
  SASPolicyName: 'RootManageSharedAccessKey',
  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
  PartitionKey: '@metadata(TableName)',
  ConsumerGroup: 'test_06_cg',
  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
  --ParallelThreads: '2'
 ) 
FORMAT USING AvroFormatter  (   schemaFileName: 'kafkaAvroTest_multipleReader.avsc'
 ) 
INPUT FROM @APPNAME@_ss2;

END APPLICATION @APPNAME@;
Deploy application @APPNAME@;
start @APPNAME@;

CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @APPNAME@_Source USING Global.MSSqlReader (
  TransactionSupport: false,
  _h_returnNumericAs: 'Double',
  Tables: 'qatest.T27342_Source',
  FetchTransactionMetadata: false,
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
  Compression: false,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Password_encrypted: 'false',
  Password: 'w3b@ct10n',
  StartPosition: 'NOW',
  adapterName: 'MSSqlReader',
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'qatest',
  Username: 'qatest',
  FetchSize: 0,
  IntegratedSecurity: false,
  FilterTransactionBoundaries: true,
  ConnectionPoolSize: 2,
  SendBeforeImage: true,
  AutoDisableTableCDC: false )
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_Target USING Global.AzureSQLDWHWriter (
  AccountName: 'testaswin',
  Tables: 'qatest.T27342_Source,dbo.test',
  Password_encrypted: 'false',
  AccountAccessKey_encrypted: 'false',
  CDDLAction: 'Process',
  StorageAccessDriverType: 'WASBS',
  ConnectionURL: 'jdbc:sqlserver://testaswin.database.windows.net:1433;database=testaswin',
  Username: 'testaswin',
  columnDelimiter: '|',
  Mode: 'MERGE',
  AccountAccessKey: 'MmlfH35Vc2mcOScbY2wnOyXol6deT8gtGA4XW3C5EXwwdFQEukP37RfHGWeUgMhfKsIvDvCHF/v3GF6frXGdYg==',
  Password: 'W3b@ct10n2020',
  uploadpolicy: 'eventcount:1,interval:5m' )
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_sysout USING Global.SysOut (
  name: 'sysout_1' )
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;

drop propertyset MysqlToMysqlPlatfm_App_KafkaPropset;
drop stream  MysqlToMysqlPlatfm_Stream CASCADE;


--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;
					
CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using MySQLReader(
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true',
  Tables: '$table1'
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using MySQLReader( 
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true',
  Tables: '@TABLENAME@2'
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'WACTION.MYSQLTOMYSQLPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

END APPLICATION @APPNAME@1;

CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using MySQLReader( 
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true',
  Tables: '$table2'
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using MySQLReader(
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true',
  Tables: '@TABLENAME@4',
  
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'WACTION.MYSQLTOMYSQLPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@;

END APPLICATION @APPNAME@2;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName1@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM1@;
Create Source @SourceName2@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM2@;
Create Source @SourceName3@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM3@;
Create Source @SourceName4@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM4@;
Create Source @SourceName5@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM5@;
Create Source @SourceName6@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM6@;
Create Source @SourceName7@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM7@;
Create Source @SourceName8@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM8@;
Create Source @SourceName9@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM9@;
Create Source @SourceName10@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM10@;

CREATE TARGET @targetName1@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM1@;
CREATE TARGET @targetName2@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM2@;
CREATE TARGET @targetName3@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM3@;
CREATE TARGET @targetName4@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM4@;
CREATE TARGET @targetName5@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM5@;
CREATE TARGET @targetName6@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM6@;
CREATE TARGET @targetName7@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM7@;
CREATE TARGET @targetName8@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM8@;
CREATE TARGET @targetName9@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM9@;
CREATE TARGET @targetName10@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM10@;


END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @APPNAME@_src1 Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
   _h_ReturnDateTimeAs:'ZonedDateTime'
) Output To @APPNAME@_stream1;

Create Source @APPNAME@_src2 Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
   _h_ReturnDateTimeAs:'ZonedDateTime'
) Output To @APPNAME@_stream2;


create or replace stream @APPNAME@_combined_stream OF Global.WAEvent;

Create CQ @APPNAME@CQ1
insert into @APPNAME@_combined_stream
select
* from @APPNAME@_stream1;

Create CQ @APPNAME@CQ2
insert into @APPNAME@_combined_stream
select
* from @APPNAME@_stream2;



create Target @APPNAME@_tgt1 using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:100'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_combined_stream;

create Target @APPNAME@_tgt2 using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:100'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@',
	members: 'Table=@metadata(TableName),OpName=@metadata(OperationName)'
)
input from @APPNAME@_stream2;

create Target @APPNAME@_tgt3 using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:100'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_stream1;

create type @APPNAME@_type(
id int,
name String,
cost float,
TableName string,
OperationName String
);

create or replace stream @APPNAME@_typed_Stream of @APPNAME@_type;

Create CQ @APPNAME@_TypedCQ
insert into @APPNAME@_typed_Stream
select
to_int(data[0]),data[1],to_float(data[2]),
meta(@APPNAME@_stream2,'TableName'),
Meta(@APPNAME@_stream2,'OperationName') from @APPNAME@_stream2;


create Target @APPNAME@_tgt4 using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:100'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_typed_Stream;




end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'data.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO @AppName@_rawstream;


CREATE CQ @BuiltinFunc@CQ
INSERT INTO @BuiltinFunc@_Stream
SELECT @BuiltinFunc@(x, 'Last_Date', data[5], 'Country', data[10])
FROM @AppName@_rawstream x;

CREATE OR REPLACE CQ cq1
INSERT INTO RemoveUserData_Stream
SELECT
removeUserData(s1, 'Last_Date')
FROM @BuiltinFunc@_Stream s1;


CREATE OR REPLACE TARGET @AppName@_FileTarget USING Global.FileWriter ( 
  flushpolicy: 'EventCount:10000,Interval:30s', 
  adapterName: 'FileWriter', 
  directory: '@logs@',
  filename: '@BuiltinFunc@_RemoveData', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING Global.JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter', 
  jsonMemberDelimiter: '\n', 
  EventsAsArrayOfJsonObjects: 'true', 
  formatterName: 'JSONFormatter', 
  jsonobjectdelimiter: '\n' ) 
INPUT FROM RemoveUserData_Stream;

End application @AppName@;
Deploy application @AppName@; 
Start application @AppName@;

stop APPLICATION @AppName@;
Undeploy APPLICATION @AppName@;
drop APPLICATION @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @AgentFlow@;

CREATE OR REPLACE SOURCE @SourceName@ USING Global.MSJet (
  Tables: 'dbo.compsrc',
  username: 'qatest',
  DatabaseName: 'qatest',
  FetchTransactionMetadata: true,
  filterTransactionBoundaries: true,
  compression: false,
  ConnectionURL: '@ConnectionURL@',
  Mode: '@mode@',
  CommittedTransactions: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
  SendBeforeImage: true,
  password: 'w3b@ct10n' )
OUTPUT TO @StreamName@;
END FLOW @AgentFlow@;

CREATE TARGET @SysTarget@ USING Global.SysOut (
  name: 'MS_CDC_SYSOUT' )
INPUT FROM @StreamName@;

CREATE FLOW @ServerFlow@;

CREATE TARGET @TargetName@ USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ParallelThreads: '',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: '@ConnectionURL@',
  StatementCacheSize: '50',
  DatabaseProviderType: 'Default',
  Username: 'qatest',
  Tables: 'dbo.compsrc,dbo.comptar',
  Password: 'w3b@ct10n',
  PreserveSourceTransactionBoundary: 'false',
  BatchPolicy: 'EventCount:1,Interval:60' )
INPUT FROM @StreamName@;

END FLOW @ServerFlow@;

END APPLICATION @AppName@;
DEPLOY APPLICATION @AppName@ with @AgentFlow@ in AGENTS ,@ServerFlow@ on any in default;
START APPLICATION @AppName@;

stop application Postgres_To_PostgresApp;
undeploy application Postgres_To_PostgresApp;
drop application Postgres_To_PostgresApp cascade;

CREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;

CREATE FLOW agentflow;

CREATE OR REPLACE SOURCE Postgres_Src1 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_1',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename0'
 ) 
OUTPUT TO Change_Data_Stream ;

CREATE OR REPLACE SOURCE Postgres_Src2 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_2',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename1'
 ) 
OUTPUT TO Change_Data_Stream ;


CREATE OR REPLACE SOURCE Postgres_Src3 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_3',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename2'
 ) 
OUTPUT TO Change_Data_Stream ;

CREATE OR REPLACE SOURCE Postgres_Src4 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_4',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename3'
 ) 
OUTPUT TO Change_Data_Stream ;

end flow agentflow;

create flow serverflow;

CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt1 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target0;public.tablename1, public.target0;public.tablename2, public.target0;public.tablename3, public.target0;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt2 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target1;public.tablename1, public.target1;public.tablename2, public.target1;public.tablename3, public.target1;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt3 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target2;public.tablename1, public.target2;public.tablename2, public.target2;public.tablename3, public.target2;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt4 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target3;public.tablename1, public.target3;public.tablename2, public.target3;public.tablename3, public.target3;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

end flow serverflow;
end application Postgres_To_PostgresApp;
deploy application Postgres_To_PostgresApp with agentflow on any in agents, serverflow in default;
start Postgres_To_PostgresApp;







stop application Postgres_To_PostgresApp2;
undeploy application Postgres_To_PostgresApp2;
drop application Postgres_To_PostgresApp2 cascade;

CREATE APPLICATION Postgres_To_PostgresApp2 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW agentflow2;

CREATE OR REPLACE SOURCE Postgres_Src21 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_1',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename0'
 ) 
OUTPUT TO Change_Data_Stream2 ;

CREATE OR REPLACE SOURCE Postgres_Src22 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_2',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename1'
 ) 
OUTPUT TO Change_Data_Stream2 ;


CREATE OR REPLACE SOURCE Postgres_Src23 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_3',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename2'
 ) 
OUTPUT TO Change_Data_Stream2 ;

CREATE OR REPLACE SOURCE Postgres_Src24 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_4',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename3'
 ) 
OUTPUT TO Change_Data_Stream2 ;

end flow agentflow2;

create flow serverflow2;

CREATE OR REPLACE TARGET Postgres_Sys2 USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Change_Data_Stream2;

CREATE OR REPLACE TARGET PostgreSQL_Tgt21 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target0;public.tablename1, public.target0;public.tablename2, public.target0;public.tablename3, public.target0;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream2;

CREATE OR REPLACE TARGET PostgreSQL_Tgt22 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target1;public.tablename1, public.target1;public.tablename2, public.target1;public.tablename3, public.target1;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream2;

CREATE OR REPLACE TARGET PostgreSQL_Tgt23 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target2;public.tablename1, public.target2;public.tablename2, public.target2;public.tablename3, public.target2;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream2;

CREATE OR REPLACE TARGET PostgreSQL_Tgt24 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target3;public.tablename1, public.target3;public.tablename2, public.target3;public.tablename3, public.target3;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream2;

end flow serverflow2;

end application Postgres_To_PostgresApp2;
deploy application Postgres_To_PostgresApp2 with agentflow2 on any in agents, serverflow2 in default;
start Postgres_To_PostgresApp2;

STOP APPLICATION SystemTimeTester.SystemTimeWindows;
UNDEPLOY APPLICATION SystemTimeTester.SystemTimeWindows;
DROP APPLICATION SystemTimeTester.SystemTimeWindows cascade;

CREATE APPLICATION SystemTimeWindows;

CREATE TYPE RandomData(
myName String,
streetAddress String,
bankName String,
bankNumber int KEY,
bankAmount double
);


CREATE SOURCE ranDataSource using StreamReader(
OutputType: 'SystemTimeTester.RandomData',
noLimit: 'false',
isSeeded: 'true',
maxRows: 0,
iterations: 30,
iterationDelay: 1000,
StringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',
NumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]R'
)OUTPUT TO CSVDataStream;

CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4])
FROM CSVDataStream;

CREATE @WINDOWTYPE@ WINDOW tierone OVER RandomDataStream keep within 20 second;

CREATE STREAM onetwostream OF RandomData;

CREATE CQ onetwocq
INSERT INTO onetwostream
SELECT *
FROM tierone;

CREATE @WINDOWTYPE@ WINDOW tiertwo OVER onetwostream keep within 40 second;

CREATE STREAM twothreestream OF RandomData;

CREATE CQ twothreecq
INSERT INTO twothreestream
SELECT *
FROM tiertwo;

CREATE @WINDOWTYPE@ WINDOW tierthree OVER twothreestream keep within 1 minute;

CREATE WACTIONSTORE MyDataActivity
CONTEXT OF RandomData
EVENT TYPES(RandomData )
@PERSIST-TYPE@

Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
Select * from @FROMSTREAM@
LINK SOURCE EVENT;


END APPLICATION SystemTimeWindows;
deploy application SystemTimeWindows;
start application SystemTimeWindows;

stop application ADW1;
undeploy application ADW1;
drop application ADW1 cascade;
CREATE APPLICATION ADW1;

CREATE  SOURCE SqlServerInitialLoad1 USING DatabaseReader  
 (
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'@SOURCE-TABLES@',
 FetchSize:2000
) 
OUTPUT TO InitialLoadStream1;

CREATE TARGET AzureDWInitialLoad1 USING AzureSQLDWHWriter(
ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
)
INPUT FROM InitialLoadStream1;

END APPLICATION ADW1;
deploy application ADW1;
start application ADW1;

CREATE APPLICATION @AppName@;

CREATE OR REPLACE SOURCE CP_Oracle_source USING OracleReader (
  ConnectionURL: '',
  Tables: '',
  Username: '',
  Password: '',
  Fetchsize: 1 )
OUTPUT TO CP_EndToEnd_DB_Adapter_Stream;

CREATE OR REPLACE TARGET CP_DB_Target USING Global.DeltaLakeWriter (
  connectionProfileName: '',
  useConnectionProfile: 'true',
  externalStageConnectionProfileName: '',
  Tables: 'QATEST.Test_CP,qa_reg_CDDL_1702803133916.OrcToDLAltAppendtarget1',
  uploadPolicy: 'eventcount:100000,interval:60s'
)
INPUT FROM CP_EndToEnd_DB_Adapter_Stream;
END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING DatabaseReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE CQ @APPNAME@CQ
INSERT INTO @APPNAME@CQStream
SELECT putUserData(x, 'city',data[6])
FROM @APPNAME@stream x;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@CQStream;

END APPLICATION @APPNAME@;

DROP TYPE T1;
DROP CACHE C1;


CREATE  TYPE T1  ( id String ,
airport_ref String,
airport_ident String KEY ,
type1 String ,
description String ,
frequency_mhz String
 );

CREATE CACHE C1 USING FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: '1MB.csv',
  charset: 'UTF-8',
  blockSize: '64',
  positionbyeof: 'false'
 )
PARSE USING DSVPARSER (
  columndelimiter: ',',
  rowdelimiter: '\n:\r',
  header: 'true'
 )
QUERY (
  keytomap: 'id'
 )
 OF T1;

-- Loading Cache C1 which is accessed by DataSourceApp.tql

load cache C1;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using S3Writer(
    bucketname:'@BUCKET@',
   objectname:'upgradeData.csv',
   foldername:'upgradefolder',
  uploadpolicy:'EventCount : 10000,Interval :1m '
)
format using DSVFormatter (
)
input from @STREAM@;

end flow @APPNAME@_serverflow;

end application @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using Ojet
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@URL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

CREATE APPLICATION @AppName@;
CREATE FLOW Il_Agent_flow;
CREATE OR REPLACE SOURCE FileReader_Src USING FileReader  (
   WildCard: 'posdata100.csv',
  directory: '@dir@',
  positionbyeof: false)
 PARSE USING DSVParser  (
 )
OUTPUT TO CsvStream ;
END FLOW Il_Agent_flow;

Create Type CSVType (
  companyid String,
  merchantId String
);

CREATE STREAM TypedCSVStream OF CSVType;

CREATE OR REPLACE  CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT
TO_STRING(data[0]).replaceAll("COMPANY ", ""),
data[1]
FROM CsvStream;

CREATE OR REPLACE TARGET initialLoadPostgres_Trg USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:100,Interval:60',
  StatementCacheSize: '50',
  ConnectionURL: '@TrgUrl@',
  Username: '@TrgUserName@',
  BatchPolicy: 'EventCount:100,Interval:60',
  Tables: '@TrgTable@',
  Password: '@TrgPswd@',
  adapterName: 'DatabaseWriter' )
INPUT FROM TypedCSVStream;
END APPLICATION @AppName@;

stop application iteratortester.iteratorapp;
undeploy application iteratortester.iteratorapp;
drop application iteratortester.iteratorapp cascade;

CREATE APPLICATION iteratorapp;

create flow sourceFlow;

CREATE SOURCE JSONAccessLogSource USING FileReader(
  directory:'@TEST-DATA-PATH@',
  wildcard:'iterator2.json'
)
parse using JSONParser (
) OUTPUT TO jsonSourceStream;

end flow sourceflow;

-- ******ARRAY LIST****** --
create flow processFlow;

create type cacheType (bankID string key, bankName string);
CREATE cache dsvcache USING FileReader (
directory:'@TEST-DATA-PATH@',
wildcard:'banks.csv',
blocksize: 10240,
positionByEOF:false
)
PARSE USING DSVParser (
header:No,
trimquote:false
) QUERY (keytomap:'bankID') OF cacheType;

CREATE TYPE listType (id integer KEY, bankname string, lst java.util.List);
CREATE TYPE listStoreType (id integer KEY, bankname string, lst java.util.List, lstoflst java.util.List);

CREATE STREAM listStream of listType partition by bankname;

CREATE JUMPING WINDOW listJWindow
OVER listStream
keep 3 rows;

CREATE WINDOW listWindow
OVER listStream
keep 3 rows;

CREATE WACTIONSTORE listStore CONTEXT OF listStoreType EVENT TYPES (listStoreType ) 
@PERSIST-TYPE@

create cq updatelistStream
insert into listStream
select TO_INT(bankID), bankName, makelist(bankID,bankName) as lst 
from dsvcache;

create cq updatelistStore 
insert into listStore
select ID, bankName, lst, makelist(lst,lst) from listStream
LINK SOURCE EVENT;

create stream listTargetStream( str String);

create cq updateListTarget
insert into listTargetStream
select itr
from listStream, iterator(listStream.lst) itr order by cast(itr as java.lang.Comparable);

-- CREATE TARGET listout USING SYSOUT(name:"list") input from listStream;

-- ******JsonNode****** --

--CREATE TYPE jsonType (id integer KEY,  lst com.fasterxml.jackson.databind.JsonNode);
CREATE TYPE jsonType (int integer, bankname string, lst com.fasterxml.jackson.databind.JsonNode key);

CREATE STREAM jsonStream of jsonType partition by bankname;

CREATE JUMPING WINDOW jsonJWindow
OVER jsonStream
keep 3 rows
partition by bankname;

CREATE WINDOW jsonWindow
OVER jsonStream
keep 3 rows;

CREATE WACTIONSTORE jsonStore CONTEXT OF jsonType EVENT TYPES (jsonType ) 
@PERSIST-TYPE@

create cq updateJsonStream
insert into jsonStream
--select TO_INT(MATCH(data[0], '.*\\\\s([0-9]+)')) , makeJSON('[{"x":"a"},{"x":"b","y":"c"},{"y":"c"}]') as lst 
select TO_INT(y.bankID), y.bankName, x.data 
from JSONSourceStream x, dsvcache y;

create cq updateJsonStore 
insert into jsonStore
select * from jsonStream
LINK SOURCE EVENT;

create stream jsonTargetStream( str String);

create cq updateJsonTarget
insert into jsonTargetStream
select to_string(itr.StringJSON)
from jsonStream, iterator(jsonStream.lst) itr order by to_string(itr.StringJSON);
CREATE TARGET jsonout USING SYSOUT(name:"jlist") input from jsonStream;

end flow processflow; 

end application iteratorapp;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ recovery 5 second Interval;
create source @srcName@ USING MySQLReader
(
  Username:'@srcusername@',
  Password:'@srcpassword@',
  ConnectionURL:'@srcurl@',
  Tables:'@srcschema@.@srctable@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries:'true'
) 
OUTPUT TO @outstreamname@;

CREATE OR REPLACE TARGET @tgtName@ USING DatabaseWriter
(
  CheckPointTable:'CHKPOINT',
  Username:'@tgtusername@',
  Password:'@tgtpassword@',
  BatchPolicy:'EventCount:1,Interval:0',
  CommitPolicy:'EventCount:1,Interval:0',
  ConnectionURL:'@tgturl@',
  Tables:'@srcschema@.@srctable@,@tgtschema@.@tgttable@'
) 
INPUT FROM @instreamname@;


End APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop application ManyToManyADLSGen1;
undeploy application ManyToManyADLSGen1;
drop application ManyToManyADLSGen1 cascade;

create application ManyToManyADLSGen1 Recovery 5 second interval;


create type ADLSGen1csv_type(
id String,
name String,
seq String
);

create type ADLSGen1Order_type(
id String,
Name String,
Company String
);



create source ADLSGen1CSVSource_multi using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'Canon1000_All.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO ADLSGen1CsvStream_user;

create source ADLSGen1CSVSource2 using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'portfolio.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO ADLSGen1CsvStream;

create source ADLSGen1CSVSource3 using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'dynamicdirectory.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO ADLSGen1CsvStream;


CREATE SOURCE ADLSGen1OraSource1 USING OracleReader
(
 Username:'miner',
 Password:'miner',
 ConnectionURL:'localhost:1521:xe',
 Tables:'QATEST.CUSTOMER1,QATEST.CUSTOMER2,QATEST.CUSTOMER3',
 OnlineCatalog:true,
 FetchSize:10000,
 QueueSize:2148,
 CommittedTransactions:false,
 Compression:true
)
OUTPUT TO ADLSGen1OrdersStream;

CREATE SOURCE ADLSGen1OraSource2 USING OracleReader
(
 Username:'miner',
 Password:'miner',
 ConnectionURL:'localhost:1521:xe',
 Tables:'QATEST.CUSTOMER4,QATEST.CUSTOMER5',
 OnlineCatalog:true,
 FetchSize:10000,
 QueueSize:2148,
 CommittedTransactions:false,
 Compression:true
)
OUTPUT TO ADLSGen1OrdersStream;


create Target ADLSGen1_tgt1 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'%@metadata(FileName)%',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
		rolloverpolicy:'interval:30s'
)
FORMAT USING JSONFormatter()
input from ADLSGen1CsvStream; 

create Target ADLSGen1_tgt2 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'%@metadata(TableName)%',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
		rolloverpolicy:'filesize:10M',
		compressiontype: 'true'
)
format using DSVFormatter (
)
input from ADLSGen1OrdersStream; 

create stream ADLSGen1UserdataStream of Global.WAEvent;

Create CQ ADLSGen1CQUser
insert into ADLSGen1UserdataStream
select 
putuserdata (data1,'Fileowner',data[1]) from ADLSGen1CsvStream_user data1;

create stream ADLSGen1CSVTypedStream1 of csv_type;
create stream ADLSGen1CSVTypedStream2 of csv_type;
create stream ADLSGen1CSVTypedStream3 of csv_type;

CREATE CQ ADLSGen1cq1
INSERT INTO ADLSGen1CSVTypedStream1
SELECT data[0],
data[1],
data[2]
FROM ADLSGen1UserdataStream
WHERE USERDATA(ADLSGen1UserdataStream,'Fileowner').toString() == 'Lorem';

CREATE CQ ADLSGen1cq2
INSERT INTO ADLSGen1CSVTypedStream2
SELECT data[0],
data[1],
data[2]
FROM ADLSGen1UserdataStream
WHERE USERDATA(ADLSGen1UserdataStream,'Fileowner').toString() == 'doloremque';

CREATE CQ ADLSGen1cq3
INSERT INTO ADLSGen1CSVTypedStream3
SELECT data[0],
data[1],
data[2]
FROM ADLSGen1UserdataStream
WHERE USERDATA(ADLSGen1UserdataStream,'Fileowner').toString() == 'accusantium';

create Target ADLSGen1_tgt3 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'1_%name%',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
	rolloverpolicy:'eventcount:74'
)
format using XMLFormatter (
  elementtuple: 'Eventname:name:id:seq:text=name',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from ADLSGen1CSVTypedStream1; 

create Target ADLSGen1_tgt4 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'2_%name%',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
    rolloverpolicy:'eventcount:4,interval:50s'
)
format using XMLFormatter (
  elementtuple: 'Eventname:name:id:seq:text=name',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from ADLSGen1CSVTypedStream2; 

create Target ADLSGen1_tgt5 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'3_%name%',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
    rolloverpolicy:'eventcount:12'
)
format using XMLFormatter (
  elementtuple: 'Eventname:name:id:seq:text=name',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from ADLSGen1CSVTypedStream3; 


create stream ADLSGen1OrderTypedStream1 of Order_type;
create stream ADLSGen1OrderTypedStream2 of Order_type;
create stream ADLSGen1OrderTypedStream3 of Order_type;

CREATE CQ ADLSGen1cq1_db
INSERT INTO ADLSGen1OrderTypedStream1
SELECT data[0],
data[1],
data[2]
FROM ADLSGen1OrdersStream
WHERE META(ADLSGen1OrdersStream,'TableName').toString() == 'QATEST.CUSTOMER1';

CREATE CQ ADLSGen1cq2_db
INSERT INTO ADLSGen1OrderTypedStream2
SELECT data[0],
data[1],
data[2]
FROM ADLSGen1OrdersStream
WHERE META(ADLSGen1OrdersStream,'TableName').toString() == 'QATEST.CUSTOMER2';

CREATE CQ ADLSGen1cq3_db
INSERT INTO ADLSGen1OrderTypedStream3
SELECT data[0],
data[1],
data[2]
FROM ADLSGen1OrdersStream
WHERE META(ADLSGen1OrdersStream,'TableName').toString() == 'QATEST.CUSTOMER3';


create Target ADLSGen1_tgt6 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'Customer1',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
	rolloverpolicy:'eventcount:10000'
)
format using AvroFormatter(
  formatAs: 'Default',
  handler: 'com.webaction.proc.AvroFormatter',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA@'
)
input from ADLSGen1OrderTypedStream1; 

create Target ADLSGen1_tgt7 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'Customer2',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
    rolloverpolicy:'eventcount:10000'
)
format using AvroFormatter(
  formatAs: 'Default',
  handler: 'com.webaction.proc.AvroFormatter',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA@'
)input from ADLSGen1OrderTypedStream2; 

create Target ADLSGen1_tgt8 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'Customer3',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
    rolloverpolicy:'eventcount:10000'
)
format using AvroFormatter(
  formatAs: 'Default',
  handler: 'com.webaction.proc.AvroFormatter',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA@'
)
input from ADLSGen1OrderTypedStream3; 

end application ManyToManyADLSGen1;

deploy application ManyToManyADLSGen1;
start application ManyToManyADLSGen1;

create Target @TARGET_NAME@ using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (

)
input from @STREAM@;

STOP APPLICATION orrs;
UNDEPLOY APPLICATION orrs;
DROP APPLICATION orrs CASCADE;
CREATE APPLICATION orrs;
Create Source oraSource1 Using DatabaseReader
(
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'QATEST.ORACLETOREDSHIFTIL1;QATEST.ORACLETOREDSHIFTIL2',
 FilterTransactionBoundaries:true,
 FetchSize:1000
) Output To LCRStream1;

Create Source oraSource2 Using DatabaseReader
(
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'QATEST.ORACLETOREDSHIFTIL3;QATEST.ORACLETOREDSHIFTIL4',
 FilterTransactionBoundaries:true,
 FetchSize:1000
) Output To LCRStream2;

CREATE TARGET RSTarget USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: 'access_key',
	  --secretaccesskey: 'secret_access',
	  Tables: 'QATEST.%,QATEST.%',
	   S3IAMRole:'@IAMROLE@',
	  uploadpolicy:'eventcount:1000,interval:1m'
	) INPUT FROM LCRStream1;

CREATE TARGET RSTarget2 USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: 'access_key',
	  --secretaccesskey: 'secret_access',
	  Tables: 'QATEST.%,QATEST.%',
	   S3IAMRole:'@IAMROLE@',
	  Tables: 'QATEST.%,QATEST.%',
	  uploadpolicy:'eventcount:1000,interval:1m'
	) INPUT FROM LCRStream2;

END APPLICATION orrs;
DEPLOY APPLICATION orrs;
START APPLICATION orrs;

STOP APPLICATION OneAgentCQTester.CSV;
UNDEPLOY APPLICATION OneAgentCQTester.CSV;
DROP APPLICATION OneAgentCQTester.CSV cascade;

create application CSV;

CREATE FLOW AgentFlow;

create source CSVSource using CSVReader
(
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'StoreNames.csv',
  columndelimiter:',',
  positionByEOF:false
)OUTPUT TO CsvStream;

CREATE TYPE MyType (
Store_Id String KEY,
Store_Name String
);

CREATE STREAM TypedStream of MyType;

CREATE CQ TypeConversionCQ
INSERT INTO TypedStream
SELECT data[0], data[1]
from CsvStream;

END FLOW AgentFlow;

CREATE FLOW ServerFlow;

CREATE WACTIONSTORE StoreInfo CONTEXT OF MyType
EVENT TYPES ( MyType )
@PERSIST-TYPE@

CREATE CQ StoreWaction
INSERT INTO StoreInfo
SELECT * FROM TypedStream
LINK SOURCE EVENT;

END FLOW ServerFlow;

end application CSV;
DEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;
START CSV;

create application SybaseJaguarApp;

create source SybaseJaguarSource using FileReader (
        directory:'@TEST-DATA-PATH@',
        WildCard:'SybaseJaguar.log',
        charset:'ISO-8859-1',
        positionByEOF:false
)
parse using FreeformTextParser (
        -- TimeStamp:'%E %mon %d %H:%M:%S %y %z',
        RecordBegin:'%H:%M:%S:%sss %yyyy%mm%d:1234567890:\n[TRACE]',
        RecordEnd:')\n',
	IgnoreMultipleRecordBegin: false,	
        regex:'((?<=logonname\\()[a-z,0-9]*)|((?<=localHostName\\()[a-z,A-Z,0-9]*)|((?<=localIP\\()[0-9,.]*)',
        separator:'~'
)
OUTPUT TO SybaseJaguarStream;

CREATE TYPE LoginInfo (
        userName String,
        hostName String,
        hostIP  String,
        eventTime long
);

create stream LoginInfoStream of LoginInfo;

create cq LoginInfoCQ
insert into LoginInfoStream
select
        data[0],
        data[1],
        data[2],
        TO_LONG(META(x,'OriginTimestamp'))
from SybaseJaguarStream x;


create Target SybaseJaguarDump using CSVWriter(filename:'@FEATURE-DIR@/logs/SybaseJaguar.log') input from LoginInfoStream;
end application SybaseJaguarApp;

STOP TQLwithinTqlApp;
UNDEPLOY APPLICATION TQLwithinTqlApp;
DROP APPLICATION TQLwithinTqlApp CASCADE;

CREATE APPLICATION TQLwithinTqlApp;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',

  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;

@@FEATURE-DIR@/tql/TQLTobeCalled.tql


END APPLICATION TQLwithinTqlApp;
DEPLOY APPLICATION TQLwithinTqlApp;
START TQLwithinTqlApp;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@ (a String, b String ,c String,g String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]) where TO_String(data[0]) > '1' ;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

stop application APP_KAFKA_DATASOURCES;
undeploy application APP_KAFKA_DATASOURCES;
drop application APP_KAFKA_DATASOURCES cascade;

CREATE APPLICATION APP_KAFKA_DATASOURCES;

CREATE OR REPLACE SOURCE SRC_FR_KAFKA_HOURLYTOTALS USING Global.FileReader (
  adapterName: 'FileReader',
  rolloverstyle: 'Default',
  blocksize: 64,
  skipbom: true,
  wildcard: 'kafka_hourly_total*.txt',
  directory: '@confDir@',
  includesubdirectories: false,
  positionbyeof: false ) 
PARSE USING Global.DSVParser (
  trimwhitespace: false,
  linenumber: '-1',
  columndelimittill: '-1',
  trimquote: true,
  ignoreemptycolumn: false,
  parserName: 'DSVParser',
  quoteset: '\"',
  handler: 'com.webaction.proc.DSVParser_1_0',
  charset: 'UTF-8',
  columndelimiter: ':',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  separator: ',',
  header: false,
  blockascompleterecord: false,
  rowdelimiter: '\n',
  nocolumndelimiter: false,
  headerlineno: 0 )
OUTPUT TO STREAM_SRC_FR_KAFKA_HOURLYTOTALS;

CREATE OR REPLACE TYPE SRC_FILECACHE_KAFKA_HOURLYTOTALS_Type (
 topic java.lang.String KEY,
 timerange java.lang.Integer,
 rawdatacount java.lang.Integer);

CREATE OR REPLACE CQ CQ_FR_KAFKA_HOURLYTOTALS_COLMAP
INSERT INTO STREAM_CQ_FR_KAFKA_HOURLYTOTALS_COLMAP
SELECT to_string(data[0]) as topic, to_int(data[1]) as timerange, to_int(data[2]) as rawdatacount, to_string(to_string(data[0])+ '-' + to_string(data[1])) as TopicTimeRange FROM STREAM_SRC_FR_KAFKA_HOURLYTOTALS s;

CREATE OR REPLACE EVENTTABLE ET_HOURLYTOTALS_KAFKADATA_FILE USING STREAM (
  name: 'STREAM_CQ_FR_KAFKA_HOURLYTOTALS_COLMAP' )
QUERY (
  keytomap: 'TopicTimeRange',
  replicas: 'all',
  persistPolicy: 'true' )
OF STREAM_CQ_FR_KAFKA_HOURLYTOTALS_COLMAP_Type;

CREATE WINDOW SLIDE_WND_HOURLYTOTALS_KAFKADATA_FILE OVER STREAM_CQ_FR_KAFKA_HOURLYTOTALS_COLMAP
KEEP WITHIN 1 DAY
PARTITION BY timerange;

END APPLICATION APP_KAFKA_DATASOURCES;

CREATE FLOW @STREAM@_SourceFlow;

CREATE SOURCE @SOURCE_NAME@ USING MySQLReader (
 Username: '@READER-UNAME@',
Password: '@READER-PASSWORD@',
ConnectionURL: '@CDC-READER-URL@',
Tables: @WATABLES@,
sendBeforeImage:'true',
FilterTransactionBoundaries: 'true'
) OUTPUT TO @STREAM@;

END FLOW @STREAM@_SourceFlow;

CREATE OR REPLACE PROPERTYVARIABLE Mode='sync';
CREATE OR REPLACE PROPERTYVARIABLE BatchPolicy='Size:900000,Interval:1';
create application KinesisTest;
create source CSVSource using FileReader (
	directory:'/home/dz/src/product/Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  companyName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0], data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	BatchPolicy: '$BatchPolicy',
    Mode: '$Mode'
)
format using XMLFormatter (
	rootelement:'document',
	elementtuple:'CompanyName:merchantid:text=companyname,ZipCode:text=zip,Amount:text=amount'
)
input from TypedCSVStream;
end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

Stop IR;
Undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;
CREATE OR REPLACE SOURCE Teradata_source1 USING IncrementalBatchReader  ( 
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.test01=id',
  startPosition: '%=0',
  PollingInterval: '20sec'
 )
OUTPUT TO data_stream1;

CREATE OR REPLACE SOURCE Teradata_source2 USING IncrementalBatchReader  ( 
  FetchSize: 10000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.test01=t1',
  startPosition: '%=0',
  PollingInterval: '20sec' )
OUTPUT TO data_stream2;

CREATE OR REPLACE SOURCE Teradata_source3 USING IncrementalBatchReader  ( 
  FetchSize: 10000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.test01=id',
  startPosition: '%=1',
  PollingInterval: '20sec'
 )
OUTPUT TO data_stream3;

CREATE OR REPLACE SOURCE Teradata_source4 USING IncrementalBatchReader  ( 
  FetchSize: 10000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.test01=t1',
  startPosition: '%=1',
  PollingInterval: '20sec' )
OUTPUT TO data_stream4;
CREATE OR REPLACE SOURCE Teradata_source5 USING IncrementalBatchReader  ( 
  FetchSize: 10000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.test01=id',
  startPosition: '%=1',
  PollingInterval: '20sec'
 )
OUTPUT TO data_stream5;

CREATE OR REPLACE SOURCE Teradata_source6 USING IncrementalBatchReader  ( 
  FetchSize: 10000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.test01=t1',
  startPosition: '%=0',
  PollingInterval: '20sec' )
OUTPUT TO data_stream5;


create target AzureSQLDWHTarget1 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test4 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream1;


create target AzureSQLDWHTarget2 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test5',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream2;

create target AzureSQLDWHTarget3 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test5',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream3;

create target AzureSQLDWHTarget4 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test5',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream4;

create target AzureSQLDWHTarget5 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test5',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream5;



create target AzureSQLDWHTarget6 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test5',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream5;


END APPLICATION IR;
deploy application IR on all in default;
start application IR;

STOP banker.bankApp;
UNDEPLOY APPLICATION banker.bankApp;
DROP APPLICATION banker.bankApp cascade;

CREATE APPLICATION bankApp;


CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;

CREATE TYPE cacheType( merchantId String, 
			hourValue int, 
			hourlyAve int);

CREATE TYPE wsData
(
bankID Integer KEY,
bankName String
);

CREATE CACHE adhcache using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'banks.csv',
  header: Yes,
  columndelimiter: ',',
  trimquote:false
) QUERY (keytomap:'bankID') OF wsData;


CREATE STREAM wsStream OF wsData;

--Select data from QaStream and insert into wsStream

CREATE CQ csvTowsData
INSERT INTO wsStream
SELECT TO_INT(data[0]),data[1] FROM QaStream;

--create jumping window over data in wsStream

CREATE WACTIONSTORE oneWS
CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@

--get data from wsStream and place into wactionStore oneWS
CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT * FROM wsStream
LINK SOURCE EVENT;


END APPLICATION bankApp;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@ recovery 1 second interval;

CREATE SOURCE @parquetsrc@ USING FileReader (
  directory: '',
  positionByEOF: false,
  WildCard: '' )
PARSE USING ParquetParser ()
OUTPUT TO @appname@Streams;

CREATE OR REPLACE CQ @appname@CQOrder3
INSERT INTO @appname@Stream3
SELECT
PUTUSERDATA(s,'schemaName',s.data.getSchema().getName())
FROM @appname@Streams s;

CREATE TARGET @adlstarget@ USING Global.ADLSGen2Writer (
    accountname:'',
  	sastoken:'',
  	filesystemname:'',
  	filename:'',
  	directory:'',
  	uploadpolicy:'eventcount:10' )
format using ParquetFormatter (
schemaFileName: 'ParquetSchema'
)
INPUT FROM @appname@Stream3;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;


CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using MysqlReader
(
   adapterName: MysqlReader,
   CDDLAction: Process,
   CDDLCapture: false,
   Compression: false,
   ConnectionURL: jdbc:mysql://localhost:3306/waction,
   FilterTransactionBoundaries: true,
   Password: ReaderPassword,
   SendBeforeImage: true,
   Tables: waction.MultiMultiDownstream_src,
   Username: ReaderUsername
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;

CREATE APPLICATION @APPNAME3@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName1@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME3@;
DEPLOY APPLICATION @APPNAME3@;
START APPLICATION @APPNAME3@;

STOP bq;
UNDEPLOY APPLICATION bq;
DROP APPLICATION bq CASCADE;

CREATE APPLICATION bq RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:orcl',
	Tables: 'QATEST.TABLE_TEST_1000001',
	FetchSize: '1'
)
OUTPUT TO SS;


CREATE or replace TARGET T USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
    Tables:'QATEST.TABLE_TEST_1000001,qatest.% keycolumns(RONUM)',
    mode:'Appendonly',
    datalocation: 'US',
	nullmarker: 'defaultNULL',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:100,Interval:10'	
) INPUT FROM ss;

END APPLICATION bq;
DEPLOY APPLICATION bq;
START APPLICATION bq;

Stop Oracle_IRLogWriter;
Undeploy application Oracle_IRLogWriter;
drop application Oracle_IRLogWriter cascade;
CREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;
create flow AgentFlow;
CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( 
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.119/DBS_PORT=1025',
  Tables: 'striim.upgrade01',
  CheckColumn:'striim.upgrade01=t1',
  startPosition:'striim.upgrade01=2018-09-20 06:43:59',
  ReturnDateTimeAs:'string'
  )
OUTPUT TO data_stream1;
end flow AgentFlow;

create flow serverFlow;
create target AzureSQLDWHTarget using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test26,dbo.test26',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream1;

end flow serverFlow;
END APPLICATION Oracle_IRLogWriter;
deploy application Oracle_IRLogWriter with AgentFlow in Agents, ServerFlow in default;
start application Oracle_IRLogWriter;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSV1Source using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO Csv1Stream;

Create Type CSV1Type (
  merchantId String,
  merchantName String
);

Create Stream TypedCSV1Stream of CSV1Type;

CREATE CQ CsvToMerchantNames
INSERT INTO TypedCSV1Stream
SELECT data[0],
       data[1]
FROM Csv1Stream;

create Target t using FileWriter(
  filename:'Merchant',
  sequence:'00',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00'
)
format using DSVFormatter (

)
input from TypedCSV1Stream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetTS_Merchant_actual.log') input from TypedCSV1Stream;

end application DSV;

create application JuniperSaLog;
create source JuniperSaLogSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'juniper-SA*',
	charset:'UTF-8',
	positionByEOF:false
) PARSE USING JuniperSA2000LogParser (
	columndelimitTill:5
) OUTPUT TO JuniperSaLogStream;
create Target JuniperSaDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/junipersa_log',charset:'UTF-8') input from JuniperSaLogStream;
end application JuniperSaLog;

STOP APPLICATION admin.PosApp;
UNDEPLOY APPLICATION admin.PosApp;
DROP APPLICATION admin.PosApp cascade;
drop namedquery admin.PosAppMainPageMap;

drop namedquery admin.PosAppMainPageBar;

drop namedquery admin.PosAppMainPageBar2;
drop namedquery admin.PosAppMainPageScatter;

drop namedquery admin.PosAppHeatMapDrilldownHeatMap;
drop namedquery admin.PosAppHeatMapDrilldownDonuts;
drop namedquery admin.PosAppMainPageHeatMap;
drop namedquery admin.PosAppCompanyDrilldownCharts;
drop namedquery admin.PosAppHeatMapDrilldownMap;
drop namedquery admin.PosAppMainSearchBox;
drop namedquery admin.PosAppMcount;

drop user PosTester;

drop namespace PosTester cascade;
drop dashboard admin.PosAppDash;

CREATE OR REPLACE PROPERTYVARIABLE RetryPolicy='timeOut=00,retryInterval=1,maxRetries=3';
CREATE OR REPLACE PROPERTYVARIABLE KafkaBrokerAddress='localhost:9099';

STOP @WRITERAPPNAME@;
UNDEPLOY APPLICATION @WRITERAPPNAME@;
DROP APPLICATION @WRITERAPPNAME@ CASCADE;

CREATE APPLICATION @WRITERAPPNAME@ @Recovery@;
create flow AgentFlow;
CREATE SOURCE S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.oracle_kw_test%',
	FetchSize: '1',
	connectionRetryPolicy:'$RetryPolicy'
)
OUTPUT TO SS;
end flow AgentFlow;
create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;
create stream out_cq_select_SS_1 of global.waevent;
create stream out_cq_select_SS_2 of global.waevent;
create stream out_cq_select_SS_3 of global.waevent;
create stream out_cq_select_SS_4 of global.waevent;
create stream out_cq_select_SS_5 of global.waevent;
create stream out_cq_select_SS_6 of global.waevent;

CREATE OR REPLACE CQ cq_select_SS1 
INSERT INTO out_cq_select_SS_1
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST1';

CREATE OR REPLACE CQ cq_select_SS2 
INSERT INTO out_cq_select_SS_2
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST2';

CREATE OR REPLACE CQ cq_select_SS3 
INSERT INTO out_cq_select_SS_3
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST3';

CREATE OR REPLACE CQ cq_select_SS4 
INSERT INTO out_cq_select_SS_4
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST4';

CREATE OR REPLACE CQ cq_select_SS5 
INSERT INTO out_cq_select_SS_5
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST5';

CREATE OR REPLACE CQ cq_select_SS6 
INSERT INTO out_cq_select_SS_6
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST6';

create Target TARGET1 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_sync_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from out_cq_select_SS_1;

create Target TARGET2 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_sync_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from out_cq_select_SS_2;

create Target TARGET3 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_sync_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_sync_CQ.avsc')
input from out_cq_select_SS_3;

create Target TARGET4 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_Async_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(ROWID)',
Mode:'Async',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from out_cq_select_SS_4;

create Target TARGET5 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_Async_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(ROWID)',
Mode:'Async',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from out_cq_select_SS_5;

create Target TARGET6 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_Async_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(ROWID)',
Mode:'Async',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_Async_CQ.avsc')
input from out_cq_select_SS_6;

create Target TARGET7 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_sync',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from ss;

create Target TARGET8 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_sync',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from ss;

create Target TARGET9 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_sync',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_sync.avsc')
input from ss;

create Target TARGET10 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_Async',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'Async',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from ss;

create Target TARGET11 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_Async',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'Async',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from ss;

create Target TARGET12 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_Async',
--ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'Async',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_Async.avsc')
input from ss;

end flow serverFlow;
end application @WRITERAPPNAME@;
deploy application @WRITERAPPNAME@;
start @WRITERAPPNAME@;



stop application @READERAPPNAME@;
undeploy application @READERAPPNAME@;
drop application @READERAPPNAME@ cascade;
CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @SOURCE@_DSV_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_sync_CQ',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;

CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
rolloverpolicy:'filesize:500M',
filename:'@READERAPPNAME@_@SOURCE@_dsv_sync_CQ')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream1;

CREATE SOURCE @SOURCE@_JSON_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_sync_CQ',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;

CREATE SOURCE @SOURCE@_AVRO_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_sync_CQ',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_sync_CQ.avsc'
)
OUTPUT TO KafkaReaderStream3;

CREATE SOURCE @SOURCE@_DSV_Async_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_Async_CQ',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream4;

CREATE SOURCE @SOURCE@_JSON_Async_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_Async_CQ',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream5;

CREATE SOURCE @SOURCE@_AVRO_Async_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_Async_CQ',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_Async_CQ.avsc'
)
OUTPUT TO KafkaReaderStream6;

CREATE SOURCE @SOURCE@_DSV_sync USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_sync',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream7;

CREATE TARGET kafkaDumpDSV_rawstream USING FileWriter(
name:kafkaOuputDSV_rawstream,
rolloverpolicy:'filesize:500M',
filename:'@READERAPPNAME@_@SOURCE@_dsv_sync')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream7;

CREATE SOURCE @SOURCE@_JSON_sync USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_sync',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream8;

CREATE SOURCE @SOURCE@_AVRO_sync USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_sync',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_sync.avsc'
)
OUTPUT TO KafkaReaderStream9;

CREATE SOURCE @SOURCE@_DSV_Async USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_Async',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream10;

CREATE SOURCE @SOURCE@_JSON_Async USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_Async',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream11;

CREATE SOURCE @SOURCE@_AVRO_Async USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_Async',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_Async.avsc'
)
OUTPUT TO KafkaReaderStream12;

end application @READERAPPNAME@;
deploy application @READERAPPNAME@;

create source @SOURCE_NAME@ USING MariaDBReader 
(
Username: '@READER-UNAME@',
Password: '@READER-PASSWORD@',
ConnectionURL: '@CDC-READER-URL@',
Tables: @WATABLES@,
sendBeforeImage:'true',
FilterTransactionBoundaries: 'true'
) 
OUTPUT TO @STREAM@;

stop application GGTrailReaderApp;
undeploy application GGTrailReaderApp;
drop application GGTrailReaderApp cascade;

create application GGTrailReaderApp recovery 5 second interval;

create source GGTrailSource using GGTrailReader (
tRaildIrectory:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario15',
tRAilfilepattern:'15*',
positionByEOF:false,
FilterTransactionBoundaries: true,
DefinitionFile:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario15/Scn15_beforeddl.def',
captureCDdl: true,
CDDLAction:'Process',
--CDDLAction:'ignore',
--CDDLAction:'quiesce',
--cddlAction:'Error',
TrailByTeOrder:'LittleEndian',
recoveryInterval: 5,
TABLES:'QATEST.INT2;QATEST.INT1'
)
OUTPUT TO GGTrailStream;

create Target t2 using SysOut(name:Foo2) input from GGTrailStream;

CREATE TARGET WriteCDCOracle1 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost/ORCL',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1,Interval:1',
CommitPolicy:'Eventcount:1,Interval:1',
Checkpointtable:'RGRN_CHKPOINT',
Tables:'QATEST.GGDDL5,QATEST.GGDDL5_TGT'
) INPUT FROM GGTrailStream;


end application GGTrailReaderApp;

deploy application GGTrailReaderApp;
start application GGTrailReaderApp;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSV1Source using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO Csv1Stream;

Create Type CSV1Type (
  merchantId String,
  merchantName String
);

Create Stream TypedCSV1Stream of CSV1Type;

CREATE CQ CsvToMerchantNames
INSERT INTO TypedCSV1Stream
SELECT data[0],
       data[1]
FROM Csv1Stream;

create Target t using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'@FEATURE-DIR@/logs/',
  compressiontype : 'garbage',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (

)
input from TypedCSV1Stream;

end application DSV;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_DBSource USING Global.OracleReader (
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  Compression: false,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Password_encrypted: 'false',
  SupportPDB: false,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  FetchSize: 1000,
  CDDLAction: 'Process',
  ConnectionURL: '192.168.56.3:1521:orcl',
  DictionaryMode: 'OnlineCatalog',
  QueueSize: 2048,
  CommittedTransactions: true,
  SetConservativeRange: false,
  CDDLCapture: false,
  Username: 'fan',
  Tables: 'FAN.S_BLOB',
  TransactionBufferType: 'Disk',
  Password: '9S5GnbGmBQNDD5c/baD0Tw==',
  TransactionBufferSpilloverSize: '100MB',
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  DatabaseRole: 'Primary' )
OUTPUT TO @APPNAME@_stream;

CREATE OR REPLACE TARGET @APPNAME@_target USING Global.BigQueryWriter (
  ColumnDelimiter: '|',
  projectId: 'striim-support',
  BatchPolicy: 'eventCount:1, Interval:1',
  NullMarker: 'NULL',
  streamingUpload: 'false',
  ServiceAccountKey: '/Users/fzhang/fan/u01/app/product/striim/striim_latest/UploadedFiles/striim-support-286429beb74d.json',
  Encoding: 'UTF-8',
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30',
  AllowQuotedNewLines: 'false',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  Tables: 'FAN.S_BLOB,Fan.s_blob columnmap(A=A,B=B,C=C,D=C,E=@metadata(OperationName));',
  TransportOptions: 'connectionTimeout=300, readTimeout=120',
  adapterName: 'BigQueryWriter',
  Mode: 'APPENDONLY',
  StandardSQL: 'true',
  includeInsertId: 'true',
  QuoteCharacter: '\"' )
INPUT FROM @APPNAME@_stream;


END APPLICATION @APPNAME@;

CREATE  TARGET @TARGET_NAME@ USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@1;

 CREATE  TARGET @TARGET_NAME@2 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@2;

 CREATE  TARGET @TARGET_NAME@3 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@3;

 CREATE  TARGET @TARGET_NAME@4 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@4;

 CREATE  TARGET @TARGET_NAME@5 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@5;

 CREATE  TARGET @TARGET_NAME@6 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@6;

 CREATE  TARGET @TARGET_NAME@7 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@7;

 CREATE  TARGET @TARGET_NAME@8 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@8;

 CREATE  TARGET @TARGET_NAME@9 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@9;

 CREATE  TARGET @TARGET_NAME@10 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@10;

--
-- Canon Test W72
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for sliding count window
--
-- S -> SWc5p -> CQ1(aggregate) -> WS
-- S -> SWc2p -> CQ1(aggregate) -> WS
--


UNDEPLOY APPLICATION NameW72.W72;
DROP APPLICATION NameW72.W72 CASCADE;
CREATE APPLICATION W72 RECOVERY 5 SECOND INTERVAL;




CREATE FLOW DataAcquisitionW72;


CREATE SOURCE CsvSourceW72 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW72;


END FLOW DataAcquisitionW72;




CREATE FLOW DataProcessingW72;

CREATE TYPE DataTypeW72 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW72 OF DataTypeW72
PARTITION BY word;

CREATE CQ CSVStreamW72_to_DataStreamW72
INSERT INTO DataStreamW72
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW72;

CREATE JUMPING WINDOW JWc5pW72
OVER DataStreamW72
KEEP 5 ROWS
PARTITION BY word;

CREATE JUMPING WINDOW JWc2pW72
OVER DataStreamW72
KEEP 2 ROWS
PARTITION BY word;

CREATE WACTIONSTORE WactionStoreW72 CONTEXT OF DataTypeW72
EVENT TYPES ( DataTypeW72 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWc5pW72_to_WactionStoreW72
INSERT INTO WactionStoreW72
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc5pW72 p;

CREATE CQ JWc2pW72_to_WactionStoreW72
INSERT INTO WactionStoreW72
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc2pW72;

END FLOW DataProcessingW72;



END APPLICATION W72;

--
-- Crash Recovery Test 5 with Jumping window and partitioned on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION KStreamN2S2CR5Tester.KStreamN2S2CRTest5;
UNDEPLOY APPLICATION KStreamN2S2CR5Tester.KStreamN2S2CRTest5;
DROP APPLICATION KStreamN2S2CR5Tester.KStreamN2S2CRTest5 CASCADE;

DROP USER KStreamN2S2CR5Tester;
DROP NAMESPACE KStreamN2S2CR5Tester CASCADE;
CREATE USER KStreamN2S2CR5Tester IDENTIFIED BY KStreamN2S2CR5Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR5Tester;
CONNECT KStreamN2S2CR5Tester KStreamN2S2CR5Tester;

CREATE APPLICATION KStreamN2S2CRTest5 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest5;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest5 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest5;

CREATE FLOW DataProcessingKStreamN2S2CRTest5;

CREATE TYPE CsvDataTypeKStreamN2S2CRTest5 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvDataTypeKStreamN2S2CRTest5 PARTITION BY merchantId;

CREATE CQ CsvToDataKStreamN2S2CRTest5
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest5 CONTEXT OF CsvDataTypeKStreamN2S2CRTest5
EVENT TYPES ( CsvDataTypeKStreamN2S2CRTest5 )
@PERSIST-TYPE@

CREATE CQ DataToWactionKStreamN2S2CRTest5
INSERT INTO WactionsKStreamN2S2CRTest5
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingKStreamN2S2CRTest5;

END APPLICATION KStreamN2S2CRTest5;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
CREATE SOURCE @APPNAME@_S USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.test01',
  connectionRetryPolicy:'retryInterval=1,maxRetries=3'
 ) 
OUTPUT TO @APPNAME@_SS;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'public.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

STOP APPLICATION FileReaderWithBinaryParserApp;
UNDEPLOY APPLICATION FileReaderWithBinaryParserApp;
DROP APPLICATION FileReaderWithBinaryParserApp cascade;

CREATE APPLICATION FileReaderWithBinaryParserApp;

create source BinarySource using FileReader (
  directory:'@TEST-DATA-PATH@/binary',
  wildcard:'10rows.bin',
  positionByEOF:false
)
parse using BinaryParser (
  metadata:'@TEST-DATA-PATH@/binary/metadata.json',
  endian:true,
  StringTerminatedByNull:false
)
OUTPUT TO BinaryStream;

create Target DSVDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/binaryDetails') input from BinaryStream;
CREATE TARGET RawOut using SysOut(name: TCPRaw) INPUT FROM BinaryStream;


END APPLICATION FileReaderWithBinaryParserApp;
deploy application FileReaderWithBinaryParserApp;
start application FileReaderWithBinaryParserApp;

--
-- Recovery Test 22 with two sources, two sliding attribute windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sa5W -> CQ1 -> WS
-- S2 -> Sa6W -> CQ2 -> WS
--

STOP KStreamRecov22Tester.KStreamRecovTest22;
UNDEPLOY APPLICATION KStreamRecov22Tester.KStreamRecovTest22;
DROP APPLICATION KStreamRecov22Tester.KStreamRecovTest22 CASCADE;
DROP USER KStreamRecov22Tester;
DROP NAMESPACE KStreamRecov22Tester CASCADE;
CREATE USER KStreamRecov22Tester IDENTIFIED BY KStreamRecov22Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov22Tester;
CONNECT KStreamRecov22Tester KStreamRecov22Tester;

CREATE APPLICATION KStreamRecovTest22 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION KStreamRecovTest22;

CREATE OR REPLACE APPLICATION @AppName@;
Create connectionProfile @CP@ TYPE Snowflake(ConnectionURL:'@SFurl@',username:'@SFUsername@',Password:'@SFPassword@',authenticationType:'Password');

Create Source @AppName@_source Using OracleReader(
  Username:'@username@',
  Password:'@password@',
  ConnectionURL:'@url@',
  Tables:'@tableName@',
  Fetchsize:1
)
Output To @AppName@_Stream;
CREATE OR REPLACE TARGET @AppName@_Target USING Global.SnowflakeWriter (
  streamingUpload: 'false',
  useConnectionProfile:'true',
  connectionProfileName: 'admin.@CP@',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  columnDelimiter: '|',
  tables: '@tableName@,SANJAYPRATAP.SAMPLESCHEMA.SAMPLE_PK',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:1,interval:5m',
  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',
  externalStageType: 'Local',
  adapterName: 'SnowflakeWriter',
  fileFormatOptions: 'null_if = \"\"' )
INPUT FROM @AppName@_Stream;
END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

STOP ttlTester.ttlApp;
UNDEPLOY APPLICATION ttlTester.ttlApp;
DROP APPLICATION ttlTester.ttlApp cascade;

CREATE APPLICATION ttlApp;


CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;


CREATE TYPE wsData
(
bankID Integer KEY,
bankName String
);


CREATE STREAM wsStream OF wsData;

--Select data from QaStream and insert into wsStream

CREATE CQ csvTowsData
INSERT INTO wsStream
SELECT TO_INT(data[0]),data[1] FROM QaStream;

--create jumping window over data in wsStream

CREATE WACTIONSTORE oneWS
CONTEXT OF wsData
EVENT TYPES(wsData )
PERSIST IMMEDIATE USING ( storageProvider: 'elasticsearch', elasticsearch.time_to_live: '10000ms' ) ;

--get data from wsStream and place into wactionStore oneWS
CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT * FROM wsStream
LINK SOURCE EVENT;


END APPLICATION ttlApp;

create Application UdpDsv;
create source UdpDsvCSVSource using UDPReader (
	IpAddress:'127.0.0.1',
	PortNo:'3546',
	charset: 'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO UdpDsvCsvStream;
create Target UdpDsvDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/userLookup') input from UdpDsvCsvStream;
end Application UdpDsv;

stop application MSSQLTransactionSupportMSSQLToMySQL1;
undeploy application MSSQLTransactionSupportMSSQLToMySQL1;
drop application MSSQLTransactionSupportMSSQLToMySQL1 cascade;

CREATE APPLICATION MSSQLTransactionSupportMSSQLToMySQL1 recovery 1 second interval;

Create Source ReadFromMSSQL6
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
FetchTransactionMetadata:'true',
FilterTransactionBoundaries: true,
Compression:'false',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportMSSQLToMySQL1Stream;


CREATE TARGET WriteToMySQL6 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC@,@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportMSSQLToMySQL1Stream;

CREATE TARGET MSSqlReaderOutput6 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportMSSQLToMySQL1Stream; 


CREATE OR REPLACE TARGET MSSQLFileOut6 USING FileWriter  ( 
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  directory:'@FEATURE-DIR@/logs/',
  filename: 'TransactionSupportMSSQLToMySQL.txt'
 ) 
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 ) 
INPUT FROM MSSQLTransactionSupportMSSQLToMySQL1Stream;

END APPLICATION MSSQLTransactionSupportMSSQLToMySQL1;
deploy application MSSQLTransactionSupportMSSQLToMySQL1;
start application MSSQLTransactionSupportMSSQLToMySQL1;

-- Creating a namespace ensures there won't be conflicts with the regular version of
-- PosApp. The only difference between this version and the regular version is
-- that the CQ that parses the source stream includes a PAUSE clauses that introduces a
-- 40-millisecond pause after each event is read, simulating the way the dashboard would
-- work with real-time data.
Stop PosAppOracle.PosAppOracle;
undeploy application PosAppOracle.PosAppOracle;
drop application PosAppOracle.PosAppOracle cascade;


-- The PosApp sample application demonstrates how a credit card
-- payment processor might use WebAction to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.

CREATE Application PosAppOracle;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING OracleReader (
OnlineCatalog:true,
FetchSize:1000,
QueueSize:2048,
CommittedTransactions:false,
Compression:false,
Username:'@LOGMINER-UNAME@',
Password:'@LOGMINER-PASSWORD@',
ConnectionURL:'@LOGMINER-URL@',
Tables:'@LOGMINER-SCHEMA@.POSDATA',
OnlineCatalog:true
) output to CsvStream;


-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells WebAction that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells WebAction to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvToPosData
--
-- A stream's type must be declared before the stream, and a CQ's
-- output stream must be defined before the CQ. Hence type-stream-CQ
-- sequences like the following are very common.

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATE(data[4]) as dateTime,
       DHOURS(TO_DATE(data[4])) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       To_String(data[9]) as zip
FROM CsvStream c;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP. These correspond to the merchantId,
-- dateTime, hourValue, amount, and zip fields in PosDataStream, as
-- defined by the PosData type.
--
-- The DATETIME field from the source is converted to both a DateTime
-- value, used as the event timestamp by the application, and an int,
-- which is used to look up historical hourly averages from the
-- HourlyAveLookup cache, discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)

-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateOnly
--
-- The PosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue int,
  hourlyAve int
);

CREATE CACHE HourlyAveLookup using DatabaseReader (
        ConnectionURL:'@READER-URL@',
        Username:'@READER-UNAME@',
        Password:'@READER-PASSWORD@',
        Table:'@READER-SCHEMA@.HOURLYDATA',
        FetchSize:12000
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId as merchantId,
       p.zip as zip,
       FIRST(p.dateTime) as startingTime,
       COUNT(p.merchantId) as count,
       SUM(p.amount) as totalAmount,
       l.hourlyAve/12 as hourlyAve,
       l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END as upperLimit,
       l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END as lowerLimit,
       '<NOTSET>' as category,
       '<NOTSET>' as status
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId as merchantId,
       zip as zip,
       startingTime as startingTime,
       count as count,
       totalAmount as totalAmount,
       hourlyAve as hourlyAve,
       upperLimit as upperLimit,
       lowerLimit as lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END as category,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END as status
FROM MerchantTxRateOnlyStream;


-- WAction store MerchantActivity
--
-- The following group of statements create and populate the MerchantActivity
-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced
-- with merchant details from NameLookup cache and with latitude and longitude
-- values from the USAddressData cache.

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count int,
  HourlyAve int,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);
CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count int,
  totalAmount double,
  hourlyAve int,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
@PERSIST-TYPE@

CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using DatabaseReader (
        ConnectionURL:'@READER-URL@',
        Username:'@READER-UNAME@',
        Password:'@READER-PASSWORD@',
        Table:'@READER-SCHEMA@.MERCHANTNAMES',
        FetchSize:12000
) QUERY (keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using DatabaseReader (
        ConnectionURL:'@READER-URL@',
        Username:'@READER-UNAME@',
        Password:'@READER-PASSWORD@',
        Table:'@READER-SCHEMA@.USADDRESSES',
        FetchSize:12000
) QUERY (keytomap:'zip') OF USAddressData;

CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;


-- CQ GenerateAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;


-- The following statement loads visualization (Dashboard) settings
-- from a file.


CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;


END APPLICATION PosAppOracle;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING DatabaseReader  (
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  Tables: @SOURCE_TABLE@,
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true'
 )
OUTPUT TO @STREAM@;


CREATE OR REPLACE SOURCE @SOURCE_NAME@2 USING DatabaseReader  (
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  Tables: @SOURCE_TABLE@,
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true'
 )
OUTPUT TO @STREAM@;

STOP APPLICATION MysqlToDbApp;
UNDEPLOY APPLICATION MysqlToDbApp;
DROP APPLICATION MysqlToDbApp CASCADE;


CREATE APPLICATION MysqlToDbApp RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE MysqlSource USING MysqlReader
(
  Username: '',
  Password: '',
  Tables: '',
  ConnectionURL: '',
  Password_encrypted: 'false',
  connectionRetryPolicy: 'retryInterval=30, maxRetries=3'
)
OUTPUT TO MysqlReaderOut;


CREATE TARGET DbTarget USING DatabaseWriter
(
  Username: '',
  Password: '',
  Tables: '',
  ConnectionURL: '',
  Password_encrypted: 'false',
  connectionRetryPolicy: 'retryInterval=30, maxRetries=3'
)
INPUT FROM MysqlReaderOut;




END APPLICATION MysqlToDbApp;
DEPLOY APPLICATION MysqlToDbApp;
START APPLICATION MysqlToDbApp;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;

Create Source @SourceName@ Using Ojet

(
  Username:'c##qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',
  Tables:'CDB$ROOT."C##QATEST".ojet_src;ORCLPDB.QATEST.ojet_src',
  _h_useClassic:false,
  Fetchsize:1,
  Compression: true,
  SupportPDB:true,
  ReplicationSlotName:'null'
)
Output To @SRCINPUTSTREAM@;

CREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'))) FROM @SRCINPUTSTREAM@ x; ;


CREATE TARGET @targetName@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',
  Username:'c##qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'CDB$ROOT."C##QATEST".ojet_src,CDB$ROOT."C##QATEST".ojet_tgt'
) INPUT FROM admin.sqlreader_cq_out;



create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

CREATE APPLICATION @APPNAME@ USE EXCEPTIONSTORE TTL : '7d' ;

CREATE SOURCE @APPNAME@_Source USING Global.OracleReader (
  Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@', )
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_Target1 USING Global.SnowflakeWriter (
  connectionUrl: '@tgturl@',
    tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
    password: '@tgtpassword@',
    username: '@tgtusername@',
    appendOnly: 'true',
    uploadPolicy: 'eventcount:1,interval:5m',
    externalStageType: 'Local',
    adapterName: 'SnowflakeWriter' )
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_Target2 USING Global.SnowflakeWriter (
  connectionUrl: '@tgturl@',
    tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
    password: '@tgtpassword@',
    username: '@tgtusername@',
    appendOnly: 'false',
    optimizedMerge: 'false',
    uploadPolicy: 'eventcount:1,interval:5m',
    externalStageType: 'Local',
    adapterName: 'SnowflakeWriter' )
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_Target3 USING Global.SnowflakeWriter (
  connectionUrl: '@tgturl@',
    tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
    password: '@tgtpassword@',
    username: '@tgtusername@',
    optimizedMerge: 'true',
    uploadPolicy: 'eventcount:1,interval:5m',
    externalStageType: 'Local',
    adapterName: 'SnowflakeWriter' )
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using S3Writer(
    bucketname:'@BUCKET@',
   objectname:'upgradeData.csv',
   foldername:'upgradefolder',
  uploadpolicy:'EventCount : 10000,Interval :1m '
)
format using DSVFormatter (
)
input from @STREAM@;

end application @APPNAME@;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@ @APP_PROPERTY@ USE EXCEPTIONSTORE;

CREATE OR REPLACE STREAM @APP_NAME@_DataStream OF Global.WAEvent;

Create Source @APP_NAME@_Source Using @SOURCE_ADAPTER@ (

) OUTPUT TO @APP_NAME@_DataStream;

CREATE TARGET @APP_NAME@_Target USING @TARGET_ADAPTER@ ( 

) INPUT FROM @APP_NAME@_DataStream;

CREATE OR REPLACE TARGET @APP_NAME@_SysOut USING Global.SysOut ( 
	name: '@APP_NAME@_SysOutWA' 
) INPUT FROM @APP_NAME@_DataStream;

END APPLICATION @APP_NAME@;

DEPLOY APPLICATION @APP_NAME@ IN DEFAULT;
START APPLICATION @APP_NAME@;

--
-- Recovery Test 28 with two sources, two jumping time-count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5t9W  -> CQ1 -> WS
--   S2 -> Jc6t11W -> CQ2 -> WS
--

STOP Recov28Tester.RecovTest28;
UNDEPLOY APPLICATION Recov28Tester.RecovTest28;
DROP APPLICATION Recov28Tester.RecovTest28 CASCADE;
CREATE APPLICATION RecovTest28 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION RecovTest28;

-- The PosApp sample application demonstrates how a credit card
-- payment processor might use Striim to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.
STOP APPLICATION PosAppKafka.PosAppKafka;
UNDEPLOY APPLICATION PosAppKafka.PosAppKafka;
DROP APPLICATION PosAppKafka.PosAppKafka CASCADE;

CREATE APPLICATION PosAppKafka;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells Striim that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells Striim to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvToPosData

create type posdatatype(
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip string
);

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092');

create stream PosDataStream of posdatatype persist using KafkaProps;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT TO_STRING(data[1]),
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       TO_STRING(data[9])
FROM CsvStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP.
--
-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as
-- the types to be used in PosDataStream, which is created automatically.

-- The DATETIME field from the source is converted to both a dateTime
-- value, used as the event timestamp by the application, and (via the
-- function) an integer hourValue, which is used to look up
-- historical hourly averages from the HourlyAveLookup cache,
-- discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)
-- 17 (hourValue)
-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateOnly
--
-- The PosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startingTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;


-- WAction store MerchantActivity
--
-- The following group of statements create and populate the MerchantActivity
-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced
-- with merchant details from NameLookup cache and with latitude and longitude
-- values from the USAddressData cache.

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
@PERSIST-TYPE@

CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false
)
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;


CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

-- CQ GenerateAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;



--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM AlertStream;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;

END APPLICATION PosAppKafka;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE application @APPNAME@ @Recovery@ AUTORESUME MAXRETRIES 2 RETRYINTERVAL 10;

create type @APPNAME@type1(
  companyName String,
  merchantId String,
  city string
);

create type @APPNAME@type2(
  c1 integer,
  c2 String,
  c3 string
);

create type @APPNAME@type3(
c1 integer
);

create type @APPNAME@type4(
c1 integer,
c2 integer
);

create stream @APPNAME@in_memory_typedStream of @APPNAME@type1 partition by city;
create stream @APPNAME@in_memory_typedStream_num of @APPNAME@type2;
create stream @APPNAME@in_memory_typedStream_num1 of @APPNAME@type2;
create stream @APPNAME@in_memory_typedStream_num2 of @APPNAME@type2;
create stream @APPNAME@in_memory_typedStream_num3 of @APPNAME@type2;
create stream @APPNAME@in_memory_typedStream_num4 of @APPNAME@type2;
create stream @APPNAME@in_memory_typedStream_num5 of @APPNAME@type2;
create stream @APPNAME@finalstream6 of @APPNAME@type4;

create source @APPNAME@s using FileReader (
        directory:'Product/IntegrationTests/TestData/',
        wildcard:'posdata5L.csv',
        positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:',',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  header: true,
  separator:'~'

)
OUTPUT TO @APPNAME@in_memory_rawStream;


create CQ @APPNAME@cq1
INSERT INTO @APPNAME@kps_waevent
SELECT *
FROM @APPNAME@in_memory_rawStream  ;

create CQ @APPNAME@cq2
INSERT INTO @APPNAME@in_memory_typedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", ""),
TO_STRING(data[1]),
TO_STRING(data[10])
FROM @APPNAME@kps_waevent ;

create CQ @APPNAME@cq3
INSERT INTO @APPNAME@in_memory_typedStream_num1
SELECT TO_INT(companyName) as c1, merchantId as c2,city as c3
FROM @APPNAME@in_memory_typedStream;
-- order by c3;

create CQ @APPNAME@cq4
INSERT INTO @APPNAME@in_memory_typedStream_num2
SELECT TO_INT(companyName) as c1, merchantId as c2,city as c3
FROM @APPNAME@in_memory_typedStream;

create CQ @APPNAME@cq5
INSERT INTO @APPNAME@in_memory_typedStream_num3
SELECT TO_INT(companyName) as c1, merchantId as c2,city as c3
FROM @APPNAME@in_memory_typedStream;

create CQ @APPNAME@cq6
INSERT INTO @APPNAME@in_memory_typedStream_num4
SELECT TO_INT(companyName) as c1, merchantId as c2,city as c3
FROM @APPNAME@in_memory_typedStream;

create CQ @APPNAME@cq7
INSERT INTO @APPNAME@in_memory_typedStream_num5
SELECT TO_INT(companyName) as c1, merchantId as c2,city as c3
FROM @APPNAME@in_memory_typedStream;

CREATE CQ @APPNAME@cq8
INSERT INTO @APPNAME@in_memory_typedStream_num6
SELECT TO_INT(companyName) as c1
FROM @APPNAME@in_memory_typedStream;


CREATE JUMPING WINDOW @APPNAME@DataStream1_100000Rows
OVER @APPNAME@in_memory_typedStream_num1 KEEP 100000 ROWS;


CREATE JUMPING WINDOW @APPNAME@DataStream2_100000Rows
OVER @APPNAME@in_memory_typedStream_num2 KEEP 100000 ROWS;


CREATE JUMPING WINDOW @APPNAME@DataStream3_100000Rows
OVER @APPNAME@in_memory_typedStream_num3 KEEP 100000 ROWS;


CREATE JUMPING WINDOW @APPNAME@DataStream4_100000Rows
OVER @APPNAME@in_memory_typedStream_num4 KEEP 100000 ROWS;


CREATE JUMPING WINDOW @APPNAME@DataStream5_100000Rows
OVER @APPNAME@in_memory_typedStream_num5 KEEP 100000 ROWS;

CREATE JUMPING WINDOW @APPNAME@DataStream6_100000Rows
OVER @APPNAME@in_memory_typedStream_num6 KEEP 100000 ROWS;

create CQ @APPNAME@cq9
INSERT INTO @APPNAME@finalstream1
SELECT c1 FROM @APPNAME@DataStream1_100000Rows sample by c1;

create CQ @APPNAME@cq10
INSERT INTO @APPNAME@finalstream2
SELECT c1 FROM @APPNAME@DataStream2_100000Rows sample by c1 selectivity 0.1;

create CQ @APPNAME@cq11
INSERT INTO @APPNAME@finalstream3
SELECT c1 FROM @APPNAME@DataStream3_100000Rows sample by c1 selectivity 0.25;


create CQ @APPNAME@cq12
INSERT INTO @APPNAME@finalstream4
SELECT c1 FROM @APPNAME@DataStream4_100000Rows sample by c1 selectivity 0.05;
--SELECT count(*) FROM @APPNAME@DataStream4Rows10000Seconds sample by c1 selectivity 0.05;

create CQ @APPNAME@cq13
INSERT INTO @APPNAME@finalstream5
SELECT c1 FROM @APPNAME@DataStream5_100000Rows sample by c1 selectivity 0.01;

create CQ @APPNAME@cq14
INSERT INTO @APPNAME@finalstream6
SELECT c1,c1 as c2 FROM @APPNAME@DataStream6_100000Rows sample by c1,c2 selectivity 0.01;

create target @APPNAME@target1 using filewriter (
filename:'FEATURE-DIR/logs/@APPNAME@target1.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using dsvFormatter()
input from @APPNAME@finalstream1;

create target @APPNAME@target2 using filewriter (
filename:'FEATURE-DIR/logs/@APPNAME@target2.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using dsvFormatter()
input from @APPNAME@finalstream2;

create target @APPNAME@target3 using filewriter (
filename:'FEATURE-DIR/logs/@APPNAME@target3.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using dsvFormatter()
input from @APPNAME@finalstream3;

create target @APPNAME@target4 using filewriter (
filename:'FEATURE-DIR/logs/@APPNAME@target4.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using dsvFormatter()
input from @APPNAME@finalstream4;

create target @APPNAME@target5 using filewriter (
filename:'FEATURE-DIR/logs/@APPNAME@target5.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using dsvFormatter()
input from @APPNAME@finalstream5;

CREATE WACTIONSTORE @APPNAME@Wactions1 CONTEXT OF @APPNAME@type3
EVENT TYPES ( @APPNAME@type2 )
USING ( storageProvider:'elasticsearch' );

CREATE WACTIONSTORE @APPNAME@Wactions2 CONTEXT OF @APPNAME@type3
EVENT TYPES ( @APPNAME@type2 )
USING ( storageProvider:'elasticsearch' );

CREATE WACTIONSTORE @APPNAME@Wactions3 CONTEXT OF @APPNAME@type3
EVENT TYPES ( @APPNAME@type2 )
USING ( storageProvider:'elasticsearch' );

CREATE WACTIONSTORE @APPNAME@Wactions4 CONTEXT OF @APPNAME@type4
EVENT TYPES ( @APPNAME@type4 )
USING ( storageProvider:'elasticsearch' );

CREATE WACTIONSTORE @APPNAME@Wactions5 CONTEXT OF @APPNAME@type4
EVENT TYPES ( @APPNAME@type4 )
USING ( storageProvider:'elasticsearch' );

--sampling twice: one in finalstream1 and another in select query.
CREATE CQ @APPNAME@cq15
INSERT INTO @APPNAME@Wactions1
SELECT FIRST(p.c1) FROM @APPNAME@finalstream1 p GROUP BY p.c1 sample by p.c1 ;

--sampling once: results will be same as target2 and target1.
CREATE CQ @APPNAME@cq16
INSERT INTO @APPNAME@Wactions2
SELECT * from @APPNAME@finalstream1 order by c1 desc limit 10000 ;

--sampling twice: one in finalstream1 and another in select query.
CREATE CQ @APPNAME@cq17
INSERT INTO @APPNAME@Wactions3
SELECT * from @APPNAME@finalstream1 order by c1 sample by c1;

--sampling using 2 fields, 2800 for single field and 332 for 2 field
CREATE CQ @APPNAME@cq18
INSERT INTO @APPNAME@Wactions4
SELECT c1,c1 from @APPNAME@finalstream6 order by c1 sample by c1;

--same as Wactions4 - here selectivity alone varies, so output is 8
CREATE CQ @APPNAME@cq19
INSERT INTO @APPNAME@Wactions5
SELECT c1,c1 from @APPNAME@finalstream6 order by c1 sample by c1 selectivity 0.0001;

end application @APPNAME@;
deploy application @APPNAME@;
--start @APPNAME@;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]);

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

STOP APPLICATION DBRTOCW;
UNDEPLOY APPLICATION DBRTOCW;
DROP APPLICATION DBRTOCW CASCADE;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  --QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 ) OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) INPUT FROM Oracle_ChangeDataStream;


CREATE TARGET t2 USING SysOut(name:Foo2) INPUT FROM Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;

DEPLOY APPLICATION DBRTOCW on ANY in default;

START APPLICATION DBRTOCW;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using DatabaseReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@ RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE @parquetsrc@ USING FileReader (
wildcard: '',
directory: '',
positionbyeof: false )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: ''
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application ManyToManyADLSGen2;
undeploy application ManyToManyADLSGen2;
drop application ManyToManyADLSGen2 cascade;

create application ManyToManyADLSGen2 Recovery 5 second interval;


create type csv_type_gen2(
id String,
name String,
seq String
);

create type Order_type_gen2(
id String,
Name String,
Company String
);



create source CSVSource_multi_gen2 using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'Canon1000_All.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO CsvStream_user_gen2;

create source CSVSource2_gen2 using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'portfolio.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO CsvStream_gen2;

create source CSVSource3_gen2 using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'DataCenterData.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO CsvStream_gen2;


CREATE SOURCE OraSource1_gen2 USING OracleReader
(
 Username:'miner',
 Password:'miner',
 ConnectionURL:'localhost:1521:xe',
 Tables:'QATEST.CUSTOMER1,QATEST.CUSTOMER2,QATEST.CUSTOMER3',
 OnlineCatalog:true,
 FetchSize:10000,
 QueueSize:2148,
 CommittedTransactions:false,
 Compression:true
)
OUTPUT TO OrdersStream_gen2;

CREATE SOURCE OraSource2_gen2 USING OracleReader
(
 Username:'miner',
 Password:'miner',
 ConnectionURL:'localhost:1521:xe',
 Tables:'QATEST.CUSTOMER4,QATEST.CUSTOMER5',
 OnlineCatalog:true,
 FetchSize:10000,
 QueueSize:2148,
 CommittedTransactions:false,
 Compression:true
)
OUTPUT TO OrdersStream_gen2;


create Target ADLSGen2_tgt1 using ADLSGen2Writer(
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatest',
        directory:'%@metadata(FileName)%',
        filename:'event_data.csv',
        uploadpolicy:'eventcount:5'
)
FORMAT USING JSONFormatter()
input from CsvStream_gen2; 

create Target ADLSGen2_tgt2 using ADLSGen2Writer(
          accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'%@metadata(TableName)%',
        filename:'table.csv',
        uploadpolicy:'filesize:10M',
        compressiontype: 'true'
)
format using DSVFormatter (
)
input from OrdersStream_gen2; 

create stream UserdataStream_gen2 of Global.WAEvent;

Create CQ CQUser_gen2
insert into UserdataStream_gen2
select 
putuserdata (data1,'Fileowner',data[1]) from CsvStream_user_gen2 data1;

create stream CSVTypedStream1_gen2 of csv_type_gen2;
create stream CSVTypedStream2_gen2 of csv_type_gen2;
create stream CSVTypedStream3_gen2 of csv_type_gen2;

CREATE CQ cq1_gen2
INSERT INTO CSVTypedStream1_gen2
SELECT data[0],
data[1],
data[2]
FROM UserdataStream_gen2
WHERE USERDATA(UserdataStream_gen2,'Fileowner').toString() == 'Lorem';

CREATE CQ cq2
INSERT INTO CSVTypedStream2_gen2
SELECT data[0],
data[1],
data[2]
FROM UserdataStream_gen2
WHERE USERDATA(UserdataStream_gen2,'Fileowner').toString() == 'doloremque';

CREATE CQ cq3
INSERT INTO CSVTypedStream3_gen2
SELECT data[0],
data[1],
data[2]
FROM UserdataStream_gen2
WHERE USERDATA(UserdataStream_gen2,'Fileowner').toString() == 'accusantium';

create Target ADLSGen2_tgt3 using ADLSGen2Writer(
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'1up_%name%',
        filename:'many_event_data.csv',
        uploadpolicy:'eventcount:74'
)
format using XMLFormatter (
  elementtuple: 'Eventname:name:id:seq:text=name',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from CSVTypedStream1_gen2; 

create Target ADLSGen2_tgt4 using ADLSGen2Writer(
         accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'2up_%name%',
        filename:'many_event_data.csv',
        uploadpolicy:'interval:10s'
)
format using XMLFormatter (
  elementtuple: 'Eventname:name:id:seq:text=name',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from CSVTypedStream2_gen2; 

create Target ADLSGen2_tgt5 using ADLSGen2Writer(
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'3up_%name%',
        filename:'many_event_data.csv',
        uploadpolicy:'eventcount:10,intervals:30s'
)
format using XMLFormatter (
  elementtuple: 'Eventname:name:id:seq:text=name',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from CSVTypedStream3_gen2; 


create stream OrderTypedStream1_gen2 of Order_type_gen2;
create stream OrderTypedStream2_gen2 of Order_type_gen2;
create stream OrderTypedStream3_gen2 of Order_type_gen2;

CREATE CQ cq1_gen2_db
INSERT INTO OrderTypedStream1_gen2
SELECT data[0],
data[1],
data[2]
FROM OrdersStream_gen2
WHERE META(OrdersStream_gen2,'TableName').toString() == 'QATEST.CUSTOMER1';

CREATE CQ cq2_gen2_db
INSERT INTO OrderTypedStream2_gen2
SELECT data[0],
data[1],
data[2]
FROM OrdersStream_gen2
WHERE META(OrdersStream_gen2,'TableName').toString() == 'QATEST.CUSTOMER2';

CREATE CQ cq3_gen2_db
INSERT INTO OrderTypedStream3_gen2
SELECT data[0],
data[1],
data[2]
FROM OrdersStream_gen2
WHERE META(OrdersStream_gen2,'TableName').toString() == 'QATEST.CUSTOMER3';


create Target ADLSGen2_tgt6 using ADLSGen2Writer(
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'Customer1',
        filename:'many_event_data.csv',
        uploadpolicy:'eventcount:10000'
)
format using AvroFormatter (
  formatAs: 'Default',
  handler: 'com.webaction.proc.AvroFormatter',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA@1.avsc'
)
input from OrderTypedStream1_gen2; 

create Target ADLSGen2_tgt7 using ADLSGen2Writer(
       accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'Customer2',
        filename:'many_event_data.csv',
        uploadpolicy:'eventcount:10000'
)
format using AvroFormatter (
  formatAs: 'Default',
  handler: 'com.webaction.proc.AvroFormatter',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA@2.avsc'
)
input from OrderTypedStream2_gen2; 

create Target ADLSGen2_tgt8 using ADLSGen2Writer(
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'Customer3',
        filename:'many_event_data.csv',
        uploadpolicy:'eventcount:10000'
)
format using AvroFormatter (
  formatAs: 'Default',
  handler: 'com.webaction.proc.AvroFormatter',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA@3.avsc'
)
input from OrderTypedStream3_gen2; 

end application ManyToManyADLSGen2;

deploy application ManyToManyADLSGen2;
start application ManyToManyADLSGen2;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu RECOVERY 5 SECOND INTERVAL;
Create Source oracSource
 Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;
CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
ConnectionRetryPolicy: 'retryInterval=40,maxRetries=7',
batchpolicy: 'EventCount:20,Interval:60')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

create application access;

create source AALAccessSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'access_log',
  charset:'UTF-8',
  positionByEOF:false
) PARSE USING AALParser (
  columndelimiter:' ',
  IgnoreEmptyColumn:'Yes'
) OUTPUT TO AalAccessStream;

create Target AALAccessDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/logdata') input from AalAccessStream;

end application access;

--
-- Kafka Stream Recovery Test 1
-- Bert Hashemi and Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> KS -> WS

STOP KStreamRecov1Tester.KStreamRecovTest1;
UNDEPLOY APPLICATION KStreamRecov1Tester.KStreamRecovTest1;
DROP APPLICATION KStreamRecov1Tester.KStreamRecovTest1 CASCADE;
DROP USER KStreamRecov1Tester;
DROP NAMESPACE KStreamRecov1Tester CASCADE;
CREATE USER KStreamRecov1Tester IDENTIFIED BY KStreamRecov1Tester;
-- GRANT 'Global:create,drop:deploymentgroup:*' TO USER KStreamRecov1Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov1Tester;
CONNECT KStreamRecov1Tester KStreamRecov1Tester;

CREATE APPLICATION KStreamRecovTest1 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE or REPLACE TYPE KafkaCsvStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF KafkaCsvStreamType 
EVENT TYPES ( KafkaCsvStreamType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

END APPLICATION KStreamRecovTest1;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using AzureblobWriter(
    accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:7'
)
format using DSVFormatter (
)
input from @STREAM@;

end application @APPNAME@;

create Target @TARGET_NAME@ using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'eventCount:1000',
    ServiceAccountKey:'@file-path@'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @STREAM@;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;
Create Source oracSource
 Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;

CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

undeploy application reconnect;
alter application reconnect;

create or replace TARGET dbtarget USING DatabaseWriter  (
  ConnectionURL:'@URL@',
  Username:'@USERNAME@',
  Password:'@PASSWORD@',
  ConnectionRetryPolicy: 'retryInterval=20s, maxRetries=3',
  BatchPolicy:'EventCount:5,Interval:30',
  CommitPolicy:'EventCount:5,Interval:30',
  Tables: '@TABLES@'
 )
INPUT FROM sqlstream;

alter application reconnect recompile;
deploy application reconnect;
start application reconnect;

--
-- Recovery Test 32 with two sources, two sliding attribute windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sa5W/p -> CQ1 -> WS
-- S2 -> Sa6W/p -> CQ2 -> WS
--

STOP KStreamRecov32Tester.KStreamRecovTest32;
UNDEPLOY APPLICATION KStreamRecov32Tester.KStreamRecovTest32;
DROP APPLICATION KStreamRecov32Tester.KStreamRecovTest32 CASCADE;

DROP USER KStreamRecov32Tester;
DROP NAMESPACE KStreamRecov32Tester CASCADE;
CREATE USER KStreamRecov32Tester IDENTIFIED BY KStreamRecov32Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov32Tester;
CONNECT KStreamRecov32Tester KStreamRecov32Tester;

CREATE APPLICATION KStreamRecovTest32 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION KStreamRecovTest32;

create source @SOURCE_NAME@ USING MySQLReader 
(
Username: '@READER-UNAME@',
Password: '@READER-PASSWORD@',
ConnectionURL: 'jdbc:mysql://127.0.0.1:3306/@DBName@',
Tables: @WATABLES@,
sendBeforeImage:'true',
FilterTransactionBoundaries: 'true'
) 
OUTPUT TO @STREAM@;