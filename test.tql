STOP LngIS2noder.LongRunningISStreamApp;
UNDEPLOY APPLICATION LngIS2noder.LongRunningISStreamApp;
DROP APPLICATION LngIS2noder.LongRunningISStreamApp CASCADE;

CREATE APPLICATION LongRunningISStreamApp;

CREATE FLOW ISFLOW1;
----------------------------------------------------
CREATE source implicitSOurce USING StreamReader
(
   OutputType: 'LngIS2noder.Atm',
   noLimit: 'true',
   maxRows: 1000,
   iterationDelay: 50,
   StringSet: 'productID[001-002-003-004],stateID[AS-CA-WA-NY],currentDate[2014-2015]',
   NumberSet: 'productWeight[1-10]R,quantity[50.5-160.1]G,size[763872-4778823]L'
) OUTPUT TO CsvStream;


CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity double,
  size long,
  currentDate org.joda.time.DateTime);

END FLOW ISFLOW1;
----------------------------------------------------

CREATE FLOW ISFLOW2;

CREATE CACHE cache1 USING CsvReader(
  directory: '@TEST-DATA-PATH@',
  wildcard: 'ISdata.csv',
  header: false,
  columndelimiter: ',',
  trimquote: false
  ) QUERY (keytomap:'productID') OF Atm;


CREATE STREAM newStream OF Atm;


CREATE CQ newCQ
INSERT INTO newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]), TO_DATE(data[5]) FROM
CsvStream;

CREATE WINDOW win1
OVER newStream
KEEP 50 rows;


CREATE CQ newCQ2
INSERT INTO newStream2
SELECT productID as A , stateID AS B, productWeight AS C, quantity AS D, size AS E, currentDate AS F FROM
newStream;


CREATE CQ newCQ3
INSERT INTO newStream3 PARTITION BY A
SELECT A,B,C,D,E,F FROM newStream2 order by C,D
link source event;

CREATE CQ newCQ4
INSERT INTO newStream4
SELECT count(productID),currentDate FROM newStream ORDER BY currentDate
link source event;

CREATE CQ newCQ5
INSERT INTO newStream5
SELECT x.*, y.* from cache1 x, newStream y WHERE x.productweight > 6 ORDER BY x.currentDate;


CREATE WACTIONSTORE WS1 CONTEXT OF Atm
EVENT TYPES(Atm );

CREATE CQ newCQ6
INSERT INTO WS1
SELECT * FROM newStream WHERE productID = '001';

CREATE CQ newCQ7
INSERT INTO newStream6
SELECT aa.productID FROM WS1 [push] aa, cache1 bb;


CREATE CQ newCQ8
INSERT INTO newStream7
SELECT Sum(X.size) FROM (Select size from win1 where productweight > 5) X;


END FLOW ISFLOW2;
----------------------------------------------------

END APPLICATION LongRunningISStreamApp;

stop Oracle_IRLogWriter;
undeploy application Oracle_IRLogWriter;
drop application Oracle_IRLogWriter cascade;

CREATE APPLICATION Oracle_IRLogWriter;

CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.autotest01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.autotest01=id',
 startPosition: 'striim.autotest01=2',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream;

  create type AutoType(
  ID string,
  name string,
  company string,
  country string
);

CREATE STREAM CDCdata_stream OF AutoType;

CREATE CQ Lookup
INSERT INTO CDCdata_stream
select data[0],data[1],data[2],data[3] from data_stream;

CREATE  TARGET AzureSQLDWHTarget USING AzureSQLDWHWriter  ( 
  ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
  username: 'striim',
  password: 'W3b@ct10n',
   accountname: 'striimqatestdonotdelete',
   Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
   Tables: 'STRIIM.AUTOTEST01',
  uploadpolicy: 'eventcount:1,interval:10s'
 ) 
INPUT FROM CDCdata_stream;

CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM CDCdata_stream;


END APPLICATION Oracle_IRLogWriter;
deploy application Oracle_IRLogWriter;
start Oracle_IRLogWriter;

stop application iteratortester.iteratorapp;
undeploy application iteratortester.iteratorapp;
drop application iteratortester.iteratorapp cascade;

CREATE APPLICATION iteratorapp;

create flow sourceFlow;

CREATE SOURCE JSONAccessLogSource USING FileReader(
  directory:'@TEST-DATA-PATH@',
  wildcard:'iterator.json',
  positionByEOF:'false'
)
parse using JSONParser (
) OUTPUT TO jsonSourceStream;

end flow sourceflow;

-- ******ARRAY LIST****** --
create flow processFlow;

create type cacheType (bankID string key, bankName string);
CREATE cache dsvcache USING FileReader (
directory:'@TEST-DATA-PATH@',
wildcard:'banks.csv',
blocksize: 10240,
positionByEOF:false
)
PARSE USING DSVParser (
header:No,
trimquote:false
) QUERY (keytomap:'bankID') OF cacheType;

CREATE TYPE listType (id integer KEY, bankname string, lst java.util.List);
CREATE TYPE listStoreType (id integer KEY, bankname string, lst java.util.List, lstoflst java.util.List);

CREATE STREAM listStream of listType partition by bankname;

CREATE JUMPING WINDOW listJWindow
OVER listStream
keep 3 rows;

CREATE WINDOW listWindow
OVER listStream
keep 3 rows;

CREATE WACTIONSTORE listStore CONTEXT OF listStoreType EVENT TYPES (listStoreType ) 
@PERSIST-TYPE@

create cq updatelistStream
insert into listStream
select TO_INT(bankID), bankName, makelist(bankID,bankName) as lst 
from dsvcache;

create cq updatelistStore 
insert into listStore
select ID, bankName, lst, makelist(lst,lst) from listStream
LINK SOURCE EVENT;

create stream listTargetStream( str String);

create cq updateListTarget
insert into listTargetStream
select itr
from listStream, iterator(listStream.lst) itr order by cast(itr as java.lang.Comparable);

-- CREATE TARGET listout USING SYSOUT(name:"list") input from listStream;

-- ******JsonNode****** --

--CREATE TYPE jsonType (id integer KEY,  lst com.fasterxml.jackson.databind.JsonNode);
CREATE TYPE jsonType (int integer, bankname string, lst com.fasterxml.jackson.databind.JsonNode key);

CREATE STREAM jsonStream of jsonType partition by bankname;

CREATE JUMPING WINDOW jsonJWindow
OVER jsonStream
keep 3 rows
partition by bankname;

CREATE WINDOW jsonWindow
OVER jsonStream
keep 3 rows;

CREATE WACTIONSTORE jsonStore CONTEXT OF jsonType EVENT TYPES (jsonType ) 
@PERSIST-TYPE@

create cq updateJsonStream
insert into jsonStream
--select TO_INT(MATCH(data[0], '.*\\\\s([0-9]+)')) , makeJSON('[{"x":"a"},{"x":"b","y":"c"},{"y":"c"}]') as lst 
select TO_INT(y.bankID), y.bankName, x.data 
from JSONSourceStream x, dsvcache y;

create cq updateJsonStore 
insert into jsonStore
select * from jsonStream
LINK SOURCE EVENT;

create stream jsonTargetStream( str String);

create cq updateJsonTarget
insert into jsonTargetStream
select to_string(itr.StringJSON)
from jsonStream, iterator(jsonStream.lst) itr order by to_string(itr.StringJSON);
CREATE TARGET jsonout USING SYSOUT(name:"jlist") input from jsonStream;

end flow processflow; 

end application iteratorapp;

--
-- Recovery Test 99
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov99Tester.RecovTest99;
UNDEPLOY APPLICATION Recov99Tester.RecovTest99;
DROP APPLICATION Recov99Tester.RecovTest99 CASCADE;
CREATE APPLICATION RecovTest99 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW BasicComponentTestsFlow;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

-- unpartitioned windows

CREATE WINDOW DataStreamSliding3U
OVER DataStream KEEP 3 ROWS;

CREATE WINDOW DataStreamSliding11U
OVER DataStream KEEP 11 ROWS;

CREATE JUMPING WINDOW DataStreamJumping5U
OVER DataStream KEEP 5 ROWS;

CREATE JUMPING WINDOW DataStreamJumping7U
OVER DataStream KEEP 7 ROWS;

CREATE JUMPING WINDOW DataStream5MinutesU
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6MinutesU
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;


-- partitioned windows

CREATE WINDOW DataStreamSliding3P
OVER DataStream KEEP 3 ROWS
PARTITION BY merchantId;

CREATE WINDOW DataStreamSliding11P
OVER DataStream KEEP 11 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStreamJumping5P
OVER DataStream KEEP 5 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStreamJumping7P
OVER DataStream KEEP 7 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream5MinutesP
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6MinutesP
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;





-- THE TEST OUTPUTS


-- Wactions01 receives data straight through from the Source
-- should contain the exact data emitted
CREATE WACTIONSTORE Wactions_S1 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions01
INSERT INTO Wactions01
SELECT *
FROM DataStream;








-- test test boundaries for jumping/sliding partitioned/unpartitioned aggregate/non-aggregate

-- SLIDING

-- Wactions_SW3UN data goes through an unpartitioned sliding window then a non-aggregate CQ

CREATE WACTIONSTORE Wactions_SW3UN CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_SW3UN
INSERT INTO Wactions_SW3UN
SELECT *
FROM DataStreamSliding3U;


-- Wactions_SW3UN data goes through a partitioned sliding window then a non-aggregate CQ

CREATE WACTIONSTORE Wactions_SW3PN CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_SW3PN
INSERT INTO Wactions_SW3PN
SELECT *
FROM DataStreamSliding3P;


-- Wactions_SW3UN data goes through an unpartitioned sliding window then an aggregate CQ

CREATE WACTIONSTORE Wactions_SW3UA CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_SW3UA
INSERT INTO Wactions_SW3UA
SELECT FIRST(*)
FROM DataStreamSliding3U;


-- Wactions_SW3UN data goes through a partitioned sliding window then an aggregate CQ

CREATE WACTIONSTORE Wactions_SW3PA CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_SW3PA
INSERT INTO Wactions_SW3PA
SELECT FIRST(*)
FROM DataStreamSliding3P;



-- JUMPING


-- Wactions_JW3UN data goes through an unpartitioned jumping window then a non-aggregate CQ

CREATE WACTIONSTORE Wactions_JW5UN CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_JW5UN
INSERT INTO Wactions_JW5UN
SELECT *
FROM DataStreamJumping5U;


-- Wactions_JW5UN data goes through a partitioned sliding jumping then a non-aggregate CQ

CREATE WACTIONSTORE Wactions_JW5PN CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_JW5PN
INSERT INTO Wactions_JW5PN
SELECT *
FROM DataStreamJumping5P;


-- Wactions_JW5UN data goes through an unpartitioned jumping window then an aggregate CQ

CREATE WACTIONSTORE Wactions_JW5UA CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_JW5UA
INSERT INTO Wactions_JW5UA
SELECT FIRST(*)
FROM DataStreamJumping5U;


-- Wactions_JW5UN data goes through a partitioned jumping window then an aggregate CQ

CREATE WACTIONSTORE Wactions_JW5PA CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_JW5PA
INSERT INTO Wactions_JW5PA
SELECT FIRST(*)
FROM DataStreamJumping5P;







END FLOW BasicComponentTestsFlow;










CREATE FLOW ComplexScenariosFlow;





CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;








-- Wactions03 data goes through an unpartitioned jumping window then an aggregate CQ
-- should contain one waction for every 5 events
CREATE WACTIONSTORE Wactions03 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions03
INSERT INTO Wactions03
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStreamJumping5 p;


-- Wactions04 data goes through a size-6 unpartitioned jumping window then an aggregate CQ
-- should contain one waction for every 6 events
CREATE WACTIONSTORE Wactions04 CONTEXT OF WactionType
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data6ToWactios04
INSERT INTO Wactions04
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;





END FLOW ComplexScenariosFlow;






END APPLICATION RecovTest99;

STOP AES;
UNDEPLOY APPLICATION AES;
DROP APPLICATION AES CASCADE;

CREATE APPLICATION AES;


CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity double,
  size long,
  currentDate DateTime);

CREATE source implicitSOurce USING FileReader (
      directory:'@TEST-DATA-PATH@',
      columndelimiter: ',',
      wildcard:'ISdata.csv',
      blocksize: 10240,
      positionByEOF:false
)
PARSE USING DSVParser (
      header:False,
      trimquote:false
) OUTPUT TO CsvStream; 

CREATE TYPE wsType(
  quantity double KEY,
  currentDate DateTime
  );


CREATE STREAM newStream OF Atm;

CREATE CQ newCQ
INSERT INTO newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]), TO_DATE(data[5]) FROM
CsvStream;


CREATE WINDOW win1
OVER newStream
keep within 3 minute;

CREATE STREAM newStream2 of wsType;



CREATE WACTIONSTORE WS1 CONTEXT OF wsType
EVENT TYPES (wsType );


Create cq newCQ2
insert into ws1 (quantity,currentDate)
select quantity, currentDate from newStream;


END APPLICATION AES;
deploy APPLICATION AES;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;


stop OracleToKudu_ExpStore;
undeploy application OracleToKudu_ExpStore;
drop application OracleToKudu_ExpStore cascade;

--drop exceptionstore admin.OracleToKudu_ExceptionStore;
CREATE APPLICATION OracleToKudu use exceptionstore;
Create Source oracSource Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;

CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;



CREATE APPLICATION OracleToKudu_ExpStore;

CREATE TYPE OracleToKudu_ExpStore_CDCStreams_Type  (
 exceptionType java.lang.String,
  action java.lang.String,
  appName java.lang.String,
  entityType java.lang.String,
  entityName java.lang.String,
  className java.lang.String,
  message java.lang.String 
 );

CREATE STREAM OracleToKudu_ExpStore_CDCStreams OF OracleToKudu_ExpStore_CDCStreams_Type;

CREATE CQ OracleToKudu_ReadFromExpStore
INSERT INTO OracleToKudu_ExpStore_CDCStreams
select s.exceptionType,s.action,s.appName,s.entityType,s.entityName,s.className,s.message from admin.OracleToKudu_ExceptionStore [jumping 10 second] s;
        
CREATE OR REPLACE TARGET OracleToKudu_ExpStore_WriteToFileAsJSON USING FileWriter  ( 
  filename: 'expEvent_Kudu.log',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:2,interval:30',
  adapterName: 'FileWriter',
  directory: 'ExpStore_logs',
  rolloverpolicy: 'eventcount:2,interval:30s'
 ) 
FORMAT USING JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter',
  formatterName: 'JSONFormatter',
  jsonMemberDelimiter: '\n',
  jsonobjectdelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true'
 ) 
INPUT FROM OracleToKudu_ExpStore_CDCStreams;
        
END APPLICATION OracleToKudu_ExpStore;

deploy application OracleToKudu_ExpStore;
start OracleToKudu_ExpStore;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW ;

 

CREATE OR REPLACE SOURCE DBSource USING DatabaseReader  ( 
  Username: '@SOURCE_USER@',
  Password_encrypted: false,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SOURCE_PASS@'
 ) 
OUTPUT TO Oracle_ChangeDataStream;


CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1000,Interval:0',
  CommitPolicy: 'EventCount:1000,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

\create Target @TARGET_NAME@ using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (

)
input from @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING Global.FabricDataWarehouseWriter (
  Tables: '',
  StorageAccessDriverType: 'WASBS',
  ConnectionURL: '',
  Username: '',
  AccountAccessKey: '',
  Mode: 'APPENDONLY',
  Password: '',
  uploadpolicy: 'eventcount:1,interval:10s',
  AccountName: '')
INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;;

create Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'Default',
  ProcessCopyBookFileAs: 'SingleEvent',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
  filename: '%@metadata(FileName)%',
  directory: '@DIR@',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter()
INPUT FROM CCBStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@  RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @srcName@ USING OracleReader (
 Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@'
)
OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@ USING Global.DeltaLakeWriter (
  personalAccessToken:'@tgtpassword@',
  hostname:'@tgthostname@',
  stageLocation:'/',
  Mode:'MERGE',
  AuthenticationType: 'PersonalAccessToken',
  Tables:'@srcschema@.@srctable@,@tgtschema@.@tgttable@',
  adapterName:'DeltaLakeWriter',
  personalAccessToken_encrypted:'false',
  optimizedMerge:'false',
  uploadPolicy:'eventcount:1,interval:10s',
  connectionUrl:'@tgturl@',
  IgnorableExceptionCode:'TABLE_NOT_FOUND',
  externalStageType:'DBFSROOT'
)
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop application MySQLAllDataTypes;
undeploy application MySQLAllDataTypes;
drop application MySQLAllDataTypes CASCADE;
create application MySQLAllDataTypes;

CREATE OR REPLACE SOURCE MySQLSource USING MySQLReader  (
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Database:'@DATABASE@',
  Tables: '@SOURCE_TABLES@'
 ) Output To MySQLStream;
 
--create Target t2 using SysOut(name:Foo2) input from MySQLStream; 
CREATE TARGET RedshiftTarget USING RedshiftWriter
	(
	  ConnectionURL: '@TARGET-URL@',
	  Username: '@TARGET-UNAME@',
	  Password: '@TARGET-PASSWORD@',
	  bucketname: '@BUCKETNAME@',
	  accesskeyId: '@ACCESS-KEY-ID@',
	  secretaccesskey: '@SECRET-ACCESS-KEY@',
	  Tables: '@TARGET-TABLES@',
	  uploadpolicy:'eventcount:1,interval:20s',
	  Mode:'incremental'
	) INPUT FROM MySQLStream;
	
END APPLICATION MySQLAllDataTypes;
deploy application MySQLAllDataTypes;
START application MySQLAllDataTypes;

CREATE APPLICATION SourcePosApp;

CREATE SOURCE PosCsvDataSource USING FileReader ( 
  directory: '@TEST-DATA-PATH@', 
  wildcard: 'posdata.csv', 
  positionbyeof: false ) 
PARSE USING DSVParser ( 
  charset: 'UTF-8' )
OUTPUT TO PosCsvStream;

CREATE TARGET PosSourceDump using FileWriter(
  directory: '@FEATURE-DIR@/logs',  
  filename: 'SourcePosAppData',
  rolloverpolicy: 'EventCount:6000000'
   )
FORMAT USING Global.DSVFormatter (
  members: 'data',
  charset: 'UTF-8' ) 
 input from PosCsvStream;

CREATE Source PosHourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt',
  positionByEOF:false )
PARSE USING DSVParser ( 
    charset: 'UTF-8' )
OUTPUT TO PosCacheSource1;

CREATE TARGET PosCacheDump1 using FileWriter(
  directory: '@FEATURE-DIR@/logs',  
  filename: 'SourcePosCacheData1',
  rolloverpolicy: 'EventCount:6000000' ) 
FORMAT USING DSVFormatter (
    members: 'data', 
    charset: 'UTF-8') 
 input from PosCacheSource1;
 
CREATE Source PosNameLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'MerchantNames.csv',
  positionByEOF:false )
PARSE USING DSVParser ( 
    charset: 'UTF-8' )
 OUTPUT TO PosCacheSource2;

CREATE TARGET PosCacheDump2 using FileWriter(
  directory: '@FEATURE-DIR@/logs',  
  filename: 'SourcePosCacheData2',
  rolloverpolicy: 'EventCount:6000000' ) 
FORMAT USING DSVFormatter (
    members: 'data', 
    charset: 'UTF-8')
 input from PosCacheSource2;
 
CREATE Source PosZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  positionByEOF:false ) 
PARSE USING DSVParser ( 
    columndelimiter:'\t',
    charset: 'UTF-8' )
OUTPUT To PosCacheSource3;

CREATE TARGET PosCacheDump3 using FileWriter(
  directory: '@FEATURE-DIR@/logs',  
  filename: 'SourcePosCacheData3',
  rolloverpolicy: 'EventCount:6000000') 
FORMAT USING DSVFormatter (
    members: 'data', 
    charset: 'UTF-8')
INPUT from PosCacheSource3;

END APPLICATION SourcePosApp;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.OJet (
  PrimaryDatabasePassword: '@PRIMARY_PASSWORD@',
  ConnectionURL: '@DOWNSTREAM_URL@',
  PrimaryDatabaseUsername: '@PRIMARY_USER@',
  Password: '@DOWNSTREAM_PASSWORD@',
  DownstreamCaptureMode: 'REAL_TIME',
  DownstreamCapture: true,
  PrimaryDatabaseConnectionURL: '@PRIMARY_URL@',
  Tables: '@SOURCE_TABLES@',
  Username: '@DOWNSTREAM_USER@' )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET_SYS@ USING Global.SysOut (
  name: 'Out' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter (
  ConnectionURL: '@TARGET_URL@',
  Username: '@TARGET_USER@',
  Password: '@TARGET_PASSWORD@',
  CheckPointTable: 'CHKPOINT',
  CommitPolicy: 'EventCount:1',
  Tables: '@TARGET_TABLES@',
  BatchPolicy: 'EventCount:1' )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

--
-- Crash Recovery Test 7 with Jumping window and partitioned on two node cluster with one agent
-- Bert Hashemi, WebAction, Inc.
--
-- S -> KafkaStream -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION N2S2CR7Tester.N2S2CRTest7;
UNDEPLOY APPLICATION N2S2CR7Tester.N2S2CRTest7;
DROP APPLICATION N2S2CR7Tester.N2S2CRTest7 CASCADE;
CREATE APPLICATION N2S2CRTest7 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest7;

CREATE SOURCE CsvSourceN2S2CRTest7 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;


CREATE TYPE CsvDataTypeN2S2CRTest7 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream of CsvDataTypeN2S2CRTest7 using KafkaProps;

CREATE CQ TransferToKafka
INSERT INTO KafkaCsvStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

END FLOW DataAcquisitionN2S2CRTest7;





CREATE FLOW DataProcessingN2S2CRTest7;

CREATE STREAM DataStream OF CsvDataTypeN2S2CRTest7 PARTITION BY merchantId;

CREATE CQ CsvToDataN2S2CRTest7
INSERT INTO DataStream
SELECT
    *
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsN2S2CRTest7 CONTEXT OF CsvDataTypeN2S2CRTest7
EVENT TYPES ( CsvDataTypeN2S2CRTest7 )
@PERSIST-TYPE@

CREATE CQ DataToWactionN2S2CRTest7
INSERT INTO WactionsN2S2CRTest7
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN2S2CRTest7;

END APPLICATION N2S2CRTest7;

STOP banker.bankApp;
UNDEPLOY APPLICATION banker.bankApp;
DROP APPLICATION banker.bankApp cascade;

CREATE APPLICATION bankApp;


CREATE source wsSource USING FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;


CREATE TYPE wsData
(
bankID Integer KEY,
bankName String
);


CREATE WACTIONSTORE oneWS
CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@

CREATE CQ oneWSCQ
INSERT INTO oneWS
SELECT TO_INT(data[0]),data[1] FROM QaStream
LINK SOURCE EVENT;

END APPLICATION bankApp;
deploy application bankapp;
start application bankapp;

STOP APPLICATION orrs;
UNDEPLOY APPLICATION orrs;
DROP APPLICATION orrs CASCADE;
CREATE APPLICATION orrs;
Create Source oraSource1 Using DatabaseReader
(
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'QATEST.ORACLETOREDSHIFTIL1;QATEST.ORACLETOREDSHIFTIL2',
 FilterTransactionBoundaries:true,
 FetchSize:1000
) Output To LCRStream1;

Create Source oraSource2 Using DatabaseReader
(
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'QATEST.ORACLETOREDSHIFTIL3;QATEST.ORACLETOREDSHIFTIL4',
 FilterTransactionBoundaries:true,
 FetchSize:1000
) Output To LCRStream2;

CREATE TARGET RSTarget USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: 'access_key',
	  --secretaccesskey: 'secret_access',
	  Tables: 'QATEST.%,QATEST.%',
	   S3IAMRole:'@IAMROLE@',
	  uploadpolicy:'eventcount:1000,interval:1m'
	) INPUT FROM LCRStream1;

CREATE TARGET RSTarget2 USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: 'access_key',
	  --secretaccesskey: 'secret_access',
	  Tables: 'QATEST.%,QATEST.%',
	   S3IAMRole:'@IAMROLE@',
	  Tables: 'QATEST.%,QATEST.%',
	  uploadpolicy:'eventcount:1000,interval:1m'
	) INPUT FROM LCRStream2;

END APPLICATION orrs;
DEPLOY APPLICATION orrs;
START APPLICATION orrs;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using S3Writer(
    bucketname:'@BUCKET@',
   objectname:'upgradeData.csv',
   foldername:'upgradefolder',
  uploadpolicy:'EventCount : 10000,Interval :1m '
)
format using DSVFormatter (
)
input from @STREAM@;

end flow @APPNAME@_serverflow;

end application @APPNAME@;

-----------------------------------

stop application TargetServerApp3;
undeploy application TargetServerApp3;
drop application TargetServerApp3 cascade;


-- another app consuming from app running in agent

CREATE APPLICATION TargetServerApp3;
create flow flow4;

CREATE TARGET T4 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp3_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')
 FORMAT USING DSVFormatter ()
 INPUT FROM CsvStream;
end flow flow4;

END APPLICATION TargetServerApp3;
deploy application TargetServerApp3 with flow4 in default;

--
-- Recovery Test T20
-- Nicholas Keene, WebAction, Inc.
--
-- Snum -> CQ -> WS
--


UNDEPLOY APPLICATION NameT20.T20;
DROP APPLICATION NameT20.T20 CASCADE;
CREATE APPLICATION T20;




CREATE FLOW DataAcquisitionT20;


CREATE SOURCE CsvSourceT20 USING NumberSource ( 
  lowValue: '1',
  highValue: '1003',
  delayMillis: '10',
  delayNanos: '0',
  repeat: 'false'
 ) 
OUTPUT TO OutputStreamT20;


END FLOW DataAcquisitionT20;




CREATE FLOW DataProcessingT20;


Create Target OutputTargetT20
Using Sysout (name: 'OutputTargetT20')
Input From OutputStreamT20;


END FLOW DataProcessingT20;



END APPLICATION T20;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
directory:'@FEATURE-DIR@/logs/',
filename:'PosData',
rolloverpolicy:'EventCount:5000000,Interval:60s'
)
format using DSVFormatter (

)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizePosDataDefault_actual.log') input from TypedCSVStream;

end application DSV;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src1 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;


CREATE OR REPLACE TARGET @APP_NAME@_tgt1 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt2 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt3 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt4 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt5 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

--
-- Recovery Test 20 with two sources going to one wactionstore
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> CQ1 -> WS
-- S2 -> CQ2 -> WS
--

STOP Recov20Tester.RecovTest20;
UNDEPLOY APPLICATION Recov20Tester.RecovTest20;
DROP APPLICATION Recov20Tester.RecovTest20 CASCADE;
CREATE APPLICATION RecovTest20 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions1
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ InsertWactions2
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream2;

END APPLICATION RecovTest20;

-- The PosAppAgent sample application demonstrates how a credit card
-- payment processor might use Striim to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.


CREATE APPLICATION PosAppAgent;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosAppAgent application.

-- source CsvAgentDataSource

CREATE FLOW AgentFlow;

CREATE source CsvAgentDataSource USING FileReader (
  directory:'/opt/striim/Samples/PosApp/appData',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvAgentStream;

END FLOW AgentFlow;

-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells Striim that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells Striim to start reading
-- from the beginning of the file. (In a real-world application 
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvAgentStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvAgentToPosDataCq

CREATE FLOW ProcessFlow;

CREATE CQ CsvAgentToPosDataCq
INSERT INTO PosDataAgentStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvAgentStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP.
--
-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as 
-- the types to be used in PosDataAgentStream, which is created automatically.

-- The DATETIME field from the source is converted to both a dateTime 
-- value, used as the event timestamp by the application, and (via the 
-- function) an integer hourValue, which is used to look up 
-- historical hourly averages from the HourlyAgentAveLookup cache, 
-- discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)
-- 17 (hourValue)
-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateAgentOnly
--
-- The AgentPosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAgentAveLookup cache. (Aggregate functions cannot be used and 
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAgentAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW AgentPosData5Minutes
OVER PosDataAgentStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantAgentHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAgentAveLookup using FileReader (
  directory: 'Samples/PosApp/appData',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'merchantId') OF MerchantAgentHourlyAve;

CREATE TYPE MerchantTxRateAgent(
  merchantId String KEY,
  zip String,
  startTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateAgentOnlyStream OF MerchantTxRateAgent PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateAgentOnly
INSERT INTO MerchantTxRateAgentOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM AgentPosData5Minutes p, HourlyAgentAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAgentAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateAgentWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE STREAM MerchantTxRateAgentWithStatusStream OF MerchantTxRateAgent;

CREATE CQ GenerateMerchantTxRateAgentWithStatus
INSERT INTO MerchantTxRateAgentWithStatusStream
SELECT merchantId,
       zip,
       startTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateAgentOnlyStream;


-- WAction store MerchantActivityAgent
--
-- The following group of statements create and populate the MerchantActivityAgent
-- WAction store. Data from the MerchantTxRateAgentWithStatusStream is enhanced
-- with merchant details from NameLookupAgent cache and with latitude and longitude
-- values from the USAddressDataAgent cache.

CREATE TYPE MerchantActivityAgentContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivityAgent CONTEXT OF MerchantActivityAgentContext
EVENT TYPES ( MerchantTxRateAgent )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );


CREATE TYPE MerchantAgentNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressDataAgent(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookupAgent using FileReader (
  directory:'Samples/PosApp/appData',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false,
  trimwhitespace:true
) 
QUERY(keytomap:'merchantId') OF MerchantAgentNameData;

CREATE CACHE ZipLookupAgent using FileReader (
  directory: 'Samples/PosApp/appData',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'zip') OF USAddressDataAgent;


CREATE CQ GenerateWactionAgentContext
INSERT INTO MerchantActivityAgent
SELECT  m.merchantId,
        m.startTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateAgentWithStatusStream m, NameLookupAgent n, ZipLookupAgent z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

-- CQ GenerateAgentAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertAgentStream OF Global.AlertEvent;

CREATE CQ GenerateAgentAlerts
INSERT INTO AlertAgentStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateAgentWithStatusStream m, NameLookupAgent n
WHERE m.merchantId = n.merchantId;

CREATE SUBSCRIPTION AgentAlertSub USING WebAlertAdapter( ) INPUT FROM AlertAgentStream;

END FLOW ProcessFlow;

END APPLICATION PosAppAgent;

DEPLOY APPLICATION PosAppAgent with AgentFlow in AGENTS, ProcessFlow in default;

-- CREATE DASHBOARD USING "Samples/PosApp/PosAppDashboard.json";

stop ROLLUPMON_CDC;
undeploy application ROLLUPMON_CDC;
alter application ROLLUPMON_CDC;
CREATE or replace FLOW ROLLUPMON_CDC_flow;
Create or replace Source ROLLUPMON_CDC_Oraclesrc Using oraclereader(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',
 Tables:'QATEST.ROLLUPMON_TABLE1;QATEST.ROLLUPMON_TABLE2;QATEST.ROLLUPMON_TABLE3;QATEST.ROLLUPMON_TABLE4;QATEST.ROLLUPMON_TABLE5',
 Fetchsize:1000,
 connectionRetryPolicy:'maxRetries=4',
 _h_fetchexactrowcount: 'true'
)
Output To ROLLUPMON_CDC_OrcStrm;
END FLOW ROLLUPMON_CDC_flow;
alter application ROLLUPMON_CDC recompile;
DEPLOY APPLICATION ROLLUPMON_CDC;
start application ROLLUPMON_CDC;

CREATE APPLICATION @AppName@;

CREATE SOURCE @AppName@_FileReaderSource USING FileReader ( 
  wildcard: 'posdata.csv', 
  positionByEOF: false, 
  blocksize: 10240, 
  directory: 'Samples/PosApp/appData' ) 
PARSE USING DSVParser ( 
  trimquote: false, 
  header: 'Yes' ) 
OUTPUT TO @AppName@_CsvStream;

CREATE TYPE n1 (
 c1 java.lang.Integer KEY,
 _plan java.lang.String AS "plan");

CREATE OR REPLACE TARGET @AppName@_NullWriterTrg USING NullWriter ( 
 ) 
INPUT FROM @AppName@_CsvStream;

END APPLICATION @AppName@;
DEPLOY APPLICATION @AppName@;
START APPLICATION @AppName@;

stop application Postgres_To_PostgresApp;
undeploy application Postgres_To_PostgresApp;
drop application Postgres_To_PostgresApp cascade;

CREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE Postgres_Src1 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_1',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename0'
 ) 
OUTPUT TO Change_Data_Stream ;

CREATE OR REPLACE SOURCE Postgres_Src2 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_2',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename1'
 ) 
OUTPUT TO Change_Data_Stream ;


CREATE OR REPLACE SOURCE Postgres_Src3 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_3',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename2'
 ) 
OUTPUT TO Change_Data_Stream ;

CREATE OR REPLACE SOURCE Postgres_Src4 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot_4',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.tablename3'
 ) 
OUTPUT TO Change_Data_Stream ;


CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt1 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target0;public.tablename1, public.target0;public.tablename2, public.target0;public.tablename3, public.target0;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt2 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target1;public.tablename1, public.target1;public.tablename2, public.target1;public.tablename3, public.target1;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt3 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target2;public.tablename1, public.target2;public.tablename2, public.target2;public.tablename3, public.target2;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET PostgreSQL_Tgt4 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.tablename0, public.target3;public.tablename1, public.target3;public.tablename2, public.target3;public.tablename3, public.target3;',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;


end application Postgres_To_PostgresApp;
deploy application Postgres_To_PostgresApp;
start Postgres_To_PostgresApp;

create or replace Target Quiesce_orcl_kwTARGET1 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'Quiesce_orcl_kw_dsv_sync_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(COMMIT_TIMESTAMP)',
Mode:'sync',
KafkaConfig: 'batch.size=1048576;linger.ms=300000;')
FORMAT USING dsvFormatter ()
input from Quiesce_orcl_kwss;

CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatc (
  StartPosition: '@startPosition@',
  ConnectionURL: '@sourceURL@',
  Username: '@userName@',
  Tables: '@tables@',
  CheckColumn: '@checkColum@',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  Password: '@password@',
  Password_encrypted: 'false',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

stop DBRTOCW;
 undeploy application DBRTOCW;
 drop application DBRTOCW cascade;
 CREATE APPLICATION DBRTOCW;

 Create Source MSSQLSource Using MSSqlReader
(
Username:'qatest',
Password:'w@ct10n',
DatabaseName:'qatest',
ConnectionURL:'10.77.61.30:1433',
Tables:'qatest.MssqlTocql_Alldatatypes',
ConnectionPoolSize:1,
Compression:'true'
)
OUTPUT TO Oracle_ChangeDataStream;

 CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM Oracle_ChangeDataStream;

 create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

 END APPLICATION DBRTOCW;

 deploy application DBRTOCW in default;

 start DBRTOCW;

DROP TYPE MerchantActivityContext;
DROP TYPE MerchantTxRate;
DROP WACTIONSTORE MerchantActivity;

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count int,
  HourlyAve int,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count int,
  totalAmount double,
  hourlyAve int,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);

-- Loading WACTIONSTORE MerchantActivity to be accessed by PosAppWS.tql

CREATE WACTIONSTORE MerchantActivity CONTEXT OF DS.MerchantActivityContext
EVENT TYPES ( DS.MerchantTxRate )
@PERSIST-TYPE@

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING AvroFormatter (
schemaFileName: '@SCHEMAFILE@'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING DSVFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

END APPLICATION @APPNAME@;

create flow serverFlow;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@;

end flow serverFlow;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;;

Create Source OracleSource1 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source OracleSource2 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source OracleSource3 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source OracleSource4 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source OracleSource5 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

create target AzureTarget using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',  
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;
END APPLICATION ADW;
deploy application ADW;
start application ADW;

--
-- Crash Recovery Test 4 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS
-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS
--

STOP APPLICATION N2S2CR4Tester.N2S2CRTest4;
UNDEPLOY APPLICATION N2S2CR4Tester.N2S2CRTest4;
DROP APPLICATION N2S2CR4Tester.N2S2CRTest4 CASCADE;
CREATE APPLICATION N2S2CRTest4 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest4;

CREATE SOURCE CsvSourceN2S2CRTest4 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN2S2CRTest4;

CREATE FLOW DataProcessingN2S2CRTest4;

CREATE TYPE CsvDataN2S2CRTest4 (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionTypeN2S2CRTest4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream OF CsvDataN2S2CRTest4;

CREATE CQ CsvToDataN2S2CRTest4
INSERT INTO DataStream
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN2S2CRTest4 CONTEXT OF WactionTypeN2S2CRTest4
EVENT TYPES ( CsvDataN2S2CRTest4 )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO WactionsN2S2CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO WactionsN2S2CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END FLOW DataProcessingN2S2CRTest4;

END APPLICATION N2S2CRTest4;

stop Application zoneDateTime_CaseSensistive_CQ;
undeploy application zoneDateTime_CaseSensistive_CQ;
drop application zoneDateTime_CaseSensistive_CQ cascade;

create application zoneDateTime_CaseSensistive_CQ;

CREATE OR REPLACE SOURCE csvinput USING FileReader  ( 
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '@ResourceDirectory@',
  skipbom: true,
  wildcard: 'ZonedDateTime_data.csv'
 ) 
 PARSE USING DSVParser  ( 
  _h_ReturnDateTimeAs: 'ZonedDateTime'
 ) 
OUTPUT TO csvinput_out ;

CREATE  TYPE cq_zonedtime_out_Type  ( 
a java.lang.Integer , 
b string,
c string,
d string,
e string,
f string
--b java.time.ZonedDateTime , 
--c java.time.ZonedDateTime ,
--d java.time.ZonedDateTime , 
--e java.time.ZonedDateTime ,
--f java.time.ZonedDateTime   
 );

CREATE STREAM cq_zonedtime_out OF cq_zonedtime_out_Type;

CREATE OR REPLACE   CQ cq_zonedtime 
INSERT INTO cq_zonedtime_out
SELECT TO_INT(data[0]) as a,
TO_STRING(TO_ZONEDDATETIME(data[1],"dd-MMM-yy hh.mm.ss.SSSSSSSSS a"),"dd-MMM-yy hh.mm.ss.SSSSSSSSS a") as b,
TO_STRING(TO_ZONEDDATETIME(data[2], "dd-MMM-yy hh.mm.ss.SSSSSSSSS a z"),"dd-MMM-yy hh.mm.ss.SSSSSSSSS a z") as c,
TO_STRING(TO_ZONEDDATETIME(data[3], "EEEEE dd-MMM-yy hh.mm.ss.SSSSSSSSS a"),"EEEEE dd-MMM-yy hh.mm.ss.SSSSSSSSS a") as d,
TO_STRING(TO_ZONEDDATETIME(data[4], "E dd-MMM-yy hh.mm.ss.SSSSSSSSS a z"),"E dd-MMM-yy hh.mm.ss.SSSSSSSSS a z") as e,
TO_STRING(TO_ZONEDDATETIME(data[5], "EEEE dd-MMMM-yy hh.mm.ss.SSSSSSSSS a z"),"EEEE dd-MMMM-yy hh.mm.ss.SSSSSSSSS a z") as f
FROM csvinput_out c;

create or replace target sample_out using Sysout(
  name:'Foo'
)INPUT FROM cq_zonedtime_out;

create Target File_tgt using FileWriter(
  filename:'ZonedDateTime_ActualOutput.csv',
  directory:'@OutputDirectory@',
  rolloverpolicy:'eventcount:1000'
)
format using DSVFormatter (

)
input from cq_zonedtime_out;

END APPLICATION zoneDateTime_CaseSensistive_CQ;

deploy application zoneDateTime_CaseSensistive_CQ;

start application zoneDateTime_CaseSensistive_CQ;

stop application CDCTester.CDCTest;
undeploy application CDCTester.CDCTest;
drop application CDCTester.CDCTest cascade;

create application CDCTest;

Create Source Rac11g Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:$Tables,
 FetchSize:$FetchSize,
 QueueSize:$QueueSize

)
Output To LCRStream;


end application CDCTest;
deploy application CDCTest;

--
-- Recovery Test 25 with two sources, two jumping count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5W -> CQ1 -> WS
--   S2 -> Jc6W -> CQ2 -> WS
--

STOP KStreamRecov25Tester.KStreamRecovTest25;
UNDEPLOY APPLICATION KStreamRecov25Tester.KStreamRecovTest25;
DROP APPLICATION KStreamRecov25Tester.KStreamRecovTest25 CASCADE;
DROP USER KStreamRecov25Tester;
DROP NAMESPACE KStreamRecov25Tester CASCADE;
CREATE USER KStreamRecov25Tester IDENTIFIED BY KStreamRecov25Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov25Tester;
CONNECT KStreamRecov25Tester KStreamRecov25Tester;

CREATE APPLICATION KStreamRecovTest25 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION KStreamRecovTest25;

--
-- Kafka Stream with KryoParser and Kafka Reader Recovery Test 1
-- Bert Hashemi and Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> KS -> WS
-- S -> K -> CQ -> WS

STOP KStreamKryoParser1Tester.KStreamKryoParserTest1;
UNDEPLOY APPLICATION KStreamKryoParser1Tester.KStreamKryoParserTest1;
DROP APPLICATION KStreamKryoParser1Tester.KStreamKryoParserTest1 CASCADE;
DROP USER KStreamKryoParser1Tester;
DROP NAMESPACE KStreamKryoParser1Tester CASCADE;
CREATE USER KStreamKryoParser1Tester IDENTIFIED BY KStreamKryoParser1Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamKryoParser1Tester;
CONNECT KStreamKryoParser1Tester KStreamKryoParser1Tester;

CREATE APPLICATION KStreamKryoParserTest1 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'1');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE or REPLACE TYPE KafkaCsvStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF KafkaCsvStreamType 
EVENT TYPES ( KafkaCsvStreamType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE or REPLACE TYPE KafkaStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

--CREATE STREAM KafkaTypedStream OF KafkaStreamType;

CREATE STREAM KafkaStream OF Global.waevent;

CREATE SOURCE KafkaSource USING KafkaReader Version '0.8.0'
(
        brokerAddress:'localhost:9092',
        Topic:'KStreamKryoParser1Tester_KafkaCsvStream',
        PartitionIDList:'0',
        startOffset:0
)
PARSE USING StriimParser ()
OUTPUT TO KafkaStream;

CREATE WACTIONSTORE KRWactions CONTEXT OF KafkaStreamType
EVENT TYPES ( KafkaStreamType )
@PERSIST-TYPE@

CREATE CQ KRInsertWactions
INSERT INTO KRWactions
SELECT TO_STRING(data[1]) as merchantId,
    TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
    TO_DOUBLE(data[7]) as amount,
    TO_STRING(data[10]) as city 
FROM KafkaStream;

/*
CREATE CQ CQ2KafkaTypedStream
INSERT INTO KafkaTypedStream
SELECT TO_STRING(data[1]) as merchantId,
    TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
    TO_DOUBLE(data[7]) as amount,
    TO_STRING(data[10]) as city 
FROM KafkaStream;
*/

END APPLICATION KStreamKryoParserTest1;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
filename:'',
directory:'@FEATURE-DIR@/logs/',
    sequence:'00',
rolloverpolicy:'eventcount:200,sequence:00'
)
format using DSVFormatter (

)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetEventCount_actual.log') input from TypedCSVStream;

end application DSV;

-- Creating a namespace ensures there won't be conflicts with the regular version of
-- PosApp. The only difference between this version and the regular version is
-- that the CQ that parses the source stream includes a PAUSE clauses that introduces a
-- 40-millisecond pause after each event is read, simulating the way the dashboard would
-- work with real-time data.
Stop PosAppOracle.PosAppOracle;
undeploy application PosAppOracle.PosAppOracle;
drop application PosAppOracle.PosAppOracle cascade;


-- The PosApp sample application demonstrates how a credit card
-- payment processor might use WebAction to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.

CREATE Application PosAppOracle;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING OracleReader (
OnlineCatalog:true,
FetchSize:1000,
QueueSize:2048,
CommittedTransactions:false,
Compression:false,
Username:'@LOGMINER-UNAME@',
Password:'@LOGMINER-PASSWORD@',
ConnectionURL:'@LOGMINER-URL@',
Tables:'@LOGMINER-SCHEMA@.POSDATA',
OnlineCatalog:true
) output to CsvStream;


-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells WebAction that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells WebAction to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvToPosData
--
-- A stream's type must be declared before the stream, and a CQ's
-- output stream must be defined before the CQ. Hence type-stream-CQ
-- sequences like the following are very common.

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATE(data[4]) as dateTime,
       DHOURS(TO_DATE(data[4])) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       To_String(data[9]) as zip
FROM CsvStream c;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP. These correspond to the merchantId,
-- dateTime, hourValue, amount, and zip fields in PosDataStream, as
-- defined by the PosData type.
--
-- The DATETIME field from the source is converted to both a DateTime
-- value, used as the event timestamp by the application, and an int,
-- which is used to look up historical hourly averages from the
-- HourlyAveLookup cache, discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)

-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateOnly
--
-- The PosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue int,
  hourlyAve int
);

CREATE CACHE HourlyAveLookup using DatabaseReader (
        ConnectionURL:'@READER-URL@',
        Username:'@READER-UNAME@',
        Password:'@READER-PASSWORD@',
        Table:'@READER-SCHEMA@.HOURLYDATA',
        FetchSize:12000
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId as merchantId,
       p.zip as zip,
       FIRST(p.dateTime) as startingTime,
       COUNT(p.merchantId) as count,
       SUM(p.amount) as totalAmount,
       l.hourlyAve/12 as hourlyAve,
       l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END as upperLimit,
       l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END as lowerLimit,
       '<NOTSET>' as category,
       '<NOTSET>' as status
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId as merchantId,
       zip as zip,
       startingTime as startingTime,
       count as count,
       totalAmount as totalAmount,
       hourlyAve as hourlyAve,
       upperLimit as upperLimit,
       lowerLimit as lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END as category,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END as status
FROM MerchantTxRateOnlyStream;


-- WAction store MerchantActivity
--
-- The following group of statements create and populate the MerchantActivity
-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced
-- with merchant details from NameLookup cache and with latitude and longitude
-- values from the USAddressData cache.

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count int,
  HourlyAve int,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);
CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count int,
  totalAmount double,
  hourlyAve int,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
@PERSIST-TYPE@

CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using DatabaseReader (
        ConnectionURL:'@READER-URL@',
        Username:'@READER-UNAME@',
        Password:'@READER-PASSWORD@',
        Table:'@READER-SCHEMA@.MERCHANTNAMES',
        FetchSize:12000
) QUERY (keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using DatabaseReader (
        ConnectionURL:'@READER-URL@',
        Username:'@READER-UNAME@',
        Password:'@READER-PASSWORD@',
        Table:'@READER-SCHEMA@.USADDRESSES',
        FetchSize:12000
) QUERY (keytomap:'zip') OF USAddressData;

CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;


-- CQ GenerateAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;


-- The following statement loads visualization (Dashboard) settings
-- from a file.


CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;


END APPLICATION PosAppOracle;

STOP APPLICATION testApp;
UNDEPLOY APPLICATION testApp;
DROP APPLICATION testApp CASCADE;
-- DROP EXCEPTIONSTORE testApp_exceptionstore;

CREATE APPLICATION testApp WITH ENCRYPTION RECOVERY 10 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE OR REPLACE SOURCE testApp_Source USING OracleReader  (
  Username:'qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
  OnlineCatalog:true,
  FetchSize:'1',
  Tables: 'QATEST.sourceTable'
  ) OUTPUT TO testApp_Stream  ;

CREATE OR REPLACE TARGET testApp_Target USING BigQueryWriter  (
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  projectId:'striimqa-214712',
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:5,Interval:120',
  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', 
  AllowQuotedNewLines: 'false', 
  adapterName: 'BigQueryWriter', 
  optimizedMerge: 'true', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', 
  StandardSQL: 'true', 
  QuoteCharacter: '\"', 
  Tables: 'QATEST.sourceTable,transOption_test.targettable',
  Mode: 'MERGE',
  StandardSQL: 'true',
  _h_TransportOptions:'connectionTimeout=30s, readTimeout=12s',
  QuoteCharacter: '\"'
  ) INPUT FROM testApp_Stream;

CREATE OR REPLACE TARGET testApp_SysOut USING Global.SysOut (name: 'wa') INPUT FROM testApp_Stream;

END APPLICATION testApp;
DEPLOY APPLICATION testApp;
START testApp;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW ;

CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: 'QATEST.orac_1000COL',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:60',
  CommitPolicy: 'EventCount:1,Interval:60',
  ConnectionURL: '@TARGET_URL@',
  Tables: 'QATEST.orac_1000COL,test.cassandra_1500col columnmap(field1=f1,field2=f2,field3=f3,field4=f4,field5=f5,field6=f6,field7=f7,field8=f8,field9=f9,field10=f10,field11=f11,field12=f12,field13=f13,field14=f14,field15=f15,field16=f16,field17=f17,field18=f18,field19=f19,field20=f20,field21=f21,field22=f22,field23=f23,field24=f24,field25=f25,field26=f26,field27=f27,field28=f28,field29=f29,field30=f30,field31=f31,field32=f32,field33=f33,field34=f34,field35=f35,field36=f36,field37=f37,field38=f38,field39=f39,field40=f40,field41=f41,field42=f42,field43=f43,field44=f44,field45=f45,field46=f46,field47=f47,field48=f48,field49=f49,field50=f50,field51=f51,field52=f52,field53=f53,field54=f54,field55=f55,field56=f56,field57=f57,field58=f58,field59=f59,field60=f60,field61=f61,field62=f62,field63=f63,field64=f64,field65=f65,field66=f66,field67=f67,field68=f68,field69=f69,field70=f70,field71=f71,field72=f72,field73=f73,field74=f74,field75=f75,field76=f76,field77=f77,field78=f78,field79=f79,field80=f80,field81=f81,field82=f82,field83=f83,field84=f84,field85=f85,field86=f86,field87=f87,field88=f88,field89=f89,field90=f90,field91=f91,field92=f92,field93=f93,field94=f94,field95=f95,field96=f96,field97=f97,field98=f98,field99=f99,field100=f100,field101=f101,field102=f102,field103=f103,field104=f104,field105=f105,field106=f106,field107=f107,field108=f108,field109=f109,field110=f110,field111=f111,field112=f112,field113=f113,field114=f114,field115=f115,field116=f116,field117=f117,field118=f118,field119=f119,field120=f120,field121=f121,field122=f122,field123=f123,field124=f124,field125=f125,field126=f126,field127=f127,field128=f128,field129=f129,field130=f130,field131=f131,field132=f132,field133=f133,field134=f134,field135=f135,field136=f136,field137=f137,field138=f138,field139=f139,field140=f140,field141=f141,field142=f142,field143=f143,field144=f144,field145=f145,field146=f146,field147=f147,field148=f148,field149=f149,field150=f150,field151=f151,field152=f152,field153=f153,field154=f154,field155=f155,field156=f156,field157=f157,field158=f158,field159=f159,field160=f160,field161=f161,field162=f162,field163=f163,field164=f164,field165=f165,field166=f166,field167=f167,field168=f168,field169=f169,field170=f170,field171=f171,field172=f172,field173=f173,field174=f174,field175=f175,field176=f176,field177=f177,field178=f178,field179=f179,field180=f180,field181=f181,field182=f182,field183=f183,field184=f184,field185=f185,field186=f186,field187=f187,field188=f188,field189=f189,field190=f190,field191=f191,field192=f192,field193=f193,field194=f194,field195=f195,field196=f196,field197=f197,field198=f198,field199=f199,field200=f200,field201=f201,field202=f202,field203=f203,field204=f204,field205=f205,field206=f206,field207=f207,field208=f208,field209=f209,field210=f210,field211=f211,field212=f212,field213=f213,field214=f214,field215=f215,field216=f216,field217=f217,field218=f218,field219=f219,field220=f220,field221=f221,field222=f222,field223=f223,field224=f224,field225=f225,field226=f226,field227=f227,field228=f228,field229=f229,field230=f230,field231=f231,field232=f232,field233=f233,field234=f234,field235=f235,field236=f236,field237=f237,field238=f238,field239=f239,field240=f240,field241=f241,field242=f242,field243=f243,field244=f244,field245=f245,field246=f246,field247=f247,field248=f248,field249=f249,field250=f250,field251=f251,field252=f252,field253=f253,field254=f254,field255=f255,field256=f256,field257=f257,field258=f258,field259=f259,field260=f260,field261=f261,field262=f262,field263=f263,field264=f264,field265=f265,field266=f266,field267=f267,field268=f268,field269=f269,field270=f270,field271=f271,field272=f272,field273=f273,field274=f274,field275=f275,field276=f276,field277=f277,field278=f278,field279=f279,field280=f280,field281=f281,field282=f282,field283=f283,field284=f284,field285=f285,field286=f286,field287=f287,field288=f288,field289=f289,field290=f290,field291=f291,field292=f292,field293=f293,field294=f294,field295=f295,field296=f296,field297=f297,field298=f298,field299=f299,field300=f300,field301=f301,field302=f302,field303=f303,field304=f304,field305=f305,field306=f306,field307=f307,field308=f308,field309=f309,field310=f310,field311=f311,field312=f312,field313=f313,field314=f314,field315=f315,field316=f316,field317=f317,field318=f318,field319=f319,field320=f320,field321=f321,field322=f322,field323=f323,field324=f324,field325=f325,field326=f326,field327=f327,field328=f328,field329=f329,field330=f330,field331=f331,field332=f332,field333=f333,field334=f334,field335=f335,field336=f336,field337=f337,field338=f338,field339=f339,field340=f340,field341=f341,field342=f342,field343=f343,field344=f344,field345=f345,field346=f346,field347=f347,field348=f348,field349=f349,field350=f350,field351=f351,field352=f352,field353=f353,field354=f354,field355=f355,field356=f356,field357=f357,field358=f358,field359=f359,field360=f360,field361=f361,field362=f362,field363=f363,field364=f364,field365=f365,field366=f366,field367=f367,field368=f368,field369=f369,field370=f370,field371=f371,field372=f372,field373=f373,field374=f374,field375=f375,field376=f376,field377=f377,field378=f378,field379=f379,field380=f380,field381=f381,field382=f382,field383=f383,field384=f384,field385=f385,field386=f386,field387=f387,field388=f388,field389=f389,field390=f390,field391=f391,field392=f392,field393=f393,field394=f394,field395=f395,field396=f396,field397=f397,field398=f398,field399=f399,field400=f400,field401=f401,field402=f402,field403=f403,field404=f404,field405=f405,field406=f406,field407=f407,field408=f408,field409=f409,field410=f410,field411=f411,field412=f412,field413=f413,field414=f414,field415=f415,field416=f416,field417=f417,field418=f418,field419=f419,field420=f420,field421=f421,field422=f422,field423=f423,field424=f424,field425=f425,field426=f426,field427=f427,field428=f428,field429=f429,field430=f430,field431=f431,field432=f432,field433=f433,field434=f434,field435=f435,field436=f436,field437=f437,field438=f438,field439=f439,field440=f440,field441=f441,field442=f442,field443=f443,field444=f444,field445=f445,field446=f446,field447=f447,field448=f448,field449=f449,field450=f450,field451=f451,field452=f452,field453=f453,field454=f454,field455=f455,field456=f456,field457=f457,field458=f458,field459=f459,field460=f460,field461=f461,field462=f462,field463=f463,field464=f464,field465=f465,field466=f466,field467=f467,field468=f468,field469=f469,field470=f470,field471=f471,field472=f472,field473=f473,field474=f474,field475=f475,field476=f476,field477=f477,field478=f478,field479=f479,field480=f480,field481=f481,field482=f482,field483=f483,field484=f484,field485=f485,field486=f486,field487=f487,field488=f488,field489=f489,field490=f490,field491=f491,field492=f492,field493=f493,field494=f494,field495=f495,field496=f496,field497=f497,field498=f498,field499=f499,field500=f500,field501=f501,field502=f502,field503=f503,field504=f504,field505=f505,field506=f506,field507=f507,field508=f508,field509=f509,field510=f510,field511=f511,field512=f512,field513=f513,field514=f514,field515=f515,field516=f516,field517=f517,field518=f518,field519=f519,field520=f520,field521=f521,field522=f522,field523=f523,field524=f524,field525=f525,field526=f526,field527=f527,field528=f528,field529=f529,field530=f530,field531=f531,field532=f532,field533=f533,field534=f534,field535=f535,field536=f536,field537=f537,field538=f538,field539=f539,field540=f540,field541=f541,field542=f542,field543=f543,field544=f544,field545=f545,field546=f546,field547=f547,field548=f548,field549=f549,field550=f550,field551=f551,field552=f552,field553=f553,field554=f554,field555=f555,field556=f556,field557=f557,field558=f558,field559=f559,field560=f560,field561=f561,field562=f562,field563=f563,field564=f564,field565=f565,field566=f566,field567=f567,field568=f568,field569=f569,field570=f570,field571=f571,field572=f572,field573=f573,field574=f574,field575=f575,field576=f576,field577=f577,field578=f578,field579=f579,field580=f580,field581=f581,field582=f582,field583=f583,field584=f584,field585=f585,field586=f586,field587=f587,field588=f588,field589=f589,field590=f590,field591=f591,field592=f592,field593=f593,field594=f594,field595=f595,field596=f596,field597=f597,field598=f598,field599=f599,field600=f600,field601=f601,field602=f602,field603=f603,field604=f604,field605=f605,field606=f606,field607=f607,field608=f608,field609=f609,field610=f610,field611=f611,field612=f612,field613=f613,field614=f614,field615=f615,field616=f616,field617=f617,field618=f618,field619=f619,field620=f620,field621=f621,field622=f622,field623=f623,field624=f624,field625=f625,field626=f626,field627=f627,field628=f628,field629=f629,field630=f630,field631=f631,field632=f632,field633=f633,field634=f634,field635=f635,field636=f636,field637=f637,field638=f638,field639=f639,field640=f640,field641=f641,field642=f642,field643=f643,field644=f644,field645=f645,field646=f646,field647=f647,field648=f648,field649=f649,field650=f650,field651=f651,field652=f652,field653=f653,field654=f654,field655=f655,field656=f656,field657=f657,field658=f658,field659=f659,field660=f660,field661=f661,field662=f662,field663=f663,field664=f664,field665=f665,field666=f666,field667=f667,field668=f668,field669=f669,field670=f670,field671=f671,field672=f672,field673=f673,field674=f674,field675=f675,field676=f676,field677=f677,field678=f678,field679=f679,field680=f680,field681=f681,field682=f682,field683=f683,field684=f684,field685=f685,field686=f686,field687=f687,field688=f688,field689=f689,field690=f690,field691=f691,field692=f692,field693=f693,field694=f694,field695=f695,field696=f696,field697=f697,field698=f698,field699=f699,field700=f700,field701=f701,field702=f702,field703=f703,field704=f704,field705=f705,field706=f706,field707=f707,field708=f708,field709=f709,field710=f710,field711=f711,field712=f712,field713=f713,field714=f714,field715=f715,field716=f716,field717=f717,field718=f718,field719=f719,field720=f720,field721=f721,field722=f722,field723=f723,field724=f724,field725=f725,field726=f726,field727=f727,field728=f728,field729=f729,field730=f730,field731=f731,field732=f732,field733=f733,field734=f734,field735=f735,field736=f736,field737=f737,field738=f738,field739=f739,field740=f740,field741=f741,field742=f742,field743=f743,field744=f744,field745=f745,field746=f746,field747=f747,field748=f748,field749=f749,field750=f750,field751=f751,field752=f752,field753=f753,field754=f754,field755=f755,field756=f756,field757=f757,field758=f758,field759=f759,field760=f760,field761=f761,field762=f762,field763=f763,field764=f764,field765=f765,field766=f766,field767=f767,field768=f768,field769=f769,field770=f770,field771=f771,field772=f772,field773=f773,field774=f774,field775=f775,field776=f776,field777=f777,field778=f778,field779=f779,field780=f780,field781=f781,field782=f782,field783=f783,field784=f784,field785=f785,field786=f786,field787=f787,field788=f788,field789=f789,field790=f790,field791=f791,field792=f792,field793=f793,field794=f794,field795=f795,field796=f796,field797=f797,field798=f798,field799=f799,field800=f800,field801=f801,field802=f802,field803=f803,field804=f804,field805=f805,field806=f806,field807=f807,field808=f808,field809=f809,field810=f810,field811=f811,field812=f812,field813=f813,field814=f814,field815=f815,field816=f816,field817=f817,field818=f818,field819=f819,field820=f820,field821=f821,field822=f822,field823=f823,field824=f824,field825=f825,field826=f826,field827=f827,field828=f828,field829=f829,field830=f830,field831=f831,field832=f832,field833=f833,field834=f834,field835=f835,field836=f836,field837=f837,field838=f838,field839=f839,field840=f840,field841=f841,field842=f842,field843=f843,field844=f844,field845=f845,field846=f846,field847=f847,field848=f848,field849=f849,field850=f850,field851=f851,field852=f852,field853=f853,field854=f854,field855=f855,field856=f856,field857=f857,field858=f858,field859=f859,field860=f860,field861=f861,field862=f862,field863=f863,field864=f864,field865=f865,field866=f866,field867=f867,field868=f868,field869=f869,field870=f870,field871=f871,field872=f872,field873=f873,field874=f874,field875=f875,field876=f876,field877=f877,field878=f878,field879=f879,field880=f880,field881=f881,field882=f882,field883=f883,field884=f884,field885=f885,field886=f886,field887=f887,field888=f888,field889=f889,field890=f890,field891=f891,field892=f892,field893=f893,field894=f894,field895=f895,field896=f896,field897=f897,field898=f898,field899=f899,field900=f900,field901=f901,field902=f902,field903=f903,field904=f904,field905=f905,field906=f906,field907=f907,field908=f908,field909=f909,field910=f910,field911=f911,field912=f912,field913=f913,field914=f914,field915=f915,field916=f916,field917=f917,field918=f918,field919=f919,field920=f920,field921=f921,field922=f922,field923=f923,field924=f924,field925=f925,field926=f926,field927=f927,field928=f928,field929=f929,field930=f930,field931=f931,field932=f932,field933=f933,field934=f934,field935=f935,field936=f936,field937=f937,field938=f938,field939=f939,field940=f940,field941=f941,field942=f942,field943=f943,field944=f944,field945=f945,field946=f946,field947=f947,field948=f948,field949=f949,field950=f950,field951=f951,field952=f952,field953=f953,field954=f954,field955=f955,field956=f956,field957=f957,field958=f958,field959=f959,field960=f960,field961=f961,field962=f962,field963=f963,field964=f964,field965=f965,field966=f966,field967=f967,field968=f968,field969=f969,field970=f970,field971=f971,field972=f972,field973=f973,field974=f974,field975=f975,field976=f976,field977=f977,field978=f978,field979=f979,field980=f980,field981=f981,field982=f982,field983=f983,field984=f984,field985=f985,field986=f986,field987=f987,field988=f988,field989=f989,field990=f990,field991=f991,field992=f992,field993=f993,field994=f994,field995=f995,field996=f996,field997=f997,field998=f998,field999=f999,field1000=f1000,field1001=f501,field1002=f2,field1003=f3,field1004=f4,field1005=f5,field1006=f6,field1007=f7,field1008=f8,field1009=f9,field1010=f10,field1011=f11,field1012=f12,field1013=f13,field1014=f14,field1015=f15,field1016=f16,field1017=f17,field1018=f18,field1019=f19,field1020=f20,field1021=f21,field1022=f22,field1023=f23,field1024=f24,field1025=f25,field1026=f26,field1027=f27,field1028=f28,field1029=f29,field1030=f30,field1031=f31,field1032=f32,field1033=f33,field1034=f34,field1035=f35,field1036=f36,field1037=f37,field1038=f38,field1039=f39,field1040=f40,field1041=f41,field1042=f42,field1043=f43,field1044=f44,field1045=f45,field1046=f46,field1047=f47,field1048=f48,field1049=f49,field1050=f50,field1051=f51,field1052=f52,field1053=f53,field1054=f54,field1055=f55,field1056=f56,field1057=f57,field1058=f58,field1059=f59,field1060=f60,field1061=f61,field1062=f62,field1063=f63,field1064=f64,field1065=f65,field1066=f66,field1067=f67,field1068=f68,field1069=f69,field1070=f70,field1071=f71,field1072=f72,field1073=f73,field1074=f74,field1075=f75,field1076=f76,field1077=f77,field1078=f78,field1079=f79,field1080=f80,field1081=f81,field1082=f82,field1083=f83,field1084=f84,field1085=f85,field1086=f86,field1087=f87,field1088=f88,field1089=f89,field1090=f90,field1091=f91,field1092=f92,field1093=f93,field1094=f94,field1095=f95,field1096=f96,field1097=f97,field1098=f98,field1099=f99,field1100=f100,field1101=f101,field1102=f102,field1103=f103,field1104=f104,field1105=f105,field1106=f106,field1107=f107,field1108=f108,field1109=f109,field1110=f110,field1111=f111,field1112=f112,field1113=f113,field1114=f114,field1115=f115,field1116=f116,field1117=f117,field1118=f118,field1119=f119,field1120=f120,field1121=f121,field1122=f122,field1123=f123,field1124=f124,field1125=f125,field1126=f126,field1127=f127,field1128=f128,field1129=f129,field1130=f130,field1131=f131,field1132=f132,field1133=f133,field1134=f134,field1135=f135,field1136=f136,field1137=f137,field1138=f138,field1139=f139,field1140=f140,field1141=f141,field1142=f142,field1143=f143,field1144=f144,field1145=f145,field1146=f146,field1147=f147,field1148=f148,field1149=f149,field1150=f150,field1151=f151,field1152=f152,field1153=f153,field1154=f154,field1155=f155,field1156=f156,field1157=f157,field1158=f158,field1159=f159,field1160=f160,field1161=f161,field1162=f162,field1163=f163,field1164=f164,field1165=f165,field1166=f166,field1167=f167,field1168=f168,field1169=f169,field1170=f170,field1171=f171,field1172=f172,field1173=f173,field1174=f174,field1175=f175,field1176=f176,field1177=f177,field1178=f178,field1179=f179,field1180=f180,field1181=f181,field1182=f182,field1183=f183,field1184=f184,field1185=f185,field1186=f186,field1187=f187,field1188=f188,field1189=f189,field1190=f190,field1191=f191,field1192=f192,field1193=f193,field1194=f194,field1195=f195,field1196=f196,field1197=f197,field1198=f198,field1199=f199,field1200=f200,field1201=f201,field1202=f202,field1203=f203,field1204=f204,field1205=f205,field1206=f206,field1207=f207,field1208=f208,field1209=f209,field1210=f210,field1211=f211,field1212=f212,field1213=f213,field1214=f214,field1215=f215,field1216=f216,field1217=f217,field1218=f218,field1219=f219,field1220=f220,field1221=f221,field1222=f222,field1223=f223,field1224=f224,field1225=f225,field1226=f226,field1227=f227,field1228=f228,field1229=f229,field1230=f230,field1231=f231,field1232=f232,field1233=f233,field1234=f234,field1235=f235,field1236=f236,field1237=f237,field1238=f238,field1239=f239,field1240=f240,field1241=f241,field1242=f242,field1243=f243,field1244=f244,field1245=f245,field1246=f246,field1247=f247,field1248=f248,field1249=f249,field1250=f250,field1251=f251,field1252=f252,field1253=f253,field1254=f254,field1255=f255,field1256=f256,field1257=f257,field1258=f258,field1259=f259,field1260=f260,field1261=f261,field1262=f262,field1263=f263,field1264=f264,field1265=f265,field1266=f266,field1267=f267,field1268=f268,field1269=f269,field1270=f270,field1271=f271,field1272=f272,field1273=f273,field1274=f274,field1275=f275,field1276=f276,field1277=f277,field1278=f278,field1279=f279,field1280=f280,field1281=f281,field1282=f282,field1283=f283,field1284=f284,field1285=f285,field1286=f286,field1287=f287,field1288=f288,field1289=f289,field1290=f290,field1291=f291,field1292=f292,field1293=f293,field1294=f294,field1295=f295,field1296=f296,field1297=f297,field1298=f298,field1299=f299,field1300=f300,field1301=f301,field1302=f302,field1303=f303,field1304=f304,field1305=f305,field1306=f306,field1307=f307,field1308=f308,field1309=f309,field1310=f310,field1311=f311,field1312=f312,field1313=f313,field1314=f314,field1315=f315,field1316=f316,field1317=f317,field1318=f318,field1319=f319,field1320=f320,field1321=f321,field1322=f322,field1323=f323,field1324=f324,field1325=f325,field1326=f326,field1327=f327,field1328=f328,field1329=f329,field1330=f330,field1331=f331,field1332=f332,field1333=f333,field1334=f334,field1335=f335,field1336=f336,field1337=f337,field1338=f338,field1339=f339,field1340=f340,field1341=f341,field1342=f342,field1343=f343,field1344=f344,field1345=f345,field1346=f346,field1347=f347,field1348=f348,field1349=f349,field1350=f350,field1351=f351,field1352=f352,field1353=f353,field1354=f354,field1355=f355,field1356=f356,field1357=f357,field1358=f358,field1359=f359,field1360=f360,field1361=f361,field1362=f362,field1363=f363,field1364=f364,field1365=f365,field1366=f366,field1367=f367,field1368=f368,field1369=f369,field1370=f370,field1371=f371,field1372=f372,field1373=f373,field1374=f374,field1375=f375,field1376=f376,field1377=f377,field1378=f378,field1379=f379,field1380=f380,field1381=f381,field1382=f382,field1383=f383,field1384=f384,field1385=f385,field1386=f386,field1387=f387,field1388=f388,field1389=f389,field1390=f390,field1391=f391,field1392=f392,field1393=f393,field1394=f394,field1395=f395,field1396=f396,field1397=f397,field1398=f398,field1399=f399,field1400=f400,field1401=f401,field1402=f402,field1403=f403,field1404=f404,field1405=f405,field1406=f406,field1407=f407,field1408=f408,field1409=f409,field1410=f410,field1411=f411,field1412=f412,field1413=f413,field1414=f414,field1415=f415,field1416=f416,field1417=f417,field1418=f418,field1419=f419,field1420=f420,field1421=f421,field1422=f422,field1423=f423,field1424=f424,field1425=f425,field1426=f426,field1427=f427,field1428=f428,field1429=f429,field1430=f430,field1431=f431,field1432=f432,field1433=f433,field1434=f434,field1435=f435,field1436=f436,field1437=f437,field1438=f438,field1439=f439,field1440=f440,field1441=f441,field1442=f442,field1443=f443,field1444=f444,field1445=f445,field1446=f446,field1447=f447,field1448=f448,field1449=f449,field1450=f450,field1451=f451,field1452=f452,field1453=f453,field1454=f454,field1455=f455,field1456=f456,field1457=f457,field1458=f458,field1459=f459,field1460=f460,field1461=f461,field1462=f462,field1463=f463,field1464=f464,field1465=f465,field1466=f466,field1467=f467,field1468=f468,field1469=f469,field1470=f470,field1471=f471,field1472=f472,field1473=f473,field1474=f474,field1475=f475,field1476=f476,field1477=f477,field1478=f478,field1479=f479,field1480=f480,field1481=f481,field1482=f482,field1483=f483,field1484=f484,field1485=f485,field1486=f486,field1487=f487,field1488=f488,field1489=f489,field1490=f490,field1491=f491,field1492=f492,field1493=f493,field1494=f494,field1495=f495,field1496=f496,field1497=f497,field1498=f498,field1499=f499,field1500=f500)',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 10 second interval;
create source CSVSource using FileReader (
	directory:'@DIR@',
	WildCard:'@WILDCARD@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  curr String,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       data[6],
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target BlobT using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	uploadpolicy:'EventCount:100,interval:5s'
)
format using XMLFormatter (
rootelement:'document',
elementtuple:'MerchantName:merchantId:text=merchantname'
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

CREATE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src1 USING Global.GCSReader (
  _h_GCSQueryCoolingTime: '1',
  ServiceAccountKey: '',
  ProjectId: '',
  DownloadPolicy: '',
  BucketName: '',
  compressiontype: '',
  PollingInterval: ,
  FolderName: '',
  UseStreaming: false,
  StartTimestamp: '',
  ObjectFilter: '',
  ObjectDetectionMode: '',
  IncludeSubfolders: false )
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream1;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (
  objectname: '',
  foldername: '',
  bucketname: '',
  uploadpolicy: '' )
FORMAT USING DSVFormatter (
members:'data')
INPUT FROM @APPNAME@_Stream1;

CREATE OR REPLACE SOURCE @APPNAME@_src2 USING Global.GCSReader (
  _h_GCSQueryCoolingTime: '1',
  ServiceAccountKey: '',
  ProjectId: '',
  DownloadPolicy: '',
  BucketName: '',
  compressiontype: '',
  PollingInterval: ,
  FolderName: '',
  UseStreaming: false,
  StartTimestamp: '',
  ObjectFilter: '',
  ObjectDetectionMode: '',
  IncludeSubfolders: false )
PARSE USING Global.JSONParser ()
OUTPUT TO @APPNAME@_Stream2;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (
  objectname: '',
  foldername: '',
  bucketname: '',
  uploadpolicy: '' )
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_Stream2;

CREATE OR REPLACE SOURCE @APPNAME@_src3 USING GCSReader (
  _h_GCSQueryCoolingTime: '1',
  ServiceAccountKey: '',
  ProjectId: '',
  DownloadPolicy: '',
  BucketName: '',
  compressiontype: '',
  PollingInterval: ,
  FolderName: '',
  UseStreaming: false,
  StartTimestamp: '',
  ObjectFilter: '',
  ObjectDetectionMode: '',
  IncludeSubfolders: false )
PARSE USING AvroParser ()
OUTPUT TO @APPNAME@_Stream3;

CREATE CQ @APPNAME@_CQ3
INSERT INTO @APPNAME@_CQOut3
SELECT AvroToJson(data,false) FROM @APPNAME@_Stream3;

CREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (
  objectname: '',
  foldername: '',
  bucketname: '',
  uploadpolicy: '' )
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_CQOut3;

CREATE OR REPLACE SOURCE @APPNAME@_src4 USING Global.GCSReader (
  _h_GCSQueryCoolingTime: '1',
  ServiceAccountKey: '',
  ProjectId: '',
  DownloadPolicy: '',
  BucketName: '',
  compressiontype: '',
  PollingInterval: ,
  FolderName: '',
  UseStreaming: false,
  StartTimestamp: '',
  ObjectFilter: '',
  ObjectDetectionMode: '',
  IncludeSubfolders: false )
PARSE USING Global.XMLParser (
  rootnode: ''
)
OUTPUT TO @APPNAME@_Stream4;

CREATE OR REPLACE TARGET @APPNAME@_trgt4 USING S3Writer (
  objectname: '',
  foldername: '',
  bucketname: '',
  uploadpolicy: '' )
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_Stream4;

END APPLICATION @APPNAME@;

STOP APPLICATION @AppName@;
UNDEPLOY APPLICATION @AppName@;
DROP APPLICATION @AppName@ CASCADE;
CREATE APPLICATION @AppName@ recovery 1 second interval;
CREATE SOURCE @AppName@_Source USING FileReader (
	WildCard: 'posdata100.csv',
 directory: '@dir@',
	positionByEOF:false
	)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO @AppName@_Stream;

CREATE TYPE cdctypestream(
 id int,
 name String
);

create stream @StreamType@ of cdctypestream persist using Global.DefaultKafkaProperties;
CREATE OR REPLACE CQ CsvToPosData
INSERT INTO @StreamType@
SELECT
TO_INT(TO_STRING(data[0]).replaceAll("COMPANY ", "")),
data[1]
FROM @AppName@_Stream;
Deploy application @AppName@;
Start @AppName@;

stop application app1PS;
undeploy application app1PS;
drop application app1PS cascade;

create application app1PS;

create target File_TargerPS using FileWriter
(
directory : '',
filename : ''
)
format using DSVFormatter()
input from Recoveryss1;

end application app1PS;

deploy application app1PS;
start application app1PS;

STOP APPLICATION RouterTester.RouterApp;
UNDEPLOY APPLICATION RouterTester.RouterApp;
DROP APPLICATION RouterTester.RouterApp CASCADE;

--DROP namespace RouterTester;
--CREATE namespace RouterTester;
--USE RouterTester;

CREATE APPLICATION RouterApp RECOVERY 20 SECOND INTERVAL;

CREATE OR REPLACE SOURCE CsvDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'posdata100.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO InputStreamNonTyped;

CREATE TYPE MyDataType (
  colId String,
  colStr String,
  colInt int,
  colFloat float
);

CREATE OR REPLACE STREAM InputStreamTyped of MyDataType;

CREATE OR REPLACE CQ  CreateTypedStreamCQ
INSERT INTO InputStreamTyped (colId, colStr, colInt, colFloat)
SELECT
  TO_STRING(src.data[1]) as colId,
  TO_STRING(src.data[0]) as colStr,
  TO_INT(src.data[3]) as colInt,
  TO_FLOAT(src.data[7]) as colFloat
FROM InputStreamNonTyped src;

-- Router on the stream of Generated type. It tests the predicate with expressions on
-- integer, float and string types.
CREATE OR REPLACE ROUTER TestRouterTyped INPUT FROM InputStreamTyped CASE
WHEN colInt <= 3 and colFloat     < 100  and colStr like 'COMPANY%' THEN ROUTE TO StreamTyped1,
WHEN colInt <= 6 and colFloat * 3 < 200  and colStr like 'COMPANY%' THEN ROUTE TO StreamTyped2,
ELSE ROUTE TO StreamTypedElse;

-- Router on the stream of Non-generated Type. Predicates with expressions over
-- integer and float types.
CREATE OR REPLACE ROUTER TestRouterNonTyped INPUT FROM InputStreamNonTyped CASE
WHEN TO_INT(data[3]) <= 3 and TO_FLOAT(data[7])     < 100 and TO_STRING(data[0]) like 'COMPANY%' THEN ROUTE TO StreamNonTyped1,
WHEN TO_INT(data[3]) <= 6 and TO_FLOAT(data[7]) * 3 < 200 and TO_STRING(data[0]) like 'COMPANY%' THEN ROUTE TO StreamNonTyped2,
ELSE ROUTE TO StreamNonTypedElse;

-- Output Streams to populate the waction stores (for non-typed stream only).
CREATE OR REPLACE STREAM StreamWactions1 of MyDataType;
CREATE OR REPLACE STREAM StreamWactions2 of MyDataType;
CREATE OR REPLACE STREAM StreamWactionsElse of MyDataType;

CREATE OR REPLACE CQ ConvertCQNonTyped1
INSERT INTO StreamWactions1 (colId, colStr, colInt, colFloat)
SELECT
  TO_STRING(src.data[1]) as colId,
  TO_STRING(src.data[0]) as colStr,
  TO_INT(src.data[3]) as colInt,
  TO_FLOAT(src.data[7]) * 3 as colFloat
FROM StreamNonTyped1 src;

CREATE OR REPLACE CQ ConvertCQNonTyped2
INSERT INTO StreamWactions2 (colId, colStr, colInt, colFloat)
SELECT
  TO_STRING(src.data[1]) as colId,
  TO_STRING(src.data[0]) as colStr,
  TO_INT(src.data[3]) as colInt,
  TO_FLOAT(src.data[7]) * 3 as colFloat
FROM StreamNonTyped2 src;

CREATE OR REPLACE CQ ConvertCQNonTypedElse
INSERT INTO StreamWactionsElse (colId, colStr, colInt, colFloat)
SELECT
  TO_STRING(src.data[1]) as colId,
  TO_STRING(src.data[0]) as colStr,
  TO_INT(src.data[3]) as colInt,
  TO_FLOAT(src.data[7]) * 3 as colFloat
FROM StreamNonTypedElse src;

CREATE TYPE WactionsTypedType1 (
  colId  String,
  colStr String,
  colInt int,
  colFloat float
);

CREATE TYPE WactionsTypedType2 (
  colId  String,
  colStr String,
  colInt int,
  colFloat float
);

CREATE TYPE WactionsTypedTypeElse (
  colId  String,
  colStr String,
  colInt int,
  colFloat float
);

CREATE TYPE WactionsNonTypedType1 (
  colId  String,
  colStr String,
  colInt int,
  colFloat float
);

CREATE TYPE WactionsNonTypedType2 (
  colId  String,
  colStr String,
  colInt int,
  colFloat float
);

CREATE TYPE WactionsNonTypedTypeElse (
  colId  String,
  colStr String,
  colInt int,
  colFloat float
);


CREATE WACTIONSTORE WactionsTyped1 CONTEXT OF WactionsTypedType1
EVENT TYPES ( WactionsTypedType1 )
@PERSIST-TYPE@;

CREATE WACTIONSTORE WactionsTyped2 CONTEXT OF WactionsTypedType2
EVENT TYPES ( WactionsTypedType2 )
@PERSIST-TYPE@;

CREATE WACTIONSTORE WactionsTypedElse CONTEXT OF WactionsTypedTypeElse
EVENT TYPES ( WactionsTypedTypeElse )
@PERSIST-TYPE@;

CREATE WACTIONSTORE WactionsNonTyped1 CONTEXT OF WactionsNonTypedType1
EVENT TYPES ( WactionsNonTypedType1 )
@PERSIST-TYPE@;

CREATE WACTIONSTORE WactionsNonTyped2 CONTEXT OF WactionsNonTypedType2
EVENT TYPES ( WactionsNonTypedType2 )
@PERSIST-TYPE@;

CREATE WACTIONSTORE WactionsNonTypedElse CONTEXT OF WactionsNonTypedTypeElse
EVENT TYPES ( WactionsNonTypedTypeElse )
@PERSIST-TYPE@;

CREATE CQ InsertWactionsTyped1
INSERT INTO WactionsTyped1
SELECT *
FROM StreamTyped1;

CREATE CQ InsertWactionsTyped2
INSERT INTO WactionsTyped2
SELECT *
FROM StreamTyped2;

CREATE CQ InsertWactionsTypedElse
INSERT INTO WactionsTypedElse
SELECT *
FROM StreamTypedElse;

CREATE CQ InsertWactionsNonTyped1
INSERT INTO WactionsNonTyped1
SELECT *
FROM StreamWactions1;

CREATE CQ InsertWactionsNonTyped2
INSERT INTO WactionsNonTyped2
SELECT *
FROM StreamWactions2;

CREATE CQ InsertWactionsNonTypedElse
INSERT INTO WactionsNonTypedElse
SELECT *
FROM StreamWactionsElse;

END APPLICATION RouterApp;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@;
CREATE  SOURCE @SourceName@ USING DatabaseReader  ( 
  Username: '@UserName@',
  Password: '@Password@',
  DatabaseName: 'qatest',
  ConnectionURL: '@SourceConnectionURL@',
  Tables: 'qatest.@SourceTable@',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @SRCINPUTSTREAM@;
create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;
CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:1',
  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
 ) 
INPUT FROM @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;
Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream; 


create Target FileTarget using FileWriter(
    rolloverpolicy:'@UPLOAD-SIZE@',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using AvroFormatter (
schemafilename:'@charset@',
formatAs:'@mem@',
schemaregistryurl:'@head@'

)
input from TypedCSVStream;

end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@@RECOVERY_PROP@;

CREATE OR REPLACE SOURCE @APP_NAME@_src USING FileReader (
directory:'@DIRECTORY@',
WildCard:'posdata5L.csv',
positionByEOF:false
)
parse using DSVParser (
header:true
)
OUTPUT TO @APP_NAME@_Stream;


CREATE OR REPLACE CQ @APP_NAME@_CQ
INSERT INTO @APP_NAME@_Stream2
SELECT data[0] as BUSINESS_NAME,data[1] as MERCHANT_ID,data[2] as PRIMARY_ACCOUNT,
data[3] as POS_DATA_CODE,data[4] as DATE_TIME,data[5] as EXP_DATE,data[6] as CURRENCY_CODE,data[7] as AUTH_AMOUNT,
data[8] as TERMINAL_ID,data[9] as ZIP,data[10] as CITY
FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt USING KuduWriter (
  kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
  pkupdatehandlingmode:'@MODE@',
  tables: '@TARGET_TABLES@',
  batchpolicy: 'EventCount:1,Interval:0'
 )
INPUT FROM @APP_NAME@_Stream2;


END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

--
-- Recovery Test 38 with two sources, two jumping time-count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5a3W/p -> CQ1 -> WS
--   S2 -> Jc6a4W/p -> CQ2 -> WS
--

STOP KStreamRecov38Tester.KStreamRecovTest38;
UNDEPLOY APPLICATION KStreamRecov38Tester.KStreamRecovTest38;
DROP APPLICATION KStreamRecov38Tester.KStreamRecovTest38 CASCADE;

DROP USER KStreamRecov38Tester;
DROP NAMESPACE KStreamRecov38Tester CASCADE;
CREATE USER KStreamRecov38Tester IDENTIFIED BY KStreamRecov38Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov38Tester;
CONNECT KStreamRecov38Tester KStreamRecov38Tester;

CREATE APPLICATION KStreamRecovTest38 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE JUMPING WINDOW DataStream5Rows3Seconds
OVER DataStream1 KEEP 5 ROWS WITHIN 3 SECOND
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Rows4Seconds
OVER DataStream2 KEEP 6 ROWS WITHIN 4 SECOND
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataStream5Rows3Seconds
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Rows3Seconds p
GROUP BY p.merchantId;

CREATE CQ DataStream6Rows4Seconds
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Rows4Seconds p
GROUP BY p.merchantId;

END APPLICATION KStreamRecovTest38;

--
-- Recovery Test 4
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS
-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS
--

STOP KStreamRecov4Tester.KStreamRecovTest4;
UNDEPLOY APPLICATION KStreamRecov4Tester.KStreamRecovTest4;
DROP APPLICATION KStreamRecov4Tester.KStreamRecovTest4 CASCADE;
DROP USER KStreamRecov4Tester;
DROP NAMESPACE KStreamRecov4Tester CASCADE;
CREATE USER KStreamRecov4Tester IDENTIFIED BY KStreamRecov4Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov4Tester;
CONNECT KStreamRecov4Tester KStreamRecov4Tester;

CREATE APPLICATION KStreamRecovTest4 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream OF CsvData;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION KStreamRecovTest4;

--
-- Recovery Test 6 with sliding window and partitioned feature
-- Nicholas Keene, Bert Hashemi WebAction, Inc.
--
-- S -> CQ -> SW(partitioned) -> CQ(no aggregate) -> WS
--

STOP KStreamRecov6Tester.KStreamRecovTest6;
UNDEPLOY APPLICATION KStreamRecov6Tester.KStreamRecovTest6;
DROP APPLICATION KStreamRecov6Tester.KStreamRecovTest6 CASCADE;
DROP USER KStreamRecov6Tester;
DROP NAMESPACE KStreamRecov6Tester CASCADE;
CREATE USER KStreamRecov6Tester IDENTIFIED BY KStreamRecov6Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov6Tester;
CONNECT KStreamRecov6Tester KStreamRecov6Tester;

CREATE APPLICATION KStreamRecovTest6 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvData PARTITION BY merchantId;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION KStreamRecovTest6;

stop Quiesce_IL;
undeploy application Quiesce_IL;
drop application Quiesce_IL cascade;
CREATE APPLICATION Quiesce_IL USE EXCEPTIONSTORE TTL : '7d' ;
CREATE FLOW Quiesce_IL_flow;
Create Source Quiesce_IL_Oraclesrc Using databasereader(
 Username:'@USERNAME@',
 Password:'@PASSWORD@',
 ConnectionURL:'@CONNECTION_URL@',
 Tables:'QATEST.QUIESCE_TABLE1;QATEST.QUIESCE_TABLE2',
 QuiesceOnILCompletion: 'true',
 _h_fetchexactrowcount: 'true'
)
Output To Quiesce_IL_OrcStrm;
END FLOW Quiesce_IL_flow;

CREATE TARGET Quiesce_IL_BigQueryTrg USING BigQueryWriter (
  serviceAccountKey: '@SERVICEACCOUNTKEY@',
  projectId:'@PROJECTID@',
  Tables:'QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE1;QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE2'
)

INPUT FROM Quiesce_IL_OrcStrm;
END APPLICATION Quiesce_IL;
DEPLOY APPLICATION Quiesce_IL;
start application Quiesce_IL;

stop Quiesce_CDC;
undeploy application Quiesce_CDC;
drop application Quiesce_CDC cascade;
CREATE APPLICATION Quiesce_CDC recovery 1 second interval USE EXCEPTIONSTORE TTL : '7d' ;
CREATE stream Quiesce_CDC_OrcStrm of global.waevent persist using Global.DefaultKafkaProperties;
CREATE FLOW Quiesce_CDC_flow;
Create Source Quiesce_CDC_Oraclesrc Using oraclereader(
 Username:'@USERNAME@',
 Password:'@PASSWORD@',
 ConnectionURL:'@CONNECTION_URL@',
 Tables:'QATEST.QUIESCE_TABLE1;QATEST.QUIESCE_TABLE2',
 _h_fetchexactrowcount: 'true'
)
Output To Quiesce_CDC_OrcStrm;
END FLOW Quiesce_CDC_flow;
END APPLICATION Quiesce_CDC;
DEPLOY APPLICATION Quiesce_CDC;
start application Quiesce_CDC;

stop Quiesce_CDC_BQ_TARGET;
undeploy application Quiesce_CDC_BQ_TARGET;
drop application Quiesce_CDC_BQ_TARGET cascade;
CREATE APPLICATION Quiesce_CDC_BQ_TARGET recovery 1 second interval USE EXCEPTIONSTORE TTL : '7d' ;
CREATE TARGET Quiesce_CDC_BigQueryTrg USING BigQueryWriter (
  serviceAccountKey: '@SERVICEACCOUNTKEY@',
  projectId:'@PROJECTID@',
  BatchPolicy:'Interval:10',
  _h_maxParallelStreamingRequests: '10',
  Tables:'QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE1;QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE2'
)
INPUT FROM Quiesce_CDC_OrcStrm;
END APPLICATION Quiesce_CDC_BQ_TARGET;
DEPLOY APPLICATION Quiesce_CDC_BQ_TARGET;
start application Quiesce_CDC_BQ_TARGET;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;
CREATE  SOURCE @TableSourceName@ USING DatabaseReader  ( 
  ConnectionURL: '@SourceConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  Tables: '@SourceTable@',
  ReplicationSlotName: 'null'
 ) OUTPUT TO @SRCTableINPUTSTREAM@;

 CREATE  SOURCE @QuerySourceName@ USING DatabaseReader  ( 
  ConnectionURL: '@SourceConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  Query: '@SourceQuery@',
  ReplicationSlotName: 'null'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@;

CREATE  TARGET @TabletargetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
 ) 
 INPUT FROM @SRCTableINPUTSTREAM@;

 CREATE  TARGET @QuerytargetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

DROP APPLICATION @APP_NAME@1 FORCE;
DROP APPLICATION @APP_NAME@2 FORCE;
DROP APPLICATION @APP_NAME@3 FORCE;


CREATE APPLICATION @APP_NAME@1 WITH ENCRYPTION USE EXCEPTIONSTORE TTL : '7d';
CREATE OR REPLACE PROPERTYSET @APP_NAME@1_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',
acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');
CREATE STREAM @APP_NAME@_In1 OF Global.waevent persist using @APP_NAME@1_KafkaPropset;

CREATE OR REPLACE SOURCE @APP_NAME@1_Source USING @SOURCE_ADAPTER@  ( 
  Tables:'@SRC_TABLE@'
) OUTPUT TO @APP_NAME@_In1;

CREATE OR REPLACE TARGET @APP_NAME@1_Target USING @TARGET_ADAPTER@  ( 
  Tables: '@SRC_TABLE@, @TGT_TABLE@1', 
  CheckPointTable: '@CHK_TABLE@', 
) INPUT FROM @APP_NAME@_In1;

CREATE TARGET @APP_NAME@1_SysOut
USING SysOut(name:@APP_NAME@1Sys)
INPUT FROM @APP_NAME@_In1;

CREATE TARGET @APP_NAME@1_FileWriter USING filewriter
(filename:'@APP_NAME@_FW1.log'
)
format using dsvFormatter()
INPUT FROM @APP_NAME@_In1;

END APPLICATION @APP_NAME@1;

DEPLOY APPLICATION @APP_NAME@1;
START APPLICATION @APP_NAME@1;

CREATE APPLICATION @APP_NAME@2;

CREATE OR REPLACE SOURCE @APP_NAME@2_Source USING @SOURCE_ADAPTER@  ( 
  Tables:'@SRC_TABLE@'
) OUTPUT TO @APP_NAME@_In2;

CREATE OR REPLACE TARGET @APP_NAME@2_Target USING @TARGET_ADAPTER@  ( 
  Tables: '@SRC_TABLE@, @TGT_TABLE@2', 
  CheckPointTable: '@CHK_TABLE@'
) INPUT FROM @APP_NAME@_In2;

CREATE TARGET @APP_NAME@2_SysOut
USING SysOut(name:@APP_NAME@2Sys)
INPUT FROM @APP_NAME@_In2;

CREATE TARGET @APP_NAME@2_FileWriter USING filewriter
(filename:'@APP_NAME@_FW2.log'
)
format using dsvFormatter()
INPUT FROM @APP_NAME@_In2;

END APPLICATION @APP_NAME@2;

DEPLOY APPLICATION @APP_NAME@2;
START APPLICATION @APP_NAME@2;

CREATE APPLICATION @APP_NAME@3;
CREATE FLOW @APP_NAME@_SOURCEFLOW;
CREATE OR REPLACE SOURCE @APP_NAME@3_Source USING @SOURCE_ADAPTER@  ( 
  Tables:'@SRC_TABLE@'
) OUTPUT TO @APP_NAME@_In3;
END FLOW @APP_NAME@_SOURCEFLOW;

CREATE FLOW @APP_NAME@_TARGETFLOW;
CREATE OR REPLACE TARGET @APP_NAME@3_Target USING @TARGET_ADAPTER@  ( 
  Tables: '@SRC_TABLE@, @TGT_TABLE@3', 
  CheckPointTable: '@CHK_TABLE@', 
) INPUT FROM @APP_NAME@_In3;

CREATE TARGET @APP_NAME@3_SysOut
USING SysOut(name:@APP_NAME@3Sys)
INPUT FROM @APP_NAME@_In3;

CREATE TARGET @APP_NAME@3_FileWriter USING filewriter
(filename:'@APP_NAME@_FW3.log'
)
format using dsvFormatter()
INPUT FROM @APP_NAME@_In3;
END FLOW @APP_NAME@_TARGETFLOW;

END APPLICATION @APP_NAME@3;

DEPLOY APPLICATION @APP_NAME@3;
START APPLICATION @APP_NAME@3;

--
-- Kafka Stream Agent Checkpoint Recovery tests
-- Bert Hashemi and Zalak Shah, WebAction, Inc.
--

stop application recoveryTestAgent.KSRecovCSV;
undeploy application recoveryTestAgent.KSRecovCSV;
drop application recoveryTestAgent.KSRecovCSV cascade;
DROP USER recoveryTestAgent;
DROP NAMESPACE recoveryTestAgent CASCADE;
CREATE USER recoveryTestAgent IDENTIFIED BY recoveryTestAgent;
GRANT create,drop ON deploymentgroup Global.* TO USER recoveryTestAgent;
CONNECT recoveryTestAgent recoveryTestAgent;

create application KSRecovCSV
RECOVERY 5 SECOND INTERVAL;

CREATE FLOW AgentFlow;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails-recovery.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO KafkaCsvStream;

END FLOW AgentFlow;

CREATE FLOW ServerFlow;
CREATE TYPE UserDataType
(
  UserId String KEY,
  UserName String
);

CREATE STREAM UserDataStream OF UserDataType PARTITION BY UserId;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  data[0],
        data[1]
FROM KafkaCsvStream;

CREATE WACTIONSTORE UserActivityInfo
CONTEXT OF UserDataType
EVENT TYPES ( UserDataType )
@PERSIST-TYPE@

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction
INSERT INTO UserActivityInfo
SELECT * FROM UserDataStream
LINK SOURCE EVENT;
END FLOW ServerFlow;

END APPLICATION KSRecovCSV;
DEPLOY APPLICATION KSRecovCSV with AgentFlow in AGENTS, ServerFlow on any in default;
START KSRecovCSV;

stop APPLICATION @AppName@;
Undeploy APPLICATION @AppName@;
drop APPLICATION @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @AgentFlow@1;
CREATE OR REPLACE SOURCE @SourceName@1 USING Global.MSJet (
  Tables: 'dbo.compsrc',
  username: 'qatest',
  DatabaseName: 'qatest',
  FetchTransactionMetadata: true,
  filterTransactionBoundaries: true,
  compression: false,
  ConnectionURL: '@ConnectionURL@1',
  Mode: '@mode@',
  CommittedTransactions: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
  SendBeforeImage: true,
  password: 'w3b@ct10n' )
OUTPUT TO @StreamName@;
END FLOW @AgentFlow@1;

CREATE FLOW @AgentFlow@2;
CREATE OR REPLACE SOURCE @SourceName@2 USING Global.MSJet (
  Tables: 'dbo.compsrc',
  username: 'qatest',
  DatabaseName: 'qatest',
  FetchTransactionMetadata: true,
  filterTransactionBoundaries: true,
  compression: false,
  ConnectionURL: '@ConnectionURL@2',
  CaptureType: '@captureType@',
  CommittedTransactions: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
  SendBeforeImage: true,
  password: 'w3b@ct10n' )
OUTPUT TO @StreamName@;
END FLOW @AgentFlow@2;

CREATE TARGET @SysTarget@ USING Global.SysOut (
  name: 'MS_CDC_SYSOUT' )
INPUT FROM @StreamName@;

CREATE FLOW @ServerFlow@1;

CREATE TARGET @TargetName@1 USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ParallelThreads: '',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: '@ConnectionURL@',
  StatementCacheSize: '50',
  DatabaseProviderType: 'Default',
  Username: 'qatest',
  Tables: 'dbo.compsrc,dbo.comptar',
  Password: 'w3b@ct10n',
  PreserveSourceTransactionBoundary: 'false',
  BatchPolicy: 'EventCount:1,Interval:60' )
INPUT FROM @StreamName@;

END FLOW @ServerFlow@1;

END APPLICATION @AppName@;
DEPLOY APPLICATION @AppName@ with @AgentFlow@1 in AGENTS, @AgentFlow@2 in AGENTS, @ServerFlow@1 on any in default;
START APPLICATION @AppName@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING PostgreSQLReader  (
  ReplicationSlotName: 'Slot_Name',
  FilterTransactionBoundaries: 'true',
  Username: 'User_Name',
  ConnectionURL: 'Connection_URL',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'Password',
  Tables: 'Tables'
 )
OUTPUT TO @STREAM@ ;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 USING PostgreSQLReader  (
  ReplicationSlotName: 'Slot_Name',
  FilterTransactionBoundaries: 'true',
  Username: 'User_Name',
  ConnectionURL: 'Connection_URL',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'Password',
  Tables: 'Tables'
 )
OUTPUT TO @STREAM@ ;

CREATE TARGET @SOURCE_NAME@_sysout USING Global.SysOut (
  name: '@SOURCE_NAME@_sysout' )
INPUT FROM @STREAM@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

CREATE  SOURCE @QuerySource@1 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@1;
create Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;
 CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@1;

CREATE  SOURCE @QuerySource@2 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@2;
create Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;
 CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@2;

CREATE  SOURCE @QuerySource@3 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@3;
create Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;
 CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@3;

CREATE  SOURCE @QuerySource@4 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@4;
create Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;
 CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@4;

CREATE  SOURCE @QuerySource@5 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@5;
create Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;
 CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@5;

CREATE  SOURCE @QuerySource@6 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@6;
create Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;
 CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@6;

CREATE  SOURCE @QuerySource@7 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@7;
create Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;
 CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@7;

CREATE  SOURCE @QuerySource@8 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@8;
create Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;
 CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@8;

CREATE  SOURCE @QuerySource@9 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@9;
create Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;
 CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@9;

CREATE  SOURCE @QuerySource@10 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@10;
create Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;
 CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@10;
end application @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 1 second interval;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@',
  CopybookFileFormat:'USE_COLS_6_TO_80'
  --recordSelector: '@PROP8@'
)
OUTPUT TO @APPNAME@Stream;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;

/*
create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:1,Interval:5',
  CommitPolicy: 'EventCount:1,Interval:5',
  Tables: 'QATEST.@table@'
)
input from @APPNAME@Stream;*/
end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@ConnectionURL@',
 Tables:'@SourceTables@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;


CREATE CQ @cqName@ INSERT INTO admin.oraclereader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'), 'OpTime',META(x, 'TimeStamp'))) FROM @SRCINPUTSTREAM@ x; ;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@ConnectionURL@',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@TableMapping@'
) INPUT FROM oraclereader_cq_out;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

UNDEPLOY APPLICATION admin.BadDeployGroup;
DROP APPLICATION admin.BadDeployGroup cascade;

CREATE APPLICATION BadDeployGroup;

-- This sample application demonstrates how WebAction could be used
-- by a retail chain to generate real-time reports on products and
-- stores and to send alerts of unusual activity.


CREATE FLOW BadSourceFlow;

-- RetailDataSource is the primary data source for this application.
--
-- ParseOrderData discards the fields not needed by this application and puts the
-- data into the appropriate Java types.
--
-- ParseOrderData outputs to RetailOrders stream, the start point of the
-- RetailProductFlow and RetailStoreFlow flows.

CREATE SOURCE RetailDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'retaildata2M.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO Orders;

-- A stream's type must be declared before the stream, and a CQ's
-- output stream must be defined before the CQ. Hence type-stream-CQ
-- sequences like the following are very common.

-- output for ParseOrderData
CREATE TYPE OrderType(
  storeId      String,
  orderId      String,
  sku          String,
  orderAmount  double,
  dateTime     DateTime,
  hourValue    int,
  state        String,
  city         String,
  zip          String
);
CREATE STREAM RetailOrders Of OrderType;

CREATE CQ ParseOrderData
INSERT INTO RetailOrders
SELECT  data[0],
        data[6],
        data[7],
        TO_DOUBLE(SRIGHT(data[8],1)),
        TO_DATE(data[9],'yyyyMMddHHmmss'),
        DHOURS(TO_DATE(data[9],'yyyyMMddHHmmss')),
        data[3],
        data[2],
        data[4]
FROM Orders;

END FLOW BadSourceFlow;


CREATE FLOW BadProductFlow;

-- This flow populates the ProductActivity WAction store, which
-- provides data for dashboard reports on sales by product.

-- defines context for WAction store
CREATE TYPE ProductActivityContext(
  sku String  KEY,
  OrderCount int,
  SalesAmount double,
  StartTime DateTime
);

-- defines event types for Waction store
CREATE TYPE ProductTrackingType (
  sku String KEY,
  OrderCount int,
  SalesAmount double,
  StartTime DateTime
);

CREATE WACTIONSTORE ProductActivity
CONTEXT OF ProductActivityContext
EVENT TYPES ( ProductTrackingType )
PERSIST NONE USING ( ) ;

-- input for GetProductActivity
CREATE JUMPING WINDOW ProductData_15MIN
OVER RetailOrders
KEEP WITHIN 15 MINUTE ON dateTime
PARTITION BY sku;

CREATE STREAM ProductTrackingStream OF ProductTrackingType;

-- aggregates data and populates WAction store
CREATE CQ GetProductActivity
INSERT INTO ProductTrackingStream
SELECT pd.sku, COUNT(*), SUM(pd.orderAmount), FIRST(pd.dateTime)
FROM ProductData_15MIN pd
GROUP BY pd.sku;

CREATE CQ TrackProductActivity
INSERT INTO ProductActivity
SELECT sku, OrderCount, SalesAmount, StartTime
FROM ProductTrackingStream
LINK SOURCE EVENT;

END FLOW BadProductFlow;


CREATE FLOW BadStoreFlow;

-- This flow populates the StoreActivity WAction store, which provides
-- data for dashboard reports on reports on stores, and sends alerts when
-- sales volumes are higher or lower than expected.

-- RetailStoreFlow part 1 - GetStoreActivity
--
-- The RetailData_5MIN window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions. (Aggregate functions cannot
-- be used with unbound real-time data.)
--
-- The HourlyStoreSales_Cache cache provides historical averages for the
-- current hour for each merchant

-- input for GetStoreActivity
CREATE JUMPING WINDOW RetailData_5MIN
     OVER RetailOrders
     KEEP WITHIN 5 MINUTE ON dateTime
     PARTITION BY storeId;

-- input for GetStoreActivity
CREATE TYPE StoreHourlyAvg(
  storeId String,
  hourValue int,
  hourlyAvg int,
  hourlyItemCnt int
);
CREATE CACHE HourlyStoreSales_Cache using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'storehourlyData.txt',
  header: Yes,
  columndelimiter: ','
) QUERY (keytomap:'storeId') OF StoreHourlyAvg;

-- output for GetStoreActivity
CREATE TYPE StoreOrdersTrackingType (
  storeId String KEY,
  state String,
  city  String,
  zip   String,
  StartTime DateTime,
  ordersCount int,
  salesAmount double,
  hourlyAvg int,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM StoreOrdersTracking OF StoreOrdersTrackingType;

CREATE CQ GetStoreActivity
INSERT INTO StoreOrdersTracking
SELECT rd.storeId, rd.state, rd.city, rd.zip, first(rd.dateTime),
       COUNT(rd.storeId), SUM(rd.orderAmount), l.hourlyAvg/6,
       l.hourlyAvg/6 + l.hourlyAvg/8,
       l.hourlyAvg/6 - l.hourlyAvg/10,
       '<NOTSET>', '<NOTSET>'
FROM RetailData_5MIN rd, HourlyStoreSales_Cache l
WHERE rd.storeId = l.storeId AND rd.hourValue = l.hourValue
GROUP BY rd.storeId;
-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyStoreSales_Cache cache. The category and status fields
-- are left unset to be populated by the next query.


-- RetailStoreFlow part 2 - GetStoreStatus
--
-- This query sets the count values used by the dashboard map and the
-- status values used to trigger alerts.

-- uses type previously defined for StoreOrdersTracking
CREATE STREAM StoreOrdersTracking_Status OF StoreOrdersTrackingType;

CREATE CQ GetStoreStatus
INSERT INTO StoreOrdersTracking_Status
SELECT storeId, state, city, zip, StartTime,
       ordersCount, salesAmount, hourlyAvg, upperLimit, lowerLimit,
       CASE
         WHEN salesAmount > (upperLimit + 2000) THEN 'HOT'
         WHEN salesAmount > upperLimit THEN 'MEDIUM'
         WHEN salesAmount < lowerLimit THEN 'COLD'
         ELSE 'COOL' END,
       CASE
         WHEN salesAmount > upperLimit THEN 'TOOHIGH'
         WHEN salesAmount < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM StoreOrdersTracking;


-- RetailStoreFlow part 3 - create and populate the StoreActivity WAction store

-- input for CQ TrackStoreActivity
CREATE TYPE StoreNameData(
  storeId       String KEY,
  storeName     String
);
CREATE CACHE StoreNameLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'StoreNames.csv',
  header: Yes,
  columndelimiter: ','
) QUERY(keytomap:'storeId') OF StoreNameData;

-- input for CQ TrackStoreActivity
CREATE TYPE RetailUSAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);
CREATE CACHE ZipCodeLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  header: Yes,
  columndelimiter: '	'
) QUERY (keytomap:'zip') OF RetailUSAddressData;

-- defines WAction store context
CREATE TYPE StoreActivityContext(
  storeId String KEY,
  StartTime DateTime,
  StoreName String,
  Category String,
  Status String,
  OrderCount int,
  salesamount double,
  HourlyAvg int,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

-- StoreOrdersTrackingType previously defined for StoreOrdersTracking
CREATE WACTIONSTORE StoreActivity
CONTEXT OF StoreActivityContext
EVENT TYPES (StoreOrdersTrackingType )
PERSIST NONE USING ();

CREATE CQ TrackStoreActivity
INSERT INTO StoreActivity
SELECT s.storeId,
  s.StartTime,
  n.storeName,
  s.category,
  s.status,
  s.ordersCount,
  s.salesAmount,
  s.hourlyAvg,
  s.upperLimit,
  s.lowerLimit,
  z.zip,
  z.city,
  s.state,
  z.latVal,
  z.longVal
FROM StoreOrdersTracking_Status s, StoreNameLookup n, ZipCodeLookup z
WHERE s.storeId = n.storeId AND s.zip = z.zip
LINK SOURCE EVENT;


-- RetailStoreFlow part 4 - send alerts


CREATE STREAM RetailAlertStream OF Global.AlertEvent;

CREATE CQ RetailRetailGenerateAlerts
INSERT INTO RetailAlertStream
SELECT n.storeName, s.storeId,
        CASE
          WHEN s.Status = 'OK' THEN 'info'
          ELSE 'warning' END,
        CASE
          WHEN s.Status = 'OK' THEN 'cancel'
          ELSE 'raise' END,
        CASE
          WHEN s.Status = 'OK' THEN 'Store ' + n.storeName + ' amount of $'+ s.salesAmount + ' is back between $' + s.lowerLimit + ' and $' +s.upperLimit
          WHEN s.Status = 'TOOHIGH' THEN 'Store ' + n.storeName + ' amount of $'+ s.salesAmount + ' is above upper limit of $' + s.upperLimit
          WHEN s.Status = 'TOOLOW' THEN 'Store ' + n.storeName + ' amount of $'+ s.salesAmount + ' is below lower limit of $' + s.lowerLimit
          ELSE ''
          END
FROM StoreOrdersTracking_Status s, StoreNameLookup n
WHERE s.storeId = n.storeId;

END FLOW BadStoreFlow;


-- load dashboard visualization settings from file

--CREATE VISUALIZATION BadDeployGroup "Samples/Customer/RetailApp/RetailApp_visualization_settings.json";

-- The following statement defines the user and delivery method for alerts.
CREATE SUBSCRIPTION BadAlertSub USING WebAlertAdapter( ) INPUT FROM RetailAlertStream;


END APPLICATION BadDeployGroup;

CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( 
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: '@CDC_URL@',
  Tables: '@Source1Tables@' ) 
OUTPUT TO @APPNAME@cdcStream;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@1 ( 
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',
  FetchSize: 20,
  DatabaseProviderType: 'Default',
  Table: '@Source3Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')  
OF @APPNAME@cachetype;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@2 ( 
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',
  FetchSize: 10,
  DatabaseProviderType: 'Default',
  Table: '@Source2Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')  
OF @APPNAME@cachetype;

use PosTester;
alter application PosApp;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;

end application PosApp;

alter application PosApp recompile;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_scnRange: 1000,
 _h_eoffDelay: 10,
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '',
  region: '')
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

END APPLICATION @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadpolicy:'EventCount:7'
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from @STREAM@;

end flow @APPNAME@_serverflow;

end application @APPNAME@;

--
-- Recovery Test 34 with two sources, two sliding time-count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5a9W/p  -> CQ1 -> WS
-- S2 -> Sc6a11W/p -> CQ2 -> WS
--

STOP Recov34Tester.RecovTest34;
UNDEPLOY APPLICATION Recov34Tester.RecovTest34;
DROP APPLICATION Recov34Tester.RecovTest34 CASCADE;
CREATE APPLICATION RecovTest34 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION RecovTest34;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE OR REPLACE APPLICATION @APPNAME@ recovery 5 second interval;

CREATE FLOW @APPNAME@_Agent_flow;

CREATE OR REPLACE SOURCE @SourceName@1 USING Global.MSJet (
  Tables: 'dbo.compsrc',
    username: 'qatest',
    DatabaseName: 'qatest',
    FetchTransactionMetadata: true,
    filterTransactionBoundaries: true,
    compression: false,
    ConnectionURL: '10.211.55.3:1433',
    CommittedTransactions: true,
    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
    SendBeforeImage: true,
    password: 'w3b@ct10n' )
OUTPUT TO @SRCINPUTSTREAM@;

END FLOW @APPNAME@_Agent_flow;

CREATE FLOW @APPNAME@_Agent_flow1;

CREATE OR REPLACE SOURCE @SourceName@2 USING Global.MSJet (
  Tables: 'dbo.compsrc',
    username: 'qatest',
    DatabaseName: 'qatest',
    FetchTransactionMetadata: true,
    filterTransactionBoundaries: true,
    compression: false,
    ConnectionURL: '10.211.55.3:1433',
    CommittedTransactions: true,
    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
    SendBeforeImage: true,
    password: 'w3b@ct10n' )
OUTPUT TO @SRCINPUTSTREAM@1;

END FLOW @APPNAME@_Agent_flow1;

CREATE FLOW @APPNAME@_server_flow;

CREATE OR REPLACE TARGET @targetName@1 USING Global.DatabaseWriter
(
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  BatchPolicy:'EventCount:10,Interval:60',
  CommitPolicy:'EventCount:10,Interval:60',
  ParallelThreads:'',
  CheckPointTable:'CHKPOINT',
  Password_encrypted:'false',
  Tables:'qatest.MSJEtsrc1,qatest.MSJEtar1;qatest.MSJEtsrc2,qatest.MSJEtar2;',
  CDDLAction:'Process',
  Password:'w3b@ct10n',
  StatementCacheSize:'50',
  ConnectionURL:'jdbc:sqlserver://10.211.55.3:1433;databaseName=qatest',
  DatabaseProviderType:'Default',
  Username:'qatest',
  PreserveSourceTransactionBoundary:'false',
  adapterName:'DatabaseWriter'
)
INPUT FROM @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@2 USING Global.DatabaseWriter
(
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  BatchPolicy:'EventCount:10,Interval:60',
  CommitPolicy:'EventCount:10,Interval:60',
  ParallelThreads:'',
  CheckPointTable:'CHKPOINT',
  Password_encrypted:'false',
  Tables:'qatest.MSJEtsrc1,qatest.MSJEtar1;qatest.MSJEtsrc2,qatest.MSJEtar2;',
  CDDLAction:'Process',
  Password:'w3b@ct10n',
  StatementCacheSize:'50',
  ConnectionURL:'jdbc:sqlserver://10.211.55.3:1433;databaseName=qatest',
  DatabaseProviderType:'Default',
  Username:'qatest',
  PreserveSourceTransactionBoundary:'false',
  adapterName:'DatabaseWriter'
)
INPUT FROM @SRCINPUTSTREAM@1;

CREATE TARGET @targetsys@ USING Global.SysOut (
  name: '@targetsys@' )
INPUT FROM @SRCINPUTSTREAM@;

END FLOW @APPNAME@_server_flow;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@ with @APPNAME@_Agent_flow in AGENTS, @APPNAME@_Agent_flow1 in AGENTS ,@APPNAME@_server_flow on any in default;
START APPLICATION @APPNAME@;

STOP application consoletest.noApp;
undeploy application consoletest.noApp;
drop application consoletest.noApp cascade;

DROP USER consoletest;
DROP NAMESPACE consoletest CASCADE;
CREATE USER consoletest IDENTIFIED BY consoletest;
REVOKE consoletest.admin from user consoletest;
GRANT create,drop ON deploymentgroup Global.consoletest To user consoletest;
GRANT all ON namespace Global.consoletest To user consoletest;
GRANT all ON Application consoletest.noApp To user consoletest;
REVOKE drop ON Application consoletest.noApp from user consoletest;
CONNECT consoletest consoletest;

create application noApp;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'posdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
directory:'@FEATURE-DIR@/logs/',
filename:'PermissionTarget',
sequence:'00',
rolloverpolicy:'eventcount:1222001'
)
format using DSVFormatter (

)
input from TypedCSVStream;
end application noApp;

--
-- Recovery Test 42 with two sources and two WactionStores. A variety of partitioned windows in between
-- assure that we are testing a complicated recovery scenario.
--
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Stream -> JWc5 -> WS1
--   S2 -> Stream -> JWc10 -> WS2
--

STOP Recov42Tester.RecovTest42;
UNDEPLOY APPLICATION Recov42Tester.RecovTest42;
DROP APPLICATION Recov42Tester.RecovTest42 CASCADE;
CREATE APPLICATION RecovTest42 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10242,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10242,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM DataStreamTop OF CsvData using KafkaProps;

CREATE CQ Csv1ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ Csv2ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;










CREATE JUMPING WINDOW LeftJWc5
OVER DataStreamTop KEEP 5 ROWS;

CREATE JUMPING WINDOW RightJWc10
OVER DataStreamTop KEEP 10 ROWS;



CREATE WACTIONSTORE WactionsLeft CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsRight CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ ToWactionsLeft
INSERT INTO WactionsLeft
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM LeftJWc5 p;

CREATE CQ ToWactionsRight
INSERT INTO WactionsRight
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM RightJWc10 p;

END APPLICATION RecovTest42;

CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (
  StartPosition: '@startPosition@',
  ConnectionURL: '@sourceURL@',
  Username: '@userName@',
  Tables: '@tables@',
  CheckColumn: '@checkColum@',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  Password: '@password@',
  Password_encrypted: 'false',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

stop application ADW;
undeploy application ADW;
drop application ADW cascade;
CREATE APPLICATION ADW;

CREATE  SOURCE OjetIL USING DatabaseReader  
 (
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'@SOURCE-TABLES@',
 FetchSize:2000
) 
OUTPUT TO InitialLoadStream;

CREATE TARGET AzureDWInitialLoad USING AzureSQLDWHWriter(
ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
)
INPUT FROM InitialLoadStream;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

create flow @STREAM@Flow1;
create type @STREAM@type (id string, name string);

CREATE OR REPLACE STREAM @STREAM@2 OF @STREAM@type;
CREATE OR REPLACE STREAM @STREAM@3 OF @STREAM@type;
CREATE OR REPLACE STREAM @STREAM@4 OF @STREAM@type;

create cq @STREAM@cq1
insert into @STREAM@2
select 
TO_STRING(data[0]).replaceAll("COMPANY ", ""),
data[1]
from @STREAM@;

end flow @STREAM@Flow1;


create flow @STREAM@Flow2;

create cq @STREAM@cq2
insert into @STREAM@3
select * 
from @STREAM@2 where id is not null ;

create cq @STREAM@cq3
insert into @STREAM@4
select * 
from @STREAM@3 where id is not null ;

end flow @STREAM@Flow2;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING FileWriter( 
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
   ) 
FORMAT USING dsvFormatter  ()
INPUT FROM @STREAM@4;

stop application admin.@APPNAME@;
undeploy application admin.@APPNAME@;
alter application admin.@APPNAME@ AUTORESUME MAXRETRIES 5 RETRYINTERVAL 5;
Alter application admin.@APPNAME@ RECOMPILE;
Alter application admin.@APPNAME@;
create or replace source admin.@APPNAME@s using FileReader (
        directory:'Product/IntegrationTests/TestData/',
        wildcard:'posdata5L.csv',
        positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:',',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  header: true,
  separator:'~'
)
OUTPUT TO admin.@APPNAME@in_memory_rawStream;

Alter application admin.@APPNAME@ RECOMPILE;
deploy application admin.@APPNAME@;
start admin.@APPNAME@;

--
-- Recovery Test 2
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS
--


STOP KStreamRecov2Tester.KStreamRecovTest2;
UNDEPLOY APPLICATION KStreamRecov2Tester.KStreamRecovTest2;
DROP APPLICATION KStreamRecov2Tester.KStreamRecovTest2 CASCADE;
DROP USER KStreamRecov2Tester;
DROP NAMESPACE KStreamRecov2Tester CASCADE;
CREATE USER KStreamRecov2Tester IDENTIFIED BY KStreamRecov2Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov2Tester;
CONNECT KStreamRecov2Tester KStreamRecov2Tester;

CREATE APPLICATION KStreamRecovTest2 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION KStreamRecovTest2;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;


CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using MysqlReader
(
   adapterName: MysqlReader,
   CDDLAction: Process,
   CDDLCapture: false,
   Compression: false,
   ConnectionURL: jdbc:mysql://localhost:3306/waction,
   FilterTransactionBoundaries: true,
   Password: ReaderPassword,
   SendBeforeImage: true,
   Tables: waction.MultiMultiDownstream_src,
   Username: ReaderUsername
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;

CREATE APPLICATION @APPNAME3@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName1@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME3@;
DEPLOY APPLICATION @APPNAME3@;
START APPLICATION @APPNAME3@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING XMLParserV2 ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE CQ @APPNAME@_CQ
INSERT INTO @APPNAME@_CQOut
SELECT
data.attributeValue("merchantid") as merchantID,
data.getText() as companyName
FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_CQOut;

END APPLICATION @APPNAME@;

STOP noParser;
UNDEPLOY APPLICATION noParser;
DROP APPLICATION noParser CASCADE;

CREATE APPLICATION noParser;

CREATE TYPE Atm(
productID String KEY,
stateID String,
productWeight int,
quantity double,
size long,
currentDate DateTime);


CREATE CACHE cache1 USING FileReader(
  directory: '@TEST-DATA-PATH@',
  wildcard: 'ISdata.csv',
  header: false,
  columndelimiter: ',',
  trimquote: false
  ) QUERY (keytomap:'productID') OF Atm;

END APPLICATION noParser;

create application KinesisTest 
 RECOVERY 10 SECOND INTERVAL
;
create source CSVSource using FileReader (
	directory:'/home/dz/src/product/Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  companyName String,
  merID String,
  primNum String,
  posDataCode int,
  dateTime String,
  expDate String,
  curCode String,
  Amnt String,
  termId String,
  zip String,
  city String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],data[2],TO_INT(data[3]),
       data[4],data[5],data[6],
       data[7],data[8],
       data[9],data[10]
FROM CsvStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM'
)
format using DSVFormatter (
)
input from TypedCSVStream;
end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

stop application APP_HEARTBEATS;
undeploy application APP_HEARTBEATS;
drop application APP_HEARTBEATS cascade;

CREATE APPLICATION APP_HEARTBEATS;

CREATE OR REPLACE CQ CQ_HB_90SEC 
INSERT INTO STREAM_CQ_HB_90SEC 
select makeWAEvent(dnow()) as dummy from heartbeat(interval 90 second) h;

CREATE OR REPLACE CQ CQ_HB_10SEC 
INSERT INTO STREAM_CQ_HB_10SEC 
select makeWAEvent(dnow()) as dummy from heartbeat(interval 10 second) h;

CREATE OR REPLACE CQ CQ_HB_1MIN 
INSERT INTO STREAM_CQ_HB_1MIN 
select makeWAEvent(dnow()) as dummy from heartbeat(interval 1 minute) h;

CREATE OR REPLACE CQ CQ_HB_30SEC 
INSERT INTO STREAM_CQ_HB_30SEC 
select makeWAEvent(dnow()) as dummy from heartbeat(interval 30 second) h;

CREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_90SEC OVER STREAM_CQ_HB_90SEC 
KEEP 1 ROWS;

CREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_10SEC OVER STREAM_CQ_HB_10SEC 
KEEP 1 ROWS;

CREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_1MIN OVER STREAM_CQ_HB_1MIN 
KEEP 1 ROWS;

CREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_30SEC OVER STREAM_CQ_HB_30SEC 
KEEP 1 ROWS;

END APPLICATION APP_HEARTBEATS;

STOP APPLICATION FileReaderWithBinaryParserApp;
UNDEPLOY APPLICATION FileReaderWithBinaryParserApp;
DROP APPLICATION FileReaderWithBinaryParserApp cascade;

CREATE APPLICATION FileReaderWithBinaryParserApp;

create source BinarySource using FileReader (
  directory:'@TEST-DATA-PATH@/binary',
  wildcard:'10rows.bin',
  positionByEOF:false
)
parse using BinaryParser (
  metadata:'@TEST-DATA-PATH@/binary/metadata.json',
  endian:true,
  StringTerminatedByNull:false
)
OUTPUT TO BinaryStream;

create Target DSVDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/binaryDetails') input from BinaryStream;
CREATE TARGET RawOut using SysOut(name: TCPRaw) INPUT FROM BinaryStream;


END APPLICATION FileReaderWithBinaryParserApp;
deploy application FileReaderWithBinaryParserApp;
start application FileReaderWithBinaryParserApp;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ RECOVERY 1 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @SourceName@ USING PostgreSQLReader  ( 
 ReplicationSlotName: 'striim_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src'
 ) 
OUTPUT TO @SRCINPUTSTREAM@ ;


CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy:'EventCount:1000,Interval:60',
CommitPolicy:'EventCount:1000,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM @SRCINPUTSTREAM@;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;

Create Source oracSource
 Using DatabaseReader
(
 Username:'oracle_username',
 Password:'oracle_password',
 ConnectionURL:'oracle_connection',
 Tables:'src_tables',
 Query:"SELECT * FROM qatest.oracle_alldatatypes",
 OnlineCatalog:true,
 FetchSize:10000,
 QueueSize:2048,
 CommittedTransactions:true,
 Compression:false
) Output To DataStream;
CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkuPDaTehAnDliNgmOdE:'DELETEANDINSERT',
tables: 'tgt_tables',
batchpolicy: 'EventCount:10000,Interval:30')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

--This TQL only works with HL7v2 2.3 Messages, specifically generated from SimHospital Docker Simulator

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE STREAM @APPNAME@_AlertStream OF Global.AlertEvent;

CREATE OR REPLACE SOURCE @APPNAME@_src USING Global.TCPReader ()
PARSE USING Global.HL7v2Parser ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE CQ @APPNAME@_ADT_CQ
INSERT INTO @APPNAME@_ADT_CQOut
SELECT 
data.element("MSH").element("MSH.6").element("HD.1").getText() as Facility,
TO_DATEF(data.element("MSH").element("MSH.7").element("TS.1").getText(),'yyyyMMddHHmmss') as MsgTime,
data.element("PID").element("PID.3").element("CX.1").getText() as PatientID, 
data.element("PID").element("PID.5").element("XPN.1").getText() as FirstName, 
data.element("PID").element("PID.5").element("XPN.2").getText() as LastName, 
TO_DATEF(SLEFT(data.element("PID").element("PID.7").element("TS.1").getText(),8),'yyyyMMdd') as DOB, 
data.element("PID").element("PID.8").getText() as Gender, 
data.element("PID").element("PID.11").element("XAD.5").getText() as ZipCode,
data.element("PV1").element("PV1.2").getText() as PatientClass,  
data.element("EVN").element("EVN.1").getText() as EventCode,
TO_DATEF(data.element("EVN").element("EVN.2").element("TS.1").getText(),'yyyyMMddHHmmss') as EventTime 
FROM @APPNAME@_Stream where data.getName() like "ADT%";

CREATE OR REPLACE CQ @APPNAME@_ORU_CQ
INSERT INTO @APPNAME@_ORU_CQOut
SELECT 
data.element("MSH").element("MSH.6").element("HD.1").getText() as Facility,
TO_DATEF(data.element("MSH").element("MSH.7").element("TS.1").getText(),'yyyyMMddHHmmss') as MsgTime,
data.element("ORU_R01.RESPONSE").element("ORU_R01.PATIENT").element("PID").element("PID.3").element("CX.1").getText() as PatientID,
data.element("ORU_R01.RESPONSE").element("ORU_R01.ORDER_OBSERVATION").element("OBR").element("OBR.4").element("CE.1").getText() as OrderIdentifier,
data.element("ORU_R01.RESPONSE").element("ORU_R01.ORDER_OBSERVATION").element("OBR").element("OBR.4").element("CE.2").getText() as OrderText 
FROM @APPNAME@_Stream where data.getName() like "ORU%";

CREATE TARGET @APPNAME@_FileTarget USING FileWriter ()
FORMAT USING Global.XMLFormatter  (
  rootelement: 'document' )
INPUT FROM @APPNAME@_Stream;

CREATE TARGET @APPNAME@_OLTPTarget USING DatabaseWriter ()
INPUT FROM @APPNAME@_ADT_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_NoSqlTarget USING MongoDBWriter ()
INPUT FROM @APPNAME@_ADT_CQOut;

--CREATE OR REPLACE TARGET @APPNAME@_KafkaTarget USING KafkaWriter VERSION @KAFKA_VERSION@()
--FORMAT USING JSONFormatter (
--schemaFileName: 'avroSchema'
--)
--INPUT FROM @APPNAME@_ORU_CQOut;

CREATE TARGET @APPNAME@_DWHTarget USING BigQueryWriter ()
INPUT FROM @APPNAME@_ORU_CQOut;

CREATE TARGET @APPNAME@_ADLSTarget USING AdlsGen2Writer ()
FORMAT USING Global.JSONFormatter  ()
INPUT FROM @APPNAME@_ORU_CQOut;

CREATE OR REPLACE CQ @APPNAME@_ADT_FilterCQ
INSERT INTO @APPNAME@_AlertCQ
SELECT 
PatientID as PatientID,
PatientClass as PatientClass
FROM @APPNAME@_ADT_CQOut p where  p.PatientClass='E';;

CREATE OR REPLACE CQ @APPNAME@_AlertEventCQ
INSERT INTO @APPNAME@_AlertStream
SELECT 
'Patient Found' as name,
'Emergency' as keyVal,
'info' as severity,
'raise' as flag,
'Flagged patient `' + f.PatientID +'` is an Emergency Patient. NEED IMMEDIATE ATTENTION!!!' as message
FROM @APPNAME@_AlertCQ f;

CREATE OR REPLACE TARGET @APPNAME@_SlackTarget USING SlackAlertAdapter ()
INPUT FROM @APPNAME@_AlertStream;

END APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;

Create Source @SourceName@ Using Ojet

(
  Username:'c##qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',
  Tables:'CDB$ROOT."C##QATEST".ojet_src;ORCLPDB.QATEST.ojet_src',
  _h_useClassic:false,
  Fetchsize:1,
  Compression: true,
  SupportPDB:true,
  ReplicationSlotName:'null'
)
Output To @SRCINPUTSTREAM@;

CREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'))) FROM @SRCINPUTSTREAM@ x; ;


CREATE TARGET @targetName@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',
  Username:'c##qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'CDB$ROOT."C##QATEST".ojet_src,CDB$ROOT."C##QATEST".ojet_tgt'
) INPUT FROM admin.sqlreader_cq_out;



create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP QueryAggTester.ws_one;
UNDEPLOY APPLICATION QueryAggTester.ws_one;
DROP APPLICATION QueryAggTester.ws_one cascade;

CREATE APPLICATION ws_one;


CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;


CREATE TYPE wsData
(
bankID Integer KEY,
bankName String
);


CREATE STREAM wsStream OF wsData;

--Select data from QaStream and insert into wsStream

CREATE CQ csvTowsData
INSERT INTO wsStream
SELECT TO_INT(data[0]),data[1] FROM QaStream;

--create jumping window over data in wsStream

CREATE WACTIONSTORE oneWS
CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@

--get data from wsStream and place into wactionStore oneWS
CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT * FROM wsStream
LINK SOURCE EVENT;


END APPLICATION ws_one;

Create Source @SOURCE_NAME@ Using OracleReader
(
 Compression: false,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  --StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
) Output To @STREAM@;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;

drop propertyset OrcToOrcPlatfm_App_KafkaPropset;
drop stream  OrcToOrcPlatfm_Stream CASCADE;


--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;

CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNEC4TION_URL@',
  Tables: '$table1',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'QATEST.ORCTOORCPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '$table2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@4',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'QATEST.ORCTOORCPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;

END APPLICATION @APPNAME@2;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  directory:'@FEATURE-DIR@/logs/',
  filename:'RoundUPPosData',
  rolloverpolicy:'FileSizeRollingPolicy,filesize:2.5M,sequence:00'
)
format using DSVFormatter (

)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizeRoundUp_actual.log') input from TypedCSVStream;

end application DSV;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW ;

 

CREATE OR REPLACE SOURCE DBSource USING DatabaseReader  ( 
  Username: '@SOURCE_USER@',
  Password_encrypted: false,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Query: "SELECT * FROM waction.MysqlToCql_alldatatypes",
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SOURCE_PASS@'
 ) 
OUTPUT TO Oracle_ChangeDataStream;


CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

CREATE APPLICATION applicationApiSaas;

CREATE FLOW SaasSourceFlow;

CREATE SOURCE SaasLog4JSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'SaasMonitorLog4JOutput.xml',
  positionByEOF:false
)
PARSE USING XMLParser(
  rootnode:'/log4j:event',
  columnlist:'log4j:event/@timestamp,log4j:event/@level,log4j:event/log4j:message,log4j:event/log4j:locationInfo/@method'
)
OUTPUT TO SaasRawXMLStream;

CREATE TYPE SaasLog4JEvent (
    ts DateTime,
    level String,
    message String,
    method String
);
CREATE STREAM SaasLog4JStream OF SaasLog4JEvent;

CREATE CQ SaasGetLog4JData
INSERT INTO SaasLog4JStream
SELECT TO_DATE(TO_LONG(data[0])), data[1], data[2], data[3]
FROM SaasRawXMLStream;

CREATE STREAM SaasErrorStream OF SaasLog4JEvent;
CREATE STREAM SaasWarningStream OF SaasLog4JEvent;
CREATE STREAM SaasInfoStream OF SaasLog4JEvent;

CREATE CQ SaasGetErrors
INSERT INTO SaasErrorStream
SELECT log4j
FROM SaasLog4JStream log4j WHERE log4j.level = 'ERROR';

CREATE CQ SaasGetWarnings
INSERT INTO SaasWarningStream
SELECT log4j
FROM SaasLog4JStream log4j WHERE log4j.level = 'WARN';

CREATE CQ SaasGetInfo
INSERT INTO SaasInfoStream
SELECT log4j
FROM SaasLog4JStream log4j WHERE log4j.level = 'INFO';

END FLOW SaasSourceFlow;


CREATE FLOW SaasErrorFlow;

CREATE STREAM SaasErrorAlertStream OF Global.AlertEvent;

CREATE CQ SaasSendErrorAlerts
INSERT INTO SaasErrorAlertStream
SELECT 'ErrorAlert', ''+ts, 'error', 'raise', 'Error in log ' + message
FROM SaasErrorStream;

CREATE SUBSCRIPTION SaasErrorAlertSub USING WebAlertAdapter( ) INPUT FROM SaasErrorAlertStream;

END FLOW SaasErrorFlow;


CREATE FLOW SaasWarningFlow;

CREATE JUMPING WINDOW SaasWarningWindow
OVER SaasWarningStream KEEP WITHIN 30 SECOND ON ts;

CREATE STREAM SaasWarningAlertStream OF Global.AlertEvent;

CREATE CQ SaasSendWarningAlerts
INSERT INTO SaasWarningAlertStream
SELECT 'WarningAlert', ''+ts, 'warning', 'raise',
        COUNT(ts) + ' Warnings in log for api ' + method
FROM SaasWarningWindow
GROUP BY method
HAVING count(ts) > 25;
CREATE SUBSCRIPTION SaasWarningAlertSub USING WebAlertAdapter( ) INPUT FROM SaasWarningAlertStream;

END FLOW SaasWarningFlow;


CREATE FLOW SaasInfoFlow;

CREATE TYPE SaasApiCall (
  userId String,
  apiCall String,
  sobject String,
  ts DateTime,
  userName String,
  company String,
  userZip String,
  companyZip String
);
CREATE STREAM SaasApiStream OF SaasApiCall;

CREATE CQ SaasGetApiDetails
INSERT INTO SaasApiStream(userId,apiCall,sobject,ts)
SELECT MATCH(i.message, '\\\\[user=([a-zA-Z0-9]*)\\\\]'),
       MATCH(i.message, '\\\\[api=([a-zA-Z0-9]*)\\\\]'),
       MATCH(i.message, '\\\\[sobject=([a-zA-Z0-9]*)\\\\]'),
       i.ts
FROM SaasInfoStream i;

CREATE STREAM SaasApiEnrichedStream OF SaasApiCall;

CREATE TYPE SaasUserInfo (
  userId String,
  userName String,
  company String,
  userZip String,
  companyZip String
);
CREATE CACHE SaasUserLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'userLookup.txt',
  header: Yes,
  columndelimiter: ','
) QUERY (keytomap:'userId') OF SaasUserInfo;


CREATE CQ SaasGetUserDetails
INSERT INTO SaasApiEnrichedStream
SELECT a.userId, a.apiCall, a.sobject, a.ts, u.userName, u.company, u.userZip, u.companyZip
FROM SaasApiStream a, SaasUserLookup u
WHERE a.userId = u.userId;

END FLOW SaasInfoFlow;


CREATE FLOW SaasUserApiFlow;

CREATE JUMPING WINDOW SaasUserApiWindow
OVER SaasApiEnrichedStream KEEP WITHIN 5 SECOND ON ts
PARTITION BY userId;

CREATE TYPE SaasUserApiUsage (
  userId String key,
  userName String,
  userZip String,
  userLat double,
  userLong double,
  company String,
  apiCall String,
  count integer,
  state String,
  topObject String,
  ts DateTime
);
CREATE STREAM SaasUserApiUsageStream OF SaasUserApiUsage;

CREATE TYPE SaasUSAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);
CREATE CACHE SaasZipLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  header: Yes,
  columndelimiter: '	'
) QUERY (keytomap:'zip') OF SaasUSAddressData;


CREATE CQ SaasGetUserApiUsage
INSERT INTO SaasUserApiUsageStream
SELECT a.userId, a.userName, a.userZip, z.latVal, z.longVal, a.company,
       a.apiCall, COUNT(a.sobject),
       CASE WHEN COUNT(a.sobject) > 175 THEN 'HOT'
            ELSE 'COLD' END,
       MAXOCCURS(a.sobject),
       FIRST(a.ts)
FROM SaasUserApiWindow a, SaasZipLookup z
WHERE a.userZip = z.zip
GROUP BY a.userId, a.apiCall
HAVING FIRST(a.ts) IS NOT NULL;

CREATE TYPE SaasUserApiContext (
  userId String key,
  userName String,
  userZip String,
  userLat double,
  userLong double,
  company String,
  count integer,
  state String,
  topObject String,
  ts DateTime
);
CREATE WACTIONSTORE SaasUserApiActivity
CONTEXT OF SaasUserApiContext
EVENT TYPES ( SaasUserApiUsage )
@PERSIST-TYPE@

CREATE JUMPING WINDOW SaasUserWindow
OVER SaasUserApiUsageStream KEEP WITHIN 5 SECOND ON ts
PARTITION BY userId;

CREATE CQ SaasTrackUserApiSummary
INSERT INTO SaasUserApiActivity
SELECT a.userId, a.userName, a.userZip, a.userLat, a.userLong,
       a.company, SUM(count),
       CASE WHEN SUM(count) > 1000 THEN 'HOT'
            ELSE 'COLD' END,
       MAXOCCURS(a.topObject),
       FIRST(a.ts)
FROM SaasUserWindow a
GROUP BY a.userId
LINK SOURCE EVENT;

CREATE STREAM SaasUserApiAlertStream OF Global.AlertEvent;

CREATE CQ SaasSendUserApiAlerts
INSERT INTO SaasUserApiAlertStream
SELECT 'UserAPIAlert', ''+ts, 'warning', 'raise',
       'User ' + userName + ' has used api ' + apiCall + ' ' + count + ' times for ' + topObject
FROM SaasUserApiUsageStream
WHERE state = 'HOT' AND count > 300;

CREATE SUBSCRIPTION SaasUserApiAlertSub USING WebAlertAdapter( ) INPUT FROM SaasUserApiAlertStream;

END FLOW SaasUserApiFlow;


CREATE FLOW SaasCompanyApiFlow;

CREATE JUMPING WINDOW SaasCompanyApiWindow
OVER SaasApiEnrichedStream KEEP WITHIN 5 SECOND ON ts
PARTITION BY company;

CREATE TYPE SaasCompanyApiUsage (
  company String key,
  companyZip String,
  companyLat double,
  companyLong double,
  apiCall String,
  count integer,
  state String,
  topObject String,
  ts DateTime
);
CREATE STREAM SaasCompanyApiUsageStream OF SaasCompanyApiUsage;

CREATE CQ SaasGetCompanyApiUsage
INSERT INTO SaasCompanyApiUsageStream
SELECT a.company, a.companyZip, z.latVal, z.longVal,
       a.apiCall, COUNT(a.sobject),
       CASE WHEN COUNT(a.sobject) > 3000 THEN 'HOT'
            ELSE 'COLD' END,
       MAXOCCURS(a.sobject),
       FIRST(a.ts)
FROM SaasCompanyApiWindow a, SaasZipLookup z
WHERE a.companyZip = z.zip
GROUP BY a.company, a.apiCall
HAVING FIRST(a.ts) IS NOT NULL;

CREATE TYPE SaasCompanyApiContext (
  company String key,
  companyZip String,
  companyLat double,
  companyLong double,
  count integer,
  state String,
  topObject String,
  ts DateTime
);
CREATE JUMPING WINDOW SaasCompanyWindow
OVER SaasCompanyApiUsageStream KEEP WITHIN 5 SECOND ON ts
PARTITION BY company;

CREATE WACTIONSTORE SaasCompanyApiActivity
CONTEXT OF SaasCompanyApiContext
EVENT TYPES ( SaasCompanyApiUsage )
@PERSIST-TYPE@


CREATE CQ SaasTrackCompanyApiSummary
INSERT INTO SaasCompanyApiActivity
SELECT a.company, a.companyZip, a.companyLat, a.companyLong,
       SUM(a.count),
       CASE WHEN SUM(a.count) > 15000 THEN 'HOT'
            ELSE 'COLD' END,
       MAXOCCURS(a.topObject),
       FIRST(a.ts)
FROM SaasCompanyWindow a
GROUP BY a.company
LINK SOURCE EVENT;

END FLOW SaasCompanyApiFlow;


CREATE FLOW SaasApiFlow;

CREATE JUMPING WINDOW SaasApiWindow
OVER SaasApiEnrichedStream KEEP WITHIN 5 SECOND ON ts
PARTITION BY apiCall;

CREATE TYPE SaasApiUsage (
  apiCall String key,
  sobject String,
  count integer,
  state String,
  ts DateTime
);
CREATE STREAM SaasApiUsageStream OF SaasApiUsage;

CREATE CQ SaasGetApiUsage
INSERT INTO SaasApiUsageStream
SELECT a.apiCall, a.sobject,
       COUNT(a.userId),
       CASE WHEN COUNT(a.userId) > 3000 THEN 'HOT'
            ELSE 'COLD' END,
       FIRST(a.ts)
FROM SaasApiWindow a
GROUP BY a.apiCall, a.sobject
HAVING FIRST(a.ts) IS NOT NULL;

CREATE TYPE SaasApiContext (
  apiCall String key,
  count integer,
  state String,
  topObject String,
  ts DateTime
);

CREATE JUMPING WINDOW SaasApiAggWindow
OVER SaasApiUsageStream KEEP WITHIN 5 SECOND ON ts
PARTITION BY apiCall;

CREATE WACTIONSTORE SaasApiActivity
CONTEXT OF SaasApiContext
EVENT TYPES ( SaasApiUsage )
@PERSIST-TYPE@

CREATE CQ SaasTrackApiSummary
INSERT INTO SaasApiActivity
SELECT a.apiCall,
       SUM(a.count),
       CASE WHEN SUM(a.count) > 20000 THEN 'HOT'
            ELSE 'COLD' END,
       MAXOCCURS(a.sobject),
       first(a.ts)
FROM SaasApiAggWindow a
GROUP BY a.apiCall
LINK SOURCE EVENT;

END FLOW SaasApiFlow;

END APPLICATION applicationApiSaas;

create application JuniperLog;
create source JuniperLogSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'juniper-NSM*',
	charset:'UTF-8',
	positionByEOF:false
) PARSE USING JuniperNSMLogParser (
	trimwhitespace: yes
)
OUTPUT TO JuniperLogStream;
create Target JuniperDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/junipernsm_log') input from JuniperLogStream;
end application JuniperLog;

use PosTester;
alter application PosApp;

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

end application PosApp;

alter application PosApp recompile;

STOP application consoleconsoletest.noApp;
undeploy application consoleconsoletest.noApp;
drop application consoleconsoletest.noApp cascade;

DROP USER consoletest;
DROP NAMESPACE consoletest CASCADE;
CREATE USER consoletest IDENTIFIED BY consoletest;

--Creating two roles
Create role consoletest.R1;
Create role consoletest.R2;

--Granting consoletest.dev to new role R1 and granting single role of Drop to R2
Grant consoletest.dev to role consoletest.R1;
Grant deploy on application consoletest.* to role consoletest.dev;
Grant undeploy on application consoletest.* to role consoletest.dev;
Revoke update on Target consoletest.noApp from role consoletest.dev;
Revoke select on WACTIONSTORE consoletest.* from role consoletest.dev;
Revoke consoletest.admin from user consoletest;
Grant drop on application consoletest.* to role consoletest.R2;

--Granting new roles to the user
Grant consoletest.R1 to user consoletest;
Grant consoletest.R2 to user consoletest;

--Creating application
Create application consoletest.noApp;

CREATE TYPE consoletest.Atm(
productID String KEY,
stateID String,
productWeight int,
quantity double,
size long,
currentDate DateTime);

CREATE source consoletest.implicitSOurce USING FileReader (
directory:'@TEST-DATA-PATH@',
columndelimiter: ',',
wildcard:'ISdata.csv',
blocksize: 10240,
positionByEOF:false
)
PARSE USING DSVParser (
header:False,
trimquote:false
) OUTPUT TO consoletest.CsvStream;

CREATE TYPE consoletest.wsType(
quantity double KEY,
currentDate DateTime
);

CREATE STREAM consoletest.newStream OF consoletest.Atm;

CREATE CQ consoletest.newCQ
INSERT INTO consoletest.newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]),TO_DATE(data[5]) FROM
consoletest.CsvStream;

CREATE WINDOW consoletest.win1
OVER consoletest.newStream
keep within 3 minute;

CREATE STREAM consoletest.newStream2 of consoletest.wsType;

CREATE WACTIONSTORE consoletest.WS1 CONTEXT OF consoletest.wsType
EVENT TYPES(consoletest.wsType );

Create cq consoletest.newCQ2
insert into consoletest.ws1 (quantity,currentDate)
select quantity, currentDate from consoletest.newStream;

Create Target consoletest.Trace Using Sysout (name: 'Trace') Input From consoletest.newStream;

End Application consoletest.noApp;


Connect consoletest consoletest;


Alter application noApp;

create or replace type consoletest.wsType(quantity double KEY, size long, currentDate DateTime);

Create or replace Target consoletest.Trace1 Using Sysout (name:'Trace1') Input From consoletest.newStream;

end application consoletest.noApp;
alter application consoletest.noApp recompile;

Deploy application consoletest.noApp;
Start application consoletest.noApp;

-- The following step should fail as expected 
Select count (*) from WS1;

CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  (
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: '@CDC_URL@',
  DatabaseName:'@DatabaseName@',
  Tables: '@Source1Tables@' )
OUTPUT TO @APPNAME@cdcStream;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@1 (
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
  FetchSize: 20,
  DatabaseProviderType: 'Default',
  Table: '@Source3Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')
OF @APPNAME@cachetype;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@2 (
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
  FetchSize: 10,
  DatabaseProviderType: 'Default',
  Table: '@Source2Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')
OF @APPNAME@cachetype;

STOP APPLICATION LongRunningQueryTester.LongRunningQuery;
UNDEPLOY APPLICATION LongRunningQueryTester.LongRunningQuery;
DROP APPLICATION LongRunningQueryTester.LongRunningQuery cascade;

CREATE APPLICATION LongRunningQuery;




CREATE TYPE RandomData(
myName String,
streetAddress String,
bankName String,
bankNumber int KEY,
bankAmount double
);


CREATE SOURCE ranDataSource using StreamReader(
OutputType: 'LongRunningQueryTester.RandomData',
noLimit: 'false',
maxRows: 0,
iterations: 2,
iterationDelay: 1000,
StringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',
NumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]G'
)OUTPUT TO CSVDataStream;

CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4])
FROM CSVDataStream;




CREATE TYPE myData(
myName String,
myAddress String,
myBankName String,
myBankNumber int KEY,
myBankAmount double
);

CREATE STREAM myDataStream OF myData;

CREATE CQ GetMyData
INSERT INTO MyDataStream
SELECT myName, streetAddress, bankName, bankNumber, bankAmount
FROM RandomDataStream;


CREATE WACTIONSTORE MyDataActivity
CONTEXT OF MyData
EVENT TYPES(myData )
@PERSIST-TYPE@

Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
Select * from myDataStream
LINK SOURCE EVENT;


END APPLICATION LongRunningQuery;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ Recovery 5 second interval;
--create application @APPNAME@;

create type @APPNAME@employee
(
id integer,
name String,
company String
);
CREATE STREAM @APPNAME@cosmoscassandra_TypedStream of @APPNAME@employee;

CREATE OR REPLACE SOURCE @APPNAME@OnPrem_Oracle USING OracleReader  (
  Compression: false,
  StartTimestamp: 'null',
  SupportPDB: false,
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  FetchSize: 1,
  DDLCaptureMode: 'All',
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'localhost:1521:xe',
 Tables: 'QATEST.EMP%',
-- Tables: 'QATEST.OracToCql_alldatatypes',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  TransactionBufferType: 'Memory',
  DictionaryMode: 'OnlineCatalog',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '1MB'
 )
OUTPUT TO @APPNAME@kperststream ;

create stream @APPNAME@UserdataStream1 of Global.WAEvent;

Create CQ @APPNAME@CQUser
insert into @APPNAME@UserdataStream1
select putuserdata (data1,'OperationName',META(data1,'OperationName').toString()) from @APPNAME@kperststream data1;

Create CQ @APPNAME@CQUser_typed
insert into @APPNAME@cosmoscassandra_TypedStream
select 
to_int(data[0]),
data[1],
data[2]
from @APPNAME@UserdataStream1 u 
where USERDATA(u,'OperationName').toString()=='INSERT' and meta(u,'TableName').toString()="QATEST.EMP3";


CREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target1 USING CassandraCosmosDBWriter  ( 
  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --CheckPointTable: '',
  FlushPolicy: 'EventCount:1000,Interval:60',
  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',
  AccountKey:'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  --ConsistencyLevel:'fh',
  --port:'10505',
  Tables: 'test.emp_tgt',
  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',
  adapterName: 'CassandraCosmosDBWriter'
 ) 
INPUT FROM @APPNAME@cosmoscassandra_TypedStream;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

--
-- Recovery Test 13 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same compound key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> CW(p#p) -> CQ -> WS
--

STOP Recov13Tester.RecovTest13;
UNDEPLOY APPLICATION Recov13Tester.RecovTest13;
DROP APPLICATION Recov13Tester.RecovTest13 CASCADE;
CREATE APPLICATION RecovTest13 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTest10Data.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  partKey String KEY,
  serialNumber int,
  partKey2 String KEY
);

CREATE TYPE WactionData (
  partKey String KEY,
  serialNumber int
);

CREATE STREAM DataStream OF CsvData PARTITION BY partKey, partKey2;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    TO_INT(data[1]),
    data[0]
FROM CsvStream;

CREATE JUMPING WINDOW DataStreamTwoItems
OVER DataStream KEEP 2 ROWS
PARTITION BY partKey, partKey2;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    first(partKey),
    to_int(first(serialNumber))
FROM DataStreamTwoItems
GROUP BY partKey, partKey2;

END APPLICATION RecovTest13;

--
-- Kafka Stream Recovery Test 1 New with internal test data generation
-- Bert Hashemi and Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> KS -> WS

STOP KStreamRecovTester.KStreamRecovTest;
UNDEPLOY APPLICATION KStreamRecovTester.KStreamRecovTest;
DROP APPLICATION KStreamRecovTester.KStreamRecovTest CASCADE;
DROP USER KStreamRecovTester;
DROP NAMESPACE KStreamRecovTester CASCADE;
CREATE USER KStreamRecovTester IDENTIFIED BY KStreamRecovTester;
-- GRANT 'Global:create,drop:deploymentgroup:*' TO USER KStreamRecov1Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecovTester;
CONNECT KStreamRecovTester KStreamRecovTester;

CREATE APPLICATION KStreamRecovTest RECOVERY 5 SECOND INTERVAL;

CREATE or REPLACE TYPE KafkaType(
  value java.lang.Long KEY
);

CREATE SOURCE KafkaSource USING NumberSource (
  lowValue: '1',
  highValue: '1003',
  delayMillis: '10',
  delayNanos: '0',
  repeat: 'false'
 )
OUTPUT TO NumberSourceOut;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaStream OF KafkaType persist using KafkaPropset;

CREATE OR REPLACE CQ KafkaStreamPopulate
INSERT INTO KafkaStream
SELECT data[1]
FROM NumberSourceOut;

CREATE WACTIONSTORE Wactions CONTEXT of KafkaType
@PERSIST-TYPE@

CREATE CQ WactionsPopulate
INSERT INTO Wactions
SELECT * FROM KafkaStream;

END APPLICATION KStreamRecovTest;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using googlepubsubwriter(
    ServiceAccountKey:'@SAS-KEY@',
ProjectId:'@PROJECTID@',
topic:'@topic@',
BatchPolicy:'@BATCHPOLICY@'
)
format using DSVFormatter (
)
input from @STREAM@;


end flow @APPNAME@_serverflow;

end application @APPNAME@;

STOP APPLICATION KafkaWPTester.KWApp;
STOP APPLICATION KafkaWPTester.KRApp;
UNDEPLOY APPLICATION KafkaWPTester.KWApp;
UNDEPLOY APPLICATION KafkaWPTester.KRApp;
DROP APPLICATION KafkaWPTester.KWApp CASCADE;
DROP APPLICATION KafkaWPTester.KRApp CASCADE;

CREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;
GRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;
CONNECT KafkaWPTester KafkaWPTester;


CREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE CSVSource USING FileReader (
	directory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',
    WildCard:'access_log_half',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;

CREATE TYPE AccessLogType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;

CREATE CQ AceeslogCQ
INSERT INTO TypedAccessLogStream
SELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]
FROM FileStream;

create Target DsvWriter using KafkaWriter VERSION '0.11.0' ( 
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V11dsvExistTopic',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream;


create Target JsonWriter using KafkaWriter VERSION '0.11.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V11jsonExistTopic',
	Mode:'Sync'
    )
 
FORMAT USING JSONFormatter ()
input from TypedAccessLogStream;


create Target AvroWriter using KafkaWriter VERSION '0.11.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V11avroExistTopic',
    Mode:'Sync'
    )
 
FORMAT USING AvroFormatter (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')
input from TypedAccessLogStream;


END APPLICATION KWApp;

DEPLOY APPLICATION KWApp on any in default;


-- Kafka Reader Apps

CREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;


CREATE TYPE AccessLogType2(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE Stream KafkaDSVReaderStream of AccessLogType2;
CREATE Stream KafkaJsonReaderStream of AccessLogType2;
CREATE Stream KafkaAvroReaderStream of AccessLogType2;


CREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '0.11.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V11dsvExistTopic',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;




CREATE CQ DSVtoKafkaDSVReaderStream
INSERT INTO KafkaDSVReaderStream
SELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]
FROM KafkaReaderStream1;

CREATE TARGET DSVDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V11_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaDSVReaderStream;

CREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '0.11.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V11jsonExistTopic',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;




CREATE CQ DSVtoKafkaJsonReaderStream
INSERT INTO KafkaJsonReaderStream
SELECT 
data.get('Col1').textValue(),
data.get('Col2').textValue(),
data.get('Col3').textValue(),
data.get('Col4').textValue(),
data.get('Col5').textValue(),
data.get('Col6').textValue(),
data.get('Col7').textValue(),
data.get('Col8').textValue(),
data.get('Col9').textValue(),
data.get('Col10').textValue()
FROM KafkaReaderStream2;

CREATE TARGET JSONDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V11_RT_JSON')
FORMAT USING DSVFormatter()
INPUT FROM KafkaJsonReaderStream;

CREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '0.11.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V11avroExistTopic',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'
)
OUTPUT TO KafkaReaderStream3;



CREATE Stream KRTypedStream of AccessLogType;

CREATE CQ DSVtoKafkaAvroReaderStream
INSERT INTO KafkaAvroReaderStream
SELECT    
data.get("Col1").toString(), 
data.get("Col2").toString(), 
data.get("Col3").toString(), 
data.get("Col4").toString(),
data.get("Col5").toString(), 
data.get("Col6").toString(), 
data.get("Col7").toString(), 
data.get("Col8").toString(),
data.get("Col9").toString(), 
data.get("Col10").toString()
FROM KafkaReaderStream3;

CREATE TARGET AVRODump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V11_RT_AVRO')
FORMAT USING DSVFormatter()
INPUT FROM KafkaAvroReaderStream;

END APPLICATION KRApp;
DEPLOY APPLICATION KRApp on any in default;

STOP DSAPP;
UNDEPLOY APPLICATION DSAPP;
DROP APPLICATION DSAPP CASCADE;

CREATE APPLICATION DSAPP;

-- CacheWaction WACTIONSTORE is being loaded from DSCache

CREATE OR REPLACE WACTIONSTORE CacheWaction CONTEXT OF T1
EVENT TYPES ( T1 )
@PERSIST-TYPE@

CREATE CQ CQ2Derby
INSERT INTO CacheWaction
select * from C1
LINK SOURCE EVENT;

END APPLICATION DSAPP;
DEPLOY APPLICATION DSAPP;
START APPLICATION DSAPP;

create or replace propertyvariable TEAMSREFTOKEN = '@refershtoken@';
 create or replace propertyvariable TEAMSCLIENTSECRET = '@clientSecret@';
 create or replace propertyvariable TEAMSCHANNELURL = '@channelUrl@';
 create or replace propertyvariable TEAMSCLIENTID = '@clientID@';

CREATE APPLICATION @AppName@ WITH ENCRYPTION EXCEPTIONHANDLER (AdapterException: 'IGNORE', ArithmeticException: 'IGNORE', ClassCastException: 'IGNORE', ConnectionException: 'IGNORE', InvalidDataException: 'IGNORE', NullPointerException: 'IGNORE', NumberFormatException: 'IGNORE', SystemException: 'IGNORE', UnExpectedDDLException: 'IGNORE', UnknownException: 'IGNORE')  USE EXCEPTIONSTORE TTL : '7d' ;

CREATE OR REPLACE SOURCE OracleReader_AlertSource USING oraclereader (
  ConnectionURL: '@ConnectionURl@',
  Tables: '@table@',
  Password: '@password@',
  Username: '@username@' )
OUTPUT TO AlertUpgradeStream;

CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE STREAM SlackAndTeamsStream OF Global.SlackAlertEvent;

CREATE OR REPLACE TARGET SmartAlertUpgradeSysOut USING SysOut (
  name: 'SmartAlertUpgradeSysOut' )
INPUT FROM AlertUpgradeStream;

CREATE OR REPLACE CQ GenerateAlertsForWebAndEmail
INSERT INTO AlertStream
SELECT data[1],
 data[1],
 'info',
 'raise',
'Welcome\n to Striim'
FROM AlertUpgradeStream;


 CREATE OR REPLACE CQ GenerateAlertsForSlackAndTeams
 INSERT INTO SlackAndTeamsStream
 SELECT data[1],
  data[1],
 'info',
 'raise',
 'Welcome\n to Striim',
 'TestChannel'
 FROM AlertUpgradeStream;


CREATE TARGET TeamsAlertSender USING Global.TeamsAlertAdapter (
  refreshToken: '$TEAMSREFTOKEN',
  clientSecret: '$TEAMSCLIENTSECRET',
  channelURL: '$TEAMSCHANNELURL',
  clientID: '$TEAMSCLIENTID' )
INPUT FROM SlackAndTeamsStream;


CREATE SUBSCRIPTION WebAlertSender USING WebAlertAdapter (
  isSubscription: 'true',
  channelName: 'admin_PosAppWebAlert' )
INPUT FROM AlertStream;


CREATE TARGET slackalertSender USING Global.SlackAlertAdapter (
  OauthToken: '@oauth@',
  ChannelName: '@slackChannel@',
  OauthToken_encrypted: 'false' )
INPUT FROM SlackAndTeamsStream;

CREATE SUBSCRIPTION EmailAlertsender USING EmailAdapter (
  smtpurl: '@smtpUrl@',
  starttls_enable: '@starttls@',
  SMTPUSER: '@smtpuser@',
  SMTPPASSWORD: '@stmpPwsd@',
  emailList: '@emailList@',
  subject: '@subject@',
  senderEmail: '@senderEmail@',
  SMTPPASSWORD_encrypted: 'false')
INPUT FROM AlertStream;

END APPLICATION @AppName@;

CREATE FLOW ServerFlow;


CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
Float_col Double,
BFLoat_col Double,
bdouble_col Double,
long_col String,
Table String,
Operation String
);

CREATE STREAM CDCFilteredStream OF LogType;

CREATE CQ ToFilteredStream
INSERT INTO CDCFilteredStream
SELECT data[0],
data[1],
data[2],
to_double(data[3]),
to_double(data[4]),
to_double(data[5]),
data[6],
META(a, "TableName"),
META(a, "OperationName")
from @STREAM@ a;



CREATE WINDOW CDCWindow
OVER CDCFilteredStream
KEEP 3 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

END FLOW ServerFlow;


CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
 )
INPUT FROM @STREAM@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
  directory: '',
  positionByEOF: false,
  WildCard: '' )
PARSE USING ParquetParser (
 )
OUTPUT TO @appname@Streams;

CREATE OR REPLACE CQ @appname@CQOrder3
INSERT INTO @appname@Stream3
SELECT
PUTUSERDATA(s,'schemaName',s.data.getSchema().getName())
FROM @appname@Streams s;

CREATE TARGET @filetarget@ USING Global.FileWriter (
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000,Interval:30s',
  directory: '',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  filename: 'AvroOutFile',
  DataEncryptionKeyPassphrase_encrypted: 'true',
  rolloverpolicy: 'EventCount:390,Interval:30m' )
FORMAT USING Global.AvroFormatter  (
  schemaFileName: 'AvroSchema',
  formatAs: 'default',
  schemaregistryConfiguration: '' )
INPUT FROM @appname@Stream3;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

create application SybaseJaguarApp;

create source SybaseJaguarSource using FileReader (
        directory:'@TEST-DATA-PATH@',
        WildCard:'SybaseJaguar.log',
        charset:'ISO-8859-1',
        positionByEOF:false
)
parse using FreeformTextParser (
        -- TimeStamp:'%E %mon %d %H:%M:%S %y %z',
        RecordBegin:'%H:%M:%S:%sss %yyyy%mm%d:1234567890:\n[TRACE]',
        RecordEnd:')\n',
	IgnoreMultipleRecordBegin: false,	
        regex:'((?<=logonname\\()[a-z,0-9]*)|((?<=localHostName\\()[a-z,A-Z,0-9]*)|((?<=localIP\\()[0-9,.]*)',
        separator:'~'
)
OUTPUT TO SybaseJaguarStream;

CREATE TYPE LoginInfo (
        userName String,
        hostName String,
        hostIP  String,
        eventTime long
);

create stream LoginInfoStream of LoginInfo;

create cq LoginInfoCQ
insert into LoginInfoStream
select
        data[0],
        data[1],
        data[2],
        TO_LONG(META(x,'OriginTimestamp'))
from SybaseJaguarStream x;


create Target SybaseJaguarDump using CSVWriter(filename:'@FEATURE-DIR@/logs/SybaseJaguar.log') input from LoginInfoStream;
end application SybaseJaguarApp;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 10 second interval;
CREATE SOURCE OracleSource USING OracleReader
(
    Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
 Tables:'@TABLES@',
    FetchSize: 1
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId int,
  curr String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT TO_INT(data[0]),
       data[1]
FROM CsvStream;

create Target BlobT using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:5,interval:5s'
)
format using AvroFormatter (
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

STOP TQLwithinTqlApp;
UNDEPLOY APPLICATION TQLwithinTqlApp;
DROP APPLICATION TQLwithinTqlApp CASCADE;

CREATE APPLICATION TQLwithinTqlApp;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',

  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;

@@FEATURE-DIR@/tql/TQLTobeCalled.tql


END APPLICATION TQLwithinTqlApp;
DEPLOY APPLICATION TQLwithinTqlApp;
START TQLwithinTqlApp;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src1 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE SOURCE @APP_NAME@_src2 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE SOURCE @APP_NAME@_src3 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE SOURCE @APP_NAME@_src4 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE SOURCE @APP_NAME@_src5 USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;


CREATE OR REPLACE TARGET @APP_NAME@_tgt1 USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;


CREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

CREATE APPLICATION @APPNAME@;

CREATE TYPE @APPNAME@type1 (
 companyName java.lang.String,
 merchantId java.lang.String,
 city java.lang.String);

CREATE STREAM @APPNAME@TypedStream OF @APPNAME@type1 PARTITION BY city;

CREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (
  wildcard: '',
  positionByEOF: false,
  directory: ''
  )
PARSE USING DSVParser (
header:'true'
)
OUTPUT TO @APPNAME@Stream;

CREATE OR REPLACE CQ @APPNAME@CQ
INSERT INTO @APPNAME@TypedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantID,
TO_STRING(data[10]) as city
FROM @APPNAME@Stream;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt1 USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@TypedStream;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt2 USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@TypedStream;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt3 USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@TypedStream;

END APPLICATION @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE OR REPLACE SOURCE @parquetsrc@ USING Global.HDFSReader (
  wildcard: '',
  directory: '',
  hadoopurl: '',
  hadoopconfigurationpath: '',
  positionbyeof: false )
  PARSE USING ParquetParser (
   )
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: 'ParquetFileSchema'
)
INPUT FROM @appname@CqOut;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using ParquetFormatter (
schemaFileName: 'ParquetS3Schema'
)
input from @appname@CqOut;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using ParquetFormatter (
schemaFileName: 'ParquetAzureSchema'
)
INPUT FROM @appname@CqOut;

CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using ParquetFormatter (
schemaFileName: 'ParquetGCSSchema'
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING DatabaseReader  (
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  Tables: @SOURCE_TABLE@,
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true'
 )
OUTPUT TO @STREAM@;


CREATE OR REPLACE SOURCE @SOURCE_NAME@2 USING DatabaseReader  (
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  Tables: @SOURCE_TABLE@,
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true'
 )
OUTPUT TO @STREAM@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using S3Writer(
    bucketname:'@BUCKET@',
   objectname:'upgradeData.csv',
   foldername:'upgradefolder',
  uploadpolicy:'EventCount : 10000,Interval :1m '
)
format using DSVFormatter (
)
input from @STREAM@;

end application @APPNAME@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

CREATE APPLICATION DSV;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE  SOURCE CSVSource USING FileReader (
  directory: '@TEST-DATA-PATH@',
  WildCard: 'posdata.csv',
  positionByEOF: false,
  charset: 'UTF-8'
 )
 PARSE USING DSVParser (
  header: 'yes'
 )
OUTPUT TO CsvStream;

CREATE OR REPLACE TARGET t USING FileWriter (
  filename: 'TargetDefault',
  directory:'@FEATURE-DIR@/logs/',
  sequence:'00',
  --flushinterval: '0',
  rolloverpolicy:'EventCount:5000000,Interval:200s'
 )
 format using DSVFormatter (

)
INPUT FROM TypedCSVStream;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetDefaultPD_actual.log') input from TypedCSVStream;

END APPLICATION DSV;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@ recovery 1 second interval;

CREATE OR REPLACE SOURCE @parquetsrc@ USING Global.HDFSReader (
  eofdelay: 100,
  wildcard: '@File@',
  rolloverstyle: 'Default',
  directory: '@DIR@',
  adapterName: 'HDFSReader',
  hadoopurl: 'hdfs://dockerhost:9000',
  hadoopconfigurationpath: '@CONF@',
  skipbom: true,
  includesubdirectories: false,
  positionbyeof: false )
  PARSE USING ParquetParser (
   )
OUTPUT TO @appname@ParquetStreams;

CREATE OR REPLACE CQ @appname@CQOrder3
INSERT INTO @appname@newStream3
SELECT
PUTUSERDATA(s,'schemaName',s.data.getSchema().getName())
FROM @appname@ParquetStreams s;

CREATE TARGET @avrotarget@ USING Global.FileWriter (
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000,Interval:30s',
  directory: '@FOLDER@',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  filename: 'AvroOutFile',
  DataEncryptionKeyPassphrase_encrypted: 'true',
  flushpolicy: 'EventCount:10,Interval:30s',
  rolloverpolicy: 'EventCount:10,Interval:30s' )
FORMAT USING Global.AvroFormatter  (
  schemaFileName: 'AvroSchema',
  formatAs: 'default',
  schemaregistryConfiguration: '' )
INPUT FROM @appname@newStream3;

CREATE TARGET @parquettarget@ USING Global.FileWriter (
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000,Interval:30s',
  directory: '@FOLDER@',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  filename: 'AvroOutFile',
  DataEncryptionKeyPassphrase_encrypted: 'true',
  flushpolicy: 'EventCount:10,Interval:30s',
  rolloverpolicy: 'EventCount:10,Interval:30s' )
FORMAT USING Global.ParquetFormatter  (
  blocksize: '128000000',
  compressiontype: 'UNCOMPRESSED',
  formatAs: 'Default',
  handler: 'com.webaction.proc.ParquetFormatter',
  formatterName: 'ParquetFormatter' )
INPUT FROM @appname@newStream3;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE @SOURCE@ USING MSSQLReader
 (
   Username: '@LOGMINER-UNAME@',
   Password: '@LOGMINER-PASSWORD@',
   ConnectionURL: '@LOGMINER-URL@',
   DatabaseName:'qatest',
   Tables: '@SOURCE_TABLE@',
    Compression:false,
    AutoDisableTableCDC:false,FetchTransactionMetadata:true,
    StartPosition:'EOF'
 )
OUTPUT TO @STREAM@;


CREATE OR REPLACE TARGET @TARGET@1 USING Global.DeltaLakeWriter (
  uploadPolicy: 'eventcount:10,interval:10s',
  Mode: 'MERGE',
  hostname: ' ',
  Tables: ' ',
  stageLocation: '/',
  CDDLAction: 'Process',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken: ' ',
  connectionUrl: ' '  )

INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@2 USING Global.DeltaLakeWriter (
  uploadPolicy: 'eventcount:10,interval:10s',
  Mode: 'MERGE',
  optimizedMerge: 'true',
  hostname: ' ',
  Tables: ' ',
  stageLocation: '/',
  CDDLAction: 'Process',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken: ' ',
  connectionUrl: ' '  )

INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@3 USING Global.DeltaLakeWriter (
  uploadPolicy: 'eventcount:10,interval:10s',
  Mode: 'APPENDONLY',
  hostname: ' ',
  Tables: ' ',
  stageLocation: '/',
  CDDLAction: 'Process',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken: ' ',
  connectionUrl: ' '  )

INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

stop application GCSWriterTest;
undeploy application GCSWriterTest;
drop application GCSWriterTest cascade;
create application GCSWriterTest recovery 1 second interval;
Create Source OracleSource Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To OracleStream;

create Target OracleGCSTarget using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@',
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from OracleStream;

end application GCSWriterTest;
deploy application GCSWriterTest on all in default;
start application GCSWriterTest;

STOP APPLICATION oracletokudu;
UNDEPLOY APPLICATION oracletokudu;
DROP APPLICATION oracletokudu CASCADE;
CREATE APPLICATION oracletokudu;
Create Type CSVType (
	companyname String,
  merchantName String
);

Create Stream TypedFileStream of CSVType;

create source CSVSource using FileReader (
	directory:'/Users/jenniffer/Product2/IntegrationTests/TestData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	
)
OUTPUT TO FileStream;

CREATE CQ CsvToPosData
INSERT INTO TypedFileStream
SELECT TO_STRING(data[0]),TO_STRING(data[1])
FROM FileStream;

CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:10000,Interval:20s')
INPUT FROM TypedFileStream;
DEPLOY APPLICATION oracletokudu;
START APPLICATION oracletokudu;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING MSSqlReader
(
  Compression: false,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.test01',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'NULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true	,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;
--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;
Create Source OS Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;

CREATE TARGET T using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

stop application ADW;
undeploy application ADW;
drop application ADW cascade;
CREATE APPLICATION ADW;

CREATE  SOURCE OracleInitialLoad USING DatabaseReader  
 (
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'@SOURCE-TABLES@',
 FetchSize:2000
) 
OUTPUT TO InitialLoadStream;

CREATE TARGET AzureDWInitialLoad USING AzureSQLDWHWriter(
ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
)
INPUT FROM InitialLoadStream;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

--
-- Recovery Test 2
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS
--

STOP Recov2Tester.RecovTest2;
UNDEPLOY APPLICATION Recov2Tester.RecovTest2;
DROP APPLICATION Recov2Tester.RecovTest2 CASCADE;
CREATE APPLICATION RecovTest2 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION RecovTest2;

STOP TQLwithinTqlApp;
UNDEPLOY APPLICATION TQLwithinTqlApp;
DROP APPLICATION TQLwithinTqlApp CASCADE;

CREATE APPLICATION TQLwithinTqlApp;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',

  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


END APPLICATION TQLwithinTqlApp;
DEPLOY APPLICATION TQLwithinTqlApp;
START TQLwithinTqlApp;

@@FEATURE-DIR@/tql/TQLTobeCalled.tql

STOP application AlterTester.DSV;
undeploy application AlterTester.DSV;
drop application AlterTester.DSV cascade;


create application DSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;


end application DSV;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'posdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'TargetPosDataXmlFS',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'FileSizeRollingPolicy,filesize:101M,sequence:00'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:merchantid:text=merchantname'
)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlFS_actual.log') input from TypedCSVStream;

end application DSV;

STOP APPLICATION SystemTimeTester.SystemTimeWindows;
UNDEPLOY APPLICATION SystemTimeTester.SystemTimeWindows;
DROP APPLICATION SystemTimeTester.SystemTimeWindows cascade;

CREATE APPLICATION SystemTimeWindows;

CREATE TYPE RandomData(
myName String,
streetAddress String,
bankName String,
bankNumber int KEY,
bankAmount double
);


CREATE SOURCE ranDataSource using StreamReader(
OutputType: 'SystemTimeTester.RandomData',
noLimit: 'false',
isSeeded: 'true',
maxRows: 0,
iterations: 30,
iterationDelay: 1000,
StringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',
NumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]R'
)OUTPUT TO CSVDataStream;

CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4])
FROM CSVDataStream;

CREATE @WINDOWTYPE@ WINDOW tierone OVER RandomDataStream keep within 20 second;

CREATE STREAM onetwostream OF RandomData;

CREATE CQ onetwocq
INSERT INTO onetwostream
SELECT *
FROM tierone;

CREATE @WINDOWTYPE@ WINDOW tiertwo OVER onetwostream keep within 40 second;

CREATE STREAM twothreestream OF RandomData;

CREATE CQ twothreecq
INSERT INTO twothreestream
SELECT *
FROM tiertwo;

CREATE @WINDOWTYPE@ WINDOW tierthree OVER twothreestream keep within 1 minute;

CREATE WACTIONSTORE MyDataActivity
CONTEXT OF RandomData
EVENT TYPES(RandomData )
@PERSIST-TYPE@

Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
Select * from @FROMSTREAM@
LINK SOURCE EVENT;


END APPLICATION SystemTimeWindows;
deploy application SystemTimeWindows;
start application SystemTimeWindows;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;

drop propertyset OrcToSpnPlatfm_App_KafkaPropset;
drop stream  OrcToSpnPlatfm_Stream CASCADE;


--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;

CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNEC4TION_URL@',
  Tables: '$table1',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING SpannerWriter (
  Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'QATEST.ORCTOSPNPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING SpannerWriter (
  Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
 )
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '$table2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@4',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING SpannerWriter  (
  Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'QATEST.ORCTOSPNPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING SpannerWriter  (
  Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
 )
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;

END APPLICATION @APPNAME@2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.GGTrailReader (
  Tables:'@TABLES@',
  CDDLCapture: false,
  TrailDirectory: '@TRAIL_FILE_DIR@',
  TrailFilePattern: '@WILDCARD@',
  Compression: false,
  SupportColumnCharset: false,
  CDDLAction: 'Process',
  FilterTransactionBoundaries: true,
  adapterName: 'GGTrailReader',
  TrailByteOrder: '@ENDIAN@' )
OUTPUT TO @STREAM@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 USING Global.GGTrailReader (
  Tables:'@TABLES@',
  CDDLCapture: false,
  TrailDirectory: '@TRAIL_FILE_DIR@',
  TrailFilePattern: '@WILDCARD@',
  Compression: false,
  SupportColumnCharset: false,
  CDDLAction: 'Process',
  FilterTransactionBoundaries: true,
  adapterName: 'GGTrailReader',
  TrailByteOrder: '@ENDIAN@' )
OUTPUT TO @STREAM@;

STOP APPLICATION DGLimitServerApp1;
UNDEPLOY APPLICATION DGLimitServerApp1;
DROP APPLICATION DGLimitServerApp1 CASCADE;
CREATE APPLICATION DGLimitServerApp1;
CREATE FLOW DGLimitServerAgentFlow;
CREATE OR REPLACE SOURCE DGLimitServerApp1_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO DGLimitServerApp1_SampleStream;
END FLOW DGLimitServerAgentFlow;
CREATE FLOW DGLimitServerServerFlow;
CREATE OR REPLACE TARGET DGLimitServerApp1_NullTarget using NullWriter()
INPUT FROM DGLimitServerApp1_SampleStream;
END FLOW DGLimitServerServerFlow;
END APPLICATION DGLimitServerApp1;
deploy application DGLimitServerApp1 on any in ServerDG1 with DGLimitServerAgentFlow on any in Agents, DGLimitServerServerFlow on any in ServerDG1;
START APPLICATION DGLimitServerApp1;

STOP APPLICATION DGLimitServerApp2;
UNDEPLOY APPLICATION DGLimitServerApp2;
DROP APPLICATION DGLimitServerApp2 CASCADE;
CREATE APPLICATION DGLimitServerApp2;
CREATE FLOW DGLimitServerAgentFlow2;
CREATE OR REPLACE SOURCE DGLimitServerApp2_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO DGLimitServerApp2_SampleStream;
END FLOW DGLimitServerAgentFlow2;
CREATE FLOW DGLimitServerServerFlow2;
CREATE OR REPLACE TARGET DGLimitServerApp2_NullTarget using NullWriter()
INPUT FROM DGLimitServerApp2_SampleStream;
END FLOW DGLimitServerServerFlow2;
END APPLICATION DGLimitServerApp2;
deploy application DGLimitServerApp2 on any in ServerDG1 with DGLimitServerAgentFlow2 on any in Agents, DGLimitServerServerFlow2 on any in ServerDG1;
START APPLICATION DGLimitServerApp2;

STOP APPLICATION ExceedApp1;
UNDEPLOY APPLICATION ExceedApp1;
DROP APPLICATION ExceedApp1 CASCADE;
CREATE APPLICATION ExceedApp1;
CREATE FLOW ExceedAgentFlow;
CREATE OR REPLACE SOURCE ExceedApp1_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO ExceedApp_SampleStream;
END FLOW ExceedAgentFlow;
CREATE FLOW ExceedServerFlow;
CREATE OR REPLACE TARGET ExceedApp1_NullTarget using NullWriter()
INPUT FROM ExceedApp_SampleStream;
END FLOW ExceedServerFlow;
END APPLICATION ExceedApp1;
deploy application ExceedApp1 on any in ServerDG1 with ExceedAgentFlow on any in Agents, ExceedServerFlow on any in ServerDG1;

CREATE TYPE PosData(
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING PostgreSQLReader  (
  ReplicationSlotName: 'Slot_Name',
  FilterTransactionBoundaries: 'true',
  Username: 'User_Name',
  ConnectionURL: 'Connection_URL',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'Password',
  Tables: 'Tables',
  PostgresConfig:'{"ReplicationPluginConfig": {"Name": "WAL2JSON", "Format": "2"}}'
 )
OUTPUT TO @STREAM@ ;

CREATE TARGET @SOURCE_NAME@_sysout USING Global.SysOut (
  name: '@SOURCE_NAME@_sysout' )
INPUT FROM @STREAM@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @APPNAME@_src Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
   _h_ReturnDateTimeAs:'ZonedDateTime'
) Output To @APPNAME@_stream;

create Target @APPNAME@_tgt using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_stream;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

STOP PartiallyJumping1Tester.PartiallyJumping1;
UNDEPLOY APPLICATION PartiallyJumping1Tester.PartiallyJumping1;
DROP APPLICATION PartiallyJumping1Tester.PartiallyJumping1 CASCADE;
CREATE APPLICATION PartiallyJumping1;

create source CsvSource1 using FileReader
(
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'WindowsTest.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
)
 parse using DSVParser
(
	header:'yes',
	columndelimiter:','
)
OUTPUT TO CsvStream1;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);
CREATE TYPE CsvData1 (
  zip double
);

CREATE TYPE WactionData1 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData2 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData3 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData5 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData6 (
  zip double
);
CREATE TYPE WactionData7 (
  zip double
);

CREATE STREAM DataStream1 OF CsvData;

CREATE STREAM DataStream2 OF CsvData;

CREATE STREAM DataStream3 OF CsvData
PARTITION BY city;

CREATE STREAM DataStream4 OF CsvData;

CREATE STREAM DataStream5 OF CsvData
PARTITION BY city;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData3
INSERT INTO DataStream3
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData4
INSERT INTO DataStream4
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData5
INSERT INTO DataStream5
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData6
INSERT INTO DataStream6
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

CREATE CQ CsvToData7
INSERT INTO DataStream7
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

-- Count based partially jumping window
CREATE WINDOW DataStreamCount
OVER DataStream1 KEEP 5 ROWS
SLIDE 2
PARTITION BY companyName;

-- Time based partially jumping window
CREATE WINDOW DataStreamTime OVER DataStream2 KEEP
within 240 second
SLIDE 15 second;

-- Attribute based partially jumping window
CREATE WINDOW DataStreamAtrribute
OVER DataStream3 KEEP
RANGE 30 SECOND
ON dateTime
SLIDE 20 second;

-- Count + time based partially jumping window
CREATE WINDOW DataStreamCountTime
OVER DataStream4 KEEP
10 rows
within 150 second
SLIDE 10;

-- Attribute + time based partially jumping window
CREATE WINDOW DataStreamAttributeTime
OVER DataStream5 KEEP
range 50 second
ON dateTime
within 25 second
SLIDE 2 SECOND
PARTITION BY city;

CREATE WACTIONSTORE Wactions1 CONTEXT OF WactionData1
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions2 CONTEXT OF WactionData2
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions3 CONTEXT OF WactionData3
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions4 CONTEXT OF WactionData4
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions5 CONTEXT OF WactionData5
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions6 CONTEXT OF WactionData6
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions7 CONTEXT OF WactionData7
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE CQ Data1ToWaction
INSERT INTO Wactions1
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamCount p
group by companyName;

CREATE CQ Data2ToWaction
INSERT INTO Wactions2
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamTime p;

CREATE CQ Data3ToWaction
INSERT INTO Wactions3
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamAtrribute p;

CREATE CQ Data4ToWaction
INSERT INTO Wactions4
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamCountTime p;

CREATE CQ Data5ToWaction
INSERT INTO Wactions5
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamAttributeTime p
group by city;

CREATE CQ Data6ToWaction
INSERT INTO Wactions6
SELECT
    count(*)
FROM DataStreamCount p
group by companyName;

CREATE CQ Data7ToWaction
INSERT INTO Wactions7
SELECT
    count(*)
FROM DataStreamTime p;

END APPLICATION PartiallyJumping1;

CREATE APPLICATION SourceRetailApp;

CREATE source RetailDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:no,
  wildcard:'retaildata2M.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO RetailOrders;

CREATE TARGET RetailSourceDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceRetailAppData') input from RetailOrders;

CREATE Source RetailHourlyStoreSales using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'storehourlyData.txt',
  header: no,
  columndelimiter: ',',
  positionByEOF:false
) OUTPUT TO RetailCacheSource1;

CREATE TARGET RetailCacheDump1 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceRetailCacheData1') input from RetailCacheSource1;

CREATE Source RetailStoreNameLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'StoreNames.csv',
  header: no,
  columndelimiter: ',',
  positionByEOF:false
) OUTPUT TO RetailCacheSource2;

CREATE TARGET RetailCacheDump2 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceRetailCacheData2') input from RetailCacheSource2;

CREATE Source RetailZipCodeLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  header: no,
  columndelimiter: '	',
  positionByEOF:false
) OUTPUT TO  RetailCacheSource3;

CREATE TARGET RetailCacheDump3 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceRetailCacheData3') input from RetailCacheSource3;


END APPLICATION SourceRetailApp;

stop application Postgres_To_PostgresApp;
undeploy application Postgres_To_PostgresApp;
drop application Postgres_To_PostgresApp cascade;

CREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE Postgres_src USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 ) 
OUTPUT TO Change_Data_Stream ;


CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET Postgres_tgt USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:100,Interval:60',
CommitPolicy: 'EventCount:100,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

end application Postgres_To_PostgresApp;
deploy application Postgres_To_PostgresApp;
start Postgres_To_PostgresApp;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ RECOVERY 30 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;

CREATE FLOW @AppName@_SourceFlow;

CREATE SOURCE @srcName@ USING ZendeskReader ( 
  MaxConnections: 20, 
  Password_encrypted: 'false', 
  ThreadPoolCount: '10', 
  ConnectionTimeOut: 60, 
  ConnectionPoolSize: '20', 
  MigrateSchema: 'true', 
  Mode: 'Automated', 
  authflow: 'false', 
  Username: '@srcusername@', 
  Password: '@srcpassword@', 
  ConnectionRetries: '3', 
  ZendeskObjects: 'sessions;settings;targets;tickets', 
  AccessToken: '', 
  ConnectionUrl: '@connectionUrl@' ) 
OUTPUT TO @outstreamname@;

END FLOW @AppName@_SourceFlow;

CREATE TARGET @tgtName@ USING Global.BigQueryWriter ( 
  Tables: '%,zendeskallresize.%', 
  projectId: '@projectId@', 
  batchPolicy: 'eventcount:10000,interval:60', 
  ServiceAccountKey: '@keyFileName@', 
  streamingUpload: 'true', 
  Mode: 'MERGE', 
  CDDLOptions: '{\"CreateTable\":{\"action\":\"IgnoreIfExists\",\"options\":[{\"CreateSchema\":{\"action\":\"IgnoreIfExists\"}}]}}' ) 
INPUT FROM @outstreamname@;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ RECOVERY 1 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @SourceName@ USING PostgreSQLReader  ( 
 ReplicationSlotName: 'striim_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src'
 ) 
OUTPUT TO @SRCINPUTSTREAM@ ;

CREATE OR REPLACE SOURCE @SourceName@1 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src'
 ) 
OUTPUT TO @SRCINPUTSTREAM@1 ;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy:'EventCount:1000,Interval:60',
CommitPolicy:'EventCount:1000,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@1 USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy:'EventCount:1000,Interval:60',
CommitPolicy:'EventCount:1000,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM @SRCINPUTSTREAM@1;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using S3Writer(
    bucketname:'@BUCKET@',
   objectname:'upgradeData.csv',
   foldername:'upgradefolder',
  uploadpolicy:'EventCount : 10000,Interval :1m '
)
format using DSVFormatter (
)
input from @STREAM@;

end application @APPNAME@;

CREATE  TARGET @TARGET_NAME@ USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@1;

 CREATE  TARGET @TARGET_NAME@2 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@2;

 CREATE  TARGET @TARGET_NAME@3 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@3;

 CREATE  TARGET @TARGET_NAME@4 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@4;

 CREATE  TARGET @TARGET_NAME@5 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@5;

 CREATE  TARGET @TARGET_NAME@6 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@6;

 CREATE  TARGET @TARGET_NAME@7 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@7;

 CREATE  TARGET @TARGET_NAME@8 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@8;

 CREATE  TARGET @TARGET_NAME@9 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@9;

 CREATE  TARGET @TARGET_NAME@10 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @STREAM@10;

--
-- Recovery Test 1
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--


UNDEPLOY APPLICATION NameM00.M00;
DROP APPLICATION NameM00.M00 CASCADE;
CREATE APPLICATION M00 RECOVERY 5 SECOND INTERVAL;




CREATE FLOW DataAcquisitionM00;


CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;


END FLOW DataAcquisitionM00;




CREATE FLOW DataProcessingM00;


CREATE TYPE WactionTypeM00 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE WACTIONSTORE WactionsM00 CONTEXT OF WactionTypeM00
EVENT TYPES ( WactionTypeM00 KEY(word) )
@PERSIST-TYPE@

CREATE CQ InsertWactionsM00
INSERT INTO WactionsM00
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStream;


END FLOW DataProcessingM00;



END APPLICATION M00;

stop APPLICATION @APPNAME@_ExceptionStore;;
undeploy APPLICATION @APPNAME@_ExceptionStore;
drop APPLICATION @APPNAME@_ExceptionStore cascade;

CREATE APPLICATION @APPNAME@_ExceptionStore;

CREATE TYPE @APPNAME@2_CDCStreams_Type  (
 exceptionType java.lang.String,
  action java.lang.String,
  appName java.lang.String,
  entityType java.lang.String,
  entityName java.lang.String,
  className java.lang.String,
  message java.lang.String,
  relatedEntity java.lang.String,
  exceptionCode java.lang.String
 );

CREATE STREAM @APPNAME@2_CDCStreams OF @APPNAME@2_CDCStreams_Type;

CREATE CQ @APPNAME@2_ReadFromExpStore
INSERT INTO @APPNAME@2_CDCStreams
select s.exceptionType,s.action,s.appName,s.entityType,s.entityName,s.className,s.message,s.relatedEntity,s.exceptionCode from @APPNAME@_App_ExceptionStore [jumping @WINDOWINTERVAL@ seconds] s;

CREATE OR REPLACE TARGET @APPNAME@2_WriteToFileAsJSON USING FileWriter  (
  filename: 'expEvent.log',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:5',
  adapterName: 'FileWriter',
  directory: 'ExpStore_logs',
  rolloverpolicy: 'eventcount:5'
 )
FORMAT USING JSONFormatter  (
  handler: 'com.webaction.proc.JSONFormatter',
  formatterName: 'JSONFormatter',
  jsonMemberDelimiter: '\n',
  jsonobjectdelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true'
 )
INPUT FROM @APPNAME@2_CDCStreams;

END APPLICATION @APPNAME@_ExceptionStore;

deploy application @APPNAME@_ExceptionStore;
start @APPNAME@_ExceptionStore;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using Ojet
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@URL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

STOP APPLICATION persistencetester.ranGenAppES;
UNDEPLOY APPLICATION persistencetester.ranGenAppES;
DROP APPLICATION persistencetester.ranGenAppES cascade;

CREATE APPLICATION ranGenAppES;

CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity int,
  size int,
  myDate datetime
);

CREATE  SOURCE liveSource USING StreamReader (
  OutputType: 'Atm',
  noLimit: 'false',
  maxRows: 20,
  iterations: 0,
  iterationDelay: 2000,
  StringSet: 'productID[1001-1002-1003-1004],stateID[BofA-WellsFargo-Chase-NYBank]',
  NumberSet: 'productWeight[3-3]R,quantity[0-200]R,size[250-1250]R'
 )
OUTPUT TO CsvStream;

CREATE STREAM ranDataStream OF Atm;

CREATE CQ ranToRanData
INSERT INTO ranDataStream
SELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4]), TO_DATE(data[5])
FROM CsvStream;

CREATE WACTIONSTORE RanGenDataStore
CONTEXT OF Atm
EVENT TYPES ( Atm )
@PERSIST-TYPE@

CREATE CQ TrackRanGenDataActivity
INSERT INTO RanGenDataStore
Select * from ranDataStream
LINK SOURCE EVENT;


END APPLICATION ranGenAppES;

stop @appname@;
undeploy application @appname@;
DROP APPLICATION @appname@ CASCADE;
CREATE APPLICATION @appname@;

CREATE SOURCE @appname@_src USING databaseReader  (
  Username: '@@',
  Password: '@@',
  ConnectionURL: '@@',
  Tables: '@@',
  FetchSize: '100'
 )
OUTPUT TO @appname@_ss;

----1st set of window and cache

CREATE JUMPING WINDOW @appname@_win OVER @appname@_ss KEEP @winsize@ ROWS;

CREATE TYPE @appname@_MapType
    (   
       id INTEGER,
        name STRING,
        city  STRING
    );
    
CREATE EXTERNAL CACHE @appname@_cach (
  AdapterName: 'DatabaseReader',
    ConnectionURL: '@url@',
    UserName: '@uname@',
    Password: '@pwd@',
   Table: '@tablename@2',
  FetchSize: 100,
  Columns: 'id,name,city',
  trimquote: false,
  KeyToMap: '@key@'
 )
 OF @appname@_MapType;   
 
CREATE TYPE @appname@_MapTypenew
    (   id_t            INTEGER,
        name_t           STRING,
        city_t            STRING,
        id_c            INTEGER,
        name_c            STRING,
        city_c            STRING
    );
    
CREATE STREAM @appname@_JoinedData OF @appname@_MapTypenew;

CREATE CQ @appname@_JoinDataCQ
INSERT INTO @appname@_JoinedData
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_STRING(f.data[2]),
        z.id,
        z.name,
        z.city
FROM @appname@_win f, @appname@_cach z
where TO_INT(f.data[0]) = z.id
@Ex@;

----2nd set of window and cache

CREATE JUMPING WINDOW @appname@_win2 OVER @appname@_ss KEEP @winsize@ ROWS;
 
 CREATE EXTERNAL CACHE @appname@_cach2 (
  AdapterName: 'DatabaseReader',
    ConnectionURL: '@url@',
    UserName: '@uname@',
    Password: '@pwd@',
   Table: '@tablename@3',
  FetchSize: 100,
  Columns: 'id,name,city',
  trimquote: false,
  KeyToMap: '@key@'
 )
 OF @appname@_MapType;
 
CREATE CQ @appname@_JoinDataCQ2
INSERT INTO @appname@_JoinedData
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_STRING(f.data[2]),
        z.id,
        z.name,
        z.city
FROM @appname@_win2 f, @appname@_cach2 z
where TO_INT(f.data[0]) = z.id
@Ex@;


----3rd set of window and cache

CREATE JUMPING WINDOW @appname@_win3 OVER @appname@_ss KEEP @winsize@ ROWS;
 
 CREATE EXTERNAL CACHE @appname@_cach3 (
  AdapterName: 'DatabaseReader',
    ConnectionURL: '@url@',
    UserName: '@uname@',
    Password: '@pwd@',
   Table: '@tablename@4',
  FetchSize: 100,
  Columns: 'id,name,city',
  trimquote: false,
  KeyToMap: '@key@'
 )
 OF @appname@_MapType;
 
CREATE CQ @appname@_JoinDataCQ3
INSERT INTO @appname@_JoinedData
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_STRING(f.data[2]),
        z.id,
        z.name,
        z.city
FROM @appname@_win3 f, @appname@_cach3 z
where TO_INT(f.data[0]) = z.id
@Ex@;


CREATE TARGET @appname@_tgt USING DatabaseWriter
(
  ConnectionURL:'@@',
  Username:'@@',
  Password:'@@',
  BatchPolicy:'Eventcount:10000,Interval:1',
  CommitPolicy:'Interval:1,Eventcount:10000',
  Tables:'@@'
) 
INPUT FROM @appname@_JoinedData;

END APPLICATION @appname@;
deploy application @appname@;
start @appname@;

create Target @TARGET_NAME@ using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:50'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@',
	members:'Table=@metadata(TableName),OpName=@metadata(OperationName)'
)
input from @STREAM@;

CREATE FLOW @STREAM@_SourceFlow;

CREATE SOURCE @SOURCE_NAME@ Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
   _h_ReturnDateTimeAs:'ZonedDateTime'
) OUTPUT TO @STREAM@;

END FLOW @STREAM@_SourceFlow;

Stop Oracle_IRLogWriter;
Undeploy application Oracle_IRLogWriter;
drop application Oracle_IRLogWriter cascade;

CREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;

CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( 
 
  FetchSize: 5000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=t1',
 startPosition: '%=-1',
  PollingInterval: '20sec'
  )
  OUTPUT TO data_stream;
create target AzureSQLDWHTarget using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10000,interval:300s'
) INPUT FROM data_stream;
  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

END APPLICATION Oracle_IRLogWriter;
deploy application Oracle_IRLogWriter;
start Oracle_IRLogWriter;

Stop Oracle_IRLogWriter2;
Undeploy application Oracle_IRLogWriter2;
drop application Oracle_IRLogWriter2 cascade;

CREATE APPLICATION Oracle_IRLogWriter2 recovery 5 second interval;

CREATE OR REPLACE SOURCE Oracle_IRSource2 USING IncrementalBatchReader  ( 
 
  FetchSize: 5000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=t1',
 startPosition: '%=-1',
  PollingInterval: '20sec'
  )
  OUTPUT TO data_stream2;
create target AzureSQLDWHTarget2 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10000,interval:300s'
) INPUT FROM data_stream2;
  CREATE OR REPLACE TARGET TeraSys2 USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream2;

END APPLICATION Oracle_IRLogWriter2;
deploy application Oracle_IRLogWriter2;
start Oracle_IRLogWriter2;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING ParquetFormatter (
schemaFileName: '@SCHEMAFILE@',
members:'data'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING DSVFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

END APPLICATION @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@ recovery 1 second interval;

CREATE SOURCE @parquetsrc@ USING FileReader (
  directory: '',
  positionByEOF: false,
  WildCard: '' )
PARSE USING ParquetParser (
 )
OUTPUT TO @appname@Streams;

CREATE OR REPLACE CQ @appname@CQOrder3
INSERT INTO @appname@Stream3
SELECT
PUTUSERDATA(s,'schemaName',s.data.getSchema().getName())
FROM @appname@Streams s;

CREATE TARGET @adlstarget@ USING Global.ADLSGen2Writer (
    accountname:'',
  	sastoken:'',
  	filesystemname:'',
  	filename:'',
  	directory:'',
  	uploadpolicy:'eventcount:10' )

format using AvroFormatter (
)
INPUT FROM @appname@Stream3;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE OR REPLACE APPLICATION @APPNAME@ recovery 5 second interval;

CREATE FLOW @APPNAME@_Agent_flow;

CREATE OR REPLACE SOURCE @SourceName@1 USING Global.MSJet (
  Tables: 'dbo.compsrc',
    username: 'qatest',
    DatabaseName: 'qatest',
    FetchTransactionMetadata: true,
    filterTransactionBoundaries: true,
    compression: true,
    ConnectionURL: '10.211.55.3:1433',
    CommittedTransactions: true,
    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
    SendBeforeImage: true,
    password: 'w3b@ct10n' )
OUTPUT TO @SRCINPUTSTREAM@;

END FLOW @APPNAME@_Agent_flow;

CREATE FLOW @APPNAME@_Agent_flow1;

CREATE OR REPLACE SOURCE @SourceName@2 USING Global.MSJet (
  Tables: 'dbo.compsrc',
    username: 'qatest',
    DatabaseName: 'qatest',
    FetchTransactionMetadata: true,
    filterTransactionBoundaries: true,
    compression: false,
    ConnectionURL: '10.211.55.3:1433',
    CommittedTransactions: true,
    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
    SendBeforeImage: true,
    password: 'w3b@ct10n' )
OUTPUT TO @SRCINPUTSTREAM@;

END FLOW @APPNAME@_Agent_flow1;

CREATE FLOW @APPNAME@_server_flow;

CREATE OR REPLACE TARGET @targetName@ USING Global.DatabaseWriter
(
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  BatchPolicy:'EventCount:10,Interval:60',
  CommitPolicy:'EventCount:10,Interval:60',
  ParallelThreads:'',
  CheckPointTable:'CHKPOINT',
  Password_encrypted:'false',
  Tables:'qatest.MSJEtsrc1,qatest.MSJEtar1;qatest.MSJEtsrc2,qatest.MSJEtar2;',
  CDDLAction:'Process',
  Password:'w3b@ct10n',
  StatementCacheSize:'50',
  ConnectionURL:'jdbc:sqlserver://10.211.55.3:1433;databaseName=qatest',
  DatabaseProviderType:'Default',
  Username:'qatest',
  PreserveSourceTransactionBoundary:'false',
  adapterName:'DatabaseWriter'
)
INPUT FROM @SRCINPUTSTREAM@;

CREATE TARGET @targetsys@ USING Global.SysOut (
  name: '@targetsys@' )
INPUT FROM @SRCINPUTSTREAM@;

END FLOW @APPNAME@_server_flow;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@ with @APPNAME@_Agent_flow in AGENTS, @APPNAME@_Agent_flow1 in AGENTS ,@APPNAME@_server_flow on any in default;
START APPLICATION @APPNAME@;

CREATE OR REPLACE TARGET @appName@_AzureEventHubWriter USING AzureEventHubWriter (
  SASKey: '',
  EventHubName: '',
  EventHubNamespace: '',
  SASPolicyName: '',
  BatchPolicy: 'Size:1000000,Interval:10s' )
FORMAT USING JSONFormatter  (
 )
INPUT FROM @appName@_MCQOut1;

STOP APPLICATION eh;
UNDEPLOY APPLICATION eh;
DROP APPLICATION eh CASCADE;
CREATE APPLICATION eh @Recovery@;
create flow AgentFlow;
CREATE OR REPLACE SOURCE s USING IncrementalBatchReader  ( 
  FetchSize: 1,
  StartPosition:'QATEST.IBR01=-1;QATEST.IBR02=-1;QATEST.IBR03=0;QATEST.IBR04=0;QATEST.IBR05=1;QATEST.IBR06=2018-09-20 06:43:59;QATEST.%=-1',
  Username: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@//dockerhost:1521/orcl',
  Tables: 'QATEST.IBR%',
  CheckColumn: 'QATEST.IBR01=id;QATEST.IBR03=id;QATEST.IBR05=id;QATEST.%=t1',
  Password: 'qatest' ) 
OUTPUT TO sourcestream ;


CREATE TYPE cdctype(
  id int,
  name String  
);

CREATE STREAM cdctypestream OF cdctype;

CREATE CQ cdcstreamcq
INSERT INTO cdctypestream
SELECT TO_INT(p.data[0]), 
       TO_STRING(p.data[1])
FROM sourcestream p;

end flow AgentFlow;

create flow serverFlow;

create Target t1_dsv using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_01',
	E1P:'true',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
	PartitionKey:'id',
	OperationTimeoutMS:'120000',
    BatchPolicy:'Size:256000,Interval:30s',
    ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
	--ParallelThreads:'2'
)
format using DSVFormatter ( 
)
input from cdctypestream;

create Target t2_json using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_02',
	E1P:'true',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
	PartitionKey:'id',
	OperationTimeoutMS:'140000',
	BatchPolicy:'Size:200000,Interval:1m',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m',
	ConsumerGroup:'reader')
format using JSONFormatter ( 
)
input from cdctypestream;

create Target t3_avro using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_03',
	E1P:'true',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
	PartitionKey:'id',
	OperationTimeoutMS:'140000',
	BatchPolicy:'Size:256000,Interval:1h',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m',
	ConsumerGroup:'reader')
format using AvroFormatter (
schemaFileName:'kafkaAvroTest_Agent_ibr_orcl.avsc'
)input from cdctypestream;
end flow serverFlow;

END APPLICATION eh;
--deploy application eh;
deploy application eh with AgentFlow in Agents, ServerFlow in default;

start application eh;

STOP APPLICATION ER;
UNDEPLOY APPLICATION ER;
DROP APPLICATION ER CASCADE;
CREATE APPLICATION ER;
CREATE STREAM ER_SS2 OF Global.JsonNodeEvent;

CREATE SOURCE ER_S1 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING dsvParser (
)OUTPUT TO ER_SS1;

CREATE SOURCE ER_S2 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING jsonparser (members:'data')
OUTPUT TO ER_SS2;


CREATE SOURCE ER_S3 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING avroParser (
schemaFileName:'kafkaAvroTest_Agent_ibr_orcl.avsc'
)OUTPUT TO ER_SS3;


create Type CustType 
(writerdata com.fasterxml.jackson.databind.JsonNode,
TopicName java.lang.String,
PartitionID java.lang.String);

Create Stream datastream3 of CustType;

CREATE CQ CustCQ3
INSERT INTO datastream3
SELECT AvroToJson(s3.data),
metadata.get("TopicName").toString() AS TopicName,
metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS3 s3;



create Target ER_t1 using FileWriter (
filename:'FT1_5L_AVRO_RT',
directory:'FEATURE-DIR/logs/',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from ER_SS1;

create Target ER_t2 using FileWriter (
filename:'FT2_JSON_RT',
directory:'FEATURE-DIR/logs/',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000'	
)
format using jsonFormatter(members: 'data' )
input from ER_SS2;

create Target ER_t3 using FileWriter (
filename:'FT2_JSON_RT',
directory:'FEATURE-DIR/logs/',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from datastream3;


end application ER;
deploy application ER;

--
-- Recovery Test 28 with two sources, two jumping time-count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5a9W  -> CQ1 -> WS
--   S2 -> Jc6a11W -> CQ2 -> WS
--

STOP Recov28Tester.RecovTest28;
UNDEPLOY APPLICATION Recov28Tester.RecovTest28;
DROP APPLICATION Recov28Tester.RecovTest28 CASCADE;
CREATE APPLICATION RecovTest28 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION RecovTest28;

UNDEPLOY APPLICATION admin.LongRunningAppOracle;
DROP APPLICATION admin.LongRunningAppOracle cascade;

CREATE APPLICATION LongRunningAppOracle;


 --COMMENT::   Modify type attributes as desired.
 --COMMENT::   Type must be created first before creating a source using ranReader

CREATE TYPE RandomData(
  myName String,
  streetAddress String,
  bankName String,
  bankNumber int KEY,
  bankAmount double
);

CREATE source ranDataSource USING ranReader(
  OutputType:'LongRunningAppOracle.RandomData',
  TimeInterval:5,
  NoLimit:true,
  SampleSize:10000,
  DataKey:bankName,
  NumberOfUniqueKeys:500
) OUTPUT TO CSVDataStream;


CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4])
FROM CSVDataStream;


CREATE JUMPING WINDOW RandomData10Rows
OVER RandomDataStream KEEP 10 ROWS
PARTITION BY bankNumber;


CREATE TYPE myData(
  myName String,
  myAddress String,
  myBankName String,
  myBankNumber int KEY,
  myBankAmount double
);

CREATE STREAM myDataStream OF myData;

CREATE CQ GetMyData
INSERT INTO MyDataStream
SELECT myName, streetAddress, bankName, bankNumber, bankAmount
FROM RandomData10Rows WHERE bankNumber > 20000 AND bankNumber < 20300;


CREATE WACTIONSTORE MyDataActivity CONTEXT OF MyData
EVENT TYPES(myData )
PERSIST EVERY 10 second USING (
JDBC_DRIVER:'@WASTORE-DRIVER@',
JDBC_URL:'@WASTORE-URL@',
JDBC_USER:'@WASTORE-UNAME@',
JDBC_PASSWORD:'@WASTORE-PASSWORD@',
pu_name:oracle,
DDL_GENERATION:'drop-and-CREATE-tables'
);

Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
Select * from myDataStream
LINK SOURCE EVENT;


END APPLICATION LongRunningAppOracle;

stop bankApp;
undeploy application bankApp;

alter application bankApp;
CREATE OR REPLACE TYPE wsData  ( bankID java.lang.Integer KEY, 
nameOfBank java.lang.String
);

CREATE OR REPLACE WACTIONSTORE oneWS  CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@;



alter application bankApp recompile;
deploy application bankApp;
start application bankApp;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'JsonTargetEC',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'eventcount:2000,sequence:00'
)
format using JSONFormatter (
  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)

input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetJsonECBig_actual.log') input from TypedCSVStream;

end application DSV;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;
Create Source oracSource
 Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;
CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

stop application @appname@Out;
undeploy application @appname@Out;
drop application @appname@Out cascade;

drop stream @appname@PSStream;
CREATE OR REPLACE PROPERTYSET @appname@KafkaPropset (zk.address:'localhost:@keeperport@', bootstrap.brokers:'localhost:@brokerport@', partitions:'50');
CREATE STREAM @appname@PSStream@rand@ OF Global.JSONNodeEvent PERSIST USING @appname@KafkaPropset;

CREATE APPLICATION @appname@ recovery 5 second interval;;

CREATE OR REPLACE SOURCE @cobolsrc@ USING FileReader (
  wildcard: '',
  positionbyeof: false,
  directory: ''
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: 'ProcessRecordAsEvent',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'SingleEvent',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'Level01',
  copybookFileName: ''
   )
OUTPUT TO @appname@PSStream@rand@;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE APPLICATION @appname@Out recovery 5 second interval;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
  filename: '',
  directory: '',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter  (
  members: 'data',
  EventsAsArrayOfJsonObjects: 'true'
 )
INPUT FROM @appname@PSStream@rand@;

END APPLICATION @appname@Out;
deploy application @appname@Out on all in default;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadpolicy:'EventCount:7'
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from @STREAM@;
end application @APPNAME@;

stop APPLICATION @AppName@;
Undeploy APPLICATION @AppName@;
drop APPLICATION @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @AgentFlow@;

CREATE OR REPLACE SOURCE @SourceName@ USING Global.MSJet (
  Tables: 'dbo.compsrc',
  username: 'qatest',
  DatabaseName: 'qatest',
  FetchTransactionMetadata: true,
  filterTransactionBoundaries: true,
  compression: false,
   Mode: '@mode@',
  ConnectionURL: '@ConnectionURL@',
  CommittedTransactions: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
  SendBeforeImage: true,
  password: 'w3b@ct10n' )
OUTPUT TO @StreamName@;
END FLOW @AgentFlow@;

CREATE TARGET @SysTarget@ USING Global.SysOut (
  name: 'MS_CDC_SYSOUT' )
INPUT FROM @StreamName@;

CREATE FLOW @ServerFlow@;

CREATE TARGET @TargetName@ USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ParallelThreads: '',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: '@ConnectionURL@',
  StatementCacheSize: '50',
  DatabaseProviderType: 'Default',
  Username: 'qatest',
  Tables: 'dbo.compsrc,dbo.comptar',
  Password: 'w3b@ct10n',
  PreserveSourceTransactionBoundary: 'false',
  BatchPolicy: 'EventCount:1,Interval:60' )
INPUT FROM @StreamName@;

create Target @TargetName@_File using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (

)
INPUT FROM @StreamName@;

END FLOW @ServerFlow@;

END APPLICATION @AppName@;
DEPLOY APPLICATION @AppName@ with @AgentFlow@ in AGENTS ,@ServerFlow@ on any in default;
START APPLICATION @AppName@;

stop tpcc;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;


Create Source @SourceName@
 Using OracleReader
(
 Username:'@READER-UNAME@',
 Password:'@READER-PASSWORD@',
 ConnectionURL:'@CDC-READER-URL@',
 Tables: '@WATABLES-SRC@',
 FetchSize:1,
 QueueSize:25000,
 CommittedTransactions:true,
 Compression:true,
 CaptureDDL: true,
 SendBeforeImage:true
) Output To @SRCINPUTSTREAM@;


create Target @targetsys@ using SysOut(name:OrgData) input from DataStream;

CREATE TARGET @targetName@ USING databasewriter(
  Username: '@WRITER-UNAME@',
  Password: '@WRITER-PASSWORD@',
  ConnectionURL:'@WRITER-URL@',
  BatchPolicy:'EventCount:1,Interval:1',
  CommitPolicy:'EventCount:1,Interval:1',
  Tables: '@WATABLES-SRC@,@WATABLES-TGT@'
) INPUT FROM @SRCINPUTSTREAM@;


END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

Stop application @externalCacheName@;
undeploy application @externalCacheName@;
drop application @externalCacheName@ cascade;
drop EXCEPTIONSTORE @externalCacheName@_Exceptionstore;

drop application @ApplicationName@ cascade;

CREATE APPLICATION @ApplicationName@ RECOVERY 5 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;

CREATE OR REPLACE SOURCE @SourceName@ USING Global.DatabaseReader
(
  Username: '@Username@',
  Password: '@Password@',
  ConnectionURL: '@ConnectionURL@',
  Tables: '@SourceTableName@',
  FetchSize: 1
)
Output To @SourceName@_st;

CREATE TYPE @TYPEName@_type (
 id Integer,
 email STRING,
 address STRING
 );

CREATE EXTERNAL CACHE @externalCacheName@ (
  keytomap: 'id',
  DatabaseProviderType: 'Default',
  Table: '@LookUpTableName@',
  AdapterName: 'DatabaseReader',
  ConnectionURL: '@ConnectionURL@',
  FetchSize: 1,
  Password_encrypted: 'false',
  Columns: 'id,email,address',
  Password: '@Password@',
  connectionRetryPolicy: 'timeOut=5, retryInterval=5, maxRetries=5',
  Username: '@Username@' )
OF @TYPEName@_type;


CREATE CQ @CQName@
INSERT INTO @TargetName@_st
SELECT t1.data[0] as ID,t1.data[1] as Name,t1.data[2] as Deptarment,t2.email as Email,t2.address as address
FROM @SourceName@_st t1 left outer join @externalCacheName@ t2
on TO_INT(t1.data[0]) = t2.id;


CREATE OR REPLACE TARGET @TargetName@ USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  BatchPolicy: 'EventCount:1,Interval:10',
  CheckPointTable: 'CHKPOINT',
  ConnectionURL: '@ConnectionURL@',
  Password_encrypted: 'false',
  CDDLAction: 'Process',
  Password: '@Password@',
  CommitPolicy: 'EventCount:1,Interval:30',
  StatementCacheSize: '50',
  DatabaseProviderType: 'Default',
  PreserveSourceTransactionBoundary: 'false',
  Tables: '@TargetTableName@',
  Username: '@Username@',
  adapterName: 'DatabaseWriter' )
INPUT FROM @TargetName@_st;

END APPLICATION @ApplicationName@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second Interval ;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectURL@',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop application @AppName@;
Undeploy application @AppName@;
Alter application @AppName@;

CREATE FLOW @AppName@_Agent_flow;

CREATE OR REPLACE SOURCE @AppName@_FileReaderSource USING FileReader (
wildcard: 'posdata.csv', 
  positionByEOF: false, 
  blocksize: 10100,
  rolloverpolicy: 'EventCount:100,Interval:30s', 
  directory: '@dir@' ) 
PARSE USING DSVParser ( 
  trimquote: false, 
  header: 'Yes' ) 
OUTPUT TO @AppName@_CsvStream;

END FLOW @AppName@_Agent_flow;

alter application @AppName@ recompile;
DEPLOY APPLICATION @AppName@ with @AppName@_Agent_flow on any in AGENTS;
start application @AppName@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.JsonNodeEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (
  ProviderName: '',
  Provider: '',
  Ctx: '',
  QueueName: '',
  Topic:'',
  UserName: '',
  Password: '',
  EnableTransaction: '',
  transactionpolicy: ''
 )
PARSE USING JSONParser ()
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING JSONFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream@RANDOM@;

END APPLICATION @APPNAME@;

stop MySQLToAzure;
undeploy application MySQLToAzure;
DROP APPLICATION MySQLToAzure CASCADE;
CREATE APPLICATION MySQLToAzure recovery 5 second interval;;

Create Source MySQLSOURCE Using MySQLReader
(
  Username: '@MYSQL-USERNAME@',
  Password: '@MYSQL-PASSWORD@',
  ConnectionURL: '@MYSQL-URL@',
  Database:'@MYSQL-DATABASE@',
  Tables: '@SOURCE_TABLES@'
) 
Output To str;


CREATE  TARGET t4 USING SysOut  ( 
  name: 'sqltors'
 ) 
INPUT FROM str;

create target MySQLAzureTarget using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

END APPLICATION MySQLToAzure;
deploy application MySQLToAzure;
start application MySQLToAzure;

use PosTester;
DROP WINDOW PosData5Minutes;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;
CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'TargetPosDataXmlEC',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'eventcount:2000,sequence:00'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:zip:text=merchantname'
)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlEC_actual.log') input from TypedCSVStream;

end application DSV;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@;

create TYPE CountTYPE(numcol INT);

CREATE JUMPING WINDOW nEvents OVER @STREAM@ KEEP 10 ROWS;

CREATE STREAM TypedCountStream of CountTYPE;

CREATE CQ CountCQ INSERT INTO TypedCountStream SELECT TO_INT(data[0]) FROM nEvents;

create Target @TARGET_NAME@ using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (

)
input from @STREAM@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

CREATE OR REPLACE SOURCE @SOURCE@ USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '85d7qFnwTW8=',
  Password_encrypted: 'true',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

CREATE OR REPLACE TARGET @TARGET@ USING @TARGET_ADAPTER@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

end flow @APPNAME@_serverflow;

END APPLICATION @APPNAME@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ (
  positionbyeof: false
)
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING ParquetFormatter (
  schemaFileName: 'parquetSchema'
)
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

STOP APPLICATION eh;
UNDEPLOY APPLICATION eh;
DROP APPLICATION eh CASCADE;
CREATE APPLICATION eh @Recovery@;

Create Source s1_orcl_w Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: 'localhost:1521:orcl',
 Tables:'QATEST.TEST_%',
 FetchSize:1
) 
Output To sourcestream;

Create Source s2_orcl Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: 'localhost:1521:orcl',
 Tables:'QATEST.TEST_01',
 FetchSize:1
) 
Output To sourcestream;

Create Source s3_orcl Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: 'localhost:1521:orcl',
 Tables:'QATEST1.TEST_01',
 FetchSize:1
) 
Output To sourcestream;


CREATE CQ OperationType
INSERT INTO OpsStream
SELECT  
CASE WHEN (META(c,"OperationName").toString() == "DELETE")
THEN putUserData(c, 'isDelete', 'true') 
ELSE
putUserData(c,'isDelete', 'false')
END
FROM sourcestream c;



CREATE OR REPLACE SOURCE s4_orcl_ibr USING IncrementalBatchReader  ( 
  FetchSize: 1,
  StartPosition: '%=0',
  Username: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@//dockerhost:1521/orcl',
  Tables: 'QATEST.TEST_%',
  CheckColumn: '%=id',
  Password: 'qatest' ) 
OUTPUT TO sourcestream ;

CREATE SOURCE s5_fr USING FileReader (
	directory:'/Users/saranyad/Product/IntegrationTests/TestData/',
    WildCard:'banks.csv',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:',',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;


CREATE TYPE cdctype(
  id int,
  name String  
);

CREATE STREAM cdctypestream OF cdctype;

CREATE CQ cdcstreamcq
INSERT INTO cdctypestream
SELECT TO_INT(p.data[0]), 
       TO_STRING(p.data[1])
FROM FileStream p;


create cq cqtowaevent 
insert into sourcestream
select convertTypedeventToWAevent(c, 'admin.cdctype')
from cdctypestream c;


create Target t1_dsv using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_01',
	E1P:'true',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
	PartitionKey:'@metadata(TableName)',
	ConsumerGroup:'reader',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
	--ParallelThreads:'2'
)
format using DSVFormatter ( 
)
input from sourcestream;





create Target t2_json using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_02',
	E1P:'true',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
	PartitionKey:'@userdata(isDelete)',
	ConsumerGroup:'reader',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:30s'
	--ParallelThreads:'2'
)
format using JSONFormatter ( 
)
input from OpsStream;


create Target t3_avro using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_03',
	E1P:'true',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
	PartitionKey:'@metadata(TableName)',
	ConsumerGroup:'reader',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
	--ParallelThreads:'2'
)
format using AvroFormatter (schemaFileName:'kafkaAvroTest_multipleReader.avsc') 
input from sourcestream;


END APPLICATION eh;

DEPLOY APPLICATION eh on any in default;

start application eh;

drop application Component_Disappear cascade;
drop cq Test_Subquery_Cq;
drop stream Test_Subquery_Cq_out;

CREATE APPLICATION Component_Disappear;

create or replace CQ Test_Subquery_Cq
insert into Test_Subquery_Cq_out
SELECT f.topic as topic, sum(f.rawdatacount) as TotalLast24hour, B.rawdatacount as TotalLast1hour FROM JUMP_WND_1EVT_1MIN h
   join SLIDE_WND_HOURLYTOTALS_KAFKADATA_FILE f on 1=1
   join (SELECT rawdatacount, topic,timerange from  ET_HOURLYTOTALS_KAFKADATA_FILE,JUMP_WND_1EVT_30SEC where timerange = DHOURS(DNOW())-1) B on B.topic=f.topic
   Group by f.topic;

END APPLICATION Component_Disappear;

STOP application admin.SampleApp;
undeploy application admin.SampleApp;
drop application admin.SampleApp cascade;


CREATE APPLICATION SampleApp RECOVERY 10 SECOND INTERVAL;

CREATE SOURCE Oracle_Src USING Global.OracleReader (
  Tables: 'QATEST.TEST01',
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  OutboundServerProcessName: 'WebActionXStream',
  Password: 'qatest',
  Compression: false,
  ReaderType: 'LogMiner',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  FetchSize: 1,
  Password_encrypted: 'false',
  SupportPDB: false,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CDDLAction: 'Process',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  DictionaryMode: 'OnlineCatalog',
  QueueSize: 2048,
  CommittedTransactions: true,
  XstreamTimeOut: 600,
  CDDLCapture: false,
  TransactionBufferType: 'Disk',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '100MB',
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  DatabaseRole: 'Primary' )
OUTPUT TO Striim_Buffer;

CREATE TARGET Oracle_tgt USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'qatest',
  ParallelThreads: '',
  DatabaseProviderType: 'Oracle',
  CheckPointTable: 'CHKPOINT',
  Password_encrypted: 'false',
  CDDLAction: 'Process',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.TEST01,QATEST.TEST02',
  CommitPolicy: 'EventCount:1,Interval:10',
  StatementCacheSize: '50',
  Username: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:10',
  PreserveSourceTransactionBoundary: 'false' )
INPUT FROM Striim_Buffer;

END APPLICATION SampleApp;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING MSSqlReader
(
  Compression: false,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.test01',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
)
OUTPUT TO @APPNAME@_SS;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'NULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true	,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING JSONParser ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

-------------------------
-- The tql checks the auto conversion feature for multiple tables.
-- It covers all the data types supported by sqlmp (except "interval").
-------------------------

IMPORT static com.webaction.runtime.converters.DateConverter.*;

UNDEPLOY APPLICATION admin.SQLMPReaderApp;
DROP APPLICATION admin.SQLMPReaderApp cascade;

CREATE APPLICATION SQLMPReaderApp;
create source SQLMPSource using HPNonStopSQLMPReader (
    portno:2020,
	ipaddress:'10.10.196.103',
	Name:'intg',
	AuditTrails:'parallel',
	AgentPortNo:8012,
	AgentIpAddress:'10.10.197.150', 
    Tables:'$DATA06.MAHA.ESA;$DATA06.MAHA.ESB;$DATA06.MAHA.ESC'
) OUTPUT TO CDCStream,
ESAStream MAP (table:'\\RPC4.$DATA06.MAHA.ESA'),
ESBStream MAP (table:'\\RPC4.$DATA06.MAHA.ESB');


CREATE TYPE ESCStreamType(
C0 Short,
C1 Long,
C2 Long,
C3 String,
C4 String,
C5 Integer,
C6 Long,
C7 String,
C8 String,
C9 Double,
C10 Double,
OPR String,
TABLENAME String
);

CREATE STREAM ESCStream OF ESCStreamType;


CREATE JUMPING WINDOW SQLMPDataWindow
OVER ESCStream KEEP 5 ROWS
PARTITION BY OPR;

CREATE CQ ESCStreamCq
INSERT INTO ESCStream
SELECT TO_SHORT(data[0]),
    TO_LONG(data[1]),
    TO_LONG(data[2]),
    data[3],
    data[4],
    TO_INT(data[5]),
       TO_LONG(data[6]),
    data[7],
    data[8],
    TO_DOUBLE(data[9]),
    TO_DOUBLE(data[10]),
    META(x,"OperationName").toString(),
    META(x, "TableName").toString()
FROM CDCStream x
WHERE not(META(x,"OperationName").toString() = "BEGIN") AND not(META(x,"OperationName").toString() = "COMMIT") AND not(META(x, "TableName").toString() is null) AND META(x, "TableName").toString() = "\\\\RPC4.$DATA06.MAHA.ESC";


CREATE TYPE SQLMPOperationData(
    TableName String,
    OperationName String,
    Count Integer
);

CREATE STREAM SQLMPOperationDataStream OF SQLMPOperationData;

CREATE CQ SQLMPOperationCheck
INSERT INTO SQLMPOperationDataStream
SELECT x.TABLENAME,
CASE WHEN x.OPR = 'INSERT' THEN x.OPR
     WHEN x.OPR = 'DELETE' THEN x.OPR
     WHEN x.OPR = 'UPDATE' THEN x.OPR
     ELSE 'UNSUPPORTED OPREATION' END,
CASE WHEN x.OPR = 'INSERT' THEN COUNT(x.OPR)
     WHEN x.OPR = 'DELETE' THEN COUNT(x.OPR)
     WHEN x.OPR = 'UPDATE' THEN COUNT(x.OPR)
     ELSE 0 END
FROM SQLMPDataWindow x
GROUP BY OPR;

CREATE TARGET Log USING LogWriter(
  name:SQLMPReaderAppESA,
-- filename:'@FEATURE-DIR@/logs/SQLMPReaderAppESA.log'
  filename:'mp.log'
) INPUT FROM ESAStream;


CREATE TARGET Log1 USING LogWriter(
  name:SQLMPReaderAppESB,
--  filename:'@FEATURE-DIR@/logs/SQLMPReaderAppESB.log'
  filename:'mp1.log'
) INPUT FROM ESBStream;


CREATE TARGET OperationLog USING LogWriter(
  name:SQLMPReaderAppESC,
--  filename:'@FEATURE-DIR@/logs/SQLMPReaderOperationCheck.log'
  filename:'mp2.log'
) INPUT FROM SQLMPOperationDataStream;

END APPLICATION SQLMPReaderApp;

stop application APP_KAFKA_DATASOURCES;
undeploy application APP_KAFKA_DATASOURCES;
alter application APP_KAFKA_DATASOURCES;

CREATE OR REPLACE SOURCE SRC_FR_KAFKA_HOURLYTOTALS USING Global.FileReader (
  adapterName: 'FileReader',
  rolloverstyle: 'Default',
  blocksize: 64,
  skipbom: true,
  wildcard: 'kafka_hourly_total_20210316.txt',
  directory: '@confDir@',
  includesubdirectories: false,
  positionbyeof: false ) 
PARSE USING Global.DSVParser (
  trimwhitespace: false,
  linenumber: '-1',
  columndelimittill: '-1',
  trimquote: true,
  ignoreemptycolumn: false,
  parserName: 'DSVParser',
  quoteset: '\"',
  handler: 'com.webaction.proc.DSVParser_1_0',
  charset: 'UTF-8',
  columndelimiter: ':',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  separator: ',',
  header: false,
  blockascompleterecord: false,
  rowdelimiter: '\n',
  nocolumndelimiter: false,
  headerlineno: 0 )
OUTPUT TO STREAM_SRC_FR_KAFKA_HOURLYTOTALS;

alter application APP_KAFKA_DATASOURCES recompile;

STOP APPLICATION testApp;
UNDEPLOY APPLICATION testApp;
DROP APPLICATION testApp CASCADE;
-- DROP EXCEPTIONSTORE testApp_exceptionstore;

CREATE APPLICATION testApp WITH ENCRYPTION RECOVERY 10 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE OR REPLACE SOURCE testApp_Source USING GGTrailReader ( 

  SupportColumnCharset: false, 
  TrailFilePattern: 'd1*', 
  DefinitionFile: '/Users/gopinaths/Product/IntegrationTests/TestData/OGG/alldatatype/alldtype.def', 
  Compression: false, 
  TrailDirectory: '/Users/gopinaths/Product/IntegrationTests/TestData/OGG/alldatatype', 
  Tables: 'QATEST.ALLDTYPE', 
  FilterTransactionBoundaries: true, 
  ExcludedTables:'waction.CHKPOINT',
  TrailByteOrder: 'LittleEndian' ) 
OUTPUT TO testApp_Stream;

CREATE OR REPLACE TARGET testApp_Target USING BigQueryWriter  (
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  projectId:'striimqa-214712',
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:5,Interval:120',
  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', 
  AllowQuotedNewLines: 'false', 
  adapterName: 'BigQueryWriter', 
  optimizedMerge: 'true', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', 
  StandardSQL: 'true', 
  QuoteCharacter: '\"', 
  Tables: 'QATEST.ALLDTYPE,.oratobqtgt',
  Mode: 'MERGE',
  StandardSQL: 'true',
  QuoteCharacter: '\"'
  ) INPUT FROM testApp_Stream;

CREATE OR REPLACE TARGET testApp_SysOut USING Global.SysOut (name: 'wa') INPUT FROM testApp_Stream;

END APPLICATION testApp;
DEPLOY APPLICATION testApp;
START testApp;

STOP APPLICATION @WRITERAPPNAME@;
UNDEPLOY APPLICATION @WRITERAPPNAME@;
DROP APPLICATION @WRITERAPPNAME@ CASCADE;
create or replace PROPERTYVARIABLE SRC_PASSWORD='@PROP_VAR@';
CREATE APPLICATION @WRITERAPPNAME@ RECOVERY 10 SECOND INTERVAL;
-- USE EXCEPTIONSTORE;

CREATE SOURCE @SOURCE@ USING OracleReader
(
FetchSize:1,
Username:'@SOURCE_USER@',
Password:'85d7qFnwTW8=',
--Password:'$SRC_PASSWORD',
ConnectionURL:'@CONNECTION_URL@',
Tables:'@SOURCE_TABLE@',
password_encrypted: 'true'
)
OUTPUT TO @STREAM1@;

CREATE OR REPLACE TYPE @TYPE@( 
datae java.util.HashMap , 
TABLE_NAME java.lang.String , 
OPS_NAME java.lang.String , 
DB_TIMESTAMP java.lang.String  ,
COMMITSCN java.lang.String ,
SCN java.lang.String ,
REC_INS_TIME java.lang.String );

CREATE CQ @CQ1@
INSERT INTO @STREAM2@
SELECT  
CASE WHEN (META(c,"OperationName").toString() == "DELETE")
THEN putUserData(c, 'isDelete', 'true') 
ELSE
putUserData(c,'isDelete', 'false')
END
FROM @STREAM1@ c;

CREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;

CREATE OR REPLACE CQ @CQ2@ 
INSERT INTO @STREAM3@
SELECT 
data(e),
META(e,"TableName").toString() as TABLE_NAME,
META(e, "OperationName").toString() as OPS_NAME,
META(e, "TimeStamp").toString() as DB_TIMESTAMP,
META(e,"COMMITSCN").toString() as COMMITSCN ,
META(e,"SCN").toString() as  SCN ,
DNOW().toString() as REC_INS_TIME
FROM @STREAM1@ e;

create Target @TARGET1@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC1',
ParallelThreads:'',
PartitionKey:'@metadata(TableName)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from @STREAM1@;


create Target @TARGET2@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC2',
ParallelThreads:'2',
PartitionKey:'TABLE_NAME',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from @STREAM3@;

create Target @TARGET3@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC3',
ParallelThreads:'',
PartitionKey:'@userdata(isDelete)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest.avsc')
input from @STREAM2@;

end application @WRITERAPPNAME@;
deploy application @WRITERAPPNAME@;
start @WRITERAPPNAME@;
stop application @READERAPPNAME@;
undeploy application @READERAPPNAME@;
drop application @READERAPPNAME@ cascade;
CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC1',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;


CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
filename:'@READERAPPNAME@_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream1;

CREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC2',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;


CREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC3',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest.avsc'
)
OUTPUT TO KafkaReaderStream3;


end application @READERAPPNAME@;

STOP application FileWriterDSVTester.DSV;
undeploy application FileWriterDSVTester.DSV;
drop application FileWriterDSVTester.DSV cascade;

create application DSV;

create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;


CREATE SOURCE CSVPosDataSource USING FileReader (
  directory: '@TEST-DATA-PATH@',
  WildCard: 'posdata.csv',
  positionByEOF: false,
  charset: 'UTF-8'
 )
 PARSE USING DSVParser (
  header: 'yes'
 )
OUTPUT TO PosDataCsvStream;

create source CSVMerchantNamesSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvMerchantNamesStream;

create source CSVSmallRetailSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallretaildata2M.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvSmallRetailStream;


Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Type CSVMerchantNamesType (
  merchantId String,
  merchantName String
);

Create Type CSVSmallRetailType (
storeId String,
nameId String,
city String,
state String
);

Create Stream TypedCSVStream of CSVType;
Create Stream TypedPosDataCSVStream of CSVType;
Create Stream TypedCSVMerchantNamesStream of CSVMerchantNamesType;
Create Stream TypedCSVSmallRetailStream of CSVSmallRetailType;


CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;


CREATE CQ CsvPosData
INSERT INTO TypedPosDataCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM PosDataCsvStream;


CREATE CQ CsvToMerchantNames
INSERT INTO TypedCSVMerchantNamesStream
SELECT data[0],
       data[1]
FROM CsvMerchantNamesStream;


CREATE CQ CsvToSmallRetailData
INSERT INTO TypedCSVSmallRetailStream
SELECT data[0],
       data[1],
       data[2],
       data[3]
FROM CsvSmallRetailStream;

/**
* 3.4.5.c FileWriter DSV ParserNegativeEC 100
**/
create Target DSVNegativeEventCount using FileWriter(
filename:'EventNCDefault',
directory:'@FEATURE-DIR@/logs/',
sequence:'00',
--filelimit: '5',
rolloverpolicy:'eventcount:-100',
buffersize:1
)
format using DSVFormatter (

)
input from TypedCSVSmallRetailStream;


/**
* 3.3.1.c FileWriter DSV TimeInterval
**/
create Target DSVTimeIntervalRollingPolicy using FileWriter(
  filename:'MerchantTIRP',
  sequence:'00',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00',
  buffersize:1
)
format using DSVFormatter (

)
input from TypedCSVMerchantNamesStream;

/**
* 3.4.6.d FileWriter DSV DefaultRP
**/
create Target DSVDefaultRollingPolicy using FileWriter(
directory:'@FEATURE-DIR@/logs/',
filename:'PosData',
rolloverpolicy:'EventCount:5000000',
buffersize:1
)
format using DSVFormatter (

)
input from TypedCSVStream;

/**
* 3.4.5.d FileWriter DSV EventCount 100
**/
create Target DSVEventCountDecimal using FileWriter(
filename:'Events',
directory:'@FEATURE-DIR@/logs/',
rolloverpolicy:'eventcount:200,sequence:00',
buffersize:1
)
format using DSVFormatter (

)
input from TypedCSVStream;

/**
* 3.3.1.a FileWriter DSVFileSize 1MB
**/
create Target DSVFileSize using FileWriter(
directory:'@FEATURE-DIR@/logs',
filename:'PosDataFS',
rolloverpolicy:'FileSizeRollingPolicy,filesize:1M,sequence:00',
buffersize:1
)
format using DSVFormatter (

)
input from TypedCSVStream;

/**
* 3.4.5.a FileWriter DSVFileSizeRoundUp 3MB
**/
create Target DSVFileSizeDecimal using FileWriter(
  directory:'@FEATURE-DIR@/logs/',
  filename:'RoundUPPosData',
  rolloverpolicy:'FileSizeRollingPolicy,filesize:2.5M,sequence:00',
  buffersize:1
)
format using DSVFormatter (

)
input from TypedCSVStream;


/**
* 3.1.1.b FileWriter DSV EventCount
**/

CREATE OR REPLACE TARGET DSVEventCount USING FileWriter (
  filename: 'TargetDefault',
  directory:'@FEATURE-DIR@/logs/',
  sequence:'00',
  flushinterval: '0',
  rolloverpolicy:'EventCount:5000000',
  buffersize:1
 )
 format using DSVFormatter (

)
INPUT FROM TypedPosDataCSVStream;


create Target TargetSmallPosData using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizePosDataDefault_actual.log') input from TypedCSVStream;

end application DSV;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;

Create Source @SourceName@ Using Ojet

(
  Username:'c##qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',
  Tables:'CDB$ROOT."C##QATEST".ojet_src;ORCLPDB.QATEST.ojet_src',
  _h_useClassic:false,
  Fetchsize:1,
  Compression: true,
  SupportPDB:true,
  ReplicationSlotName:'null'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',
  Username:'c##qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'CDB$ROOT."C##QATEST".ojet_src,CDB$ROOT."C##QATEST".ojet_tgt'
) INPUT FROM @SRCINPUTSTREAM@;



create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop tpcc;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;


Create Source @SourceName@
 Using Ojet
(
 Username:'@READER-UNAME@',
 Password:'@READER-PASSWORD@',
 ConnectionURL:'@CDC-READER-URL@',
 Tables: '@WATABLES-SRC@',
 FetchSize:1,
 QueueSize:25000,
 CommittedTransactions:true,
 Compression:true,
 CaptureDDL: true,
 SendBeforeImage:true
) Output To @SRCINPUTSTREAM@;


create Target @targetsys@ using SysOut(name:OrgData) input from DataStream;

CREATE TARGET @targetName@ USING databasewriter(
  Username: '@WRITER-UNAME@',
  Password: '@WRITER-PASSWORD@',
  ConnectionURL:'@WRITER-URL@',
  BatchPolicy:'EventCount:1,Interval:1',
  CommitPolicy:'EventCount:1,Interval:1',
  Tables: '@WATABLES-SRC@,@WATABLES-TGT@'
) INPUT FROM @SRCINPUTSTREAM@;


END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.e1ptest%',
	FetchSize: '1'
)
OUTPUT TO @APPNAME@_SS;


CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.e1ptest%,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12)',
Mode:'merge',
optimizedMerge: 'true',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:2',
StandardSQL:true		
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

--
-- Recovery Test 42 with two sources and two WactionStores. A variety of partitioned windows in between
-- assure that we are testing a complicated recovery scenario.
--
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Stream -> JWc5 -> WS1
--   S2 -> Stream -> JWc10 -> WS2
--

STOP Recov42Tester.RecovTest42;
UNDEPLOY APPLICATION Recov42Tester.RecovTest42;
DROP APPLICATION Recov42Tester.RecovTest42 CASCADE;
CREATE APPLICATION RecovTest42 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10242,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10242,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM DataStreamTop OF CsvData using KafkaProps;

CREATE CQ Csv1ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ Csv2ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;










CREATE JUMPING WINDOW LeftJWc5
OVER DataStreamTop KEEP 5 ROWS;

CREATE JUMPING WINDOW RightJWc10
OVER DataStreamTop KEEP 10 ROWS;



CREATE WACTIONSTORE WactionsLeft CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsRight CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ ToWactionsLeft
INSERT INTO WactionsLeft
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM LeftJWc5 p;

CREATE CQ ToWactionsRight
INSERT INTO WactionsRight
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM RightJWc10 p;

END APPLICATION RecovTest42;

stop application AzureApp;
undeploy application AzureApp;
drop application AzureApp cascade;

create application AzureApp
RECOVERY 10 second interval;
create source CSVSource using FileReader (
	directory:'@DIR@',
	WildCard:'@WILDCARD@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  curr String,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       data[6],
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target BlobT using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:50,interval:5s'
)
format using JSONFormatter (
)
input from TypedCSVStream;
end application AzureApp;
deploy application AzureApp in default;
start application AzureApp;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using AzureblobWriter(
    accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:7'
)
format using DSVFormatter (
)
input from @STREAM@;

end application @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]);

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;
Create Source @APPNAME@_s Using DatabaseReader
(
 Username:'@UNAME@',
 Password:'@PASSWORD@',
 ConnectionURL:'@SRCURL@',
 Tables:'QATEST.HIVE_IL_%',
 FetchSize:1,
 QuiesceOnILCompletion: true
)
Output To @APPNAME@_ss;


create Target @APPNAME@_t using HiveWriter(
            ConnectionURL:'@TGTURL@',
            Username:'@TGTUNAME@', 
            Password:'@TGTPASSWORD@',
            hadoopurl:'hdfs://dockerhost:9000/',
	        Mode:'initialLoad',
	        mergepolicy: '@MERGEPOLICY@',
            Tables:'QATEST.HIVE_IL_01,default.hive_il_01;QATEST.HIVE_IL_02,default.hive_il_02',
            hadoopConfigurationPath:'@CONF@'
	) input from @APPNAME@_ss;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

--
-- Recovery Test 24 with two sources, two sliding time-count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5a9W  -> CQ1 -> WS
-- S2 -> Sc6a11W -> CQ2 -> WS
--

STOP KStreamRecov24Tester.KStreamRecovTest24;
UNDEPLOY APPLICATION KStreamRecov24Tester.KStreamRecovTest24;
DROP APPLICATION KStreamRecov24Tester.KStreamRecovTest24 CASCADE;
DROP USER KStreamRecov24Tester;
DROP NAMESPACE KStreamRecov24Tester CASCADE;
CREATE USER KStreamRecov24Tester IDENTIFIED BY KStreamRecov24Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov24Tester;
CONNECT KStreamRecov24Tester KStreamRecov24Tester;

CREATE APPLICATION KStreamRecovTest24 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION KStreamRecovTest24;

create source @SOURCE_NAME@ USING MariaDBReader 
(
Username: '@READER-UNAME@',
Password: '@READER-PASSWORD@',
ConnectionURL: '@CDC-READER-URL@',
Tables: @WATABLES@,
sendBeforeImage:'true',
FilterTransactionBoundaries: 'true'
) 
OUTPUT TO @STREAM@;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;

drop propertyset OrcToOrcPlatfm_App_KafkaPropset;
drop stream  OrcToOrcPlatfm_Stream CASCADE;


--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;

CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNEC4TION_URL@',
  Tables: '$table1',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING AzureSQLDWHWriter  (
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'QATEST.ORCTOORCPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING AzureSQLDWHWriter  (
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
 )
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '$table2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@4',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING AzureSQLDWHWriter  (
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'QATEST.ORCTOORCPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING AzureSQLDWHWriter  (
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
 )
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;

END APPLICATION @APPNAME@2;

--
-- Recovery Test 12 with two sources, two jumping attribute windows, one wactionstore with recovery, and another wactionstore without -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W/p -> CQ1 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS2 (no persists)
--

STOP Recov12Tester.RecovTest12;
UNDEPLOY APPLICATION Recov12Tester.RecovTest12;
DROP APPLICATION Recov12Tester.RecovTest12 CASCADE;
CREATE APPLICATION RecovTest12 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsNoPersist CONTEXT OF WactionData
EVENT TYPES ( CsvData )
		PERSIST NONE USING ( ) ;

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWactionNoPersist
INSERT INTO WactionsNoPersist
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION RecovTest12;

use PosTester;
alter application PosApp;

CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

end application PosApp;
alter application PosApp recompile;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING XMLParser (
  rootnode: ''
)
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

STOP application consoletest.noApp;
undeploy application consoletest.noApp;
drop application consoletest.noApp cascade;

create application noApp;
create source CSVSource using FileReader (
  directory:'Wrong/Dir/Path',
  WildCard:'posdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'Foo',
        sequence:'00',
  rolloverpolicy:'eventcount:200'
)
format using DSVFormatter (

)
input from TypedCSVStream;
end application noApp;

CREATE APPLICATION BankDataApp;

CREATE TYPE MoreBankData (
  bankID java.lang.Integer KEY,
  bankName java.lang.String,
  bankRouting java.lang.Long,
  bankAmount java.lang.Double
);

CREATE STREAM BankDataStream OF MoreBankData;

CREATE TARGET SysOut USING Global.SysOut ( 
  name: 'SysOut'
) 
INPUT FROM BankDataStream;

END APPLICATION BankDataApp;

DEPLOY APPLICATION BankDataApp;
START BankDataApp;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '@GRPPOLICY@',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'MultipleEvents',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
  filename: '%@metadata(FileName)%',
  directory: '@DIR@',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter()
INPUT FROM CCBStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

DROP APPLICATION SliderSorter CASCADE;

CREATE APPLICATION SliderSorter;

CREATE OR REPLACE TYPE OrdersDataPARSED_TP (
    tDateTime       org.joda.time.DateTime KEY,
    sBusinessName   java.lang.String KEY,
    sMerchantID     java.lang.String KEY,
    sOrderId        java.lang.String,
    sZip            java.lang.String,
    lTerminalID     java.lang.Long,
    fPaidAmount     java.lang.Float  
);

CREATE OR REPLACE STREAM OrdersDataPARSED_ST OF OrdersDataPARSED_TP;

CREATE OR REPLACE TYPE ReturnsDataPARSED_TP (
    tDateTime org.joda.time.DateTime KEY,
    sOrderId java.lang.String KEY,
    fReturnedAmount java.lang.Float
);

CREATE OR REPLACE STREAM ReturnsDataPARSED_ST OF ReturnsDataPARSED_TP;

CREATE OR REPLACE STREAM OrdersDataSORTED_ST OF OrdersDataPARSED_TP;
CREATE OR REPLACE STREAM ReturnsDataSORTED_ST OF ReturnsDataPARSED_TP;
CREATE OR REPLACE STREAM Errors_ST OF Global.WAEvent;

CREATE SORTER MySorter OVER
OrdersDataPARSED_ST  ON tDateTime OUTPUT TO OrdersDataSORTED_ST,
ReturnsDataPARSED_ST ON tDateTime OUTPUT TO ReturnsDataSORTED_ST
WITHIN 2 MINUTE
OUTPUT ERRORS TO Errors_ST;

END APPLICATION SliderSorter;

--
-- Crash Recovery Test 1 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP APPLICATION N2S2CR1Tester.N2S2CRTest1;
UNDEPLOY APPLICATION N2S2CR1Tester.N2S2CRTest1;
DROP APPLICATION N2S2CR1Tester.N2S2CRTest1 CASCADE;
CREATE APPLICATION N2S2CRTest1 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest1;

CREATE SOURCE CsvSourceN2S2CRTest1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN2S2CRTest1;

CREATE FLOW DataProcessingN2S2CRTest1;

CREATE TYPE WactionTypeN2S2CRTest1 (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE WactionsN2S2CRTest1 CONTEXT OF WactionTypeN2S2CRTest1
EVENT TYPES ( WactionTypeN2S2CRTest1 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN2S2CRTest1
INSERT INTO WactionsN2S2CRTest1
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END FLOW DataProcessingN2S2CRTest1;

END APPLICATION N2S2CRTest1;

stop Oracle_IRLogWriter;
undeploy application Oracle_IRLogWriter;
drop application Oracle_IRLogWriter cascade;
CREATE APPLICATION Oracle_IRLogWriter;

Create Source s1 Using IncrementalBatchReader (
 FetchSize: 1,
  Username: 'striim',
  Password: 'o4l1uMpwIDQ=',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.autotest01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.autotest01=id',
  startPosition: '%=0'
 )
OUTPUT TO data_stream1;

create source s2 using IncrementalBatchReader (
FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.autotest02',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.autotest02=id',
  startPosition: '%=0'
 )
OUTPUT TO data_stream2;

create source s3 using IncrementalBatchReader (
FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.autotest03',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.autotest03=id',
  startPosition: '%=0'
 )
OUTPUT TO data_stream3;

Create Type EventType (
ID int,
NAME string,
COMPANY string,
COUNTRY string
);

CREATE STREAM insertData1  of EventType;
CREATE STREAM deleteData1 of EventType;
CREATE STREAM joinData1 of EventType;
CREATE STREAM joinData2 of EventType;
CREATE STREAM deleteData2 of EventType;
CREATE STREAM OutStream of EventType;

CREATE CQ cq1 INSERT INTO insertData1  SELECT TO_INT(data[0]),data[1],data[2],data[3] FROM data_stream1;

CREATE CQ cq2 INSERT INTO deleteData1 SELECT TO_INT(data[0]),data[1],data[2],data[3] FROM data_stream2;

CREATE CQ cq3 INSERT INTO joinData1 SELECT TO_INT(data[0]),data[1],data[2],data[3] FROM data_stream3;

CREATE JUMPING WINDOW DataWin1 OVER deleteData1 KEEP 1 ROWS;

CREATE CQ TypedEventKeyCQ6 INSERT INTO deleteData2 SELECT * from DataWin1;

CREATE JUMPING WINDOW DataWin2 OVER joinData1 KEEP 1 ROWS;

CREATE CQ TypedEventKeyCQ5 INSERT INTO joinData2 SELECT * from DataWin2;

CREATE EVENTTABLE ETABLE1 using STREAM ( NAME: 'insertData1 ' )
--DELETE using STREAM ( NAME: 'deleteData1')
QUERY (keytomap:"ID", persistPolicy: 'true') OF EventType;

CREATE CQ cq4 INSERT INTO OutStream SELECT B.ID,B.NAME,B.COMPANY,B.COUNTRY FROM joinData2 A, ETABLE1 B where A.ID=B.ID;

CREATE TARGET EventTableFW USING FileWriter
(filename:'BasicOracle_IRLogWriter_RT.log',
 rolloverpolicy: 'EventCount:1000000')
FORMAT USING DSVFormatter () INPUT FROM OutStream;

create target Target_Azure using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'STRIIM',
        password: 'W3b@ct10n',
        AccountName: 'striimqatestdonotdelete',
        accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables:'dbo.autotest01',
        uploadpolicy:'eventcount:0,interval:0s'
) INPUT FROM OutStream;

END APPLICATION Oracle_IRLogWriter;
deploy application Oracle_IRLogWriter in default;
start Oracle_IRLogWriter;

Stop application DEV21349.app1;
Undeploy application DEV21349.app1;
Drop application DEV21349.app1 cascade;
Drop namespace DEV21349 cascade;
Create namespace DEV21349;
Use DEV21349;

CREATE APPLICATION app1;

CREATE CQ cq1 INSERT INTO cq1_outputstream select * from global.exceptionsstream exStream;

CREATE WACTIONSTORE exceptionsWS CONTEXT OF cq1_outputstream_Type @PERSIST-TYPE@;

CREATE CQ cq2 INSERT INTO exceptionsWS SELECT * FROM cq1_outputstream c;

CREATE OR REPLACE CQ readingWS_cq INSERT INTO outputstream2 select * from exceptionsWS;

CREATE OR REPLACE TARGET ExTarget USING Global.FileWriter  (
DataEncryptionKeyPassphrase: '',
  filename: 'ExceptionsOutPutFile',
  flushpolicy: 'EventCount:10000,Interval:30s',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  directory: '@TEST-DATA-PATH@',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  ( jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n' )
INPUT FROM outputstream2;

END APPLICATION app1;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;

 CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01;striim.test02;striim.test03',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',
 startPosition: 'striim.test01=1;striim.test02=-1;%=0',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream ;

  CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

create target AzureSQLDWHTarget1 using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'STRIIM.AUTO_LENGTHY_TABLE_NAME_FROM_INCREMENTAL_READER_TO_AZURE_STRUCTURED_QUERY_LANGUAGE_DATABASE_WAREHOUSE_WITH_128_CHARACTER_MAXIMUM,DBO.AUTOTEST_LENGTHY1 COLUMNMAP(Field1=AUTO_LENGTHY_COLUMNNAME_FROM_INCREMENTAL_READER_TO_AZURE_STRUCTURED_QUERY_LANGUAGE_DATABASE_WAREHOUSE_WITH_128_CHARACTER_MAXIMUM)',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream;

create target AzureSQLDWHTarget2 using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'STRIIM.AUTO_LENGTHY_TABLE_NAME_FROM_INCREMENTAL_READER_TO_AZURE_STRUCTURED_QUERY_LANGUAGE_DATABASE_WAREHOUSE_WITH_128_CHARACTER_MAXIMUM,DBO.AUTOTEST_LENGTHY2 COLUMNMAP(Field1=AUTO_LENGTHY_COLUMNNAME_FROM_INCREMENTAL_READER_TO_AZURE_STRUCTURED_QUERY_LANGUAGE_DATABASE_WAREHOUSE_WITH_128_CHARACTER_MAXIMUM,Field2=Field1026,Field3=Field1027,Field4=Field1028,Field5=Field1029,Field6=Field1030,Field7=Field1031,Field8=Field1032,Field9=Field1033,Field10=Field1034,Field11=Field1035,Field12=Field1036,Field13=Field1037,Field14=Field1038,Field15=Field1039,Field16=Field1040,Field17=Field1041,Field18=Field1042,Field19=Field1043,Field20=Field1044,Field21=Field1045,Field22=Field1046,Field23=Field1047,Field24=Field1048,Field25=Field1049,Field26=Field1050,Field27=Field1051,Field28=Field1052,Field29=Field1053,Field30=Field1054,Field31=Field1055,Field32=Field1056,Field33=Field1057,Field34=Field1058,Field35=Field1059,Field36=Field1060,Field37=Field1061,Field38=Field1062,Field39=Field1063,Field40=Field1064,Field41=Field1065,Field42=Field1066,Field43=Field1067,Field44=Field1068,Field45=Field1069,Field46=Field1070,Field47=Field1071,Field48=Field1072,Field49=Field1073,Field50=Field1074,Field51=Field1075,Field52=Field1076,Field53=Field1077,Field54=Field1078,Field55=Field1079,Field56=Field1080,Field57=Field1081,Field58=Field1082,Field59=Field1083,Field60=Field1084,Field61=Field1085,Field62=Field1086,Field63=Field1087,Field64=Field1088,Field65=Field1089,Field66=Field1090,Field67=Field1091,Field68=Field1092,Field69=Field1093,Field70=Field1094,Field71=Field1095,Field72=Field1096,Field73=Field1097,Field74=Field1098,Field75=Field1099,Field76=Field1100,Field77=Field1101,Field78=Field1102,Field79=Field1103,Field80=Field1104,Field81=Field1105,Field82=Field1106,Field83=Field1107,Field84=Field1108,Field85=Field1109,Field86=Field1110,Field87=Field1111,Field88=Field1112,Field89=Field1113,Field90=Field1114,Field91=Field1115,Field92=Field1116,Field93=Field1117,Field94=Field1118,Field95=Field1119,Field96=Field1120,Field97=Field1121,Field98=Field1122,Field99=Field1123,Field100=Field1124,Field101=Field1125,Field102=Field1126,Field103=Field1127,Field104=Field1128,Field105=Field1129,Field106=Field1130,Field107=Field1131,Field108=Field1132,Field109=Field1133,Field110=Field1134,Field111=Field1135,Field112=Field1136,Field113=Field1137,Field114=Field1138,Field115=Field1139,Field116=Field1140,Field117=Field1141,Field118=Field1142,Field119=Field1143,Field120=Field1144,Field121=Field1145,Field122=Field1146,Field123=Field1147,Field124=Field1148,Field125=Field1149,Field126=Field1150,Field127=Field1151,Field128=Field1152,Field129=Field1153,Field130=Field1154,Field131=Field1155,Field132=Field1156,Field133=Field1157,Field134=Field1158,Field135=Field1159,Field136=Field1160,Field137=Field1161,Field138=Field1162,Field139=Field1163,Field140=Field1164,Field141=Field1165,Field142=Field1166,Field143=Field1167,Field144=Field1168,Field145=Field1169,Field146=Field1170,Field147=Field1171,Field148=Field1172,Field149=Field1173,Field150=Field1174,Field151=Field1175,Field152=Field1176,Field153=Field1177,Field154=Field1178,Field155=Field1179,Field156=Field1180,Field157=Field1181,Field158=Field1182,Field159=Field1183,Field160=Field1184,Field161=Field1185,Field162=Field1186,Field163=Field1187,Field164=Field1188,Field165=Field1189,Field166=Field1190,Field167=Field1191,Field168=Field1192,Field169=Field1193,Field170=Field1194,Field171=Field1195,Field172=Field1196,Field173=Field1197,Field174=Field1198,Field175=Field1199,Field176=Field1200,Field177=Field1201,Field178=Field1202,Field179=Field1203,Field180=Field1204,Field181=Field1205,Field182=Field1206,Field183=Field1207,Field184=Field1208,Field185=Field1209,Field186=Field1210,Field187=Field1211,Field188=Field1212,Field189=Field1213,Field190=Field1214,Field191=Field1215,Field192=Field1216,Field193=Field1217,Field194=Field1218,Field195=Field1219,Field196=Field1220,Field197=Field1221,Field198=Field1222,Field199=Field1223,Field200=Field1224,Field201=Field1225,Field202=Field1226,Field203=Field1227,Field204=Field1228,Field205=Field1229,Field206=Field1230,Field207=Field1231,Field208=Field1232,Field209=Field1233,Field210=Field1234,Field211=Field1235,Field212=Field1236,Field213=Field1237,Field214=Field1238,Field215=Field1239,Field216=Field1240,Field217=Field1241,Field218=Field1242,Field219=Field1243,Field220=Field1244,Field221=Field1245,Field222=Field1246,Field223=Field1247,Field224=Field1248,Field225=Field1249,Field226=Field1250,Field227=Field1251,Field228=Field1252,Field229=Field1253,Field230=Field1254,Field231=Field1255,Field232=Field1256,Field233=Field1257,Field234=Field1258,Field235=Field1259,Field236=Field1260,Field237=Field1261,Field238=Field1262,Field239=Field1263,Field240=Field1264,Field241=Field1265,Field242=Field1266,Field243=Field1267,Field244=Field1268,Field245=Field1269,Field246=Field1270,Field247=Field1271,Field248=Field1272,Field249=Field1273,Field250=Field1274,Field251=Field1275,Field252=Field1276,Field253=Field1277,Field254=Field1278,Field255=Field1279,Field256=Field1280,Field257=Field1281,Field258=Field1282,Field259=Field1283,Field260=Field1284,Field261=Field1285,Field262=Field1286,Field263=Field1287,Field264=Field1288,Field265=Field1289,Field266=Field1290,Field267=Field1291,Field268=Field1292,Field269=Field1293,Field270=Field1294,Field271=Field1295,Field272=Field1296,Field273=Field1297,Field274=Field1298,Field275=Field1299,Field276=Field1300,Field277=Field1301,Field278=Field1302,Field279=Field1303,Field280=Field1304,Field281=Field1305,Field282=Field1306,Field283=Field1307,Field284=Field1308,Field285=Field1309,Field286=Field1310,Field287=Field1311,Field288=Field1312,Field289=Field1313,Field290=Field1314,Field291=Field1315,Field292=Field1316,Field293=Field1317,Field294=Field1318,Field295=Field1319,Field296=Field1320,Field297=Field1321,Field298=Field1322,Field299=Field1323,Field300=Field1324,Field301=Field1325,Field302=Field1326,Field303=Field1327,Field304=Field1328,Field305=Field1329,Field306=Field1330,Field307=Field1331,Field308=Field1332,Field309=Field1333,Field310=Field1334,Field311=Field1335,Field312=Field1336,Field313=Field1337,Field314=Field1338,Field315=Field1339,Field316=Field1340,Field317=Field1341,Field318=Field1342,Field319=Field1343,Field320=Field1344,Field321=Field1345,Field322=Field1346,Field323=Field1347,Field324=Field1348,Field325=Field1349,Field326=Field1350,Field327=Field1351,Field328=Field1352,Field329=Field1353,Field330=Field1354,Field331=Field1355,Field332=Field1356,Field333=Field1357,Field334=Field1358,Field335=Field1359,Field336=Field1360,Field337=Field1361,Field338=Field1362,Field339=Field1363,Field340=Field1364,Field341=Field1365,Field342=Field1366,Field343=Field1367,Field344=Field1368,Field345=Field1369,Field346=Field1370,Field347=Field1371,Field348=Field1372,Field349=Field1373,Field350=Field1374,Field351=Field1375,Field352=Field1376,Field353=Field1377,Field354=Field1378,Field355=Field1379,Field356=Field1380,Field357=Field1381,Field358=Field1382,Field359=Field1383,Field360=Field1384,Field361=Field1385,Field362=Field1386,Field363=Field1387,Field364=Field1388,Field365=Field1389,Field366=Field1390,Field367=Field1391,Field368=Field1392,Field369=Field1393,Field370=Field1394,Field371=Field1395,Field372=Field1396,Field373=Field1397,Field374=Field1398,Field375=Field1399,Field376=Field1400,Field377=Field1401,Field378=Field1402,Field379=Field1403,Field380=Field1404,Field381=Field1405,Field382=Field1406,Field383=Field1407,Field384=Field1408,Field385=Field1409,Field386=Field1410,Field387=Field1411,Field388=Field1412,Field389=Field1413,Field390=Field1414,Field391=Field1415,Field392=Field1416,Field393=Field1417,Field394=Field1418,Field395=Field1419,Field396=Field1420,Field397=Field1421,Field398=Field1422,Field399=Field1423,Field400=Field1424,Field401=Field1425,Field402=Field1426,Field403=Field1427,Field404=Field1428,Field405=Field1429,Field406=Field1430,Field407=Field1431,Field408=Field1432,Field409=Field1433,Field410=Field1434,Field411=Field1435,Field412=Field1436,Field413=Field1437,Field414=Field1438,Field415=Field1439,Field416=Field1440,Field417=Field1441,Field418=Field1442,Field419=Field1443,Field420=Field1444,Field421=Field1445,Field422=Field1446,Field423=Field1447,Field424=Field1448,Field425=Field1449,Field426=Field1450,Field427=Field1451,Field428=Field1452,Field429=Field1453,Field430=Field1454,Field431=Field1455,Field432=Field1456,Field433=Field1457,Field434=Field1458,Field435=Field1459,Field436=Field1460,Field437=Field1461,Field438=Field1462,Field439=Field1463,Field440=Field1464,Field441=Field1465,Field442=Field1466,Field443=Field1467,Field444=Field1468,Field445=Field1469,Field446=Field1470,Field447=Field1471,Field448=Field1472,Field449=Field1473,Field450=Field1474,Field451=Field1475,Field452=Field1476,Field453=Field1477,Field454=Field1478,Field455=Field1479,Field456=Field1480,Field457=Field1481,Field458=Field1482,Field459=Field1483,Field460=Field1484,Field461=Field1485,Field462=Field1486,Field463=Field1487,Field464=Field1488,Field465=Field1489,Field466=Field1490,Field467=Field1491,Field468=Field1492,Field469=Field1493,Field470=Field1494,Field471=Field1495,Field472=Field1496,Field473=Field1497,Field474=Field1498,Field475=Field1499,Field476=Field1500,Field477=Field1501,Field478=Field1502,Field479=Field1503,Field480=Field1504,Field481=Field1505,Field482=Field1506,Field483=Field1507,Field484=Field1508,Field485=Field1509,Field486=Field1510,Field487=Field1511,Field488=Field1512,Field489=Field1513,Field490=Field1514,Field491=Field1515,Field492=Field1516,Field493=Field1517,Field494=Field1518,Field495=Field1519,Field496=Field1520,Field497=Field1521,Field498=Field1522,Field499=Field1523,Field500=Field1524,Field501=Field1525,Field502=Field1526,Field503=Field1527,Field504=Field1528,Field505=Field1529,Field506=Field1530,Field507=Field1531,Field508=Field1532,Field509=Field1533,Field510=Field1534,Field511=Field1535,Field512=Field1536,Field513=Field1537,Field514=Field1538,Field515=Field1539,Field516=Field1540,Field517=Field1541,Field518=Field1542,Field519=Field1543,Field520=Field1544,Field521=Field1545,Field522=Field1546,Field523=Field1547,Field524=Field1548,Field525=Field1549,Field526=Field1550,Field527=Field1551,Field528=Field1552,Field529=Field1553,Field530=Field1554,Field531=Field1555,Field532=Field1556,Field533=Field1557,Field534=Field1558,Field535=Field1559,Field536=Field1560,Field537=Field1561,Field538=Field1562,Field539=Field1563,Field540=Field1564,Field541=Field1565,Field542=Field1566,Field543=Field1567,Field544=Field1568,Field545=Field1569,Field546=Field1570,Field547=Field1571,Field548=Field1572,Field549=Field1573,Field550=Field1574,Field551=Field1575,Field552=Field1576,Field553=Field1577,Field554=Field1578,Field555=Field1579,Field556=Field1580,Field557=Field1581,Field558=Field1582,Field559=Field1583,Field560=Field1584,Field561=Field1585,Field562=Field1586,Field563=Field1587,Field564=Field1588,Field565=Field1589,Field566=Field1590,Field567=Field1591,Field568=Field1592,Field569=Field1593,Field570=Field1594,Field571=Field1595,Field572=Field1596,Field573=Field1597,Field574=Field1598,Field575=Field1599,Field576=Field1600,Field577=Field1601,Field578=Field1602,Field579=Field1603,Field580=Field1604,Field581=Field1605,Field582=Field1606,Field583=Field1607,Field584=Field1608,Field585=Field1609,Field586=Field1610,Field587=Field1611,Field588=Field1612,Field589=Field1613,Field590=Field1614,Field591=Field1615,Field592=Field1616,Field593=Field1617,Field594=Field1618,Field595=Field1619,Field596=Field1620,Field597=Field1621,Field598=Field1622,Field599=Field1623,Field600=Field1624,Field601=Field1625,Field602=Field1626,Field603=Field1627,Field604=Field1628,Field605=Field1629,Field606=Field1630,Field607=Field1631,Field608=Field1632,Field609=Field1633,Field610=Field1634,Field611=Field1635,Field612=Field1636,Field613=Field1637,Field614=Field1638,Field615=Field1639,Field616=Field1640,Field617=Field1641,Field618=Field1642,Field619=Field1643,Field620=Field1644,Field621=Field1645,Field622=Field1646,Field623=Field1647,Field624=Field1648,Field625=Field1649,Field626=Field1650,Field627=Field1651,Field628=Field1652,Field629=Field1653,Field630=Field1654,Field631=Field1655,Field632=Field1656,Field633=Field1657,Field634=Field1658,Field635=Field1659,Field636=Field1660,Field637=Field1661,Field638=Field1662,Field639=Field1663,Field640=Field1664,Field641=Field1665,Field642=Field1666,Field643=Field1667,Field644=Field1668,Field645=Field1669,Field646=Field1670,Field647=Field1671,Field648=Field1672,Field649=Field1673,Field650=Field1674,Field651=Field1675,Field652=Field1676,Field653=Field1677,Field654=Field1678,Field655=Field1679,Field656=Field1680,Field657=Field1681,Field658=Field1682,Field659=Field1683,Field660=Field1684,Field661=Field1685,Field662=Field1686,Field663=Field1687,Field664=Field1688,Field665=Field1689,Field666=Field1690,Field667=Field1691,Field668=Field1692,Field669=Field1693,Field670=Field1694,Field671=Field1695,Field672=Field1696,Field673=Field1697,Field674=Field1698,Field675=Field1699,Field676=Field1700,Field677=Field1701,Field678=Field1702,Field679=Field1703,Field680=Field1704,Field681=Field1705,Field682=Field1706,Field683=Field1707,Field684=Field1708,Field685=Field1709,Field686=Field1710,Field687=Field1711,Field688=Field1712,Field689=Field1713,Field690=Field1714,Field691=Field1715,Field692=Field1716,Field693=Field1717,Field694=Field1718,Field695=Field1719,Field696=Field1720,Field697=Field1721,Field698=Field1722,Field699=Field1723,Field700=Field1724,Field701=Field1725,Field702=Field1726,Field703=Field1727,Field704=Field1728,Field705=Field1729,Field706=Field1730,Field707=Field1731,Field708=Field1732,Field709=Field1733,Field710=Field1734,Field711=Field1735,Field712=Field1736,Field713=Field1737,Field714=Field1738,Field715=Field1739,Field716=Field1740,Field717=Field1741,Field718=Field1742,Field719=Field1743,Field720=Field1744,Field721=Field1745,Field722=Field1746,Field723=Field1747,Field724=Field1748,Field725=Field1749,Field726=Field1750,Field727=Field1751,Field728=Field1752,Field729=Field1753,Field730=Field1754,Field731=Field1755,Field732=Field1756,Field733=Field1757,Field734=Field1758,Field735=Field1759,Field736=Field1760,Field737=Field1761,Field738=Field1762,Field739=Field1763,Field740=Field1764,Field741=Field1765,Field742=Field1766,Field743=Field1767,Field744=Field1768,Field745=Field1769,Field746=Field1770,Field747=Field1771,Field748=Field1772,Field749=Field1773,Field750=Field1774,Field751=Field1775,Field752=Field1776,Field753=Field1777,Field754=Field1778,Field755=Field1779,Field756=Field1780,Field757=Field1781,Field758=Field1782,Field759=Field1783,Field760=Field1784,Field761=Field1785,Field762=Field1786,Field763=Field1787,Field764=Field1788,Field765=Field1789,Field766=Field1790,Field767=Field1791,Field768=Field1792,Field769=Field1793,Field770=Field1794,Field771=Field1795,Field772=Field1796,Field773=Field1797,Field774=Field1798,Field775=Field1799,Field776=Field1800,Field777=Field1801,Field778=Field1802,Field779=Field1803,Field780=Field1804,Field781=Field1805,Field782=Field1806,Field783=Field1807,Field784=Field1808,Field785=Field1809,Field786=Field1810,Field787=Field1811,Field788=Field1812,Field789=Field1813,Field790=Field1814,Field791=Field1815,Field792=Field1816,Field793=Field1817,Field794=Field1818,Field795=Field1819,Field796=Field1820,Field797=Field1821,Field798=Field1822,Field799=Field1823,Field800=Field1824,Field801=Field1825,Field802=Field1826,Field803=Field1827,Field804=Field1828,Field805=Field1829,Field806=Field1830,Field807=Field1831,Field808=Field1832,Field809=Field1833,Field810=Field1834,Field811=Field1835,Field812=Field1836,Field813=Field1837,Field814=Field1838,Field815=Field1839,Field816=Field1840,Field817=Field1841,Field818=Field1842,Field819=Field1843,Field820=Field1844,Field821=Field1845,Field822=Field1846,Field823=Field1847,Field824=Field1848,Field825=Field1849,Field826=Field1850,Field827=Field1851,Field828=Field1852,Field829=Field1853,Field830=Field1854,Field831=Field1855,Field832=Field1856,Field833=Field1857,Field834=Field1858,Field835=Field1859,Field836=Field1860,Field837=Field1861,Field838=Field1862,Field839=Field1863,Field840=Field1864,Field841=Field1865,Field842=Field1866,Field843=Field1867,Field844=Field1868,Field845=Field1869,Field846=Field1870,Field847=Field1871,Field848=Field1872,Field849=Field1873,Field850=Field1874,Field851=Field1875,Field852=Field1876,Field853=Field1877,Field854=Field1878,Field855=Field1879,Field856=Field1880,Field857=Field1881,Field858=Field1882,Field859=Field1883,Field860=Field1884,Field861=Field1885,Field862=Field1886,Field863=Field1887,Field864=Field1888,Field865=Field1889,Field866=Field1890,Field867=Field1891,Field868=Field1892,Field869=Field1893,Field870=Field1894,Field871=Field1895,Field872=Field1896,Field873=Field1897,Field874=Field1898,Field875=Field1899,Field876=Field1900,Field877=Field1901,Field878=Field1902,Field879=Field1903,Field880=Field1904,Field881=Field1905,Field882=Field1906,Field883=Field1907,Field884=Field1908,Field885=Field1909,Field886=Field1910,Field887=Field1911,Field888=Field1912,Field889=Field1913,Field890=Field1914,Field891=Field1915,Field892=Field1916,Field893=Field1917,Field894=Field1918,Field895=Field1919,Field896=Field1920,Field897=Field1921,Field898=Field1922,Field899=Field1923,Field900=Field1924,Field901=Field1925,Field902=Field1926,Field903=Field1927,Field904=Field1928,Field905=Field1929,Field906=Field1930,Field907=Field1931,Field908=Field1932,Field909=Field1933,Field910=Field1934,Field911=Field1935,Field912=Field1936,Field913=Field1937,Field914=Field1938,Field915=Field1939,Field916=Field1940,Field917=Field1941,Field918=Field1942,Field919=Field1943,Field920=Field1944,Field921=Field1945,Field922=Field1946,Field923=Field1947,Field924=Field1948,Field925=Field1949,Field926=Field1950,Field927=Field1951,Field928=Field1952,Field929=Field1953,Field930=Field1954,Field931=Field1955,Field932=Field1956,Field933=Field1957,Field934=Field1958,Field935=Field1959,Field936=Field1960,Field937=Field1961,Field938=Field1962,Field939=Field1963,Field940=Field1964,Field941=Field1965,Field942=Field1966,Field943=Field1967,Field944=Field1968,Field945=Field1969,Field946=Field1970,Field947=Field1971,Field948=Field1972,Field949=Field1973,Field950=Field1974,Field951=Field1975,Field952=Field1976,Field953=Field1977,Field954=Field1978,Field955=Field1979,Field956=Field1980,Field957=Field1981,Field958=Field1982,Field959=Field1983,Field960=Field1984,Field961=Field1985,Field962=Field1986,Field963=Field1987,Field964=Field1988,Field965=Field1989,Field966=Field1990,Field967=Field1991,Field968=Field1992,Field969=Field1993,Field970=Field1994,Field971=Field1995,Field972=Field1996,Field973=Field1997,Field974=Field1998,Field975=Field1999,Field976=Field2000,Field977=Field2001,Field978=Field2002,Field979=Field2003,Field980=Field2004,Field981=Field2005,Field982=Field2006,Field983=Field2007,Field984=Field2008,Field985=Field2009,Field986=Field2010,Field987=Field2011,Field988=Field2012,Field989=Field2013,Field990=Field2014,Field991=Field2015,Field992=Field2016,Field993=Field2017,Field994=Field2018,Field995=Field2019,Field996=Field2020,Field997=Field2021,Field998=Field2022,Field999=Field2023,Field1000=Field2024,Field1001=Field2025,Field1002=Field2026,Field1003=Field2027,Field1004=Field2028,Field1005=Field2029,Field1006=Field2030,Field1007=Field2031,Field1008=Field2032,Field1009=Field2033,Field1010=Field2034,Field1011=Field2035,Field1012=Field2036,Field1013=Field2037,Field1014=Field2038,Field1015=Field2039,Field1016=Field2040,Field1017=Field2041,Field1018=Field2042,Field1019=Field2043,Field1020=Field2044,Field1021=Field2045,Field1022=Field2046)',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream;


CREATE TARGET log USING LOGWRITER
(
 name:log,
 filename:'LOGFILENAME'
)
INPUT FROM data_stream;


END APPLICATION IR;
deploy application IR;
start application IR;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ @RECOVERY@;
create source @SOURCE@ using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'dynamicdirectory.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO @STREAM@;

create Target @TARGET@ using ADLSGen2Writer(
    accountname:'',
	sastoken:'',
	filesystemname:'',
	filename:'',
	directory:'',
	uploadpolicy:'eventcount:5000'
)format using DSVFormatter (
 members: 'data'
)
input from @STREAM@;

end application @APPNAME@;

deploy application @APPNAME@;
start application @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @dsvsrc@ USING FileReader ( 
  directory: '', 
  wildcard: '', 
  positionbyeof: false ) 
PARSE USING DSVParser ()
OUTPUT TO @appname@OUT;

CREATE TARGET @prqttrgt@ USING FileWriter ( 
  filename: '', 
  directory: '', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING ParquetFormatter  ( 
  schemaFileName: '',
  compressiontype: 'GZIP',
  members:'data')
INPUT FROM @appname@OUT;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

create application ConsoleApplication;
create type someType(zip Int);
drop application ConsoleApplication;