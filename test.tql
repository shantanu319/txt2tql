CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING DataBaseReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

END APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_scnRange: 1000,
 _h_eoffDelay: 10,
 SupportPDB: false,
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

UNDEPLOY APPLICATION admin.BadDeployGroup;
DROP APPLICATION admin.BadDeployGroup cascade;

CREATE APPLICATION BadDeployGroup;

-- This sample application demonstrates how WebAction could be used
-- by a retail chain to generate real-time reports on products and
-- stores and to send alerts of unusual activity.


CREATE FLOW BadSourceFlow;

-- RetailDataSource is the primary data source for this application.
--
-- ParseOrderData discards the fields not needed by this application and puts the
-- data into the appropriate Java types.
--
-- ParseOrderData outputs to RetailOrders stream, the start point of the
-- RetailProductFlow and RetailStoreFlow flows.

CREATE SOURCE RetailDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'retaildata2M.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO Orders;

-- A stream's type must be declared before the stream, and a CQ's
-- output stream must be defined before the CQ. Hence type-stream-CQ
-- sequences like the following are very common.

-- output for ParseOrderData
CREATE TYPE OrderType(
  storeId      String,
  orderId      String,
  sku          String,
  orderAmount  double,
  dateTime     DateTime,
  hourValue    int,
  state        String,
  city         String,
  zip          String
);
CREATE STREAM RetailOrders Of OrderType;

CREATE CQ ParseOrderData
INSERT INTO RetailOrders
SELECT  data[0],
        data[6],
        data[7],
        TO_DOUBLE(SRIGHT(data[8],1)),
        TO_DATE(data[9],'yyyyMMddHHmmss'),
        DHOURS(TO_DATE(data[9],'yyyyMMddHHmmss')),
        data[3],
        data[2],
        data[4]
FROM Orders;

END FLOW BadSourceFlow;


CREATE FLOW BadProductFlow;

-- This flow populates the ProductActivity WAction store, which
-- provides data for dashboard reports on sales by product.

-- defines context for WAction store
CREATE TYPE ProductActivityContext(
  sku String  KEY,
  OrderCount int,
  SalesAmount double,
  StartTime DateTime
);

-- defines event types for Waction store
CREATE TYPE ProductTrackingType (
  sku String KEY,
  OrderCount int,
  SalesAmount double,
  StartTime DateTime
);

CREATE WACTIONSTORE ProductActivity
CONTEXT OF ProductActivityContext
EVENT TYPES ( ProductTrackingType )
PERSIST NONE USING ( ) ;

-- input for GetProductActivity
CREATE JUMPING WINDOW ProductData_15MIN
OVER RetailOrders
KEEP WITHIN 15 MINUTE ON dateTime
PARTITION BY sku;

CREATE STREAM ProductTrackingStream OF ProductTrackingType;

-- aggregates data and populates WAction store
CREATE CQ GetProductActivity
INSERT INTO ProductTrackingStream
SELECT pd.sku, COUNT(*), SUM(pd.orderAmount), FIRST(pd.dateTime)
FROM ProductData_15MIN pd
GROUP BY pd.sku;

CREATE CQ TrackProductActivity
INSERT INTO ProductActivity
SELECT sku, OrderCount, SalesAmount, StartTime
FROM ProductTrackingStream
LINK SOURCE EVENT;

END FLOW BadProductFlow;


CREATE FLOW BadStoreFlow;

-- This flow populates the StoreActivity WAction store, which provides
-- data for dashboard reports on reports on stores, and sends alerts when
-- sales volumes are higher or lower than expected.

-- RetailStoreFlow part 1 - GetStoreActivity
--
-- The RetailData_5MIN window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions. (Aggregate functions cannot
-- be used with unbound real-time data.)
--
-- The HourlyStoreSales_Cache cache provides historical averages for the
-- current hour for each merchant

-- input for GetStoreActivity
CREATE JUMPING WINDOW RetailData_5MIN
     OVER RetailOrders
     KEEP WITHIN 5 MINUTE ON dateTime
     PARTITION BY storeId;

-- input for GetStoreActivity
CREATE TYPE StoreHourlyAvg(
  storeId String,
  hourValue int,
  hourlyAvg int,
  hourlyItemCnt int
);
CREATE CACHE HourlyStoreSales_Cache using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'storehourlyData.txt',
  header: Yes,
  columndelimiter: ','
) QUERY (keytomap:'storeId') OF StoreHourlyAvg;

-- output for GetStoreActivity
CREATE TYPE StoreOrdersTrackingType (
  storeId String KEY,
  state String,
  city  String,
  zip   String,
  StartTime DateTime,
  ordersCount int,
  salesAmount double,
  hourlyAvg int,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM StoreOrdersTracking OF StoreOrdersTrackingType;

CREATE CQ GetStoreActivity
INSERT INTO StoreOrdersTracking
SELECT rd.storeId, rd.state, rd.city, rd.zip, first(rd.dateTime),
       COUNT(rd.storeId), SUM(rd.orderAmount), l.hourlyAvg/6,
       l.hourlyAvg/6 + l.hourlyAvg/8,
       l.hourlyAvg/6 - l.hourlyAvg/10,
       '<NOTSET>', '<NOTSET>'
FROM RetailData_5MIN rd, HourlyStoreSales_Cache l
WHERE rd.storeId = l.storeId AND rd.hourValue = l.hourValue
GROUP BY rd.storeId;
-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyStoreSales_Cache cache. The category and status fields
-- are left unset to be populated by the next query.


-- RetailStoreFlow part 2 - GetStoreStatus
--
-- This query sets the count values used by the dashboard map and the
-- status values used to trigger alerts.

-- uses type previously defined for StoreOrdersTracking
CREATE STREAM StoreOrdersTracking_Status OF StoreOrdersTrackingType;

CREATE CQ GetStoreStatus
INSERT INTO StoreOrdersTracking_Status
SELECT storeId, state, city, zip, StartTime,
       ordersCount, salesAmount, hourlyAvg, upperLimit, lowerLimit,
       CASE
         WHEN salesAmount > (upperLimit + 2000) THEN 'HOT'
         WHEN salesAmount > upperLimit THEN 'MEDIUM'
         WHEN salesAmount < lowerLimit THEN 'COLD'
         ELSE 'COOL' END,
       CASE
         WHEN salesAmount > upperLimit THEN 'TOOHIGH'
         WHEN salesAmount < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM StoreOrdersTracking;


-- RetailStoreFlow part 3 - create and populate the StoreActivity WAction store

-- input for CQ TrackStoreActivity
CREATE TYPE StoreNameData(
  storeId       String KEY,
  storeName     String
);
CREATE CACHE StoreNameLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'StoreNames.csv',
  header: Yes,
  columndelimiter: ','
) QUERY(keytomap:'storeId') OF StoreNameData;

-- input for CQ TrackStoreActivity
CREATE TYPE RetailUSAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);
CREATE CACHE ZipCodeLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  header: Yes,
  columndelimiter: '	'
) QUERY (keytomap:'zip') OF RetailUSAddressData;

-- defines WAction store context
CREATE TYPE StoreActivityContext(
  storeId String KEY,
  StartTime DateTime,
  StoreName String,
  Category String,
  Status String,
  OrderCount int,
  salesamount double,
  HourlyAvg int,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

-- StoreOrdersTrackingType previously defined for StoreOrdersTracking
CREATE WACTIONSTORE StoreActivity
CONTEXT OF StoreActivityContext
EVENT TYPES (StoreOrdersTrackingType )
PERSIST NONE USING ();

CREATE CQ TrackStoreActivity
INSERT INTO StoreActivity
SELECT s.storeId,
  s.StartTime,
  n.storeName,
  s.category,
  s.status,
  s.ordersCount,
  s.salesAmount,
  s.hourlyAvg,
  s.upperLimit,
  s.lowerLimit,
  z.zip,
  z.city,
  s.state,
  z.latVal,
  z.longVal
FROM StoreOrdersTracking_Status s, StoreNameLookup n, ZipCodeLookup z
WHERE s.storeId = n.storeId AND s.zip = z.zip
LINK SOURCE EVENT;


-- RetailStoreFlow part 4 - send alerts


CREATE STREAM RetailAlertStream OF Global.AlertEvent;

CREATE CQ RetailRetailGenerateAlerts
INSERT INTO RetailAlertStream
SELECT n.storeName, s.storeId,
        CASE
          WHEN s.Status = 'OK' THEN 'info'
          ELSE 'warning' END,
        CASE
          WHEN s.Status = 'OK' THEN 'cancel'
          ELSE 'raise' END,
        CASE
          WHEN s.Status = 'OK' THEN 'Store ' + n.storeName + ' amount of $'+ s.salesAmount + ' is back between $' + s.lowerLimit + ' and $' +s.upperLimit
          WHEN s.Status = 'TOOHIGH' THEN 'Store ' + n.storeName + ' amount of $'+ s.salesAmount + ' is above upper limit of $' + s.upperLimit
          WHEN s.Status = 'TOOLOW' THEN 'Store ' + n.storeName + ' amount of $'+ s.salesAmount + ' is below lower limit of $' + s.lowerLimit
          ELSE ''
          END
FROM StoreOrdersTracking_Status s, StoreNameLookup n
WHERE s.storeId = n.storeId;

END FLOW BadStoreFlow;


-- load dashboard visualization settings from file

--CREATE VISUALIZATION BadDeployGroup "Samples/Customer/RetailApp/RetailApp_visualization_settings.json";

-- The following statement defines the user and delivery method for alerts.
CREATE SUBSCRIPTION BadAlertSub USING WebAlertAdapter( ) INPUT FROM RetailAlertStream;


END APPLICATION BadDeployGroup;

--
-- Crash Recovery Test 5 with Jumping window and partitioned on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION N4S4CR5Tester.N4S4CRTest5;
UNDEPLOY APPLICATION N4S4CR5Tester.N4S4CRTest5;
DROP APPLICATION N4S4CR5Tester.N4S4CRTest5 CASCADE;
CREATE APPLICATION N4S4CRTest5 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest5;

CREATE SOURCE CsvSourceN4S4CRTest5 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest5;

CREATE FLOW DataProcessingN4S4CRTest5;

CREATE TYPE CsvDataN4S4CRTest5 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvDataN4S4CRTest5 PARTITION BY merchantId;

CREATE CQ CsvToDataN4S4CRTest5
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsN4S4CRTest5 CONTEXT OF CsvDataN4S4CRTest5
EVENT TYPES ( CsvDataN4S4CRTest5 )
@PERSIST-TYPE@

CREATE CQ DataToWactionN4S4CRTest5
INSERT INTO WactionsN4S4CRTest5
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN4S4CRTest5;

END APPLICATION N4S4CRTest5;

CREATE OR REPLACE APPLICATION @AppName@;

Create Source @AppName@_source Using OracleReader(
  Username:'@username@',
  Password:'@password@',
  ConnectionURL:'@url@',
  Tables:'@srctableName@',
  Fetchsize:1
)
Output To @AppName@_Stream;


CREATE OR REPLACE TARGET @AppName@_DB_Target USING Global.DeltaLakeWriter (
connectionProfileName: 'admin.@DBCP@',
   useConnectionProfile: 'true',
  Tables: '@srctableName@,@trgtableName@',
  uploadPolicy: 'eventcount:100000,interval:60s'
)

INPUT FROM @AppName@_Stream;
END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;

Create Source @SourceName@ Using OracleReader

(
  Username:'c##qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',
  Tables:'CDB$ROOT."C##QATEST".ojet_src;ORCLPDB.QATEST.ojet_src',
  _h_useClassic:false,
  Fetchsize:1,
  Compression: true,
  SupportPDB:true,
  ReplicationSlotName:'null'
)
Output To @SRCINPUTSTREAM@;

CREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'))) FROM @SRCINPUTSTREAM@ x; ;


CREATE TARGET @targetName@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',
  Username:'c##qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'CDB$ROOT."C##QATEST".ojet_src,CDB$ROOT."C##QATEST".ojet_tgt'
) INPUT FROM admin.sqlreader_cq_out;



create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
--drop exceptionstore admin.MSSQLServer_To_MSSQLServerApp_ExceptionStore;
drop application @APPNAME@ cascade;
create application @APPNAME@ use exceptionstore;


Create Source @SourceName@ Using MSSqlReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'@SourceTable@',
 ConnectionPoolSize:1,
 StartPosition:'EOF'
 ) Output To @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;


 CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@SourceTable@,@TargetTable@',
    CommitPolicy: 'Interval:5'

) INPUT FROM @SRCINPUTSTREAM@;

create or replace cq @cq@
insert into @finalstream@
select exceptionType,action,appName,entityType,entityName,className,message,relatedActivity from @APPNAME@_ExceptionStore;

Create target @targetfile@ using filewriter (
filename:'@APPNAME@_file.log',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000,sequence:00'
)
format using jsonFormatter()
input from @finalstream@;


end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop @APPNAME@_ExpStore;
undeploy application @APPNAME@_ExpStore;
drop application @APPNAME@_ExpStore cascade;
CREATE APPLICATION @APPNAME@_ExpStore;

CREATE TYPE @APPNAME@_ExpStore_CDCStreams_Type  (
  evtlist java.util.List
 );

CREATE STREAM @APPNAME@_ExpStore_CDCStreams OF @APPNAME@_ExpStore_CDCStreams_Type;

CREATE CQ @APPNAME@_ReadFromExpStore
INSERT INTO @APPNAME@_ExpStore_CDCStreams
select to_waevent(s.relatedObjects) as evtlist from admin.@APPNAME@_ExceptionStore [jumping 5 second] s;

CREATE STREAM @APPNAME@_ExpStore_CDCEventStream OF Global.WAEvent;

CREATE CQ @APPNAME@_ExpStore_GetCDCEvent
INSERT INTO @APPNAME@_ExpStore_CDCEventStream
SELECT com.webaction.proc.events.WAEvent.makecopy(cdcevent) FROM @APPNAME@_ExpStore_CDCStreams a, iterator(a.evtlist) cdcevent;

CREATE CQ @APPNAME@_ExpStore_JoinDataCQ
INSERT INTO @APPNAME@_ExpStore_JoinedDataStream
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1])
        from @APPNAME@_ExpStore_CDCEventStream f;

CREATE OR REPLACE TARGET @APPNAME@_ExpStore_WriteToFileAsJSON USING FileWriter  (
  filename: 'expEvent_MSSQL',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:1,interval:30',
  adapterName: 'FileWriter',
  directory: 'ExpStore_logs',
  rolloverpolicy: 'eventcount:6,interval:30s'
 )
FORMAT USING JSONFormatter  (
  handler: 'com.webaction.proc.JSONFormatter',
  formatterName: 'JSONFormatter',
  jsonMemberDelimiter: '\n',
  jsonobjectdelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true'
 )
INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;

CREATE TARGET @APPNAME@_ExpStore_dbtarget USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1,Interval:1',
CommitPolicy:'Interval:1,Eventcount:1',
Tables:'@TargetTable@'
) INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;

END APPLICATION @APPNAME@_ExpStore;

deploy application @APPNAME@_ExpStore;
start @APPNAME@_ExpStore;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter
  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1'
 )

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'posdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'TargetPosDataXmlFS',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'FileSizeRollingPolicy,filesize:101M,sequence:00'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:merchantid:text=merchantname'
)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlFS_actual.log') input from TypedCSVStream;

end application DSV;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.OJet (
  PrimaryDatabasePassword: '@PRIMARY_PASSWORD@',
  ConnectionURL: '@DOWNSTREAM_URL@',
  PrimaryDatabaseUsername: '@PRIMARY_USER@',
  Password: '@DOWNSTREAM_PASSWORD@',
  DownstreamCaptureMode: 'REAL_TIME',
  DownstreamCapture: true,
  PrimaryDatabaseConnectionURL: '@PRIMARY_URL@',
  Tables: '@SOURCE_TABLES@',
  Username: '@DOWNSTREAM_USER@' )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET_SYS@ USING Global.SysOut (
  name: 'Out' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter (
  ConnectionURL: '@TARGET_URL@',
  Username: '@TARGET_USER@',
  Password: '@TARGET_PASSWORD@',
  CheckPointTable: 'CHKPOINT',
  CommitPolicy: 'EventCount:1',
  Tables: '@TARGET_TABLES@',
  BatchPolicy: 'EventCount:1' )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

STOP QueryAggTester.ws_one;
UNDEPLOY APPLICATION QueryAggTester.ws_one;
DROP APPLICATION QueryAggTester.ws_one cascade;

CREATE APPLICATION ws_one;


CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;


CREATE TYPE wsData
(
bankID Integer KEY,
bankName String
);


CREATE STREAM wsStream OF wsData;

--Select data from QaStream and insert into wsStream

CREATE CQ csvTowsData
INSERT INTO wsStream
SELECT TO_INT(data[0]),data[1] FROM QaStream;

--create jumping window over data in wsStream

CREATE WACTIONSTORE oneWS
CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@

--get data from wsStream and place into wactionStore oneWS
CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT * FROM wsStream
LINK SOURCE EVENT;


END APPLICATION ws_one;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING PostgreSQLReader  (
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 )
OUTPUT TO @STREAM@ ;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING MSSqlReader
(
  Compression: false,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.test01',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
)
OUTPUT TO @APPNAME@_SS;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'NULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true	,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

--
-- Crash Recovery Test 4 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS
-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS
--

STOP APPLICATION N2S2CR4Tester.N2S2CRTest4;
UNDEPLOY APPLICATION N2S2CR4Tester.N2S2CRTest4;
DROP APPLICATION N2S2CR4Tester.N2S2CRTest4 CASCADE;
CREATE APPLICATION N2S2CRTest4 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest4;

CREATE SOURCE CsvSourceN2S2CRTest4 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN2S2CRTest4;

CREATE FLOW DataProcessingN2S2CRTest4;

CREATE TYPE CsvDataN2S2CRTest4 (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionTypeN2S2CRTest4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream OF CsvDataN2S2CRTest4;

CREATE CQ CsvToDataN2S2CRTest4
INSERT INTO DataStream
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN2S2CRTest4 CONTEXT OF WactionTypeN2S2CRTest4
EVENT TYPES ( CsvDataN2S2CRTest4 )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO WactionsN2S2CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO WactionsN2S2CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END FLOW DataProcessingN2S2CRTest4;

END APPLICATION N2S2CRTest4;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@ recovery 5 SECOND Interval;
CREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.BQ1',
  Username: 'qatest',
  Password: 'qatest'
) OUTPUT TO @APP_NAME@_Stream;


CREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:30s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

--
-- Recovery Test 20 with two sources going to one wactionstore
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> CQ1 -> WS
-- S2 -> CQ2 -> WS
--

STOP Recov20Tester.RecovTest20;
UNDEPLOY APPLICATION Recov20Tester.RecovTest20;
DROP APPLICATION Recov20Tester.RecovTest20 CASCADE;
CREATE APPLICATION RecovTest20 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions1
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ InsertWactions2
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream2;

END APPLICATION RecovTest20;

STOP APPLICATION @appname@routerApp;
UNDEPLOY APPLICATION @appname@routerApp;
DROP APPLICATION @appname@routerApp CASCADE;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',
  acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

CREATE APPLICATION @appname@routerApp RECOVERY 10 SECOND INTERVAL;

CREATE  SOURCE @appname@OraSource USING OracleReader  (
Username: 'qatest',
Password: 'qatest',
ConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',
Tables: 'QATEST.TGT_T%',
 FetchSize:'100'
)
OUTPUT TO @appname@MasterStream1;

-- CREATE STREAM @appname@ss1 OF Global.waevent persist using Global.DefaultKafkaProperties;
-- CREATE STREAM @appname@ss2 OF Global.waevent persist using Global.DefaultKafkaProperties;
-- CREATE STREAM @appname@ss3 OF Global.waevent persist using Global.DefaultKafkaProperties;

CREATE STREAM @appname@ss1 OF Global.waevent PERSIST USING KafkaPropset;
CREATE STREAM @appname@ss2 OF Global.waevent PERSIST USING KafkaPropset;
CREATE STREAM @appname@ss3 OF Global.waevent PERSIST USING KafkaPropset;

CREATE OR REPLACE ROUTER @appname@tablerouter1 INPUT FROM @appname@MasterStream1 s CASE
WHEN meta(s,"TableName").toString()='QATEST.TGT_T1' THEN ROUTE TO @appname@ss1,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T2' THEN ROUTE TO @appname@ss2,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T3' THEN ROUTE TO @appname@ss3,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T4' THEN ROUTE TO @appname@ss4,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T5' THEN ROUTE TO @appname@ss5,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T6' THEN ROUTE TO @appname@ss6,
ELSE ROUTE TO @appname@ss_else;

create Target @appname@FileTarget_1 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'
)
FORMAT USING dsvFormatter ()
input from @appname@ss1;

create Target @appname@FileTarget_2 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'
)
FORMAT USING dsvFormatter ()
input from @appname@ss2;

create Target @appname@FileTarget_3 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from @appname@ss3;

CREATE OR REPLACE TARGET @appname@KafkaTarget_4 USING KafkaWriter VERSION '0.11.0' (
  brokerAddress: 'localhost:9092',
  Topic: 'target4'
 )
FORMAT USING JSONFormatter  (
 )
INPUT FROM @appname@ss4;

CREATE OR REPLACE TARGET @appname@KafkaTarget_5 USING KafkaWriter VERSION '0.11.0' (
  brokerAddress: 'localhost:9092',
  Topic: 'target5'
 )
FORMAT USING JSONFormatter  (
 )
INPUT FROM @appname@ss5;

CREATE OR REPLACE TARGET @appname@KafkaTarget_6 USING KafkaWriter VERSION '0.11.0' (
  brokerAddress: 'localhost:9092',
  Topic: 'target6'
 )
FORMAT USING JSONFormatter  (
 )
INPUT FROM @appname@ss6;




end application @appname@routerApp;
deploy application @appname@routerApp;
start @appname@routerApp;

stop application APP_HEARTBEATS;
undeploy application APP_HEARTBEATS;
drop application APP_HEARTBEATS cascade;

CREATE APPLICATION APP_HEARTBEATS;

CREATE OR REPLACE CQ CQ_HB_90SEC 
INSERT INTO STREAM_CQ_HB_90SEC 
select makeWAEvent(dnow()) as dummy from heartbeat(interval 90 second) h;

CREATE OR REPLACE CQ CQ_HB_10SEC 
INSERT INTO STREAM_CQ_HB_10SEC 
select makeWAEvent(dnow()) as dummy from heartbeat(interval 10 second) h;

CREATE OR REPLACE CQ CQ_HB_1MIN 
INSERT INTO STREAM_CQ_HB_1MIN 
select makeWAEvent(dnow()) as dummy from heartbeat(interval 1 minute) h;

CREATE OR REPLACE CQ CQ_HB_30SEC 
INSERT INTO STREAM_CQ_HB_30SEC 
select makeWAEvent(dnow()) as dummy from heartbeat(interval 30 second) h;

CREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_90SEC OVER STREAM_CQ_HB_90SEC 
KEEP 1 ROWS;

CREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_10SEC OVER STREAM_CQ_HB_10SEC 
KEEP 1 ROWS;

CREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_1MIN OVER STREAM_CQ_HB_1MIN 
KEEP 1 ROWS;

CREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_30SEC OVER STREAM_CQ_HB_30SEC 
KEEP 1 ROWS;

END APPLICATION APP_HEARTBEATS;

stop application OneAgentWithMultiTester.AgentWithMultiReader;
undeploy application OneAgentWithMultiTester.AgentWithMultiReader;
drop application OneAgentWithMultiTester.AgentWithMultiReader cascade;

create application AgentWithMultiReader;


CREATE FLOW AgentFlow;

create source XMLSource using FileReader (
  Directory:'@TEST-DATA-PATH@',
  WildCard:'books.xml',
  positionByEOF:false
)
parse using XMLParser (
  RootNode:'/catalog/book'
)
OUTPUT TO XmlStream;

create source DSVCSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'customerdetails-agent.csv',
  charset: 'UTF-8',
  positionByEOF:false
)
parse using DSVParser (
  header:'no'
)
OUTPUT TO DSVCsvStream;

-- Read from File

create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'StoreNames.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CSVStream;

END FLOW AgentFlow;

--  Rest of the Stream and CQs are executed in Server flow

CREATE FLOW ServerFlow;

CREATE TARGET myout using LogWriter(name: XMLSource, filename:'@FEATURE-DIR@/logs/logXML.txt') input from XmlStream;
CREATE TARGET myout1 using LogWriter(name: DSVSource, filename:'@FEATURE-DIR@/logs/logDSV.txt', charset:'UTF-8') input from DSVCsvStream;
CREATE TARGET myout2 using LogWriter(name: CSVSource, filename:'@FEATURE-DIR@/logs/logCSV.txt') input from CSVStream;


END FLOW ServerFlow;

end application AgentWithMultiReader;
DEPLOY APPLICATION AgentWithMultiReader with AgentFlow in AGENTS, ServerFlow on any in default;

start AgentWithMultiReader;

CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'data.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO @AppName@_rawstream;

CREATE OR REPLACE STREAM @BuiltinFunc@_Stream OF Global.WAEVent;
CREATE OR REPLACE STREAM CombineStream OF Global.WAEVent;

CREATE OR REPLACE CQ cq1
INSERT INTO @BuiltinFunc@_Stream
SELECT
@BuiltinFunc@(s1, 'city',data[5])
FROM @AppName@_rawstream s1;

CREATE OR REPLACE CQ cq2
INSERT INTO CombineStream
Select *
FROM @BuiltinFunc@_Stream s4;

CREATE OR REPLACE CQ cq3
INSERT INTO CombineStream
select *
FROM @AppName@_rawstream s5;

CREATE OR REPLACE TARGET @AppName@_FileTarget USING Global.FileWriter ( 
  flushpolicy: 'EventCount:10000,Interval:30s', 
  adapterName: 'FileWriter', 
  directory: '@logs@',
  filename: '@BuiltinFunc@_Data', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING Global.JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter', 
  jsonMemberDelimiter: '\n', 
  EventsAsArrayOfJsonObjects: 'true', 
  formatterName: 'JSONFormatter', 
  jsonobjectdelimiter: '\n' ) 
INPUT FROM CombineStream;

End application @AppName@;
Deploy application @AppName@; 
Start application @AppName@;

stop application CDCTester.CDCTest;
undeploy application CDCTester.CDCTest;
drop application CDCTester.CDCTest cascade;

create application CDCTest;

Create Source Rac11g Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:$Tables,
 FetchSize:$FetchSize,
 QueueSize:$QueueSize

)
Output To LCRStream;


end application CDCTest;
deploy application CDCTest;

STOP UpdatableCacher.UpdatableCache;
UNDEPLOY APPLICATION UpdatableCacher.UpdatableCache;
DROP APPLICATION UpdatableCacher.UpdatableCache CASCADE;
CREATE APPLICATION UpdatableCacher.UpdatableCache;

CREATE TYPE MerchantHourlyAve(
  merchantId String KEY,
  hourlyAve Integer,
  theDate DateTime,
  dVal Double
);


CREATE source CsvDataSource USING FileReader (
      directory:'@TEST-DATA-PATH@',
      columndelimiter: ',',
      wildcard:'ucData.csv',
      blocksize: 10240,
      positionByEOF:false
)
PARSE USING DSVParser (
      header:No,
      trimquote:false
) OUTPUT TO CsvStream;


CREATE STREAM S1 OF MerchantHourlyAve;

CREATE CQ cq1
	insert into S1
		SELECT data[0],
				TO_INT(data[1]),
				TO_DATE(data[2]),
				TO_DOUBLE(data[3])
		FROM CsvStream;


CREATE EVENTTABLE ET1 using STREAM (
  NAME: 'S1'
) QUERY (keytomap:'dVal', persistPolicy: 'true' ) OF MerchantHourlyAve;


CREATE EVENTTABLE ET2 using STREAM (
  NAME: 'S1'
) QUERY (keytomap:'merchantId' ) OF MerchantHourlyAve;



END APPLICATION UpdatableCache;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
filename:'Events',
directory:'@FEATURE-DIR@/logs/',
rolloverpolicy:'eventcount:200,sequence:00'
)
format using DSVFormatter (

)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetEventCount_actual.log') input from TypedCSVStream;

end application DSV;

stop application @APPNAME@app3;
undeploy application @APPNAME@app3;
alter application @APPNAME@app3;
CREATE or replace TARGET @APPNAME@app3_targetNew USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS3_Alter'
) INPUT FROM @APPNAME@sourcestream;
alter application @APPNAME@app3 recompile;
deploy application @APPNAME@app3;

--
-- Recovery Test 1
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov1Tester.RecovTest1;
UNDEPLOY APPLICATION Recov1Tester.RecovTest1;
DROP APPLICATION Recov1Tester.RecovTest1 CASCADE;
CREATE APPLICATION RecovTest1 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END APPLICATION RecovTest1;

import com.webaction.proc.events.StringArrayEvent;
import com.webaction.proc.events.WAEvent;

STOP FunctionTester.BuiltInFunctionApp;
UNDEPLOY APPLICATION FunctionTester.BuiltInFunctionApp;
DROP APPLICATION FunctionTester.BuiltInFunctionApp cascade;

CREATE APPLICATION BuiltInFunctionApp;


CREATE source rawSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'csvdata.txt',
  positionByEOF:false
)
PARSE USING DSVParser
(
   header:False,
   columndelimiter:','
) OUTPUT TO rawStream;


CREATE TYPE rawDataType(
  level String,
  floatnumber float,
  doublenumber double,
  intvalue  int,
  ourEvent  WAEvent
);


CREATE STREAM testStream OF rawDataType ;


-- Create a WAEvent out of the first to items, using makeWAEvent makeWAEvent(data[0], data[1])
CREATE CQ testCQ
  INSERT INTO testStream
  SELECT data[0], ROUND_FLOAT(data[1],4), ROUND_DOUBLE(data[2],0), TO_INT(data[3]), makeWAEvent(data[0], data[1])
FROM rawStream;

CREATE OR REPLACE Target BuiltFuncTarget using LogWriter(name:BuiltFunc,filename:'@FEATURE-DIR@/logs/BuiltInFuncResults') input from testStream;


CREATE TYPE waEventDataType
(
  level String,
  floatnumber float
);

CREATE STREAM waEventStream OF waEventDataType;

-- Parse the WAEvent created by the CQ testCQ and store the values
CREATE CQ otherCQ
  INSERT INTO waEventStream
  SELECT TO_STRING(VALUE(ourEvent,0)), ROUND_FLOAT(VALUE(ourEvent,1),4)
FROM testStream;


CREATE OR REPLACE Target BuiltFuncWAEventTarget using LogWriter(name:BuiltFunc,filename:'@FEATURE-DIR@/logs/BuiltInFuncResultsWAEvent') input from waEventStream;



CREATE TYPE stringArrayEventDataType
(
  level String,
  floatnumber float,
  doublenumber double,
  intvalue  int,
  stringArrayEvent StringArrayEvent
);

CREATE STREAM stringArrayEventStream OF stringArrayEventDataType;

-- Create a StringArrayEvent from the raw stream
-- We can't use the target below to verify, as it's generating a timestamp field, based on system generated time
CREATE CQ stringArrayEventCQ
  INSERT INTO stringArrayEventStream
  SELECT data[0], ROUND_FLOAT(data[1],4), ROUND_DOUBLE(data[2],0), TO_INT(data[3]), makeStringArrayEvent(data[0], data[2], data[3])
FROM rawStream;


CREATE OR REPLACE Target BuiltFuncSimpleEventTarget using LogWriter(name:BuiltFunc,filename:'@FEATURE-DIR@/logs/BuiltInFuncResultsStringArrayEvent') input from stringArrayEventStream;


END APPLICATION BuiltInFunctionApp;

use global;

-- undeploy application T10;
drop application T10 cascade;

create application T10
RECOVERY 5 SECOND INTERVAL;

CREATE FLOW AgentFlow;
create source T10Source using CSVReader (
  directory:'Samples/AppData',
  header:Yes,
  wildcard:'customerdetails-recovery.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO T10Stream;
END FLOW AgentFlow;

CREATE FLOW ServerFlow;
create Target t using CSVWriter(fileName:AgentOut) input from T10Stream;
END FLOW ServerFlow;

end application T10;

DEPLOY APPLICATION T10 with AgentFlow in AGENTS, ServerFlow in SERVERS;

stop application JMSWriter.JMS;
undeploy application JMSWriter.JMS;
drop application JMSWriter.JMS cascade;

create application JMS;
create source JMSCSVSource using FileReader (
        directory:'@TEST-DATA-PATH@',
        WildCard:'AdhocQueryData2.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target JmsTarget  using JMSWriter (
		Provider:'@PROVIDER@',
		Ctx:'@CONTEXT@',
		messagetype: @MESSAGETYPE@,
		UserName:'@USERNAME@',
		Password:'@PASSWORD@',
		Queuename:'dynamicQueues/Test.bar')
format using dsvformatter (
)
input from TypedCSVStream;

end Application Jms;
deploy application jms;

start jms;

CREATE APPLICATION KafkaReader;

CREATE OR REPLACE TYPE KafkaSourceStr2_Type  ( seq java.lang.Integer
 );

CREATE OR REPLACE STREAM KafkaSourceStr2 OF KafkaSourceStr2_Type;

CREATE  JUMPING WINDOW GetTargData OVER KafkaSourceStr2 KEEP 1000000 ROWS;

CREATE OR REPLACE SOURCE KafkaSource USING KafkaReader VERSION '0.11.0' (
  KafkaConfigPropertySeparator: ';',
  startOffset: 0,
  adapterName: 'KafkaReader',
  Topic: 'kafkaTopic7',
  AutoMapPartition: true,
  brokerAddress: 'localhost:9092',
  KafkaConfigValueSeparator: '=',
  KafkaConfig: 'max.partition.fetch.bytes=10485760;fetch.min.bytes=1048576;fetch.max.wait.ms=1000;receive.buffer.bytes=2000000;poll.timeout.ms=10000;request.timeout.ms=60001;session.timeout.ms=60000'
 )
 PARSE USING DSVParser  (
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: 0,
  nocolumndelimiter: false,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: false,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 )
OUTPUT TO KafkaSourceStr1 ;

CREATE OR REPLACE CQ GetKafkaDataQuery
INSERT INTO KafkaSourceStr2
SELECT TO_INT(data[1]) as seq
FROM KafkaSourceStr1;

CREATE  TYPE KafkaSourceStr3_Type  ( SUMKafkaSourceStr2seq java.lang.Long
 );

CREATE STREAM KafkaSourceStr3 OF KafkaSourceStr3_Type;

CREATE OR REPLACE CQ GetTheSum
INSERT INTO KafkaSourceStr3
SELECT SUM(GetTargData .seq)
FROM GetTargData;

CREATE OR REPLACE TARGET KafkaFile USING FileWriter  (
  filename: 'TargetResults',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:10000,interval:30',
  adapterName: 'FileWriter',
  directory: '@FEATURE-DIR@/logs',
  rolloverpolicy: 'eventcount:10000,interval:30s'
 )
FORMAT USING DSVFormatter  (   nullvalue: 'NULL',
  standard: 'none',
  handler: 'com.webaction.proc.DSVFormatter',
  formatterName: 'DSVFormatter',
  usequotes: 'false',
  rowdelimiter: '\n',
  quotecharacter: '\"',
  header: 'false',
  columndelimiter: ','
 )
INPUT FROM KafkaSourceStr3;

END APPLICATION KafkaReader;

stop application APP_KAFKA_DATASOURCES;
undeploy application APP_KAFKA_DATASOURCES;
alter application APP_KAFKA_DATASOURCES;

CREATE OR REPLACE SOURCE SRC_FR_KAFKA_HOURLYTOTALS USING Global.FileReader (
  adapterName: 'FileReader',
  rolloverstyle: 'Default',
  blocksize: 64,
  skipbom: true,
  wildcard: 'kafka_hourly_total_20210316.txt',
  directory: '@confDir@',
  includesubdirectories: false,
  positionbyeof: false ) 
PARSE USING Global.DSVParser (
  trimwhitespace: false,
  linenumber: '-1',
  columndelimittill: '-1',
  trimquote: true,
  ignoreemptycolumn: false,
  parserName: 'DSVParser',
  quoteset: '\"',
  handler: 'com.webaction.proc.DSVParser_1_0',
  charset: 'UTF-8',
  columndelimiter: ':',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  separator: ',',
  header: false,
  blockascompleterecord: false,
  rowdelimiter: '\n',
  nocolumndelimiter: false,
  headerlineno: 0 )
OUTPUT TO STREAM_SRC_FR_KAFKA_HOURLYTOTALS;

alter application APP_KAFKA_DATASOURCES recompile;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;


CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using MysqlReader
(
   adapterName: MysqlReader,
   CDDLAction: Process,
   CDDLCapture: false,
   Compression: false,
   ConnectionURL: jdbc:mysql://localhost:3306/waction,
   FilterTransactionBoundaries: true,
   Password: ReaderPassword,
   SendBeforeImage: true,
   Tables: waction.MultiMultiDownstream_src,
   Username: ReaderUsername
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;

CREATE APPLICATION @APPNAME3@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName1@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME3@;
DEPLOY APPLICATION @APPNAME3@;
START APPLICATION @APPNAME3@;

CREATE APPLICATION @AppName@;

CREATE OR REPLACE SOURCE initialLoad_Src USING Global.DatabaseReader (
  QuiesceOnILCompletion: false,
  Tables: '@SrcTableName@',
  adapterName: 'DatabaseReader',
  Password: '@Password@',
  Username: '@UserName@',
  ConnectionURL: '@Srcurl@',
   FetchSize: 10000)
OUTPUT TO CommonRawStream;

Create Source OrcReader_Src Using OracleReader(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@Srcurl@',
 Tables:'@SrcTableName@',
 Fetchsize:100)
Output To CommonRawStream;

CREATE OR REPLACE TARGET Postgres_Trg USING Global.DatabaseWriter (
  ConnectionURL: '@trgUrl@',
  Username: '@trgUsrName@',
  Tables: '@SrcTableName@,@trgTable@',
  Password: '@trgPswd@',
  CommitPolicy: 'EventCount:10000,Interval:60',
  adapterName: 'DatabaseWriter' )
INPUT FROM CommonRawStream;

CREATE TARGET filewriter_tgt USING Global.FileWriter (
 directory:'@trgDir@',
  filename: '@fileName@',
  flushpolicy: 'EventCount:10000,Interval:30s',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n' )
INPUT FROM CommonRawStream;

CREATE OR REPLACE TARGET BigQuery_Target USING Global.BigQueryWriter (
  streamingUpload: 'false',
  projectId: '@projectID@',
  Tables: '@SrcTableName@,@BQTableName@',
  optimizedMerge: 'false',
  ServiceAccountKey: '@ServiceAccountKey@',
  BatchPolicy: 'EventCount:1000000,Interval:90',
  Mode: 'APPENDONLY' )
INPUT from CommonRawStream;

END APPLICATION @AppName@;

undeploy application GCSWriterTest;
alter application GCSWriterTest;

CREATE OR REPLACE SOURCE OracleSource USING OracleReader  (
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: '@SOURCE_TABLES@',
  FetchSize: 1
 ) Output To OracleStream;

 create or replace Target OracleGCSTarget using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@',
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from OracleStream;
end application GCSWriterTest;
alter application GCSWriterTest recompile;
deploy application GCSWriterTest;
start application GCSWriterTest;

stop application iteratortester.iteratorapp;
undeploy application iteratortester.iteratorapp;
drop application iteratortester.iteratorapp cascade;

CREATE APPLICATION iteratorapp;

create flow sourceFlow;

CREATE SOURCE JSONAccessLogSource USING FileReader(
  directory:'@TEST-DATA-PATH@',
  wildcard:'iterator2.json'
)
parse using JSONParser (
) OUTPUT TO jsonSourceStream;

end flow sourceflow;

-- ******ARRAY LIST****** --
create flow processFlow;

create type cacheType (bankID string key, bankName string);
CREATE cache dsvcache USING FileReader (
directory:'@TEST-DATA-PATH@',
wildcard:'banks.csv',
blocksize: 10240,
positionByEOF:false
)
PARSE USING DSVParser (
header:No,
trimquote:false
) QUERY (keytomap:'bankID') OF cacheType;

CREATE TYPE listType (id integer KEY, bankname string, lst java.util.List);
CREATE TYPE listStoreType (id integer KEY, bankname string, lst java.util.List, lstoflst java.util.List);

CREATE STREAM listStream of listType partition by bankname;

CREATE JUMPING WINDOW listJWindow
OVER listStream
keep 3 rows;

CREATE WINDOW listWindow
OVER listStream
keep 3 rows;

CREATE WACTIONSTORE listStore CONTEXT OF listStoreType EVENT TYPES (listStoreType ) 
@PERSIST-TYPE@

create cq updatelistStream
insert into listStream
select TO_INT(bankID), bankName, makelist(bankID,bankName) as lst 
from dsvcache;

create cq updatelistStore 
insert into listStore
select ID, bankName, lst, makelist(lst,lst) from listStream
LINK SOURCE EVENT;

create stream listTargetStream( str String);

create cq updateListTarget
insert into listTargetStream
select itr
from listStream, iterator(listStream.lst) itr order by cast(itr as java.lang.Comparable);

-- CREATE TARGET listout USING SYSOUT(name:"list") input from listStream;

-- ******JsonNode****** --

--CREATE TYPE jsonType (id integer KEY,  lst com.fasterxml.jackson.databind.JsonNode);
CREATE TYPE jsonType (int integer, bankname string, lst com.fasterxml.jackson.databind.JsonNode key);

CREATE STREAM jsonStream of jsonType partition by bankname;

CREATE JUMPING WINDOW jsonJWindow
OVER jsonStream
keep 3 rows
partition by bankname;

CREATE WINDOW jsonWindow
OVER jsonStream
keep 3 rows;

CREATE WACTIONSTORE jsonStore CONTEXT OF jsonType EVENT TYPES (jsonType ) 
@PERSIST-TYPE@

create cq updateJsonStream
insert into jsonStream
--select TO_INT(MATCH(data[0], '.*\\\\s([0-9]+)')) , makeJSON('[{"x":"a"},{"x":"b","y":"c"},{"y":"c"}]') as lst 
select TO_INT(y.bankID), y.bankName, x.data 
from JSONSourceStream x, dsvcache y;

create cq updateJsonStore 
insert into jsonStore
select * from jsonStream
LINK SOURCE EVENT;

create stream jsonTargetStream( str String);

create cq updateJsonTarget
insert into jsonTargetStream
select to_string(itr.StringJSON)
from jsonStream, iterator(jsonStream.lst) itr order by to_string(itr.StringJSON);
CREATE TARGET jsonout USING SYSOUT(name:"jlist") input from jsonStream;

end flow processflow; 

end application iteratorapp;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;

 CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01;striim.test02;striim.test03',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',
 startPosition: 'striim.test01=1;striim.test02=-1;%=0',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream ;

  CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

create target AzureSQLDWHTarget using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream;


CREATE TARGET log USING LOGWRITER
(
 name:log,
 filename:'LOGFILENAME'
)
INPUT FROM data_stream;


END APPLICATION IR;

deploy application IR;
start IR;

CREATE APPLICATION SourceFraudApp;

CREATE SOURCE FraudCsvDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:no,
  wildcard:'fraudPosData.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO FraudCsvStream;

CREATE TARGET FraudSourceDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceFraudAppData') input from FraudCsvStream;

CREATE SOURCE FraudZipLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  header: no,
  columndelimiter: '	',
  positionByEOF:false
) OUTPUT TO FraudCacheSource1;

CREATE TARGET FraudCacheDump1 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceFraudCacheData1') input from FraudCacheSource1;

CREATE SOURCE FraudNameLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'MerchantNames.csv',
  header: no,
  columndelimiter: ',',
  positionByEOF:false
) OUTPUT TO FraudCacheSource2;

CREATE TARGET FraudCacheDump2 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceFraudCacheData2') input from FraudCacheSource2;

CREATE SOURCE FraudCustomerLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'customerdetails.csv',
  header: no,
  columndelimiter: ',',
  positionByEOF:false
) OUTPUT TO FraudCacheSource3;

CREATE TARGET FraudCacheDump3 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceFraudCacheData3') input from FraudCacheSource3;


END APPLICATION SourceFraudApp;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.e1ptest%',
	FetchSize: '1'
)
OUTPUT TO @APPNAME@_SS;


CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.e1ptest%,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12)',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'Interval:2',
StandardSQL:true,
optimizedMerge:true		
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

create application HPTippingLog;
create source DHCPLogSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'hp*',
	charset:'UTF-8',
	positionByEOF:false 
) PARSE USING HPTippingPointLogParser (
) OUTPUT TO HPLogStream;
create Target HPDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/hp_log') input from HPLogStream;
end application HPTippingLog;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE @SOURCE@ USING MSSQLReader  ( 
  FilterTransactionBoundaries: true,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Username: '@SOURCE_USER@',
  DatabaseName: 'qatest',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@ USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;


END APPLICATION @APPNAME@;

deploy application @APPNAME@ on ANY in default;

start application @APPNAME@;

STOP APPLICATION @AppName@_App1;
UNDEPLOY APPLICATION @AppName@_App1;
DROP APPLICATION @AppName@_App1 CASCADE;
CREATE APPLICATION @AppName@_App1 recovery 1 second interval;


CREATE SOURCE @AppName@_FileSource USING FileReader (
  directory:'@TestdataDir@',
    WildCard:'banks*',
  positionByEOF:false
  )
PARSE USING DSVParser (
  header:no
)OUTPUT TO FileStream;


CREATE OR REPLACE ROUTER filerouter1 INPUT FROM FileStream s CASE
WHEN meta(s,"FileName").toString()='banks1.csv' THEN ROUTE TO stream1,
WHEN meta(s,"FileName").toString()='banks2.csv' THEN ROUTE TO stream2,
WHEN meta(s,"FileName").toString()='banks3.csv' THEN ROUTE TO stream3,
WHEN meta(s,"FileName").toString()='banks4.csv' THEN ROUTE TO stream4,
ELSE ROUTE TO ss_else;

CREATE TYPE banks1(
  id int,
  name String ,
Filename String
);

Create stream cdctypestream1 of banks1;

CREATE CQ cdcstreamcq1
INSERT INTO cdctypestream1
SELECT TO_INT(p.data[0]), 
       TO_STRING(p.data[1]), TO_STRING(META(p,'FileName'))
FROM stream1 p;


CREATE OR REPLACE TARGET @AppName@_DataBaset1 USING DatabaseWriter  ( 
ConnectionURL:'@url@',
Username:'@userName@',
Password:'@password@',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.NEWBANKS1') 
INPUT FROM cdctypestream1;

End application @AppName@_App1;
Deploy application @AppName@_App1;
start application @AppName@_App1;

stop application @AppName@_App2;
undeploy application @AppName@_App2;
drop application @AppName@_App2 CASCADE;
create application @AppName@_App2 recovery 1 second interval;

CREATE OR REPLACE TARGET @AppName@_DataBaset2 USING DatabaseWriter  ( 
ConnectionURL:'@url@',
Username:'@userName@',
Password:'@password@',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.BANKS1') 
INPUT FROM cdctypestream1;

end application @AppName@_App2;
deploy application @AppName@_App2;
start application @AppName@_App2;

--
-- Crash Recovery Test 4 on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS
-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS
--

STOP APPLICATION N4S4CR4Tester.N4S4CRTest4;
UNDEPLOY APPLICATION N4S4CR4Tester.N4S4CRTest4;
DROP APPLICATION N4S4CR4Tester.N4S4CRTest4 CASCADE;
CREATE APPLICATION N4S4CRTest4 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest4;

CREATE SOURCE CsvSourceN4S4CRTest4 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest4;

CREATE FLOW DataProcessingN4S4CRTest4;

CREATE TYPE CsvDataN4S4CRTest4 (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionDataN4S4CRTest4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream OF CsvDataN4S4CRTest4;

CREATE CQ CsvToDataN4S4CRTest4
INSERT INTO DataStream
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN4S4CRTest4 CONTEXT OF WactionDataN4S4CRTest4
EVENT TYPES ( CsvDataN4S4CRTest4 )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO WactionsN4S4CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO WactionsN4S4CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END FLOW DataProcessingN4S4CRTest4;

END APPLICATION N4S4CRTest4;

stop application app2;
undeploy application app2;
alter application app2;
CREATE or replace TARGET app2_targetNew USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS1_Alter'
) INPUT FROM sourcestream;
alter application app2 recompile;
deploy application app2;

stop application app3;
undeploy application app3;
alter application app3;

CREATE or replace TARGET app3_targetNew USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test02,QATEST.KPS2_Alter'
) INPUT FROM sourcestream;
alter application app3 recompile;
deploy application app3;


stop application app4;
undeploy application app4;
alter application app4;
CREATE TARGET app4_targetNew USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test03,QATEST.KPS3_Alter'
) INPUT FROM sourcestream;
alter application app4 recompile;
deploy application app4;


stop application app5;
undeploy application app5;
alter application app5;
CREATE or replace TARGET app5_target1_New USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'snappy_Alter',
KafkaConfig:'compression.type=snappy'
) 
FORMAT USING DSVFormatter ()
INPUT FROM kps_typedStream;

CREATE or replace TARGET app5_target2_New USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'gzip_Alter',
KafkaConfig:'compression.type=gzip'
) 
FORMAT USING DSVFormatter ()
INPUT FROM sourcestream;

CREATE or replace TARGET app5_target3_New USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'lz4_Alter',
KafkaConfig:'compression.type=lz4'
) 
FORMAT USING DSVFormatter ()
INPUT FROM sourcestream;

alter application app5 recompile;
deploy application app5;

CREATE OR REPLACE APPLICATION @AppName@;

Create Source @AppName@_source Using OracleReader(
  Username:'@username@',
  Password:'@password@',
  ConnectionURL:'@url@',
  Tables:'@srctableName@',
  Fetchsize:1
)
Output To @AppName@_Stream;

CREATE OR REPLACE TARGET @AppName@_DB_Target USING Global.DeltaLakeWriter (
connectionProfileName: 'admin.@DBCP@',
   useConnectionProfile: 'true',
  Tables: '@srctableName@,@trgtableName@',
  uploadPolicy: 'eventcount:100000,interval:60s'
)

INPUT FROM @AppName@_Stream;

CREATE OR REPLACE TARGET @AppName@_DB_Target2 USING Global.DeltaLakeWriter (
connectionProfileName: 'admin.@DBCP@',
   useConnectionProfile: 'true',
  Tables: '@srctableName@,@trgtableName@',
  uploadPolicy: 'eventcount:100000,interval:60s'
)

INPUT FROM @AppName@_Stream;

END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

create application KinesisTest RECOVERY 1 SECOND INTERVAL;
CREATE OR REPLACE SOURCE ora_reader USING OracleReader (
  Username: 'miner',
  Password: 'miner',
  ConnectionURL: '192.168.1.113:1521:ORCL',
  TABLES: 'QATEST.H_REGION;QATEST.H_NATION;QATEST.H_CUSTOMER',
  FetchSize: '1'
 )
OUTPUT TO DDLCDCStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	accesskeyid:'ACCESS_KEY',
	secretaccesskey:'SECRET_KEY'
)
format using JSONFormatter (
)
input from DDLCDCStream;

create or replace Target t2 using KinesisWriter (
  regionName:'TARGET_REGION',
  streamName:'TARGET_STREAM',
  accesskeyid:'ACCESS_KEY',
  secretaccesskey:'SECRET_KEY'
)
format using JSONFormatter (
)
input from DDLCDCStream;

end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

INPUT FROM @STREAM@;

Stop Oracle_Agent;
Undeploy application Oracle_Agent;
drop application Oracle_Agent cascade;

CREATE APPLICATION Oracle_Agent;

CREATE FLOW test_SourceFlow;

CREATE  SOURCE Oracle_Source USING OracleReader  ( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.124.25:1521/orcl',
  Tables: 'QATEST.%',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO Oracle_ChangeDataStream ;

END FLOW test_SourceFlow;

CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Oracle_ChangeDataStream;

END APPLICATION Oracle_Agent;
DEPLOY APPLICATION Oracle_Agent ON ANY IN default WITH test_SourceFlow ON ANY IN AGENTS;
start Oracle_Agent;

stop application Postgres_To_PostgresApp;
undeploy application Postgres_To_PostgresApp;
drop application Postgres_To_PostgresApp cascade;

CREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;

create type employee
(
id integer,
ename String,
operationname String,
LSN String
);
CREATE STREAM Postgres_TypedStream of employee;

CREATE OR REPLACE SOURCE Postgres_Src USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgres_src'
 ) 
OUTPUT TO Postgres_Change_Data_Stream;

create stream UserdataStream1 of Global.WAEvent;

Create CQ CQUser
insert into UserdataStream1
select putuserdata (data1,'OperationName',META(data1,'OperationName').toString()) from Postgres_Change_Data_Stream data1;

create CQ Cqfilter 
insert into Postgres_TypedStream
select 
to_int(data[0]),
data[1],
META(u,'OperationName').toString(),
META(u,'LSN').toString()
from UserdataStream1 u 
where USERDATA(u,'OperationName').toString()=='INSERT';

CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Postgres_TypedStream;

CREATE TARGET Postgres_FW USING FileWriter (
  filename:'Postgres_FW.log'
)
FORMAT USING DSVFormatter ()
INPUT FROM Postgres_TypedStream;

CREATE OR REPLACE TARGET Postgres_tgt USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:300',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Postgres_TypedStream;

end application Postgres_To_PostgresApp;
deploy application Postgres_To_PostgresApp;
start Postgres_To_PostgresApp;

STOP TQLwithinTqlApp;
UNDEPLOY APPLICATION TQLwithinTqlApp;
DROP APPLICATION TQLwithinTqlApp CASCADE;

CREATE APPLICATION TQLwithinTqlApp;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',

  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;

@@FEATURE-DIR@/tql/TQLTobeCalled.tql


END APPLICATION TQLwithinTqlApp;
DEPLOY APPLICATION TQLwithinTqlApp;
START TQLwithinTqlApp;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;
DROP EXCEPTIONSTORE @APP_NAME@_EXCEPTIONSTORE;

CREATE APPLICATION @APP_NAME@ @APP_PROPERTY@ USE EXCEPTIONSTORE;

CREATE OR REPLACE STREAM @APP_NAME@_DataStream OF Global.WAEvent;

Create Source @APP_NAME@_Source Using @SOURCE_ADAPTER@ (

) OUTPUT TO @APP_NAME@_DataStream;

CREATE TARGET @APP_NAME@_Target USING @TARGET_ADAPTER@( 

) INPUT FROM @APP_NAME@_DataStream;

CREATE OR REPLACE TARGET @APP_NAME@_SysOut USING Global.SysOut  ( 
	name: '@APP_NAME@_SysOutWA' 
) INPUT FROM @APP_NAME@_DataStream;

END APPLICATION @APP_NAME@;

DEPLOY APPLICATION @APP_NAME@ IN DEFAULT;
START APPLICATION @APP_NAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE OR REPLACE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;

CREATE OR REPLACE SOURCE @SOURCE@ USING SalesForceReader (
  autoAuthTokenRenewal: 'true',
  Username: '@userName@',
  securityToken: '@securityToken@',
  sObjects: '@srcObjectName@',
  pollingInterval: '1 min',
  Password_encrypted: 'false',
  securityToken_encrypted: 'false',
  customObjects: 'False',
  consumerKey: '@consumerKey@',
  startTimestamp: '',
  apiEndPoint: 'https://ap2.salesforce.com',
  mode: 'Incremental',
  consumerSecret: '@consumerSecert@',
  consumerSecret_encrypted: 'false',
  Password: '@Password@' )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@ USING Global.BigQueryWriter (
  ColumnDelimiter: '|',
  NullMarker: 'NULL',
  ConnectionRetryPolicy: 'retryInterval=30,\n maxRetries=3',
  streamingUpload: 'false',
  Mode: 'Merge',
  projectId: '@ProjectId@',
  Encoding: 'UTF-8',
  TransportOptions: 'connectionTimeout=300,\n readTimeout=120',
  Tables: '@srcObjectName@,@TargetTableName@ columnmap(ID=ID,checkbool__c=checkbool__c,dt__c=dt__c,percnt__c=percnt__c,phn__c=phn__c,txtlong__c=txtlong__c,url1__c=url1__c);',
  AllowQuotedNewlines: 'false',
  CDDLAction: 'Process',
  adapterName: 'BigQueryWriter',
  serviceAccountKey: '@GCS-AuthPath@',
  optimizedMerge: 'true',
  StandardSQL: 'true',
  includeInsertId: 'true',
  QuoteCharacter: '\"',
  BatchPolicy: 'eventCount:1000,Interval:10' )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYVARIABLE RetryPolicy='timeOut=00,retryInterval=1,maxRetries=3';
CREATE OR REPLACE PROPERTYVARIABLE KafkaBrokerAddress='localhost:9099';
CREATE OR REPLACE PROPERTYVARIABLE KafkaConfig='request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;';

STOP @WRITERAPPNAME@@RECOVSTATUS@;
UNDEPLOY APPLICATION @WRITERAPPNAME@@RECOVSTATUS@;
DROP APPLICATION @WRITERAPPNAME@@RECOVSTATUS@ CASCADE;

CREATE APPLICATION @WRITERAPPNAME@@RECOVSTATUS@ @Recovery@;
create flow AgentFlow;
CREATE SOURCE S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.oracle_kw_test%',
	FetchSize: '1',
	connectionRetryPolicy:'$RetryPolicy'
)
OUTPUT TO SS;
end flow AgentFlow;
create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;
create stream out_cq_select_SS_1 of global.waevent;
create stream out_cq_select_SS_2 of global.waevent;
create stream out_cq_select_SS_3 of global.waevent;
create stream out_cq_select_SS_4 of global.waevent;
create stream out_cq_select_SS_5 of global.waevent;
create stream out_cq_select_SS_6 of global.waevent;

CREATE OR REPLACE CQ cq_select_SS1 
INSERT INTO out_cq_select_SS_1
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST1';

CREATE OR REPLACE CQ cq_select_SS2 
INSERT INTO out_cq_select_SS_2
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST2';

CREATE OR REPLACE CQ cq_select_SS3 
INSERT INTO out_cq_select_SS_3
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST3';

CREATE OR REPLACE CQ cq_select_SS4 
INSERT INTO out_cq_select_SS_4
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST4';

CREATE OR REPLACE CQ cq_select_SS5 
INSERT INTO out_cq_select_SS_5
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST5';

CREATE OR REPLACE CQ cq_select_SS6 
INSERT INTO out_cq_select_SS_6
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST6';

create Target TARGET1 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_sync_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: '$KafkaConfig')
FORMAT USING dsvFormatter ()
input from out_cq_select_SS_1;

create Target TARGET2 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_sync_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: '$KafkaConfig')
FORMAT USING jsonFormatter ()
input from out_cq_select_SS_2;

create Target TARGET3 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_sync_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: '$KafkaConfig')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_sync_CQ.avsc')
input from out_cq_select_SS_3;

create Target TARGET4 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_Async_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(ROWID)',
Mode:'Async',
KafkaConfig: '$KafkaConfig')
FORMAT USING dsvFormatter ()
input from out_cq_select_SS_4;

create Target TARGET5 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_Async_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(ROWID)',
Mode:'Async',
KafkaConfig: '$KafkaConfig')
FORMAT USING jsonFormatter ()
input from out_cq_select_SS_5;

create Target TARGET6 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_Async_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(ROWID)',
Mode:'Async',
KafkaConfig: '$KafkaConfig')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_Async_CQ.avsc')
input from out_cq_select_SS_6;

create Target TARGET7 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_sync',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: '$KafkaConfig')
FORMAT USING dsvFormatter ()
input from ss;

create Target TARGET8 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_sync',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: '$KafkaConfig')
FORMAT USING jsonFormatter ()
input from ss;

create Target TARGET9 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_sync',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: '$KafkaConfig')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_sync.avsc')
input from ss;

create Target TARGET10 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_Async',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'Async',
KafkaConfig: '$KafkaConfig')
FORMAT USING dsvFormatter ()
input from ss;

create Target TARGET11 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_Async',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'Async',
KafkaConfig: '$KafkaConfig')
FORMAT USING jsonFormatter ()
input from ss;

create Target TARGET12 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_Async',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'Async',
KafkaConfig: '$KafkaConfig')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_Async.avsc')
input from ss;

end flow serverFlow;
end application @WRITERAPPNAME@@RECOVSTATUS@;
deploy application @WRITERAPPNAME@@RECOVSTATUS@;
start @WRITERAPPNAME@@RECOVSTATUS@;



stop application @READERAPPNAME@@RECOVSTATUS@;
undeploy application @READERAPPNAME@@RECOVSTATUS@;
drop application @READERAPPNAME@@RECOVSTATUS@ cascade;
CREATE APPLICATION @READERAPPNAME@@RECOVSTATUS@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @SOURCE@_DSV_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_sync_CQ',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;

CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
rolloverpolicy:'filesize:500M',
filename:'@READERAPPNAME@@RECOVSTATUS@_@SOURCE@_dsv_sync_CQ')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream1;

CREATE SOURCE @SOURCE@_JSON_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_sync_CQ',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;

CREATE SOURCE @SOURCE@_AVRO_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_sync_CQ',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_sync_CQ.avsc'
)
OUTPUT TO KafkaReaderStream3;

CREATE SOURCE @SOURCE@_DSV_Async_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_Async_CQ',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream4;

CREATE SOURCE @SOURCE@_JSON_Async_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_Async_CQ',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream5;

CREATE SOURCE @SOURCE@_AVRO_Async_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_Async_CQ',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_Async_CQ.avsc'
)
OUTPUT TO KafkaReaderStream6;

CREATE SOURCE @SOURCE@_DSV_sync USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_sync',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream7;

CREATE TARGET kafkaDumpDSV_rawstream USING FileWriter(
name:kafkaOuputDSV_rawstream,
rolloverpolicy:'filesize:500M',
filename:'@READERAPPNAME@@RECOVSTATUS@_@SOURCE@_dsv_sync')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream7;

CREATE SOURCE @SOURCE@_JSON_sync USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_sync',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream8;

CREATE SOURCE @SOURCE@_AVRO_sync USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_sync',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_sync.avsc'
)
OUTPUT TO KafkaReaderStream9;

CREATE SOURCE @SOURCE@_DSV_Async USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_Async',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream10;

CREATE SOURCE @SOURCE@_JSON_Async USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_Async',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream11;

CREATE SOURCE @SOURCE@_AVRO_Async USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_Async',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_Async.avsc'
)
OUTPUT TO KafkaReaderStream12;

end application @READERAPPNAME@@RECOVSTATUS@;
deploy application @READERAPPNAME@@RECOVSTATUS@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@;

create TYPE CountTYPE(numcol INT);

CREATE JUMPING WINDOW nEvents OVER @STREAM@ KEEP 10 ROWS;

CREATE STREAM TypedCountStream of CountTYPE;

CREATE CQ CountCQ INSERT INTO TypedCountStream SELECT TO_INT(data[0]) FROM nEvents;

--
-- Recovery Test 23 with two sources, two sliding time windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> St1W -> CQ1 -> WS
-- S2 -> St2W -> CQ2 -> WS
--

STOP Recov23Tester.RecovTest23;
UNDEPLOY APPLICATION Recov23Tester.RecovTest23;
DROP APPLICATION Recov23Tester.RecovTest23 CASCADE;
CREATE APPLICATION RecovTest23 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 1 SECOND;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 2 SECOND;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION RecovTest23;

--
-- Recovery Test 10 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> CW(p) -> CQ -> WS
--

STOP Recov10Tester.RecovTest10;
UNDEPLOY APPLICATION Recov10Tester.RecovTest10;
DROP APPLICATION Recov10Tester.RecovTest10 CASCADE;
CREATE APPLICATION RecovTest10 RECOVERY 1 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTest10Data.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  partKey String KEY,
  serialNumber int
);

CREATE STREAM DataStream OF CsvData PARTITION BY partKey;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    TO_INT(data[1])
FROM CsvStream;

CREATE JUMPING WINDOW DataStreamTwoItems
OVER DataStream KEEP 2 ROWS
PARTITION BY partKey;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    first(partKey),
    to_int(first(serialNumber))
FROM DataStreamTwoItems
GROUP BY partKey;

END APPLICATION RecovTest10;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;

CREATE  APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE @srcName@ USING Global.DatabaseReader (
  ConnectionURL: '@srcurl@', 
  Tables: '@srcschema@.@srctable@',
  ReturnDateTimeAs: 'String', 
  FetchSize: 30000, 
  Username: '@srcusername@', 
  QuiesceOnILCompletion: true, 
  Password: '@srcpassword@', 
  DatabaseProviderType: 'DEFAULT' ) 
OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@  USING Global.DatabaseWriter ( 
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  CheckPointTable: 'CHKPOINT',
  Username:'@tgtusername@', 
  Password:'@tgtpassword@', 
  CDDLAction: 'Process', 
  StatementCacheSize: '50', 
  CommitPolicy: 'EventCount:30000,Interval:60', 
  ConnectionURL:'@tgturl@',
  DatabaseProviderType: 'Default', 
  PreserveSourceTransactionBoundary: 'false', 
  BatchPolicy: 'EventCount:30000,Interval:60', 
  Tables:'@srcschema@.@srctable@,@tgtschema@.@tgttable@', 
  adapterName: 'DatabaseWriter' ) 
INPUT FROM @instreamname@;

End APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

create or replace Target Quiesce_orcl_kwTARGET1 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'Quiesce_orcl_kw_dsv_sync_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(COMMIT_TIMESTAMP)',
Mode:'sync',
KafkaConfig: 'batch.size=1048576;linger.ms=300000;')
FORMAT USING dsvFormatter ()
input from Quiesce_orcl_kwss;

stop application ThreeAgentTester.CSV;
undeploy application ThreeAgentTester.CSV;
drop application ThreeAgentTester.CSV cascade;

create application CSV;

CREATE FLOW AgentFlowOne;
create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStreamOne;
END FLOW AgentFlowOne;

CREATE FLOW AgentFlowTwo;
create source CSVSourceOne using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStreamTwo;
END FLOW AgentFlowTwo;

CREATE FLOW AgentFlowThree;
create source CSVSourceTwo using CSVReader (
  Directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStreamThree;
END FLOW AgentFlowThree;

CREATE FLOW ServerFlow;

CREATE TARGET FileDumpOne Using FileWriter (
directory:'@FEATURE-DIR@/logs/',
filename:'SourceDumpFromStreamOne_%yyyy-MM-dd_HH_mm_ss_SSS%.csv',
rolloverpolicy:'eventcount:39'

)
format using DSVFormatter (
)
input from CsvStreamOne;
CREATE TYPE UserDataType
(
  UserId String KEY,
  UserName String
);

CREATE STREAM UserDataStream OF UserDataType PARTITION BY UserId;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  data[0],
        data[1]
FROM CsvStreamOne;

CREATE TARGET FileDumpTwo Using FileWriter (
directory:'@FEATURE-DIR@/logs/',
filename:'CQDumpFromUserDataStream_%yyyy-MM-dd_HH_mm_ss_SSS%.csv',
rolloverpolicy:'eventcount:39'

)
format using DSVFormatter (
)
input from UserDataStream;

CREATE WACTIONSTORE UserActivityInfo
CONTEXT OF UserDataType
EVENT TYPES ( UserDataType )
@PERSIST-TYPE@

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction
INSERT INTO UserActivityInfo
SELECT * FROM UserDataStream
LINK SOURCE EVENT;
END FLOW ServerFlow;

END APPLICATION CSV;
DEPLOY APPLICATION CSV with AgentFlowOne on all in AGENTS, AgentFlowTwo on all in AGENTS,AgentFlowThree on all in AGENTS,ServerFlow on any in default;
START CSV;

--
-- Recovery Test 65
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ1 -> JWc10 -> CQ(aggregate) -> WS1
-- S -> CQ2 -> JWc11-> CQ(aggregate) -> WS2
--

STOP Recov65Tester.RecovTest65;
UNDEPLOY APPLICATION Recov65Tester.RecovTest65;
DROP APPLICATION Recov65Tester.RecovTest65 CASCADE;
CREATE APPLICATION RecovTest65 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStreamSize10
OVER DataStream KEEP 10 ROWS;

CREATE JUMPING WINDOW DataStreamSize11
OVER DataStream KEEP 11 ROWS;

CREATE WACTIONSTORE Wactions1 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions2 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions1
INSERT INTO Wactions1
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    TO_DATE(FIRST(p.dateTime)),
    TO_DOUBLE(FIRST(p.amount)),
    FIRST(p.city)
FROM DataStreamSize10 p;

CREATE CQ InsertWactions2
INSERT INTO Wactions2
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    TO_DATE(FIRST(p.dateTime)),
    TO_DOUBLE(FIRST(p.amount)),
    FIRST(p.city)
FROM DataStreamSize11 p;

create Target t1 using logwriter(name:Foo1, filename: output1) input from DataStreamSize10;
create Target t2 using logwriter(name:Foo2, filename: output2) input from DataStreamSize11;

END APPLICATION RecovTest65;

STOP application consoletest.noApp;
undeploy application consoletest.noApp;
drop application consoletest.noApp cascade;

create application noApp;
create source CSVSource using FileReader (
  directory:'Wrong/Dir/Path',
  WildCard:'posdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'Foo',
        sequence:'00',
  rolloverpolicy:'eventcount:200'
)
format using DSVFormatter (

)
input from TypedCSVStream;
end application noApp;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

CREATE OR REPLACE SOURCE @SOURCE@ USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '85d7qFnwTW8=',
  Password_encrypted: 'true',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

CREATE OR REPLACE TARGET @TARGET@ USING @TARGET_ADAPTER@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

end flow @APPNAME@_serverflow;

END APPLICATION @APPNAME@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ recovery 5 second Interval;

CREATE source @srcName@ USING Global.OracleReader ( 
 Username:'@srcusername@',
  Password:'@srcpassword@',
  ConnectionURL:'@srcurl@',
  Tables:'@srcschema@.@srctable@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries:'true'
  ) 
OUTPUT TO @outstreamname@;


CREATE OR REPLACE TARGET @tgtName@ USING Global.SalesforceWriter ( 
  autoAuthTokenRenewal: 'true', 
  sObjects: '@srcschema@.@srctable@,@tgtobject@ COLUMNMAP(num__c=a,Name=b)', 
  useConnectionProfile: 'false', 
  consumerSecret: '@tgtconsumersecret@', 
  JWTKeystorePath: '', 
  BatchPolicy:'EventCount:1,Interval:10s',
  CommitPolicy:'EventCount:1,Interval:10s', 
  Mode: 'APPENDONLY', 
  consumerKey: '@tgtconsumerkey@', 
  apiEndPoint: '@tgtapiurl@', 
  InMemory: 'true', 
  FieldDelimeter: 'COMMA', 
  ApplicationErrorCountThreshold: '0', 
  useBulkApi: 'true', 
  OAuthAuthorizationFlows: 'PASSWORD', 
  JWTCertificateName: '', 
  hardDelete: 'false', 
  Username: '@tgtusername@', 
  useQuotes: 'false', 
  adapterName: 'SalesforceWriter', 
  Password: '@tgtpassword@', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  securityToken: '@tgtsecuritytoken@', 
  JWTKeystorePassword: '' ) 
INPUT FROM @instreamname@;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP application SecurityApper.MultiLogDashboard;
undeploy application SecurityApper.MultiLogDashboard;
drop application SecurityApper.MultiLogDashboard cascade;

CREATE APPLICATION MultiLogDashboard;

-- This sample application shows how WebAction could be used monitor and correlate logs
-- from web and application server logs from the same web application. See the discussion
-- in the "Sample Applications" section of the WebAction documentation for additional
-- discussion.


CREATE FLOW MonitorLogs;

-- MonitorLogs sets up the two log sources used by this application. In a real-world
--implementation, each source could be reading many logs from many servers.

-- The web server logs are in Apache NCSA extended/ combined log format plus response time:
-- "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-agent}i\" %D"
-- See apache.org for more information.

CREATE SOURCE AccessLogSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'access_log',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO RawAccessStream;

CREATE TYPE AccessLogEntry (
    srcIp String KEY,
    userId String,
    sessionId String,
    accessTime DateTime,
    request String,
    code int,
    size int,
    referrer String,
    userAgent String,
    responseTime int
);
CREATE STREAM AccessStream OF AccessLogEntry;

CREATE CQ ParseAccessLog
INSERT INTO AccessStream
SELECT data[0], data[2], MATCH(data[4], ".*jsessionId=(.*) "),
       TO_DATE(data[3], "dd/MMM/yyyy:HH:mm:ss.SSS Z"), data[4], TO_INT(data[5]), TO_INT(data[6]),
       data[7], data[8], TO_INT(data[9])
FROM RawAccessStream;

-- The application server logs are in Apache's Log4J format.

CREATE SOURCE Log4JSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'log4jLog.xml',
  positionByEOF:false
)
PARSE USING XMLParser(
  rootnode:'/log4j:event',
  columnlist:'log4j:event/@timestamp,log4j:event/@level,log4j:event/log4j:message,log4j:event/log4j:throwable,log4j:event/log4j:locationInfo/@class,log4j:event/log4j:locationInfo/@method,log4j:event/log4j:locationInfo/@file,log4j:event/log4j:locationInfo/@line'
)
OUTPUT TO RawXMLStream;

CREATE TYPE Log4JEntry (
  logTime DateTime,
  level String,
  message String,
  api String,
  sessionId String,
  userId String,
  sobject String,
  xception String,
  className String,
  method String,
  fileName String,
  lineNum String
);
CREATE STREAM Log4JStream OF Log4JEntry;

CREATE CQ ParseLog4J
INSERT INTO Log4JStream
SELECT TO_DATE(TO_LONG(data[0])), data[1], data[2],
       MATCH(data[2], '\\\\[api=([a-zA-Z0-9]*)\\\\]'),
       MATCH(data[2], '\\\\[session=([a-zA-Z0-9\\-]*)\\\\]'),
       MATCH(data[2], '\\\\[user=([a-zA-Z0-9\\-]*)\\\\]'),
       MATCH(data[2], '\\\\[sobject=([a-zA-Z0-9]*)\\\\]'),
       data[3], data[4], data[5], data[6], data[7]
FROM RawXMLStream;

END FLOW MonitorLogs;


CREATE FLOW ErrorsAndWarnings;

-- ErrorsAndWarnings creates a sliding window (Log4JErrorWarningActivity) containing
-- the 300 most recent errors and warnings in the application server log. The
-- ZeroContentCheck and LargeRTCheck flows join events from this window with access log
-- events.

-- The type Log4JEntry was already defined by the MonitorLogs flow.
CREATE STREAM Log4ErrorWarningStream OF Log4JEntry;

CREATE CQ GetLog4JErrorWarning
INSERT INTO Log4ErrorWarningStream
SELECT l FROM Log4JStream l
WHERE l.level = 'ERROR' OR l.level = 'WARN';

CREATE WINDOW Log4JErrorWarningActivity
OVER Log4ErrorWarningStream KEEP 300 ROWS;

END FLOW ErrorsAndWarnings;


-- HackerCheck sends an alert when an access log srcIp value is on a blacklist.

CREATE FLOW HackerCheck;

CREATE TYPE IPEntry (
    ip String
);

/* CREATE CACHE BlackListLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'multiLogBlackList.txt',
  header: No,
  columndelimiter: ','
) QUERY (keytomap:'ip') OF IPEntry; */

CREATE CACHE BlackListLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'multiLogBlackList.txt'
)
PARSE USING DSVParser ( )
QUERY (keytomap:'ip') OF IPEntry;


CREATE STREAM HackerStream OF AccessLogEntry;

CREATE CQ FindHackers
INSERT INTO HackerStream
SELECT ale
FROM AccessStream ale, BlackListLookup bll
WHERE ale.srcIp = bll.ip;

CREATE TYPE UnusualContext (
    typeOfActivity String,
    accessTime DateTime,
    accessSessionId String,
    srcIp String KEY,
    userId String,
    country String,
    city String,
    lat double,
    lon double
);
CREATE TYPE MergedEntry (
    accessTime DateTime,
    accessSessionId String,
    srcIp String KEY,
    userId String,
    request String,
    code int,
    size int,
    referrer String,
    userAgent String,
    responseTime int,
    logTime DateTime,
    logSessionId String,
    level String,
    message String,
    api String,
    sobject String,
    xception String,
    className String,
    method String,
    fileName String,
    lineNum String
);
CREATE WACTIONSTORE UnusualActivity
CONTEXT OF UnusualContext
EVENT TYPES (MergedEntry,AccessLogEntry)
PERSIST NONE USING ( );

CREATE CQ GenerateHackerContext
INSERT INTO UnusualActivity
SELECT 'HackAttempt', accessTime, sessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM HackerStream
LINK SOURCE EVENT;

CREATE STREAM HackingAlertStream OF Global.AlertEvent;

CREATE CQ SendHackingAlerts
INSERT INTO HackingAlertStream
SELECT 'HackingAlert', ''+accessTime, 'warning', 'raise',
        'Possible Hacking Attempt from ' + srcIp + ' in ' + IP_COUNTRY(srcIp)
FROM HackerStream;

CREATE SUBSCRIPTION HackingAlertSub USING WebAlertAdapter( ) INPUT FROM HackingAlertStream;

END FLOW HackerCheck;


-- LargeRTCheck sends an alert when an access log responseTime value exceeds 2000
-- microseconds.

CREATE FLOW LargeRTCheck;

CREATE STREAM LargeRTStream of AccessLogEntry;

CREATE CQ FindLargeRT
INSERT INTO LargeRTStream
SELECT ale
FROM AccessStream ale
WHERE ale.responseTime > 2000;

CREATE WINDOW LargeRTActivity
OVER LargeRTStream KEEP 100 ROWS;

CREATE STREAM LargeRTAPIStream OF MergedEntry;

CREATE CQ MergeLargeRTAPI
INSERT INTO LargeRTAPIStream
SELECT lrt.accessTime, lrt.sessionId, lrt.srcIp, lrt.userId, lrt.request,
       lrt.code, lrt.size, lrt.referrer, lrt.userAgent, lrt.responseTime,
       log4j.logTime, log4j.sessionId, log4j.level, log4j.message, log4j.api, log4j.sobject, log4j.xception,
       log4j.className, log4j.method, log4j.fileName, log4j.lineNum
FROM LargeRTActivity lrt, Log4JErrorWarningActivity log4j
WHERE lrt.sessionId = log4j.sessionId
      AND lrt.accessTime = log4j.logTime;

CREATE CQ GenerateLargeRTContext
INSERT INTO UnusualActivity
SELECT 'LargeResponseTime', accessTime, accessSessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM LargeRTAPIStream
LINK SOURCE EVENT;

CREATE STREAM LargeRTAlertStream OF Global.AlertEvent;

CREATE CQ SendLargeRTAlerts
INSERT INTO LargeRTAlertStream
SELECT 'LargeRTAlert', ''+accessTime, 'warning', 'raise',
        'Long response time for call from ' + userId + ' api ' + api + ' message ' + message
FROM LargeRTAPIStream;

CREATE SUBSCRIPTION LargeRTAlertSub USING WebAlertAdapter( ) INPUT FROM LargeRTAlertStream;

END FLOW LargeRTCheck;


-- ProxyCheck sends an alert when an access log srcIP value is on a list of suspicious
-- proxies.

CREATE FLOW ProxyCheck;

/* CREATE CACHE ProxyLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'multiLogProxies.txt',
  header: No,
  columndelimiter: ','
) QUERY (keytomap:'ip') OF IPEntry; */

CREATE CACHE ProxyLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'multiLogProxies.txt'
)
PARSE USING DSVParser ( )
QUERY (keytomap:'ip') OF IPEntry;


CREATE STREAM ProxyStream OF AccessLogEntry;

CREATE CQ FindProxies
INSERT INTO ProxyStream
SELECT ale
FROM AccessStream ale, ProxyLookup pl
WHERE ale.srcIp = pl.ip;

CREATE CQ GenerateProxyContext
INSERT INTO UnusualActivity
SELECT 'ProxyAccess', accessTime, sessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM ProxyStream
LINK SOURCE EVENT;


CREATE STREAM ProxyAlertStream OF Global.AlertEvent;

CREATE CQ SendProxyAlerts
INSERT INTO ProxyAlertStream
SELECT 'ProxyAlert', ''+accessTime, 'warning', 'raise',
        'Possible use of Proxy from ' + srcIp + ' in ' + IP_COUNTRY(srcIp) + ' for user ' + userId
FROM ProxyStream;

CREATE SUBSCRIPTION ProxyAlertSub USING WebAlertAdapter( ) INPUT FROM ProxyAlertStream;

END FLOW ProxyCheck;


-- ZeroContentCheck sends an alert when an access log entry's code value is 200 (that is,
-- the HTTP request succeeded) but the size value is 0 (the return had no content).

CREATE FLOW ZeroContentCheck;

CREATE STREAM ZeroContentStream of AccessLogEntry;

CREATE CQ FindZeroContent
INSERT INTO ZeroContentStream
SELECT ale
FROM AccessStream ale
WHERE ale.code = 200 AND ale.size = 0;

CREATE WINDOW ZeroContentActivity
OVER ZeroContentStream KEEP 100 ROWS;

CREATE STREAM ZeroContentAPIStream OF MergedEntry;

CREATE CQ MergeZeroContentAPI
INSERT INTO ZeroContentAPIStream
SELECT zcs.accessTime, zcs.sessionId, zcs.srcIp, zcs.userId, zcs.request,
       zcs.code, zcs.size, zcs.referrer, zcs.userAgent, zcs.responseTime,
       log4j.logTime, log4j.sessionId, log4j.level, log4j.message, log4j.api, log4j.sobject, log4j.xception,
       log4j.className, log4j.method, log4j.fileName, log4j.lineNum
FROM ZeroContentActivity zcs, Log4JErrorWarningActivity log4j
WHERE zcs.sessionId = log4j.sessionId
      AND zcs.accessTime = log4j.logTime;

CREATE CQ GenerateZeroContentContext
INSERT INTO UnusualActivity
SELECT 'ZeroContent', accessTime, accessSessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM ZeroContentAPIStream
LINK SOURCE EVENT;


CREATE TYPE ZeroContentEventListType (
    srcIp String KEY,
    code int,
    size int,
    level String,
    message String,
    xception String);

CREATE WACTIONSTORE ZeroContentEventList
CONTEXT OF ZeroContentEventListType
EVENT TYPES (ZeroContentEventListType )
PERSIST NONE USING ( );


CREATE CQ GenerateZeroContentEventList
INSERT INTO ZeroContentEventList
SELECT srcIp, code, size, level, message, xception
FROM ZeroContentAPIStream;


CREATE STREAM ZeroContentAlertStream OF Global.AlertEvent;

CREATE CQ SendZeroContentAlerts
INSERT INTO ZeroContentAlertStream
SELECT 'ZeroContentAlert', ''+accessTime, 'warning', 'raise',
        'Zero content returned in call from ' + userId + ' api ' + api + ' message ' + message
FROM ZeroContentAPIStream;

CREATE SUBSCRIPTION ZeroContentAlertSub USING WebAlertAdapter( ) INPUT FROM ZeroContentAlertStream;

END FLOW ZeroContentCheck;


-- ErrorHandling is functionally identical to ErrorFlow.SaasMonitorApp. It sends an alert
-- when an error message appears in the application server log.

CREATE FLOW ErrorHandling;

CREATE STREAM ErrorStream OF Log4JEntry;

CREATE CQ GetErrors
INSERT INTO ErrorStream
SELECT log4j
FROM Log4ErrorWarningStream log4j WHERE log4j.level = 'ERROR';

CREATE STREAM ErrorAlertStream OF Global.AlertEvent;

CREATE CQ SendErrorAlerts
INSERT INTO ErrorAlertStream
SELECT 'ErrorAlert', ''+logTime, 'error', 'raise', 'Error in log ' + message
FROM ErrorStream;

CREATE SUBSCRIPTION ErrorAlertSub USING WebAlertAdapter( ) INPUT FROM ErrorAlertStream;

END FLOW ErrorHandling;


-- WarningHandling is a minor variation on WarningFlow.SaasMonitorApp. It sends an alert
-- once an hour with the count of warnings for each api call for which there has been at
-- least one alert.

CREATE FLOW WarningHandling;

CREATE STREAM WarningStream OF Log4JEntry;

CREATE CQ GetWarnings
INSERT INTO WarningStream
SELECT log4j
FROM Log4ErrorWarningStream log4j WHERE log4j.level = 'WARN';

CREATE JUMPING WINDOW WarningWindow
OVER WarningStream KEEP WITHIN 60 MINUTE ON logTime;

CREATE STREAM WarningAlertStream OF Global.AlertEvent;

CREATE CQ SendWarningAlerts
INSERT INTO WarningAlertStream
SELECT 'WarningAlert', ''+logTime, 'warning', 'raise',
        COUNT(logTime) + ' Warnings in log for api ' + api
FROM WarningWindow
GROUP BY api
HAVING count(logTime) > 1;

CREATE SUBSCRIPTION WarningAlertSub USING WebAlertAdapter( ) INPUT FROM WarningAlertStream;

END FLOW WarningHandling;


-- InfoFlow is functionally similar to InfoFlow.SaasMonitorApp. Its output is used by
-- ApiFlow, CompanyApiFlow, and UserApiFlow.

CREATE FLOW InfoFlow;

CREATE TYPE UserInfo (
  userId String,
  userName String,
  company String,
  userZip String,
  companyZip String
);

/* CREATE CACHE MLogUserLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'multiLogUser.csv',
  header: No,
  columndelimiter: ','
) QUERY (keytomap:'userId') OF UserInfo; */

CREATE CACHE MLogUserLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'multiLogUser.csv'
)
PARSE USING DSVParser ( )
QUERY (keytomap:'userId') OF UserInfo;

CREATE STREAM InfoStream OF Log4JEntry;

CREATE CQ GetInfo
INSERT INTO InfoStream
SELECT log4j
FROM Log4JStream log4j WHERE log4j.level = 'INFO';

CREATE TYPE ApiCall (
  userId String,
  api String,
  sobject String,
  logTime DateTime,
  userName String,
  company String,
  userZip String,
  companyZip String
);
CREATE STREAM ApiEnrichedStream OF ApiCall;

CREATE CQ GetUserDetails
INSERT INTO ApiEnrichedStream
SELECT a.userId, a.api, a.sobject, a.logTime, u.userName, u.company, u.userZip, u.companyZip
FROM InfoStream a, MLogUserLookup u
WHERE a.userId = u.userId;

END FLOW InfoFlow;


-- ApiFlow populates the dashboard's Detail - ApiActivity page and the pie chart on the
-- Overview page.

CREATE FLOW ApiFlow;

CREATE TYPE ApiUsage (
  api String key,
  sobject String,
  count int,
  logTime DateTime
);

CREATE TYPE ApiContext (
  api String key,
  count int,
  logTime DateTime
);

CREATE WACTIONSTORE ApiActivity
CONTEXT OF ApiContext
EVENT TYPES (ApiUsage )
PERSIST NONE USING ( );

CREATE JUMPING WINDOW ApiWindow
OVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY api;

CREATE STREAM ApiUsageStream OF ApiUsage;

CREATE CQ GetApiUsage
INSERT INTO ApiUsageStream
SELECT a.api, a.sobject,
       COUNT(a.userId), FIRST(a.logTime)
FROM ApiWindow a
GROUP BY a.api, a.sobject HAVING FIRST(a.logTime) IS NOT NULL;

CREATE JUMPING WINDOW ApiSummaryWindow
OVER ApiUsageStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY api;

CREATE CQ GetApiSummaryUsage
INSERT INTO ApiActivity
SELECT a.api,
       sum(a.count), first(a.logTime)
FROM ApiSummaryWindow a
GROUP BY a.api
LINK SOURCE EVENT;

END FLOW ApiFlow;


-- CompanyApiFlow populates the dashboard's Detail - CompanyApiActivity page and the bar
-- chart on the Overview page. It also sends an alert when an API call is used by a
-- company more than 1500 times during the flow's one-hour jumping window.

CREATE FLOW CompanyApiFlow;

CREATE TYPE CompanyApiUsage (
  company String key,
  companyZip String,
  companyLat double,
  companyLong double,
  api String,
  count int,
  unusual int,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE TYPE CompanyApiContext (
  company String key,
  companyZip String,
  companyLat double,
  companyLong double,
  count int,
  unusual int,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE WACTIONSTORE CompanyApiActivity
CONTEXT OF CompanyApiContext
EVENT TYPES ( CompanyApiUsage )
PERSIST NONE USING ( );

CREATE JUMPING WINDOW CompanyApiWindow
OVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY company;

CREATE STREAM CompanyApiUsageStream OF CompanyApiUsage;

CREATE TYPE MLogUSAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

/*CREATE CACHE MLogZipLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  header: Yes,
  columndelimiter: ','
) QUERY (keytomap:'zip') OF MLogUSAddressData; */

CREATE CACHE MLogZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt'
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false

)
QUERY (keytomap:'zip') OF MLogUSAddressData;


CREATE CQ GetCompanyApiUsage
INSERT INTO CompanyApiUsageStream
SELECT a.company, a.companyZip, z.latVal, z.longVal,
       a.api, COUNT(a.sobject),
       CASE WHEN COUNT(a.sobject) > 1500 THEN 1
            ELSE 0 END,
       CASE WHEN COUNT(a.sobject) > 1500 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.sobject),
       FIRST(a.logTime)
FROM CompanyApiWindow a, MLogZipLookup z
WHERE a.companyZip = z.zip
GROUP BY a.company, a.api HAVING FIRST(a.logTime) IS NOT NULL;

CREATE JUMPING WINDOW CompanyWindow
OVER CompanyApiUsageStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY company;

CREATE CQ GetCompanyUsage
INSERT INTO CompanyApiActivity
SELECT a.company, a.companyZip, a.companyLat, a.companyLong,
       SUM(a.count), SUM(a.unusual),
       CASE WHEN SUM(a.unusual) > 0 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.topObject),
       FIRST(a.logTime)
FROM CompanyWindow a
GROUP BY a.company
LINK SOURCE EVENT;

CREATE STREAM CompanyAlertStream OF Global.AlertEvent;

CREATE CQ SendCompanyApiAlerts
INSERT INTO CompanyAlertStream
SELECT 'CompanyAPIAlert', ''+logTime, 'warning', 'raise',
       'Company ' + company + ' has used api ' + api + ' ' + count + ' times for ' + topObject
FROM CompanyApiUsageStream
WHERE unusual = 1;

CREATE SUBSCRIPTION CompanyAlertSub USING WebAlertAdapter( ) INPUT FROM CompanyAlertStream;

END FLOW CompanyApiFlow;


-- UserApiFlow populates the dashboard's Detail - UserApiActivity page and the US map on
-- the Overview page. It also sends an alert when an API call is used by a user more than
-- 125 times during the flow's one-hour window.

CREATE FLOW UserApiFlow;

CREATE TYPE UserApiUsage (
  userId String key,
  userName String,
  userZip String,
  userLat double,
  userLong double,
  company String,
  api String,
  count int,
  unusual int,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE TYPE UserApiContext (
  userId String key,
  userName String,
  userZip String,
  userLat double,
  userLong double,
  company String,
  count int,
  unusual int,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE WACTIONSTORE UserApiActivity
CONTEXT OF UserApiContext
EVENT TYPES (  UserApiUsage )
PERSIST NONE USING ( );

CREATE JUMPING WINDOW UserApiWindow
OVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY userId;

CREATE STREAM UserApiUsageStream OF UserApiUsage;

CREATE CQ GetUserApiUsage
INSERT INTO UserApiUsageStream
SELECT a.userId, a.userName, a.userZip, z.latVal, z.longVal, a.company,
       a.api, COUNT(a.sobject),
       CASE WHEN COUNT(a.sobject) > 125 THEN 1
            ELSE 0 END,
       CASE WHEN COUNT(a.sobject) > 125 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.sobject),
       FIRST(a.logTime)
FROM UserApiWindow a, MLogZipLookup z
WHERE a.userZip = z.zip
GROUP BY a.userId, a.api HAVING FIRST(a.logTime) IS NOT NULL;

CREATE JUMPING WINDOW UserWindow
OVER UserApiUsageStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY userId;

CREATE CQ GetUserUsage
INSERT INTO UserApiActivity
SELECT a.userId, a.userName, a.userZip, a.userLat, a.userLong,
       a.company, SUM(a.count), SUM(a.unusual),
       CASE WHEN SUM(a.unusual) > 0 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.topObject),
       FIRST(a.logTime)
FROM UserWindow a
GROUP BY a.userId
LINK SOURCE EVENT;

CREATE STREAM UserAlertStream OF Global.AlertEvent;

CREATE CQ SendUserApiAlerts
INSERT INTO UserAlertStream
SELECT 'UserAPIAlert', ''+logTime, 'warning', 'raise',
       'User ' + userName + ' has used api ' + api + ' ' + count + ' times for ' + topObject
FROM UserApiUsageStream
WHERE unusual = 1;

CREATE SUBSCRIPTION UserAlertSub USING WebAlertAdapter( ) INPUT FROM UserAlertStream;

END FLOW UserApiFlow;


CREATE VISUALIZATION MultiLogApp "@FEATURE-DIR@/tql/MultiLogApp_visualization_settings.json";

END APPLICATION MultiLogDashboard;

create dashboard using "@FEATURE-DIR@/tql/MultiLogAppDashboard.json";

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@ @APP_PROPERTY@ USE EXCEPTIONSTORE;

CREATE OR REPLACE STREAM @APP_NAME@_DataStream OF Global.WAEvent;

Create Source @APP_NAME@_Source Using @SOURCE_ADAPTER@ (

) OUTPUT TO @APP_NAME@_DataStream;

CREATE TARGET @APP_NAME@_Target USING @TARGET_ADAPTER@ ( 

) INPUT FROM @APP_NAME@_DataStream;

CREATE OR REPLACE TARGET @APP_NAME@_SysOut USING Global.SysOut ( 
	name: '@APP_NAME@_SysOutWA' 
) INPUT FROM @APP_NAME@_DataStream;

END APPLICATION @APP_NAME@;

DEPLOY APPLICATION @APP_NAME@ IN DEFAULT;
START APPLICATION @APP_NAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using MSSqlReader
(
 Username:'@UserName@',
 Password:'@Password@',
 DatabaseName:'qatest',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'@SourceTable@',
 ConnectionPoolSize:1,
 StartPosition:'EOF'
 ) Output To @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;


 CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@SourceTable@,@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@_stream;

use PosTester;
alter application PosApp;

CREATE source CsvDataSource USING CSVReader (
  directory:'Samples/Customer/PosApp/appData',
  header:Yes,
  wildcard:'posdata.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

end application PosApp;

alter application PosApp recompile;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE OR REPLACE TYPE @appname@CQOUT1_Type (
 companyName java.lang.String,
 merchantId java.lang.String,
 dateTime org.joda.time.DateTime,
 hourValue java.lang.String,
 amount java.lang.String,
 zip java.lang.String,
 FileName java.lang.String);

CREATE SOURCE @parquetsrc@ USING S3Reader (
    bucketname:'',
    objectnameprefix:''
     )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE OR REPLACE STREAM @appname@CQOUT1 OF @appname@CQOUT1_Type;
CREATE OR REPLACE CQ @appname@CQ_PQEvent
INSERT INTO @appname@CQOUT1
    Select
    data.get("companyName").toString(),
    data.get("merchantId").toString(),
    TO_DATE(data.get("dateTime").toString()),
    data.get("hourValue").toString(),
    data.get("amount").toString(),
    data.get("zip").toString(),
    metadata.get("FileName").toString()
    FROM @appname@Stream p;

CREATE OR REPLACE TARGET @avrotarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING AvroFormatter  (
schemaFileName: 'AvroFileSchema'
)
INPUT FROM @appname@CQOUT1;

create Target @jsontarget@ using FileWriter(
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
format using JSONFormatter ()
INPUT FROM @appname@CQOUT1;

create Target @xmltarget@ using FileWriter(
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
format using XMLFormatter (
    rootelement:'',
    elementtuple:'',
    charset:'UTF-8'
)
INPUT FROM @appname@CQOUT1;

create Target @dsvtarget@ using FileWriter(
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
format using DSVFormatter ()
INPUT FROM @appname@CQOUT1;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

--
-- Recovery Test 3
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> JW -> CQ(aggregate) -> WS
--

STOP Recov3Tester.RecovTest3;
UNDEPLOY APPLICATION Recov3Tester.RecovTest3;
DROP APPLICATION Recov3Tester.RecovTest3 CASCADE;
CREATE APPLICATION RecovTest3 RECOVERY 1 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount int,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_INT(TO_DOUBLE(data[7])),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

END APPLICATION RecovTest3;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING MySqlReader
(
  Compression: false,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'mysql://localhost:3306',
  DatabaseName: 'waction',
  Tables: 'waction.test01',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root',
  connectionRetryPolicy:'retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'waction.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true	
) INPUT FROM @APPNAME@_ss;
--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

--
-- Kafka Stream Recovery Test 1 New with internal test data generation
-- Bert Hashemi and Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> KS -> WS

STOP KStreamRecovTester.KStreamRecovTest;
UNDEPLOY APPLICATION KStreamRecovTester.KStreamRecovTest;
DROP APPLICATION KStreamRecovTester.KStreamRecovTest CASCADE;
DROP USER KStreamRecovTester;
DROP NAMESPACE KStreamRecovTester CASCADE;
CREATE USER KStreamRecovTester IDENTIFIED BY KStreamRecovTester;
-- GRANT 'Global:create,drop:deploymentgroup:*' TO USER KStreamRecov1Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecovTester;
CONNECT KStreamRecovTester KStreamRecovTester;

CREATE APPLICATION KStreamRecovTest RECOVERY 5 SECOND INTERVAL;

CREATE or REPLACE TYPE KafkaType(
  value java.lang.Long KEY
);

CREATE SOURCE KafkaSource USING NumberSource (
  lowValue: '1',
  highValue: '1003',
  delayMillis: '10',
  delayNanos: '0',
  repeat: 'false'
 )
OUTPUT TO NumberSourceOut;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaStream OF KafkaType persist using KafkaPropset;

CREATE OR REPLACE CQ KafkaStreamPopulate
INSERT INTO KafkaStream
SELECT data[1]
FROM NumberSourceOut;

CREATE WACTIONSTORE Wactions CONTEXT of KafkaType
@PERSIST-TYPE@

CREATE CQ WactionsPopulate
INSERT INTO Wactions
SELECT * FROM KafkaStream;

END APPLICATION KStreamRecovTest;

STOP banker.bankApp;
UNDEPLOY APPLICATION banker.bankApp;
DROP APPLICATION banker.bankApp cascade;

CREATE APPLICATION bankApp;


CREATE source wsSource USING FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;


CREATE TYPE wsData
(
bankID Integer KEY,
bankName String
);


CREATE WACTIONSTORE oneWS
CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@

CREATE CQ oneWSCQ
INSERT INTO oneWS
SELECT TO_INT(data[0]),data[1] FROM QaStream
LINK SOURCE EVENT;

END APPLICATION bankApp;
deploy application bankapp;
start application bankapp;

DROP APPLICATION ns1.OPExample cascade;
DROP NAMESPACE ns1 cascade;
CREATE OR REPLACE NAMESPACE ns1;
USE ns1;
CREATE APPLICATION OPExample;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'PosDataPreview.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
)
OUTPUT TO CsvStream;
 
CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);

CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) 
QUERY (keytomap:'merchantId') 
OF MerchantHourlyAve;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
  TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
  DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
  TO_DOUBLE(data[7]) as amount,
  TO_INT(data[9]) as zip
FROM CsvStream;
 
CREATE CQ cq2
INSERT INTO SendToOPStream
SELECT makeList(dateTime) as dateTime,
  makeList(zip) as zip
FROM PosDataStream;
 
CREATE TYPE ReturnFromOPStream_Type ( time DateTime , val Integer );
CREATE STREAM ReturnFromOPStream OF ReturnFromOPStream_Type;

CREATE TARGET OPExampleTarget 
USING FileWriter (filename: 'OPExampleOut') 
FORMAT USING JSONFormatter() 
INPUT FROM ReturnFromOPStream;

CREATE OPEN PROCESSOR testOp USING Global.TupleConverter ( lastItemSeen: 0, ahead: 1 )
INSERT INTO ReturnFromOPStream FROM SendToOPStream ENRICH WITH HourlyAveLookup;
 
END APPLICATION OPExample;

CREATE APPLICATION @APPNAME@ @RECOVERY@;

CREATE FLOW @APPNAME@AgentFlow;
CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream;
END FLOW @APPNAME@AgentFlow;

CREATE FLOW @APPNAME@serverFlow;
CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING DSVFormatter (
members:'data')
INPUT FROM @APPNAME@_Stream;
END FLOW @APPNAME@serverFlow;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@ with @APPNAME@AgentFlow in Agents, @APPNAME@ServerFlow in default;
start application @APPNAME@;

--
-- Recovery Test 36 with two sources, two jumping attribute windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W/p -> CQ1 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS
--

STOP Recov36Tester.RecovTest36;
UNDEPLOY APPLICATION Recov36Tester.RecovTest36;
DROP APPLICATION Recov36Tester.RecovTest36 CASCADE;
CREATE APPLICATION RecovTest36 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION RecovTest36;

STOP APPLICATION MysqlToDbApp;
UNDEPLOY APPLICATION MysqlToDbApp;
DROP APPLICATION MysqlToDbApp CASCADE;


CREATE APPLICATION MysqlToDbApp RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE MysqlSource USING MysqlReader
(
  Username: '',
  Password: '',
  Tables: '',
  ConnectionURL: '',
  Password_encrypted: 'false',
  connectionRetryPolicy: 'retryInterval=30, maxRetries=3'
)
OUTPUT TO MysqlReaderOut;


CREATE TARGET DbTarget USING DatabaseWriter
(
  Username: '',
  Password: '',
  Tables: '',
  ConnectionURL: '',
  Password_encrypted: 'false',
  connectionRetryPolicy: 'retryInterval=30, maxRetries=3'
)
INPUT FROM MysqlReaderOut;




END APPLICATION MysqlToDbApp;
DEPLOY APPLICATION MysqlToDbApp;
START APPLICATION MysqlToDbApp;

stop application recoveryTestAgent.CSV;
undeploy application recoveryTestAgent.CSV;
drop application recoveryTestAgent.CSV cascade;

create application CSV
RECOVERY 5 SECOND INTERVAL;

CREATE FLOW AgentFlow;
create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails-recovery.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStream;
END FLOW AgentFlow;

CREATE FLOW ServerFlow;
CREATE TYPE UserDataType
(
  UserId String KEY,
  UserName String
);

CREATE STREAM UserDataStream OF UserDataType PARTITION BY UserId;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  data[0],
        data[1]
FROM CsvStream;


CREATE WACTIONSTORE UserActivityInfo
CONTEXT OF UserDataType
EVENT TYPES ( UserDataType )
@PERSIST-TYPE@

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction
INSERT INTO UserActivityInfo
SELECT * FROM UserDataStream
LINK SOURCE EVENT;
END FLOW ServerFlow;

END APPLICATION CSV;
DEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;
START CSV;

drop application Component_Disappear cascade;
drop cq Test_Subquery_Cq;
drop stream Test_Subquery_Cq_out;

CREATE APPLICATION Component_Disappear;

create or replace CQ Test_Subquery_Cq
insert into Test_Subquery_Cq_out
SELECT f.topic as topic, sum(f.rawdatacount) as TotalLast24hour, B.rawdatacount as TotalLast1hour FROM JUMP_WND_1EVT_1MIN h
   join SLIDE_WND_HOURLYTOTALS_KAFKADATA_FILE f on 1=1
   join (SELECT rawdatacount, topic,timerange from  ET_HOURLYTOTALS_KAFKADATA_FILE,JUMP_WND_1EVT_30SEC where timerange = DHOURS(DNOW())-1) B on B.topic=f.topic
   Group by f.topic;

END APPLICATION Component_Disappear;

drop namespace stripe cascade force;
create namespace stripe;
use stripe;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ RECOVERY 30 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;

CREATE FLOW @AppName@_SourceFlow;

CREATE SOURCE @srcName@ USING StripeReader ( 
 PollingInterval: '5m', 
  AccountId: '', 
  ApiKey: '@srcurl@', 
  StartPosition: '%=-1', 
  ThreadPoolCount: '10', 
  ConnectionPoolSize: '20', 
  RefreshToken: '', 
  useConnectionProfile: false,
  Mode: 'Automated', 
  IncrementalLoadMarker: 'Created', 
  ApiKey_encrypted: 'false', 
  ConnectedAccount: 'false', 
  ClientSecret: '', 
  ClientId: '', 
  Tables: 'Charges', 
  AuthMode: 'ApiKey', 
  MigrateSchema: true ) 
  OUTPUT TO @outstreamname@;

END FLOW @AppName@_SourceFlow;

CREATE TARGET Tgtstripebigquerysanity USING Global.BigQueryWriter ( 
  projectId: '@projectId@',
  batchPolicy: 'eventcount:10000,interval:2', 
  streamingUpload: 'true', 
  Mode: 'MERGE', 
  CDDLOptions: '{\"CreateTable\":{\"action\":\"IgnoreIfExists\",\"options\":[{\"CreateSchema\":{\"action\":\"IgnoreIfExists\"}}]}}', 
  ServiceAccountKey: 'UploadedFiles/google-gcs-test.json', 
  Tables: '%,rishi.%' ) 
INPUT FROM @outstreamname@;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

--
-- Recovery Test 38 with two sources, two jumping time-count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5t3W/p -> CQ1 -> WS
--   S2 -> Jc6t4W/p -> CQ2 -> WS
--

STOP Recov38Tester.RecovTest38;
UNDEPLOY APPLICATION Recov38Tester.RecovTest38;
DROP APPLICATION Recov38Tester.RecovTest38 CASCADE;
CREATE APPLICATION RecovTest38 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Rows3Seconds
OVER DataStream1 KEEP 5 ROWS WITHIN 3 SECOND
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Rows4Seconds
OVER DataStream2 KEEP 6 ROWS WITHIN 4 SECOND
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataStream5Rows3Seconds
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Rows3Seconds p
GROUP BY p.merchantId;

CREATE CQ DataStream6Rows4Seconds
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Rows4Seconds p
GROUP BY p.merchantId;

END APPLICATION RecovTest38;

IMPORT static com.webaction.runtime.converters.DateConverter.*;

UNDEPLOY APPLICATION admin.SQLMXReaderApp;
DROP APPLICATION admin.SQLMXReaderApp cascade;

CREATE APPLICATION SQLMXReaderApp;
create source SQMXSource using HPNonStopSQLMXReader (
	portno:2020,
	ipaddress:'10.10.196.122',
	Name:'intg',
	AuditTrails:'parallel',
	AgentPortNo:8012,
	AgentIpAddress:'10.10.197.116', 
	Tables:'watest.wasch.sqlmxtest1;watest.wasch.sqlmxtest2') output to CDCStream,
	SQLMXMATStream MAP (table:'WATEST.WASCH.SQLMXTEST2');


CREATE TYPE SQLMXTEST2Data(
    C0 Integer,
    C1 String,
    C2 Short,
    OPR String,
    TABLENAME String,
    AUXNAME String
);

CREATE STREAM SQLMXTEST2Stream OF SQLMXTEST2Data;


CREATE JUMPING WINDOW SQLMXDataWindow
OVER SQLMXTEST2Stream KEEP 4 ROWS
PARTITION BY OPR;


CREATE CQ ToSQLMXData
INSERT INTO SQLMXTEST2Stream
SELECT TO_INT(data[0]),
	   data[1],
       TO_SHORT(data[2]),
       META(x,"OperationName").toString(),
       META(x, "TableName").toString(),
       META(x,"AuditTrailName").toString()
FROM CDCStream x
WHERE not(META(x,"OperationName").toString() = "BEGIN") AND not(META(x,"OperationName").toString() = "COMMIT") AND not(META(x, "TableName").toString() is null) 
AND META(x, "TableName").toString() = "WATEST.WASCH.SQLMXTEST1" AND META(x, "AuditTrailName").toString() = "MAT";


--CREATE TARGET SQLMXSYSOUT using SysOut(name:sqlmx) INPUT FROM CDCStream;
CREATE TARGET SQLMXMAT USING LogWriter(
  name:SQLMXReaderAppMAT,
filename:'@FEATURE-DIR@/logs/sqlmxmat.log'
--  filename:'mat.log'
) INPUT FROM SQLMXMATStream;

CREATE TARGET SQLMXAUX01 USING LogWriter(
  name:SQLMXReaderAppMAT1,
filename:'@FEATURE-DIR@/logs/sqlmxmat1.log'
--  filename:'aux1.log'
) INPUT FROM SQLMXTEST2Stream;


END APPLICATION SQLMXReaderApp;
deploy application SQLMXReaderApp in default;

create Target @TARGET_NAME@ using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'%@metadata(TableName)%',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
		rolloverpolicy:'filesize:10M',
		compressiontype: 'false'
)
format using DSVFormatter (
)
input from @STREAM@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@ MAP (table: 'QATEST.SMFTEST6')
SELECT NUM_COL,CHAR_COL,VARCHAR2_COL,LONG_COL,DATE_COL,TIMESTAMP_COL where TO_INT(NUM_COL) > 1;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

Create Source dataGenSrc Using MSSqlReader
(
 Username:'qatest',
 Password:'w3b@ct10n',
 DatabaseName:'qatest',
 ConnectionURL:'localhost:1433',
 Tables:'@tableNames@', 
 ConnectionPoolSize:1,
 StartPosition:'EOF'
 )
 Output To LCRStream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;

END APPLICATION DataGenSampleApp;

\create Target @TARGET_NAME@ using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (

)
input from @STREAM@;

create application KinesisTest 
 RECOVERY 10 SECOND INTERVAL
;
create source CSVSource using FileReader (
	directory:'/home/dz/src/product/Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  companyName String,
  merID String,
  primNum String,
  posDataCode int,
  dateTime String,
  expDate String,
  curCode String,
  Amnt String,
  termId String,
  zip String,
  city String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],data[2],TO_INT(data[3]),
       data[4],data[5],data[6],
       data[7],data[8],
       data[9],data[10]
FROM CsvStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM'
)
format using DSVFormatter (
)
input from TypedCSVStream;
end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

stop ORAToBigquery;
undeploy application ORAToBigquery;
drop application ORAToBigquery cascade;
CREATE APPLICATION ORAToBigquery RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE Rac11g USING OracleReader  ( 
  SupportPDB: false,
  SendBeforeImage: true,
  ReaderType: 'LogMiner',
  CommittedTransactions: false,
  FetchSize: 1,
  Password: 'manager',
  DDLTracking: 'true',
  StartTimestamp: 'null',
  OutboundServerProcessName: 'WebActionXStream',
  OnlineCatalog: true,
  ConnectionURL: '192.168.33.10:1521/XE',
  SkipOpenTransactions: false,
  Compression: false,
  QueueSize: 40000,
  RedoLogfiles: 'null',
  Tables: 'SYSTEM.POSTABLE',
  Username: 'ravi',
  FilterTransactionBoundaries: true,
  adapterName: 'OracleReader',
  XstreamTimeOut: 600,
  connectionRetryPolicy: 'timeOut=30, retryInterval=60, maxRetries=3'
 ) 
OUTPUT TO DataStream;
CREATE OR REPLACE TARGET Target1 USING SysOut ( 
  name: "dstream"
 ) 
INPUT FROM DataStream;
CREATE OR REPLACE TARGET Target2 using BigqueryWriter(
  BQServiceAccountConfigurationPath:"/Users/ravipathak/Downloads/big-querytest-1963ae421e90.json",
  projectId:"big-querytest",
  Tables: "SYSTEM.GGAUTHORIZATIONS,testing1.ggauthorisation",
  parallelismCount: 2,
  BatchPolicy: "eventCount:100000,Interval:0")
INPUT FROM DataStream;
END APPLICATION ORAToBigquery;
deploy application ORAToBigquery;
start ORAToBigquery;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY;

Create Source @APPNAME@_src Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1000,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
   _h_ReturnDateTimeAs:'ZonedDateTime'
) Output To @APPNAME@_stream;

create Target @APPNAME@_tgt using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_stream;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;


create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;

deploy application DBRTOCW on ANY in default;

start application DBRTOCW;

stop PatternMatching1.CSV;
undeploy application PatternMatching1.CSV;
drop application PatternMatching1.CSV cascade;

create application CSV;

create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'posdata.csv',
  columndelimiter:',',
  positionByEOF:false
)
OUTPUT TO CsvStream;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  TO_STRING(data[0]) as temp1,
        TO_STRING(data[1]) as temp2,
        TO_DOUBLE(data[2]) as temp3
FROM CsvStream;

-- scenario 1.1 check pattern using PosApp data with alteration and quantifier(0 or 1)
CREATE CQ TypeConversionPosAppCQ1
INSERT INTO TypedStream1
SELECT A.temp1 as typeduserid,
       B.temp2 as typedtemp1,
       C.temp3 as typedtemp2
from UserDataStream
MATCH_PATTERN A | B ? C
define A = UserDataStream(temp1 = 'COMPANY 4'),
       B = UserDataStream(temp2 = 'OFp6pKTMg26n1iiFY00M9uSqh9ZfMxMBRf1'),
       C = UserDataStream(temp3 = 520236368216865619l)
PARTITION BY temp1;

-- scenario 1.2 check pattern using PosApp data with permutation and quantifier(0 or 1)
CREATE CQ TypeConversionPosAppCQ2
INSERT INTO TypedStream2
SELECT A.temp1 as typeduserid,
       B.temp2 as typedtemp1,
       C.temp3 as typedtemp2
from UserDataStream
MATCH_PATTERN A & B ? C
define A = UserDataStream(temp1 = 'COMPANY 100'),
       B = UserDataStream(temp2 = 'IYuqAbAQ07NS3lZO74VGPldfAUAGKwzR2k3'),
       C = UserDataStream(temp3 = 6388500771470313223l)
PARTITION BY temp3;

CREATE WACTIONSTORE UserActivityInfoPosApp1
CONTEXT OF TypedStream1_Type
EVENT TYPES ( TypedStream1_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoPosApp2
CONTEXT OF TypedStream2_Type
EVENT TYPES ( TypedStream2_Type )
@PERSIST-TYPE@

create Target t2 using SysOut(name:AgentTyped) input from TypedStream1;
create Target t3 using SysOut(name:AgentTyped1) input from TypedStream2;

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction1
INSERT INTO UserActivityInfoPosApp1
SELECT * FROM TypedStream1
LINK SOURCE EVENT;

CREATE CQ UserWaction2
INSERT INTO UserActivityInfoPosApp2
SELECT * FROM TypedStream2
LINK SOURCE EVENT;

end application CSV;
deploy application csv;
start csv;

CREATE APPLICATION @APPNAME@;

CREATE TYPE @APPNAME@type1 (
 companyName java.lang.String,
 merchantId java.lang.String,
 city java.lang.String);

CREATE STREAM @APPNAME@TypedStream OF @APPNAME@type1 PARTITION BY city;

CREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (
  wildcard: '',
  positionByEOF: false,
  directory: ''
  )
PARSE USING DSVParser (
header:'true'
)
OUTPUT TO @APPNAME@Stream;

CREATE OR REPLACE CQ @APPNAME@CQ
INSERT INTO @APPNAME@TypedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantID,
TO_STRING(data[10]) as city
FROM @APPNAME@Stream;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt1 USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@TypedStream;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt2 USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@TypedStream;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt3 USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@TypedStream;

END APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

CREATE  SOURCE @QuerySource@1 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@1;
create Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;
 CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@1;

CREATE  SOURCE @QuerySource@2 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@2;
create Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;
 CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@2;

CREATE  SOURCE @QuerySource@3 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@3;
create Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;
 CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@3;

CREATE  SOURCE @QuerySource@4 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@4;
create Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;
 CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@4;

CREATE  SOURCE @QuerySource@5 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@5;
create Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;
 CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@5;

CREATE  SOURCE @QuerySource@6 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@6;
create Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;
 CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@6;

CREATE  SOURCE @QuerySource@7 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@7;
create Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;
 CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@7;

CREATE  SOURCE @QuerySource@8 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@8;
create Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;
 CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@8;

CREATE  SOURCE @QuerySource@9 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@9;
create Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;
 CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@9;

CREATE  SOURCE @QuerySource@10 USING DatabaseReader  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@10;
create Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;
 CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (
  ConnectionURL: '@CONN_URL@',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@10;
end application @APPNAME@;

--
-- Recovery Test 7
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov7Tester.RecovTest7;
UNDEPLOY APPLICATION Recov7Tester.RecovTest7;
DROP APPLICATION Recov7Tester.RecovTest7 CASCADE;
CREATE APPLICATION RecovTest7 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM wacStream OF WactionType;

CREATE CQ InsertWacStream
INSERT INTO wacStream
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE WINDOW waWindow
OVER wacStream KEEP WITHIN 1 SECOND ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT *
FROM waWindow
LINK SOURCE EVENT;

END APPLICATION RecovTest7;

--
-- Crash Recovery Test 3 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW -> CQ(aggregate) -> WS
--

STOP APPLICATION N2S2CR3Tester.N2S2CRTest3;
UNDEPLOY APPLICATION N2S2CR3Tester.N2S2CRTest3;
DROP APPLICATION N2S2CR3Tester.N2S2CRTest3 CASCADE;
CREATE APPLICATION N2S2CRTest3 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest3;

CREATE SOURCE CsvSourceN2S2CRTest3 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN2S2CRTest3;

CREATE FLOW DataProcessingN2S2CRTest3;

CREATE TYPE WactionTypeN2S2CRTest3 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount int,
  city String
);

CREATE STREAM DataStream OF WactionTypeN2S2CRTest3;

CREATE CQ CsvToDataN2S2CRTest3
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_INT(TO_DOUBLE(data[7])),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN2S2CRTest3 CONTEXT OF WactionTypeN2S2CRTest3
EVENT TYPES ( WactionTypeN2S2CRTest3 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN2S2CRTest3
INSERT INTO WactionsN2S2CRTest3
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

END FLOW DataProcessingN2S2CRTest3;

END APPLICATION N2S2CRTest3;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d';

CREATE OR REPLACE SOURCE @SOURCE@1 USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
 )
OUTPUT TO @STREAM@1;

CREATE OR REPLACE SOURCE @SOURCE@2 USING PostgreSQLReader  (
    ReplicationSlotName:'test_slot',
    FilterTransactionBoundaries:'true',
    Username:'@SOURCE_USER@',
    Password_encrypted:false,
    ConnectionURL:'@CONNECTION_URL@',
    adapterName:'PostgreSQLReader',
    ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
    Password:'@SOURCE_PASS@',
    Tables:'@SOURCE_TABLE@',
    ExcludedTables:'public.chkpoint',
 )
OUTPUT TO @STREAM@1;

CREATE OR REPLACE SOURCE @SOURCE@3 USING MySQLReader (
    Username: '@READER-UNAME@',
    Password: '@READER-PASSWORD@',
    ConnectionURL: '@CDC-READER-URL@',
    Tables: @WATABLES@,
    sendBeforeImage:'true',
    FilterTransactionBoundaries: 'true',
    ExcludedTables: 'waction.CHKPOINT'
)
OUTPUT TO @STREAM@1;

 -- CREATE OR REPLACE SOURCE @SOURCE@4 USING MariaDbXpandReader
 -- (
 -- Username: '@READER-UNAME@',
 -- Password: '@READER-PASSWORD@',
 -- ConnectionURL: '@CDC-READER-URL@',
 -- Tables: @WATABLES@,
 -- sendBeforeImage:'true',
 -- FilterTransactionBoundaries: 'true',
 -- ExcludedTables: 'test.CHKPOINT'
 -- )
 -- OUTPUT TO @STREAM@1;

CREATE OR REPLACE TARGET @TARGET@1 USING SnowflakeWriter (
  ConnectionURL: '@SNOWFLAKE-URL@',
  username: '@SNOWFLAKE-USERNAME@',
  password: '@SNOWFLAKE-PASSWORD@',
  appendOnly:'false',
  uploadPolicy: 'eventcount:1,interval:5m',
  Tables: '@TARGET-TABLES@'
 )
INPUT FROM @STREAM@1;

CREATE OR REPLACE TARGET @TARGET@2 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@WATABLES@',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true
)
INPUT FROM @STREAM@1;



END APPLICATION @APPNAME@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING MySqlReader
(
  Compression: false,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'mysql://localhost:3306',
  DatabaseName: 'waction',
  Tables: 'waction.test01',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root',
  connectionRetryPolicy:'retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'waction.test01,@DATASET@.% KEYCOLUMNS(id,col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;

--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;


CREATE OR REPLACE SOURCE @APPNAME@CDC_Source1 USING MySQLReader( 
  Compression: true,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',
  Tables: 'waction.crash_type',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root'
 ) 
OUTPUT TO @APPNAME@AppStream1;


CREATE OR REPLACE TARGET @APPNAME@sap_target USING DatabaseWriter( 
  DatabaseProviderType:'SAPHANA',
  ConnectionRetryPolicy: 'retryInterval=30,maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'SYSTEM',
  Password_encrypted: 'false',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:sap://10.77.21.116:39013/?databaseName=striim&currentSchema=QA',
  Tables: 'waction.crash_type,QA.CRASH_TYPES',
  adapterName: 'DatabaseWriter',
  --IgnorableExceptionCode: '',
  Password: 'Striim_SAP@123'
 ) 
INPUT FROM @APPNAME@AppStream1;


create or replace target @APPNAME@sys_tgt using sysout(
name:Foo2
)input from @APPNAME@AppStream1;

END APPLICATION @APPNAME@;

deploy application @APPNAME@;
start application @APPNAME@;

--
-- Recovery Test 36 with two sources, two jumping attribute windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W/p -> CQ1 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS
--

STOP NameW03.W03;
UNDEPLOY APPLICATION NameW03.W03;
DROP APPLICATION NameW03.W03 CASCADE;
CREATE APPLICATION W03 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  wildcard:'Canon1000.csv',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[2],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[3])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[2],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[3])
FROM CsvStream2;

CREATE JUMPING WINDOW JWc51uW03
OVER DataStream1 KEEP 51 ROWS;

CREATE JUMPING WINDOW JWc101uW03
OVER DataStream2 KEEP 101 ROWS;

CREATE WACTIONSTORE WactionsW03 CONTEXT OF WactionData
EVENT TYPES ( CsvData KEY(merchantId) )
@PERSIST-TYPE@

CREATE CQ JWc51uW03_to_WactionsW03
INSERT INTO WactionsW03
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM JWc51uW03 p
GROUP BY p.merchantId;

CREATE CQ JWc101uW03_to_WactionsW03
INSERT INTO WactionsW03
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM JWc101uW03 p
GROUP BY p.merchantId;

END APPLICATION W03;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @APPNAME@_src Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
   _h_ReturnDateTimeAs:'ZonedDateTime'
) Output To @APPNAME@_stream;

create Target @APPNAME@_tgt using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_stream;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@ USING Ojet
(
 Username:'@OJET-UNAME@',
Password:'@OJET-PASSWORD@',
ConnectionURL:'@OCI-URL@',
Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectURL@',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop application PCAPTester.PCAPTest;
undeploy application PCAPTester.PCAPTest;
drop application PCAPTester.PCAPTest cascade;

CREATE APPLICATION PCAPTest;


    CREATE OR REPLACE TYPE PCAPData (
      ts DateTime,
      srcIp String,
      srcPort String,
      dstIp String,
      dstPort String,
      connection String
    );
    
    CREATE OR REPLACE SOURCE PCAPSource USING PCAPReader ( 
      snaplen: '65536',
      wildcard: false,
      directory: '@TEST-DATA-PATH@',
      file: 'pcapTest.pcap',
      library: '/usr/lib/libpcap.dylib',
      live: false 
     )
    OUTPUT TO PCAPOut;
    
    CREATE OR REPLACE STREAM PCAPDataStream OF PCAPData;

    CREATE OR REPLACE CQ GetPCAPData
    INSERT INTO PCAPDataStream
    SELECT TO_DATE(ts), 
           srcAddress,
           TO_STRING(srcPort),
           dstAddress,
           TO_STRING(dstPort),
           transportType + '/' + srcAddress + ':' + srcPort + '/' + dstAddress + ':' + dstPort
    FROM PCAPOut;
    



END APPLICATION PCAPTest;

create application FileXML;
create source XMLSource using FileReader (
	Directory:'@TEST-DATA-PATH@',
	WildCard:'books.xml',
	positionByEOF:false
)
parse using XMLParser (
	RootNode:'/catalog/book'
)
OUTPUT TO XmlStream;

-- Below Sysout is added to test DEV-23437.  Not directly validated in the test except the App should not crash with sysout target
CREATE TARGET XMLEventSYSout USING sysout  (
name: 'XMLEventSYSoutOut' )
INPUT FROM XmlStream;

create Target XMLDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/xmldata') input from XmlStream;
end application FileXML;

STOP virtualTester.VirtualApp;
UNDEPLOY APPLICATION virtualTester.VirtualApp;
DROP APPLICATION virtualTester.VirtualApp cascade;

CREATE APPLICATION VirtualApp;


CREATE OR REPLACE SOURCE wsSource USING FileReader
(
  directory:'@TEST-DATA-PATH@',
  wildcard:'AdhocQueryData2.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser
(
   header:True,
   columndelimiter:',',
   trimquote:false
)OUTPUT TO QaStream;

CREATE OR REPLACE Target TheData using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/QaStream.log') input from QaStream;

CREATE OR REPLACE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE OR REPLACE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'AdhocQueryData.csv'
)
parse using DSVParser
(
  header:'yes',
  columndelimiter: '	',
  trimquote:false

)QUERY (keytomap:'zip') OF USAddressData;

Create OR REPLACE TYPE wsData(
  CompanyNum String,
  CompanyName String KEY,
  CompanyCode int,
  Zip String
);


CREATE OR REPLACE STREAM wsStream OF wsData;

--Select data from QaStream and insert into wsStream

CREATE OR REPLACE CQ csvTowsData
INSERT INTO wsStream
SELECT data[0],
       data[1],
       TO_INT(data[3]),
       data[9]
FROM QaStream;


CREATE OR REPLACE WACTIONSTORE oneWS CONTEXT OF wsData
  EVENT TYPES (wsData );


CREATE OR REPLACE CQ wsToWaction
  INSERT INTO oneWS
  SELECT * FROM wsStream
  LINK SOURCE EVENT;

END APPLICATION VirtualApp;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE TYPE @APPNAME@oracleType (
 companyName java.lang.String,
 merchantId java.lang.String);

CREATE SOURCE @APPNAME@_Orcl USING OracleReader (
  ConnectionURL: '',
  Password: '',
  Tables: '',
  Username: ''
  )
OUTPUT TO @APPNAME@OracleOut;

CREATE OR REPLACE STREAM @APPNAME@TypedStream OF @APPNAME@oracleType;

CREATE OR REPLACE CQ @APPNAME@CQOut
INSERT INTO @APPNAME@TypedStream
SELECT
TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantID
FROM @APPNAME@OracleOut o;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@TypedStream;

END APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;
CREATE  SOURCE @TableSourceName@ USING DatabaseReader  ( 
  ConnectionURL: '@SourceConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  Tables: '@SourceTable@',
  ReplicationSlotName: 'null'
 ) OUTPUT TO @SRCTableINPUTSTREAM@;

 CREATE  SOURCE @QuerySourceName@ USING DatabaseReader  ( 
  ConnectionURL: '@SourceConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  Query: '@SourceQuery@',
  ReplicationSlotName: 'null'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@;

CREATE  TARGET @TabletargetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
 ) 
 INPUT FROM @SRCTableINPUTSTREAM@;

 CREATE  TARGET @QuerytargetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

CREATE  SOURCE @QuerySource@1 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@1;
create Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;
 CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@1;

CREATE  SOURCE @QuerySource@2 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@2;
create Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;
 CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@2;

CREATE  SOURCE @QuerySource@3 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@3;
create Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;
 CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@3;

CREATE  SOURCE @QuerySource@4 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@4;
create Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;
 CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@4;

CREATE  SOURCE @QuerySource@5 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@5;
create Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;
 CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@5;

CREATE  SOURCE @QuerySource@6 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@6;
create Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;
 CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@6;

CREATE  SOURCE @QuerySource@7 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@7;
create Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;
 CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@7;

CREATE  SOURCE @QuerySource@8 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@8;
create Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;
 CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@8;

CREATE  SOURCE @QuerySource@9 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@9;
create Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;
 CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@9;

CREATE  SOURCE @QuerySource@10 USING DatabaseReader  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  DatabaseName:'qatest',
  StartPosition:'EOF',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@10;
create Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;
 CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (
  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,
  Username: 'qatest',
  Password:'w3b@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@10;
end application @APPNAME@;

stop application Postgres_To_PostgresApp;
undeploy application Postgres_To_PostgresApp;
drop application Postgres_To_PostgresApp cascade;
CREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;
create type pkFlag_type
(
TableName String,
PK_UPDATE String,
OperationName String
);
CREATE STREAM Postgres_TypedStream of pkFlag_type;
CREATE OR REPLACE SOURCE Postgres_Src USING PostgreSQLReader  ( 
  ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgres_src'
 ) 
OUTPUT TO Postgres_Change_Data_Stream;
create CQ Cqfilter 
insert into Postgres_TypedStream
select 
META(u,'TableName').toString(),
META(u,'PK_UPDATE').toString(),
META(u,'OperationName').toString()
from Postgres_Change_Data_Stream u;
CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'postgres_PK_Out'
 ) INPUT FROM Postgres_TypedStream;
CREATE  TARGET Postgres_FW USING FileWriter  ( 
  filename: 'Postgres_PKOut.log',
  directory: '/Users/jenniffer/Product2/IntegrationTests/target/test-classes/testNG/PostgreSQLReader/logs'
 ) 
FORMAT USING DSVFormatter  (  ) 
INPUT FROM Postgres_TypedStream;
end application Postgres_To_PostgresApp;
deploy application Postgres_To_PostgresApp;
start Postgres_To_PostgresApp;

stop Postgres_SQLDBWHEventTableApp;
undeploy application Postgres_SQLDBWHEventTableApp;
drop application Postgres_SQLDBWHEventTableApp cascade;
CREATE APPLICATION Postgres_SQLDBWHEventTableApp;

CREATE OR REPLACE SOURCE Postgres_Src1 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgres_src1',
  ExcludedTables: ''
 ) 
OUTPUT TO data_stream1;

CREATE OR REPLACE SOURCE Postgres_Src2 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgres_src2',
  ExcludedTables: ''
 ) 
OUTPUT TO data_stream2;

CREATE OR REPLACE SOURCE Postgres_Src3 USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgres_src2',
  ExcludedTables: ''
 ) 
OUTPUT TO data_stream3;

Create Type EventType (
ID int,
NAME string,
COMPANY string
);

CREATE STREAM insertData1  of EventType;
CREATE STREAM deleteData1 of EventType;
CREATE STREAM joinData1 of EventType;
CREATE STREAM joinData2 of EventType;
CREATE STREAM deleteData2 of EventType;
CREATE STREAM OutStream of EventType;

CREATE CQ cq1 INSERT INTO insertData1  SELECT TO_INT(data[0]),data[1],data[2] FROM data_stream1;

CREATE CQ cq2 INSERT INTO deleteData1 SELECT TO_INT(data[0]),data[1],data[2] FROM data_stream2;

CREATE CQ cq3 INSERT INTO joinData1 SELECT TO_INT(data[0]),data[1],data[2] FROM data_stream3;

CREATE JUMPING WINDOW DataWin1 OVER deleteData1 KEEP 1 ROWS;

CREATE CQ TypedEventKeyCQ6 INSERT INTO deleteData2 SELECT * from DataWin1;

CREATE JUMPING WINDOW DataWin2 OVER joinData1 KEEP 1 ROWS;

CREATE CQ TypedEventKeyCQ5 INSERT INTO joinData2 SELECT * from DataWin2;

CREATE EVENTTABLE ETABLE1 using STREAM ( NAME: 'insertData1 ' )
--DELETE using STREAM ( NAME: 'deleteData1')
QUERY (keytomap:"ID", persistPolicy: 'true') OF EventType;

CREATE CQ cq4 INSERT INTO OutStream SELECT B.ID,B.NAME,B.COMPANY FROM joinData2 A, ETABLE1 B where A.ID=B.ID;

CREATE TARGET EventTableFW USING FileWriter
(filename:'BasicPostgres_SQLDBWHEventTableApp_RT.log',
 rolloverpolicy: 'EventCount:1000000')
FORMAT USING DSVFormatter () INPUT FROM OutStream;

create target Target_Azure using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'STRIIM',
        password: 'W3b@ct10n',
        AccountName: 'striimqatestdonotdelete',
        accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables:'dbo.autotest01',
        uploadpolicy:'eventcount:0,interval:0s'
) INPUT FROM OutStream;

END APPLICATION Postgres_SQLDBWHEventTableApp;
deploy application Postgres_SQLDBWHEventTableApp in default;
start Postgres_SQLDBWHEventTableApp;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade; 

create application @APPNAME@;

create or replace type @APPNAME@emp_type(
Sno integer,
Empname string,
Doj string,
Country string,
CompanyName string
);

CREATE OR REPLACE SOURCE @APPNAME@File_SOURCE1 using Filereader(
	directory:'@DIRECTORY@',
  wildcard:'File_empdata.csv',
  positionByEOF:false
)parse using dsvParser(
    header:'yes'
)
OUTPUT TO @APPNAME@File_Stream1,
OUTPUT TO @APPNAME@File_Stream1_automap MAP(filename:'File_empdata.csv');

CREATE OR REPLACE SOURCE @APPNAME@Init_Source1 USING DatabaseReader  ( 
  Username: '@SRC-USER@',
  Password_encrypted: false,
  ConnectionURL: '@SRC-URL@',
  Tables: 'QATEST.EMP_INIT',
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SRC-PASS@'
 ) 
OUTPUT TO 	@APPNAME@InitialLoad_Stream1;

CREATE OR REPLACE SOURCE @APPNAME@CDC_Source1 USING OracleReader  ( 
  StartTimestamp: 'null',
  SupportPDB: false,
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  FetchSize: 1,
  DDLCaptureMode: 'All',
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@SRC-URL@',
  Tables: 'QATEST.EMP_CDC',
  adapterName: 'OracleReader',
  Password: '@SRC-USER@',
  TransactionBufferType: 'Memory',
  DictionaryMode: 'OnlineCatalog',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SRC-USER@',
  TransactionBufferSpilloverSize: '1MB',
  compression: true,
  OutboundServerProcessName: 'WebActionXStream'
 ) 
OUTPUT TO @APPNAME@CDC_Stream1 ;

create or replace stream @APPNAME@FileSource_cdc_init_TypedStream of @APPNAME@emp_type;
create or replace cq @APPNAME@file_typed_streamcq 
insert into @APPNAME@FileSource_cdc_init_TypedStream 
SELECT to_int(data[0]),
data[1],
data[2],
data[3],
data[4]
from @APPNAME@File_Stream1;

create or replace cq @APPNAME@cdc_typed_streamcq 
insert into @APPNAME@FileSource_cdc_init_TypedStream 
SELECT to_int(data[0]),
data[1],
data[2],
data[3],
data[4]
from @APPNAME@CDC_Stream1;

create or replace cq @APPNAME@init_typed_streamcq 
insert into @APPNAME@FileSource_cdc_init_TypedStream 
SELECT to_int(data[0]),
data[1],
data[2],
data[3],
data[4]
from @APPNAME@InitialLoad_Stream1;

CREATE OR REPLACE TARGET @APPNAME@sap_target1 USING DatabaseWriter  ( 
  DatabaseProviderType:'SAPHANA',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT-USER@',
  Password_encrypted: 'false',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TGT-URL@',
  Tables: 'QA.FILE_EMP',
  adapterName: 'DatabaseWriter',
  IgnorableExceptionCode: '301',
  Password: '@TGT-PASS@'
 ) 
INPUT FROM @APPNAME@File_Stream1_automap;

CREATE OR REPLACE TARGET @APPNAME@sap_target2 USING DatabaseWriter  ( 
  DatabaseProviderType:'SAPHANA',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT-USER@',
  Password_encrypted: 'false',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TGT-URL@',
  Tables: 'QATEST.EMP,QA.CDC_EMP',
  adapterName: 'DatabaseWriter',
  --IgnorableExceptionCode: '301',
  Password: '@TGT-PASS@'
 ) 
INPUT FROM @APPNAME@InitialLoad_Stream1;

CREATE OR REPLACE TARGET @APPNAME@sap_target3 USING DatabaseWriter  ( 
  DatabaseProviderType:'SAPHANA',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT-USER@',
  Password_encrypted: 'flase',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TGT-URL@',
  Tables: 'QATEST.EMP_INIT,QA.INITIALLOAD_EMP',
  adapterName: 'DatabaseWriter',
  --IgnorableExceptionCode: '301',
  Password: '@TGT-PASS@'
 ) 
INPUT FROM @APPNAME@CDC_Stream1;


CREATE OR REPLACE TARGET @APPNAME@sap_target4 USING DatabaseWriter  ( 
  DatabaseProviderType:'SAPHANA',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TGT-USER@',
  Password_encrypted: 'false',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TGT-URL@',
  Tables: 'QA.FILE_INIT_CDC_EMP',
  adapterName: 'DatabaseWriter',
  IgnorableExceptionCode: '301',
  Password: '@TGT-PASS@'
 ) 
INPUT FROM @APPNAME@FileSource_cdc_init_TypedStream;

create or replace target @APPNAME@sys_file_tgt using sysout(
name:'foo_file'
)input from @APPNAME@FileSource_Stream1;

create or replace target @APPNAME@sys_cdc_tgt using sysout(
name:'foo_cdc'
)input from @APPNAME@CDC_Stream1;

create or replace target @APPNAME@sys_init_tgt using sysout(
name:'foo_init'
)input from @APPNAME@InitialLoad_Stream1;

End Application @APPNAME@;


deploy application @APPNAME@;
start application @APPNAME@;

use PosTester;
DROP Source CsvDataSource;

STOP TQLwithinTqlApp;
UNDEPLOY APPLICATION TQLwithinTqlApp;
DROP APPLICATION TQLwithinTqlApp CASCADE;

CREATE APPLICATION TQLwithinTqlApp;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',

  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


END APPLICATION TQLwithinTqlApp;
DEPLOY APPLICATION TQLwithinTqlApp;
START TQLwithinTqlApp;

@@FEATURE-DIR@/tql/TQLTobeCalled.tql

STOP DSLAPP;
UNDEPLOY APPLICATION DSLAPP;
DROP APPLICATION DSLAPP CASCADE;

CREATE APPLICATION DSLAPP;

-- CacheWaction WACTIONSTORE is being loaded from DSCache

CREATE OR REPLACE WACTIONSTORE CacheWactionDSL CONTEXT OF DS.T1
EVENT TYPES ( DS.T1 )
@PERSIST-TYPE@

CREATE CQ DSLDerby
INSERT INTO CacheWactionDSL
select * from DS.C1
LINK SOURCE EVENT;

END APPLICATION DSLAPP;
DEPLOY APPLICATION DSLAPP;
START APPLICATION DSLAPP;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'Default',
  ProcessCopyBookFileAs: 'SingleEvent',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
  filename: '%@metadata(FileName)%',
  directory: '@DIR@',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter()
INPUT FROM CCBStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  FetchSize:1,
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @SOURCE@ USING MSSQLReader
 (
   Username: '@LOGMINER-UNAME@',
   Password: '@LOGMINER-PASSWORD@',
   ConnectionURL: '@LOGMINER-URL@',
   DatabaseName:'qatest',
   Tables: '@SOURCE_TABLE@',
    Compression:false,
    AutoDisableTableCDC:false,FetchTransactionMetadata:true,
    StartPosition:'EOF'
 )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@1 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'false',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@2 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'false',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'true',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@3 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'true',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW;
CREATE OR REPLACE PROPERTYSET KafkaProps(zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.9');
CREATE STREAM Oracle_ChangeDataStream1 of Global.WAEvent PERSIST USING KafkaProps;
CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  QuiesceMarkeRTable:'QATEST.QUIESCEMARKER',
  Password_encrypted: false,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//127.0.0.1:1521/xe',
  Tables: 'QATEST.OracToCql_alldatatypes',
  adapterName: 'OracleReader',
  Password: 'miner',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'miner',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream1;
CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'test.chkpoint',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:60',
  CommitPolicy: 'EventCount:1,Interval:60',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  Tables: 'QATEST.OracToCql_alldatatypes,test.oractocq_alldatatypes',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream1;
create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream1;
END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

--CREATE APPLICATION @APPNAME@;
create application @APPNAME@ Recovery 5 second Interval;

--create or replace flow @APPNAME@_agentflow;

CREATE OR REPLACE SOURCE @APPNAME@_source USING MariaDBReader 
(
Username: '@READER-UNAME@',
Password: '@READER-PASSWORD@',
ConnectionURL: '@CDC-READER-URL@',
Tables: @WATABLES@,
sendBeforeImage:'true',
FilterTransactionBoundaries: 'true'
) 
OUTPUT TO @APPNAME@_stream ;

--end flow @APPNAME@_@APPNAME@_agentflow;

CREATE OR REPLACE TARGET @APPNAME@_target USING CassandraCosmosDBWriter  (
  AccountEndpoint: 'cassandracosmostest.cassandra.cosmos.azure.com',
  AccountKey: 'pqDZvVgbdSCg7VzIzD77dAhPG2odGRZPLhAQA1qnZbAKoIDk6RuQX5r2phbRQFnR1l54qxOcvBXNdz8DeijYIg==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  Tables: 'QATEST.Source1,test.target1',
  adapterName: 'CassandraCosmosDBWriter'
 )
 INPUT FROM @APPNAME@_stream;

 END APPLICATION @APPNAME@;

deploy application @APPNAME@;
 --deploy application @APPNAME@ with agentflow in agents;
 start application @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 1 second interval;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@'
  --recordSelector: '@PROP8@'
)
OUTPUT TO @APPNAME@Stream;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;

/*
create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:1,Interval:5',
  CommitPolicy: 'EventCount:1,Interval:5',
  Tables: 'QATEST.@table@'
)
input from @APPNAME@Stream;*/
end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

STOP UpdatableCacher.UpdatableCache;
UNDEPLOY APPLICATION UpdatableCacher.UpdatableCache;
DROP APPLICATION UpdatableCacher.UpdatableCache CASCADE;
CREATE APPLICATION UpdatableCacher.UpdatableCache;

CREATE TYPE MerchantHourlyAve(
  merchantId String KEY,
  hourlyAve Integer,
  theDate DateTime,
  price Double
);


CREATE source CsvDataSource USING FileReader (
      directory:'@TEST-DATA-PATH@',
      columndelimiter: ',',
      wildcard:'ucData.csv',
      blocksize: 10240,
      positionByEOF:false
)
PARSE USING DSVParser (
      header:No,
      trimquote:false
) OUTPUT TO CsvStream;


CREATE STREAM S1 OF MerchantHourlyAve;

CREATE CQ cq1
	insert into S1
		SELECT data[0],
				TO_INT(data[1]),
				TO_DATE(data[2]),
				TO_DOUBLE(data[3])
		FROM CsvStream;


CREATE EVENTTABLE ET1 using STREAM (
  NAME: 'S1'
) QUERY (keytomap:'price', persistPolicy: 'true' ) OF MerchantHourlyAve;


CREATE EVENTTABLE ET2 using STREAM (
  NAME: 'S1'
) QUERY (keytomap:'merchantId' ) OF MerchantHourlyAve;
--) QUERY (keytomap:'merchantId', uniquekey:'price' ) OF MerchantHourlyAve;



END APPLICATION UpdatableCache;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

--CREATE APPLICATION @APPNAME@;
create application @APPNAME@ Recovery 5 second Interval;

--create or replace flow @APPNAME@_agentflow;

CREATE OR REPLACE SOURCE @APPNAME@_source USING DatabaseReader  (
  Username: 'qatest',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.EMP_INIT',
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: 'qatest'
 )
OUTPUT TO @APPNAME@_stream ;

--end flow @APPNAME@_@APPNAME@_agentflow;

CREATE OR REPLACE TARGET @APPNAME@_target USING CassandraCosmosDBWriter  (
  AccountEndpoint: 'cassandracosmostest.cassandra.cosmos.azure.com',
  AccountKey: 'pqDZvVgbdSCg7VzIzD77dAhPG2odGRZPLhAQA1qnZbAKoIDk6RuQX5r2phbRQFnR1l54qxOcvBXNdz8DeijYIg==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  Tables: 'QATEST.Source1,test.target1',
  adapterName: 'CassandraCosmosDBWriter'
 )
 INPUT FROM @APPNAME@_stream;

 END APPLICATION @APPNAME@;

deploy application @APPNAME@;
 --deploy application @APPNAME@ with agentflow in agents;
 start application @APPNAME@;

Stop Oracle_LogWriter;
Undeploy application Oracle_LogWriter;
drop application Oracle_LogWriter cascade;

CREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;

CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  (

  FetchSize: 1000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=t1',
 startPosition: '%=-1',
  PollingInterval: '2sec',
  ConnectionPoolSize: 5,
  ThreadPoolSize: 5
  )
  OUTPUT TO data_stream;
create target AzureSQLDWHTarget using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:1000,interval:300s'
) INPUT FROM data_stream;
  CREATE OR REPLACE TARGET TeraSys USING SysOut  (
  name: 'ora12_out'
 ) INPUT FROM data_stream;

END APPLICATION Oracle_IRLogWriter;
deploy application Oracle_IRLogWriter;
start Oracle_IRLogWriter;

drop application ConsoleApplication cascade;
create application ConsoleApplication;

use RetailTester;
alter application RetailApp;

CREATE FLOW RetailSourceFlow;

-- RetailDataSource is the primary data source for this application.
--
-- ParseOrderData discards the fields not needed by this application and puts the
-- data into the appropriate Java types.
--
-- ParseOrderData outputs to RetailOrders stream, the start point of the
-- RetailProductFlow and RetailStoreFlow flows.

CREATE SOURCE RetailDataSource USING CSVReader (
  directory:'Samples/Customer/RetailApp/appData',
  header:Yes,
  wildcard:'retaildata2M.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO Orders;

-- A stream's type must be declared before the stream, and a CQ's
-- output stream must be defined before the CQ. Hence type-stream-CQ
-- sequences like the following are very common.

-- output for ParseOrderData
CREATE TYPE OrderType(
  storeId      String,
  orderId      String,
  sku          String,
  orderAmount  double,
  dateTime     DateTime,
  hourValue    int,
  state        String,
  city         String,
  zip          String
);
CREATE STREAM RetailOrders Of OrderType;

CREATE CQ ParseOrderData
INSERT INTO RetailOrders
SELECT  data[0],
        data[6],
        data[7],
        TO_DOUBLE(SRIGHT(data[8],1)),
        TO_DATE(data[9],'yyyyMMddHHmmss'),
        DHOURS(TO_DATE(data[9],'yyyyMMddHHmmss')),
        data[3],
        data[2],
        data[4]
FROM Orders;

END FLOW RetailSourceFlow;

end application RetailApp;

alter application RetailApp recompile;

--
-- Recovery Test 35 with two sources, two jumping count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5W/p -> CQ1 -> WS
--   S2 -> Jc6W/p -> CQ2 -> WS
--

STOP Recov35Tester.RecovTest35;
UNDEPLOY APPLICATION Recov35Tester.RecovTest35;
DROP APPLICATION Recov35Tester.RecovTest35 CASCADE;
CREATE APPLICATION RecovTest35 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION RecovTest35;

stop ROLLUPMON_IL;
undeploy application ROLLUPMON_IL;
alter application ROLLUPMON_IL;
CREATE or replace FLOW ROLLUPMON_IL_flow;
Create or replace Source ROLLUPMON_IL_Oraclesrc Using databasereader(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',
 Tables:'QATEST.ROLLUPMON_TABLE1;QATEST.ROLLUPMON_TABLE2;QATEST.ROLLUPMON_TABLE3;QATEST.ROLLUPMON_TABLE4;QATEST.ROLLUPMON_TABLE5',
 _h_fetchexactrowcount: 'true',
FetchSize:1000
)
Output To ROLLUPMON_IL_OrcStrm;
END FLOW ROLLUPMON_IL_flow;
alter application ROLLUPMON_IL recompile;
deploy application ROLLUPMON_IL;
start application ROLLUPMON_IL;

CREATE APPLICATION @APPNAME@;

CREATE TYPE @APPNAME@_completeRecord(
completedata com.fasterxml.jackson.databind.JsonNode);
CREATE stream @APPNAME@_CompleteRecordInJSONStream of @APPNAME@_completeRecord;

CREATE SOURCE @APPNAME@_src USING KafkaReader VERSION @KAFKA_VERSION@ ()
PARSE USING AvroParser (
schemaFileName: 'avroSchema'
)
OUTPUT TO @APPNAME@_Stream2;

CREATE CQ @APPNAME@_CQ1
 INSERT INTO @APPNAME@_CompleteRecordInJSONStream
 SELECT
 AvroToJson(y.data)
 from @APPNAME@_Stream2 y;

CREATE CQ @APPNAME@_GetNativeRecordInJSONCQ
INSERT INTO @APPNAME@_NativeRecordStream
SELECT
 completedata.get("Facility").toString() as Facility,
 completedata.get("MsgTime").toString() as MsgTime,
 completedata.get("PatientID").toString() as PatientID,
 completedata.get("OrderIdentifier").toString() as OrderIdentifier,
 completedata.get("OrderText").toString() as OrderText
FROM @APPNAME@_CompleteRecordInJSONStream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING Global.FileWriter ()
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@_NativeRecordStream;

END APPLICATION @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;


CREATE SOURCE @APPNAME@_Source USING MySqlReader
(
  Compression: false,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'mysql://localhost:3306',
  DatabaseName: 'waction',
  Tables: 'waction.test01',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root'
)
OUTPUT TO @APPNAME@_Stream;


CREATE TARGET @APPNAME@_Target USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	BatchPolicy: 'EventCount: 1; Interval: 1s',
	instanceId: 'qatest'
) INPUT FROM @APPNAME@_stream;

create Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_stream;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING MySqlReader
(
  Compression: false,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'mysql://localhost:3306',
  DatabaseName: 'waction',
  Tables: 'waction.test01',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root',
  connectionRetryPolicy:'retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'waction.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;

--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE KafkaReaderSource USING Global.KafkaReader VERSION '2.1.0' 
( 
AutoMapPartition: true, 
brokerAddress: 'localhost:9092',       
KafkaConfigPropertySeparator: ',',
Topic: 'test011', 
startOffset: 0,
KafkaConfigValueSeparator: ':', 
KafkaConfig: 'request.timeout.ms:6001,session.timeout.ms:6000,security.protocol:SASL_SSL,sasl.mechanism:SCRAM-SHA-512,sasl.jaas.config:org.apache.kafka.common.security.scram.ScramLoginModule required username=kafkauser password=\"Oppenheimer\";,value.deserializer:com.striim.avro.deserializer.LengthDelimitedAvroRecordDeserializer' 
) 
PARSE USING Global.DSVParser () 
OUTPUT TO KafkaDevReadSourceStream;

--
-- Recovery Test 25 with two sources, two jumping count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5W -> CQ1 -> WS
--   S2 -> Jc6W -> CQ2 -> WS
--

STOP KStreamRecov25Tester.KStreamRecovTest25;
UNDEPLOY APPLICATION KStreamRecov25Tester.KStreamRecovTest25;
DROP APPLICATION KStreamRecov25Tester.KStreamRecovTest25 CASCADE;
DROP USER KStreamRecov25Tester;
DROP NAMESPACE KStreamRecov25Tester CASCADE;
CREATE USER KStreamRecov25Tester IDENTIFIED BY KStreamRecov25Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov25Tester;
CONNECT KStreamRecov25Tester KStreamRecov25Tester;

CREATE APPLICATION KStreamRecovTest25 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION KStreamRecovTest25;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE FLOW @APP_NAME@_SrcFlow;

CREATE OR REPLACE SOURCE @APP_NAME@_src USING FileReader (
directory:'',
WildCard:''
)
parse using DSVParser (
header:'no'
)
OUTPUT TO @APP_NAME@_Stream;
END FLOW @APP_NAME@_SrcFlow;

CREATE FLOW @APP_NAME@_TgtFlow;

CREATE OR REPLACE TYPE @APP_NAME@_Type  ( BUSINESS_NAME java.lang.String KEY,
MERCHANT_ID java.lang.String,
PRIMARY_ACCOUNT_NUMBER java.lang.String
 ) ;

CREATE OR REPLACE STREAM @APP_NAME@_Stream2 OF @APP_NAME@_Type;
CREATE OR REPLACE CQ @APP_NAME@_CQ
INSERT INTO @APP_NAME@_Stream2
SELECT data[0],data[1],data[2]
FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.FabricDataWarehouseWriter (
  Tables: '',
  ConnectionURL: '@CONN_URL@',
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  uploadpolicy: 'eventcount:1',
  AccountName: '@ACCOUNTNAME@')
INPUT FROM @APP_NAME@_Stream2;

END FLOW @APP_NAME@_TgtFlow;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@ WITH @APP_NAME@_SrcFlow IN agents,@APP_NAME@_TgtFlow IN default;
START APPLICATION @APP_NAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '@GRPPOLICY@',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'MultipleEvents',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

create Target KafkaTarget using KafkaWriter VERSION '2.1.0' (
brokerAddress:'',
Topic:'',
Mode: 'Sync',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;linger.ms=30000'
)
FORMAT USING JSONFormatter (
members:'data')
input from CCBStream;

create source KafkaSource using KafkaReader VERSION '2.1.0'(
brokerAddress:'',
	Topic:''
)
parse using JSONParser ()
output to KafkaStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING JSONFormatter ()
INPUT FROM KafkaStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
  directory: '',
  positionByEOF: false,
  WildCard: '' )
PARSE USING ParquetParser (
 )
OUTPUT TO @appname@Streams;

CREATE OR REPLACE CQ @appname@CQOrder3
INSERT INTO @appname@Stream3
SELECT
PUTUSERDATA(s,'schemaName',s.data.getSchema().getName())
FROM @appname@Streams s;

CREATE TARGET @filetarget@ USING Global.FileWriter (
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000,Interval:30s',
  directory: '',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  filename: 'AvroOutFile',
  DataEncryptionKeyPassphrase_encrypted: 'true',
  rolloverpolicy: 'EventCount:390,Interval:30m' )
FORMAT USING Global.AvroFormatter  (
  schemaFileName: 'AvroSchema',
  formatAs: 'default',
  schemaregistryConfiguration: '' )
INPUT FROM @appname@Stream3;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	charset:'@charset@'
)
parse using AvroParser (
	schemaFileName:'@fname@',
	schemaRegistryURI:'@evty@'
)
OUTPUT TO CsvStream;
/*
create Target FileTarget using FileWriter(
    rolloverpolicy:'eventcount:100',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using AvroFormatter (
schemaFileName:'@fname@'
)
input from CsvStream;
*/
end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

--
-- Kafka Stream Recovery Test 1
-- Bert Hashemi and Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> KS -> WS

STOP KStreamRecov1Tester.KStreamRecovTest1;
UNDEPLOY APPLICATION KStreamRecov1Tester.KStreamRecovTest1;
DROP APPLICATION KStreamRecov1Tester.KStreamRecovTest1 CASCADE;
DROP USER KStreamRecov1Tester;
DROP NAMESPACE KStreamRecov1Tester CASCADE;
CREATE USER KStreamRecov1Tester IDENTIFIED BY KStreamRecov1Tester;
-- GRANT 'Global:create,drop:deploymentgroup:*' TO USER KStreamRecov1Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov1Tester;
CONNECT KStreamRecov1Tester KStreamRecov1Tester;

CREATE APPLICATION KStreamRecovTest1 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE or REPLACE TYPE KafkaCsvStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF KafkaCsvStreamType 
EVENT TYPES ( KafkaCsvStreamType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

END APPLICATION KStreamRecovTest1;

create application KinesisTest RECOVERY 1 SECOND INTERVAL;
CREATE OR REPLACE SOURCE ora_reader USING OracleReader (
  Username: 'miner',
  Password: 'miner',
  ConnectionURL: '192.168.1.113:1521:ORCL',
  TABLES: 'QATEST.H_REGION;QATEST.H_NATION;QATEST.H_CUSTOMER',
  FetchSize: '1'
 )
OUTPUT TO DDLCDCStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	accesskeyid:'ACCESS_KEY',
	secretaccesskey:'SECRET_KEY'
)
format using JSONFormatter (
)
input from DDLCDCStream;

end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

UNDEPLOY APPLICATION TcpDsvAgentTester.TcpDsvWithAgent;
DROP APPLICATION TcpDsvAgentTester.TcpDsvWithAgent cascade;

create Application TcpDsvWithAgent;


create source TcpDsvAgent using TCPReader
(
  IpAddress:'127.0.0.1',
  PortNo:'3549',
  charset: 'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO TcpDsvAgentStream;


CREATE TYPE UserDataType
(
  UserId String KEY,
  UserName String,
  CompanyName String,
  UserZip int,
  CompanyZip int
);

CREATE STREAM UserDataStream OF UserDataType PARTITION BY UserId;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  data[0],
        data[1],
        data[2],
        TO_INT(data[3]),
        TO_INT(data[4])
FROM TcpDsvAgentStream;


CREATE WACTIONSTORE UserActivityInfo
CONTEXT OF UserDataType
EVENT TYPES ( UserDataType )
PERSIST EVERY 6 second USING (
JDBC_DRIVER:'@WASTORE-DRIVER@',
JDBC_URL:'@WASTORE-URL@',
JDBC_USER:'@WASTORE-UNAME@',
JDBC_PASSWORD:'@WASTORE-PASSWORD@',
DDL_GENERATION:'drop-and-create-tables',
LOGGING_LEVEL:'SEVERE',
CONTEXT_TABLE:'USERTABLE',
EVENT_TABLE:'USEREVENTS'
);


--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction
INSERT INTO UserActivityInfo
SELECT * FROM UserDataStream
LINK SOURCE EVENT;



end Application TcpDsvWithAgent;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @SOURCE@ USING Ojet  ( 
  FilterTransactionBoundaries: true,
  ConnectionURL: '@OCI-URL@',
  Tables: '@SOURCE_TABLE@',
  Password: '@OJET-PASSWORD@',
  fetchsize: 1,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Username: '@OJET-UNAME@'
 ) 
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@1 USING BigQueryWriter ( 
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  streamingUpload: 'false', 
  BatchPolicy: 'eventCount:1000000, Interval:90', 
  Tables: 'sample.tab', 
  ServiceAccountKey: '', 
  projectId: 'striimqa-214712', 
  Encoding: 'UTF-8', 
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30', 
  AllowQuotedNewLines: 'false', 
  CDDLAction: 'Process', 
  optimizedMerge: 'false', 
  TransportOptions: 'connectionTimeout=300, readTimeout=120', 
  Mode: 'MERGE', 
  StandardSQL: 'true', 
  includeInsertId: 'true', 
  QuoteCharacter: '\"' ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@2 USING BigQueryWriter ( 
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  streamingUpload: 'false', 
  BatchPolicy: 'eventCount:1000000, Interval:90', 
  Tables: 'sample.tab', 
  ServiceAccountKey: '', 
  projectId: 'striimqa-214712', 
  Encoding: 'UTF-8', 
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30', 
  AllowQuotedNewLines: 'false', 
  CDDLAction: 'Process', 
  optimizedMerge: 'true', 
  TransportOptions: 'connectionTimeout=300, readTimeout=120', 
  Mode: 'MERGE', 
  StandardSQL: 'true', 
  includeInsertId: 'true', 
  QuoteCharacter: '\"' ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@3 USING BigQueryWriter ( 
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  streamingUpload: 'false', 
  BatchPolicy: 'eventCount:1000000, Interval:90', 
  Tables: 'sample.tab', 
  ServiceAccountKey: '', 
  projectId: 'striimqa-214712', 
  Encoding: 'UTF-8', 
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30', 
  AllowQuotedNewLines: 'false', 
  CDDLAction: 'Process', 
  optimizedMerge: 'false', 
  TransportOptions: 'connectionTimeout=300, readTimeout=120', 
  Mode: 'APPENDONLY', 
  StandardSQL: 'true', 
  includeInsertId: 'true', 
  QuoteCharacter: '\"' ) 
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

STOP Sliding1Tester.Sliding1;
UNDEPLOY APPLICATION Sliding1Tester.Sliding1;
DROP APPLICATION Sliding1Tester.Sliding1 CASCADE;
CREATE APPLICATION Sliding1;

create source CsvSource1 using FileReader
(
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'WindowsTest.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
)
 parse using DSVParser
(
	header:'yes',
	columndelimiter:','
)
OUTPUT TO CsvStream1;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);
CREATE TYPE CsvData1 (
  zip double
);

CREATE TYPE WactionData1 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData2 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData3 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData5 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData6 (
  zip double
);
CREATE TYPE WactionData7 (
  zip double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY companyName;

CREATE STREAM DataStream2 OF CsvData
PARTITION BY city;

CREATE STREAM DataStream3 OF CsvData;
CREATE STREAM DataStream4 OF CsvData;
CREATE STREAM DataStream5 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData3
INSERT INTO DataStream3
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData4
INSERT INTO DataStream4
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData5
INSERT INTO DataStream5
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData6
INSERT INTO DataStream6
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

CREATE CQ CsvToData7
INSERT INTO DataStream7
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

-- Count based sliding window
CREATE WINDOW DataStreamCount
OVER DataStream1 KEEP 5 ROWS
PARTITION BY companyName;

-- Time based jumping window
CREATE WINDOW DataStreamTime OVER DataStream2 KEEP
within 350 second
PARTITION BY companyName,city;

-- Attribute based sliding window
CREATE WINDOW DataStreamAtrribute
OVER DataStream3 KEEP
range 180 second
ON dateTime;

-- Count + time based sliding window
CREATE WINDOW DataStreamCountTime
OVER DataStream4 KEEP
5 rows
within 155 second;

-- Attribute + time based sliding window
CREATE WINDOW DataStreamAttributeTime
OVER DataStream5 KEEP
range 50 second
ON dateTime
within 400 second;

CREATE WACTIONSTORE Wactions1 CONTEXT OF WactionData1
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions2 CONTEXT OF WactionData2
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions3 CONTEXT OF WactionData3
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions4 CONTEXT OF WactionData4
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions5 CONTEXT OF WactionData5
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions6 CONTEXT OF WactionData6
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions7 CONTEXT OF WactionData7
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE CQ Data1ToWaction
INSERT INTO Wactions1
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamCount p
group by companyName;

CREATE CQ Data2ToWaction
INSERT INTO Wactions2
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamTime p
group by companyName,city;

CREATE CQ Data3ToWaction
INSERT INTO Wactions3
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamAtrribute p;

CREATE CQ Data4ToWaction
INSERT INTO Wactions4
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamCountTime p;

CREATE CQ Data5ToWaction
INSERT INTO Wactions5
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamAttributeTime p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions6
SELECT
    count(*)
FROM DataStreamCount p;

CREATE CQ Data7ToWaction
INSERT INTO Wactions7
SELECT
    count(*)
FROM DataStreamTime p;

END APPLICATION Sliding1;

CREATE TARGET @TARGET_NAME@ USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
) INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@2 USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
) INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@3 USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
) INPUT FROM @STREAM@;


CREATE TARGET @TARGET_NAME@4 USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
) INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;;

create Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;

--use global;
CREATE OR REPLACE PROPERTYSET LDAP1 ( PROVIDER_URL:"@LDAP_URL@", SECURITY_AUTHENTICATION:@LDAP_AUTH@, SECURITY_PRINCIPAL: "@LDAP_PRINCIPAL@" , SECURITY_CREDENTIALS:@LDAP_CRED@, USER_BASE_DN:"@LDAP_DN@", User_userId:@LDAP_USERID@ );

stop application MSSQLTransactionSupportAutoDisableCdcTrue;
undeploy application MSSQLTransactionSupportAutoDisableCdcTrue;
drop application MSSQLTransactionSupportAutoDisableCdcTrue cascade;

CREATE APPLICATION MSSQLTransactionSupportAutoDisableCdcTrue recovery 1 second interval;

Create Source ReadFromMSSQL3
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
AutoDisableTableCDC:'true',
FetchTransactionMetadata:'true',
FilterTransactionBoundaries: false,
Compression:'false',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportAutoDisableCdcTrueStream;


CREATE TARGET WriteToMSSQL3 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC,@@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportAutoDisableCdcTrueStream;

CREATE TARGET MSSqlReaderOutput3 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportAutoDisableCdcTrueStream; 


CREATE OR REPLACE TARGET MSSQLFileOut3 USING FileWriter  ( 
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  directory:'@FEATURE-DIR@/logs/',
  filename: 'TransactionSupportAutoDisableTableCdcTrue.txt'
 ) 
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 ) 
INPUT FROM MSSQLTransactionSupportAutoDisableCdcTrueStream;

END APPLICATION MSSQLTransactionSupportAutoDisableCdcTrue;
deploy application MSSQLTransactionSupportAutoDisableCdcTrue;
start application MSSQLTransactionSupportAutoDisableCdcTrue;

CREATE FLOW @STREAM@_SourceFlow;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING PostgreSQLReader  ( 
 ReplicationSlotName: 'slotname',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 ) 
OUTPUT TO @STREAM@ ;

END FLOW @STREAM@_SourceFlow;

Stop Oracle_Oracle_IRLogWriterLogWriter;
Undeploy application Oracle_Oracle_IRLogWriterLogWriter;
drop application Oracle_Oracle_IRLogWriterLogWriter cascade;

CREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;

CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( 
 
  FetchSize: 5000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=t1',
 startPosition: '%=-1',
  PollingInterval: '20sec'
  )
  OUTPUT TO data_stream;
create target AzureSQLDWHTarget using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:0,interval:0s'
) INPUT FROM data_stream;
  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

END APPLICATION Oracle_IRLogWriter;
deploy application Oracle_IRLogWriter;
start Oracle_IRLogWriter;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@
 Using OracleReader
(
Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@SourceConnectURL@',
Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP application admin.SampleApp;
undeploy application admin.SampleApp;
drop application admin.SampleApp cascade;


CREATE APPLICATION SampleApp RECOVERY 10 SECOND INTERVAL;

CREATE SOURCE Oracle_Src USING Global.OracleReader (
  Tables: 'QATEST.TEST01',
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  OutboundServerProcessName: 'WebActionXStream',
  Password: 'qatest',
  Compression: false,
  ReaderType: 'LogMiner',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  FetchSize: 1,
  Password_encrypted: 'false',
  SupportPDB: false,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CDDLAction: 'Process',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  DictionaryMode: 'OnlineCatalog',
  QueueSize: 2048,
  CommittedTransactions: true,
  XstreamTimeOut: 600,
  CDDLCapture: false,
  TransactionBufferType: 'Disk',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '100MB',
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  DatabaseRole: 'Primary' )
OUTPUT TO Striim_Buffer;

CREATE TARGET Oracle_tgt USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'qatest',
  ParallelThreads: '',
  DatabaseProviderType: 'Oracle',
  CheckPointTable: 'CHKPOINT',
  Password_encrypted: 'false',
  CDDLAction: 'Process',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.TEST01,QATEST.TEST02',
  CommitPolicy: 'EventCount:1,Interval:10',
  StatementCacheSize: '50',
  Username: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:10',
  PreserveSourceTransactionBoundary: 'false' )
INPUT FROM Striim_Buffer;

END APPLICATION SampleApp;

STOP APPLICATION EH;
UNDEPLOY APPLICATION EH;
DROP APPLICATION EH CASCADE;
CREATE APPLICATION EH recovery 5 second interval;
CREATE Source s USING PostgreSQLReader  ( 
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Tables: 'public.tablename1000%') 
OUTPUT TO ss ;

CREATE OR REPLACE TYPE jsontype( 
datae java.util.HashMap , 
TABLE_NAME java.lang.String , 
OPS_NAME java.lang.String , 
DB_TIMESTAMP java.lang.String );

CREATE STREAM cq_json_out OF jsontype PARTITION BY TABLE_NAME;

CREATE OR REPLACE CQ cq_json 
INSERT INTO cq_json_out
SELECT 
data(e),
META(e,"TableName").toString() as TABLE_NAME,
META(e, "OperationName").toString() as OPS_NAME,
META(e, "TimeStamp").toString() as DB_TIMESTAMP
FROM ss e;

CREATE CQ cq1
INSERT INTO TypedAccessLogStream1
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000101'; 

create Target t1 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_101',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_101',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream1;

CREATE CQ cq2
INSERT INTO TypedAccessLogStream2
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000102'; 

create Target t2 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_102',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_102',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream2;

CREATE CQ cq3
INSERT INTO TypedAccessLogStream3
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000103'; 

create Target t3 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_103',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_103',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream3;

CREATE CQ cq4
INSERT INTO TypedAccessLogStream4
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000104'; 

create Target t4 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_104',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_104',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream4;

CREATE CQ cq5
INSERT INTO TypedAccessLogStream5
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000105'; 

create Target t5 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_105',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_105',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream5;

CREATE CQ cq6
INSERT INTO TypedAccessLogStream6
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000106'; 

create Target t6 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_106',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_106',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream6;

CREATE CQ cq7
INSERT INTO TypedAccessLogStream7
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000107'; 

create Target t7 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_107',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_107',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream7;

CREATE CQ cq8
INSERT INTO TypedAccessLogStream8
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000108'; 

create Target t8 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_108',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_108',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream8;

CREATE CQ cq9
INSERT INTO TypedAccessLogStream9
SELECT *
FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000109'; 

create Target t9 using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_109',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_cg_109',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000,interval:30s',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from TypedAccessLogStream9;

-- CREATE CQ cq10
-- INSERT INTO TypedAccessLogStream10
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000110'; 
-- 
-- create Target t10 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_110',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_110',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream10;


-- CREATE CQ cq11
-- INSERT INTO TypedAccessLogStream11
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000111'; 
-- 
-- create Target t11 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_111',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_111',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream11;
-- 
-- CREATE CQ cq12
-- INSERT INTO TypedAccessLogStream12
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000112'; 
-- 
-- create Target t12 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_112',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_112',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream12;
-- 
-- CREATE CQ cq13
-- INSERT INTO TypedAccessLogStream13
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000113'; 
-- 
-- create Target t13 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_113',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_113',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream13;
-- 
-- CREATE CQ cq14
-- INSERT INTO TypedAccessLogStream14
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000114'; 
-- 
-- create Target t14 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_114',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_114',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream14;
-- 
-- CREATE CQ cq15
-- INSERT INTO TypedAccessLogStream15
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000115'; 
-- 
-- create Target t15 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_115',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_115',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream15;
-- 
-- CREATE CQ cq16
-- INSERT INTO TypedAccessLogStream16
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000116'; 
-- 
-- create Target t16 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_116',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_116',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream16;
-- 
-- CREATE CQ cq17
-- INSERT INTO TypedAccessLogStream17
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000117'; 
-- 
-- create Target t17 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_117',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_117',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream17;
-- 
-- CREATE CQ cq18
-- INSERT INTO TypedAccessLogStream18
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000118'; 
-- 
-- create Target t18 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_118',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_118',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream18;
-- 
-- CREATE CQ cq19
-- INSERT INTO TypedAccessLogStream19
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000119'; 
-- 
-- create Target t19 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_119',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_119',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream19;
-- 
-- CREATE CQ cq20
-- INSERT INTO TypedAccessLogStream20
-- SELECT *
-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000120'; 
-- 
-- create Target t20 using AzureEventHubWriter (
-- 	EventHubNamespace:'EventHubWriterTest',
-- 	EventHubName:'test_120',
-- 	SASPolicyName:'RootManageSharedAccessKey',
-- 	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
-- 	ConsumerGroup:'test_cg_120',
-- 	E1P:'true',
-- 	OperationTimeoutMS:'200000',
-- 	BatchPolicy:'Size:256000,interval:30s'
-- )
-- format using jsonFormatter(
-- )
-- input from TypedAccessLogStream20;


END APPLICATION EH;
DEPLOY APPLICATION EH;
start application EH;

DROP TYPE T1;
DROP CACHE C1;


CREATE  TYPE T1  ( id String ,
airport_ref String,
airport_ident String KEY ,
type1 String ,
description String ,
frequency_mhz String
 );

CREATE CACHE C1 USING FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: '1MB.csv',
  charset: 'UTF-8',
  blockSize: '64',
  positionbyeof: 'false'
 )
PARSE USING DSVPARSER (
  columndelimiter: ',',
  rowdelimiter: '\n:\r',
  header: 'true'
 )
QUERY (
  keytomap: 'id'
 )
 OF T1;

-- Loading Cache C1 which is accessed by DataSourceApp.tql

load cache C1;

list servers;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;


Create Source @SourceName@ Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'jdbc:oracle:thin:@//192.168.124.25:1522/orcl',
 Tables:'qatest.sourcetable',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;


CREATE TARGET @targetsys@ USING Global.SysOut (
  name: 'sysout' )
INPUT FROM @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
  BatchPolicy: 'EventCount:10000,Interval:30',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:10000,Interval:100',
  StatementCacheSize: '50',
  Password: 'w3b@ct10n',
  Username: 'qatest',
  IgnorableExceptionCode: '547,DUPLICATE_ROW_EXISTS,NO_OP_UPDATE,NO_OP_DELETE,NO_OP_PKUPDATE',
  DatabaseProviderType: 'SQLServer',
  PreserveSourceTransactionBoundary: 'false',
  Tables: 'dbo.sourceTable,dbo.targetTable',
  VendorConfiguration: 'enableIdentityInsert=true',
  adapterName: 'DatabaseWriter' )
INPUT FROM @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;

--
-- Recovery Test 31 with two sources, two sliding count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5W/p -> CQ1 -> WS
-- S2 -> Sc6W/p -> CQ2 -> WS
--

STOP Recov31Tester.RecovTest31;
UNDEPLOY APPLICATION Recov31Tester.RecovTest31;
DROP APPLICATION Recov31Tester.RecovTest31 CASCADE;
CREATE APPLICATION RecovTest31 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION RecovTest31;

--To be used with NonCdcAdapterCommons.startExceptionStoreApp()
CREATE APPLICATION @APPNAME@;

CREATE CQ @APPNAME@CQ INSERT INTO @APPNAME@CQOut
select x.exceptionType,
x.action,
x.appName,
x.entityType,
x.entityName,
x.className,
x.message,
x.relatedActivity,
x.relatedObjects,
x.relatedEntity,
x.exceptionCode
from @EXCEPTIONAPPNAME@_ExceptionStore x;

CREATE TARGET @APPNAME@Trgt USING FileWriter ()
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@CQOut;

END APPLICATION @APPNAME@;

STOP TestAlertsEmail.TestAlertsEmailApp;
UNDEPLOY APPLICATION TestAlertsEmail.TestAlertsEmailApp;
DROP APPLICATION TestAlertsEmail.TestAlertsEmailApp CASCADE;

CREATE APPLICATION TestAlertsEmailApp;

CREATE source rawSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:No,
  wildcard:'@TESTDATAFILE@',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO rawStream;

CREATE STREAM MyAlertStream OF Global.AlertEvent;
CREATE CQ GenerateMyAlerts
INSERT INTO MyAlertStream (name, keyVal, severity, flag, message)
SELECT "Testing Alerts", data[0], data[1], data[2], data[3]
FROM rawStream s;
CREATE TARGET output2 USING SysOut(name : alertsrecevied) input FROM MyAlertStream;

CREATE SUBSCRIPTION alertSubscription USING EmailAdapter
(
SMTPUSER:'@Alerts_Smtpuser@',
--, ${alerts.smtpuser}
SMTPUSER:' ${alerts.smtpuser}',
SMTPPASSWORD:'@Alerts_Smtppassword@',
smtpurl:"@Alerts_Smtpurl@",
starttls_enable:"@Alerts_Starttls_enable@",
smtp_auth:"@Alerts_Smtp_auth@",
subject:"@Alerts_Subject@",
emailList:"@Alerts_Emaillist@",
userIds:"@Alerts_UserId@",
threadCount:"@Alerts_Threadcount@",
@CONTENTTYPE@
senderEmail:"@Alerts_SenderEmail@",
)
INPUT FROM MyAlertStream;

END APPLICATION TestAlertsEmailApp;
DEPLOY APPLICATION TestAlertsEmail.TestAlertsEmailApp;
START TestAlertsEmail.TestAlertsEmailApp;

CREATE TARGET @TARGET_NAME@ USING RedshiftWriter
	(
	  ConnectionURL: '@CONNECTION_URL@',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  accesskeyId: 'access_key',
	  secretaccesskey: 'secret_access',
	  Tables: 'tgt_table',
	  uploadpolicy:'eventcount:10,interval:1m'
	) INPUT FROM @STREAM@;

--
-- Recovery Test 3
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> JW -> CQ(aggregate) -> WS
--

STOP KStreamRecov3Tester.KStreamRecovTest3;
UNDEPLOY APPLICATION KStreamRecov3Tester.KStreamRecovTest3;
DROP APPLICATION KStreamRecov3Tester.KStreamRecovTest3 CASCADE;

DROP USER KStreamRecov3Tester;
DROP NAMESPACE KStreamRecov3Tester CASCADE;
CREATE USER KStreamRecov3Tester IDENTIFIED BY KStreamRecov3Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov3Tester;
CONNECT KStreamRecov3Tester KStreamRecov3Tester;

CREATE APPLICATION KStreamRecovTest3 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount int,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_INT(TO_DOUBLE(data[7])),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

END APPLICATION KStreamRecovTest3;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@;
CREATE SOURCE @srcName@ USING Global.OracleReader ( 
  Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@') 
OUTPUT TO @instreamname@;

CREATE OR REPLACE TARGET @tgtName@ USING Global.SnowflakeWriter ( 
  connectionUrl: '@tgturl@', 
  tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@', 
  password: '@tgtpassword@',   
  username: '@tgtusername@', 
  uploadPolicy: 'eventcount:1,interval:5m', 
  authenticationType: 'Password',
  externalStageType: 'Local', 
  adapterName: 'SnowflakeWriter' ) 
INPUT FROM @outstreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

use PosTester;
alter application PosApp;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startTime DateTime,
  count int,
  totalAmount double,
  hourlyAve int,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);

end application PosApp;
alter application PosApp recompile;

--
-- Recovery Test 41 with two sources, two jumping count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5W/p -> CQ1 -> WS
--   S2 -> Jc6W/p -> CQ2 -> WS
--

STOP Recov41Tester.RecovTest41;
UNDEPLOY APPLICATION Recov41Tester.RecovTest41;
DROP APPLICATION Recov41Tester.RecovTest41 CASCADE;
CREATE APPLICATION RecovTest41 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION RecovTest41;

stop application @AppName@;
Undeploy application @AppName@;
Alter application @AppName@;

CREATE FLOW @AppName@_Agent_flow;

CREATE OR REPLACE SOURCE @AppName@_FileReaderSource USING FileReader (
wildcard: 'posdata.csv', 
  positionByEOF: false, 
  blocksize: 10100,
  rolloverpolicy: 'EventCount:100,Interval:30s', 
  directory: '@dir@' ) 
PARSE USING DSVParser ( 
  trimquote: false, 
  header: 'Yes' ) 
OUTPUT TO @AppName@_CsvStream;

END FLOW @AppName@_Agent_flow;

alter application @AppName@ recompile;
DEPLOY APPLICATION @AppName@ with @AppName@_Agent_flow on any in AGENTS;
start application @AppName@;

STOP bq;
UNDEPLOY APPLICATION bq;
DROP APPLICATION bq CASCADE;

CREATE APPLICATION bq RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:orcl',
	Tables: 'QATEST.TABLE_TEST_1000100',
	DictionaryMode: offlineCatalog,
	FetchSize: '1'
)
OUTPUT TO SS;


CREATE or replace TARGET T USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
    Tables:'QATEST.TABLE_TEST_1000100,qatest.% keycolumns(RONUM)',
    mode:'Appendonly',
    datalocation: 'US',
	nullmarker: 'defaultNULL',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:100,Interval:10'	
) INPUT FROM ss;

END APPLICATION bq;
DEPLOY APPLICATION bq;
START APPLICATION bq;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@;

CREATE OR REPLACE TYPE @APPNAME@_Type (
 HEADER java.lang.String,
 DETAILNode com.fasterxml.jackson.databind.JsonNode
 );

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '@GRPPOLICY@',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'MultipleEvents',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE OR REPLACE CQ @APPNAME@_CQ
INSERT INTO @APPNAME@_CQOut
SELECT data.get('HEADER-RECORD').get('DSD-BATCH').textvalue() as HEADER,
data.get('DETAIL-RECORD') as DETAILNode FROM CCBStream c;

CREATE OR REPLACE CQ @APPNAME@_CQ1
INSERT INTO @APPNAME@_CQOut1
SELECT HEADER,
DETAILNode.get('REC-TYPE').textvalue() as DRECTYPE,
DETAILNode.get('AP-VENDOR').textvalue() as DAPVENDOR,
DETAILNode.get('FACILITY').textvalue() as DFACILITY,
DETAILNode.get('INVOICE-NUM').textvalue() as DINVOICENUM,
DETAILNode.get('DIV').textvalue() as DDIV,
DETAILNode.get('BILLING-COST').doubleValue() as DBILLINGCOST,
DETAILNode.get('BILLING-RETAIL').doubleValue() as DBILLINGRETAIL,
DETAILNode.get('TAX-AMOUNT').doubleValue() as DTAXAMOUNT,
DETAILNode.get('CASH-DISCOUNT').doubleValue() as DCASHDISCOUNT
FROM @APPNAME@_CQOut c, iterator(c.DETAILNode) DETAILNode;;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
  filename: '%@metadata(FileName)%',
  directory: '@DIR@',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter  ()
INPUT FROM @APPNAME@_CQOut1;

CREATE OR REPLACE TARGET OracleTarget USING DatabaseWriter (
  ConnectionURL: '', 
  Password: '', 
  Username: '',
  Tables: '',  
  CommitPolicy: 'EventCount:10,Interval:10', 
  BatchPolicy: 'EventCount:10,Interval:10'
  )
INPUT FROM @APPNAME@_CQOut1;

CREATE TARGET BigQueryTarget USING BigQueryWriter (
  Tables: '',
  projectId:'',
  BatchPolicy: 'eventCount:1, Interval:1',
  ServiceAccountKey: '',
)
INPUT FROM @APPNAME@_CQOut1;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

stop application PublishAPITester.NS;
undeploy application PublishAPITester.NS;
drop application PublishAPITester.NS cascade;

create application NS;

create Stream MyStream of Global.WAEvent;

CREATE TYPE bankData
(
bankID Integer KEY,
bankName String
);


CREATE TYPE moreBankData
(
bankID Integer KEY,
bankName String,
bankRouting long,
bankAmount double
);

create stream bankStream of bankData;

create stream dataStream of moreBankData;

--create Target t3 using SysOut(name:AgentOut) input from MyStream;

end application NS;

DEPLOY APPLICATION NS; 
start NS;

STOP APPLICATION DGLimitServerApp1;
UNDEPLOY APPLICATION DGLimitServerApp1;
DROP APPLICATION DGLimitServerApp1 CASCADE;
CREATE APPLICATION DGLimitServerApp1;
CREATE FLOW DGLimitServerAgentFlow;
CREATE OR REPLACE SOURCE DGLimitServerApp1_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO DGLimitServerApp1_SampleStream;
END FLOW DGLimitServerAgentFlow;
CREATE FLOW DGLimitServerServerFlow;
CREATE OR REPLACE TARGET DGLimitServerApp1_NullTarget using NullWriter()
INPUT FROM DGLimitServerApp1_SampleStream;
END FLOW DGLimitServerServerFlow;
END APPLICATION DGLimitServerApp1;
deploy application DGLimitServerApp1 on any in ServerDG1 with DGLimitServerAgentFlow on any in Agents, DGLimitServerServerFlow on any in ServerDG1;
START APPLICATION DGLimitServerApp1;

STOP APPLICATION DGLimitServerApp2;
UNDEPLOY APPLICATION DGLimitServerApp2;
DROP APPLICATION DGLimitServerApp2 CASCADE;
CREATE APPLICATION DGLimitServerApp2;
CREATE FLOW DGLimitServerAgentFlow2;
CREATE OR REPLACE SOURCE DGLimitServerApp2_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO DGLimitServerApp2_SampleStream;
END FLOW DGLimitServerAgentFlow2;
CREATE FLOW DGLimitServerServerFlow2;
CREATE OR REPLACE TARGET DGLimitServerApp2_NullTarget using NullWriter()
INPUT FROM DGLimitServerApp2_SampleStream;
END FLOW DGLimitServerServerFlow2;
END APPLICATION DGLimitServerApp2;
deploy application DGLimitServerApp2 on any in ServerDG1 with DGLimitServerAgentFlow2 on any in Agents, DGLimitServerServerFlow2 on any in ServerDG1;
START APPLICATION DGLimitServerApp2;

STOP APPLICATION ExceedApp1;
UNDEPLOY APPLICATION ExceedApp1;
DROP APPLICATION ExceedApp1 CASCADE;
CREATE APPLICATION ExceedApp1;
CREATE FLOW ExceedAgentFlow;
CREATE OR REPLACE SOURCE ExceedApp1_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO ExceedApp_SampleStream;
END FLOW ExceedAgentFlow;
CREATE FLOW ExceedServerFlow;
CREATE OR REPLACE TARGET ExceedApp1_NullTarget using NullWriter()
INPUT FROM ExceedApp_SampleStream;
END FLOW ExceedServerFlow;
END APPLICATION ExceedApp1;
deploy application ExceedApp1 on any in ServerDG1 with ExceedAgentFlow on any in Agents, ExceedServerFlow on any in ServerDG1;

create application sorted;

create source AALSortedSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'sorted_log',
  charset:'UTF-8',
  positionByEOF:false
) PARSE USING AALParser (
  columndelimiter:' ',
  IgnoreEmptyColumn:'Yes',
  columndelimittill:5
) OUTPUT TO AalSortedStream;

create Target AALSortedDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/sorted_log') input from AalSortedStream;

end application sorted;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using AzureblobWriter(
    accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:7'
)
format using DSVFormatter (
)
input from @STREAM@;

end flow @APPNAME@_serverflow;

end application @APPNAME@;

stop httpjsonapp;
undeploy application httpjsonapp;
drop APPLICATION httpjsonapp cascade;
create application httpjsonapp;
create source HTTPSource using HTTPReader (
        IpAddress:'127.0.0.1',
        PortNo:'10001'
) OUTPUT TO HttpDataStream;

create Target HttpDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/httpjsondata') input from HttpDataStream;
end application httpjsonapp;

STOP APPLICATION KafkaWPTester.KWApp;
STOP APPLICATION KafkaWPTester.KRApp;
UNDEPLOY APPLICATION KafkaWPTester.KWApp;
UNDEPLOY APPLICATION KafkaWPTester.KRApp;
DROP APPLICATION KafkaWPTester.KWApp CASCADE;
DROP APPLICATION KafkaWPTester.KRApp CASCADE;

CREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;
GRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;
CONNECT KafkaWPTester KafkaWPTester;


CREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE CSVSource USING FileReader (
	directory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',
    WildCard:'access_log_half',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;

CREATE TYPE AccessLogType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;

CREATE CQ AceeslogCQ
INSERT INTO TypedAccessLogStream
SELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]
FROM FileStream;

create Target DsvWriter using KafkaWriter VERSION '0.8.0' ( 
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V8dsvExistTopic',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream;


create Target JsonWriter using KafkaWriter VERSION '0.8.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V8jsonExistTopic',
	Mode:'Sync'
    )
 
FORMAT USING JSONFormatter ()
input from TypedAccessLogStream;


create Target AvroWriter using KafkaWriter VERSION '0.8.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V8avroExistTopic',
    Mode:'Sync'
    )
 
FORMAT USING AvroFormatter (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')
input from TypedAccessLogStream;


END APPLICATION KWApp;

DEPLOY APPLICATION KWApp on any in default;


-- Kafka Reader Apps

CREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;


CREATE TYPE AccessLogType2(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE Stream KafkaDSVReaderStream of AccessLogType2;
CREATE Stream KafkaJsonReaderStream of AccessLogType2;
CREATE Stream KafkaAvroReaderStream of AccessLogType2;


CREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '0.8.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V8dsvExistTopic',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;




CREATE CQ DSVtoKafkaDSVReaderStream
INSERT INTO KafkaDSVReaderStream
SELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]
FROM KafkaReaderStream1;

CREATE TARGET DSVDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaDSVReaderStream;

CREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '0.8.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V8jsonExistTopic',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;




CREATE CQ DSVtoKafkaJsonReaderStream
INSERT INTO KafkaJsonReaderStream
SELECT 
data.get('Col1').textValue(),
data.get('Col2').textValue(),
data.get('Col3').textValue(),
data.get('Col4').textValue(),
data.get('Col5').textValue(),
data.get('Col6').textValue(),
data.get('Col7').textValue(),
data.get('Col8').textValue(),
data.get('Col9').textValue(),
data.get('Col10').textValue()
FROM KafkaReaderStream2;

CREATE TARGET JSONDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8_RT_JSON')
FORMAT USING DSVFormatter()
INPUT FROM KafkaJsonReaderStream;

CREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '0.8.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V8avroExistTopic',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'
)
OUTPUT TO KafkaReaderStream3;



CREATE Stream KRTypedStream of AccessLogType;

CREATE CQ DSVtoKafkaAvroReaderStream
INSERT INTO KafkaAvroReaderStream
SELECT    
data.get("Col1").toString(), 
data.get("Col2").toString(), 
data.get("Col3").toString(), 
data.get("Col4").toString(),
data.get("Col5").toString(), 
data.get("Col6").toString(), 
data.get("Col7").toString(), 
data.get("Col8").toString(),
data.get("Col9").toString(), 
data.get("Col10").toString()
FROM KafkaReaderStream3;

CREATE TARGET AVRODump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8_RT_AVRO')
FORMAT USING DSVFormatter()
INPUT FROM KafkaAvroReaderStream;

END APPLICATION KRApp;
DEPLOY APPLICATION KRApp on any in default;

STOP BuiltInTester.AnomalyBoundTest;
UNDEPLOY APPLICATION BuiltInTester.AnomalyBoundTest;
DROP APPLICATION BuiltInTester.AnomalyBoundTest CASCADE;

CREATE APPLICATION AnomalyBoundTest;

CREATE OR REPLACE TYPE s1_Type  ( mString java.lang.String , 
mDouble java.lang.Double , 
mInt java.lang.Integer , 
mShort java.lang.Short , 
mLong java.lang.Long , 
mFloat java.lang.Float  
 );

CREATE OR REPLACE STREAM s1 OF s1_Type;

CREATE  WINDOW anomalyWin OVER s1 KEEP 4 ROWS;

CREATE OR REPLACE TYPE S3_Type  ( val java.lang.Integer , 
upperInt java.lang.Double , 
lowerInt java.lang.Double , 
upperDouble java.lang.Double , 
lowerDouble java.lang.Double , 
upperFloat java.lang.Double , 
lowerFloat java.lang.Double , 
upperLong java.lang.Double , 
lowerLong java.lang.Double , 
upperShort java.lang.Double , 
lowerShort java.lang.Double  
 ) ;

CREATE WACTIONSTORE anomalyWS  CONTEXT OF S3_Type
EVENT TYPES(S3_Type )
@PERSIST-TYPE@


CREATE OR REPLACE STREAM S3 OF S3_Type;

CREATE OR REPLACE CQ wsCQ 
INSERT INTO anomalyWS
SELECT * FROM S3 s;
;

CREATE OR REPLACE CQ anomalyCQ 
INSERT INTO S3
SELECT last(mInt) as val, 
round_double(distributionUpperBound(mInt), 2) as upperInt,  round_double(distributionLowerBound(mInt), 2) as lowerInt, round_double(distributionUpperBound(mDouble), 2) as upperDouble,  round_double(distributionLowerBound(mDouble), 2) as lowerDouble, round_double(distributionUpperBound(mFloat), 2) as upperFloat, round_double(distributionLowerBound(mFloat), 2) as lowerFloat, round_double(distributionUpperBound(mLong), 2) as upperLong, round_double(distributionLowerBound(mLong), 2) as lowerLong, round_double(distributionUpperBound(mShort), 2) as upperShort, round_double(distributionLowerBound(mShort), 2) as lowerShort 
FROM anomalyWin a
;

CREATE OR REPLACE TYPE hpstream3_Type  ( hpTime org.joda.time.DateTime , 
mDouble java.lang.Double , 
mInt java.lang.Integer , 
mShort java.lang.Short , 
mLong java.lang.Long  
 );

CREATE OR REPLACE SOURCE housePowerCSV USING FileReader  ( 
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '@TEST-DATA-PATH@',
  skipbom: true,
  wildcard: 'anomalyBound.csv'
 ) 
 PARSE USING DSVParser  ( 
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: '-1',
  nocolumndelimiter: false,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: true,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 ) 
OUTPUT TO housePowerStream ;

CREATE OR REPLACE CQ HPparseCQ 
INSERT INTO s1
SELECT to_string(data[0]) as mString, to_double(data[0]) as mDouble,  to_int(data[0]) as mInt, to_short(data[0]) as mShort, to_long(data[0]) as mLong, to_float(data[0]) as mFloat 
FROM housePowerStream s;

CREATE OR REPLACE TYPE hpstream_Type  ( hpTime org.joda.time.DateTime , 
mDouble java.lang.Double , 
mInt java.lang.Integer , 
mShort java.lang.Short , 
mString java.lang.String  
 );

CREATE OR REPLACE TYPE hqType  ( hpTime org.joda.time.DateTime , 
m1 java.lang.Double , 
m2 java.lang.Double , 
m3 java.lang.Double , 
m4 java.lang.Double  
 );

CREATE OR REPLACE TYPE s2_Type  ( val java.lang.Integer , 
upper java.lang.Double , 
lower java.lang.Double , 
isAnomaly java.lang.Boolean  
 );

CREATE OR REPLACE TYPE hpStream2_Type  ( hpTime org.joda.time.DateTime , 
m1 java.lang.Double , 
m2 java.lang.Double , 
m3 java.lang.Double , 
m4 java.lang.Double  
 );

END APPLICATION AnomalyBoundTest;

STOP OuterJoinTester.OJApp;
UNDEPLOY APPLICATION OuterJoinTester.OJApp;
DROP APPLICATION OuterJoinTester.OJApp cascade;

CREATE APPLICATION OJApp;

CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;


CREATE source wsSource2 USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'bankCards.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO stream2;



CREATE TYPE bankData
(
bankID Integer KEY,
bankName String
);

CREATE TYPE cardData
(
cardID Integer KEY,
cardName String
);

CREATE STREAM wsStream OF bankData;
CREATE STREAM wsStream2 OF cardData;


CREATE CACHE cache1 USING CsvReader(
directory: '@TEST-DATA-PATH@',
wildcard: 'banks.csv',
header: No,
columndelimiter: ',',
trimquote: false
) QUERY (keytomap:'bankID') OF bankData;


CREATE CACHE cache2 USING CsvReader(
directory: '@TEST-DATA-PATH@',
wildcard: 'bankCards.csv',
header: No,
columndelimiter: ',',
trimquote: false
) QUERY (keytomap:'cardID') OF cardData;


CREATE WACTIONSTORE oneWS CONTEXT OF bankData
EVENT TYPES(bankData )
@PERSIST-TYPE@

CREATE WACTIONSTORE twoWS CONTEXT OF cardData
EVENT TYPES(cardData )
@PERSIST-TYPE@



CREATE CQ csvTobankData
INSERT INTO oneWS
SELECT TO_INT(data[0]), data[1] FROM QaStream;



CREATE CQ csvTobankData2
INSERT INTO wsStream
SELECT TO_INT(data[0]), data[1] FROM QaStream;

CREATE CQ csvTobankData3
INSERT INTO wsStream2
SELECT TO_INT(data[0]), data[1] FROM stream2;

CREATE CQ csvTobankData4
INSERT INTO twoWS
SELECT TO_INT(data[0]), data[1] FROM stream2;


CREATE JUMPING WINDOW win1 OVER wsStream KEEP 20 rows;


CREATE JUMPING WINDOW win2 OVER wsStream2 KEEP 4 rows;



END APPLICATION OJApp;

STOP APPLICATION OneAgentEncryptionTester.CSV;
UNDEPLOY APPLICATION OneAgentEncryptionTester.CSV;
DROP APPLICATION OneAgentEncryptionTester.CSV cascade;

create application CSV WITH ENCRYPTION;

CREATE FLOW AgentFlow;
create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails-agent.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStream1;


CREATE TYPE MyTypeCsv(
PAN String,
FNAME String KEY,
LNAME String,
ADDRESS String,
CITY String,
STATE String,
ZIP String,
GENDER String
);

CREATE STREAM TypedStreamCsv of MyTypeCsv;

CREATE CQ TypeConversionCQCsv
INSERT INTO TypedStreamCsv
SELECT
data[0],
data[1],
data[2],
data[3],
data[4],
data[5],
data[6],
data[7]
from CsvStream1;

END FLOW AgentFlow;

CREATE FLOW ServerFlow;

CREATE WACTIONSTORE StoreInfoCsv CONTEXT OF MyTypeCsv
EVENT TYPES ( MyTypeCsv )
@PERSIST-TYPE@

CREATE CQ StoreWactionCsv
INSERT INTO StoreInfoCsv
SELECT * FROM TypedStreamCsv
LINK SOURCE EVENT;


END FLOW ServerFlow;

end application CSV;

DEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;
START CSV;

-- The PosApp sample application demonstrates how a credit card
-- payment processor might use Striim to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.


STOP admin.PosApp;
UNDEPLOY APPLICATION admin.PosApp;
Drop Application admin.PosApp cascade;
CREATE APPLICATION PosApp;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING FileReader (
  directory:'/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/appData',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells Striim that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells Striim to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvToPosData

CREATE CQ CsvToPosData
INSERT INTO PosDataStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP.
--
-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as
-- the types to be used in PosDataStream, which is created automatically.

-- The DATETIME field from the source is converted to both a dateTime
-- value, used as the event timestamp by the application, and (via the
-- function) an integer hourValue, which is used to look up
-- historical hourly averages from the HourlyAveLookup cache,
-- discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)
-- 17 (hourValue)
-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateOnly
--
-- The PosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: '/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/appData',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;


-- WAction store MerchantActivity
--
-- The following group of statements create and populate the MerchantActivity
-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced
-- with merchant details from NameLookup cache and with latitude and longitude
-- values from the USAddressData cache.

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
PERSIST EVERY 10 second USING ( JDBC_DRIVER:'org.apache.derby.jdbc.ClientDriver',  JDBC_URL:'jdbc:derby://192.168.1.5:1527/wactionrepos', JDBC_USER:'waction', JDBC_PASSWORD:'w@ct10n', pu_name:derby, DDL_GENERATION:'create-or-extend-tables',  LOGGING_LEVEL:'SEVERE' );


CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/appData',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false,
  trimwhitespace:true
) 
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: '/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/appData',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'zip') OF USAddressData;


CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

-- CQ GenerateAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;

END APPLICATION PosApp;

CREATE DASHBOARD USING "/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/PosAppDashboard.json";

undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;
Create Source @APPNAME@_s Using DatabaseReader
(
 Username:'@UNAME@',
 Password:'@PASSWORD@',
 ConnectionURL:'@SRCURL@',
 Tables:'QATEST.HIVE_IL_%',
 FetchSize:1,
 QuiesceOnILCompletion: true
)
Output To @APPNAME@_ss;


create Target @APPNAME@_t using HiveWriter(
            ConnectionURL:'@TGTURL@',
            Username:'@TGTUNAME@', 
            Password:'@TGTPASSWORD@',
            hadoopurl:'hdfs://dockerhost:9000/',
	        Mode:'initialLoad',
	        mergepolicy: '@MERGEPOLICY@',
            Tables:'QATEST.HIVE_IL_01,default.hive_il_01;QATEST.HIVE_IL_02,default.hive_il_02',
            hadoopConfigurationPath:'@CONF@'
	) input from @APPNAME@_ss;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

-- The PosApp sample application demonstrates how a credit card
-- payment processor might use Striim to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.

stop test.PosApp;
undeploy application test.PosApp;
drop application test.PosApp cascade;

CREATE APPLICATION PosApp;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING FileReader (
  directory:'Samples/Customer/PosApp/appData',
  wildcard : '$admin.wildcard',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells Striim that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells Striim to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvToPosData

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP.
--
-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as
-- the types to be used in PosDataStream, which is created automatically.

-- The DATETIME field from the source is converted to both a dateTime
-- value, used as the event timestamp by the application, and (via the
-- function) an integer hourValue, which is used to look up
-- historical hourly averages from the HourlyAveLookup cache,
-- discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)
-- 17 (hourValue)
-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateOnly
--
-- The PosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: 'Samples/Customer/PosApp/appData',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startingTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;


-- WAction store MerchantActivity
--
-- The following group of statements create and populate the MerchantActivity
-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced
-- with merchant details from NameLookup cache and with latitude and longitude
-- values from the USAddressData cache.

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );


CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'Samples/Customer/PosApp/appData',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false
)
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: 'Samples/Customer/PosApp/appData',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;


-- CQ GenerateAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;



--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM AlertStream;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;


END APPLICATION PosApp;


CREATE DASHBOARD USING "Samples/Customer/PosApp/PosAppDashboard.json";

--This TQL only works with HL7v2 2.3 Messages, specifically generated from SimHospital Docker Simulator

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE STREAM @APPNAME@_AlertStream OF Global.AlertEvent;

CREATE OR REPLACE SOURCE @APPNAME@_src USING Global.TCPReader ()
PARSE USING Global.HL7v2Parser ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE CQ @APPNAME@_ADT_CQ
INSERT INTO @APPNAME@_ADT_CQOut
SELECT 
data.element("MSH").element("MSH.6").element("HD.1").getText() as Facility,
TO_DATEF(data.element("MSH").element("MSH.7").element("TS.1").getText(),'yyyyMMddHHmmss') as MsgTime,
data.element("PID").element("PID.3").element("CX.1").getText() as PatientID, 
data.element("PID").element("PID.5").element("XPN.1").getText() as FirstName, 
data.element("PID").element("PID.5").element("XPN.2").getText() as LastName, 
TO_DATEF(SLEFT(data.element("PID").element("PID.7").element("TS.1").getText(),8),'yyyyMMdd') as DOB, 
data.element("PID").element("PID.8").getText() as Gender, 
data.element("PID").element("PID.11").element("XAD.5").getText() as ZipCode,
data.element("PV1").element("PV1.2").getText() as PatientClass,  
data.element("EVN").element("EVN.1").getText() as EventCode,
TO_DATEF(data.element("EVN").element("EVN.2").element("TS.1").getText(),'yyyyMMddHHmmss') as EventTime 
FROM @APPNAME@_Stream where data.getName() like "ADT%";

CREATE OR REPLACE CQ @APPNAME@_ORU_CQ
INSERT INTO @APPNAME@_ORU_CQOut
SELECT 
data.element("MSH").element("MSH.6").element("HD.1").getText() as Facility,
TO_DATEF(data.element("MSH").element("MSH.7").element("TS.1").getText(),'yyyyMMddHHmmss') as MsgTime,
data.element("ORU_R01.RESPONSE").element("ORU_R01.PATIENT").element("PID").element("PID.3").element("CX.1").getText() as PatientID,
data.element("ORU_R01.RESPONSE").element("ORU_R01.ORDER_OBSERVATION").element("OBR").element("OBR.4").element("CE.1").getText() as OrderIdentifier,
data.element("ORU_R01.RESPONSE").element("ORU_R01.ORDER_OBSERVATION").element("OBR").element("OBR.4").element("CE.2").getText() as OrderText 
FROM @APPNAME@_Stream where data.getName() like "ORU%";

CREATE TARGET @APPNAME@_FileTarget USING FileWriter ()
FORMAT USING Global.XMLFormatter  (
  rootelement: 'document' )
INPUT FROM @APPNAME@_Stream;

CREATE TARGET @APPNAME@_OLTPTarget USING DatabaseWriter ()
INPUT FROM @APPNAME@_ADT_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_NoSqlTarget USING MongoDBWriter ()
INPUT FROM @APPNAME@_ADT_CQOut;

--CREATE OR REPLACE TARGET @APPNAME@_KafkaTarget USING KafkaWriter VERSION @KAFKA_VERSION@()
--FORMAT USING JSONFormatter (
--schemaFileName: 'avroSchema'
--)
--INPUT FROM @APPNAME@_ORU_CQOut;

CREATE TARGET @APPNAME@_DWHTarget USING BigQueryWriter ()
INPUT FROM @APPNAME@_ORU_CQOut;

CREATE TARGET @APPNAME@_ADLSTarget USING AdlsGen2Writer ()
FORMAT USING Global.JSONFormatter  ()
INPUT FROM @APPNAME@_ORU_CQOut;

CREATE OR REPLACE CQ @APPNAME@_ADT_FilterCQ
INSERT INTO @APPNAME@_AlertCQ
SELECT 
PatientID as PatientID,
PatientClass as PatientClass
FROM @APPNAME@_ADT_CQOut p where  p.PatientClass='E';;

CREATE OR REPLACE CQ @APPNAME@_AlertEventCQ
INSERT INTO @APPNAME@_AlertStream
SELECT 
'Patient Found' as name,
'Emergency' as keyVal,
'info' as severity,
'raise' as flag,
'Flagged patient `' + f.PatientID +'` is an Emergency Patient. NEED IMMEDIATE ATTENTION!!!' as message
FROM @APPNAME@_AlertCQ f;

CREATE OR REPLACE TARGET @APPNAME@_SlackTarget USING SlackAlertAdapter ()
INPUT FROM @APPNAME@_AlertStream;

END APPLICATION @APPNAME@;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

create Target FileTarget using FileWriter(
    rolloverpolicy:'@UPLOAD-SIZE@',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using XMLFormatter (
charset:'@charset@',
rootelement:'@mem@'
)
input from CsvStream;

end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

STOP APPLICATION TCPReader.TCPAPP;
UNDEPLOY APPLICATION TCPReader.TCPAPP;
DROP APPLICATION TCPReader.TCPAPP cascade;

CREATE APPLICATION TCPAPP;


CREATE SOURCE Tsource USING TCPReader (
@CHARSET@,
IpAddress:'@TCPREADERIPADDR@',
PortNo:'@TCPREADERPORT@'
)
PARSE USING @TSOURCEFORMATTERTYPE@ (
@TSOURCEFORMATTERMEMBERS@
)
OUTPUT TO TCPStream;


END APPLICATION TCPAPP;
deploy application TCPAPP in default;
start application TCPAPP;

CREATE OR REPLACE PROPERTYVARIABLE Mode='sync';
CREATE OR REPLACE PROPERTYVARIABLE BatchPolicy='Size:900000,Interval:1';
create application KinesisTest;
create source CSVSource using FileReader (
	directory:'/home/dz/src/product/Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  companyName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0], data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	BatchPolicy: '$BatchPolicy',
    Mode: '$Mode'
)
format using XMLFormatter (
	rootelement:'document',
	elementtuple:'CompanyName:merchantid:text=companyname,ZipCode:text=zip,Amount:text=amount'
)
input from TypedCSVStream;
end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using googlepubsubwriter(
    ServiceAccountKey:'@SAS-KEY@',
ProjectId:'@PROJECTID@',
topic:'@topic@',
BatchPolicy:'@BATCHPOLICY@'
)
format using DSVFormatter (
)
input from @STREAM@;


end flow @APPNAME@_serverflow;

end application @APPNAME@;

stop application SQLtoRedshift;
undeploy application SQLtoRedshift;
drop application SQLtoRedshift cascade;
CREATE APPLICATION SQLtoRedshift RECOVERY 1 SECOND INTERVAL;

CREATE  SOURCE SQLSource USING MSSqlReader  ( 
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  DatabaseName: '@DATABASE-NAME@',
  ConnectionURL: '@URL@',
  Tables: '@SOURCE-TABLES@'
 ) 
OUTPUT TO sqlstream ;

--CREATE  TARGET t2 USING SysOut  ( name: 'sqltors') INPUT FROM sqlstream;

CREATE TARGET RedshiftTarget USING RedshiftWriter
	(
	  ConnectionURL: '@TARGET-URL@',
	  Username: '@TARGET-UNAME@',
	  Password: '@TARGET-PASSWORD@',
	  bucketname: '@BUCKETNAME@',
	  --accesskeyId: '@ACCESS-KEY-ID@',
	  --secretaccesskey: '@SECRET-ACCESS-KEY@',
	  S3IAMRole:'@IAMROLE@',
	  Tables: '@TARGET-TABLES@',
	  uploadpolicy:'eventcount:1,interval:5s',
	  Mode:'incremental'
	) INPUT FROM sqlstream;

END APPLICATION SQLtoRedshift;
deploy application SQLtoRedshift;
start application SQLtoRedshift;

Alter application app1;

CREATE OR REPLACE SOURCE s USING oracleReader  ( 
  Username:'qatest',
  Password:'qatest',
  ConnectionURL:'localhost:1521/xe',
  Tables:'QATEST.test%',
  FetchSize:2
 ) 
OUTPUT TO rawstream;
Alter application app1 recompile;



Alter application app2;
CREATE or replace TARGET app2_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Interval:60',
CommitPolicy:'Interval:60',
Tables:'qatest.test01,QATEST.KPS1'
) INPUT FROM rawstream;
Alter application app2 recompile;




Alter application app3;

CREATE or replace TARGET app3_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Interval:60',
CommitPolicy:'Interval:60',
Tables:'qatest.test02,QATEST.KPS2'
) INPUT FROM rawstream;

Alter application app3 recompile;



Alter application app4;

CREATE or replace TARGET app4_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Interval:60',
CommitPolicy:'Interval:60',
Tables:'qatest.test03,QATEST.KPS3'
) INPUT FROM rawstream;

Alter application app4 recompile;

create Target @TARGET_NAME@ using ADLSGen2Writer(
          accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'%@metadata(TableName)%',
        filename:'table.csv',
        uploadpolicy:'filesize:10M'
)
FORMAT USING dsvFormatter()
input from @STREAM@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ recovery 5 second Interval;

CREATE SOURCE @src1Name@ USING MySQLReader (
  Username:'@srcusername@',
  Password:'@srcpassword@',
  ConnectionURL:'@srcurl@',
  Tables:'@srcschema@.@srctable@',
  BidirectionalMarkerTable: '@srcschema@.@srcbidirectionaltable@'
)
OUTPUT TO @outstream1name@;

CREATE TARGET @tgt1Name@ USING DatabaseWriter (
  ConnectionURL:'@tgturl@',
  Username:'@tgtusername@',
  Password:'@tgtpassword@',
  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
  CheckPointTable: 'CHKPOINT',
  BidirectionalMarkerTable: '@tgtschema@.@tgtbidirectionaltable@'
)
INPUT FROM @instream1name@;

CREATE SOURCE @src2Name@ USING MSSQLReader (
  ConnectionURL:'@tgturl@',
  DatabaseName:'@databasename@',
  Username:'@tgtusername@',
  Password:'@tgtpassword@',
  Tables:'@tgtschema@.@tgttable@',
  BidirectionalMarkerTable: '@tgtschema@.@tgtbidirectionaltable@',
  TransactionSupport: true
)
OUTPUT TO @outstream2name@;

CREATE TARGET @tgt2Name@ USING DatabaseWriter (
  Username:'@srcusername@',
  Password:'@srcpassword@',
  ConnectionURL:'@srcurl@',
  Tables: '@tgtschema@.@tgttable@,@srcschema@.@srctable@',
  CheckPointTable: 'CHKPOINT',
  BidirectionalMarkerTable: '@srcschema@.@srcbidirectionaltable@'
)
INPUT FROM @instream2name@;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP APPLICATION DBRTOCW;
UNDEPLOY APPLICATION DBRTOCW;
DROP APPLICATION DBRTOCW CASCADE;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  --QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 ) OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) INPUT FROM Oracle_ChangeDataStream;


CREATE TARGET t2 USING SysOut(name:Foo2) INPUT FROM Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;

DEPLOY APPLICATION DBRTOCW on ANY in default;

START APPLICATION DBRTOCW;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@;

create TYPE CountTYPE(numcol INT);

CREATE JUMPING WINDOW nEvents OVER @STREAM@ KEEP 10 ROWS;

CREATE STREAM TypedCountStream of CountTYPE;

CREATE CQ CountCQ INSERT INTO TypedCountStream SELECT TO_INT(data[0]) FROM nEvents;

STOP Istreamer.ISAPP;
UNDEPLOY APPLICATION Istreamer.ISAPP;
DROP APPLICATION Istreamer.ISAPP CASCADE;

CREATE APPLICATION ISAPP;


CREATE source implicitSource USING FileReader (
      directory:'@TEST-DATA-PATH@',
      columndelimiter: ',',
      wildcard:'ISdata.csv',
      blocksize: 10240,
      positionByEOF:false
)
PARSE USING DSVParser (
      header:False,
      trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity double,
  size long,
  currentDate DateTime);

CREATE CACHE cache1 USING CsvReader(
  directory: '@TEST-DATA-PATH@',
  wildcard: 'ISdata.csv',
  header: false,
  columndelimiter: ',',
  trimquote: false
  ) QUERY (keytomap:'productID') OF Atm;


CREATE STREAM newStream OF Atm;


CREATE CQ newCQ
INSERT INTO newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]), TO_DATE(data[5]) FROM
CsvStream;

CREATE WINDOW win1
OVER newStream
KEEP 50 rows;


CREATE CQ newCQ2
INSERT INTO newStream2
SELECT productID as A , stateID AS B, productWeight AS C, quantity AS D, size AS E, currentDate AS F FROM
newStream;


CREATE CQ newCQ3
INSERT INTO newStream3 PARTITION BY A
SELECT A,B,C,D,E,F FROM newStream2
link source event;

CREATE CQ newCQ4
INSERT INTO newStream4
SELECT count(productID),currentDate FROM newStream ORDER BY currentDate
link source event;

CREATE CQ newCQ5
INSERT INTO newStream5
SELECT x.*, y.* from cache1 x, newStream y WHERE x.productweight > 6 ORDER BY x.currentDate, x.productID;


CREATE WACTIONSTORE WS1 CONTEXT OF Atm
EVENT TYPES(Atm );

CREATE CQ newCQ6
INSERT INTO WS1
SELECT * FROM newStream WHERE productID = '001';

CREATE CQ newCQ7
INSERT INTO newStream6
SELECT aa.productID FROM WS1 [push] aa, cache1 bb;

CREATE CQ newCQ8
INSERT INTO newStream7
SELECT Sum(X.size) FROM (Select size from win1 where productweight > 5) X;

CREATE CQ newCQ9
INSERT INTO newStream8
SELECT count(productID) FROM WS1 [push] ORDER BY productID;


END APPLICATION ISAPP;
deploy APPLICATION ISAPP;

--
-- Kafka Stream Agent Checkpoint Recovery tests
-- Bert Hashemi and Zalak Shah, WebAction, Inc.
--

stop application recoveryTestAgent.KSRecovCSV;
undeploy application recoveryTestAgent.KSRecovCSV;
drop application recoveryTestAgent.KSRecovCSV cascade;
DROP USER recoveryTestAgent;
DROP NAMESPACE recoveryTestAgent CASCADE;
CREATE USER recoveryTestAgent IDENTIFIED BY recoveryTestAgent;
GRANT create,drop ON deploymentgroup Global.* TO USER recoveryTestAgent;
CONNECT recoveryTestAgent recoveryTestAgent;

create application KSRecovCSV
RECOVERY 5 SECOND INTERVAL;

CREATE FLOW AgentFlow;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails-recovery.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO KafkaCsvStream;

END FLOW AgentFlow;

CREATE FLOW ServerFlow;
CREATE TYPE UserDataType
(
  UserId String KEY,
  UserName String
);

CREATE STREAM UserDataStream OF UserDataType PARTITION BY UserId;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  data[0],
        data[1]
FROM KafkaCsvStream;

CREATE WACTIONSTORE UserActivityInfo
CONTEXT OF UserDataType
EVENT TYPES ( UserDataType )
@PERSIST-TYPE@

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction
INSERT INTO UserActivityInfo
SELECT * FROM UserDataStream
LINK SOURCE EVENT;
END FLOW ServerFlow;

END APPLICATION KSRecovCSV;
DEPLOY APPLICATION KSRecovCSV with AgentFlow in AGENTS, ServerFlow on any in default;
START KSRecovCSV;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING PostgreSQLReader  (
  ReplicationSlotName: 'Slot_Name',
  FilterTransactionBoundaries: 'true',
  Username: 'User_Name',
  ConnectionURL: 'Connection_URL',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'Password',
  Tables: 'Tables'
 )
OUTPUT TO @STREAM@ ;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 USING PostgreSQLReader  (
  ReplicationSlotName: 'Slot_Name',
  FilterTransactionBoundaries: 'true',
  Username: 'User_Name',
  ConnectionURL: 'Connection_URL',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'Password',
  Tables: 'Tables'
 )
OUTPUT TO @STREAM@ ;

CREATE TARGET @SOURCE_NAME@_sysout USING Global.SysOut (
  name: '@SOURCE_NAME@_sysout' )
INPUT FROM @STREAM@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
CREATE SOURCE @APPNAME@_S USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.test01',
  connectionRetryPolicy:'retryInterval=1,maxRetries=3'
 ) 
OUTPUT TO @APPNAME@_SS;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'public.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

STOP APPLICATION BQ;
UNDEPLOY APPLICATION BQ;
DROP APPLICATION BQ CASCADE;
CREATE APPLICATION BQ RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE s USING FileReader (
	directory:'Product/IntegrationTests/TestData/',
    WildCard:'posdata5L.csv',
	positionByEOF:false
	)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO ss;

CREATE TYPE userType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String
);

CREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;

CREATE CQ cq1
INSERT INTO userDefinedTypedStream
SELECT data[0],data[1],data[2], data[3],data[4]
FROM ss;

CREATE or replace TARGET TABLE1 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE1@.TABLE1',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE2 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE2@.TABLE2',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE3 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE3@.TABLE3',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE4 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE4@.TABLE4',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE5 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE5@.TABLE5',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE6 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE6@.TABLE6',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE7 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE7@.TABLE7',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE8 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE8@.TABLE8',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE9 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE9@.TABLE9',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

CREATE or replace TARGET TABLE10 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'@TABLE10@.TABLE10',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:10',
StandardSQL:true	
) INPUT FROM userDefinedTypedStream;

END APPLICATION BQ;
DEPLOY APPLICATION BQ;
start application BQ;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.test01',
	FetchSize: '1',
	connectionRetryPolicy:'timeOut=00,retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(id,name) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true		
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

UNDEPLOY APPLICATION admin.BasicAppNoFlow;
DROP APPLICATION admin.BasicAppNoFlow cascade;

CREATE APPLICATION BasicAppNoFlow;

CREATE SOURCE CsvDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt',
  header: Yes,
  columndelimiter: ',',
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE MerchantHourlyAve(
  merchantId	String,
  hourValue int,
  hourlyAve int
);

CREATE STREAM MerchantHourlyStream OF MerchantHourlyAve PARTITION BY merchantId;

CREATE CQ CsvToPosData
INSERT INTO MerchantHourlyStream
SELECT data[1], TO_INT(data[2]),
       TO_INT(data[3])
FROM CsvStream;


END APPLICATION BasicAppNoFlow;

STOP rest2.applicationApi;
UNDEPLOY APPLICATION rest2.applicationApi;
DROP APPLICATION rest2.applicationApi cascade;

CREATE APPLICATION applicationApi;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)

PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;


CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startingTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
@PERSIST-TYPE@


CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false,
  trimwhitespace:true
) 
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'zip') OF USAddressData;


CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;
        
END APPLICATION applicationApi;

stop Application zoneDateTime_CaseSensistive_CQ;
undeploy application zoneDateTime_CaseSensistive_CQ;
drop application zoneDateTime_CaseSensistive_CQ cascade;

create application zoneDateTime_CaseSensistive_CQ;

CREATE OR REPLACE SOURCE csvinput USING FileReader  ( 
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '@ResourceDirectory@',
  skipbom: true,
  wildcard: 'ZonedDateTime_data.csv'
 ) 
 PARSE USING DSVParser  ( 
  _h_ReturnDateTimeAs: 'ZonedDateTime'
 ) 
OUTPUT TO csvinput_out ;

CREATE  TYPE cq_zonedtime_out_Type  ( 
a java.lang.Integer , 
b string,
c string,
d string,
e string,
f string
--b java.time.ZonedDateTime , 
--c java.time.ZonedDateTime ,
--d java.time.ZonedDateTime , 
--e java.time.ZonedDateTime ,
--f java.time.ZonedDateTime   
 );

CREATE STREAM cq_zonedtime_out OF cq_zonedtime_out_Type;

CREATE OR REPLACE   CQ cq_zonedtime 
INSERT INTO cq_zonedtime_out
SELECT TO_INT(data[0]) as a,
TO_STRING(TO_ZONEDDATETIME(data[1],"dd-MMM-yy hh.mm.ss.SSSSSSSSS a"),"dd-MMM-yy hh.mm.ss.SSSSSSSSS a") as b,
TO_STRING(TO_ZONEDDATETIME(data[2], "dd-MMM-yy hh.mm.ss.SSSSSSSSS a z"),"dd-MMM-yy hh.mm.ss.SSSSSSSSS a z") as c,
TO_STRING(TO_ZONEDDATETIME(data[3], "EEEEE dd-MMM-yy hh.mm.ss.SSSSSSSSS a"),"EEEEE dd-MMM-yy hh.mm.ss.SSSSSSSSS a") as d,
TO_STRING(TO_ZONEDDATETIME(data[4], "E dd-MMM-yy hh.mm.ss.SSSSSSSSS a z"),"E dd-MMM-yy hh.mm.ss.SSSSSSSSS a z") as e,
TO_STRING(TO_ZONEDDATETIME(data[5], "EEEE dd-MMMM-yy hh.mm.ss.SSSSSSSSS a z"),"EEEE dd-MMMM-yy hh.mm.ss.SSSSSSSSS a z") as f
FROM csvinput_out c;

create or replace target sample_out using Sysout(
  name:'Foo'
)INPUT FROM cq_zonedtime_out;

create Target File_tgt using FileWriter(
  filename:'ZonedDateTime_ActualOutput.csv',
  directory:'@OutputDirectory@',
  rolloverpolicy:'eventcount:1000'
)
format using DSVFormatter (

)
input from cq_zonedtime_out;

END APPLICATION zoneDateTime_CaseSensistive_CQ;

deploy application zoneDateTime_CaseSensistive_CQ;

start application zoneDateTime_CaseSensistive_CQ;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@;
CREATE  SOURCE @SourceName@ USING MSSqlReader  ( 
  Username: '@UserName@',
  Password: '@Password@',
  DatabaseName: 'qatest',
  ConnectionURL: '@SourceConnectionURL@',
  Tables: 'qatest.@SourceTable@',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @SRCINPUTSTREAM@;
create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;
CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:1',
  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
 ) 
INPUT FROM @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application DS.DSAPP;
undeploy application DS.DSAPP;

stop application DS.MyPosApp;
undeploy application DS.MyPosApp;

stop application DS.DsPosApp;
undeploy application DS.DsPosApp;

stop application DSL.DSLAPP;
undeploy application DSL.DSLAPP;

unload cache DS.C1;
unload wactionstore DS.MerchantActivity;

drop namespace DS cascade;
drop user DS cascade;

drop namespace DSL cascade;
drop user DSL cascade;

--
-- Recovery Test 33 with two sources, two sliding time windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> St1W/p -> CQ1 -> WS
-- S2 -> St2W/p -> CQ2 -> WS
--

STOP Recov33Tester.RecovTest33;
UNDEPLOY APPLICATION Recov33Tester.RecovTest33;
DROP APPLICATION Recov33Tester.RecovTest33 CASCADE;
CREATE APPLICATION RecovTest33 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 1 SECOND
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 2 SECOND
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION RecovTest33;

STOP APPLICATION AgenCQTester.CSV;
UNDEPLOY APPLICATION AgenCQTester.CSV;
DROP APPLICATION AgenCQTester.CSV cascade;

create application CSV;

CREATE FLOW AgentFlow;
create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails-agent.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStream;

CREATE TYPE MyType (
    PAN String,
    FNAME String
);

CREATE STREAM TypedStream of MyType;

CREATE CQ TypeConversionCQ
INSERT INTO TypedStream
SELECT data[0], data[1]
from CsvStream;

END FLOW AgentFlow;

CREATE FLOW ServerFlow;
CREATE TARGET myout1 using LogWriter(name: CQSource, filename:'@FEATURE-DIR@/logs/logCQ.txt') input from TypedStream;
END FLOW ServerFlow;

end application CSV;
DEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;
START CSV;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING S3Reader (
    bucketname:'',
    objectnameprefix:'',
     )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING AvroFormatter  (
schemaFileName: 'AvroFileSchema'
)
INPUT FROM @appname@CqOut;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using AvroFormatter (
schemaFileName: 'AvroS3Schema'
)
input from @appname@CqOut;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using AvroFormatter (
schemaFileName: 'AvroAzureSchema'
)
INPUT FROM @appname@CqOut;

CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using AvroFormatter (
schemaFileName: 'AvroGCSSchema'
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;

drop propertyset MysqlToMysqlPlatfm_App_KafkaPropset;
drop stream  MysqlToMysqlPlatfm_Stream CASCADE;


--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;
					
CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using MySQLReader(
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true',
  Tables: '$table1'
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using MySQLReader( 
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true',
  Tables: '@TABLENAME@2'
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'WACTION.MYSQLTOMYSQLPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

END APPLICATION @APPNAME@1;

CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using MySQLReader( 
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true',
  Tables: '$table2'
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using MySQLReader(
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true',
  Tables: '@TABLENAME@4',
  
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'WACTION.MYSQLTOMYSQLPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@;

END APPLICATION @APPNAME@2;

create or replace type @STREAM@orderBill(
id int,
name String,
cost float,
TableName string,
operationName String
);

create or replace stream @STREAM@_TYPED of @STREAM@OrderBill;

Create or replace CQ @STREAM@orderbillCQ
insert into @STREAM@_TYPED
select 
to_int(data[0]),data[1],to_float(data[2]),
meta(@STREAM@,'TableName'),Meta(@STREAM@,'OperationName') from @STREAM@;


create or replace Target @TARGET_NAME@ using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @STREAM@_TYPED;

STOP APPLICATION @WRITERAPPNAME@;
UNDEPLOY APPLICATION @WRITERAPPNAME@;
DROP APPLICATION @WRITERAPPNAME@ CASCADE;

CREATE APPLICATION @WRITERAPPNAME@ RECOVERY 1 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

CREATE SOURCE @SOURCE@ USING OracleReader
(
FetchSize:1,
Username:'@SOURCE_USER@',
Password:'85d7qFnwTW8=',
ConnectionURL:'@CONNECTION_URL@',
Tables:'@SOURCE_TABLE@',
password_encrypted: 'true'
)
OUTPUT TO @STREAM1@;


end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

CREATE OR REPLACE TYPE @TYPE@( 
datae java.util.HashMap , 
TABLE_NAME java.lang.String , 
OPS_NAME java.lang.String , 
DB_TIMESTAMP java.lang.String  ,
COMMITSCN java.lang.String ,
SCN java.lang.String ,
REC_INS_TIME java.lang.String );

CREATE CQ @CQ1@
INSERT INTO @STREAM2@
SELECT  
CASE WHEN (META(c,"OperationName").toString() == "DELETE")
THEN putUserData(c, 'isDelete', 'true') 
ELSE
putUserData(c,'isDelete', 'false')
END
FROM @STREAM1@ c;

CREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;

CREATE OR REPLACE CQ @CQ2@ 
INSERT INTO @STREAM3@
SELECT 
data(e),
META(e,"TableName").toString() as TABLE_NAME,
META(e, "OperationName").toString() as OPS_NAME,
META(e, "TimeStamp").toString() as DB_TIMESTAMP,
META(e,"COMMITSCN").toString() as COMMITSCN ,
META(e,"SCN").toString() as  SCN ,
DNOW().toString() as REC_INS_TIME
FROM @STREAM1@ e;

create Target @TARGET1@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC1',
ParallelThreads:'',
PartitionKey:'@metadata(TableName)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from @STREAM1@;


create Target @TARGET2@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC2',
ParallelThreads:'2',
PartitionKey:'TABLE_NAME',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from @STREAM3@;

create Target @TARGET3@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC3',
ParallelThreads:'',
PartitionKey:'@userdata(isDelete)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest.avsc')
input from @STREAM2@;

end application @WRITERAPPNAME@;
deploy application @WRITERAPPNAME@;
start @WRITERAPPNAME@;
stop application @READERAPPNAME@;
undeploy application @READERAPPNAME@;
drop application @READERAPPNAME@ cascade;
CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC1',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;


CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
filename:'@READERAPPNAME@_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream1;

CREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC2',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;


CREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC3',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest.avsc'
)
OUTPUT TO KafkaReaderStream3;

end flow @APPNAME@_serverflow;

end application @READERAPPNAME@;
deploy application @READERAPPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 SupportPDB: false,
 Fetchsize:1000
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

--
-- Crash Recovery Test 4 on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS
-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS
--

STOP APPLICATION N4S4CR4Tester.N4S4CRTest4;
UNDEPLOY APPLICATION N4S4CR4Tester.N4S4CRTest4;
DROP APPLICATION N4S4CR4Tester.N4S4CRTest4 CASCADE;
CREATE APPLICATION N4S4CRTest4 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest4;

CREATE SOURCE CsvSourceN4S4CRTest4 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest4;

CREATE FLOW DataProcessingN4S4CRTest4;

CREATE TYPE CsvDataN4S4CRTest4 (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionDataN4S4CRTest4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream OF CsvDataN4S4CRTest4;

CREATE CQ CsvToDataN4S4CRTest4
INSERT INTO DataStream
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN4S4CRTest4 CONTEXT OF WactionDataN4S4CRTest4
EVENT TYPES ( CsvDataN4S4CRTest4 )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO WactionsN4S4CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO WactionsN4S4CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END FLOW DataProcessingN4S4CRTest4;

END APPLICATION N4S4CRTest4;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 1 second interval;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@'
  --recordSelector: '@PROP8@'
)
OUTPUT TO @APPNAME@Stream;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;

CREATE TYPE test_type (
 account_no com.fasterxml.jackson.databind.JsonNode,
 first_name com.fasterxml.jackson.databind.JsonNode,
 last_name com.fasterxml.jackson.databind.JsonNode,
 addr1 com.fasterxml.jackson.databind.JsonNode,
Addr2 com.fasterxml.jackson.databind.JsonNode,
City com.fasterxml.jackson.databind.JsonNode,
State com.fasterxml.jackson.databind.JsonNode,
Zip com.fasterxml.jackson.databind.JsonNode
);

Create stream cqAsJSONNodeStream of test_type;

CREATE CQ GetPOAsJsonNodes
INSERT into cqAsJSONNodeStream
    select 
    data.get('ACCTS-RECORD').get('ACCOUNT-NO'),
data.get('ACCTS-RECORD').get('NAME').get('FIRST-NAME'),
data.get('ACCTS-RECORD').get('NAME').get('LAST-NAME'),
data.get('ACCTS-RECORD').get('ADDRESS1'),
data.get('ACCTS-RECORD').get('ADDRESS2'),
data.get('ACCTS-RECORD').get('ADDRESS3').get('CITY'),
data.get('ACCTS-RECORD').get('ADDRESS3').get('STATE'),
data.get('ACCTS-RECORD').get('ADDRESS3').get('ZIP-CODE')
from @APPNAME@Stream js;

create type finaldtype(
      ACCOUNT_NO String, 
      FIRST_NAME String,
      LAST_NAME String,
      ADDRESS1 String,
      ADDRESS2 String,
      CITY String,
      STATE String,
      ZIP_CODE String
);

CREATE CQ getdata
INSERT into getdataStream
    select account_no.toString(),
    first_name.toString(),
    last_name.toString(),
    addr1.toString(),
    Addr2.toString(),
    City.toString(),
    State.toString(),
    Zip.toString()
from cqAsJSONNodeStream x;

create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:10000',
  CommitPolicy: 'EventCount:10000',
  Tables: 'QATEST.@APPNAME@'
)
input from getdataStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

Stop Oracle_IRLogWriter;
Undeploy application Oracle_IRLogWriter;
drop application Oracle_IRLogWriter cascade;

CREATE APPLICATION Oracle_IRLogWriter WITH ENCRYPTION recovery 5 second interval;

CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( 
  FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.TDSOURCE',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.TEST01=ID;',
  PollingInterval: '5sec',
  ReturnDateTimeAs: 'String',
  startPosition:'striim.test01=0'
  )
  OUTPUT TO data_stream;

  CREATE OR REPLACE TARGET Oracle_IRSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

CREATE TARGET BinaryDump USING LogWriter(
  name: 'TeraData',
  filename:'TeraData.log'
)INPUT FROM data_stream;

END APPLICATION Oracle_IRLogWriter;

deploy application Oracle_IRLogWriter in default;

start application Oracle_IRLogWriter;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;


CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',
					acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

CREATE OR REPLACE STREAM @APPNAME@_stream OF Global.waevent persist using @APPNAME@_KafkaPropset;

CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @APPNAME@_stream2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNEC4TION_URL@',
  Tables: '$table1',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'QATEST.ORCTOMSSQLPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo) input from @STREAM@;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @APPNAME@_stream3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '$table2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@4',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'QATEST.ORCTOMSSQLPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@3;

END APPLICATION @APPNAME@2;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING ParquetFormatter (
schemaFileName: '@SCHEMAFILE@',
members:'data'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING DSVFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

END APPLICATION @APPNAME@;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW ;

 

CREATE OR REPLACE SOURCE DBSource USING DatabaseReader  ( 
  Username: '@SOURCE_USER@',
  Password_encrypted: false,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Query: "SELECT * FROM waction.MysqlToCql_alldatatypes",
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SOURCE_PASS@'
 ) 
OUTPUT TO Oracle_ChangeDataStream;


CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

--
-- Recovery Test 26 with two sources, two jumping attribute windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W -> CQ1 -> WS
-- S2 -> Ja6W -> CQ2 -> WS
--

STOP KStreamRecov26Tester.KStreamRecovTest26;
UNDEPLOY APPLICATION KStreamRecov26Tester.KStreamRecovTest26;
DROP APPLICATION KStreamRecov26Tester.KStreamRecovTest26 CASCADE;
DROP USER KStreamRecov26Tester;
DROP NAMESPACE KStreamRecov26Tester CASCADE;
CREATE USER KStreamRecov26Tester IDENTIFIED BY KStreamRecov26Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov26Tester;
CONNECT KStreamRecov26Tester KStreamRecov26Tester;

CREATE APPLICATION KStreamRecovTest26 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION KStreamRecovTest26;

stop tpcc;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;


Create Source @SourceName@
 Using Ojet
(
 Username:'@READER-UNAME@',
 Password:'@READER-PASSWORD@',
 ConnectionURL:'@CDC-READER-URL@',
 Tables: '@WATABLES-SRC@',
 FetchSize:1,
 QueueSize:25000,
 CommittedTransactions:true,
 Compression:true,
 CaptureDDL: true,
 SendBeforeImage:true
) Output To @SRCINPUTSTREAM@;


create Target @targetsys@ using SysOut(name:OrgData) input from DataStream;

CREATE TARGET @targetName@ USING databasewriter(
  Username: '@WRITER-UNAME@',
  Password: '@WRITER-PASSWORD@',
  ConnectionURL:'@WRITER-URL@',
  BatchPolicy:'EventCount:1,Interval:1',
  CommitPolicy:'EventCount:1,Interval:1',
  Tables: '@WATABLES-SRC@,@WATABLES-TGT@'
) INPUT FROM @SRCINPUTSTREAM@;


END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

--
-- Recovery Test 28 with two sources, two jumping time-count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5t9W  -> CQ1 -> WS
--   S2 -> Jc6t11W -> CQ2 -> WS
--

STOP Recov28Tester.RecovTest28;
UNDEPLOY APPLICATION Recov28Tester.RecovTest28;
DROP APPLICATION Recov28Tester.RecovTest28 CASCADE;
CREATE APPLICATION RecovTest28 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION RecovTest28;

--
-- Crash Recovery Test 2 on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS
--

STOP APPLICATION N4S4CR2Tester.N4S4CRTest2;
UNDEPLOY APPLICATION N4S4CR2Tester.N4S4CRTest2;
DROP APPLICATION N4S4CR2Tester.N4S4CRTest2 CASCADE;
CREATE APPLICATION N4S4CRTest2 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest2;

CREATE SOURCE CsvSourceN4S4CRTest2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest2;

CREATE FLOW DataProcessingN4S4CRTest2;

CREATE TYPE WactionTypeN4S4CRTest2 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionTypeN4S4CRTest2;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN4S4CRTest2 CONTEXT OF WactionTypeN4S4CRTest2
EVENT TYPES ( WactionTypeN4S4CRTest2 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN4S4CRTest2
INSERT INTO WactionsN4S4CRTest2
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN4S4CRTest2;

END APPLICATION N4S4CRTest2;