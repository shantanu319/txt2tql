{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop DataGenSampleApp;\nundeploy application DataGenSampleApp;\ndrop application DataGenSampleApp cascade;\n\n\nCREATE APPLICATION DataGenSampleApp;\n\nCreate Source dataGenSrc USING MariaDBReader  ( \n  Username:'qatest',\n  Password:'w3b@ct10n',\n  ConnectionURL:'jdbc:mariadb://10.77.21.53:3306/qatest',\n  Tables: '@tableNames@',\n  ClusterSupport: 'Galera'\n ) \n Output To LCRStream;\n\ncreate Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;\n\nEND APPLICATION DataGenSampleApp;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop DBRTOCW;\n undeploy application DBRTOCW;\n drop application DBRTOCW cascade;\n CREATE APPLICATION DBRTOCW;\n\n Create Source MSSQLSource Using MSSqlReader\n(\nUsername:'qatest',\nPassword:'w@ct10n',\nDatabaseName:'qatest',\nConnectionURL:'10.77.61.30:1433',\nTables:'qatest.MssqlTocql_Alldatatypes',\nConnectionPoolSize:1,\nCompression:'true'\n)\nOUTPUT TO Oracle_ChangeDataStream;\n\n CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n ) \nINPUT FROM Oracle_ChangeDataStream;\n\n create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\n END APPLICATION DBRTOCW;\n\n deploy application DBRTOCW in default;\n\n start DBRTOCW;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY;\n\nCreate Source @APPNAME@_src Using OracleReader\n(\n Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1000,\n -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n   _h_ReturnDateTimeAs:'ZonedDateTime'\n) Output To @APPNAME@_stream;\n\ncreate Target @APPNAME@_tgt using FileWriter(\n  filename:'CompressedMerchant.gz',\n  directory:'/logs/',\n  rolloverpolicy:'EventCount:10000'\n)\nformat using ParquetFormatter (\n\tschemaFileName:'@FILENAME@',\n\tFormatAs:'@FORMATAS@'\n)\ninput from @APPNAME@_stream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (\n  Tables: '',\n  ConnectionURL: '',\n  Password: '',\n  Username: ''\n  )\nOUTPUT TO @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING AvroFormatter (\nschemaFileName: '@SCHEMAFILE@'\n)\nINPUT FROM @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING DSVFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nCREATE APPLICATION KafkaReader;\n\nCREATE OR REPLACE TYPE KafkaSourceStr2_Type  ( seq java.lang.Integer\n );\n\nCREATE OR REPLACE STREAM KafkaSourceStr2 OF KafkaSourceStr2_Type;\n\nCREATE  JUMPING WINDOW GetTargData OVER KafkaSourceStr2 KEEP 1000000 ROWS;\n\nCREATE OR REPLACE SOURCE KafkaSource USING KafkaReader VERSION '0.11.0' (\n  KafkaConfigPropertySeparator: ';',\n  startOffset: 0,\n  adapterName: 'KafkaReader',\n  Topic: 'kafkaTopic7',\n  AutoMapPartition: true,\n  brokerAddress: 'localhost:9092',\n  KafkaConfigValueSeparator: '=',\n  KafkaConfig: 'max.partition.fetch.bytes=10485760;fetch.min.bytes=1048576;fetch.max.wait.ms=1000;receive.buffer.bytes=2000000;poll.timeout.ms=10000;request.timeout.ms=60001;session.timeout.ms=60000'\n )\n PARSE USING DSVParser  (\n  charset: 'UTF-8',\n  handler: 'com.webaction.proc.DSVParser_1_0',\n  linenumber: 0,\n  nocolumndelimiter: false,\n  trimwhitespace: false,\n  columndelimiter: ',',\n  columndelimittill: '-1',\n  ignoremultiplerecordbegin: 'true',\n  ignorerowdelimiterinquote: false,\n  parserName: 'DSVParser',\n  separator: ':',\n  blockascompleterecord: false,\n  ignoreemptycolumn: false,\n  rowdelimiter: '\\n',\n  header: false,\n  headerlineno: 0,\n  quoteset: '\\\"',\n  trimquote: true\n )\nOUTPUT TO KafkaSourceStr1 ;\n\nCREATE OR REPLACE CQ GetKafkaDataQuery\nINSERT INTO KafkaSourceStr2\nSELECT TO_INT(data[1]) as seq\nFROM KafkaSourceStr1;\n\nCREATE  TYPE KafkaSourceStr3_Type  ( SUMKafkaSourceStr2seq java.lang.Long\n );\n\nCREATE STREAM KafkaSourceStr3 OF KafkaSourceStr3_Type;\n\nCREATE OR REPLACE CQ GetTheSum\nINSERT INTO KafkaSourceStr3\nSELECT SUM(GetTargData .seq)\nFROM GetTargData;\n\nCREATE OR REPLACE TARGET KafkaFile USING FileWriter  (\n  filename: 'TargetResults',\n  rolloveronddl: 'true',\n  flushpolicy: 'eventcount:10000,interval:30',\n  adapterName: 'FileWriter',\n  directory: '@FEATURE-DIR@/logs',\n  rolloverpolicy: 'eventcount:10000,interval:30s'\n )\nFORMAT USING DSVFormatter  (   nullvalue: 'NULL',\n  standard: 'none',\n  handler: 'com.webaction.proc.DSVFormatter',\n  formatterName: 'DSVFormatter',\n  usequotes: 'false',\n  rowdelimiter: '\\n',\n  quotecharacter: '\\\"',\n  header: 'false',\n  columndelimiter: ','\n )\nINPUT FROM KafkaSourceStr3;\n\nEND APPLICATION KafkaReader;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @WRITERAPPNAME@;\nUNDEPLOY APPLICATION @WRITERAPPNAME@;\nDROP APPLICATION @WRITERAPPNAME@ CASCADE;\n\nCREATE APPLICATION @WRITERAPPNAME@ RECOVERY 1 SECOND INTERVAL;\n\ncreate flow @APPNAME@_agentflow;\n\nCREATE SOURCE @SOURCE@ USING OracleReader\n(\nFetchSize:1,\nUsername:'@SOURCE_USER@',\nPassword:'85d7qFnwTW8=',\nConnectionURL:'@CONNECTION_URL@',\nTables:'@SOURCE_TABLE@',\npassword_encrypted: 'true'\n)\nOUTPUT TO @STREAM1@;\n\n\nend flow @APPNAME@_agentflow;\n\ncreate flow @APPNAME@_serverflow;\n\nCREATE OR REPLACE TYPE @TYPE@( \ndatae java.util.HashMap , \nTABLE_NAME java.lang.String , \nOPS_NAME java.lang.String , \nDB_TIMESTAMP java.lang.String  ,\nCOMMITSCN java.lang.String ,\nSCN java.lang.String ,\nREC_INS_TIME java.lang.String );\n\nCREATE CQ @CQ1@\nINSERT INTO @STREAM2@\nSELECT  \nCASE WHEN (META(c,\"OperationName\").toString() == \"DELETE\")\nTHEN putUserData(c, 'isDelete', 'true') \nELSE\nputUserData(c,'isDelete', 'false')\nEND\nFROM @STREAM1@ c;\n\nCREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;\n\nCREATE OR REPLACE CQ @CQ2@ \nINSERT INTO @STREAM3@\nSELECT \ndata(e),\nMETA(e,\"TableName\").toString() as TABLE_NAME,\nMETA(e, \"OperationName\").toString() as OPS_NAME,\nMETA(e, \"TimeStamp\").toString() as DB_TIMESTAMP,\nMETA(e,\"COMMITSCN\").toString() as COMMITSCN ,\nMETA(e,\"SCN\").toString() as  SCN ,\nDNOW().toString() as REC_INS_TIME\nFROM @STREAM1@ e;\n\ncreate Target @TARGET1@ using KafkaWriter VERSION @kafkaAdpVersion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC1',\nParallelThreads:'',\nPartitionKey:'@metadata(TableName)',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING dsvFormatter ()\ninput from @STREAM1@;\n\n\ncreate Target @TARGET2@ using KafkaWriter VERSION @kafkaAdpVersion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC2',\nParallelThreads:'2',\nPartitionKey:'TABLE_NAME',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING jsonFormatter ()\ninput from @STREAM3@;\n\ncreate Target @TARGET3@ using KafkaWriter VERSION @kafkaAdpVersion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC3',\nParallelThreads:'',\nPartitionKey:'@userdata(isDelete)',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING avroFormatter (\nschemaFileName:'KafkaAvroTest.avsc')\ninput from @STREAM2@;\n\nend application @WRITERAPPNAME@;\ndeploy application @WRITERAPPNAME@;\nstart @WRITERAPPNAME@;\nstop application @READERAPPNAME@;\nundeploy application @READERAPPNAME@;\ndrop application @READERAPPNAME@ cascade;\nCREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafkaAdpVersion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC1',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream1;\n\n\nCREATE TARGET kafkaDumpDSV USING FileWriter(\nname:kafkaOuputDSV,\nfilename:'@READERAPPNAME@_RT_DSV')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaReaderStream1;\n\nCREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafkaAdpVersion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC2',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO KafkaReaderStream2;\n\n\nCREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafkaAdpVersion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC3',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'KafkaAvroTest.avsc'\n)\nOUTPUT TO KafkaReaderStream3;\n\nend flow @APPNAME@_serverflow;\n\nend application @READERAPPNAME@;\ndeploy application @READERAPPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\nCREATE APPLICATION @APPNAME@ RECOVERY 10 SECOND INTERVAL;\n\n create flow Mysqlflow;\nCREATE SOURCE MysqlToDBRoutersource USING MysqlReader\n(\n  Username: '',\n  Password: '',\n  Tables: '',\n  ConnectionURL: '',\n  Password_encrypted: 'false',\n  connectionRetryPolicy: 'retryInterval=30, maxRetries=3'\n)\nOUTPUT TO RouterTestMasterStream;\n\nend flow Mysqlflow;\n\nCREATE OR REPLACE ROUTER RouterTestRs1 INPUT FROM RouterTestMasterStream s CASE\nWHEN meta(s,\"TableName\").toString()='waction.source1' THEN ROUTE TO RouterTestTyped1,\nWHEN meta(s,\"TableName\").toString()='waction.source2' THEN ROUTE TO RouterTestTyped2,\nELSE ROUTE TO RouterTestTypedElse;\n\nCREATE TARGET MysqlToDBRoutertarget1 USING DatabaseWriter(\n   Username: '',\n   Password: '',\n   Tables: '',\n   ConnectionURL: '',\n   Password_encrypted: 'false',\n   connectionRetryPolicy: 'retryInterval=30, maxRetries=3'\n) INPUT FROM RouterTestTyped1;\n\n\nCREATE TARGET MysqlToDBRoutertarget2 USING DatabaseWriter(\n    Username: '',\n    Password: '',\n    Tables: '',\n    ConnectionURL: '',\n    Password_encrypted: 'false',\n    connectionRetryPolicy: 'retryInterval=30, maxRetries=3'\n) INPUT FROM RouterTestTyped2;\n\n\nend application @APPNAME@;\ndeploy application @APPNAME@ with Mysqlflow in AGENTs;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 2 on four node all server cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS\n--\n\nSTOP APPLICATION N4S4CR2Tester.N4S4CRTest2;\nUNDEPLOY APPLICATION N4S4CR2Tester.N4S4CRTest2;\nDROP APPLICATION N4S4CR2Tester.N4S4CRTest2 CASCADE;\nCREATE APPLICATION N4S4CRTest2 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN4S4CRTest2;\n\nCREATE SOURCE CsvSourceN4S4CRTest2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN4S4CRTest2;\n\nCREATE FLOW DataProcessingN4S4CRTest2;\n\nCREATE TYPE WactionTypeN4S4CRTest2 (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionTypeN4S4CRTest2;\n\nCREATE CQ CsvToWaction\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsN4S4CRTest2 CONTEXT OF WactionTypeN4S4CRTest2\nEVENT TYPES ( WactionTypeN4S4CRTest2 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsN4S4CRTest2\nINSERT INTO WactionsN4S4CRTest2\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND FLOW DataProcessingN4S4CRTest2;\n\nEND APPLICATION N4S4CRTest2;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop OracleToKudu;\nundeploy application OracleToKudu;\ndrop application OracleToKudu cascade;\n\nCREATE APPLICATION OracleToKudu RECOVERY 5 SECOND INTERVAL;\nCreate Source oracSource\n Using OracleReader\n(\n Username:'@LOGMINER-UNAME@',\n Password:'@LOGMINER-PASSWORD@',\n ConnectionURL:'@LOGMINER-URL@',\n Tables:'@SOURCE_TABLES@',\n OnlineCatalog:true,\n FetchSize:1\n) Output To DataStream;\nCREATE TARGET WriteintoKudu using KuduWriter (\nkuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',\npkupdatehandlingmode:'@MODE@',\ntables: '@TARGET_TABLES@',\nConnectionRetryPolicy: 'retryInterval=40,maxRetries=7',\nbatchpolicy: 'EventCount:20,Interval:60')\nINPUT FROM DataStream;\n\nEND APPLICATION OracleToKudu;\ndeploy application OracleToKudu in default;\nstart OracleToKudu;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION EH;\nUNDEPLOY APPLICATION EH;\nDROP APPLICATION EH CASCADE;\nCREATE APPLICATION EH recovery 5 second interval;\nCREATE Source s USING PostgreSQLReader  ( \n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Tables: 'public.tablename1000%') \nOUTPUT TO ss ;\n\nCREATE OR REPLACE TYPE jsontype( \ndatae java.util.HashMap , \nTABLE_NAME java.lang.String , \nOPS_NAME java.lang.String , \nDB_TIMESTAMP java.lang.String );\n\nCREATE STREAM cq_json_out OF jsontype PARTITION BY TABLE_NAME;\n\nCREATE OR REPLACE CQ cq_json \nINSERT INTO cq_json_out\nSELECT \ndata(e),\nMETA(e,\"TableName\").toString() as TABLE_NAME,\nMETA(e, \"OperationName\").toString() as OPS_NAME,\nMETA(e, \"TimeStamp\").toString() as DB_TIMESTAMP\nFROM ss e;\n\nCREATE CQ cq1\nINSERT INTO TypedAccessLogStream1\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000101'; \n\ncreate Target t1 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_101',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_101',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream1;\n\nCREATE CQ cq2\nINSERT INTO TypedAccessLogStream2\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000102'; \n\ncreate Target t2 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_102',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_102',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream2;\n\nCREATE CQ cq3\nINSERT INTO TypedAccessLogStream3\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000103'; \n\ncreate Target t3 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_103',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_103',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream3;\n\nCREATE CQ cq4\nINSERT INTO TypedAccessLogStream4\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000104'; \n\ncreate Target t4 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_104',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_104',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream4;\n\nCREATE CQ cq5\nINSERT INTO TypedAccessLogStream5\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000105'; \n\ncreate Target t5 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_105',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_105',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream5;\n\nCREATE CQ cq6\nINSERT INTO TypedAccessLogStream6\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000106'; \n\ncreate Target t6 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_106',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_106',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream6;\n\nCREATE CQ cq7\nINSERT INTO TypedAccessLogStream7\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000107'; \n\ncreate Target t7 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_107',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_107',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream7;\n\nCREATE CQ cq8\nINSERT INTO TypedAccessLogStream8\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000108'; \n\ncreate Target t8 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_108',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_108',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream8;\n\nCREATE CQ cq9\nINSERT INTO TypedAccessLogStream9\nSELECT *\nFROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000109'; \n\ncreate Target t9 using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_109',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_cg_109',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000,interval:30s',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from TypedAccessLogStream9;\n\n-- CREATE CQ cq10\n-- INSERT INTO TypedAccessLogStream10\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000110'; \n-- \n-- create Target t10 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_110',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_110',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream10;\n\n\n-- CREATE CQ cq11\n-- INSERT INTO TypedAccessLogStream11\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000111'; \n-- \n-- create Target t11 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_111',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_111',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream11;\n-- \n-- CREATE CQ cq12\n-- INSERT INTO TypedAccessLogStream12\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000112'; \n-- \n-- create Target t12 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_112',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_112',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream12;\n-- \n-- CREATE CQ cq13\n-- INSERT INTO TypedAccessLogStream13\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000113'; \n-- \n-- create Target t13 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_113',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_113',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream13;\n-- \n-- CREATE CQ cq14\n-- INSERT INTO TypedAccessLogStream14\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000114'; \n-- \n-- create Target t14 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_114',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_114',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream14;\n-- \n-- CREATE CQ cq15\n-- INSERT INTO TypedAccessLogStream15\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000115'; \n-- \n-- create Target t15 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_115',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_115',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream15;\n-- \n-- CREATE CQ cq16\n-- INSERT INTO TypedAccessLogStream16\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000116'; \n-- \n-- create Target t16 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_116',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_116',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream16;\n-- \n-- CREATE CQ cq17\n-- INSERT INTO TypedAccessLogStream17\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000117'; \n-- \n-- create Target t17 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_117',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_117',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream17;\n-- \n-- CREATE CQ cq18\n-- INSERT INTO TypedAccessLogStream18\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000118'; \n-- \n-- create Target t18 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_118',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_118',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream18;\n-- \n-- CREATE CQ cq19\n-- INSERT INTO TypedAccessLogStream19\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000119'; \n-- \n-- create Target t19 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_119',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_119',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream19;\n-- \n-- CREATE CQ cq20\n-- INSERT INTO TypedAccessLogStream20\n-- SELECT *\n-- FROM cq_json_out PS WHERE PS.Table_Name = 'public.tablename1000120'; \n-- \n-- create Target t20 using AzureEventHubWriter (\n-- \tEventHubNamespace:'EventHubWriterTest',\n-- \tEventHubName:'test_120',\n-- \tSASPolicyName:'RootManageSharedAccessKey',\n-- \tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n-- \tConsumerGroup:'test_cg_120',\n-- \tE1P:'true',\n-- \tOperationTimeoutMS:'200000',\n-- \tBatchPolicy:'Size:256000,interval:30s'\n-- )\n-- format using jsonFormatter(\n-- )\n-- input from TypedAccessLogStream20;\n\n\nEND APPLICATION EH;\nDEPLOY APPLICATION EH;\nstart application EH;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application CSVToJSON;\ncreate source CSVSource using FileReader (\n\tdirectory:'Samples/AppData',\n\tWildCard:'posdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\n\tfilename:'posdata_JSON',\n\trolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'\n)\nformat using JSONFormatter (\n\tmembers:'merchantname,merchantid,dateTime,hourValue,amount,zip'\n)\n\ninput from TypedCSVStream;\nend application CSVToJSON;\n\ndeploy application CSVToJSON;\nstart application CSVToJSON;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@;\n\nCREATE SOURCE @SourceName@ USING PostgreSQLReader  ( \nReaderType: 'LogMiner', \n  Password_encrypted: 'false', \n  SupportPDB: false, \n  ReplicationSlotName: 'test_slot',\n  QuiesceMarkerTable: 'QUIESCEMARKER', \n  QueueSize: 2048, \n  CommittedTransactions: true, \n  Username: '@UserName@', \n  TransactionBufferType: 'Memory', \n  TransactionBufferDiskLocation: '.striim/LargeBuffer', \n  OutboundServerProcessName: 'WebActionXStream', \n  Password: '@Password@', \n  DDLCaptureMode: 'All', \n  Compression: false, \n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', \n  FetchSize: 1, \n  Tables: '@SourceTables@', \n  DictionaryMode: 'OnlineCatalog', \n  XstreamTimeOut: 600, \n  TransactionBufferSpilloverSize: '1MB', \n  FilterTransactionBoundaries: true, \n  ConnectionURL: '@ConnectionURL@', \n  SendBeforeImage: true ) \nOUTPUT TO @AppStream@  ;\n\nCREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream SELECT data[2], to_string(to_date(data[2]), \"dd-MMM-yy hh.mm.ss\") FROM @AppStream@ o ;\n\nCREATE  TARGET @targetsys@ USING Global.SysOut  ( \nname: 'ora1_sys' ) \nINPUT FROM admin.ZDT_cq_stream;\n\ncreate Target @TargetFile@ using FileWriter(\n  filename:'toStringOut.log',\n  directory:'@FilePath@',\n  rolloverpolicy:'eventcount:1000'\n)\nformat using DSVFormatter (\n\n)\ninput from admin.ZDT_cq_stream;\n\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 13 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same compound key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> CW(p#p) -> CQ -> WS\n--\n\nSTOP Recov13Tester.RecovTest13;\nUNDEPLOY APPLICATION Recov13Tester.RecovTest13;\nDROP APPLICATION Recov13Tester.RecovTest13 CASCADE;\nCREATE APPLICATION RecovTest13 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTest10Data.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE CsvData (\n  partKey String KEY,\n  serialNumber int,\n  partKey2 String KEY\n);\n\nCREATE TYPE WactionData (\n  partKey String KEY,\n  serialNumber int\n);\n\nCREATE STREAM DataStream OF CsvData PARTITION BY partKey, partKey2;\n\nCREATE CQ CsvToData\nINSERT INTO DataStream\nSELECT\n    data[0],\n    TO_INT(data[1]),\n    data[0]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStreamTwoItems\nOVER DataStream KEEP 2 ROWS\nPARTITION BY partKey, partKey2;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction\nINSERT INTO Wactions\nSELECT\n    first(partKey),\n    to_int(first(serialNumber))\nFROM DataStreamTwoItems\nGROUP BY partKey, partKey2;\n\nEND APPLICATION RecovTest13;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()\nPARSE USING XMLParserV2 ()\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE OR REPLACE CQ @APPNAME@_CQ\nINSERT INTO @APPNAME@_CQOut\nSELECT\ndata.attributeValue(\"merchantid\") as merchantID,\ndata.getText() as companyName\nFROM @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()\nFORMAT USING JSONFormatter ()\nINPUT FROM @APPNAME@_CQOut;\n\nEND APPLICATION @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use PosTester;\nalter application PosApp;\n\nCREATE TYPE MerchantTxRate(\n  merchantId String KEY,\n  zip String,\n  startTime DateTime,\n  count int,\n  totalAmount double,\n  hourlyAve int,\n  upperLimit double,\n  lowerLimit double,\n  category String,\n  status String\n);\n\nend application PosApp;\nalter application PosApp recompile;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION ORACLETOBIGQUERY;\nUNDEPLOY APPLICATION ORACLETOBIGQUERY;\nDROP APPLICATION ORACLETOBIGQUERY CASCADE;\n\n--create application \nCREATE APPLICATION ORACLETOBIGQUERY RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE OR REPLACE SOURCE OracleSource USING OracleReader (\n ConnectionURL: '192.168.123.12:1521/ORCL',\n Tables: 'QATEST.ORATOBIGQALLDATATYPE',\n Username: 'qatest',\n Password: 'qatest',\n FetchSize:1\n) OUTPUT TO CDCStream;\n\nCREATE OR REPLACE TARGET bqtables using BigqueryWriter(\n BQServiceAccountConfigurationPath:\"/Users/karthikmurugan/Downloads/bqtest-540227c31980.json\",\n projectId:\"bqtest-158706\",\n Tables: \"QATEST.ORATOBIGQALLDATATYPE,QATEST.ORATOBIGQALLDATATYPE\",\n BatchPolicy: \"eventCount:1,Interval:90\")\nINPUT FROM CDCStream;\n\n\nCREATE OR REPLACE TARGET T1 using SysOut(name : \"some text\") INPUT FROM CDCStream;\n\nEND APPLICATION ORACLETOBIGQUERY;\n\nDEPLOY APPLICATION ORACLETOBIGQUERY;\nSTART APPLICATION ORACLETOBIGQUERY;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application reconnect;\nundeploy application reconnect;\ndrop application reconnect cascade;\nCREATE APPLICATION reconnect recovery 1 second interval;\n\nCREATE  SOURCE mssqlsource USING MssqlReader  ( \n  Username: '@USERNAME@',\n  Password: '@PASSWORD@',\n  ConnectionURL: '@URL@',\n  Tables: '@TABLE@',\n  FetchSize: 1\n ) \nOUTPUT TO sqlstream;\n\nCREATE TARGET dbtarget USING CassandraWriter(\n  ConnectionURL:'@URL@',\n  Username:'@USERNAME@',\n  Password:'@PASSWORD@',\n  ConnectionRetryPolicy: 'retryInterval=15s,maxRetries=2',\n  BatchPolicy:'EventCount:5,Interval:30',\n  CommitPolicy:'EventCount:5,Interval:30',\n  Tables: '@TABLES@'\n ) INPUT FROM sqlstream;\n\n create Target tSysOut using Sysout(name:OrgData) input from sqlstream;\n end application reconnect;\n deploy application reconnect;\n start application reconnect;\n \n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION App1;\nUNDEPLOY APPLICATION App1;\nDROP APPLICATION App1 CASCADE;\nCREATE APPLICATION App1;\nCREATE FLOW AgentFlow;\nCREATE OR REPLACE SOURCE App1_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App1_SampleStream;\nEND FLOW AgentFlow;\nCREATE FLOW ServerFlow;\nCREATE OR REPLACE TARGET App1_NullTarget using NullWriter()\nINPUT FROM App1_SampleStream;\nEND FLOW ServerFlow;\nEND APPLICATION App1;\ndeploy application App1 on any in ServerDG1 with AgentFlow on any in Agents, ServerFlow on any in ServerDG1;\nSTART APPLICATION App1;\n\nSTOP APPLICATION App2;\nUNDEPLOY APPLICATION App2;\nDROP APPLICATION App2 CASCADE;\nCREATE APPLICATION App2;\nCREATE FLOW AgentFlow2;\nCREATE OR REPLACE SOURCE App2_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App2_SampleStream;\nEND FLOW AgentFlow2;\nCREATE FLOW ServerFlow2;\nCREATE OR REPLACE TARGET App2_NullTarget using NullWriter()\nINPUT FROM App2_SampleStream;\nEND FLOW ServerFlow2;\nEND APPLICATION App2;\ndeploy application App2 on any in ServerDG1 with AgentFlow2 on any in Agents, ServerFlow2 on any in ServerDG1;\nSTART APPLICATION App2;\n\nSTOP APPLICATION App3;\nUNDEPLOY APPLICATION App3;\nDROP APPLICATION App3 CASCADE;\nCREATE APPLICATION App3;\nCREATE OR REPLACE SOURCE App3_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App3_SampleStream;\nCREATE OR REPLACE TARGET App3_NullTarget using NullWriter()\nINPUT FROM App3_SampleStream;\nEND APPLICATION App3;\nDEPLOY APPLICATION App3;\nSTART APPLICATION App3;\n\nSTOP APPLICATION App4;\nUNDEPLOY APPLICATION App4;\nDROP APPLICATION App4 CASCADE;\nCREATE APPLICATION App4;\nCREATE OR REPLACE SOURCE App4_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App4_SampleStream;\nCREATE OR REPLACE TARGET App4_NullTarget using NullWriter()\nINPUT FROM App4_SampleStream;\nEND APPLICATION App4;\nDEPLOY APPLICATION App4 ON ONE IN ServerDG1;\nSTART APPLICATION App4;\n\nSTOP APPLICATION App5;\nUNDEPLOY APPLICATION App5;\nDROP APPLICATION App5 CASCADE;\nCREATE APPLICATION App5;\nCREATE OR REPLACE SOURCE App5_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App5_SampleStream;\nCREATE OR REPLACE TARGET App5_NullTarget using NullWriter()\nINPUT FROM App5_SampleStream;\nEND APPLICATION App5;\nDEPLOY APPLICATION App5 ON ONE IN ServerDG1;\nSTART APPLICATION App5;\n\nSTOP APPLICATION App6;\nUNDEPLOY APPLICATION App6;\nDROP APPLICATION App6 CASCADE;\nCREATE APPLICATION App6;\nCREATE OR REPLACE SOURCE App6_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App6_SampleStream;\nCREATE OR REPLACE TARGET App6_NullTarget using NullWriter()\nINPUT FROM App6_SampleStream;\nEND APPLICATION App6;\nDEPLOY APPLICATION App6 ON ONE IN ServerDG1;\nSTART APPLICATION App6;\n\nSTOP APPLICATION App7;\nUNDEPLOY APPLICATION App7;\nDROP APPLICATION App7 CASCADE;\nCREATE APPLICATION App7;\nCREATE OR REPLACE SOURCE App7_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App7_SampleStream;\nCREATE OR REPLACE TARGET App7_NullTarget using NullWriter()\nINPUT FROM App7_SampleStream;\nEND APPLICATION App7;\nDEPLOY APPLICATION App7 ON ONE IN ServerDG1;\nSTART APPLICATION App7;\n\nSTOP APPLICATION App8;\nUNDEPLOY APPLICATION App8;\nDROP APPLICATION App8 CASCADE;\nCREATE APPLICATION App8;\nCREATE OR REPLACE SOURCE App8_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App8_SampleStream;\nCREATE OR REPLACE TARGET App8_NullTarget using NullWriter()\nINPUT FROM App8_SampleStream;\nEND APPLICATION App8;\nDEPLOY APPLICATION App8 ON ONE IN ServerDG1;\nSTART APPLICATION App8;\n\n\nSTOP APPLICATION App9;\nUNDEPLOY APPLICATION App9;\nDROP APPLICATION App9 CASCADE;\nCREATE APPLICATION App9;\nCREATE OR REPLACE SOURCE App9_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App9_SampleStream;\nCREATE OR REPLACE TARGET App9_NullTarget using NullWriter()\nINPUT FROM App9_SampleStream;\nEND APPLICATION App9;\nDEPLOY APPLICATION App9;\nSTART APPLICATION App9;\n\nSTOP APPLICATION App10;\nUNDEPLOY APPLICATION App10;\nDROP APPLICATION App10 CASCADE;\nCREATE APPLICATION App10;\nCREATE OR REPLACE SOURCE App10_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App10_SampleStream;\nCREATE OR REPLACE TARGET App10_NullTarget using NullWriter()\nINPUT FROM App10_SampleStream;\nEND APPLICATION App10;\nDEPLOY APPLICATION App10;\nSTART APPLICATION App10;\n\nSTOP APPLICATION App11;\nUNDEPLOY APPLICATION App11;\nDROP APPLICATION App11 CASCADE;\nCREATE APPLICATION App11;\nCREATE OR REPLACE SOURCE App11_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App11_SampleStream;\nCREATE OR REPLACE TARGET App11_NullTarget using NullWriter()\nINPUT FROM App11_SampleStream;\nEND APPLICATION App11;\nDEPLOY APPLICATION App11;\nSTART APPLICATION App11;\n\nSTOP APPLICATION App12;\nUNDEPLOY APPLICATION App12;\nDROP APPLICATION App12 CASCADE;\nCREATE APPLICATION App12;\nCREATE OR REPLACE SOURCE App12_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App12_SampleStream;\nCREATE OR REPLACE TARGET App12_NullTarget using NullWriter()\nINPUT FROM App12_SampleStream;\nEND APPLICATION App12;\nDEPLOY APPLICATION App12;\nSTART APPLICATION App12;\n\nSTOP APPLICATION App13;\nUNDEPLOY APPLICATION App13;\nDROP APPLICATION App13 CASCADE;\nCREATE APPLICATION App13;\nCREATE FLOW AgentFlow13;\nCREATE OR REPLACE SOURCE App13_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App13_SampleStream;\nEND FLOW AgentFlow13;\nCREATE FLOW ServerFlow13;\nCREATE OR REPLACE TARGET App13_NullTarget using NullWriter()\nINPUT FROM App13_SampleStream;\nEND FLOW ServerFlow13;\nEND APPLICATION App13;\ndeploy application App13 on any in ServerDG1 with AgentFlow13 on any in Agents, ServerFlow13 on any in ServerDG1;\nSTART APPLICATION App13;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n\nCREATE SOURCE @APPNAME@_S USING MSSqlReader\n(\n  Compression: false,\n  cdcRoleName: 'STRIIM_READER',\n  DatabaseName: 'QATEST',\n  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',\n  ConnectionPoolSize: 1,\n  FetchTransactionMetadata: false,\n  StartPosition: 'EOF',\n  Username: 'qatest',\n  SendBeforeImage: true,\n  AutoDisableTableCDC: true,\n  ConnectionURL: 'localhost:1433',\n  Tables: 'qatest.test01',\n  adapterName: 'MSSqlReader',\n  Password: 'w3b@ct10n'\n)\nOUTPUT TO @APPNAME@_SS;\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'NULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:0',\nStandardSQL:true\t,\noptimizedMerge:true\t\n) INPUT FROM @APPNAME@_ss;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\n--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 2 on two node cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS\n--\n\nSTOP APPLICATION N2S2CR2Tester.N2S2CRTest2;\nUNDEPLOY APPLICATION N2S2CR2Tester.N2S2CRTest2;\nDROP APPLICATION N2S2CR2Tester.N2S2CRTest2 CASCADE;\nCREATE APPLICATION N2S2CRTest2 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN2S2CRTest2;\n\nCREATE SOURCE CsvSourceN2S2CRTest2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN2S2CRTest2;\n\nCREATE FLOW DataProcessingN2S2CRTest2;\n\nCREATE TYPE WactionTypeN2S2CRTest2 (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionTypeN2S2CRTest2;\n\nCREATE CQ CsvToWaction\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsN2S2CRTest2 CONTEXT OF WactionTypeN2S2CRTest2\nEVENT TYPES ( WactionTypeN2S2CRTest2 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsN2S2CRTest2\nINSERT INTO WactionsN2S2CRTest2\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND FLOW DataProcessingN2S2CRTest2;\n\nEND APPLICATION N2S2CRTest2;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application AzureApp;\nundeploy application AzureApp;\ndrop application AzureApp cascade;\n\ncreate application AzureApp\nRECOVERY 10 second interval;\ncreate source CSVSource using FileReader (\n\tdirectory:'@DIR@',\n\tWildCard:'@WILDCARD@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  curr String,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       data[6],\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target BlobT using AzureBlobWriter(\n\taccountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:30,interval:5s'\n)\nformat using AvroFormatter (\n)\ninput from TypedCSVStream;\nend application AzureApp;\ndeploy application AzureApp in default;\nstart application AzureApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ recovery 1 second interval;\n\nCREATE SOURCE @SOURCENAME@ USING OracleReader\n(\n    Username: '@SRCUSERNAME@',\n    Password: '@SRCPASSWORD@',\n    ConnectionURL: '@SRCURL',\n    Tables: '@SRCTABLE',\n    FetchSize: '@FETCHSIZE@',\n    CommittedTransactions: true\n)\n\nOUTPUT TO @STREAM@ ;\n\nCREATE TARGET @TARGETNAME@ using DatabaseWriter\n(\n    ConnectionURL: '@TARGETURL',\n    username: '@TARGETUSERNAME@',\n    Password: '@TARGETPASSWORD@',\n    Tables: '@TARGETTABLE@',\n    BatchPolicy:'EventCount:1,Interval:1',\n    CommitPolicy:'EventCount:1,Interval:1'\n)\nINPUT FROM @STREAM@;\n\nEND APPLICATION @APPNAME@;\n\nDEPLOY APPLICATION @APPNAME@;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@ recovery 5 second Interval;\ncreate source @srcName@ USING MySQLReader\n(\n  Username:'@srcusername@',\n  Password:'@srcpassword@',\n  ConnectionURL:'@srcurl@',\n  Tables:'@srcschema@.@srctable@',\n  sendBeforeImage:'true',\n  FilterTransactionBoundaries:'true'\n) \nOUTPUT TO @outstreamname@;\n\nCREATE OR REPLACE TARGET @tgtName@ USING DatabaseWriter\n(\n  CheckPointTable:'CHKPOINT',\n  Username:'@tgtusername@',\n  Password:'@tgtpassword@',\n  BatchPolicy:'EventCount:1,Interval:0',\n  CommitPolicy:'EventCount:1,Interval:0',\n  ConnectionURL:'@tgturl@',\n  Tables:'@srcschema@.@srctable@,@tgtschema@.@tgttable@'\n) \nINPUT FROM @instreamname@;\n\n\nEnd APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( \n  Username: '@SRC_USERNAME@',\n  Password: '@SRC_PASSWORD@',\n  ConnectionURL: '@CDC_URL@',\n  Tables: '@Source1Tables@',\n  FetchSize: 1) \nOUTPUT TO @APPNAME@cdcStream;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@ RECOVERY 1 SECOND INTERVAL;\nCREATE SOURCE @srcName@ USING OracleReader (\n Username: '@srcusername@',\n  Password: '@srcpassword@',\n  ConnectionURL: '@srcurl@',\n  Tables: '@srcschema@.@srctable@'\n)\nOUTPUT TO @outstreamname@;\n\nCREATE OR REPLACE TARGET @tgtName@ USING DatabaseWriter\n(\n  DatabaseProviderType:'Default',\n  CheckPointTable:'CHKPOINT',\n  PreserveSourceTransactionBoundary:'false',\n  Username:'@tgtusername@',\n  BatchPolicy:'EventCount:1,Interval:0',\n  CommitPolicy:'EventCount:1,Interval:0',\n  ConnectionURL:'@tgturl@',\n  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',\n  Password:'@tgtpassword@'\n)\nINPUT FROM @instreamname@;\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE TARGET @TARGET_NAME@ USING SpannerWriter (\n\tTables: 'QATEST.%,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tinstanceId: 'qatest'\n) INPUT FROM @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 37 with two sources, two jumping time windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jt1W/p -> CQ1 -> WS\n--   S2 -> Jt2W/p -> CQ2 -> WS\n--\n\nSTOP KStreamRecov37Tester.KStreamRecovTest37;\nUNDEPLOY APPLICATION KStreamRecov37Tester.KStreamRecovTest37;\nDROP APPLICATION KStreamRecov37Tester.KStreamRecovTest37 CASCADE;\n\nDROP USER KStreamRecov37Tester;\nDROP NAMESPACE KStreamRecov37Tester CASCADE;\nCREATE USER KStreamRecov37Tester IDENTIFIED BY KStreamRecov37Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov37Tester;\nCONNECT KStreamRecov37Tester KStreamRecov37Tester;\n\nCREATE APPLICATION KStreamRecovTest37 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP WITHIN 1 SECOND\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP WITHIN 2 SECOND\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nEND APPLICATION KStreamRecovTest37;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 23 with two sources, two sliding time windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> St1W -> CQ1 -> WS\n-- S2 -> St2W -> CQ2 -> WS\n--\n\nSTOP KStreamRecov23Tester.KStreamRecovTest23;\nUNDEPLOY APPLICATION KStreamRecov23Tester.KStreamRecovTest23;\nDROP APPLICATION KStreamRecov23Tester.KStreamRecovTest23 CASCADE;\nDROP USER KStreamRecov23Tester;\nDROP NAMESPACE KStreamRecov23Tester CASCADE;\nCREATE USER KStreamRecov23Tester IDENTIFIED BY KStreamRecov23Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov23Tester;\nCONNECT KStreamRecov23Tester KStreamRecov23Tester;\n\nCREATE APPLICATION KStreamRecovTest23 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP WITHIN 1 SECOND;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP WITHIN 2 SECOND;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2;\n\nEND APPLICATION KStreamRecovTest23;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@ recovery 1 second interval;\n\ncreate source @APPNAME@_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using CobolCopybookParser (\ncopybookFileName : '@TD@/@PROP1@',\n  dataFileFont: '@PROP2@',\n  copybookSplit: '@PROP3@',\n  dataFileOrganization: '@PROP4@',\n  copybookDialect: '@PROP5@', \n  skipIndent:'@PROP6@',\n  DatahandlingScheme:'@PROP7@'\n  --recordSelector: '@PROP8@'\n)\nOUTPUT TO @APPNAME@Stream;\n\ncreate Target @APPNAME@Target using FileWriter(\n    filename :'@FILE@',\n    directory : '@FOLDER@'\n)\nformat using JsonFormatter (\n)\ninput from @APPNAME@Stream;\n\nCREATE TYPE test_type (\n account_no com.fasterxml.jackson.databind.JsonNode,\n first_name com.fasterxml.jackson.databind.JsonNode,\n last_name com.fasterxml.jackson.databind.JsonNode,\n addr1 com.fasterxml.jackson.databind.JsonNode,\nAddr2 com.fasterxml.jackson.databind.JsonNode,\nCity com.fasterxml.jackson.databind.JsonNode,\nState com.fasterxml.jackson.databind.JsonNode,\nZip com.fasterxml.jackson.databind.JsonNode\n);\n\nCreate stream cqAsJSONNodeStream of test_type;\n\nCREATE CQ GetPOAsJsonNodes\nINSERT into cqAsJSONNodeStream\n    select \n    data.get('ACCTS-RECORD').get('ACCOUNT-NO'),\ndata.get('ACCTS-RECORD').get('NAME').get('FIRST-NAME'),\ndata.get('ACCTS-RECORD').get('NAME').get('LAST-NAME'),\ndata.get('ACCTS-RECORD').get('ADDRESS1'),\ndata.get('ACCTS-RECORD').get('ADDRESS2'),\ndata.get('ACCTS-RECORD').get('ADDRESS3').get('CITY'),\ndata.get('ACCTS-RECORD').get('ADDRESS3').get('STATE'),\ndata.get('ACCTS-RECORD').get('ADDRESS3').get('ZIP-CODE')\nfrom @APPNAME@Stream js;\n\ncreate type finaldtype(\n      ACCOUNT_NO String, \n      FIRST_NAME String,\n      LAST_NAME String,\n      ADDRESS1 String,\n      ADDRESS2 String,\n      CITY String,\n      STATE String,\n      ZIP_CODE String\n);\n\nCREATE CQ getdata\nINSERT into getdataStream\n    select account_no.toString(),\n    first_name.toString(),\n    last_name.toString(),\n    addr1.toString(),\n    Addr2.toString(),\n    City.toString(),\n    State.toString(),\n    Zip.toString()\nfrom cqAsJSONNodeStream x;\n\ncreate Target @APPNAME@DBTarget using DatabaseWriter(\n  Username: 'qatest',\n  Password: 'qatest',\n  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',\n  BatchPolicy: 'EventCount:10000',\n  CommitPolicy: 'EventCount:10000',\n  Tables: 'QATEST.@APPNAME@'\n)\ninput from getdataStream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@ on all in default;\nstart application @APPNAME@;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application AzureDLSGen1_sanity;\nundeploy application AzureDLSGen1_sanity;\ndrop application AzureDLSGen1_sanity cascade;\n\n\ncreate application AzureDLSGen1_sanity recovery 5 Second interval;\ncreate source CSVSource using FileReader (\n        directory:'./Samples/AppData/',\n        WildCard:'dynamicdirectory.csv',\n        positionByEOF:false,\n        charset:'UTF-8'\n)\nparse using DSVParser (\n        header:'no'\n)\nOUTPUT TO CsvStream;\n\ncreate Target WriteToADLSGen1 using ADLSGen1Writer(\n        filename:'',\n        directory:'',\n        datalakestorename:'',\n        clientid:'',\n        authtokenendpoint:'',\n        clientkey:'',\n\t\trolloverpolicy:'eventcount:100000'\n)\nformat using DSVFormatter (\n)\ninput from CsvStream; \n\nend application AzureDLSGen1_sanity;\n\ndeploy application AzureDLSGen1_sanity;\nstart application AzureDLSGen1_sanity;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\ncreate application @appname@ recovery 1 second interval;\n\nCREATE SOURCE @parquetsrc@ USING FileReader (\n  directory: '',\n  positionByEOF: false,\n  WildCard: '' )\nPARSE USING ParquetParser (\n )\nOUTPUT TO @appname@Streams;\n\nCREATE OR REPLACE CQ @appname@CQOrder3\nINSERT INTO @appname@Stream3\nSELECT\nPUTUSERDATA(s,'schemaName',s.data.getSchema().getName())\nFROM @appname@Streams s;\n\nCREATE TARGET @adlstarget@ USING Global.ADLSGen2Writer (\n    accountname:'',\n  \tsastoken:'',\n  \tfilesystemname:'',\n  \tfilename:'',\n  \tdirectory:'',\n  \tuploadpolicy:'eventcount:10' )\n\nformat using AvroFormatter (\n)\nINPUT FROM @appname@Stream3;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop ORAToBigquery;\nundeploy application ORAToBigquery;\ndrop application ORAToBigquery cascade;\n\nCREATE APPLICATION ORAToBigquery RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE Rac11g USING OracleReader ( \n  SupportPDB: false,\n  SendBeforeImage: true,\n  ReaderType: 'LogMiner',\n  CommittedTransactions: false,\n  FetchSize: 1,\n  Password: 'manager',\n  DDLTracking: false,\n  StartTimestamp: 'null',\n  OutboundServerProcessName: 'WebActionXStream',\n  OnlineCatalog: true,\n  ConnectionURL: '192.168.33.10:1521/XE',\n  SkipOpenTransactions: false,\n  Compression: false,\n  QueueSize: 40000,\n  RedoLogfiles: 'null',\n  Tables: 'SYSTEM.GGAUTHORIZATIONS',\n  Username: 'system',\n  FilterTransactionBoundaries: true,\n  adapterName: 'OracleReader',\n  XstreamTimeOut: 600,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=60, maxRetries=3'\n ) \nOUTPUT TO DataStream;\n\nCREATE OR REPLACE TARGET Target1 USING SysOut ( \n  name: \"dstream\"\n ) \nINPUT FROM DataStream;\n\nCREATE OR REPLACE TARGET Target2 using BigqueryWriter(\n  BQServiceAccountConfigurationPath:\"/Users/ravipathak/Downloads/big-querytest-1963ae421e90.json\",\n  projectId:\"big-querytest\",\n  Tables: \"SYSTEM.GGAUTHORIZATIONS,testing1.ggauthorisation\",\n  parallelismCount: 2,\n  BatchPolicy: \"eventCount:100000,Interval:0\")\nINPUT FROM DataStream;\n\nEND APPLICATION ORAToBigquery;\n\ndeploy application ORAToBigquery;\nstart ORAToBigquery;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING DatabaseReader (\n  Tables: '',\n  ConnectionURL: '',\n  Password: '',\n  Username: ''\n  )\nOUTPUT TO @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@ Using DatabaseReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'qatest.@SourceTable@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop ADW;\nundeploy application ADW;\nDROP APPLICATION ADW CASCADE;\nCREATE APPLICATION ADW recovery 5 second interval;\nCreate Source OracleSource Using OracleReader\n(\n Username:'@ORACLE-USERNAME',\n Password:'@ORACLE-PASSWORD',\n ConnectionURL: '@ORACLE-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'@FETCH-SIZE@'\n) \nOutput To str;\n\n\ncreate target AzureTarget using AzureSQLDWHWriter (\n\t\tConnectionURL: '@SQLDW-URL@',\n        username: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@',\n        uploadpolicy:'@EVENT-COUNT@'\n) INPUT FROM str;\n\ncreate Target t2 using SysOut(name:Foo2) input from str;\n\nEND APPLICATION ADW;\ndeploy application ADW;\nstart application ADW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCreate Source @SourceName1@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM1@;\nCreate Source @SourceName2@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM2@;\nCreate Source @SourceName3@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM3@;\nCreate Source @SourceName4@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM4@;\nCreate Source @SourceName5@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM5@;\nCreate Source @SourceName6@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM6@;\nCreate Source @SourceName7@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM7@;\nCreate Source @SourceName8@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM8@;\nCreate Source @SourceName9@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM9@;\nCreate Source @SourceName10@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM10@;\n\nCREATE TARGET @targetName1@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM1@;\nCREATE TARGET @targetName2@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM2@;\nCREATE TARGET @targetName3@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM3@;\nCREATE TARGET @targetName4@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM4@;\nCREATE TARGET @targetName5@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM5@;\nCREATE TARGET @targetName6@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM6@;\nCREATE TARGET @targetName7@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM7@;\nCREATE TARGET @targetName8@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM8@;\nCREATE TARGET @targetName9@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM9@;\nCREATE TARGET @targetName10@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM10@;\n\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@ Using PostgreSQLReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'qatest.@SourceTable@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test T20\n-- Nicholas Keene, WebAction, Inc.\n--\n-- Snum -> CQ -> WS\n--\n\n\nUNDEPLOY APPLICATION NameT20.T20;\nDROP APPLICATION NameT20.T20 CASCADE;\nCREATE APPLICATION T20;\n\n\n\n\nCREATE FLOW DataAcquisitionT20;\n\n\nCREATE SOURCE CsvSourceT20 USING NumberSource ( \n  lowValue: '1',\n  highValue: '1003',\n  delayMillis: '10',\n  delayNanos: '0',\n  repeat: 'false'\n ) \nOUTPUT TO OutputStreamT20;\n\n\nEND FLOW DataAcquisitionT20;\n\n\n\n\nCREATE FLOW DataProcessingT20;\n\n\nCreate Target OutputTargetT20\nUsing Sysout (name: 'OutputTargetT20')\nInput From OutputStreamT20;\n\n\nEND FLOW DataProcessingT20;\n\n\n\nEND APPLICATION T20;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application GGTrailReaderApp;\nundeploy application GGTrailReaderApp;\ndrop application GGTrailReaderApp cascade;\n\ncreate application GGTrailReaderApp recovery 5 second interval;\n\ncreate source GGTrailSource using GGTrailReader (\ntRaildIrectory:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario1',\ntRAilfilepattern:'n1*',\npositionByEOF:false,\nFilterTransactionBoundaries: true,\nDefinitionFile:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario1/Scn1_beforeddl.def',\ncaptureCDdl: true,\nCDDLAction:'Process',\n--CDDLAction:'Ignore',\nTrailByTeOrder:'LittleEndian',\nrecoveryInterval: 5\n)\nOUTPUT TO GGTrailStream;\n\ncreate Target t2 using SysOut(name:Foo2) input from GGTrailStream;\n\nCREATE TARGET WriteCDCOracle1 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost/orcl',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:1,Interval:1',\nCommitPolicy:'Eventcount:1,Interval:1',\nCheckpointtable:'RGRN_CHKPOINT',\nTables:'QATEST.GGDDL1,QATEST.GGDDL1_TGT'\n) INPUT FROM GGTrailStream1;\n\n\nend application GGTrailReaderApp;\n\ndeploy application GGTrailReaderApp;\nstart application GGTrailReaderApp;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "DROP APPLICATION ns1.OPExample cascade;\nDROP NAMESPACE ns1 cascade;\nCREATE OR REPLACE NAMESPACE ns1;\nUSE ns1;\nCREATE APPLICATION OPExample;\n\nCREATE source CsvDataSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'PosDataPreview.csv',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n)\nOUTPUT TO CsvStream;\n \nCREATE TYPE MerchantHourlyAve(\n  merchantId String,\n  hourValue integer,\n  hourlyAve integer\n);\n\nCREATE CACHE HourlyAveLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'hourlyData.txt'\n)\nPARSE USING DSVParser (\n  header: Yes,\n  trimquote:false,\n  trimwhitespace:true\n) \nQUERY (keytomap:'merchantId') \nOF MerchantHourlyAve;\n\nCREATE CQ CsvToPosData\nINSERT INTO PosDataStream partition by merchantId\nSELECT TO_STRING(data[1]) as merchantId,\n  TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\n  DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,\n  TO_DOUBLE(data[7]) as amount,\n  TO_INT(data[9]) as zip\nFROM CsvStream;\n \nCREATE CQ cq2\nINSERT INTO SendToOPStream\nSELECT makeList(dateTime) as dateTime,\n  makeList(zip) as zip\nFROM PosDataStream;\n \nCREATE TYPE ReturnFromOPStream_Type ( time DateTime , val Integer );\nCREATE STREAM ReturnFromOPStream OF ReturnFromOPStream_Type;\n\nCREATE TARGET OPExampleTarget \nUSING FileWriter (filename: 'OPExampleOut') \nFORMAT USING JSONFormatter() \nINPUT FROM ReturnFromOPStream;\n\nCREATE OPEN PROCESSOR testOp USING Global.TupleConverter ( lastItemSeen: 0, ahead: 1 )\nINSERT INTO ReturnFromOPStream FROM SendToOPStream ENRICH WITH HourlyAveLookup;\n \nEND APPLICATION OPExample;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application FileXML;\ncreate source XMLSource using FileReader (\n\tDirectory:'@TEST-DATA-PATH@',\n\tWildCard:'books.xml',\n\tpositionByEOF:false\n)\nparse using XMLParser (\n\tRootNode:'/catalog/book'\n)\nOUTPUT TO XmlStream;\n\n-- Below Sysout is added to test DEV-23437.  Not directly validated in the test except the App should not crash with sysout target\nCREATE TARGET XMLEventSYSout USING sysout  (\nname: 'XMLEventSYSoutOut' )\nINPUT FROM XmlStream;\n\ncreate Target XMLDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/xmldata') input from XmlStream;\nend application FileXML;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE OR REPLACE SOURCE @APPNAME@DataSrc USING OracleReader (\n  Tables: '',\n  ConnectionURL: '',\n  Password: '',\n  Username: ''\n)\nOUTPUT TO @APPNAME@DataStream;\n\nCREATE OR REPLACE TARGET @APPNAME@DataTrgt USING MongoDBWriter (\n  ConnectionURL: '',\n  Username: '',\n  Password: '',\n  collections: ''\n  AuthDB: '',\n  batchpolicy: 'EventCount:1000, Interval:30',\n )\nINPUT FROM @APPNAME@DataStream;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING MongoDBReader (\n  ConnectionURL: '',\n  Username: '',\n  password: '',\n  authDB: '',\n  collections: '',\n  mode: 'Incremental'\n  )\nOUTPUT TO @APPNAME@stream;\n\nCREATE CQ @APPNAME@CQ\nINSERT INTO @APPNAME@CQSTREAM\nSELECT data.get(\"NUM_COL\").toString() AS NUM_COL,\n  data.get(\"CHAR_COL\").toString() AS CHAR_COL,\n  data.get(\"VARCHAR2_COL\").toString() AS VARCHAR2_COL,\n  data.get(\"FLOAT_COL\").toString() AS FLOAT_COL,\n  data.get(\"BINARY_FLOAT_COL\").toString() AS BINARY_FLOAT_COL,\n  data.get(\"BINARY_DOUBLE_COL\").toString() AS BINARY_DOUBLE_COL,\n  data.get(\"DATE_COL\").toString() AS DATE_COL,\n  data.get(\"TIMESTAMP_COL\").toString() AS TIMESTAMP_COL\nFROM @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING AvroFormatter (\nschemaFileName: '@SCHEMAFILE@'\n)\nINPUT FROM @APPNAME@CQSTREAM;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING DSVFormatter ()\nINPUT FROM @APPNAME@CQSTREAM;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application AzureApp;\nundeploy application AzureApp;\ndrop application AzureApp cascade;\n\ncreate application AzureApp\nRECOVERY 10 second interval;\nCREATE SOURCE OracleSource USING OracleReader\n(\n    Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n Tables:'@TABLES@',\n    FetchSize: 1\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  tablename String,\n  data java.util.HashMap  \n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT TO_LOWER(META(s, \"TableName\").toString()) as tablename,\n       DATA(s) as data\nFROM CsvStream s;\n\ncreate Target BlobT using AzureBlobWriter(\n\taccountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:5,interval:5s'\n)\nformat using AvroFormatter (\n)\ninput from TypedCSVStream;\nend application AzureApp;\ndeploy application AzureApp in default;\nstart application AzureApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @appname@routerApp;\nUNDEPLOY APPLICATION @appname@routerApp;\nDROP APPLICATION @appname@routerApp CASCADE;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',\n  acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\nCREATE APPLICATION @appname@routerApp RECOVERY 10 SECOND INTERVAL;\n\nCREATE  SOURCE @appname@OraSource USING OracleReader  (\nUsername: 'qatest',\nPassword: 'qatest',\nConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',\nTables: 'QATEST.TGT_T%',\n FetchSize:'100'\n)\nOUTPUT TO @appname@MasterStream1;\n\n-- CREATE STREAM @appname@ss1 OF Global.waevent persist using Global.DefaultKafkaProperties;\n-- CREATE STREAM @appname@ss2 OF Global.waevent persist using Global.DefaultKafkaProperties;\n-- CREATE STREAM @appname@ss3 OF Global.waevent persist using Global.DefaultKafkaProperties;\n\nCREATE STREAM @appname@ss1 OF Global.waevent PERSIST USING KafkaPropset;\nCREATE STREAM @appname@ss2 OF Global.waevent PERSIST USING KafkaPropset;\nCREATE STREAM @appname@ss3 OF Global.waevent PERSIST USING KafkaPropset;\n\nCREATE OR REPLACE ROUTER @appname@tablerouter1 INPUT FROM @appname@MasterStream1 s CASE\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T1' THEN ROUTE TO @appname@ss1,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T2' THEN ROUTE TO @appname@ss2,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T3' THEN ROUTE TO @appname@ss3,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T4' THEN ROUTE TO @appname@ss4,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T5' THEN ROUTE TO @appname@ss5,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T6' THEN ROUTE TO @appname@ss6,\nELSE ROUTE TO @appname@ss_else;\n\ncreate Target @appname@FileTarget_1 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n)\nFORMAT USING dsvFormatter ()\ninput from @appname@ss1;\n\ncreate Target @appname@FileTarget_2 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n)\nFORMAT USING dsvFormatter ()\ninput from @appname@ss2;\n\ncreate Target @appname@FileTarget_3 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n\n)\nFORMAT USING dsvFormatter ()\ninput from @appname@ss3;\n\nCREATE OR REPLACE TARGET @appname@KafkaTarget_4 USING KafkaWriter VERSION '0.11.0' (\n  brokerAddress: 'localhost:9092',\n  Topic: 'target4'\n )\nFORMAT USING JSONFormatter  (\n )\nINPUT FROM @appname@ss4;\n\nCREATE OR REPLACE TARGET @appname@KafkaTarget_5 USING KafkaWriter VERSION '0.11.0' (\n  brokerAddress: 'localhost:9092',\n  Topic: 'target5'\n )\nFORMAT USING JSONFormatter  (\n )\nINPUT FROM @appname@ss5;\n\nCREATE OR REPLACE TARGET @appname@KafkaTarget_6 USING KafkaWriter VERSION '0.11.0' (\n  brokerAddress: 'localhost:9092',\n  Topic: 'target6'\n )\nFORMAT USING JSONFormatter  (\n )\nINPUT FROM @appname@ss6;\n\n\n\n\nend application @appname@routerApp;\ndeploy application @appname@routerApp;\nstart @appname@routerApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;\n\nCreate Source @SourceName@ Using Ojet\n\n(\n  Username:'c##qatest',\n  Password:'qatest',\n  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',\n  Tables:'CDB$ROOT.\"C##QATEST\".ojet_src;ORCLPDB.QATEST.ojet_src',\n  _h_useClassic:false,\n  Fetchsize:1,\n  Compression: true,\n  SupportPDB:true,\n  ReplicationSlotName:'null'\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter\n(\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',\n  Username:'c##qatest',\n  Password:'qatest',\n  BatchPolicy:'EventCount:1,Interval:1',\n  Tables:'CDB$ROOT.\"C##QATEST\".ojet_src,CDB$ROOT.\"C##QATEST\".ojet_tgt'\n) INPUT FROM @SRCINPUTSTREAM@;\n\n\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING DataBaseReader (\n  Tables: '',\n  ConnectionURL: '',\n  Password: '',\n  Username: ''\n  )\nOUTPUT TO @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application KinesisTest;\ncreate source CSVSource using FileReader (\n\tdirectory:'/home/dz/src/product/Samples/AppData',\n\tWildCard:'posdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  companyName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0], data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate or replace Target t using KinesisWriter (\n\tregionName:'TARGET_REGION',\n\tstreamName:'TARGET_STREAM'\n)\nformat using DSVFormatter (\n)\ninput from TypedCSVStream;\nend application KinesisTest;\ndeploy application KinesisTest in default;\nstart application KinesisTest;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE application @APPNAME@ @Recovery@ AUTORESUME MAXRETRIES 2 RETRYINTERVAL 10;\n\ncreate type @APPNAME@type1(\n  companyName String,\n  merchantId String,\n  city string\n);\n\ncreate type @APPNAME@type2(\n  c1 integer,\n  c2 String,\n  c3 string\n);\n\ncreate type @APPNAME@type3(\nc1 integer\n);\n\ncreate type @APPNAME@type4(\nc1 integer,\nc2 integer\n);\n\ncreate stream @APPNAME@in_memory_typedStream of @APPNAME@type1 partition by city;\ncreate stream @APPNAME@in_memory_typedStream_num of @APPNAME@type2;\ncreate stream @APPNAME@in_memory_typedStream_num1 of @APPNAME@type2;\ncreate stream @APPNAME@in_memory_typedStream_num2 of @APPNAME@type2;\ncreate stream @APPNAME@in_memory_typedStream_num3 of @APPNAME@type2;\ncreate stream @APPNAME@in_memory_typedStream_num4 of @APPNAME@type2;\ncreate stream @APPNAME@in_memory_typedStream_num5 of @APPNAME@type2;\ncreate stream @APPNAME@finalstream6 of @APPNAME@type4;\n\ncreate source @APPNAME@s using FileReader (\n        directory:'Product/IntegrationTests/TestData/',\n        wildcard:'posdata5L.csv',\n        positionByEOF:false,\n        Charset:'UTF-8'\n)\nPARSE USING DSVParser (\n  columndelimiter:',',\n  ignoreemptycolumn:'Yes',\n  quoteset:'[]~\"',\n  header: true,\n  separator:'~'\n\n)\nOUTPUT TO @APPNAME@in_memory_rawStream;\n\n\ncreate CQ @APPNAME@cq1\nINSERT INTO @APPNAME@kps_waevent\nSELECT *\nFROM @APPNAME@in_memory_rawStream  ;\n\ncreate CQ @APPNAME@cq2\nINSERT INTO @APPNAME@in_memory_typedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\"),\nTO_STRING(data[1]),\nTO_STRING(data[10])\nFROM @APPNAME@kps_waevent ;\n\ncreate CQ @APPNAME@cq3\nINSERT INTO @APPNAME@in_memory_typedStream_num1\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\nFROM @APPNAME@in_memory_typedStream;\n-- order by c3;\n\ncreate CQ @APPNAME@cq4\nINSERT INTO @APPNAME@in_memory_typedStream_num2\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\nFROM @APPNAME@in_memory_typedStream;\n\ncreate CQ @APPNAME@cq5\nINSERT INTO @APPNAME@in_memory_typedStream_num3\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\nFROM @APPNAME@in_memory_typedStream;\n\ncreate CQ @APPNAME@cq6\nINSERT INTO @APPNAME@in_memory_typedStream_num4\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\nFROM @APPNAME@in_memory_typedStream;\n\ncreate CQ @APPNAME@cq7\nINSERT INTO @APPNAME@in_memory_typedStream_num5\nSELECT TO_INT(companyName) as c1, merchantId as c2,city as c3\nFROM @APPNAME@in_memory_typedStream;\n\nCREATE CQ @APPNAME@cq8\nINSERT INTO @APPNAME@in_memory_typedStream_num6\nSELECT TO_INT(companyName) as c1\nFROM @APPNAME@in_memory_typedStream;\n\n\nCREATE JUMPING WINDOW @APPNAME@DataStream1_100000Rows\nOVER @APPNAME@in_memory_typedStream_num1 KEEP 100000 ROWS;\n\n\nCREATE JUMPING WINDOW @APPNAME@DataStream2_100000Rows\nOVER @APPNAME@in_memory_typedStream_num2 KEEP 100000 ROWS;\n\n\nCREATE JUMPING WINDOW @APPNAME@DataStream3_100000Rows\nOVER @APPNAME@in_memory_typedStream_num3 KEEP 100000 ROWS;\n\n\nCREATE JUMPING WINDOW @APPNAME@DataStream4_100000Rows\nOVER @APPNAME@in_memory_typedStream_num4 KEEP 100000 ROWS;\n\n\nCREATE JUMPING WINDOW @APPNAME@DataStream5_100000Rows\nOVER @APPNAME@in_memory_typedStream_num5 KEEP 100000 ROWS;\n\nCREATE JUMPING WINDOW @APPNAME@DataStream6_100000Rows\nOVER @APPNAME@in_memory_typedStream_num6 KEEP 100000 ROWS;\n\ncreate CQ @APPNAME@cq9\nINSERT INTO @APPNAME@finalstream1\nSELECT c1 FROM @APPNAME@DataStream1_100000Rows sample by c1;\n\ncreate CQ @APPNAME@cq10\nINSERT INTO @APPNAME@finalstream2\nSELECT c1 FROM @APPNAME@DataStream2_100000Rows sample by c1 selectivity 0.1;\n\ncreate CQ @APPNAME@cq11\nINSERT INTO @APPNAME@finalstream3\nSELECT c1 FROM @APPNAME@DataStream3_100000Rows sample by c1 selectivity 0.25;\n\n\ncreate CQ @APPNAME@cq12\nINSERT INTO @APPNAME@finalstream4\nSELECT c1 FROM @APPNAME@DataStream4_100000Rows sample by c1 selectivity 0.05;\n--SELECT count(*) FROM @APPNAME@DataStream4Rows10000Seconds sample by c1 selectivity 0.05;\n\ncreate CQ @APPNAME@cq13\nINSERT INTO @APPNAME@finalstream5\nSELECT c1 FROM @APPNAME@DataStream5_100000Rows sample by c1 selectivity 0.01;\n\ncreate CQ @APPNAME@cq14\nINSERT INTO @APPNAME@finalstream6\nSELECT c1,c1 as c2 FROM @APPNAME@DataStream6_100000Rows sample by c1,c2 selectivity 0.01;\n\ncreate target @APPNAME@target1 using filewriter (\nfilename:'FEATURE-DIR/logs/@APPNAME@target1.log',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000,sequence:00'\n)\nformat using dsvFormatter()\ninput from @APPNAME@finalstream1;\n\ncreate target @APPNAME@target2 using filewriter (\nfilename:'FEATURE-DIR/logs/@APPNAME@target2.log',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000,sequence:00'\n)\nformat using dsvFormatter()\ninput from @APPNAME@finalstream2;\n\ncreate target @APPNAME@target3 using filewriter (\nfilename:'FEATURE-DIR/logs/@APPNAME@target3.log',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000,sequence:00'\n)\nformat using dsvFormatter()\ninput from @APPNAME@finalstream3;\n\ncreate target @APPNAME@target4 using filewriter (\nfilename:'FEATURE-DIR/logs/@APPNAME@target4.log',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000,sequence:00'\n)\nformat using dsvFormatter()\ninput from @APPNAME@finalstream4;\n\ncreate target @APPNAME@target5 using filewriter (\nfilename:'FEATURE-DIR/logs/@APPNAME@target5.log',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000,sequence:00'\n)\nformat using dsvFormatter()\ninput from @APPNAME@finalstream5;\n\nCREATE WACTIONSTORE @APPNAME@Wactions1 CONTEXT OF @APPNAME@type3\nEVENT TYPES ( @APPNAME@type2 )\nUSING ( storageProvider:'elasticsearch' );\n\nCREATE WACTIONSTORE @APPNAME@Wactions2 CONTEXT OF @APPNAME@type3\nEVENT TYPES ( @APPNAME@type2 )\nUSING ( storageProvider:'elasticsearch' );\n\nCREATE WACTIONSTORE @APPNAME@Wactions3 CONTEXT OF @APPNAME@type3\nEVENT TYPES ( @APPNAME@type2 )\nUSING ( storageProvider:'elasticsearch' );\n\nCREATE WACTIONSTORE @APPNAME@Wactions4 CONTEXT OF @APPNAME@type4\nEVENT TYPES ( @APPNAME@type4 )\nUSING ( storageProvider:'elasticsearch' );\n\nCREATE WACTIONSTORE @APPNAME@Wactions5 CONTEXT OF @APPNAME@type4\nEVENT TYPES ( @APPNAME@type4 )\nUSING ( storageProvider:'elasticsearch' );\n\n--sampling twice: one in finalstream1 and another in select query.\nCREATE CQ @APPNAME@cq15\nINSERT INTO @APPNAME@Wactions1\nSELECT FIRST(p.c1) FROM @APPNAME@finalstream1 p GROUP BY p.c1 sample by p.c1 ;\n\n--sampling once: results will be same as target2 and target1.\nCREATE CQ @APPNAME@cq16\nINSERT INTO @APPNAME@Wactions2\nSELECT * from @APPNAME@finalstream1 order by c1 desc limit 10000 ;\n\n--sampling twice: one in finalstream1 and another in select query.\nCREATE CQ @APPNAME@cq17\nINSERT INTO @APPNAME@Wactions3\nSELECT * from @APPNAME@finalstream1 order by c1 sample by c1;\n\n--sampling using 2 fields, 2800 for single field and 332 for 2 field\nCREATE CQ @APPNAME@cq18\nINSERT INTO @APPNAME@Wactions4\nSELECT c1,c1 from @APPNAME@finalstream6 order by c1 sample by c1;\n\n--same as Wactions4 - here selectivity alone varies, so output is 8\nCREATE CQ @APPNAME@cq19\nINSERT INTO @APPNAME@Wactions5\nSELECT c1,c1 from @APPNAME@finalstream6 order by c1 sample by c1 selectivity 0.0001;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\n--start @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 22 with two sources, two sliding attribute windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Sa5W -> CQ1 -> WS\n-- S2 -> Sa6W -> CQ2 -> WS\n--\n\nSTOP KStreamRecov22Tester.KStreamRecovTest22;\nUNDEPLOY APPLICATION KStreamRecov22Tester.KStreamRecovTest22;\nDROP APPLICATION KStreamRecov22Tester.KStreamRecovTest22 CASCADE;\nDROP USER KStreamRecov22Tester;\nDROP NAMESPACE KStreamRecov22Tester CASCADE;\nCREATE USER KStreamRecov22Tester IDENTIFIED BY KStreamRecov22Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov22Tester;\nCONNECT KStreamRecov22Tester KStreamRecov22Tester;\n\nCREATE APPLICATION KStreamRecovTest22 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2;\n\nEND APPLICATION KStreamRecovTest22;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 36 with two sources, two jumping attribute windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Ja5W/p -> CQ1 -> WS\n-- S2 -> Ja6W/p -> CQ2 -> WS\n--\n\nSTOP Recov36Tester.RecovTest36;\nUNDEPLOY APPLICATION Recov36Tester.RecovTest36;\nDROP APPLICATION Recov36Tester.RecovTest36 CASCADE;\n\nDROP USER KStreamRecov36Tester;\nDROP NAMESPACE KStreamRecov36Tester CASCADE;\nCREATE USER KStreamRecov36Tester IDENTIFIED BY KStreamRecov36Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov36Tester;\nCONNECT KStreamRecov36Tester KStreamRecov36Tester;\n\nCREATE APPLICATION KStreamRecovTest36 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nEND APPLICATION KStreamRecovTest36;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ recovery 5 second interval;\nCREATE  SOURCE @SourceName@ USING MySqlReader  ( \n  Username: '@Username@',\n  Password: '@Password@',\n  DatabaseName: 'qatest',\n  connectionRetryPolicy: @ConnectionRetryPolicy@,\n  ConnectionURL: '@ConnectionURL@',\n  Tables: '@SourceTables@',\n  ConnectionPoolSize: 1,\n  ReplicationSlotName: 'null'\n ) \nOUTPUT TO @SRCINPUTSTREAM@;\ncreate Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP JSONRecoveryApp;\n\nUNDEPLOY APPLICATION admin.JSONRecoveryApp;\nDROP APPLICATION admin.JSONRecoveryApp cascade;\n\nCREATE APPLICATION JSONRecoveryApp RECOVERY 1 SECOND INTERVAL;\n\nCREATE TYPE Emptype (\nfirstName String,\nlastName String );\n\nCREATE STREAM EmpStream of Emptype;\n\ncreate source CSVSource using FileReader (\n\tdirectory:'/Users/bhashemi/Product/IntegrationTests/TestData/jsonRecov',\n\tWildCard:'jsonRecov*.json',\n\tpositionByEOF:false\n) PARSE USING\nJSONParser (\neventType:''\n) OUTPUT TO EmpStream;\n\nCREATE WACTIONSTORE jsonWactions CONTEXT OF Emptype\nEVENT TYPES ( Emptype )\n@PERSIST-TYPE@\n\nCREATE CQ InsertjsonWactions\nINSERT INTO jsonWactions\nSELECT firstName, lastName\nFROM EmpStream;\n\nCREATE TARGET jsonRecovSYSOUT using SysOut(name:jsonrecov) INPUT FROM EmpStream;\nEND APPLICATION JSONRecoveryApp;\n\ndeploy application JSONRecoveryApp in default;\nSTART JSONRecoveryApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING FileReader  (\n  blocksize: 64,\n  positionbyeof: false,\n  rolloverstyle: 'Default',\n  includesubdirectories: false,\n  adapterName: 'FileReader',\n  directory: '/Users/jenniffer/Downloads',\n  skipbom: true,\n  wildcard: 'dk000000000'\n )\n PARSE USING GGTrailParser  (\n  handler: 'com.webaction.proc.GGTrailParser_1_0',\n  metadata: '@METADATA@',\n  FilterTransactionBoundaries: true,\n  TrailByteOrder: '@BYTEORDDER@',\n  Tables: '@TABLES@',\n  parserName: 'GGTrailParser',\n  _h_ReturnDateTimeAs: '@DATETIME@',\n  Compression:'@COMPRESSION@'\n )OUTPUT to @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 3 on two node cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> JW -> CQ(aggregate) -> WS\n--\n\nSTOP APPLICATION N2S2CR3Tester.N2S2CRTest3;\nUNDEPLOY APPLICATION N2S2CR3Tester.N2S2CRTest3;\nDROP APPLICATION N2S2CR3Tester.N2S2CRTest3 CASCADE;\nCREATE APPLICATION N2S2CRTest3 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN2S2CRTest3;\n\nCREATE SOURCE CsvSourceN2S2CRTest3 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN2S2CRTest3;\n\nCREATE FLOW DataProcessingN2S2CRTest3;\n\nCREATE TYPE WactionTypeN2S2CRTest3 (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount int,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionTypeN2S2CRTest3;\n\nCREATE CQ CsvToDataN2S2CRTest3\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_INT(TO_DOUBLE(data[7])),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsN2S2CRTest3 CONTEXT OF WactionTypeN2S2CRTest3\nEVENT TYPES ( WactionTypeN2S2CRTest3 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsN2S2CRTest3\nINSERT INTO WactionsN2S2CRTest3\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.merchantId),\n    FIRST(p.dateTime),\n    SUM(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nEND FLOW DataProcessingN2S2CRTest3;\n\nEND APPLICATION N2S2CRTest3;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second Interval ;\n\nCreate Source @SourceName@\n Using OracleReader\n(\n Username:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@SourceConnectURL@',\n Tables:'@SourceTable@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectURL@',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Kafka Stream Agent Checkpoint Recovery tests\n-- Bert Hashemi and Zalak Shah, WebAction, Inc.\n--\n\nstop application recoveryTestAgent.KSRecovCSV;\nundeploy application recoveryTestAgent.KSRecovCSV;\ndrop application recoveryTestAgent.KSRecovCSV cascade;\nDROP USER recoveryTestAgent;\nDROP NAMESPACE recoveryTestAgent CASCADE;\nCREATE USER recoveryTestAgent IDENTIFIED BY recoveryTestAgent;\nGRANT create,drop ON deploymentgroup Global.* TO USER recoveryTestAgent;\nCONNECT recoveryTestAgent recoveryTestAgent;\n\ncreate application KSRecovCSV\nRECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW AgentFlow;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\ncreate source CSVSource using CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'customerdetails-recovery.csv',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO KafkaCsvStream;\n\nEND FLOW AgentFlow;\n\nCREATE FLOW ServerFlow;\nCREATE TYPE UserDataType\n(\n  UserId String KEY,\n  UserName String\n);\n\nCREATE STREAM UserDataStream OF UserDataType PARTITION BY UserId;\n\nCREATE CQ ParseUserData\nINSERT INTO UserDataStream\nSELECT  data[0],\n        data[1]\nFROM KafkaCsvStream;\n\nCREATE WACTIONSTORE UserActivityInfo\nCONTEXT OF UserDataType\nEVENT TYPES ( UserDataType )\n@PERSIST-TYPE@\n\n--get data from UserDataStream and place into wactionStore UserWaction\nCREATE CQ UserWaction\nINSERT INTO UserActivityInfo\nSELECT * FROM UserDataStream\nLINK SOURCE EVENT;\nEND FLOW ServerFlow;\n\nEND APPLICATION KSRecovCSV;\nDEPLOY APPLICATION KSRecovCSV with AgentFlow in AGENTS, ServerFlow on any in default;\nSTART KSRecovCSV;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\nCREATE APPLICATION DSV;\n\nCREATE FLOW HTTPsource;\n\nCREATE SOURCE HTTPSOURCE USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'http.txt',\n  skipbom: true,\n  rolloverpolicy: 'DefaultFileComparator',\n  blocksize: '64',\n  charset: 'UTF-8',\n  positionbyeof: false\n )\n PARSE USING DSVParser (\n  trimquote: true,\n  columndelimiter: ' ',\n  rowdelimiter: '\\n',\n  header: false,\n  quoteset: '{}'\n )\nOUTPUT TO RawHTTPStream;\n\nEND FLOW HTTPSource;\n\nCREATE FLOW main;\n\nCREATE TYPE HTTPLogEntry (\n     start_time String,\n     srcIp      String KEY,\n     port       String,\n     method     String,\n     url        String,\n     error_num  String,\n     status     String,\n     end_time   String,\n     host       String\n );\n\nCREATE STREAM HTTPStream OF HTTPLogEntry;\n\nCREATE CQ ParseHTTPLog\n  INSERT INTO HTTPStream\n  SELECT  MATCH(data[1],'start\\\\s+(\\\\w+.\\\\w+)'),\n          matchIP(data[2]),\n          MATCH(data[3],'port\\\\W+(\\\\w+)'),\n          MATCH(data[4],'method\\\\W+(\\\\w+)'),\n          data[5],\n          data[7],\n          data[8],\n          data[9],\n          data[10]\n  FROM RawHTTPStream;\n\ncreate Target t using FileWriter(\n  filename:'XmlTrimQuote',\n  sequence:'00',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:10s,sequence:00'\n)\nformat using DSVFormatter (\n\n)\ninput from HTTPStream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TrimQuoteTest_actual.log') input from HTTPStream;\n\nEND FLOW main;\n\nEND APPLICATION DSV;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n\nCREATE SOURCE @APPNAME@_S USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.test01'\n ) \nOUTPUT TO @APPNAME@_SS;\n\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'public.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12)',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:10',\nStandardSQL:true\t\n) INPUT FROM @APPNAME@_ss;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE FLOW @APPNAME@AgentFlow;\nCREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()\nPARSE USING Global.DSVParser ()\nOUTPUT TO @APPNAME@_Stream;\nEND FLOW @APPNAME@AgentFlow;\n\nCREATE FLOW @APPNAME@serverFlow;\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()\nFORMAT USING DSVFormatter (\nmembers:'data')\nINPUT FROM @APPNAME@_Stream;\nEND FLOW @APPNAME@serverFlow;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@ with @APPNAME@AgentFlow in Agents, @APPNAME@ServerFlow in default;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application XmlFormatterTester.DSV;\nundeploy application XmlFormatterTester.DSV;\ndrop application XmlFormatterTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSVSmallPosDataSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'smallposdata.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO CsvSmallPosDataStream;\n\ncreate source CSVPosDataSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'posdata.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO CsvPosDataStream;\n\n\nCreate Type CSVPosDataType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVSmallPosDataStream of CSVPosDataType;\nCreate Stream TypedCSVPosDataStream of CSVPosDataType;\n\n\nCREATE CQ CsvToSmallPosData\nINSERT INTO TypedCSVSmallPosDataStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvSmallPosDataStream;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVPosDataStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvPosDataStream;\n\n/**\n*  3.2.1.b FileWriter XML TimeInterval\n**/\ncreate Target XmlFormatterTimeInterval using FileWriter(\n  filename:'TargetPosDataXmlTIOpt',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00'\n)\nformat using XMLFormatter (\n  rootelement:'document',\n  elementtuple:'MerchantName:merchantid:text=merchantname',\n  charset:'UTF-8'\n)\ninput from TypedCSVSmallPosDataStream;\n\n/**\n* 3.2.1.c FileWriter XMLFileSize 101MB\n**/\ncreate Target XmlFormatterFileSize101 using FileWriter(\n  filename:'TargetPosDataXmlFS',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'FileSizeRollingPolicy,filesize:101M,sequence:00'\n)\nformat using XMLFormatter (\n  rootelement:'document',\n  elementtuple:'MerchantName:merchantid:text=merchantname',\n  charset:'UTF-8'\n)\ninput from TypedCSVPosDataStream;\n\n/**\n* 3.2.1.d FileWriter XMLDefaultFS 10 MB\n**/\ncreate Target XmlFormatterDefault using FileWriter(\n  filename:'TargetPosDataXmlFSDefault',\n  directory:'@FEATURE-DIR@/logs/',\n    sequence:'00',\n  rolloverpolicy:'FileSizeRollingPolicy'\n)\nformat using XMLFormatter (\n  rootelement:'document',\n  elementtuple:'MerchantName:zip:text=merchantname',\n  charset:'UTF-8'\n)\ninput from TypedCSVSmallPosDataStream;\n\n\n/**\n* 3.2.1.e FileWriter XML EventCount 2000\n**/\ncreate Target XmlFormatterEventCount2000 using FileWriter(\n  filename:'TargetPosDataXmlEC',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'eventcount:2000,sequence:00'\n)\nformat using XMLFormatter (\n  rootelement:'document',\n  elementtuple:'MerchantName:zip:text=merchantname',\n  charset:'UTF-8'\n)\ninput from TypedCSVSmallPosDataStream;\n\ncreate Target LogWriterSmallPosData using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlDFS_actual.log') input from TypedCSVSmallPosDataStream;\n\nend application DSV;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 12 with two sources, two jumping attribute windows, one wactionstore with recovery, and another wactionstore without -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Ja5W/p -> CQ1 -> WS\n-- S2 -> Ja6W/p -> CQ2 -> WS\n-- S2 -> Ja6W/p -> CQ2 -> WS2 (no persists)\n--\n\nSTOP Recov12Tester.RecovTest12;\nUNDEPLOY APPLICATION Recov12Tester.RecovTest12;\nDROP APPLICATION Recov12Tester.RecovTest12 CASCADE;\nCREATE APPLICATION RecovTest12 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE WactionsNoPersist CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n\t\tPERSIST NONE USING ( ) ;\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWactionNoPersist\nINSERT INTO WactionsNoPersist\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nEND APPLICATION RecovTest12;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "undeploy application dev15823;\nalter application dev15823;\n\nCREATE TYPE ModifyNotNull (\n  x int,\n  y string\n);\n\nCREATE STREAM FormattedStream OF ModifyNotNull;\n\nCREATE  CQ InsertWactions\nINSERT INTO FormattedStream\nSELECT\n    TO_INT(data[0]),\n   \t\"notnull\"\nFROM LogminerStream;\n\nCreate or replace Target test using SysOut (name:test) input from FormattedStream;\n\nCREATE OR REPLACE TARGET WriteCDCMySQL USING DatabaseWriter  ( \n  Username: '@USERNAME@',\n  BatchPolicy: 'Eventcount:5,Interval:5',\n  CommitPolicy: 'Eventcount:5,Interval:5',\n  ConnectionURL: '@URL@',\n  Tables: '@TABLES@',\n  Checkpointtable: 'CHKPOINT',\n  --IgnorableExceptionCode:'1062',\n  Password: '@PASSWORD@'\n ) \nINPUT FROM FormattedStream;\n\nEND APPLICATION dev15823;\nalter application dev15823 recompile;\ndeploy application dev15823;\nstart dev15823;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\n\nCREATE  APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE @srcName@ USING Global.DatabaseReader (\n  ConnectionURL: '@srcurl@', \n  Tables: '@srcschema@.@srctable@',\n  ReturnDateTimeAs: 'String', \n  FetchSize: 30000, \n  Username: '@srcusername@', \n  QuiesceOnILCompletion: true, \n  Password: '@srcpassword@', \n  DatabaseProviderType: 'DEFAULT' ) \nOUTPUT TO @outstreamname@;\n\nCREATE TARGET @tgtName@  USING Global.DatabaseWriter ( \n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', \n  CheckPointTable: 'CHKPOINT',\n  Username:'@tgtusername@', \n  Password:'@tgtpassword@', \n  CDDLAction: 'Process', \n  StatementCacheSize: '50', \n  CommitPolicy: 'EventCount:30000,Interval:60', \n  ConnectionURL:'@tgturl@',\n  DatabaseProviderType: 'Default', \n  PreserveSourceTransactionBoundary: 'false', \n  BatchPolicy: 'EventCount:30000,Interval:60', \n  Tables:'@srcschema@.@srctable@,@tgtschema@.@tgttable@', \n  adapterName: 'DatabaseWriter' ) \nINPUT FROM @instreamname@;\n\nEnd APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 1\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> WS\n--\n\nSTOP Recov1Tester.RecovTest1;\nUNDEPLOY APPLICATION Recov1Tester.RecovTest1;\nDROP APPLICATION Recov1Tester.RecovTest1 CASCADE;\nCREATE APPLICATION RecovTest1 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE WactionType (\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions\nINSERT INTO Wactions\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nEND APPLICATION RecovTest1;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@;\nCREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: '@ORACLE-URL@',\n  Tables: '@SOURCE-TABLES@',\n  Username: '@ORACLE-USERNAME@',\n  Password: '@ORACLE-PASSWORD@'\n) OUTPUT TO @APP_NAME@_Stream;\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt USING SnowflakeWriter\n\n(\n  ConnectionURL:'@SNOWFLAKE-URL@',\n  username:'@SNOWFLAKE-USERNAME@',\n  appendOnly:'false',\n  Tables:'@TARGET-TABLES@',\n  uploadpolicy:'eventcount:3,interval:10s',\n  externalStageType:'local'\n)\nINPUT FROM @APP_NAME@_Stream;\n\n\nEND APPLICATION @APP_NAME@;\nDEPLOY APPLICATION @APP_NAME@;\nSTART APPLICATION @APP_NAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Canon Test W80\n-- Nicholas Keene, WebAction, Inc.\n--\n-- Basic test for a partitioned jumping attribute window\n--\n-- S -> JWa5p -> CQ -> WS\n--\n\n\nUNDEPLOY APPLICATION NameW80.W80;\nDROP APPLICATION NameW80.W80 CASCADE;\nCREATE APPLICATION W80 RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE FLOW DataAcquisitionW80;\n\nCREATE SOURCE CsvSourceW80 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'Canon1000.csv',\n  columndelimiter:',',\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStreamW80;\n\nEND FLOW DataAcquisitionW80;\n\n\n\nCREATE FLOW DataProcessingW80;\n\nCREATE TYPE DataTypeW80 (\n    eventID String,\n    word String KEY,\n    dateTime DateTime\n);\n\nCREATE STREAM DataStreamW80 OF DataTypeW80;\n\nCREATE CQ CSVStreamW80_to_DataStreamW80\nINSERT INTO DataStreamW80\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[2],'yyyyMMddHHmmss')\nFROM CsvStreamW80;\n\nCREATE JUMPING WINDOW JWa5pW80\nOVER DataStreamW80\nKEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY word;\n\nCREATE WACTIONSTORE WactionStoreW80 CONTEXT OF DataTypeW80\nEVENT TYPES ( DataTypeW80 KEY(word) )\n@PERSIST-TYPE@\n\nCREATE CQ JWa5pW80_to_WactionStoreW80\nINSERT INTO WactionStoreW80\nSELECT\n    FIRST(eventID),\n    FIRST(word),\n    FIRST(dateTime)\nFROM JWa5pW80\nGROUP BY word;\n\nEND FLOW DataProcessingW80;\n\n\n\nEND APPLICATION W80;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application AlterTester.DSV;\nundeploy application AlterTester.DSV;\ndrop application AlterTester.DSV cascade;\n\n\ncreate application DSV;\ncreate source CSVSource using FileReader (\ndirectory:'@TEST-DATA-PATH@',\nWildCard:'smallposdata.csv',\npositionByEOF:false,\ncharset:'UTF-8'\n)\nparse using DSVParser (\nheader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\n\nend application DSV;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application ADW;\nundeploy application ADW;\ndrop application ADW cascade;\nCREATE APPLICATION ADW;\n\nCREATE  SOURCE OracleInitialLoad USING DatabaseReader  \n (\n Username:'src_username',\n Password:'src_password',\n ConnectionURL: 'src_url',\n Tables:'@SOURCE-TABLES@',\n FetchSize:2000\n) \nOUTPUT TO InitialLoadStream;\n\nCREATE TARGET AzureDWInitialLoad USING AzureSQLDWHWriter(\nConnectionURL: '@SQLDW-URL@',\n        username: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@',\n        uploadpolicy:'@EVENT-COUNT@'\n)\nINPUT FROM InitialLoadStream;\n\nEND APPLICATION ADW;\ndeploy application ADW;\nstart application ADW;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 3 on four node all server cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> JW -> CQ(aggregate) -> WS\n--\n\nSTOP APPLICATION N4S4CR3Tester.N4S4CRTest3;\nUNDEPLOY APPLICATION N4S4CR3Tester.N4S4CRTest3;\nDROP APPLICATION N4S4CR3Tester.N4S4CRTest3 CASCADE;\nCREATE APPLICATION N4S4CRTest3 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN4S4CRTest3;\n\nCREATE SOURCE CsvSourceN4S4CRTest3 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN4S4CRTest3;\n\nCREATE FLOW DataProcessingN4S4CRTest3;\n\nCREATE TYPE WactionTypeN4S4CRTest3 (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount int,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionTypeN4S4CRTest3;\n\nCREATE CQ CsvToDataN4S4CRTest3\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_INT(TO_DOUBLE(data[7])),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsN4S4CRTest3 CONTEXT OF WactionTypeN4S4CRTest3\nEVENT TYPES ( WactionTypeN4S4CRTest3 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsN4S4CRTest3\nINSERT INTO WactionsN4S4CRTest3\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.merchantId),\n    FIRST(p.dateTime),\n    SUM(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nEND FLOW DataProcessingN4S4CRTest3;\n\nEND APPLICATION N4S4CRTest3;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@;\n\nCREATE OR REPLACE SOURCE CCBReader USING FileReader (\n  wildcard: '@WILDCARD@',\n  positionbyeof: false,\n  directory: '@TESTDIR@'\n  )\nPARSE USING Global.CobolCopybookParser (\n  copybookDialect: 'Mainframe',\n  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',\n  GroupPolicy: '',\n  TimeoutPolicy: '1s',\n  dataFileOrganization: 'FixedLength',\n  ProcessCopyBookFileAs: 'SingleEvent',\n  skipIndent: 0,\n  dataFileFont: 'utf8',\n  CopybookFileFormat: 'USE_STANDARD_COLUMNS',\n  copybookSplit: 'None',\n  copybookFileName: '@CCBFILE@'\n   )\nOUTPUT TO CCBStream;\n\nCREATE OR REPLACE TARGET JSONWriter USING FileWriter (\n  filename: '%@metadata(FileName)%',\n  directory: '@DIR@',\n  rolloverpolicy: 'EventCount:10000,Interval:30s',\n  flushpolicy: 'EventCount:1,Interval:30s'\n  )\nFORMAT USING JSONFormatter  ()\nINPUT FROM CCBStream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@ on all in default;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop OracleToKudu;\nundeploy application OracleToKudu;\ndrop application OracleToKudu cascade;\n\nCREATE APPLICATION OracleToKudu;\nCreate Source oracSource\n Using OracleReader\n(\n Username:'@LOGMINER-UNAME@',\n Password:'@LOGMINER-PASSWORD@',\n ConnectionURL:'@LOGMINER-URL@',\n Tables:'@SOURCE_TABLES@',\n OnlineCatalog:true,\n FetchSize:1\n) Output To DataStream;\nCREATE TARGET WriteintoKudu using KuduWriter (\nkuduclientconfig:'',\npkupdatehandlingmode:'@MODE@',\ntables: '@TARGET_TABLES@',\nbatchpolicy: 'EventCount:1,Interval:0')\nINPUT FROM DataStream;\n\nEND APPLICATION OracleToKudu;\ndeploy application OracleToKudu in default;\nstart OracleToKudu;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE EMBEDDINGGENERATOR @EMB_NAME@ USING @MODEL@ (\nmodelProvider: '@MODEL@',\nmodelName: '@MODEL_NAME@',\napiKey: '@API_KEY@'\n);\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP WStester.WSusingApp;\nUNDEPLOY APPLICATION WStester.WSusingApp;\nDROP APPLICATION WStester.WSusingApp CASCADE;\n\nCREATE APPLICATION WSusingApp;\n\nCREATE TYPE Atm(\n  productID String KEY,\n  stateID String,\n  productWeight int,\n  quantity int,\n  size int);\n\nCREATE SOURCE liveSource using StreamReader(\n  OutputType: 'WStester.Atm',\n  noLimit: 'false',\n  maxRows: 20,\n  iterations: 0,\n  iterationDelay: 100,\n  StringSet: 'productID[001-002-003-004],stateID[AS-CA-WA-NY]',\n  NumberSet: 'productWeight[3-3]R,quantity[20-20]R,size[250-250]R'\n  )OUTPUT TO CsvStream;\n\n\nCREATE STREAM newStream OF Atm;\n\nCREATE CQ newCQ\nINSERT INTO newStream\nSELECT data[0], data[1], TO_INT(data[2]), TO_INT(data[3]), TO_INT(data[4]) FROM\nCsvStream;\n\ncreate target myOut8 using SysOut(name : testOut8) input from newStream;\n\nCREATE WACTIONSTORE streamActivityMEM CONTEXT OF Atm\nEVENT TYPES ( Atm )\nUSING MEMORY;\n\nCREATE WACTIONSTORE streamActivityES CONTEXT OF Atm\nEVENT TYPES ( Atm );\n\nCREATE WACTIONSTORE streamActivityES2 CONTEXT OF Atm\nEVENT TYPES ( Atm )\nUSING ( storageProvider: 'elasticsearch' );\n\nCREATE WACTIONSTORE streamActivityDerby CONTEXT OF Atm\nEVENT TYPES ( Atm )\nUSING (\nstorageProvider:'jdbc',\npersistence_interval:'10 sec',\nJDBC_DRIVER:'@WASTORE-DRIVER@',\nJDBC_URL:'@WASTORE-URL@',\nJDBC_USER:'@WASTORE-UNAME@',\nJDBC_PASSWORD:'@WASTORE-PASSWORD@',\nDDL_GENERATION:'drop-and-create-tables',\nLOGGING_LEVEL:'SEVERE');\n\nCREATE CQ newCQ2\nINSERT INTO streamActivityMEM\nSELECT * FROM newStream\nlink source event;\n\nCREATE CQ newCQ3\nINSERT INTO streamActivityES\nSELECT * FROM newStream\nlink source event;\n\nCREATE CQ newCQ4\nINSERT INTO streamActivityES2\nSELECT * FROM newStream\nlink source event;\n\nCREATE CQ newCQ5\nINSERT INTO streamActivityDerby\nSELECT * FROM newStream\nlink source event;\n\nEND APPLICATION WSusingApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');\n\nCREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.JsonNodeEvent PERSIST USING @APPNAME@KafkaPropset;\n\nCREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (\n  ProviderName: '',\n  Provider: '',\n  Ctx: '',\n  QueueName: '',\n  Topic:'',\n  UserName: '',\n  Password: '',\n  EnableTransaction: '',\n  transactionpolicy: ''\n )\nPARSE USING JSONParser ()\nOUTPUT TO @APPNAME@PersistStream@RANDOM@;\n\nCREATE OR REPLACE TARGET @APPNAME@_filetrgt USING FileWriter (\n  filename: '',\n  flushpolicy: '',\n  directory: '',\n  rolloverpolicy: '' )\nFORMAT USING JSONFormatter  (\n  members: 'data' )\nINPUT FROM @APPNAME@PersistStream@RANDOM@;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE TARGET @TARGET_NAME@ using KuduWriter (\nkuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',\npkuPDaTehAnDliNgmOdE:'DELETEANDINSERT',\ntables: 'QATEST.ORCLALLDATATYPES,@TARGET_TABLE@ ColumnMap(Log_Id=LOG_ID,Description=Description,LogChar=LogChar,LogVarchar2=LogVarchar2,LogNumber=LogNumber,LogBinaryFloat=LogBinaryFloat,LogBinaryDouble=LogBinaryDouble,LogFloat=LogFloat,LogDate=LogDate,LogTimestamp=LogTimestamp,LogNchar=LogNchar,LogNvarchar2=LogNvarchar2,LogTimezone=LogTimezone,LogTimelocal=LogTimelocal,LogInterval=LogInterval,LogInterval2=LogInterval2,LogInteger=LogInteger)',\nbatchpolicy: 'EventCount:1,Interval:0')\nINPUT FROM @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n--create flow AgentFlow;\nCREATE SOURCE @APPNAME@_S USING MySqlReader\n(\n  Compression: false,\n  FetchSize: 1,\n  SendBeforeImage: true,\n  ConnectionURL: 'mysql://localhost:3306',\n  DatabaseName: 'waction',\n  Tables: 'waction.test01',\n  Password: 'w@ct10n',\n  Password_encrypted: 'false',\n  Username: 'root',\n  connectionRetryPolicy:'retryInterval=1,maxRetries=3'\n)\nOUTPUT TO @APPNAME@_SS;\n--end flow AgentFlow;\n--create flow serverFlow;\n--create target sys_out using sysout (name:sysoutput) input from ss;\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'waction.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:0',\nStandardSQL:true\t\n) INPUT FROM @APPNAME@_ss;\n--end flow serverFlow;\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\n--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "list servers;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@;\n\nCREATE OR REPLACE TYPE @appname@CQOUT1_Type (\n companyName java.lang.String,\n merchantId java.lang.String,\n dateTime org.joda.time.DateTime,\n hourValue java.lang.String,\n amount java.lang.String,\n zip java.lang.String,\n FileName java.lang.String);\n\nCREATE SOURCE @parquetsrc@ USING S3Reader (\n    bucketname:'',\n    objectnameprefix:''\n     )\nPARSE USING ParquetParser (\n)\nOUTPUT TO @appname@Stream;\n\nCREATE OR REPLACE STREAM @appname@CQOUT1 OF @appname@CQOUT1_Type;\nCREATE OR REPLACE CQ @appname@CQ_PQEvent\nINSERT INTO @appname@CQOUT1\n    Select\n    data.get(\"companyName\").toString(),\n    data.get(\"merchantId\").toString(),\n    TO_DATE(data.get(\"dateTime\").toString()),\n    data.get(\"hourValue\").toString(),\n    data.get(\"amount\").toString(),\n    data.get(\"zip\").toString(),\n    metadata.get(\"FileName\").toString()\n    FROM @appname@Stream p;\n\ncreate Target @s3target@ using S3Writer(\n    bucketname:'',\n    objectname:'',\n    uploadpolicy:'',\n    foldername:''\n)\nformat using AvroFormatter (\nschemaFileName: 'AvroS3Schema'\n)\ninput from @appname@CQOUT1;\n\ncreate Target @blobtarget@ using AzureBlobWriter(\n\taccountname:'',\n\taccountaccesskey:'',\n\tcontainername:'',\n    blobname:'',\n\tfoldername:'',\n\tuploadpolicy:'EventCount:10,interval:5s'\n)\nformat using JSONFormatter ()\nINPUT FROM @appname@CQOUT1;\n\n\nCREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (\n    bucketname:'',\n    objectname:'',\n    foldername:'',\n    projectId:'',\n    uploadPolicy:''\n)\nformat using JSONFormatter (\n)\nINPUT FROM @appname@CQOUT1;\n\nCREATE OR REPLACE TARGET @dbtarget@ USING DatabaseWriter (\n  Tables: '',\n  ConnectionURL:'',\n  Username:'',\n  Password:'',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  BatchPolicy:'EventCount:1,Interval:0'\n)\nINPUT FROM @appname@CQOUT1;\n\nCREATE TARGET @bqtarget@ USING BigQueryWriter (\n  Tables: '',\n  projectId:'',\n  BatchPolicy: 'eventCount:1, Interval:1',\n  ServiceAccountKey: '',\n   )\nINPUT FROM @appname@CQOUT1;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create flow AgentFlow;\n\nCREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (\n  StartPosition: '@startPosition@',\n  ConnectionURL: '@sourceURL@',\n  Username: '@userName@',\n  Tables: '@tables@',\n  CheckColumn: '@checkColum@',\n  FetchSize: 1,\n  Password: '@password@',\n  DatabaseProviderType: 'Default',\n  ThreadPoolSize: 5,\n  pollingInterval: '2sec',\n  ConnectionPoolSize: 1 )\nOUTPUT TO @STREAM@;\n\nend flow AgentFlow;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION oraddl;\nUNDEPLOY APPLICATION oraddl;\nDROP APPLICATION oraddl CASCADE;\nCREATE APPLICATION oraddl recovery 5 second interval;\n \nCreate Source Ora Using OracleReader \n(\n Username:'@user-name@',\n Password:'@password@',\n ConnectionURL:'src_url',\n Tables:'QATEST.ORACLEDDL%',\n DictionaryMode:OfflineCatalog,\n DDLCaptureMode : 'All',\n FetchSize:1\n) Output To LogminerStream;\n\nCreate Target tgt using DatabaseWriter \n(\n Username:'@username@',\n Password:'@password@',\n ConnectionURL:'TGT_URL',\n BatchPolicy:'EventCount:1,Interval:1',\n CommitPolicy:'EventCount:1,Interval:1',\n IgnorableExceptionCode: '1,2290,942',\n Tables :'QATEST.ORACLEDDL%,QATEST2.%'\n) input from LogminerStream;\n\nCREATE TARGET cdcDump USING LogWriter(\nname:testOuput,\ndirectory:'/Users/abinandan/product/IntegrationTests/target/test-classes/testNG/AllTargetWriters/OracleDDLDatabaseWriter/logs',\nfilename:'oraclecdc.log',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING JSONFormatter ()\nINPUT FROM LogminerStream;\nend application oraddl;\ndeploy application oraddl;\nstart application oraddl;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop FileReaderToKuduWriter;\nundeploy application FileReaderToKuduWriter;\ndrop application FileReaderToKuduWriter cascade;\n\nCREATE APPLICATION FileReaderToKuduWriter recovery 5 second interval ;;\n\nCREATE OR REPLACE SOURCE CSVPoller USING FileReader (\n        directory:'/Users/Striim/',\n        WildCard:'typetest.csv',\n        positionByEOF:false\n)\nparse using DSVParser (\n        header:'yes'\n)\nOUTPUT TO CsvStream;\n\nCREATE OR REPLACE TYPE CSVStream_Type  ( ID1 String KEY,\nID2 String\n);\n\nCREATE OR REPLACE STREAM CSVTypeStream OF CSVStream_Type;\n\nCREATE OR REPLACE CQ CQ1\nINSERT INTO CSVTypeStream\nSELECT TO_STRING(data[0]),TO_STRING(data[1])\nFROM CsvStream;\n\nCREATE TARGET WriteintoKudu using KuduWriter (\nKuduClientConfig:'master.addresses->192.168.56.101:7051;socketreadtimeout->10000;operationtimeout->30000',\nTables: 'INTEGRATIONTEST',\nBatchPolicy: 'EventCount:1,Interval:10') INPUT FROM CSVTypeStream;\n\nEND APPLICATION FileReaderToKuduWriter;\ndeploy application FileReaderToKuduWriter;\nstart FileReaderToKuduWriter;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (\n  ConnectionURL: @sourceURL@,\n  Username: '@userName',\n  Tables: '@tables@',\n  CheckColumn: '@checkColumn',\n  FetchSize: 1,\n  ReturnDateTimeAs: 'JODA',\n  Password: '@password',\n  Password_encrypted: 'false',\n  DatabaseProviderType: 'Default',\n  ThreadPoolSize: 5,\n  pollingInterval: '2sec',\n  ConnectionPoolSize: 1 )\nOUTPUT TO @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop Oracle_IRLogWriter;\nUndeploy application Oracle_IRLogWriter;\ndrop application Oracle_IRLogWriter cascade;\n\nCREATE APPLICATION Oracle_IRLogWriter WITH ENCRYPTION recovery 5 second interval;\n\nCREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( \n  FetchSize: 1,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.TDSOURCE',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.TEST01=ID;',\n  PollingInterval: '5sec',\n  ReturnDateTimeAs: 'String',\n  startPosition:'striim.test01=0'\n  )\n  OUTPUT TO data_stream;\n\n  CREATE OR REPLACE TARGET Oracle_IRSys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM data_stream;\n\nCREATE TARGET BinaryDump USING LogWriter(\n  name: 'TeraData',\n  filename:'TeraData.log'\n)INPUT FROM data_stream;\n\nEND APPLICATION Oracle_IRLogWriter;\n\ndeploy application Oracle_IRLogWriter in default;\n\nstart application Oracle_IRLogWriter;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW ;\n\n \n\nCREATE OR REPLACE SOURCE DBSource USING DatabaseReader  ( \n  Username: '@SOURCE_USER@',\n  Password_encrypted: false,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'DatabaseReader',\n  FetchSize: 100,\n  Password: '@SOURCE_PASS@'\n ) \nOUTPUT TO Oracle_ChangeDataStream;\n\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1000,Interval:0',\n  CommitPolicy: 'EventCount:1000,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n ) \nINPUT FROM Oracle_ChangeDataStream;\n\ncreate Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\nEND APPLICATION DBRTOCW;\ndeploy application DBRTOCW in default;\nstart DBRTOCW;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME1@;\nundeploy application @APPNAME1@;\nstop application @APPNAME2@;\nundeploy application @APPNAME2@;\ndrop application @APPNAME1@ cascade;\ndrop application @APPNAME2@ cascade;\n\n\nCREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\nCREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;\nCREATE OR REPLACE SOURCE @SourceName@ Using MysqlReader\n(\n   adapterName: MysqlReader,\n   CDDLAction: Process,\n   CDDLCapture: false,\n   Compression: false,\n   ConnectionURL: jdbc:mysql://localhost:3306/waction,\n   FilterTransactionBoundaries: true,\n   Password: ReaderPassword,\n   SendBeforeImage: true,\n   Tables: srcTable,\n   Username: ReaderUsername\n)OUTPUT TO @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME1@;\nDEPLOY APPLICATION @APPNAME1@;\nSTART APPLICATION @APPNAME1@;\n\nCREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  \n(\n  DatabaseProviderType:'Default',\n  CheckPointTable:'CHKPOINT',\n  PreserveSourceTransactionBoundary:'false',\n  Username:'qatest',\n  BatchPolicy:'EventCount:1',\n  CommitPolicy:'EventCount:1',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',\n  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',\n  Password:'qatest',\n  CDDLAction:'Process'\n) \nINPUT FROM @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME2@;\nDEPLOY APPLICATION @APPNAME2@;\nSTART APPLICATION @APPNAME2@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.TABLE_TEST_%,@DATASET@.%',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:10',\nStandardSQL:true\t\n) INPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@2 USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.TABLE_TEST_%,@DATASET@.%',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:10',\nStandardSQL:true\t\n) INPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@3 USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.TABLE_TEST_%,@DATASET@.%',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:10',\nStandardSQL:true\t\n) INPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@4 USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.TABLE_TEST_%,@DATASET@.%',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:10',\nStandardSQL:true\t\n) INPUT FROM @STREAM@;\n\nCREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(\n  name: 'CDDL',\n  filename:'WAEventDump.log'\n)INPUT FROM @STREAM@;;\n\ncreate Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop PatternMatchingTimer.CSV;\nundeploy application PatternMatchingTimer.CSV;\ndrop application PatternMatchingTimer.CSV cascade;\n\ncreate application CSV;\n\ncreate source CSVSource using CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'ctest.csv',\n  columndelimiter:',',\n  positionByEOF:false\n)\nOUTPUT TO CsvStream;\n\nCREATE CQ ParseUserData\nINSERT INTO UserDataStream\nSELECT  TO_INT(data[0]) as UserId,\n\t    TO_INT(data[1]) as temp1,\n        TO_DOUBLE(data[2]) as temp2,\n\t    TO_STRING(data[3]) as temp3\nFROM CsvStream;\n\n-- scenario 1.1 check pattern using timer within 10 seconds and wait\nCREATE CQ TypeConversionTimerCQ1\nINSERT INTO TypedStream1\nSELECT UserId as typeduserid,\n\tA.temp1 as typedtemp1,\n\tB.temp2 as typedtemp2,\n\tC.temp3 as typedtemp3\nfrom UserDataStream\nMATCH_PATTERN T A (W | B | C)\ndefine T = timer(interval 10 second),\nA = UserDataStream(temp1 >= 20), B= UserDataStream(temp2 < 30.40), C= UserDataStream(temp3 = 'Bret'), W = wait(T)\nPARTITION BY UserId;\n\n-- scenario 1.2 check pattern using timer within 20 seconds\nCREATE CQ TypeConversionTimerCQ2\nINSERT INTO TypedStream2\nSELECT UserId as typeduserid,\n\tA.temp1 as typedtemp1,\n\tB.temp2 as typedtemp2,\n\tC.temp3 as typedtemp3\nfrom UserDataStream\nMATCH_PATTERN T A C\ndefine T = timer(interval 20 second), B= UserDataStream(temp2 < 30.40), C= UserDataStream(temp3 = 'zalak'),\nA = UserDataStream(temp1 >= 20)\nPARTITION BY UserId;\n\n-- scenario 1.3 check pattern using timer within 5 seconds with between values\nCREATE CQ TypeConversionTimerCQ3\nINSERT INTO TypedStream3\nSELECT UserId as typeduserid,\n\t   A.temp1 as typedtemp1\nfrom UserDataStream\nMATCH_PATTERN T A\ndefine T = timer(interval 5 second),\nA = UserDataStream(temp1 between 10 and 40)\nPARTITION BY UserId;\n\n-- scenario 1.4 check pattern using timer which match no events\nCREATE CQ TypeConversionTimerCQ4\nINSERT INTO TypedStream4\nSELECT UserId as typeduserid\nfrom UserDataStream\nMATCH_PATTERN T W\ndefine T = timer(interval 50 second), W = wait(T)\nPARTITION BY UserId;\n\n-- scenario 1.5 check pattern using stop timer\nCREATE CQ TypeConversionTimerCQ5\nINSERT INTO TypedStream5\nSELECT UserId as typeduserid,\n       A.temp1 as typedtemp1,\n       B.temp2 as typedtemp2\nfrom UserDataStream\nMATCH_PATTERN T A C T2 B\ndefine\nT = timer(interval 50 second),\nA = UserDataStream(temp1 between 10 and 40),\nC = stoptimer(T),\nT2 = timer(interval 30 second),\nB = UserDataStream(temp2 >= 20)\nPARTITION BY UserId;\n\nCREATE WACTIONSTORE UserActivityInfoTimer1\nCONTEXT OF TypedStream1_Type\nEVENT TYPES ( TypedStream1_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfoTimer2\nCONTEXT OF TypedStream2_Type\nEVENT TYPES ( TypedStream2_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfoTimer3\nCONTEXT OF TypedStream3_Type\nEVENT TYPES ( TypedStream3_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfoTimer4\nCONTEXT OF TypedStream4_Type\nEVENT TYPES ( TypedStream4_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfoTimer5\nCONTEXT OF TypedStream5_Type\nEVENT TYPES ( TypedStream5_Type )\n@PERSIST-TYPE@\n\n--get data from UserDataStream and place into wactionStore UserWaction\nCREATE CQ UserWaction1\nINSERT INTO UserActivityInfoTimer1\nSELECT * FROM TypedStream1\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction2\nINSERT INTO UserActivityInfoTimer2\nSELECT * FROM TypedStream2\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction3\nINSERT INTO UserActivityInfoTimer3\nSELECT * FROM TypedStream3\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction4\nINSERT INTO UserActivityInfoTimer4\nSELECT * FROM TypedStream4\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction5\nINSERT INTO UserActivityInfoTimer5\nSELECT * FROM TypedStream5\nLINK SOURCE EVENT;\n\nend application CSV;\ndeploy application csv;\nstart csv;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @appname@;\nundeploy application @appname@;\nDROP APPLICATION @appname@ CASCADE;\nCREATE APPLICATION @appname@;\n\nCREATE SOURCE @appname@_src USING databaseReader  (\n  Username: '@@',\n  Password: '@@',\n  ConnectionURL: '@@',\n  Tables: '@@',\n  FetchSize: '100'\n )\nOUTPUT TO @appname@_ss;\n\nCREATE JUMPING WINDOW @appname@_win OVER @appname@_ss KEEP @winsize@ ROWS;\n\nCREATE TYPE @appname@_MapType\n    (   \n       id INTEGER,\n        name STRING,\n        city  STRING\n    );\n    \nCREATE EXTERNAL CACHE @appname@_cach (\n  AdapterName: 'DatabaseReader',\n    ConnectionURL: '@url@',\n    UserName: '@uname@',\n    Password: '@pwd@',\n   Table: '@tablename@',\n  FetchSize: 100,\n  Columns: 'id,name,city',\n  trimquote: false,\n  KeyToMap: '@key@'\n )\n OF @appname@_MapType;\n \nCREATE TYPE @appname@_MapTypenew\n    (   id_t            INTEGER,\n        name_t           STRING,\n        city_t            STRING,\n        id_c            INTEGER,\n        name_c            STRING,\n        city_c            STRING\n    );\n    \nCREATE STREAM @appname@_JoinedData OF @appname@_MapTypenew;\n\nCREATE CQ @appname@_JoinDataCQ\nINSERT INTO @appname@_JoinedData\nSELECT  TO_INT(f.data[0]),\n        TO_STRING(f.data[1]),\n        TO_STRING(f.data[2]),\n        z.id,\n        z.name,\n        z.city\nFROM @appname@_win f, @appname@_cach z\nwhere TO_INT(f.data[0]) = z.id\n@Ex@;\n\nCREATE TARGET @appname@_tgt USING DatabaseWriter\n(\n  ConnectionURL:'@@',\n  Username:'@@',\n  Password:'@@',\n  BatchPolicy:'Eventcount:10000,Interval:1',\n  CommitPolicy:'Interval:1,Eventcount:10000',\n  Tables:'@@'\n) \nINPUT FROM @appname@_JoinedData;\n\nEND APPLICATION @appname@;\ndeploy application @appname@;\nstart @appname@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME1@;\nundeploy application @APPNAME1@;\nstop application @APPNAME2@;\nundeploy application @APPNAME2@;\ndrop application @APPNAME1@ cascade;\ndrop application @APPNAME2@ cascade;\n\n\nCREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\nCREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;\nCREATE OR REPLACE SOURCE @SourceName@ Using MysqlReader\n(\n   adapterName: MysqlReader,\n   CDDLAction: Process,\n   CDDLCapture: false,\n   Compression: false,\n   ConnectionURL: jdbc:mysql://localhost:3306/waction,\n   FilterTransactionBoundaries: true,\n   Password: ReaderPassword,\n   SendBeforeImage: true,\n   Tables: waction.MultiMultiDownstream_src,\n   Username: ReaderUsername\n)OUTPUT TO @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME1@;\nDEPLOY APPLICATION @APPNAME1@;\nSTART APPLICATION @APPNAME1@;\n\nCREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  \n(\n  DatabaseProviderType:'Default',\n  CheckPointTable:'CHKPOINT',\n  PreserveSourceTransactionBoundary:'false',\n  Username:'qatest',\n  BatchPolicy:'EventCount:1',\n  CommitPolicy:'EventCount:1',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',\n  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',\n  Password:'qatest',\n  CDDLAction:'Process'\n) \nINPUT FROM @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME2@;\nDEPLOY APPLICATION @APPNAME2@;\nSTART APPLICATION @APPNAME2@;\n\nCREATE APPLICATION @APPNAME3@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE TARGET @targetName1@ USING DatabaseWriter  \n(\n  DatabaseProviderType:'Default',\n  CheckPointTable:'CHKPOINT',\n  PreserveSourceTransactionBoundary:'false',\n  Username:'qatest',\n  BatchPolicy:'EventCount:1',\n  CommitPolicy:'EventCount:1',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',\n  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',\n  Password:'qatest',\n  CDDLAction:'Process'\n) \nINPUT FROM @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME3@;\nDEPLOY APPLICATION @APPNAME3@;\nSTART APPLICATION @APPNAME3@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--spool on to '/Users/jeyaselvan/Product/spoolfile.log';\nselect * from Oracle12C_To_Oracle12CApp_ExceptionStore;\n--spool off;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 1 on Four node all server cluster \n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> WS\n--\n\nSTOP N4S4CR1Tester.N4S4CRTest1;\nUNDEPLOY APPLICATION N4S4CR1Tester.N4S4CRTest1;\nDROP APPLICATION N4S4CR1Tester.N4S4CRTest1 CASCADE;\nCREATE APPLICATION N4S4CRTest1 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN4S4CRTest1;\n\nCREATE SOURCE CsvSourceN4S4CRTest1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN4S4CRTest1;\n\nCREATE FLOW DataProcessingN4S4CRTest1;\n\nCREATE TYPE WactionTypeN4S4CRTest1 (\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE WactionsN4S4CRTest1 CONTEXT OF WactionTypeN4S4CRTest1\nEVENT TYPES ( WactionTypeN4S4CRTest1 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsN4S4CRTest1\nINSERT INTO WactionsN4S4CRTest1\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nEND FLOW DataProcessingN4S4CRTest1;\n\nEND APPLICATION N4S4CRTest1;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application KinesisTest \n RECOVERY 10 SECOND INTERVAL\n;\ncreate source CSVSource using FileReader (\n\tdirectory:'/home/dz/src/product/Samples/AppData',\n\tWildCard:'posdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  companyName String,\n  merID String,\n  primNum String,\n  posDataCode int,\n  dateTime String,\n  expDate String,\n  curCode String,\n  Amnt String,\n  termId String,\n  zip String,\n  city String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],data[2],TO_INT(data[3]),\n       data[4],data[5],data[6],\n       data[7],data[8],\n       data[9],data[10]\nFROM CsvStream;\n\ncreate or replace Target t using KinesisWriter (\n\tregionName:'TARGET_REGION',\n\tstreamName:'TARGET_STREAM'\n)\nformat using DSVFormatter (\n)\ninput from TypedCSVStream;\nend application KinesisTest;\ndeploy application KinesisTest in default;\nstart application KinesisTest;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "IMPORT static com.webaction.runtime.converters.DateConverter.*;\n\nUNDEPLOY APPLICATION admin.SQLMXReaderApp;\nDROP APPLICATION admin.SQLMXReaderApp cascade;\n\nCREATE APPLICATION SQLMXReaderApp;\ncreate source SQMXSource using HPNonStopSQLMXReader (\n\tportno:2020,\n\tipaddress:'10.10.196.122',\n\tName:'intg',\n\tAuditTrails:'parallel',\n\tAgentPortNo:8012,\n\tAgentIpAddress:'10.10.197.116', \n\tTables:'watest.wasch.sqlmxtest1;watest.wasch.sqlmxtest2') output to CDCStream,\n\tSQLMXMATStream MAP (table:'WATEST.WASCH.SQLMXTEST2');\n\n\nCREATE TYPE SQLMXTEST2Data(\n    C0 Integer,\n    C1 String,\n    C2 Short,\n    OPR String,\n    TABLENAME String,\n    AUXNAME String\n);\n\nCREATE STREAM SQLMXTEST2Stream OF SQLMXTEST2Data;\n\n\nCREATE JUMPING WINDOW SQLMXDataWindow\nOVER SQLMXTEST2Stream KEEP 4 ROWS\nPARTITION BY OPR;\n\n\nCREATE CQ ToSQLMXData\nINSERT INTO SQLMXTEST2Stream\nSELECT TO_INT(data[0]),\n\t   data[1],\n       TO_SHORT(data[2]),\n       META(x,\"OperationName\").toString(),\n       META(x, \"TableName\").toString(),\n       META(x,\"AuditTrailName\").toString()\nFROM CDCStream x\nWHERE not(META(x,\"OperationName\").toString() = \"BEGIN\") AND not(META(x,\"OperationName\").toString() = \"COMMIT\") AND not(META(x, \"TableName\").toString() is null) \nAND META(x, \"TableName\").toString() = \"WATEST.WASCH.SQLMXTEST1\" AND META(x, \"AuditTrailName\").toString() = \"MAT\";\n\n\n--CREATE TARGET SQLMXSYSOUT using SysOut(name:sqlmx) INPUT FROM CDCStream;\nCREATE TARGET SQLMXMAT USING LogWriter(\n  name:SQLMXReaderAppMAT,\nfilename:'@FEATURE-DIR@/logs/sqlmxmat.log'\n--  filename:'mat.log'\n) INPUT FROM SQLMXMATStream;\n\nCREATE TARGET SQLMXAUX01 USING LogWriter(\n  name:SQLMXReaderAppMAT1,\nfilename:'@FEATURE-DIR@/logs/sqlmxmat1.log'\n--  filename:'aux1.log'\n) INPUT FROM SQLMXTEST2Stream;\n\n\nEND APPLICATION SQLMXReaderApp;\ndeploy application SQLMXReaderApp in default;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\nCREATE APPLICATION @APPNAME@ WITH ENCRYPTION @Recovery@ USE EXCEPTIONSTORE;\ncreate flow @APPNAME@_SourceFlow;\nCREATE SOURCE @APPNAME@_s USING FileReader(\n  directory:'Samples/AppData',\n  wildcard:'PO.JSON',\n  positionByEOF:false\n)\nparse using JSONParser (\n) OUTPUT TO @APPNAME@_ss1;\nend flow @APPNAME@_SourceFlow;\ncreate target @APPNAME@_t using sysout (name:ss1) input from @APPNAME@_ss1;\nend application @APPNAME@;\ndeploy application @APPNAME@ @DP@;\nstart @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 38 with two sources, two jumping time-count windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jc5t3W/p -> CQ1 -> WS\n--   S2 -> Jc6t4W/p -> CQ2 -> WS\n--\n\nSTOP Recov38Tester.RecovTest38;\nUNDEPLOY APPLICATION Recov38Tester.RecovTest38;\nDROP APPLICATION Recov38Tester.RecovTest38 CASCADE;\nCREATE APPLICATION RecovTest38 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Rows3Seconds\nOVER DataStream1 KEEP 5 ROWS WITHIN 3 SECOND\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Rows4Seconds\nOVER DataStream2 KEEP 6 ROWS WITHIN 4 SECOND\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataStream5Rows3Seconds\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Rows3Seconds p\nGROUP BY p.merchantId;\n\nCREATE CQ DataStream6Rows4Seconds\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Rows4Seconds p\nGROUP BY p.merchantId;\n\nEND APPLICATION RecovTest38;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": " create or replace propertyvariable TEAMSREFTOKEN = '@refershtoken@';\n create or replace propertyvariable TEAMSCLIENTSECRET = '@clientSecret@';\n create or replace propertyvariable TEAMSCHANNELURL = '@channelUrl@';\n create or replace propertyvariable TEAMSCLIENTID = '@clientID@';\n\nCREATE APPLICATION @AppName@ WITH ENCRYPTION EXCEPTIONHANDLER (AdapterException: 'IGNORE', ArithmeticException: 'IGNORE', ClassCastException: 'IGNORE', ConnectionException: 'IGNORE', InvalidDataException: 'IGNORE', NullPointerException: 'IGNORE', NumberFormatException: 'IGNORE', SystemException: 'IGNORE', UnExpectedDDLException: 'IGNORE', UnknownException: 'IGNORE')  USE EXCEPTIONSTORE TTL : '7d' ;\n\nCREATE OR REPLACE SOURCE OracleReader_AlertSource USING oraclereader (\n  ConnectionURL: '@ConnectionURl@',\n  Tables: '@table@',\n  Password: '@password@',\n  Username: '@username@' )\nOUTPUT TO AlertUpgradeStream;\n\nCREATE STREAM AlertStream OF Global.AlertEvent;\n\nCREATE STREAM SlackAndTeamsStream OF Global.SlackAlertEvent;\n\nCREATE OR REPLACE TARGET SmartAlertUpgradeSysOut USING SysOut (\n  name: 'SmartAlertUpgradeSysOut' )\nINPUT FROM AlertUpgradeStream;\n\nCREATE OR REPLACE CQ GenerateAlertsForWebAndEmail\nINSERT INTO AlertStream\nSELECT data[1],\n data[1],\n 'info',\n 'raise',\n'Welcome\\n to Striim'\nFROM AlertUpgradeStream;\n\n\n CREATE OR REPLACE CQ GenerateAlertsForSlackAndTeams\n INSERT INTO SlackAndTeamsStream\n SELECT data[1],\n  data[1],\n 'info',\n 'raise',\n 'Welcome\\n to Striim',\n 'TestChannel'\n FROM AlertUpgradeStream;\n\n\nCREATE TARGET TeamsAlertSender USING Global.TeamsAlertAdapter (\n  refreshToken: '$TEAMSREFTOKEN',\n  clientSecret: '$TEAMSCLIENTSECRET',\n  channelURL: '$TEAMSCHANNELURL',\n  clientID: '$TEAMSCLIENTID' )\nINPUT FROM SlackAndTeamsStream;\n\n\nCREATE SUBSCRIPTION WebAlertSender USING WebAlertAdapter (\n  isSubscription: 'true',\n  channelName: 'admin_PosAppWebAlert' )\nINPUT FROM AlertStream;\n\n\nCREATE TARGET slackalertSender USING Global.SlackAlertAdapter (\n  OauthToken: '@oauth@',\n  ChannelName: '@slackChannel@',\n  OauthToken_encrypted: 'false' )\nINPUT FROM SlackAndTeamsStream;\n\nCREATE SUBSCRIPTION EmailAlertsender USING EmailAdapter (\n  smtpurl: '@smtpUrl@',\n  starttls_enable: '@starttls@',\n  SMTPUSER: '@smtpuser@',\n  SMTPPASSWORD: '@stmpPwsd@',\n  emailList: '@emailList@',\n  subject: '@subject@',\n  senderEmail: '@senderEmail@',\n  SMTPPASSWORD_encrypted: 'false')\nINPUT FROM AlertStream;\n\nEND APPLICATION @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use admin;\ndrop namespace @namespace@ cascade;\ncreate namespace @namespace@;\nuse @namespace@;\nCREATE APPLICATION @appName@;\ncreate flow @flowName@;\nCREATE SOURCE @appName@_s1 USING Global.FileReader (\n  wildcard: 'posdata100.csv',\n  blocksize: 64,\n  directory: '@TestDataDir@',\n  positionByEOF:false)\nPARSE USING Global.DSVParser ()\nOUTPUT TO @appName@_st1;\nend flow srcFlow;\ncreate target @appName@_t1 using NullWriter() input from @appName@_st1;\nEND APPLICATION @appName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "-- The PosApp sample application demonstrates how a credit card\n-- payment processor might use Striim to generate reports on current\n-- transaction activity by merchant and send alerts when transaction\n-- counts for a merchant are higher or lower than average for the time\n-- of day.\n\nstop admin.PosApp;\nundeploy application admin.PosApp;\ndrop application admin.PosApp cascade;\n\nCREATE APPLICATION PosApp;\n\n-- All CREATE statements between here and the END APPLICATION\n-- statement will create objects in the PosApp application.\n\n-- source CsvDataSource\n\nCREATE source CsvDataSource USING FileReader (\n  directory:'Samples/Customer/PosApp/appData',\n  wildcard:'$wildcard',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\n\n-- This is the primary data source for this application.\n-- In a real-world application, it would be real-time data. Here,\n-- the data comes from a comma-delimited file, posdata.csv. The first\n-- two lines of that file are:\n--\n-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY\n-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand\n--\n-- The \"header:Yes\" setting tells Striim that the first line contains\n-- field labels that should not be treated as data.\n--\n-- The \"positionByEOF:false\" setting tells Striim to start reading\n-- from the beginning of the file. (In a real-world application\n-- reading real log files, you would typically use the default \"true\"\n-- setting so that the application would read only new data.)\n--\n-- The OUTPUT TO clause automatically creates the stream\n-- CsvStream using the WAEvent type associated with the CSVReader\n-- adapater. The only field from WAEvent used by this application\n-- is \"data\", an array containing the delimited fields.\n\n\n-- CQ CsvToPosData\n\nCREATE CQ CsvToPosData\nINSERT INTO PosDataStream\nSELECT TO_STRING(data[1]) as merchantId,\n       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,\n       TO_DOUBLE(data[7]) as amount,\n       TO_STRING(data[9]) as zip\nFROM CsvStream;\n\n-- Here, \"data\" refers to the array mentioned above, and the number\n-- in brackets specifies a field from the array, counting from zero.\n-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH\n-- AMOUNT, and data[9] is ZIP.\n--\n-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as\n-- the types to be used in PosDataStream, which is created automatically.\n\n-- The DATETIME field from the source is converted to both a dateTime\n-- value, used as the event timestamp by the application, and (via the\n-- function) an integer hourValue, which is used to look up\n-- historical hourly averages from the HourlyAveLookup cache,\n-- discussed below.\n--\n-- The other six fields are discarded. Thus the first line of data\n-- from posdata.csv has at this point been reduced to five values:\n--\n-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)\n-- 20130312173210 (DateTime)\n-- 17 (hourValue)\n-- 2.20 (amount)\n-- 41363 (zip)\n\n\n-- CQ GenerateMerchantTxRateOnly\n--\n-- The PosData5Minutes window bounds the data so that the query can\n-- use the COUNT, FIRST, and SUM functions and join data from the\n-- HourlyAveLookup cache. (Aggregate functions cannot be used and\n-- joins cannot be performed on unbound real-time data.)\n--\n-- The HourlyAveLookup cache provides historical average sales\n-- amounts for the current hour for each merchant.\n\nCREATE JUMPING WINDOW PosData5Minutes\nOVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;\n\nCREATE TYPE MerchantHourlyAve(\n  merchantId String,\n  hourValue integer,\n  hourlyAve integer\n);\nCREATE CACHE HourlyAveLookup using FileReader (\n  directory: 'Samples/Customer/PosApp/appData',\n  wildcard: 'hourlyData.txt'\n)\nPARSE USING DSVParser (\n  header: Yes,\n  trimquote:false\n) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;\n\nCREATE TYPE MerchantTxRate(\n  merchantId String KEY,\n  zip String,\n  startingTime DateTime,\n  count integer,\n  totalAmount double,\n  hourlyAve integer,\n  upperLimit double,\n  lowerLimit double,\n  category String,\n  status String\n);\nCREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;\n\nCREATE CQ GenerateMerchantTxRateOnly\nINSERT INTO MerchantTxRateOnlyStream\nSELECT p.merchantId,\n       FIRST(p.zip),\n       FIRST(p.dateTime),\n       COUNT(p.merchantId),\n       SUM(p.amount),\n       FIRST(l.hourlyAve/12),\n       FIRST(l.hourlyAve/12 * CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       FIRST(l.hourlyAve/12 / CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       '<NOTSET>',\n       '<NOTSET>'\nFROM PosData5Minutes p, HourlyAveLookup l\nWHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue\nGROUP BY p.merchantId;\n\n-- This query aggregates five minutes' worth of data for each\n-- merchant, calculating the total transaction count and amount, and\n-- calculates the upperLimit and lowerLimit values based on the\n-- historical average transaction count for the current hour of the\n-- day from the HourlyAveLookup cache. The category and status fields\n-- are left unset to be populated by the next query.\n\n\n-- CQ GenerateMerchantTxRateWithStatus\n--\n-- This query sets the count values used by the Dashboard map and the\n-- status values used to trigger alerts.\n\nCREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;\n\nCREATE CQ GenerateMerchantTxRateWithStatus\nINSERT INTO MerchantTxRateWithStatusStream\nSELECT merchantId,\n       zip,\n       startingTime,\n       count,\n       totalAmount,\n       hourlyAve,\n       upperLimit,\n       lowerLimit,\n       CASE\n         WHEN count >10000 THEN 'HOT'\n         WHEN count > 800 THEN 'WARM'\n         WHEN count >200 THEN 'COOL'\n         ELSE 'COLD' END,\n       CASE\n         WHEN count > upperLimit THEN 'TOOHIGH'\n         WHEN count < lowerLimit THEN 'TOOLOW'\n         ELSE 'OK' END\nFROM MerchantTxRateOnlyStream;\n\n\n-- WAction store MerchantActivity\n--\n-- The following group of statements create and populate the MerchantActivity\n-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced\n-- with merchant details from NameLookup cache and with latitude and longitude\n-- values from the USAddressData cache.\n\nCREATE TYPE MerchantActivityContext(\n  MerchantId String KEY,\n  StartTime  DateTime,\n  CompanyName String,\n  Category String,\n  Status String,\n  Count integer,\n  HourlyAve integer,\n  UpperLimit double,\n  LowerLimit double,\n  Zip String,\n  City String,\n  State String,\n  LatVal double,\n  LongVal double\n);\n\nCREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext\nEVENT TYPES ( MerchantTxRate )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\n\nCREATE TYPE MerchantNameData(\n  merchantId String KEY,\n  companyName String\n);\n\nCREATE TYPE USAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\nCREATE CACHE NameLookup using FileReader (\n  directory:'Samples/Customer/PosApp/appData',\n  WildCard:'MerchantNames.csv',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:'yes',\n  trimquote:false\n)\nQUERY(keytomap:'merchantId') OF MerchantNameData;\n\nCREATE CACHE ZipLookup using FileReader (\n  directory: 'Samples/Customer/PosApp/appData',\n  wildcard: 'USAddresses.txt',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header: Yes,\n  columndelimiter: '\\t',\n  trimquote:false\n) QUERY (keytomap:'zip') OF USAddressData;\n\nCREATE CQ GenerateWactionContext\nINSERT INTO MerchantActivity\nSELECT  m.merchantId,\n        m.startingTime,\n        n.companyName,\n        m.category,\n        m.status,\n        m.count,\n        m.hourlyAve,\n        m.upperLimit,\n        m.lowerLimit,\n        m.zip,\n        z.city,\n        z.state,\n        z.latVal,\n        z.longVal\nFROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z\nWHERE m.merchantId = n.merchantId AND m.zip = z.zip\nLINK SOURCE EVENT;\n\n\n-- CQ GenerateAlerts\n--\n-- This CQ sends an alert when a merchant's status value changes to\n-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.\n\n\nCREATE STREAM AlertStream OF Global.AlertEvent;\n\nCREATE CQ GenerateAlerts\nINSERT INTO AlertStream\nSELECT n.CompanyName,\n       m.MerchantId,\n       CASE\n         WHEN m.Status = 'OK' THEN 'info'\n         ELSE 'warning' END,\n       CASE\n         WHEN m.Status = 'OK' THEN 'cancel'\n         ELSE 'raise' END,\n       CASE\n         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)\n         ELSE ''\n         END\nFROM MerchantTxRateWithStatusStream m, NameLookup n\nWHERE m.merchantId = n.merchantId;\n\n\n\n--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM AlertStream;\n\nCREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;\n\n\nEND APPLICATION PosApp;\n\n\nCREATE DASHBOARD USING \"Samples/Customer/PosApp/PosAppDashboard.json\";\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.OJet (\n  PrimaryDatabasePassword: '@PRIMARY_PASSWORD@',\n  ConnectionURL: '@DOWNSTREAM_URL@',\n  PrimaryDatabaseUsername: '@PRIMARY_USER@',\n  Password: '@DOWNSTREAM_PASSWORD@',\n  DownstreamCaptureMode: 'REAL_TIME',\n  DownstreamCapture: true,\n  PrimaryDatabaseConnectionURL: '@PRIMARY_URL@',\n  Tables: '@SOURCE_TABLES@',\n  Username: '@DOWNSTREAM_USER@'\n  )\nOUTPUT TO @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_SYS@ USING Global.SysOut (\n  name: 'Out' )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter (\n  ConnectionURL: '@TARGET_URL@',\n  Username: '@TARGET_USER@',\n  Password: '@TARGET_PASSWORD@',\n  CheckPointTable: 'CHKPOINT',\n  CommitPolicy: 'EventCount:1',\n  Tables: '@TARGET_TABLES@',\n  BatchPolicy: 'EventCount:1' )\nINPUT FROM @STREAM@;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 10 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> CW(p) -> CQ -> WS\n--\n\nSTOP Recov10Tester.RecovTest10;\nUNDEPLOY APPLICATION Recov10Tester.RecovTest10;\nDROP APPLICATION Recov10Tester.RecovTest10 CASCADE;\nCREATE APPLICATION RecovTest10 RECOVERY 1 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTest10Data.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE CsvData (\n  partKey String KEY,\n  serialNumber int\n);\n\nCREATE STREAM DataStream OF CsvData PARTITION BY partKey;\n\nCREATE CQ CsvToData\nINSERT INTO DataStream\nSELECT\n    data[0],\n    TO_INT(data[1])\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStreamTwoItems\nOVER DataStream KEEP 2 ROWS\nPARTITION BY partKey;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction\nINSERT INTO Wactions\nSELECT\n    first(partKey),\n    to_int(first(serialNumber))\nFROM DataStreamTwoItems\nGROUP BY partKey;\n\nEND APPLICATION RecovTest10;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 1 on two node cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> WS\n--\n\nSTOP APPLICATION N2S2CR1Tester.N2S2CRTest1;\nUNDEPLOY APPLICATION N2S2CR1Tester.N2S2CRTest1;\nDROP APPLICATION N2S2CR1Tester.N2S2CRTest1 CASCADE;\nCREATE APPLICATION N2S2CRTest1 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN2S2CRTest1;\n\nCREATE SOURCE CsvSourceN2S2CRTest1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN2S2CRTest1;\n\nCREATE FLOW DataProcessingN2S2CRTest1;\n\nCREATE TYPE WactionTypeN2S2CRTest1 (\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE WactionsN2S2CRTest1 CONTEXT OF WactionTypeN2S2CRTest1\nEVENT TYPES ( WactionTypeN2S2CRTest1 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsN2S2CRTest1\nINSERT INTO WactionsN2S2CRTest1\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nEND FLOW DataProcessingN2S2CRTest1;\n\nEND APPLICATION N2S2CRTest1;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@ RECOVERY 1 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE @SourceName@ USING PostgreSQLReader  ( \n ReplicationSlotName: 'striim_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.postgrestopostgres_src'\n ) \nOUTPUT TO @SRCINPUTSTREAM@ ;\n\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy:'EventCount:1000,Interval:60',\nCommitPolicy:'EventCount:1000,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM @SRCINPUTSTREAM@;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 20 with two sources going to one wactionstore\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> CQ1 -> WS\n-- S2 -> CQ2 -> WS\n--\n\nSTOP KStreamRecov20Tester.KStreamRecovTest20;\nUNDEPLOY APPLICATION KStreamRecov20Tester.KStreamRecovTest20;\nDROP APPLICATION KStreamRecov20Tester.KStreamRecovTest20 CASCADE;\nDROP USER KStreamRecov20Tester;\nDROP NAMESPACE KStreamRecov20Tester CASCADE;\nCREATE USER KStreamRecov20Tester IDENTIFIED BY KStreamRecov20Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov20Tester;\nCONNECT KStreamRecov20Tester KStreamRecov20Tester;\n\nCREATE APPLICATION KStreamRecovTest20 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE WactionType (\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions1\nINSERT INTO Wactions\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream1;\n\nCREATE CQ InsertWactions2\nINSERT INTO Wactions\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream2;\n\nEND APPLICATION KStreamRecovTest20;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE OR REPLACE SOURCE DBSource USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//192.168.56.101:1521/orcl',\n  Tables: 'QATEST.oracle_200',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\n\ncreate stream xferredDataStream1 of Global.WAEvent;\n\nCREATE CQ CQ1\ninsert into xferredDataStream1\nselect  putuserdata (data1,'ID', data[0]) from Oracle_ChangeDataStream data1;\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:4,Interval:60',\n  CommitPolicy: 'EventCount:4,Interval:60',\n  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',\n  IgnorableExceptionCode:'PRIMARY KEY',\n  Tables: 'QATEST.oracle_200,test.cassandra_200 columnmap(field1=field1,field2=field2,field3=field3,field4=field4,field5=field5,field6=field6,field7=field7,field8=field8,field9=field9,field10=field10,field11=field11,field12=field12,field13=field13,field14=field14,field15=field15,field16=field16,field17=field17,field18=field18,field19=field19,field20=field20,field21=field21,field22=field22,field23=field23,field24=field24,field25=field25,field26=field26,field27=field27,field28=field28,field29=field29,field30=field30,field31=field31,field32=field32,field33=field33,field34=field34,field35=field35,field36=field36,field37=field37,field38=field38,field39=field39,field40=field40,field41=field41,field42=field42,field43=field43,field44=field44,field45=field45,field46=field46,field47=field47,field48=field48,field49=field49,field50=field50,field51=field51,field52=field52,field53=field53,field54=field54,field55=field55,field56=field56,field57=field57,field58=field58,field59=field59,field60=field60,field61=field61,field62=field62,field63=field63,field64=field64,field65=field65,field66=field66,field67=field67,field68=field68,field69=field69,field70=field70,field71=field71,field72=field72,field73=field73,field74=field74,field75=field75,field76=field76,field77=field77,field78=field78,field79=field79,field80=field80,field81=field81,field82=field82,field83=field83,field84=field84,field85=field85,field86=field86,field87=field87,field88=field88,field89=field89,field90=field90,field91=field91,field92=field92,field93=field93,field94=field94,field95=field95,field96=field96,field97=field97,field98=field98,field99=field99,field100=field100,field101=field101,field102=field102,field103=field103,field104=field104,field105=field105,field106=field106,field107=field107,field108=field108,field109=field109,field110=field110,field111=field111,field112=field112,field113=field113,field114=field114,field115=field115,field116=field116,field117=field117,field118=field118,field119=field119,field120=field120,field121=field121,field122=field122,field123=field123,field124=field124,field125=field125,field126=field126,field127=field127,field128=field128,field129=field129,field130=field130,field131=field131,field132=field132,field133=field133,field134=field134,field135=field135,field136=field136,field137=field137,field138=field138,field139=field139,field140=field140,field141=field141,field142=field142,field143=field143,field144=field144,field145=field145,field146=field146,field147=field147,field148=field148,field149=field149,field150=field150,field151=@METADATA(OperationName),field152=field1,field153=@METADATA(SQLRedoLength),field154=@METADATA(SEQUENCE),field155=@METADATA(SegmentName),field156=@METADATA(OperationType),field157=@METADATA(TxnUserID),field158=@METADATA(ThreadID),field159=@METADATA(TxnUserID),field160=@USERDATA(ID),field161=@USERDATA(ID),field162=@USERDATA(ID),field163=@USERDATA(ID),field164=field3,field165=field150,field166=field151)',\n  Password: 'cassandra',\n  Password_encrypted: false\n )\nINPUT FROM xferredDataStream1;\n\ncreate Target t2 using SysOut(name:Foo2) input from xferredDataStream1;\n\nEND APPLICATION DBRTOCW;\n\ndeploy application DBRTOCW in  default;\n\nstart application DBRTOCW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application ps1;\nstop application ps2;\nundeploy application ps1;\nundeploy application ps2;\ndrop application ps1 cascade;\ndrop application ps2 cascade;\nCREATE application ps1 recovery 5 second interval;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',\nacks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\ncreate type type1(\n  companyName String,\n  merchantId String,\n  city string\n);\n\nCREATE STREAM kps1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM kps2 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM kps3 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM kps4 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM kps5_typedStream OF type1 partition by city persist using KafkaPropset;\n\ncreate source s using FileReader (\n        directory:'/Users/saranyad/Product/IntegrationTests/TestData',\n        wildcard:'posdata.csv',\n        positionByEOF:false,\n        Charset:'UTF-8'\n)\nPARSE USING DSVParser (\n  columndelimiter:',',\n  ignoreemptycolumn:'Yes',\n  quoteset:'[]~\"',\n  separator:'~'\n)\nOUTPUT TO kps;\n\nCREATE CQ cq1\nINSERT INTO kps1\nSELECT *\nFROM kps;\n\nCREATE CQ cq2\nINSERT INTO kps2\nSELECT *\nFROM kps;\n\nCREATE CQ cq3\nINSERT INTO kps3\nSELECT *\nFROM kps;\n\nCREATE CQ cq4\nINSERT INTO kps4\nSELECT *\nFROM kps;\n\n\nCREATE CQ cq5\nINSERT INTO kps5_typedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\") as companyName,\nTO_STRING(data[1]) as merchantId,\nTO_STRING(data[10]) as city\nFROM kps;\n\nend application ps1;\n\nCREATE application ps2 recovery 5 second interval;\n\ncreate type type2(\n  companyName String,\n  merchantId String,\n  city string\n);\n\nCREATE STREAM kps1_typedStream OF type2 partition by city;\nCREATE STREAM kps2_typedStream OF type2 partition by city;\nCREATE STREAM kps3_typedStream OF type2 partition by city;\nCREATE STREAM kps4_typedStream OF type2 partition by city;\n\nCREATE CQ cq6\nINSERT INTO kps1_typedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\") as companyName,\nTO_STRING(data[1]) as merchantId,\nTO_STRING(data[10]) as city\nFROM kps1;\n\nCREATE CQ cq7\nINSERT INTO kps2_typedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\") as companyName,\nTO_STRING(data[1]) as merchantId,\nTO_STRING(data[10]) as city\nFROM kps2;\n\nCREATE CQ cq8\nINSERT INTO kps3_typedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\") as companyName,\nTO_STRING(data[1]) as merchantId,\nTO_STRING(data[10]) as city\nFROM kps3;\n\nCREATE CQ cq9\nINSERT INTO kps4_typedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\") as companyName,\nTO_STRING(data[1]) as merchantId,\nTO_STRING(data[10]) as city\nFROM kps4;\n\nCREATE TARGET target1 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'QATEST.KPS1'\n) INPUT FROM kps1_typedStream;\n\nCREATE TARGET target2 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'QATEST.KPS2'\n) INPUT FROM kps2_typedStream;\n\nCREATE TARGET target3 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'QATEST.KPS3'\n) INPUT FROM kps3_typedStream;\n\nCREATE TARGET target4 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'QATEST.KPS4'\n) INPUT FROM kps4_typedStream;\n\nCREATE TARGET target5 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'QATEST.KPS5'\n) INPUT FROM kps5_typedStream;\n\nend application ps2;\n--DEPLOY APPLICATION KafkaWriterApp with sourceFlow  ON ANY IN DEFAULT, targetFlow ON ALL IN DEFAULT;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 34 with two sources, two sliding time-count windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Sc5a9W/p  -> CQ1 -> WS\n-- S2 -> Sc6a11W/p -> CQ2 -> WS\n--\n\nSTOP KStreamRecov3Tester.KStreamRecovTest34;\nUNDEPLOY APPLICATION KStreamRecov3Tester.KStreamRecovTest34;\nDROP APPLICATION KStreamRecov34Tester.KStreamRecovTest34 CASCADE;\n\nDROP USER KStreamRecov34Tester;\nDROP NAMESPACE KStreamRecov34Tester CASCADE;\nCREATE USER KStreamRecov34Tester IDENTIFIED BY KStreamRecov34Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov34Tester;\nCONNECT KStreamRecov34Tester KStreamRecov34Tester;\n\nCREATE APPLICATION KStreamRecovTest34 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE\nPARTITION BY merchantId;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1\nGROUP BY merchantId;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2\nGROUP BY merchantId;\n\nEND APPLICATION KStreamRecovTest34;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSV1Source using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'RFC4180.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'no'\n)\nOUTPUT TO Csv1Stream;\n\ncreate Target t using FileWriter(\n  filename:'FileWriterStandardRFC4180',\n  directory:'@FEATURE-DIR@/logs/',\n  standard : 'RFC4180',\n  rolloverpolicy:'EventCount:10000,Interval:30s'\n)\nformat using DSVFormatter (\nmembers:'data'\n)\ninput from Csv1Stream;\n\nend application DSV;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APPNAME@app1;\nSTOP APPLICATION @APPNAME@app2;\nSTOP APPLICATION @APPNAME@app3;\nSTOP APPLICATION @APPNAME@app4;\nSTOP APPLICATION @APPNAME@app5;\nUNDEPLOY APPLICATION @APPNAME@app1;\nUNDEPLOY APPLICATION @APPNAME@app2;\nUNDEPLOY APPLICATION @APPNAME@app3;\nUNDEPLOY APPLICATION @APPNAME@app4;\nUNDEPLOY APPLICATION @APPNAME@app5;\nDROP APPLICATION @APPNAME@app1 CASCADE;\nDROP APPLICATION @APPNAME@app2 CASCADE;\nDROP APPLICATION @APPNAME@app3 CASCADE;\nDROP APPLICATION @APPNAME@app4 CASCADE;\nDROP APPLICATION @APPNAME@app5 CASCADE;\n\nCREATE APPLICATION @APPNAME@app1 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;\n\ncreate flow @APPNAME@agentflowps;\nCREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',\nacks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\n\ncreate type @APPNAME@type1(\n  id String,\n  name String,\n  city string\n);\n\nCREATE STREAM @APPNAME@sourcestream OF Global.waevent persist using @APPNAME@KafkaPropset;\nCREATE STREAM @APPNAME@kps_typedStream OF @APPNAME@type1 partition by city persist using @APPNAME@KafkaPropset;\n\n\nCREATE OR REPLACE SOURCE @APPNAME@s USING oracleReader  ( \n  Username:'qatest',\n  Password:'qatest',\n  ConnectionURL:'localhost:1521/xe',\n  Tables:'QATEST.test01',\n  FetchSize:1\n ) \nOUTPUT TO @APPNAME@rawstream;\n\ncreate cq @APPNAME@cq1\nINSERT INTO @APPNAME@sourcestream\nSELECT * from @APPNAME@rawstream;\n\nCREATE CQ @APPNAME@cq2\nINSERT INTO @APPNAME@kps_typedStream\nSELECT TO_STRING(data[0]),\nTO_STRING(data[1]),\nTO_STRING(data[2])FROM @APPNAME@rawstream;\nend flow @APPNAME@agentflowps;\nend application @APPNAME@app1;\n--deploy application app1;\n--deploy application app1 with agentflowps on AGENTS;\n@DEPLOY@;\n\nCREATE APPLICATION @APPNAME@app2 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;\n\nCREATE TARGET @APPNAME@app2_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test01,QATEST.KPS1'\n) INPUT FROM @APPNAME@sourcestream;\n\n\nend application @APPNAME@app2;\ndeploy application @APPNAME@app2;\n\n\nCREATE APPLICATION @APPNAME@app3 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;\n\nCREATE TARGET @APPNAME@app3_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test01,QATEST.KPS2'\n) INPUT FROM @APPNAME@sourcestream;\n\nend application @APPNAME@app3;\n--deploy application app3;\n\n\nCREATE APPLICATION @APPNAME@app4 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;\n\nCREATE TARGET @APPNAME@app4_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test01,QATEST.KPS3'\n) INPUT FROM @APPNAME@sourcestream;\n\nend application @APPNAME@app4;\n--deploy application app4;\n\n\nCREATE APPLICATION @APPNAME@app5 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;\n\nCREATE TARGET @APPNAME@app5_target1 USING KafkaWriter VERSION '0.11.0'(\nbrokeraddress:'localhost:9092',\ntopic:'snappy1',\nKafkaConfig:'compression.type=snappy'\n) \nFORMAT USING DSVFormatter ()\nINPUT FROM @APPNAME@kps_typedStream;\n\nCREATE TARGET @APPNAME@app5_target2 USING KafkaWriter VERSION '0.11.0'(\nbrokeraddress:'localhost:9092',\ntopic:'gzip1',\nKafkaConfig:'compression.type=gzip'\n) \nFORMAT USING DSVFormatter ()\nINPUT FROM @APPNAME@sourcestream;\n\nCREATE TARGET @APPNAME@app5_target3 USING KafkaWriter VERSION '0.11.0'(\nbrokeraddress:'localhost:9092',\ntopic:'lz41',\nKafkaConfig:'compression.type=lz4'\n) \nFORMAT USING DSVFormatter ()\nINPUT FROM @APPNAME@sourcestream;\n\nend application @APPNAME@app5;\n--deploy application app5;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCreate Source @SourceName@\n Using OracleReader\n(\nUsername:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@SourceConnectURL@',\n Tables:'@SourceTable@',\n _h_useClassic: false,\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@SourceConnectURL@',\nUsername:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE OR REPLACE APPLICATION @AppName@;\nCREATE SOURCE @srcName@ USING Global.OracleReader ( \n  Username: '@srcusername@',\n  Password: '@srcpassword@',\n  ConnectionURL: '@srcurl@',\n  Tables: '@srcschema@.@srctable@') \nOUTPUT TO @instreamname@;\n\nCREATE OR REPLACE TARGET @tgtName@ USING Global.ServiceNowWriter ( \n  MaxConnections: 20, \n  ClientSecret: '@clientsecret@', \n  ApplicationErrorCountThreshold: 0, \n  BatchPolicy: 'eventCount:10000, Interval:60', \n  ConnectionUrl: '@tgturl@', \n  Password: '@tgtpassword@', \n  BatchAPI: false,  \n  ConnectionTimeOut: 60, \n  Mode: 'APPENDONLY', \n  ClientID: '@clientid@', \n  Tables: '@srcschema@.@srctable@,@tgttable@ COLUMNMAP()', \n  ConnectionRetries: 3, \n  useConnectionProfile: false, \n  UserName: '@tgtusername@', \n  adapterName: 'ServiceNowWriter' ) \nINPUT FROM @instreamname@;\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION HW ;\nundeploy application HW ;\ndrop application HW cascade;\n\nCREATE APPLICATION HW Recovery 5 second interval;\n\nCREATE  SOURCE S USING OracleReader  ( \n  Username: 'miner',\n  Password: '@miner',\n  ConnectionURL: '@conn-url@',\n  Tables: '@src@',\n  FetchSize: 1) \nOUTPUT TO hivestream;\n\nCreate Target T using ClouderaHiveWriter (\n  ConnectionURL:'@hive-url@',\n  Username:'@uname@', \n            Password:'@pwd@',\n            hadoopurl:'hdfs://dockerhost:9000/',\n\t        Mode:'incremental',\n\t        mergepolicy: 'eventcount:5,interval:1s',\n            Tables:'@tgt-table@',\n            hadoopConfigurationPath:'/Users/saranyad/Documents/hello/'\n )\nINPUT FROM hivestream;\n\n\nEND APPLICATION HW;\ndeploy application HW on all in default;\n\nStart application HW;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "DROP APPLICATION ns1.OPExample cascade;\nDROP NAMESPACE ns1 cascade;\nCREATE OR REPLACE NAMESPACE ns1;\nUSE ns1;\nCREATE APPLICATION OPExample;\n\nCREATE source CsvDataSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'PosDataPreview.csv',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n)\nOUTPUT TO CsvStream;\n \nCREATE TYPE MerchantHourlyAve(\n  merchantId String,\n  hourValue integer,\n  hourlyAve integer\n);\n\nCREATE CACHE HourlyAveLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'hourlyData.txt'\n)\nPARSE USING DSVParser (\n  header: Yes,\n  trimquote:false,\n  trimwhitespace:true\n) \nQUERY (keytomap:'merchantId') \nOF MerchantHourlyAve;\n\nCREATE CQ CsvToPosData\nINSERT INTO PosDataStream partition by merchantId\nSELECT TO_STRING(data[1]) as merchantId,\n  TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\n  DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,\n  TO_DOUBLE(data[7]) as amount,\n  TO_INT(data[9]) as zip\nFROM CsvStream;\n \nCREATE CQ cq2\nINSERT INTO SendToOPStream\nSELECT makeList(dateTime) as dateTime,\n  makeList(zip) as zip\nFROM PosDataStream;\n \nCREATE TYPE ReturnFromOPStream_Type ( time DateTime , val Integer );\nCREATE STREAM ReturnFromOPStream OF ReturnFromOPStream_Type;\n\nCREATE TARGET OPExampleTarget \nUSING FileWriter (filename: 'OPExampleOut') \nFORMAT USING JSONFormatter() \nINPUT FROM ReturnFromOPStream;\n \nEND APPLICATION OPExample;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application access;\n\ncreate source AALAccessSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'access_log',\n  charset:'UTF-8',\n  positionByEOF:false\n) PARSE USING AALParser (\n  columndelimiter:' ',\n  IgnoreEmptyColumn:'Yes'\n) OUTPUT TO AalAccessStream;\n\ncreate Target AALAccessDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/logdata') input from AalAccessStream;\n\nend application access;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION testApp;\nUNDEPLOY APPLICATION testApp;\nDROP APPLICATION testApp CASCADE;\nCREATE APPLICATION testApp recovery 5 SECOND Interval;\n\n\n  CREATE OR REPLACE SOURCE testApp_Source Using PostgreSQLReader( \n  \n  ReplicationSlotName:'test_slot',\n  FilterTransactionBoundaries:'true',\n  Username:'waction',\n  Password_encrypted:false,\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  adapterName:'PostgreSQLReader',\n  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',\n  Password:'w@ct10n',\n  Tables:'public.sourceTable',\n  ExcludedTables:'public.chkpoint'\n ) OUTPUT TO PGtoBQ_Stream;\n\n\nCREATE OR REPLACE TARGET testApp_Target USING BigQueryWriter  (\n  ColumnDelimiter: '|', \n  NullMarker: 'NULL', \n  projectId:'striimqa-214712',\n  Encoding: 'UTF-8', \n  BatchPolicy: 'eventCount:5,Interval:120',\n  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', \n  AllowQuotedNewLines: 'false', \n  adapterName: 'BigQueryWriter', \n  optimizedMerge: 'true', \n  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', \n  StandardSQL: 'true', \n  QuoteCharacter: '\\\"', \n  Tables: 'public.sourceTable,BQAllpl.oratobqtgt',\n  Mode: 'MERGE',\n  StandardSQL: 'true',\n  QuoteCharacter: '\\\"'\n  ) INPUT FROM PGtoBQ_Stream;\n\nCREATE OR REPLACE TARGET testApp_SysOut USING Global.SysOut (name: 'wa') INPUT FROM PGtoBQ_Stream;\n\nEND APPLICATION testApp;\nDEPLOY APPLICATION testApp;\nSTART testApp;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application AlterTester.DSV;\nundeploy application AlterTester.DSV;\ndrop application AlterTester.DSV cascade;\n\n\ncreate application DSV;\n\ncreate flow myFlowDSV;\ncreate source CSVSource using FileReader (\ndirectory:'@TEST-DATA-PATH@',\nWildCard:'smallposdata.csv',\npositionByEOF:false,\ncharset:'UTF-8'\n)\nparse using DSVParser (\nheader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\nend flow myFlowDSV;\nend application DSV;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Create Source @SOURCE_NAME@ Using OracleReader\n(\n Compression: false,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  --StartSCN: 'null',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n) Output To @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP TestAlertsEmail.TestAlertsEmailApp;\nUNDEPLOY APPLICATION TestAlertsEmail.TestAlertsEmailApp;\nDROP APPLICATION TestAlertsEmail.TestAlertsEmailApp CASCADE;\n\nCREATE APPLICATION TestAlertsEmailApp;\n\nCREATE source rawSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:No,\n  wildcard:'alerts_csv.txt',\n  coldelimiter:' ',\n  positionByEOF:false\n) OUTPUT TO rawStream;\n\nCREATE STREAM MyAlertStream OF Global.AlertEvent;\nCREATE CQ GenerateMyAlerts\nINSERT INTO MyAlertStream (name, keyVal, severity, flag, message)\nSELECT \"Testing Alerts\", data[0], data[1], data[2], data[3]\nFROM rawStream s;\nCREATE TARGET output2 USING SysOut(name : alertsrecevied) input FROM MyAlertStream;\n\nCREATE SUBSCRIPTION alertSubscription USING EmailAdapter\n(SMTPUSER:'zalakalerts@gmail.com',\nSMTPPASSWORD:'paloalto',\nsmtpurl:'smtp.gmail.com',\nstarttls_enable:'true',\nsmtp_auth:'true',\nsubject:\"Test Email Alerts With Security Enabled\",\nemailList:\"siddhika@striim.com,s.henry@striim.com,saranya@striim.com,invalidmailid.@striim.com\",\nuserIds:'admin',\nthreadCount:\"5\",\nsenderEmail:\"doga@striim.com\"\n)\nINPUT FROM MyAlertStream;\n\nEND APPLICATION TestAlertsEmailApp;\nDEPLOY APPLICATION TestAlertsEmail.TestAlertsEmailApp;\nSTART TestAlertsEmail.TestAlertsEmailApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;\nCREATE OR REPLACE SOURCE @AppName@_Source USING FileReader (\ndirectory:'@dataDir@',\nwildcard:'data.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO @AppName@_rawstream;\n\nCREATE OR REPLACE STREAM @BuiltinFunc@_Stream OF Global.WAEVent;\nCREATE OR REPLACE STREAM CombineStream OF Global.WAEVent;\n\nCREATE OR REPLACE CQ cq1\nINSERT INTO @BuiltinFunc@_Stream\nSELECT\n@BuiltinFunc@(s1, 'city',data[5])\nFROM @AppName@_rawstream s1;\n\nCREATE OR REPLACE CQ cq2\nINSERT INTO CombineStream\nSelect *\nFROM @BuiltinFunc@_Stream s4;\n\nCREATE OR REPLACE CQ cq3\nINSERT INTO CombineStream\nselect *\nFROM @AppName@_rawstream s5;\n\nCREATE OR REPLACE TARGET @AppName@_FileTarget USING Global.FileWriter ( \n  flushpolicy: 'EventCount:10000,Interval:30s', \n  adapterName: 'FileWriter', \n  directory: '@logs@',\n  filename: '@BuiltinFunc@_Data', \n  rolloverpolicy: 'EventCount:10000,Interval:30s' ) \nFORMAT USING Global.JSONFormatter  ( \n  handler: 'com.webaction.proc.JSONFormatter', \n  jsonMemberDelimiter: '\\n', \n  EventsAsArrayOfJsonObjects: 'true', \n  formatterName: 'JSONFormatter', \n  jsonobjectdelimiter: '\\n' ) \nINPUT FROM CombineStream;\n\nEnd application @AppName@;\nDeploy application @AppName@; \nStart application @AppName@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use PosTester;\nDROP TYPE MerchantTxRate;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader\n(\nFetchSize:'@FETCHSIZE@',\nQueueSize:'@QUEUESIZE@',\nUsername:'@USERNAME@',\nPassword:'@PASSWORD@',\nConnectionURL:'@URL@',\n_h_useClassic: @CLASSICMODE@,\nTables:'@TABLES@',\nCommittedTransactions:'@COMMITEDT@'\nCompression:'@COMPRESSION@'\n)\nOUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)\nSELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[0]) != '2';\n\nCREATE TYPE LogType(\nNum_col String key,\nChar_col String,\nVarchar2_col String,\nlong_col String,\nTable String,\nOperation String\n);\n\nCREATE WINDOW CDCWindow\nOVER @STREAM@\nKEEP 1 ROWS;\n\nCREATE WACTIONSTORE CDCWS CONTEXT of LogType\nEVENT TYPES ( LogType )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ ToWactionStore\nINSERT INTO CDCWS\nSELECT * FROM CDCWindow\nLINK SOURCE EVENT;\n\nCREATE TARGET @SOURCE_NAME@_SYS USING SysOut (\n  name: '@SOURCE_NAME@_SYS' )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION applicationApiSaas;\n\nCREATE FLOW SaasSourceFlow;\n\nCREATE SOURCE SaasLog4JSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'SaasMonitorLog4JOutput.xml',\n  positionByEOF:false\n)\nPARSE USING XMLParser(\n  rootnode:'/log4j:event',\n  columnlist:'log4j:event/@timestamp,log4j:event/@level,log4j:event/log4j:message,log4j:event/log4j:locationInfo/@method'\n)\nOUTPUT TO SaasRawXMLStream;\n\nCREATE TYPE SaasLog4JEvent (\n    ts DateTime,\n    level String,\n    message String,\n    method String\n);\nCREATE STREAM SaasLog4JStream OF SaasLog4JEvent;\n\nCREATE CQ SaasGetLog4JData\nINSERT INTO SaasLog4JStream\nSELECT TO_DATE(TO_LONG(data[0])), data[1], data[2], data[3]\nFROM SaasRawXMLStream;\n\nCREATE STREAM SaasErrorStream OF SaasLog4JEvent;\nCREATE STREAM SaasWarningStream OF SaasLog4JEvent;\nCREATE STREAM SaasInfoStream OF SaasLog4JEvent;\n\nCREATE CQ SaasGetErrors\nINSERT INTO SaasErrorStream\nSELECT log4j\nFROM SaasLog4JStream log4j WHERE log4j.level = 'ERROR';\n\nCREATE CQ SaasGetWarnings\nINSERT INTO SaasWarningStream\nSELECT log4j\nFROM SaasLog4JStream log4j WHERE log4j.level = 'WARN';\n\nCREATE CQ SaasGetInfo\nINSERT INTO SaasInfoStream\nSELECT log4j\nFROM SaasLog4JStream log4j WHERE log4j.level = 'INFO';\n\nEND FLOW SaasSourceFlow;\n\n\nCREATE FLOW SaasErrorFlow;\n\nCREATE STREAM SaasErrorAlertStream OF Global.AlertEvent;\n\nCREATE CQ SaasSendErrorAlerts\nINSERT INTO SaasErrorAlertStream\nSELECT 'ErrorAlert', ''+ts, 'error', 'raise', 'Error in log ' + message\nFROM SaasErrorStream;\n\nCREATE SUBSCRIPTION SaasErrorAlertSub USING WebAlertAdapter( ) INPUT FROM SaasErrorAlertStream;\n\nEND FLOW SaasErrorFlow;\n\n\nCREATE FLOW SaasWarningFlow;\n\nCREATE JUMPING WINDOW SaasWarningWindow\nOVER SaasWarningStream KEEP WITHIN 30 SECOND ON ts;\n\nCREATE STREAM SaasWarningAlertStream OF Global.AlertEvent;\n\nCREATE CQ SaasSendWarningAlerts\nINSERT INTO SaasWarningAlertStream\nSELECT 'WarningAlert', ''+ts, 'warning', 'raise',\n        COUNT(ts) + ' Warnings in log for api ' + method\nFROM SaasWarningWindow\nGROUP BY method\nHAVING count(ts) > 25;\nCREATE SUBSCRIPTION SaasWarningAlertSub USING WebAlertAdapter( ) INPUT FROM SaasWarningAlertStream;\n\nEND FLOW SaasWarningFlow;\n\n\nCREATE FLOW SaasInfoFlow;\n\nCREATE TYPE SaasApiCall (\n  userId String,\n  apiCall String,\n  sobject String,\n  ts DateTime,\n  userName String,\n  company String,\n  userZip String,\n  companyZip String\n);\nCREATE STREAM SaasApiStream OF SaasApiCall;\n\nCREATE CQ SaasGetApiDetails\nINSERT INTO SaasApiStream(userId,apiCall,sobject,ts)\nSELECT MATCH(i.message, '\\\\\\\\[user=([a-zA-Z0-9]*)\\\\\\\\]'),\n       MATCH(i.message, '\\\\\\\\[api=([a-zA-Z0-9]*)\\\\\\\\]'),\n       MATCH(i.message, '\\\\\\\\[sobject=([a-zA-Z0-9]*)\\\\\\\\]'),\n       i.ts\nFROM SaasInfoStream i;\n\nCREATE STREAM SaasApiEnrichedStream OF SaasApiCall;\n\nCREATE TYPE SaasUserInfo (\n  userId String,\n  userName String,\n  company String,\n  userZip String,\n  companyZip String\n);\nCREATE CACHE SaasUserLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'userLookup.txt',\n  header: Yes,\n  columndelimiter: ','\n) QUERY (keytomap:'userId') OF SaasUserInfo;\n\n\nCREATE CQ SaasGetUserDetails\nINSERT INTO SaasApiEnrichedStream\nSELECT a.userId, a.apiCall, a.sobject, a.ts, u.userName, u.company, u.userZip, u.companyZip\nFROM SaasApiStream a, SaasUserLookup u\nWHERE a.userId = u.userId;\n\nEND FLOW SaasInfoFlow;\n\n\nCREATE FLOW SaasUserApiFlow;\n\nCREATE JUMPING WINDOW SaasUserApiWindow\nOVER SaasApiEnrichedStream KEEP WITHIN 5 SECOND ON ts\nPARTITION BY userId;\n\nCREATE TYPE SaasUserApiUsage (\n  userId String key,\n  userName String,\n  userZip String,\n  userLat double,\n  userLong double,\n  company String,\n  apiCall String,\n  count integer,\n  state String,\n  topObject String,\n  ts DateTime\n);\nCREATE STREAM SaasUserApiUsageStream OF SaasUserApiUsage;\n\nCREATE TYPE SaasUSAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\nCREATE CACHE SaasZipLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'USAddresses.txt',\n  header: Yes,\n  columndelimiter: '\t'\n) QUERY (keytomap:'zip') OF SaasUSAddressData;\n\n\nCREATE CQ SaasGetUserApiUsage\nINSERT INTO SaasUserApiUsageStream\nSELECT a.userId, a.userName, a.userZip, z.latVal, z.longVal, a.company,\n       a.apiCall, COUNT(a.sobject),\n       CASE WHEN COUNT(a.sobject) > 175 THEN 'HOT'\n            ELSE 'COLD' END,\n       MAXOCCURS(a.sobject),\n       FIRST(a.ts)\nFROM SaasUserApiWindow a, SaasZipLookup z\nWHERE a.userZip = z.zip\nGROUP BY a.userId, a.apiCall\nHAVING FIRST(a.ts) IS NOT NULL;\n\nCREATE TYPE SaasUserApiContext (\n  userId String key,\n  userName String,\n  userZip String,\n  userLat double,\n  userLong double,\n  company String,\n  count integer,\n  state String,\n  topObject String,\n  ts DateTime\n);\nCREATE WACTIONSTORE SaasUserApiActivity\nCONTEXT OF SaasUserApiContext\nEVENT TYPES ( SaasUserApiUsage )\n@PERSIST-TYPE@\n\nCREATE JUMPING WINDOW SaasUserWindow\nOVER SaasUserApiUsageStream KEEP WITHIN 5 SECOND ON ts\nPARTITION BY userId;\n\nCREATE CQ SaasTrackUserApiSummary\nINSERT INTO SaasUserApiActivity\nSELECT a.userId, a.userName, a.userZip, a.userLat, a.userLong,\n       a.company, SUM(count),\n       CASE WHEN SUM(count) > 1000 THEN 'HOT'\n            ELSE 'COLD' END,\n       MAXOCCURS(a.topObject),\n       FIRST(a.ts)\nFROM SaasUserWindow a\nGROUP BY a.userId\nLINK SOURCE EVENT;\n\nCREATE STREAM SaasUserApiAlertStream OF Global.AlertEvent;\n\nCREATE CQ SaasSendUserApiAlerts\nINSERT INTO SaasUserApiAlertStream\nSELECT 'UserAPIAlert', ''+ts, 'warning', 'raise',\n       'User ' + userName + ' has used api ' + apiCall + ' ' + count + ' times for ' + topObject\nFROM SaasUserApiUsageStream\nWHERE state = 'HOT' AND count > 300;\n\nCREATE SUBSCRIPTION SaasUserApiAlertSub USING WebAlertAdapter( ) INPUT FROM SaasUserApiAlertStream;\n\nEND FLOW SaasUserApiFlow;\n\n\nCREATE FLOW SaasCompanyApiFlow;\n\nCREATE JUMPING WINDOW SaasCompanyApiWindow\nOVER SaasApiEnrichedStream KEEP WITHIN 5 SECOND ON ts\nPARTITION BY company;\n\nCREATE TYPE SaasCompanyApiUsage (\n  company String key,\n  companyZip String,\n  companyLat double,\n  companyLong double,\n  apiCall String,\n  count integer,\n  state String,\n  topObject String,\n  ts DateTime\n);\nCREATE STREAM SaasCompanyApiUsageStream OF SaasCompanyApiUsage;\n\nCREATE CQ SaasGetCompanyApiUsage\nINSERT INTO SaasCompanyApiUsageStream\nSELECT a.company, a.companyZip, z.latVal, z.longVal,\n       a.apiCall, COUNT(a.sobject),\n       CASE WHEN COUNT(a.sobject) > 3000 THEN 'HOT'\n            ELSE 'COLD' END,\n       MAXOCCURS(a.sobject),\n       FIRST(a.ts)\nFROM SaasCompanyApiWindow a, SaasZipLookup z\nWHERE a.companyZip = z.zip\nGROUP BY a.company, a.apiCall\nHAVING FIRST(a.ts) IS NOT NULL;\n\nCREATE TYPE SaasCompanyApiContext (\n  company String key,\n  companyZip String,\n  companyLat double,\n  companyLong double,\n  count integer,\n  state String,\n  topObject String,\n  ts DateTime\n);\nCREATE JUMPING WINDOW SaasCompanyWindow\nOVER SaasCompanyApiUsageStream KEEP WITHIN 5 SECOND ON ts\nPARTITION BY company;\n\nCREATE WACTIONSTORE SaasCompanyApiActivity\nCONTEXT OF SaasCompanyApiContext\nEVENT TYPES ( SaasCompanyApiUsage )\n@PERSIST-TYPE@\n\n\nCREATE CQ SaasTrackCompanyApiSummary\nINSERT INTO SaasCompanyApiActivity\nSELECT a.company, a.companyZip, a.companyLat, a.companyLong,\n       SUM(a.count),\n       CASE WHEN SUM(a.count) > 15000 THEN 'HOT'\n            ELSE 'COLD' END,\n       MAXOCCURS(a.topObject),\n       FIRST(a.ts)\nFROM SaasCompanyWindow a\nGROUP BY a.company\nLINK SOURCE EVENT;\n\nEND FLOW SaasCompanyApiFlow;\n\n\nCREATE FLOW SaasApiFlow;\n\nCREATE JUMPING WINDOW SaasApiWindow\nOVER SaasApiEnrichedStream KEEP WITHIN 5 SECOND ON ts\nPARTITION BY apiCall;\n\nCREATE TYPE SaasApiUsage (\n  apiCall String key,\n  sobject String,\n  count integer,\n  state String,\n  ts DateTime\n);\nCREATE STREAM SaasApiUsageStream OF SaasApiUsage;\n\nCREATE CQ SaasGetApiUsage\nINSERT INTO SaasApiUsageStream\nSELECT a.apiCall, a.sobject,\n       COUNT(a.userId),\n       CASE WHEN COUNT(a.userId) > 3000 THEN 'HOT'\n            ELSE 'COLD' END,\n       FIRST(a.ts)\nFROM SaasApiWindow a\nGROUP BY a.apiCall, a.sobject\nHAVING FIRST(a.ts) IS NOT NULL;\n\nCREATE TYPE SaasApiContext (\n  apiCall String key,\n  count integer,\n  state String,\n  topObject String,\n  ts DateTime\n);\n\nCREATE JUMPING WINDOW SaasApiAggWindow\nOVER SaasApiUsageStream KEEP WITHIN 5 SECOND ON ts\nPARTITION BY apiCall;\n\nCREATE WACTIONSTORE SaasApiActivity\nCONTEXT OF SaasApiContext\nEVENT TYPES ( SaasApiUsage )\n@PERSIST-TYPE@\n\nCREATE CQ SaasTrackApiSummary\nINSERT INTO SaasApiActivity\nSELECT a.apiCall,\n       SUM(a.count),\n       CASE WHEN SUM(a.count) > 20000 THEN 'HOT'\n            ELSE 'COLD' END,\n       MAXOCCURS(a.sobject),\n       first(a.ts)\nFROM SaasApiAggWindow a\nGROUP BY a.apiCall\nLINK SOURCE EVENT;\n\nEND FLOW SaasApiFlow;\n\nEND APPLICATION applicationApiSaas;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application GGTrailReaderApp;\nundeploy application GGTrailReaderApp;\ndrop application GGTrailReaderApp cascade;\n\ncreate application GGTrailReaderApp recovery 5 second interval;\n\ncreate source GGTrailSource using GGTrailReader (\ntRaildIrectory:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario15',\ntRAilfilepattern:'15*',\npositionByEOF:false,\nFilterTransactionBoundaries: true,\nDefinitionFile:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario15/Scn15_beforeddl.def',\ncaptureCDdl: true,\nCDDLAction:'Process',\n--CDDLAction:'ignore',\n--CDDLAction:'quiesce',\n--cddlAction:'Error',\nTrailByTeOrder:'LittleEndian',\nrecoveryInterval: 5,\nTABLES:'QATEST.INT2;QATEST.INT1'\n)\nOUTPUT TO GGTrailStream;\n\ncreate Target t2 using SysOut(name:Foo2) input from GGTrailStream;\n\nCREATE TARGET WriteCDCOracle1 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost/ORCL',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:1,Interval:1',\nCommitPolicy:'Eventcount:1,Interval:1',\nCheckpointtable:'RGRN_CHKPOINT',\nTables:'QATEST.GGDDL5,QATEST.GGDDL5_TGT'\n) INPUT FROM GGTrailStream;\n\n\nend application GGTrailReaderApp;\n\ndeploy application GGTrailReaderApp;\nstart application GGTrailReaderApp;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 35 with two sources, two jumping count windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jc5W/p -> CQ1 -> WS\n--   S2 -> Jc6W/p -> CQ2 -> WS\n--\n\nSTOP KStreamRecov35Tester.KStreamRecovTest35;\nUNDEPLOY APPLICATION KStreamRecov35Tester.KStreamRecovTest35;\nDROP APPLICATION KStreamRecov35Tester.KStreamRecovTest35 CASCADE;\n\nDROP USER KStreamRecov35Tester;\nDROP NAMESPACE KStreamRecov35Tester CASCADE;\nCREATE USER KStreamRecov35Tester IDENTIFIED BY KStreamRecov35Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov35Tester;\nCONNECT KStreamRecov35Tester KStreamRecov35Tester;\n\nCREATE APPLICATION KStreamRecovTest35 RECOVERY 5 SECOND INTERVAL\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP 5 ROWS\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP 6 ROWS\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nEND APPLICATION KStreamRecovTest35;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 21 with two sources, two sliding count windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Sc5W -> CQ1 -> WS\n-- S2 -> Sc6W -> CQ2 -> WS\n--\n\nSTOP KStreamRecov21Tester.KStreamRecovTest21;\nUNDEPLOY APPLICATION KStreamRecov21Tester.KStreamRecovTest21;\nDROP APPLICATION KStreamRecov21Tester.KStreamRecovTest21 CASCADE;\nDROP USER KStreamRecov21Tester;\nDROP NAMESPACE KStreamRecov21Tester CASCADE;\nCREATE USER KStreamRecov21Tester IDENTIFIED BY KStreamRecov21Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov21Tester;\nCONNECT KStreamRecov21Tester KStreamRecov21Tester;\n\nCREATE APPLICATION KStreamRecovTest21 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP 5 ROWS;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP 6 ROWS;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2;\n\nEND APPLICATION KStreamRecovTest21;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop IR;\nUndeploy application IR;\ndrop application IR cascade;\nCREATE APPLICATION IR recovery 5 second interval;\nCREATE OR REPLACE SOURCE Teradata_source1 USING IncrementalBatchReader  ( \n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.test01=id',\n  startPosition: '%=0',\n  PollingInterval: '20sec'\n )\nOUTPUT TO data_stream1;\n\nCREATE OR REPLACE SOURCE Teradata_source2 USING IncrementalBatchReader  ( \n  FetchSize: 10000,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.test01=t1',\n  startPosition: '%=0',\n  PollingInterval: '20sec' )\nOUTPUT TO data_stream2;\n\nCREATE OR REPLACE SOURCE Teradata_source3 USING IncrementalBatchReader  ( \n  FetchSize: 10000,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.test01=id',\n  startPosition: '%=1',\n  PollingInterval: '20sec'\n )\nOUTPUT TO data_stream3;\n\nCREATE OR REPLACE SOURCE Teradata_source4 USING IncrementalBatchReader  ( \n  FetchSize: 10000,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.test01=t1',\n  startPosition: '%=1',\n  PollingInterval: '20sec' )\nOUTPUT TO data_stream4;\nCREATE OR REPLACE SOURCE Teradata_source5 USING IncrementalBatchReader  ( \n  FetchSize: 10000,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.test01=id',\n  startPosition: '%=1',\n  PollingInterval: '20sec'\n )\nOUTPUT TO data_stream5;\n\nCREATE OR REPLACE SOURCE Teradata_source6 USING IncrementalBatchReader  ( \n  FetchSize: 10000,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.test01=t1',\n  startPosition: '%=0',\n  PollingInterval: '20sec' )\nOUTPUT TO data_stream5;\n\n\ncreate target AzureSQLDWHTarget1 using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test4 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',\n        uploadpolicy:'eventcount:10000,interval:10s'\n) INPUT FROM data_stream1;\n\n\ncreate target AzureSQLDWHTarget2 using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test5',\n        uploadpolicy:'eventcount:10000,interval:10s'\n) INPUT FROM data_stream2;\n\ncreate target AzureSQLDWHTarget3 using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test5',\n        uploadpolicy:'eventcount:10000,interval:10s'\n) INPUT FROM data_stream3;\n\ncreate target AzureSQLDWHTarget4 using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test5',\n        uploadpolicy:'eventcount:10000,interval:10s'\n) INPUT FROM data_stream4;\n\ncreate target AzureSQLDWHTarget5 using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test5',\n        uploadpolicy:'eventcount:10000,interval:10s'\n) INPUT FROM data_stream5;\n\n\n\ncreate target AzureSQLDWHTarget6 using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test5',\n        uploadpolicy:'eventcount:10000,interval:10s'\n) INPUT FROM data_stream5;\n\n\nEND APPLICATION IR;\ndeploy application IR on all in default;\nstart application IR;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create flow serverFlow;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  --Password_encrypted: false\n )\nINPUT FROM @STREAM@;\n\nend flow serverFlow;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING Global.DeltaLakeWriter (\n  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',\n  hostname: 'adb-5292730997167687.7.azuredatabricks.net',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.testaswin',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:1,interval:10s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\ncreate flow @APPNAME@_agentflow;\n\nCreate Source @SOURCE@\n        Using OracleReader\n(\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: 'qatest.test77',\n  FetchSize:1,\n  QueueSize:2000,\n  CommittedTransactions:true,\n  Compression:false\n)\nOutput To @STREAM@;\n\nend flow @APPNAME@_agentflow;\n\ncreate flow @APPNAME@_serverflow;\n\ncreate Target @TARGET@ using GCSWriter(\n    bucketname:'@bucketname@',\n    objectname:'@objectname@',\n    projectId:'@project-id@',\n    uploadpolicy:'EventCount:7'\n    ServiceAccountKey:'@file-path@'\n)\nformat using DSVFormatter (\nmembers:'data'\n)\ninput from @STREAM@;\n\nend flow @APPNAME@_serverflow;\n\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 8\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> WS\n--\n\nSTOP Recov8Tester.RecovTest8;\nUNDEPLOY APPLICATION Recov8Tester.RecovTest8;\nDROP APPLICATION Recov8Tester.RecovTest8 CASCADE;\nCREATE APPLICATION RecovTest8 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE WactionType (\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions\nINSERT INTO Wactions\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nEND APPLICATION RecovTest8;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "IMPORT static com.webaction.runtime.converters.DateConverter.*;\n\nDROP APPLICATION SampleCDCReaderApp cascade;\n\nCREATE APPLICATION SampleCDCReaderApp;\ncreate source CDCSource using SampleReader (\n\tAgentPortNo:'2012',\n\tAgentIpAddress:'127.0.0.1',\n\tportno:'2020',\n\tipaddress:'10.10.196.103',\n\tName:'testsession',\n    dataFile:'../conf/data.csv',\n    metaFile:'../conf/metadata.csv',\n\tTables:'POS;STU') output to dbstream,\n\tCheckUnSupportedTypeStream MAP (table:'STU');\n\nCREATE TYPE CDCPosData(\n    BUSINESSNAME String,\n    BUSINESSNAME_HEX String,\n    PRIMARYACCOUNTNUMBER String,\n    POSDATACODE Integer,\n    DATETIME org.joda.time.DateTime,\n    EXPDATE String,\n    CURRENCYCODE String,\n    AUTHAMOUNT Float,\n    TERMINALID String,\n    ZIP String,\n    CITY String,\n    OPR String,\n    TABLENAME String\n);\n\nCREATE STREAM CDCPosDataStream OF CDCPosData;\n\nCREATE JUMPING WINDOW CDCPosDataWindow\nOVER CDCPosDataStream KEEP 9 ROWS\nPARTITION BY OPR;\n\nCREATE CQ CDCCsvToPosData\nINSERT INTO CDCPosDataStream\nSELECT TO_STRING(data[0],\"UTF-8\"),\n\t   TO_HEX(data[0]),\n       data[2],\n       TO_INT(data[3]),\n\t   data[4],\n\t   data[5],\n       data[6],\n       TO_FLOAT(data[7]),\n\t   data[8],\n       data[9],\n\t   data[10],\n\tMETA(x,\"OperationName\").toString(),\n\tMETA(x, \"TableName\").toString()\nFROM dbstream x\nWHERE not(META(x,\"OperationName\").toString() = \"BEGIN\") AND not(META(x,\"OperationName\").toString() = \"COMMIT\") AND not(META(x, \"TableName\").toString() is null) AND META(x, \"TableName\").toString() = \"POS\";\n\nCREATE TYPE CDCSampleOperationData(\n    TableName String,\n    OperationName String,\n    Count Integer\n);\n\nCREATE STREAM CDCSampleOperationDataStream OF CDCSampleOperationData;\n-- PARTITION BY OperationName;\n\n\nCREATE CQ CDCSampleOperationCheck\nINSERT INTO CDCSampleOperationDataStream\nSELECT x.TABLENAME,\nCASE WHEN x.OPR = 'INSERT' THEN x.OPR\n     WHEN x.OPR = 'DELETE' THEN x.OPR\n     WHEN x.OPR = 'UPDATE' THEN x.OPR\n     ELSE 'UNSUPPORTED OPREATION' END,\nCASE WHEN x.OPR = 'INSERT' THEN COUNT(x.OPR)\n     WHEN x.OPR = 'DELETE' THEN COUNT(x.OPR)\n     WHEN x.OPR = 'UPDATE' THEN COUNT(x.OPR)\n     ELSE 0 END\nFROM CDCPosDataWindow x\nGROUP BY OPR;\n\nCREATE TARGET CDCOperationLog USING LogWriter(\n\tname:FILECDCP,\n\tfilename:'@FEATURE-DIR@/logs/SampleCDCReaderOperationCheck.log'\n--\tfilename:'a1.log'\n) INPUT FROM CDCSampleOperationDataStream;\n\nCREATE TARGET CDCLog USING LogWriter(\n  name:SampleCDCReaderApp,\nfilename:'@FEATURE-DIR@/logs/SampleCDCReaderApp.log'\n--  filename:'a.log'\n) INPUT FROM CDCPosDataStream;\n\nCREATE TARGET CDCLog1 USING LogWriter(\n  name:SampleCDCReaderApp,\n  filename:'@FEATURE-DIR@/logs/UnsupportedColumn.log'\n--  filename:'a2.log'\n) INPUT FROM CDCCheckUnSupportedTypeStream;\n\nEND APPLICATION SampleCDCReaderApp;\n\n--deploy application SampleCDCReaderApp.SampleCDCReaderApp in default;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@ Using MSSqlReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n DatabaseName:'@DatabaseName@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'@SourceTable@',\n ConnectionPoolSize:1,\n StartPosition:'EOF'\n ) Output To @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;\n\ncreate Target @targetFile@ using FileWriter(\n  filename:'TestOut.log',\n  directory:'@FileDirectoryPath@',\n  rolloverpolicy:'eventcount:1000'\n)\nformat using DSVFormatter (\n\n)\ninput from @SRCINPUTSTREAM@;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application SalesForceReaderTest;\nundeploy application SalesForceReaderTest;\ndrop application SalesForceReaderTest cascade;\n\nCREATE APPLICATION SalesForceReaderTest recovery 5 second interval;\n\nCREATE OR REPLACE SOURCE SFPoller USING SalesForceReader \n(\n  sObjects:'newobj__c',\n  --sObject:'Campaign',\n  pollingInterval:'1 min',\n  autoAuthTokenRenewal:'true',\n  Username:'siddhika@webaction.com',\n  Password:'webaction@1234',\n  securityToken:'qhNbKKmafFpanz8Y2oiM89UhR',\n  consumerKey:'3MVG9ZL0ppGP5UrBayz85eLnPg69gWLaE8pA3uzwcFCZ9s.J0mgE7AKvPCEhTaop4uYRbBaDnGXHjnLmngG6P',\n  consumerSecret:'2500119200751301808',\n  apiEndPoint:'https://ap2.salesforce.com',\n  mode:'InitialLoad',\n  startTimestamp:null\n)\nOUTPUT TO DataStream;\n\ncreate target tout using sysout(name : 'out')input from DataStream;\n\n/*\nCREATE TARGET dbtar USING DatabaseWriter( \n\tBatchPolicy:'EventCount:100,Interval:10',\n\tCommitPolicy:'EventCount:100,Interval:10',\n\tConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\n\tUsername:'qatest',\n\tPassword:'qatest',\n  Tables:'Position__c,QATEST.POS columnmap(Id=Id,numnum=numnum__c)'\n  -- Tables: 'newobj__c,QATEST.SFTABLE1 columnmap(Id=Id,CHECKBOOL=checkbool__c,CURR=curr__c,DT=dt__c,TIME1=time1__c,DtTime=DtTime__c,EMAILID=emailid__c,NUM=num__c,PERCNT=percnt__c,PHN=phn__c,TXTLONG=txtlong__c,URL1=url1__c,TXT=txt__c)'\n   ---,loc__latitude=loc__latitude__s,loc__longitude=loc__longitude__s)' \n) INPUT FROM DataStream;\n\n*/\nCREATE OR REPLACE TARGET Target2 using FileWriter\n(\n  filename:'Obj123.json',\n  directory:'/Users/siddhika/Product/',\n  rolloverpolicy:'EventCount:1'\n)\nFORMAT USING JSONFormatter ()\nINPUT FROM DataStream;\n\nEND APPLICATION SalesForceReaderTest;\nDEPLOY APPLICATION SalesForceReaderTest on any in default;\nSTART SalesForceReaderTest;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nSTOP Jumping1Tester.Jumping1;\nUNDEPLOY APPLICATION Jumping1Tester.Jumping1;\nDROP APPLICATION Jumping1Tester.Jumping1 CASCADE;\nCREATE APPLICATION Jumping1;\n\ncreate source CsvSource1 using FileReader\n(\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'WindowsTest.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n)\n parse using DSVParser\n(\n\theader:'yes',\n\tcolumndelimiter:','\n)\nOUTPUT TO CsvStream1;\n\nCREATE TYPE CsvData (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\nCREATE TYPE CsvData1 (\n  zip double\n);\n\nCREATE TYPE WactionData1 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData2 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData3 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData4 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData5 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData6 (\n  zip double\n);\nCREATE TYPE WactionData7 (\n  zip double\n);\nCREATE TYPE WactionData8 (\n  zip double\n);\n\nCREATE STREAM DataStream1 OF CsvData;\n\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY companyName;\n\nCREATE STREAM DataStream3 OF CsvData\nPARTITION BY city;\n\nCREATE STREAM DataStream4 OF CsvData;\nCREATE STREAM DataStream5 OF CsvData\nPARTITION BY city;\n\nCREATE STREAM DataStream6 OF CsvData1;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData3\nINSERT INTO DataStream3\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData4\nINSERT INTO DataStream4\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData5\nINSERT INTO DataStream5\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData6\nINSERT INTO DataStream6\nSELECT\n    TO_DOUBLE(data[8])\nFROM CsvStream1;\n\nCREATE CQ CsvToData7\nINSERT INTO DataStream7\nSELECT\n    TO_DOUBLE(data[8])\nFROM CsvStream1;\n\nCREATE CQ CsvToData8\nINSERT INTO DataStream8\nSELECT\n    TO_DOUBLE(data[8])\nFROM CsvStream1;\n\n-- Count based jumping window\nCREATE JUMPING WINDOW DataStreamCount\nOVER DataStream1 KEEP 5 ROWS;\n\n-- Time based jumping window\nCREATE JUMPING WINDOW DataStreamTime OVER DataStream2 KEEP\nwithin 40 second\nPARTITION BY companyName;\n\n-- Attribute based jumping window\nCREATE JUMPING WINDOW DataStreamAtrribute\nOVER DataStream3 KEEP\nrange 5 minute\nON dateTime\nPARTITION BY city;\n\n-- Count + time based jumping window\nCREATE JUMPING WINDOW DataStreamCountTime\nOVER DataStream4 KEEP\n5 rows\nwithin 8 minute;\n\n-- Attribute + time based jumping window\nCREATE JUMPING WINDOW DataStreamAttributeTime\nOVER DataStream5 KEEP\nrange 300 second\nON dateTime\nwithin 5 minute\nPARTITION BY city;\n\n-- Attribute + time based jumping window using COUNT\nCREATE JUMPING WINDOW DataStreamAttributeTime1\nOVER DataStream5 KEEP\nrange 300 second\nON dateTime\nwithin 5 minute;\n\nCREATE WACTIONSTORE Wactions1 CONTEXT OF WactionData1\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions2 CONTEXT OF WactionData2\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions3 CONTEXT OF WactionData3\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions4 CONTEXT OF WactionData4\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions5 CONTEXT OF WactionData5\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions6 CONTEXT OF WactionData6\nEVENT TYPES ( CsvData1 )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions7 CONTEXT OF WactionData7\nEVENT TYPES ( CsvData1 )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions8 CONTEXT OF WactionData8\nEVENT TYPES ( CsvData1 )\n@PERSIST-TYPE@\n\nCREATE CQ Data1ToWaction\nINSERT INTO Wactions1\nSELECT\n\tp.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamCount p;\n\nCREATE CQ Data2ToWaction\nINSERT INTO Wactions2\nSELECT\n    p.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamTime p\ngroup by companyName;\n\nCREATE CQ Data3ToWaction\nINSERT INTO Wactions3\nSELECT\n    p.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamAtrribute p;\n\nCREATE CQ Data4ToWaction\nINSERT INTO Wactions4\nSELECT\n    p.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamCountTime p;\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions5\nSELECT\n    p.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamAttributeTime p\ngroup by city;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions6\nSELECT\n    count(*)\nFROM DataStreamCount p;\n\nCREATE CQ Data7ToWaction\nINSERT INTO Wactions7\nSELECT\n    count(*)\nFROM DataStreamCountTime p;\n\nCREATE CQ Data8ToWaction\nINSERT INTO Wactions8\nSELECT\n    count(*)\nFROM DataStreamAttributeTime1 p;\n\n\nEND APPLICATION Jumping1;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 11\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS1\n-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS2\n--\n\nSTOP Recov11Tester.RecovTest11;\nUNDEPLOY APPLICATION Recov11Tester.RecovTest11;\nDROP APPLICATION Recov11Tester.RecovTest11 CASCADE;\nCREATE APPLICATION RecovTest11 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE WactionType (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionType;\n\nCREATE CQ CsvToWaction\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE Wactions1 CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions2 CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions1\nINSERT INTO Wactions1\nSELECT\n    *\nFROM DataStream5Minutes;\n\nCREATE CQ InsertWactions2\nINSERT INTO Wactions2\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND APPLICATION RecovTest11;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop DataGenSampleApp;\nundeploy application DataGenSampleApp;\ndrop application DataGenSampleApp cascade;\n\n\nCREATE APPLICATION DataGenSampleApp;\n\nCreate Source dataGenSrc using MySQLReader\n(\n Username:'root',\n Password:'w@ct10n',\n ConnectionURL: 'mysql://127.0.0.1:3306/waction',\n Tables:'@tableNames@'\n )\n Output To LCRStream;\n\ncreate Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;\n\nEND APPLICATION DataGenSampleApp;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Canon Test W40\n-- Nicholas Keene, WebAction, Inc.\n--\n-- Basic test for an unpartitioned jumping attribute window\n--\n-- S -> JWa5u -> CQ -> WS\n--\n\n\nUNDEPLOY APPLICATION NameW40.W40;\nDROP APPLICATION NameW40.W40 CASCADE;\nCREATE APPLICATION W40 RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE FLOW DataAcquisitionW40;\n\n\nCREATE SOURCE CsvSourceW40 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'Canon1000.csv',\n  columndelimiter:',',\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStreamW40;\n\n\nEND FLOW DataAcquisitionW40;\n\n\n\n\nCREATE FLOW DataProcessingW40;\n\nCREATE TYPE DataTypeW40 (\n    eventID String,\n    word String KEY,\n    dateTime DateTime\n);\n\nCREATE STREAM DataStreamW40 OF DataTypeW40;\n\nCREATE CQ CSVStreamW40_to_DataStreamW40\nINSERT INTO DataStreamW40\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[2],'yyyyMMddHHmmss')\nFROM CsvStreamW40;\n\nCREATE JUMPING WINDOW JWa5uW40\nOVER DataStreamW40\nKEEP WITHIN 5 SECOND ON dateTime;\n\nCREATE WACTIONSTORE WactionStoreW40 CONTEXT OF DataTypeW40\nEVENT TYPES ( DataTypeW40 KEY(word) )\n@PERSIST-TYPE@\n\nCREATE CQ JWa5uW40_to_WactionStoreW40\nINSERT INTO WactionStoreW40\nSELECT\n    FIRST(eventID),\n    FIRST(word),\n    FIRST(dateTime)\nFROM JWa5uW40;\n\nEND FLOW DataProcessingW40;\n\n\n\nEND APPLICATION W40;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use PosTester;\nDROP STREAM MerchantTxRateOnlyStream;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE @SourceName@ USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --startLSN:'758199500005967269518578814280000',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.postgrestopostgres_src',\n  ExcludedTables:'public.postgres_2000target'\n ) \nOUTPUT TO @SRCINPUTSTREAM@ ;\n\n\nCREATE OR REPLACE TARGET @targetsys@ USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy: 'EventCount:1,Interval:60',\nCommitPolicy: 'EventCount:1,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM @SRCINPUTSTREAM@;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( \n  Username: '@SRC_USERNAME@',\n  Password: '@SRC_PASSWORD@',\n  ConnectionURL: '@CDC_URL@',\n  Tables: '@Source1Tables@',\n  FetchSize: 1) \nOUTPUT TO @APPNAME@cdcStream;\n\nCREATE OR REPLACE EXTERNAL CACHE @APPNAME@1 ( \n  AdapterName:'DatabaseReader',\n  Username: '@SRC_USERNAME@',\n  Password: '@SRC_PASSWORD@',\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  FetchSize: 20,\n  DatabaseProviderType: 'Default',\n  Table: '@Source3Tables@',\n  Columns: 'col1,col2,col3,col4,uniquecol',\n  keytomap: 'uniquecol')  \nOF @APPNAME@cachetype;\n\nCREATE OR REPLACE EXTERNAL CACHE @APPNAME@2 ( \n  AdapterName:'DatabaseReader',\n  Username: '@SRC_USERNAME@',\n  Password: '@SRC_PASSWORD@',\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  FetchSize: 10,\n  DatabaseProviderType: 'Default',\n  Table: '@Source2Tables@',\n  Columns: 'col1,col2,col3,col4,uniquecol',\n  keytomap: 'uniquecol')  \nOF @APPNAME@cachetype;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION orrs;\nUNDEPLOY APPLICATION orrs;\nDROP APPLICATION orrs CASCADE;\nCREATE APPLICATION orrs;\nCreate Source oraSource Using DatabaseReader\n(\n Username:'src_username',\n Password:'src_password',\n ConnectionURL: 'src_url',\n Tables:'QATEST.ORACLETOREDSHIFTIL1;QATEST.ORACLETOREDSHIFTIL2',\n FilterTransactionBoundaries:true,\n FetchSize:1000\n) Output To LCRStream;\n\nCREATE TARGET RSTarget USING RedshiftWriter\n\t(\n\t  ConnectionURL: 'tgt_url',\n\t  Username: 'tgt_username',\n\t  Password: 'tgt_pwrd',\n\t  bucketname: 'bucket_name',\n\t  --accesskeyId: 'access_key',\n\t  --secretaccesskey: 'secret_access',\n\t  S3IAMRole:'@IAMROLE@',\n\t  Tables: 'QATEST.ORACLETOREDSHIFT%,QATEST.ORACLETOREDSHIFT%',\n\t  uploadpolicy:'eventcount:1000,interval:1m'\n\t) INPUT FROM LCRStream;\n\t\nEND APPLICATION orrs;\nDEPLOY APPLICATION orrs;\nSTART APPLICATION orrs;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL ;\n\n\nCREATE OR REPLACE SOURCE DBSource USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',\n  Tables: 'QATEST.OracToCql_alldatatypes1',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE SOURCE DBSource2 USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',\n  Tables: 'QATEST.OracToCql_alldatatypes2',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE SOURCE DBSource3 USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',\n  Tables: 'QATEST.OracToCql_alldatatypes3',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'TEST.user_chkpoint',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',\n  Tables: 'QATEST.OracToCql_alldatatypes1,test.oractocq_alldatatypes columnmap(NumericToBigint=NumericToBigint);QATEST.OracToCql_alldatatypes2,test.oractocq_alldatatypes columnmap(NumericToBigint=NumericToBigint);QATEST.OracToCql_alldatatypes3,test.oractocq_alldatatypes columnmap(NumericToBigint=NumericToBigint)',\n  IgnorableExceptionCode: 'PRIMARY KEY',\n  Password: 'cassandra',\n  Password_encrypted: false\n )\nINPUT FROM Oracle_ChangeDataStream;\n\ncreate Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\n\nEND APPLICATION DBRTOCW;\ndeploy application DBRTOCW in default;\nstart DBRTOCW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE TYPE @APPNAME@type1 (\n companyName java.lang.String,\n merchantId java.lang.String,\n city java.lang.String);\n\nCREATE STREAM @APPNAME@TypedStream OF @APPNAME@type1 PARTITION BY city;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (\n  wildcard: '',\n  positionByEOF: false,\n  directory: ''\n  )\nPARSE USING DSVParser (\nheader:'true'\n)\nOUTPUT TO @APPNAME@Stream;\n\nCREATE OR REPLACE CQ @APPNAME@CQ\nINSERT INTO @APPNAME@TypedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\") as companyName,\nTO_STRING(data[1]) as merchantID,\nTO_STRING(data[10]) as city\nFROM @APPNAME@Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_jmstrgt1 USING JMSWriter (\n  QueueName: '',\n  UserName: '',\n  Password: '',\n  Ctx: '',\n  Provider: ''\n  )\nFORMAT USING DSVFormatter()\nINPUT FROM @APPNAME@TypedStream;\n\nCREATE OR REPLACE TARGET @APPNAME@_jmstrgt2 USING JMSWriter (\n  QueueName: '',\n  UserName: '',\n  Password: '',\n  Ctx: '',\n  Provider: ''\n  )\nFORMAT USING JSONFormatter()\nINPUT FROM @APPNAME@TypedStream;\n\nCREATE OR REPLACE TARGET @APPNAME@_jmstrgt3 USING JMSWriter (\n  QueueName: '',\n  UserName: '',\n  Password: '',\n  Ctx: '',\n  Provider: ''\n  )\nFORMAT USING DSVFormatter()\nINPUT FROM @APPNAME@TypedStream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@;\nCREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream;\n\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.DeltaLakeWriter (\n  personalAccessToken: '',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.student',\n  adapterName: 'DeltaLakeWriter',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:1,interval:10s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @APP_NAME@_Stream;\n\nEND APPLICATION @APP_NAME@;\nDEPLOY APPLICATION @APP_NAME@;\nSTART APPLICATION @APP_NAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@;\n\nCREATE OR REPLACE SOURCE @parquetsrc@ USING Global.HDFSReader (\n  wildcard: '',\n  directory: '',\n  hadoopurl: '',\n  hadoopconfigurationpath: '',\n  positionbyeof: false )\n  PARSE USING ParquetParser (\n   )\nOUTPUT TO @appname@Stream;\n\nCREATE CQ @appname@CQ\nINSERT INTO @appname@CqOut\n    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;\n\nCREATE OR REPLACE TARGET @filetarget@ USING FileWriter (\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:1,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING ParquetFormatter  (\nschemaFileName: 'ParquetFileSchema'\n)\nINPUT FROM @appname@CqOut;\n\ncreate Target @s3target@ using S3Writer(\n    bucketname:'',\n    objectname:'',\n    uploadpolicy:'',\n    foldername:''\n)\nformat using ParquetFormatter (\nschemaFileName: 'ParquetS3Schema'\n)\ninput from @appname@CqOut;\n\ncreate Target @blobtarget@ using AzureBlobWriter(\n\taccountname:'',\n\taccountaccesskey:'',\n\tcontainername:'',\n    blobname:'',\n\tfoldername:'',\n\tuploadpolicy:'EventCount:10,interval:5s'\n)\nformat using ParquetFormatter (\nschemaFileName: 'ParquetAzureSchema'\n)\nINPUT FROM @appname@CqOut;\n\nCREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (\n    bucketname:'',\n    objectname:'',\n    foldername:'',\n    projectId:'',\n    uploadPolicy:''\n)\nformat using ParquetFormatter (\nschemaFileName: 'ParquetGCSSchema'\n)\nINPUT FROM @appname@CqOut;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "UNDEPLOY APPLICATION admin.BadDeployGroup;\nDROP APPLICATION admin.BadDeployGroup cascade;\n\nCREATE APPLICATION BadDeployGroup;\n\n-- This sample application demonstrates how WebAction could be used\n-- by a retail chain to generate real-time reports on products and\n-- stores and to send alerts of unusual activity.\n\n\nCREATE FLOW BadSourceFlow;\n\n-- RetailDataSource is the primary data source for this application.\n--\n-- ParseOrderData discards the fields not needed by this application and puts the\n-- data into the appropriate Java types.\n--\n-- ParseOrderData outputs to RetailOrders stream, the start point of the\n-- RetailProductFlow and RetailStoreFlow flows.\n\nCREATE SOURCE RetailDataSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'retaildata2M.csv',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO Orders;\n\n-- A stream's type must be declared before the stream, and a CQ's\n-- output stream must be defined before the CQ. Hence type-stream-CQ\n-- sequences like the following are very common.\n\n-- output for ParseOrderData\nCREATE TYPE OrderType(\n  storeId      String,\n  orderId      String,\n  sku          String,\n  orderAmount  double,\n  dateTime     DateTime,\n  hourValue    int,\n  state        String,\n  city         String,\n  zip          String\n);\nCREATE STREAM RetailOrders Of OrderType;\n\nCREATE CQ ParseOrderData\nINSERT INTO RetailOrders\nSELECT  data[0],\n        data[6],\n        data[7],\n        TO_DOUBLE(SRIGHT(data[8],1)),\n        TO_DATE(data[9],'yyyyMMddHHmmss'),\n        DHOURS(TO_DATE(data[9],'yyyyMMddHHmmss')),\n        data[3],\n        data[2],\n        data[4]\nFROM Orders;\n\nEND FLOW BadSourceFlow;\n\n\nCREATE FLOW BadProductFlow;\n\n-- This flow populates the ProductActivity WAction store, which\n-- provides data for dashboard reports on sales by product.\n\n-- defines context for WAction store\nCREATE TYPE ProductActivityContext(\n  sku String  KEY,\n  OrderCount int,\n  SalesAmount double,\n  StartTime DateTime\n);\n\n-- defines event types for Waction store\nCREATE TYPE ProductTrackingType (\n  sku String KEY,\n  OrderCount int,\n  SalesAmount double,\n  StartTime DateTime\n);\n\nCREATE WACTIONSTORE ProductActivity\nCONTEXT OF ProductActivityContext\nEVENT TYPES ( ProductTrackingType )\nPERSIST NONE USING ( ) ;\n\n-- input for GetProductActivity\nCREATE JUMPING WINDOW ProductData_15MIN\nOVER RetailOrders\nKEEP WITHIN 15 MINUTE ON dateTime\nPARTITION BY sku;\n\nCREATE STREAM ProductTrackingStream OF ProductTrackingType;\n\n-- aggregates data and populates WAction store\nCREATE CQ GetProductActivity\nINSERT INTO ProductTrackingStream\nSELECT pd.sku, COUNT(*), SUM(pd.orderAmount), FIRST(pd.dateTime)\nFROM ProductData_15MIN pd\nGROUP BY pd.sku;\n\nCREATE CQ TrackProductActivity\nINSERT INTO ProductActivity\nSELECT sku, OrderCount, SalesAmount, StartTime\nFROM ProductTrackingStream\nLINK SOURCE EVENT;\n\nEND FLOW BadProductFlow;\n\n\nCREATE FLOW BadStoreFlow;\n\n-- This flow populates the StoreActivity WAction store, which provides\n-- data for dashboard reports on reports on stores, and sends alerts when\n-- sales volumes are higher or lower than expected.\n\n-- RetailStoreFlow part 1 - GetStoreActivity\n--\n-- The RetailData_5MIN window bounds the data so that the query can\n-- use the COUNT, FIRST, and SUM functions. (Aggregate functions cannot\n-- be used with unbound real-time data.)\n--\n-- The HourlyStoreSales_Cache cache provides historical averages for the\n-- current hour for each merchant\n\n-- input for GetStoreActivity\nCREATE JUMPING WINDOW RetailData_5MIN\n     OVER RetailOrders\n     KEEP WITHIN 5 MINUTE ON dateTime\n     PARTITION BY storeId;\n\n-- input for GetStoreActivity\nCREATE TYPE StoreHourlyAvg(\n  storeId String,\n  hourValue int,\n  hourlyAvg int,\n  hourlyItemCnt int\n);\nCREATE CACHE HourlyStoreSales_Cache using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'storehourlyData.txt',\n  header: Yes,\n  columndelimiter: ','\n) QUERY (keytomap:'storeId') OF StoreHourlyAvg;\n\n-- output for GetStoreActivity\nCREATE TYPE StoreOrdersTrackingType (\n  storeId String KEY,\n  state String,\n  city  String,\n  zip   String,\n  StartTime DateTime,\n  ordersCount int,\n  salesAmount double,\n  hourlyAvg int,\n  upperLimit double,\n  lowerLimit double,\n  category String,\n  status String\n);\nCREATE STREAM StoreOrdersTracking OF StoreOrdersTrackingType;\n\nCREATE CQ GetStoreActivity\nINSERT INTO StoreOrdersTracking\nSELECT rd.storeId, rd.state, rd.city, rd.zip, first(rd.dateTime),\n       COUNT(rd.storeId), SUM(rd.orderAmount), l.hourlyAvg/6,\n       l.hourlyAvg/6 + l.hourlyAvg/8,\n       l.hourlyAvg/6 - l.hourlyAvg/10,\n       '<NOTSET>', '<NOTSET>'\nFROM RetailData_5MIN rd, HourlyStoreSales_Cache l\nWHERE rd.storeId = l.storeId AND rd.hourValue = l.hourValue\nGROUP BY rd.storeId;\n-- This query aggregates five minutes' worth of data for each\n-- merchant, calculating the total transaction count and amount, and\n-- calculates the upperLimit and lowerLimit values based on the\n-- historical average transaction count for the current hour of the\n-- day from the HourlyStoreSales_Cache cache. The category and status fields\n-- are left unset to be populated by the next query.\n\n\n-- RetailStoreFlow part 2 - GetStoreStatus\n--\n-- This query sets the count values used by the dashboard map and the\n-- status values used to trigger alerts.\n\n-- uses type previously defined for StoreOrdersTracking\nCREATE STREAM StoreOrdersTracking_Status OF StoreOrdersTrackingType;\n\nCREATE CQ GetStoreStatus\nINSERT INTO StoreOrdersTracking_Status\nSELECT storeId, state, city, zip, StartTime,\n       ordersCount, salesAmount, hourlyAvg, upperLimit, lowerLimit,\n       CASE\n         WHEN salesAmount > (upperLimit + 2000) THEN 'HOT'\n         WHEN salesAmount > upperLimit THEN 'MEDIUM'\n         WHEN salesAmount < lowerLimit THEN 'COLD'\n         ELSE 'COOL' END,\n       CASE\n         WHEN salesAmount > upperLimit THEN 'TOOHIGH'\n         WHEN salesAmount < lowerLimit THEN 'TOOLOW'\n         ELSE 'OK' END\nFROM StoreOrdersTracking;\n\n\n-- RetailStoreFlow part 3 - create and populate the StoreActivity WAction store\n\n-- input for CQ TrackStoreActivity\nCREATE TYPE StoreNameData(\n  storeId       String KEY,\n  storeName     String\n);\nCREATE CACHE StoreNameLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'StoreNames.csv',\n  header: Yes,\n  columndelimiter: ','\n) QUERY(keytomap:'storeId') OF StoreNameData;\n\n-- input for CQ TrackStoreActivity\nCREATE TYPE RetailUSAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\nCREATE CACHE ZipCodeLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'USAddresses.txt',\n  header: Yes,\n  columndelimiter: '\t'\n) QUERY (keytomap:'zip') OF RetailUSAddressData;\n\n-- defines WAction store context\nCREATE TYPE StoreActivityContext(\n  storeId String KEY,\n  StartTime DateTime,\n  StoreName String,\n  Category String,\n  Status String,\n  OrderCount int,\n  salesamount double,\n  HourlyAvg int,\n  UpperLimit double,\n  LowerLimit double,\n  Zip String,\n  City String,\n  State String,\n  LatVal double,\n  LongVal double\n);\n\n-- StoreOrdersTrackingType previously defined for StoreOrdersTracking\nCREATE WACTIONSTORE StoreActivity\nCONTEXT OF StoreActivityContext\nEVENT TYPES (StoreOrdersTrackingType )\nPERSIST NONE USING ();\n\nCREATE CQ TrackStoreActivity\nINSERT INTO StoreActivity\nSELECT s.storeId,\n  s.StartTime,\n  n.storeName,\n  s.category,\n  s.status,\n  s.ordersCount,\n  s.salesAmount,\n  s.hourlyAvg,\n  s.upperLimit,\n  s.lowerLimit,\n  z.zip,\n  z.city,\n  s.state,\n  z.latVal,\n  z.longVal\nFROM StoreOrdersTracking_Status s, StoreNameLookup n, ZipCodeLookup z\nWHERE s.storeId = n.storeId AND s.zip = z.zip\nLINK SOURCE EVENT;\n\n\n-- RetailStoreFlow part 4 - send alerts\n\n\nCREATE STREAM RetailAlertStream OF Global.AlertEvent;\n\nCREATE CQ RetailRetailGenerateAlerts\nINSERT INTO RetailAlertStream\nSELECT n.storeName, s.storeId,\n        CASE\n          WHEN s.Status = 'OK' THEN 'info'\n          ELSE 'warning' END,\n        CASE\n          WHEN s.Status = 'OK' THEN 'cancel'\n          ELSE 'raise' END,\n        CASE\n          WHEN s.Status = 'OK' THEN 'Store ' + n.storeName + ' amount of $'+ s.salesAmount + ' is back between $' + s.lowerLimit + ' and $' +s.upperLimit\n          WHEN s.Status = 'TOOHIGH' THEN 'Store ' + n.storeName + ' amount of $'+ s.salesAmount + ' is above upper limit of $' + s.upperLimit\n          WHEN s.Status = 'TOOLOW' THEN 'Store ' + n.storeName + ' amount of $'+ s.salesAmount + ' is below lower limit of $' + s.lowerLimit\n          ELSE ''\n          END\nFROM StoreOrdersTracking_Status s, StoreNameLookup n\nWHERE s.storeId = n.storeId;\n\nEND FLOW BadStoreFlow;\n\n\n-- load dashboard visualization settings from file\n\n--CREATE VISUALIZATION BadDeployGroup \"Samples/Customer/RetailApp/RetailApp_visualization_settings.json\";\n\n-- The following statement defines the user and delivery method for alerts.\nCREATE SUBSCRIPTION BadAlertSub USING WebAlertAdapter( ) INPUT FROM RetailAlertStream;\n\n\nEND APPLICATION BadDeployGroup;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@;\n\ncreate source @SourceName1@ USING IncrementalBatchReader\n(\n  FetchSize: 10,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: @checkColumn@,\n  startPosition: '@startPosition@',\n  PollingInterval: '20sec'\n)\nOUTPUT TO @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:@targetsys@) input from @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter(\n  ConnectionURL:'@READER-URL@',\n  Username:'@READER-UNAME@',\n  Password:'@READER-PASSWORD@',\n  BatchPolicy:'Eventcount:1,Interval:1',\n  CommitPolicy:'Eventcount:1,Interval:1',\n  Checkpointtable:'RGRN_CHKPOINT',\n  Tables:'@WATABLES@,@WATABLES@_target'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate source @SourceName2@ USING IncrementalBatchReader\n(\nFetchSize: 10,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: @checkColumn1@,\n  startPosition: '@startPosition1@',\n  PollingInterval: '20sec'\n)\nOUTPUT TO @SRCINPUTSTREAM1@;\n\ncreate Target @targetsys1@ using SysOut(name:@targetsys1@) input from @SRCINPUTSTREAM1@;\n\nCREATE TARGET @targetName1@ USING DatabaseWriter(\n  ConnectionURL:'@READER-URL@',\n  Username:'@READER-UNAME@',\n  Password:'@READER-PASSWORD@',\n  BatchPolicy:'Eventcount:1,Interval:1',\n  CommitPolicy:'Eventcount:1,Interval:1',\n  Checkpointtable:'RGRN_CHKPOINT',\n  Tables:'@WATABLES_target'\n) INPUT FROM @SRCINPUTSTREAM1@;\n\nEND APPLICATION @APPNAME@;\n\nDEPLOY APPLICATION @APPNAME@;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nCREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()\nPARSE USING XMLParser (\n  rootnode: ''\n)\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()\nFORMAT USING JSONFormatter ()\nINPUT FROM @APPNAME@_Stream;\n\nEND APPLICATION @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@;\n\n\nCREATE SOURCE @APPNAME@_Source USING MySqlReader\n(\n  Compression: false,\n  FetchSize: 1,\n  SendBeforeImage: true,\n  ConnectionURL: 'mysql://localhost:3306',\n  DatabaseName: 'waction',\n  Tables: 'waction.test01',\n  Password: 'w@ct10n',\n  Password_encrypted: 'false',\n  Username: 'root'\n)\nOUTPUT TO @APPNAME@_Stream;\n\n\nCREATE TARGET @APPNAME@_Target USING SpannerWriter (\n\tTables: 'QATEST.%,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tBatchPolicy: 'EventCount: 1; Interval: 1s',\n\tinstanceId: 'qatest'\n) INPUT FROM @APPNAME@_stream;\n\ncreate Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_stream;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION jsonapp;\n\nCREATE TYPE JSONAccessLogEntry (\n    srcIp String KEY,\n    userId String,\n    sessionId String,\n    accessTime long,\n    request String,\n    code int,\n    size int,\n    referrer String,\n    userAgent String,\n    responseTime int\n);\nCREATE STREAM JSONSourceStream OF JSONAccessLogEntry;\n\nCREATE SOURCE JSONAccessLogSource USING FileReader(\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'FileWithJSONTest.json',\n  positionbyeof:false\n)\nparse using JSONParser (\n  eventType:'admin.JSONAccessLogEntry'\n) OUTPUT TO JSONSourceStream;\n\ncreate Target JSONDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/jsondata') input FROM JSONSourceStream;\n\nEND APPLICATION jsonapp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE CCBReader USING FileReader (\n  wildcard: '@WILDCARD@',\n  positionbyeof: false,\n  directory: '@TESTDIR@'\n  )\nPARSE USING Global.CobolCopybookParser (\n  copybookDialect: 'Mainframe',\n  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',\n  GroupPolicy: '@GRPPOLICY@',\n  TimeoutPolicy: '1s',\n  dataFileOrganization: 'FixedLength',\n  ProcessCopyBookFileAs: 'MultipleEvents',\n  skipIndent: 0,\n  dataFileFont: 'utf8',\n  CopybookFileFormat: 'USE_STANDARD_COLUMNS',\n  copybookSplit: 'None',\n  copybookFileName: '@CCBFILE@'\n   )\nOUTPUT TO CCBStream;\n\ncreate Target KafkaTarget using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'',\nTopic:'',\nMode: 'Sync',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;linger.ms=30000'\n)\nFORMAT USING JSONFormatter (\nmembers:'data')\ninput from CCBStream;\n\ncreate source KafkaSource using KafkaReader VERSION '2.1.0'(\nbrokerAddress:'',\n\tTopic:''\n)\nparse using JSONParser ()\noutput to KafkaStream;\n\nCREATE OR REPLACE TARGET JSONWriter USING FileWriter (\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:1,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING JSONFormatter ()\nINPUT FROM KafkaStream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@ on all in default;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME1@;\nundeploy application @APPNAME1@;\nstop application @APPNAME2@;\nundeploy application @APPNAME2@;\nstop application @APPNAME3@;\nundeploy application @APPNAME3@;\ndrop application @APPNAME1@ cascade;\ndrop application @APPNAME2@ cascade;\ndrop application @APPNAME3@ cascade;\n\nCREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\nCREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;\nCREATE OR REPLACE SOURCE @SourceName@ Using OracleReader\n(\n  Compression:true,\n  StartTimestamp:'null',\n  CommittedTransactions:true,\n  FilterTransactionBoundaries:true,\n  Password_encrypted:'false',\n  SendBeforeImage:true,\n  XstreamTimeOut:600,\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Tables:'qatest.oraMultiDownstream_src',\n  adapterName:'OracleReader',\n  Password:'qatest',\n  DictionaryMode:'OfflineCatalog',\n  FilterTransactionState:true,\n  connectionRetryPolicy:'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType:'LogMiner',\n  FetchSize: 1,\n  Username:'qatest',\n  OutboundServerProcessName:'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic:true,\n  CDDLAction:'Quiesce_Cascade',\n  CDDLCapture:'true'\n)OUTPUT TO @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME1@;\nDEPLOY APPLICATION @APPNAME1@;\nSTART APPLICATION @APPNAME1@;\n\nCREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  \n(\n  DatabaseProviderType:'Default',\n  CheckPointTable:'CHKPOINT',\n  PreserveSourceTransactionBoundary:'false',\n  Username:'qatest',\n  BatchPolicy:'EventCount:1',\n  CommitPolicy:'EventCount:1',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',\n  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',\n  Password:'qatest',\n  CDDLAction:'Process'\n) \nINPUT FROM @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME2@;\nDEPLOY APPLICATION @APPNAME2@;\nSTART APPLICATION @APPNAME2@;\n\n\nCREATE APPLICATION @APPNAME3@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE TARGET @targetName1@ USING DatabaseWriter  \n(\n  DatabaseProviderType:'Default',\n  CheckPointTable:'CHKPOINT',\n  PreserveSourceTransactionBoundary:'false',\n  Username:'qatest',\n  BatchPolicy:'EventCount:1',\n  CommitPolicy:'EventCount:1',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',\n  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',\n  Password:'qatest',\n  CDDLAction:'Process'\n) \nINPUT FROM @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME3@;\nDEPLOY APPLICATION @APPNAME3@;\nSTART APPLICATION @APPNAME3@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION KafkaWPTester.KWApp;\nSTOP APPLICATION KafkaWPTester.KRApp;\nUNDEPLOY APPLICATION KafkaWPTester.KWApp;\nUNDEPLOY APPLICATION KafkaWPTester.KRApp;\nDROP APPLICATION KafkaWPTester.KWApp CASCADE;\nDROP APPLICATION KafkaWPTester.KRApp CASCADE;\n\nCREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;\nCONNECT KafkaWPTester KafkaWPTester;\n\n\nCREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE SOURCE CSVSource USING FileReader (\n\tdirectory:'/Users/saranyad/Product/IntegrationTests/TestData/kafka_tmp',\n    WildCard:'mybanks*',\n\tpositionByEOF:false,\n        Charset:'UTF-8'\n)\nPARSE USING DSVParser (\n  columndelimiter:',',\n  ignoreemptycolumn:'Yes',\n  quoteset:'[]~\"',\n  separator:'~'\n)\nOUTPUT TO FileStream;\n\n\nCREATE TYPE AccessLogType(\nCol1 String,\nCol2 String\n);\n\nCREATE STREAM TypedAccessLogStream1 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream2 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream3 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream4 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream5 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream6 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream7 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream8 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream9 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream10 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream11 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream12 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream13 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream14 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream15 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream16 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream17 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream18 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream19 OF AccessLogType;\nCREATE STREAM TypedAccessLogStream20 OF AccessLogType;\n\nCREATE CQ AcceeslogCQ\nINSERT INTO TypedAccessLogStream\nSELECT \nTO_STRING(data[0]) as Col1,\nTO_STRING(data[1]) as Col2\nFROM FileStream ; \n\nCREATE CQ AcceeslogCQ1\nINSERT INTO TypedAccessLogStream1\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS WHERE FS.Col1 = '1'; \n\n\ncreate Target KW1 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test01',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream1;\n\nCREATE CQ AcceeslogCQ2\nINSERT INTO TypedAccessLogStream2\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '2'; \n\n\ncreate Target KW2 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test02',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream2;\n\nCREATE CQ AcceeslogCQ3\nINSERT INTO TypedAccessLogStream3\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '3'; \n\n\ncreate Target KW3 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test03',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream3;\n\nCREATE CQ AcceeslogCQ4\nINSERT INTO TypedAccessLogStream4\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '4'; \n\n\ncreate Target KW4 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test04',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream4;\n\nCREATE CQ AcceeslogCQ5\nINSERT INTO TypedAccessLogStream5\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '5'; \n\n\ncreate Target KW5 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test05',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream5;\n\nCREATE CQ AcceeslogCQ6\nINSERT INTO TypedAccessLogStream6\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '6'; \n\n\ncreate Target KW6 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test06',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream6;\n\nCREATE CQ AcceeslogCQ7\nINSERT INTO TypedAccessLogStream7\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '7'; \n\n\ncreate Target KW7 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test07',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream7;\n\nCREATE CQ AcceeslogCQ8\nINSERT INTO TypedAccessLogStream8\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '8'; \n\n\ncreate Target KW8 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test08',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream8;\n\nCREATE CQ AcceeslogCQ9\nINSERT INTO TypedAccessLogStream9\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '9'; \n\n\ncreate Target KW9 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test09',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream9;\n\nCREATE CQ AcceeslogCQ10\nINSERT INTO TypedAccessLogStream10\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '10'; \n\n\ncreate Target KW10 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test10',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream10;\n\nCREATE CQ AcceeslogCQ11\nINSERT INTO TypedAccessLogStream11\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '11'; \n\n\ncreate Target KW11 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test11',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream11;\n\nCREATE CQ AcceeslogCQ12\nINSERT INTO TypedAccessLogStream12\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '12'; \n\n\ncreate Target KW12 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test12',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream12;\n\nCREATE CQ AcceeslogCQ13\nINSERT INTO TypedAccessLogStream13\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '13'; \n\n\ncreate Target KW13 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test13',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream13;\n\nCREATE CQ AcceeslogCQ14\nINSERT INTO TypedAccessLogStream14\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '14'; \n\n\ncreate Target KW14 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test14',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream14;\n\nCREATE CQ AcceeslogCQ15\nINSERT INTO TypedAccessLogStream15\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '15'; \n\n\ncreate Target KW15 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test15',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream15;\n\nCREATE CQ AcceeslogCQ16\nINSERT INTO TypedAccessLogStream16\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '16'; \n\n\ncreate Target KW16 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test16',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream16;\n\nCREATE CQ AcceeslogCQ17\nINSERT INTO TypedAccessLogStream17\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '17'; \n\n\ncreate Target KW17 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test17',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream17;\n\nCREATE CQ AcceeslogCQ18\nINSERT INTO TypedAccessLogStream18\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '18'; \n\n\ncreate Target KW18 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test18',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream18;\n\nCREATE CQ AcceeslogCQ19\nINSERT INTO TypedAccessLogStream19\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '19'; \n\n\ncreate Target KW19 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test19',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream19;\n\nCREATE CQ AcceeslogCQ20\nINSERT INTO TypedAccessLogStream20\nSELECT FS.Col1, FS.Col2\nFROM TypedAccessLogStream FS\nWHERE FS.Col1 = '20'; \n\n\ncreate Target KW20 using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test20',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream20;\n\n\nEND APPLICATION KWApp;\n\nDEPLOY APPLICATION KWApp on any in default;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-- Kafka Reader Apps\n\nCREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;\n\nCREATE SOURCE KR1 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test01',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream1;\n\n\n\nCREATE SOURCE KR2 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test02',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream2;\n\n\nCREATE SOURCE KR3 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test03',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream3;\n\n\nCREATE SOURCE KR4 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test04',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream4;\n\n\nCREATE SOURCE KR5 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test05',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream5;\n\n\nCREATE SOURCE KR6 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test06',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream6;\n\n\nCREATE SOURCE KR7 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test07',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream7;\n\n\nCREATE SOURCE KR8 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test08',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream8;\n\n\nCREATE SOURCE KR9 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test09',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream9;\n\n\nCREATE SOURCE KR10 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test10',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream10;\n\n\nCREATE SOURCE KR11 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test11',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream11;\n\n\n\nCREATE SOURCE KR12 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test12',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream12;\n\n\nCREATE SOURCE KR13 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test13',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream13;\n\n\nCREATE SOURCE KR14 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test14',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream14;\n\n\nCREATE SOURCE KR15 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test15',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream15;\n\n\nCREATE SOURCE KR16 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test16',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream16;\n\n\nCREATE SOURCE KR17 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test17',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream17;\n\n\nCREATE SOURCE KR18 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test18',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream18;\n\n\nCREATE SOURCE KR19 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test19',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream19;\n\n\nCREATE SOURCE KR20 USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093',\n        Topic:'V8_test20',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream20;\n\n\n\n\nCREATE TARGET DumpKafkaReaderStream1 USING FileWriter(\n  name:KafkaROuput1,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_1',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream1;\n\n\nCREATE TARGET DumpKafkaReaderStream2 USING FileWriter(\n  name:KafkaROuput2,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_2',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream2;\n\n\nCREATE TARGET DumpKafkaReaderStream3 USING FileWriter(\n  name:KafkaROuput3,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_3',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream3;\n\n\nCREATE TARGET DumpKafkaReaderStream4 USING FileWriter(\n  name:KafkaROuput4,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_4',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream4;\n\n\nCREATE TARGET DumpKafkaReaderStream5 USING FileWriter(\n  name:KafkaROuput5,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_5',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream5;\n\n\nCREATE TARGET DumpKafkaReaderStream6 USING FileWriter(\n  name:KafkaROuput6,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_6',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream6;\n\n\nCREATE TARGET DumpKafkaReaderStream7 USING FileWriter(\n  name:KafkaROuput7,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_7',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream7;\n\n\nCREATE TARGET DumpKafkaReaderStream8 USING FileWriter(\n  name:KafkaROuput8,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_8',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream8;\n\n\nCREATE TARGET DumpKafkaReaderStream9 USING FileWriter(\n  name:KafkaROuput9,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_9',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream9;\n\n\nCREATE TARGET DumpKafkaReaderStream10 USING FileWriter(\n  name:KafkaROuput10,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_10',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream10;\n\n\nCREATE TARGET DumpKafkaReaderStream11 USING FileWriter(\n  name:KafkaROuput11,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_11',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream11;\n\n\nCREATE TARGET DumpKafkaReaderStream12 USING FileWriter(\n  name:KafkaROuput12,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_12',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream12;\n\n\nCREATE TARGET DumpKafkaReaderStream13 USING FileWriter(\n  name:KafkaROuput13,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_13',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream13;\n\n\nCREATE TARGET DumpKafkaReaderStream14 USING FileWriter(\n  name:KafkaROuput14,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_14',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream14;\n\n\nCREATE TARGET DumpKafkaReaderStream15 USING FileWriter(\n  name:KafkaROuput15,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_15',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream15;\n\n\nCREATE TARGET DumpKafkaReaderStream16 USING FileWriter(\n  name:KafkaROuput16,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_16',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream16;\n\n\nCREATE TARGET DumpKafkaReaderStream17 USING FileWriter(\n  name:KafkaROuput17,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_17',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream17;\n\n\nCREATE TARGET DumpKafkaReaderStream18 USING FileWriter(\n  name:KafkaROuput18,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_18',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream18;\n\n\nCREATE TARGET DumpKafkaReaderStream19 USING FileWriter(\n  name:KafkaROuput19,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_19',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream19;\n\n\nCREATE TARGET DumpKafkaReaderStream20 USING FileWriter(\n  name:KafkaROuput20,\n  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_20',\nflushpolicy : 'flushcount:1',\nrolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter()\n INPUT FROM KafkaReaderStream20;\n\nEND APPLICATION KRApp;\nDEPLOY APPLICATION KRApp on any in default;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\ncreate flow @APPNAME@_agentflow;\n\nCreate Source @SOURCE@\n        Using OracleReader\n(\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: 'qatest.test77',\n  FetchSize:1,\n  QueueSize:2000,\n  CommittedTransactions:true,\n  Compression:false\n)\nOutput To @STREAM@;\n\nend flow @APPNAME@_agentflow;\n\ncreate flow @APPNAME@_serverflow;\n\ncreate Target @TARGET@ using AzureblobWriter(\n    accountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:7'\n)\nformat using DSVFormatter (\n)\ninput from @STREAM@;\n\nend flow @APPNAME@_serverflow;\n\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  --Password_encrypted: false\n ) \nINPUT FROM @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCreate Source @APPNAME@_src Using OracleReader\n(\n Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n   _h_ReturnDateTimeAs:'ZonedDateTime'\n) Output To @APPNAME@_stream;\n\ncreate Target @APPNAME@_tgt using FileWriter(\n  filename:'CompressedMerchant.gz',\n  directory:'/logs/',\n  rolloverpolicy:'EventCount:10000'\n)\nformat using ParquetFormatter (\n\tschemaFileName:'@FILENAME@',\n\tFormatAs:'@FORMATAS@'\n)\ninput from @APPNAME@_stream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME1@;\nundeploy application @APPNAME1@;\nstop application @APPNAME2@;\nundeploy application @APPNAME2@;\ndrop application @APPNAME1@ cascade;\ndrop application @APPNAME2@ cascade;\n\n\nCREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\nCREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;\nCREATE OR REPLACE SOURCE @SourceName@ Using OracleReader\n(\n  Compression:true,\n  StartTimestamp:'null',\n  FetchSize:1,\n  CommittedTransactions:true,\n  QueueSize:2048,\n  FilterTransactionBoundaries:true,\n  Password_encrypted:'false',\n  SendBeforeImage:true,\n  XstreamTimeOut:600,\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',\n  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE',\n  adapterName:'OracleReader',\n  Password:'qatest',\n  DictionaryMode:'OfflineCatalog',\n  FilterTransactionState:true,\n  connectionRetryPolicy:'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType:'LogMiner',\n  Username:'qatest',\n  OutboundServerProcessName:'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic:true,\n  CDDLAction:'Quiesce_Cascade',\n  CDDLCapture:'true'\n)OUTPUT TO @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME1@;\nDEPLOY APPLICATION @APPNAME1@;\nSTART APPLICATION @APPNAME1@;\n\nCREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE TARGET @targetName@ USING BigqueryWriter  \n(\n  serviceAccountKey:'/Users/hariharasudhan/Downloads/google-gcs.json',\n  projectId:'striimqa-214712',\n  datalocation:'US',\n  Tables:'public.dbr_pg234567890123456789source1,public.dbr_pg234567890123456789Target1;public.dbr_pg234567890123456789source2,public.dbr_pg234567890123456789Target2',\n  BatchPolicy:\"eventCount:1,Interval:90\",\n  CDDLAction:'Process'\n) \nINPUT FROM @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME2@;\nDEPLOY APPLICATION @APPNAME2@;\nSTART APPLICATION @APPNAME2@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop CSVToHBase;\nundeploy application CSVToHBase;\ndrop application CSVToHBase cascade;\n\nCREATE APPLICATION CSVToHBase;\n\nCREATE OR REPLACE SOURCE CSVPoller USING FileReader ( \n\tdirectory:'/Users/ravipathak/webactionrepo/Product',\n\tWildCard:'smallpos.csv',\n\tpositionByEOF:false\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCREATE OR REPLACE TYPE CSVStream_Type  ( BUSINESS_NAME java.lang.String KEY, \nMERCHANT_ID java.lang.String, \nPRIMARY_ACCOUNT_NUMBER java.lang.String  \n ) ;\n\nCREATE OR REPLACE STREAM CSVTypeStream OF CSVStream_Type;\n\nCREATE OR REPLACE CQ CQ1 \nINSERT INTO CSVTypeStream\nSELECT data[0],data[1],data[2]\nFROM CsvStream;\n\nCREATE OR REPLACE TARGET Target1 USING SysOut ( \n  name: \"dstream\"\n ) \nINPUT FROM CsvStream;\n\nCREATE OR REPLACE TARGET Target2 using HBaseWriter(\n HBaseConfigurationPath:\"/Users/ravipathak/soft/hbase-1.1.5/conf/hbase-site.xml\",\n  Tables: \"maprtest.maprdata\",\n  --FamilyNames: \"maprdata\",\n  BatchPolicy: \"eventCount:1\")\nINPUT FROM CSVTypeStream;\n\nEND APPLICATION CSVToHBase;\n\ndeploy application CSVToHBase;\nstart CSVToHBase;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @AppName@;\nUndeploy application @AppName@;\nAlter application @AppName@;\n\nCREATE FLOW @AppName@_Agent_flow;\n\nCREATE OR REPLACE SOURCE @AppName@_FileReaderSource USING FileReader (\nwildcard: 'posdata.csv', \n  positionByEOF: false, \n  blocksize: 10100,\n  rolloverpolicy: 'EventCount:100,Interval:30s', \n  directory: '@dir@' ) \nPARSE USING DSVParser ( \n  trimquote: false, \n  header: 'Yes' ) \nOUTPUT TO @AppName@_CsvStream;\n\nEND FLOW @AppName@_Agent_flow;\n\nalter application @AppName@ recompile;\nDEPLOY APPLICATION @AppName@ with @AppName@_Agent_flow on any in AGENTS;\nstart application @AppName@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP ModifyBeforeDataTester.ModifyBeforeData;\nUNDEPLOY APPLICATION ModifyBeforeDataTester.ModifyBeforeData;\nDROP APPLICATION ModifyBeforeDataTester.ModifyBeforeData CASCADE;\n\nCREATE APPLICATION ModifyBeforeData;\n\ncreate source GGTrailSource using FileReader (\n    directory:'@TEST-DATA-PATH@/OGG/oracle_alltypes_122',\n    WildCard:'ld*',\n    positionByEOF:false\n) parse using GGTrailParser (\n    FilterTransactionBoundaries: true,\n    metadata:'@TEST-DATA-PATH@/OGG/oracle_alltypes_122/def_oracle_alltypes122.def',\n    compression:false\n)\nOUTPUT TO SourceStream;\n\nCREATE STREAM ModifiedBeforeStream OF Global.WAEvent;\nCREATE STREAM ModifiedDataStream OF Global.WAEvent;\n\nCREATE OR REPLACE CQ ModifierBeforeCQ\nINSERT INTO ModifiedBeforeStream\nSELECT * FROM SourceStream\nwhere not(before is null)\nmodify(before[1] = data[1].toString().replaceAll(\".\", \"B\"));\n\nCREATE OR REPLACE CQ ModifierDataCQ\nINSERT INTO ModifiedDataStream\nSELECT * FROM SourceStream\nwhere not(before is null)\nmodify(data[1] = before[1].toString().replaceAll(\".\", \"D\"));\n\n-- Generate the final filtered stream to write to the waction store.\nCREATE OR REPLACE CQ CopyBeforeCQ\nINSERT INTO FilteredBeforeStream\nSELECT *\nFROM (SELECT TO_STRING(before[1]) as tcolBefore\n      FROM ModifiedBeforeStream) as src\nwhere src.tcolBefore like 'BBBBBB%';\n\nCREATE OR REPLACE CQ CopyDataCQ\nINSERT INTO FilteredDataStream\nSELECT *\nFROM (SELECT TO_STRING(data[1]) as tcolData\n      FROM ModifiedDataStream) as src\nwhere src.tcolData like \"DDDDDD%\";\n\n-- Need duplicate types due to a limitation of Waction Stores: two Waction Stores\n-- should not share the same type for CONTEXT OF.\nCREATE TYPE WactionType1 (\n  colTest String KEY\n);\n\nCREATE TYPE WactionType2 (\n  colTest String KEY\n);\n\nCREATE WACTIONSTORE WactionsBefore CONTEXT OF WactionType1\nEVENT TYPES ( WactionType1 )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE WactionsData CONTEXT OF WactionType2\nEVENT TYPES ( WactionType2 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions1\nINSERT INTO WactionsBefore\nSELECT tcolBefore as colTest\nFROM FilteredBeforeStream;\n\nCREATE CQ InsertWactions2\nINSERT INTO WactionsData\nSELECT tcolData as colTest\nFROM FilteredDataStream;\n\nEND APPLICATION ModifyBeforeData;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  --Password_encrypted: false\n )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION ORACLETOBIGQUERY;\nUNDEPLOY APPLICATION ORACLETOBIGQUERY;\nDROP APPLICATION ORACLETOBIGQUERY CASCADE;\n\n--create application \nCREATE APPLICATION OracleToBigquery RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE OracleSource USING OracleReader (\n ConnectionURL: '192.168.123.30:1521:ORCL',\n Tables: 'QATEST.E1PLOADTEST',\n Username: 'qatest',\n Password: 'qatest',\n FetchSize:100,\n OnlineCatalog:true,\n QueueSize:2000,\n CommittedTransactions:true,\n Compression:false\n) OUTPUT TO CDCStream;\n\nCREATE OR REPLACE TARGET bqtables using BigqueryWriter(\n BQServiceAccountConfigurationPath:\"/Users/ravipathak/Downloads/bqtest-e287bcb47998.json\",\n projectId:\"bqtest-158706\",\n Tables: \"QATEST.E1PLOADTEST,issues.DEV11070\",\n BatchPolicy: \"eventCount:100,Interval:1\")\nINPUT FROM CDCStream;\n\nCREATE OR REPLACE TARGET T1 using SysOut(\nname : \"some text\"\n)\nINPUT FROM CDCStream;\n\nEND APPLICATION OracleToBigquery;\n\nDEPLOY APPLICATION OracleToBigquery;\nSTART APPLICATION OracleToBigquery;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION @AppName@ USE EXCEPTIONSTORE TTL : '7d';\n\nCreate Source @AppName@_source Using OracleReader(\n  Username:'@username@',\n  Password:'@password@',\n  ConnectionURL:'@url@',\n  Tables:'@srctableName@',\n  Fetchsize:1\n)\nOutput To @AppName@_Stream;\nCREATE OR REPLACE TARGET @AppName@_Target USING Global.SnowflakeWriter (\n  streamingUpload: 'false',\n  useConnectionProfile:'true',\n  connectionProfileName: '@CP@',\n  CDDLAction: 'Process',\n  optimizedMerge: 'false',\n  columnDelimiter: '|',\n  tables: '@srctableName@,@trgtableName@',\n  appendOnly: 'false',\n  uploadPolicy: 'eventcount:1000,interval:60s',\n  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',\n  externalStageType: 'Local',\n  adapterName: 'SnowflakeWriter',\n  fileFormatOptions: 'null_if = \\\"\\\"' )\nINPUT FROM @AppName@_Stream;\nEND APPLICATION @AppName@;\nDeploy application @AppName@;\nStart application @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application APP_HEARTBEATS;\nundeploy application APP_HEARTBEATS;\ndrop application APP_HEARTBEATS cascade;\n\nCREATE APPLICATION APP_HEARTBEATS;\n\nCREATE OR REPLACE CQ CQ_HB_90SEC \nINSERT INTO STREAM_CQ_HB_90SEC \nselect makeWAEvent(dnow()) as dummy from heartbeat(interval 90 second) h;\n\nCREATE OR REPLACE CQ CQ_HB_10SEC \nINSERT INTO STREAM_CQ_HB_10SEC \nselect makeWAEvent(dnow()) as dummy from heartbeat(interval 10 second) h;\n\nCREATE OR REPLACE CQ CQ_HB_1MIN \nINSERT INTO STREAM_CQ_HB_1MIN \nselect makeWAEvent(dnow()) as dummy from heartbeat(interval 1 minute) h;\n\nCREATE OR REPLACE CQ CQ_HB_30SEC \nINSERT INTO STREAM_CQ_HB_30SEC \nselect makeWAEvent(dnow()) as dummy from heartbeat(interval 30 second) h;\n\nCREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_90SEC OVER STREAM_CQ_HB_90SEC \nKEEP 1 ROWS;\n\nCREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_10SEC OVER STREAM_CQ_HB_10SEC \nKEEP 1 ROWS;\n\nCREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_1MIN OVER STREAM_CQ_HB_1MIN \nKEEP 1 ROWS;\n\nCREATE OR REPLACE JUMPING WINDOW JUMP_WND_1EVT_30SEC OVER STREAM_CQ_HB_30SEC \nKEEP 1 ROWS;\n\nEND APPLICATION APP_HEARTBEATS;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application AzureApp;\nundeploy application AzureApp;\ndrop application AzureApp cascade;\n\ncreate application AzureApp\nRECOVERY 10 second interval;\ncreate source CSVSource using FileReader (\n\tdirectory:'@DIR@',\n\tWildCard:'@WILDCARD@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  curr String,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       data[6],\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target BlobT using AzureBlobWriter(\n\taccountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:50,interval:5s'\n)\nformat using JSONFormatter (\n)\ninput from TypedCSVStream;\nend application AzureApp;\ndeploy application AzureApp in default;\nstart application AzureApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 4 on four node all server cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS\n-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS\n--\n\nSTOP APPLICATION N4S4CR4Tester.N4S4CRTest4;\nUNDEPLOY APPLICATION N4S4CR4Tester.N4S4CRTest4;\nDROP APPLICATION N4S4CR4Tester.N4S4CRTest4 CASCADE;\nCREATE APPLICATION N4S4CRTest4 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN4S4CRTest4;\n\nCREATE SOURCE CsvSourceN4S4CRTest4 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN4S4CRTest4;\n\nCREATE FLOW DataProcessingN4S4CRTest4;\n\nCREATE TYPE CsvDataN4S4CRTest4 (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE TYPE WactionDataN4S4CRTest4 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\n\nCREATE STREAM DataStream OF CsvDataN4S4CRTest4;\n\nCREATE CQ CsvToDataN4S4CRTest4\nINSERT INTO DataStream\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsN4S4CRTest4 CONTEXT OF WactionDataN4S4CRTest4\nEVENT TYPES ( CsvDataN4S4CRTest4 )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO WactionsN4S4CRTest4\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nCREATE CQ Data6ToWaction\nINSERT INTO WactionsN4S4CRTest4\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream6Minutes p;\n\nEND FLOW DataProcessingN4S4CRTest4;\n\nEND APPLICATION N4S4CRTest4;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create or replace type @STREAM@details(\nC_CUSTKEY int,\nC_MKTSEGMENT String,\nC_NATIONKEY int,\nC_NAME String,\nC_ADDRESS String,\nC_PHONE String,\nC_ACCTBAL int,\nC_COMMENT String\n);\n\ncreate or replace stream @STREAM@_TYPED of @STREAM@details;\n\nCreate or replace CQ @STREAM@detailsCQ\ninsert into @STREAM@_TYPED\nselect \nto_int(data[0]),data[1],to_int(data[2]),data[3],data[4],data[5],to_int(data[6]),data[7]\nfrom @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  --Password_encrypted: false\n ) \nINPUT FROM @STREAM@_TYPED;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@1;\nundeploy application @APPNAME@1;\nstop application @APPNAME@2;\nundeploy application @APPNAME@2;\n\nDROP STREAM @APPNAME@_STREAM;\nDROP APPLICATION @APPNAME@1 CASCADE;\nDROP APPLICATION @APPNAME@2 CASCADE;\n\ndrop propertyset PGtoPGPlatfm_App_KafkaPropset;\ndrop stream  PGToPGPlatfm_Stream CASCADE;\n\n\nCREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\nCREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;\n\t\t\t\t\t\nCREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';\ncreate application @APPNAME@1 recovery 5 SECOND INTERVAL;\n\ncreate or replace stream @STREAM@2 of Global.waevent;\n\nCREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using PostgreSQLReader(\n  ReplicationSlotName: 'slotname',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: '$table1',\n  ExcludedTables:'public.chkpoint'\n ) OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_AGENTFLOW1;\n\nCREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using PostgreSQLReader( \n  ReplicationSlotName: 'slotname',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: '@TABLENAME@2',\n  ExcludedTables:'public.chkpoint'\n ) OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_SERVERFLOW1;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n ) \nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;\nCREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x\n WHERE META(x,'TableName').toString() == 'WACTION.PGToPGPLATFM_SOURCE4';\n\nCREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n ) \nINPUT FROM @STREAM@2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(\n  name: 'CDDL',\n  filename:'WAEventDump.log'\n)INPUT FROM @STREAM@;\n\nCREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;\n\nEND APPLICATION @APPNAME@1;\n\n\n\nCREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';\nCREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;\n\ncreate or replace stream @STREAM@3 of Global.waevent;\n\nCREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using PostgreSQLReader( \n  ReplicationSlotName: 'slotname',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: '$table2',\n  ExcludedTables:'public.chkpoint'\n ) OUTPUT TO @STREAM@;\n\nEND FLOW @APPNAME@_AGENTFLOW2;\n\nCREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using PostgreSQLReader(\n  ReplicationSlotName: 'slotname',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: '@TABLENAME@4',  \n  ExcludedTables:'public.chkpoint'\n ) OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_SERVERFLOW2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n ) \nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;\nCREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y\n WHERE META(y,'TableName').toString() == 'WACTION.PGToPGPLATFM_SOURCE2';\n\nCREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n ) \nINPUT FROM @STREAM@3;\n\nCREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;\n\nEND APPLICATION @APPNAME@2;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@ USE EXCEPTIONSTORE TTL : '7d' ;\n\nCREATE SOURCE @APPNAME@_Source USING Global.OracleReader (\n  Username: '@srcusername@',\n  Password: '@srcpassword@',\n  ConnectionURL: '@srcurl@', )\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_Target1 USING Global.SnowflakeWriter (\n  connectionUrl: '@tgturl@',\n    tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',\n    password: '@tgtpassword@',\n    username: '@tgtusername@',\n    appendOnly: 'true',\n    uploadPolicy: 'eventcount:1,interval:5m',\n    externalStageType: 'Local',\n    adapterName: 'SnowflakeWriter' )\nINPUT FROM @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_Target2 USING Global.SnowflakeWriter (\n  connectionUrl: '@tgturl@',\n    tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',\n    password: '@tgtpassword@',\n    username: '@tgtusername@',\n    appendOnly: 'false',\n    optimizedMerge: 'false',\n    uploadPolicy: 'eventcount:1,interval:5m',\n    externalStageType: 'Local',\n    adapterName: 'SnowflakeWriter' )\nINPUT FROM @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_Target3 USING Global.SnowflakeWriter (\n  connectionUrl: '@tgturl@',\n    tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',\n    password: '@tgtpassword@',\n    username: '@tgtusername@',\n    optimizedMerge: 'true',\n    uploadPolicy: 'eventcount:1,interval:5m',\n    externalStageType: 'Local',\n    adapterName: 'SnowflakeWriter' )\nINPUT FROM @APPNAME@_Stream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet\n(\n  ConnectionURL: '@CONNECTION_URL@',\n  CommittedTransactions: true,\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OJet',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  Username: '@SOURCE_USER@'\n)OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)\nSELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]);\n\nCREATE TYPE LogType(\nNum_col String key,\nChar_col String,\nVarchar2_col String,\nlong_col String,\nTable String,\nOperation String\n);\n\nCREATE WINDOW CDCWindow\nOVER @STREAM@\nKEEP 1 ROWS;\n\nCREATE WACTIONSTORE CDCWS CONTEXT of LogType\nEVENT TYPES ( LogType )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ ToWactionStore\nINSERT INTO CDCWS\nSELECT * FROM CDCWindow\nLINK SOURCE EVENT;\n\nCREATE TARGET @SOURCE_NAME@_SYS USING SysOut (\n  name: '@SOURCE_NAME@_SYS' )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@;\nCREATE OR REPLACE SOURCE @APP_NAME@_src1 USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream1;\n\nCREATE OR REPLACE CQ @APP_NAME@_CQ1\nINSERT INTO @APP_NAME@_Stream6\nSELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream1 d;\n\nCREATE OR REPLACE SOURCE @APP_NAME@_src2 USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream2;\n\nCREATE OR REPLACE CQ @APP_NAME@_CQ2\nINSERT INTO @APP_NAME@_Stream6\nSELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream2 d;\n\nCREATE OR REPLACE SOURCE @APP_NAME@_src3 USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream3;\n\nCREATE OR REPLACE CQ @APP_NAME@_CQ3\nINSERT INTO @APP_NAME@_Stream6\nSELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream3 d;\n\nCREATE OR REPLACE SOURCE @APP_NAME@_src4 USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream4;\n\nCREATE OR REPLACE CQ @APP_NAME@_CQ4\nINSERT INTO @APP_NAME@_Stream6\nSELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream4 d;\n\nCREATE OR REPLACE SOURCE @APP_NAME@_src5 USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream5;\n\nCREATE OR REPLACE CQ @APP_NAME@_CQ5\nINSERT INTO @APP_NAME@_Stream6\nSELECT data[0],data[1],data[2] FROM @APP_NAME@_Stream5 d;\n\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt1 USING Global.DeltaLakeWriter (\n  personalAccessToken: '',\n  hostname: 'adb-5292730997167687.7.azuredatabricks.net',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.student',\n  adapterName: 'DeltaLakeWriter',\n  personalAccessToken_encrypted: 'false',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:1,interval:10s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @APP_NAME@_Stream6;\n\nEND APPLICATION @APP_NAME@;\nDEPLOY APPLICATION @APP_NAME@;\nSTART APPLICATION @APP_NAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE TARGET @TARGET_NAME@ USING RedshiftWriter\n\t(\n\t  ConnectionURL: '@CONNECTION_URL@',\n\t  Username: 'tgt_username',\n\t  Password: 'tgt_pwrd',\n\t  bucketname: 'bucket_name',\n\t  accesskeyId: 'access_key',\n\t  secretaccesskey: 'secret_access',\n\t  Tables: 'tgt_table',\n\t  uploadpolicy:'eventcount:10,interval:1m'\n\t) INPUT FROM @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Canon Test W50\n-- Nicholas Keene, WebAction, Inc.\n--\n-- Basic test for a partitioned sliding count window\n--\n-- S -> SWc5p -> CQ -> WS\n--\n\n\nUNDEPLOY APPLICATION NameW50.W50;\nDROP APPLICATION NameW50.W50 CASCADE;\nCREATE APPLICATION W50 RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE FLOW DataAcquisitionW50;\n\nCREATE SOURCE CsvSourceW50 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'Canon1000.csv',\n  columndelimiter:',',\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStreamW50;\n\nEND FLOW DataAcquisitionW50;\n\n\nCREATE FLOW DataProcessingW50;\n\nCREATE TYPE DataTypeW50 (\n    eventID String,\n    word String KEY,\n    dateTime DateTime\n);\n\nCREATE STREAM DataStreamW50 OF DataTypeW50;\n\nCREATE CQ CSVStreamW50_to_DataStreamW50\nINSERT INTO DataStreamW50\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[2],'yyyyMMddHHmmss')\nFROM CsvStreamW50;\n\nCREATE WINDOW SWc5pW50\nOVER DataStreamW50\nKEEP 5 ROWS\nPARTITION BY word;\n\nCREATE WACTIONSTORE WactionStoreW50 CONTEXT OF DataTypeW50\nEVENT TYPES ( DataTypeW50 KEY(word) )\n@PERSIST-TYPE@\n\nCREATE CQ SWc5pW50_to_WactionStoreW50\nINSERT INTO WactionStoreW50\nSELECT\n    FIRST(eventID),\n    FIRST(word),\n    FIRST(dateTime)\nFROM SWc5pW50\nGROUP BY word;\n\nEND FLOW DataProcessingW50;\n\n\n\nEND APPLICATION W50;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @AppName@_App1;\nUNDEPLOY APPLICATION @AppName@_App1;\nDROP APPLICATION @AppName@_App1 CASCADE;\nCREATE APPLICATION @AppName@_App1 recovery 1 second interval;\n\n\nCREATE SOURCE @AppName@_FileSource USING FileReader (\n  directory:'@TestdataDir@',\n    WildCard:'banks*',\n  positionByEOF:false\n  )\nPARSE USING DSVParser (\n  header:no\n)OUTPUT TO FileStream;\n\n\nCREATE OR REPLACE ROUTER filerouter1 INPUT FROM FileStream s CASE\nWHEN meta(s,\"FileName\").toString()='banks1.csv' THEN ROUTE TO stream1,\nWHEN meta(s,\"FileName\").toString()='banks2.csv' THEN ROUTE TO stream2,\nWHEN meta(s,\"FileName\").toString()='banks3.csv' THEN ROUTE TO stream3,\nWHEN meta(s,\"FileName\").toString()='banks4.csv' THEN ROUTE TO stream4,\nELSE ROUTE TO ss_else;\n\nCREATE TYPE banks1(\n  id int,\n  name String ,\nFilename String\n);\n\nCreate stream cdctypestream1 of banks1;\n\nCREATE CQ cdcstreamcq1\nINSERT INTO cdctypestream1\nSELECT TO_INT(p.data[0]), \n       TO_STRING(p.data[1]), TO_STRING(META(p,'FileName'))\nFROM stream1 p;\n\n\nCREATE OR REPLACE TARGET @AppName@_DataBaset1 USING DatabaseWriter  ( \nConnectionURL:'@url@',\nUsername:'@userName@',\nPassword:'@password@',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.NEWBANKS1') \nINPUT FROM cdctypestream1;\n\nEnd application @AppName@_App1;\nDeploy application @AppName@_App1;\nstart application @AppName@_App1;\n\nstop application @AppName@_App2;\nundeploy application @AppName@_App2;\ndrop application @AppName@_App2 CASCADE;\ncreate application @AppName@_App2 recovery 1 second interval;\n\nCREATE OR REPLACE TARGET @AppName@_DataBaset2 USING DatabaseWriter  ( \nConnectionURL:'@url@',\nUsername:'@userName@',\nPassword:'@password@',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.BANKS1') \nINPUT FROM cdctypestream1;\n\nend application @AppName@_App2;\ndeploy application @AppName@_App2;\nstart application @AppName@_App2;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 4 on two node cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS\n-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS\n--\n\nSTOP APPLICATION N2S2CR4Tester.N2S2CRTest4;\nUNDEPLOY APPLICATION N2S2CR4Tester.N2S2CRTest4;\nDROP APPLICATION N2S2CR4Tester.N2S2CRTest4 CASCADE;\nCREATE APPLICATION N2S2CRTest4 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN2S2CRTest4;\n\nCREATE SOURCE CsvSourceN2S2CRTest4 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN2S2CRTest4;\n\nCREATE FLOW DataProcessingN2S2CRTest4;\n\nCREATE TYPE CsvDataN2S2CRTest4 (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE TYPE WactionTypeN2S2CRTest4 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\n\nCREATE STREAM DataStream OF CsvDataN2S2CRTest4;\n\nCREATE CQ CsvToDataN2S2CRTest4\nINSERT INTO DataStream\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsN2S2CRTest4 CONTEXT OF WactionTypeN2S2CRTest4\nEVENT TYPES ( CsvDataN2S2CRTest4 )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO WactionsN2S2CRTest4\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nCREATE CQ Data6ToWaction\nINSERT INTO WactionsN2S2CRTest4\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream6Minutes p;\n\nEND FLOW DataProcessingN2S2CRTest4;\n\nEND APPLICATION N2S2CRTest4;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@;\n\n\nCREATE OR REPLACE SOURCE @APPNAME@CDC_Source1 USING MySQLReader( \n  Compression: true,\n  FetchSize: 1,\n  SendBeforeImage: true,\n  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',\n  Tables: 'waction.crash_type',\n  Password: 'w@ct10n',\n  Password_encrypted: 'false',\n  Username: 'root'\n ) \nOUTPUT TO @APPNAME@AppStream1;\n\n\nCREATE OR REPLACE TARGET @APPNAME@sap_target USING DatabaseWriter( \n  DatabaseProviderType:'SAPHANA',\n  ConnectionRetryPolicy: 'retryInterval=30,maxRetries=3',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'SYSTEM',\n  Password_encrypted: 'false',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:sap://10.77.21.116:39013/?databaseName=striim&currentSchema=QA',\n  Tables: 'waction.crash_type,QA.CRASH_TYPES',\n  adapterName: 'DatabaseWriter',\n  --IgnorableExceptionCode: '',\n  Password: 'Striim_SAP@123'\n ) \nINPUT FROM @APPNAME@AppStream1;\n\n\ncreate or replace target @APPNAME@sys_tgt using sysout(\nname:Foo2\n)input from @APPNAME@AppStream1;\n\nEND APPLICATION @APPNAME@;\n\ndeploy application @APPNAME@;\nstart application @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet\n(\n  ConnectionURL: '@CONNECTION_URL@',\n  CommittedTransactions: true,\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OJet',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  Username: '@SOURCE_USER@'\n)OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)\nSELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[0]) = '1' ;\n\nCREATE TYPE LogType(\nNum_col String key,\nChar_col String,\nVarchar2_col String,\nlong_col String,\nTable String,\nOperation String\n);\n\nCREATE WINDOW CDCWindow\nOVER @STREAM@\nKEEP 1 ROWS;\n\nCREATE WACTIONSTORE CDCWS CONTEXT of LogType\nEVENT TYPES ( LogType )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ ToWactionStore\nINSERT INTO CDCWS\nSELECT * FROM CDCWindow\nLINK SOURCE EVENT;\n\nCREATE TARGET @SOURCE_NAME@_SYS USING SysOut (\n  name: '@SOURCE_NAME@_SYS' )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP UpdatableCacher.UpdatableCache;\nUNDEPLOY APPLICATION UpdatableCacher.UpdatableCache;\nDROP APPLICATION UpdatableCacher.UpdatableCache CASCADE;\nCREATE APPLICATION UpdatableCacher.UpdatableCache;\n\nCREATE TYPE MerchantHourlyAve(\n  merchantId String KEY,\n  hourlyAve Integer,\n  theDate DateTime,\n  dVal Double\n);\n\n\nCREATE source CsvDataSource USING FileReader (\n      directory:'@TEST-DATA-PATH@',\n      columndelimiter: ',',\n      wildcard:'ucData.csv',\n      blocksize: 10240,\n      positionByEOF:false\n)\nPARSE USING DSVParser (\n      header:No,\n      trimquote:false\n) OUTPUT TO CsvStream;\n\n\nCREATE STREAM S1 OF MerchantHourlyAve;\n\nCREATE CQ cq1\n\tinsert into S1\n\t\tSELECT data[0],\n\t\t\t\tTO_INT(data[1]),\n\t\t\t\tTO_DATE(data[2]),\n\t\t\t\tTO_DOUBLE(data[3])\n\t\tFROM CsvStream;\n\n\nCREATE EVENTTABLE ET1 using STREAM (\n  NAME: 'S1'\n) QUERY (keytomap:'dVal', persistPolicy: 'true' ) OF MerchantHourlyAve;\n\n\nCREATE EVENTTABLE ET2 using STREAM (\n  NAME: 'S1'\n) QUERY (keytomap:'merchantId' ) OF MerchantHourlyAve;\n\n\n\nEND APPLICATION UpdatableCache;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop ORAToBigquery;\nundeploy application ORAToBigquery;\ndrop application ORAToBigquery cascade;\nCREATE APPLICATION ORAToBigquery RECOVERY 5 SECOND INTERVAL;\nCREATE OR REPLACE SOURCE Rac11g USING OracleReader ( \n  SupportPDB: false,\n  SendBeforeImage: true,\n  ReaderType: 'LogMiner',\n  CommittedTransactions: false,\n  FetchSize: 1,\n  Password: 'manager',\n  DDLTracking: false,\n  StartTimestamp: 'null',\n  OutboundServerProcessName: 'WebActionXStream',\n  OnlineCatalog: true,\n  ConnectionURL: '192.168.33.10:1521/XE',\n  SkipOpenTransactions: false,\n  Compression: false,\n  QueueSize: 40000,\n  RedoLogfiles: 'null',\n  Tables: 'SYSTEM.GGAUTHORIZATIONS',\n  Username: 'system',\n  FilterTransactionBoundaries: true,\n  adapterName: 'OracleReader',\n  XstreamTimeOut: 600,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=60, maxRetries=3'\n ) \nOUTPUT TO DataStream;\nCREATE OR REPLACE TARGET Target1 USING SysOut ( \n  name: \"dstream\"\n ) \nINPUT FROM DataStream;\nCREATE OR REPLACE TARGET Target2 USING BigqueryWriter  ( \n  BQServiceAccountConfigurationPath: '/Users/ravipathak/Downloads/abc.json',\n  projectId: 'big-querytest',\n  Tables: 'SYSTEM.GGAUTHORIZATIONS,testing1.ggauthorisation',\n  parallelismCount: 2,\n  BatchPolicy: 'eventCount:100000,Interval:0'\n ) \nINPUT FROM DataStream;\nEND APPLICATION ORAToBigquery;\ndeploy application ORAToBigquery;\nstart ORAToBigquery;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use PosTester;\nalter application PosApp;\n\nCREATE source CsvDataSource USING CSVReader (\n  directory:'Samples/Customer/PosApp/appData',\n  header:Yes,\n  wildcard:'posdata.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nend application PosApp;\n\nalter application PosApp recompile;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "UNDEPLOY APPLICATION FileToKWriterUpgrade;\n\nDROP APPLICATION FileToKWriterUpgrade cascade;\n\nCREATE APPLICATION FileToKWriterUpgrade;\n\nCREATE TYPE Type1\n(\n City String,\n Col2 String,\n Col3 String,\n Col4 String\n);\n\n-- Create a stream of type Type1\n\nCREATE STREAM TypedStream OF Type1;\n\n-- Create a source using FileReader\n\nCREATE SOURCE KafkaCSVSource USING FILEREADER\n(\n directory:'@DIRECTORY@',\n WildCard:'city*.dsv',\n positionByEOF:false,\n charset:'UTF-8'\n)\nPARSE USING DSVPARSER\n(\n columndelimiter:',',\n ignoreemptycolumn:'Yes'\n)\nOUTPUT TO FileStream;\n\n-- Read from raw stream to typed stream using CQ\n\nCREATE CQ RawStreamCQ\nINSERT INTO TypedStream\nSELECT \n data[0],\n data[1],\n data[2],\n data[3] \nFROM FileStream;\n\n-- Load the KafkaWriter from TypedStream\n\nCREATE TARGET KWriter USING KAFKAWRITER VERSION '0.9.0'\n(\n brokerAddress:'localhost:9092',\n Topic:'@TOPIC@',\n KafkaMessageFormatVersion:v2\n)\nFORMAT USING DSVFORMATTER()\nINPUT FROM TypedStream;\n\n\nCREATE SOURCE KReader USING KAFKAREADER VERSION '0.9.0'\n(\n brokerAddress:'localhost:9092',\n Topic:'@TOPIC@',\n charset : 'UTF-8',\n KafkaConfig:'retry.backoff.ms=5000',\n startOffset:0\n)\nPARSE USING DSVParser (\n)\n\nOUTPUT TO KafkaReaderStream;\n\nCREATE TARGET LogKafkaReaderStream USING LOGWRITER\n(\n name:KafkaLOuput,\n filename:'@LOGFILENAME@',\n flushpolicy : 'flushcount:1',\n rolloverpolicy : 'EventCount:10000,Interval:30s'\n)\nINPUT FROM KafkaReaderStream;\n\n\nEND APPLICATION FileToKWriterUpgrade;\n\nDEPLOY APPLICATION FileToKWriterUpgrade;\n\nSTART APPLICATION FileToKWriterUpgrade;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 25 with two sources, two jumping count windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jc5W -> CQ1 -> WS\n--   S2 -> Jc6W -> CQ2 -> WS\n--\n\nSTOP KStreamRecov25Tester.KStreamRecovTest25;\nUNDEPLOY APPLICATION KStreamRecov25Tester.KStreamRecovTest25;\nDROP APPLICATION KStreamRecov25Tester.KStreamRecovTest25 CASCADE;\nDROP USER KStreamRecov25Tester;\nDROP NAMESPACE KStreamRecov25Tester CASCADE;\nCREATE USER KStreamRecov25Tester IDENTIFIED BY KStreamRecov25Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov25Tester;\nCONNECT KStreamRecov25Tester KStreamRecov25Tester;\n\nCREATE APPLICATION KStreamRecovTest25 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP 5 ROWS;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP 6 ROWS;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream6Minutes p;\n\nEND APPLICATION KStreamRecovTest25;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 31 with two sources, two sliding count windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Sc5W/p -> CQ1 -> WS\n-- S2 -> Sc6W/p -> CQ2 -> WS\n--\n\nSTOP KStreamRecov31Tester.KStreamRecovTest31;\nUNDEPLOY APPLICATION KStreamRecov31Tester.KStreamRecovTest31;\nDROP APPLICATION KStreamRecov31Tester.KStreamRecovTest31 CASCADE;\n\nDROP USER KStreamRecov31Tester;\nDROP NAMESPACE KStreamRecov31Tester CASCADE;\nCREATE USER KStreamRecov31Tester IDENTIFIED BY KStreamRecov31Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov31Tester;\nCONNECT KStreamRecov31Tester KStreamRecov31Tester;\n\nCREATE APPLICATION KStreamRecovTest31 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP 5 ROWS\nPARTITION BY merchantId;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP 6 ROWS\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1\nGROUP BY merchantId;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2\nGROUP BY merchantId;\n\nEND APPLICATION KStreamRecovTest31;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @Appname@;\nUNDEPLOY APPLICATION @Appname@;\nDROP APPLICATION @Appname@ CASCADE;\n\nCREATE APPLICATION @Appname@ @Recovery@ use exceptionstore;\n\nCREATE SOURCE @Appname@_S USING OracleReader\n(\n\tUsername: 'qatest',\n\tPassword: 'qatest',\n\tConnectionURL: 'dockerhost:1521:orcl',\n\tTables: 'QATEST.TABLE_TEST_%',\n\tFetchSize: '1'\n)\nOUTPUT TO @Appname@_SS;\n\n\nCREATE or replace TARGET @Appname@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.TABLE_TEST_%,@DATASET@.%',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1000,Interval:10',\nStandardSQL:true\t\n) INPUT FROM @Appname@_ss;\n\nEND APPLICATION @Appname@;\nDEPLOY APPLICATION @Appname@;\nSTART APPLICATION @Appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application AzureApp;\nundeploy application AzureApp;\ndrop application AzureApp cascade;\n\ncreate application AzureApp\nRECOVERY 10 second interval;\ncreate source CS using FileReader (\n\tdirectory:'@DIR@',\n\tWildCard:'@WILDCARD@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target T using AzureBlobWriter(\n\taccountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:50'\n)\nformat using DSVFormatter (\n)\ninput from TypedCSVStream;\nend application AzureApp;\ndeploy application AzureApp in default;\nstart application AzureApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application HPTippingLog;\ncreate source DHCPLogSource using FileReader (\n\tdirectory:'@TEST-DATA-PATH@',\n\tWildCard:'hp*',\n\tcharset:'UTF-8',\n\tpositionByEOF:false \n) PARSE USING HPTippingPointLogParser (\n) OUTPUT TO HPLogStream;\ncreate Target HPDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/hp_log') input from HPLogStream;\nend application HPTippingLog;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@ @RECOVERY@;\ncreate source @SOURCE@ using FileReader (\n        directory:'./Samples/AppData/',\n        WildCard:'dynamicdirectory.csv',\n        positionByEOF:false,\n        charset:'UTF-8'\n)\nparse using DSVParser (\n        header:'no'\n)\nOUTPUT TO @STREAM@;\n\ncreate Target @TARGET@ using ADLSGen2Writer(\n    accountname:'',\n\tsastoken:'',\n\tfilesystemname:'',\n\tfilename:'',\n\tdirectory:'',\n\tuploadpolicy:'eventcount:5000'\n)format using DSVFormatter (\n members: 'data'\n)\ninput from @STREAM@;\n\nend application @APPNAME@;\n\ndeploy application @APPNAME@;\nstart application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@ recovery 5 SECOND Interval;\nCREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream;\n\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.DeltaLakeWriter (\n  personalAccessToken: '',\n  hostname: 'adb-5292730997167687.7.azuredatabricks.net',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.student',\n  adapterName: 'DeltaLakeWriter',\n  personalAccessToken_encrypted: 'false',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:1,interval:30s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @APP_NAME@_Stream;\n\nEND APPLICATION @APP_NAME@;\nDEPLOY APPLICATION @APP_NAME@;\nSTART APPLICATION @APP_NAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCREATE SOURCE @SourceName@ USING MySqlReader  ( \nTransactionSupport: false, \n  FetchTransactionMetadata: false, \n  Compression: false, \n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', \n  Password_encrypted: 'false', \n  ConnectionURL: '@ConnectionURL@', \n  Fetchsize: 0, \n  ConnectionPoolSize: 10, \n  Username: '@UN@', \n  cdcRoleName: 'STRIIM_READER', \n  Password: '@PWD@', \n  Tables: 'qatest.%', \n  FilterTransactionBoundaries: true, \n  SendBeforeImage: true, \n  AutoDisableTableCDC: false ) \nOUTPUT TO @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\n\nCREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'), 'OpTime',META(x, 'TimeStamp'))) FROM @SRCINPUTSTREAM@ x; ;\n\nCREATE TARGET @targetName@ USING DatabaseWriter ( \nConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', \n  ParallelThreads: '', \n  CheckPointTable: 'CHKPOINT', \n  Password_encrypted: 'false', \n  ConnectionURL: '@ConnectionURL@', \n  CommitPolicy: 'EventCount:1,Interval:60', \n  StatementCacheSize: '50', \n  DatabaseProviderType: 'Default', \n  Username: '@UN@', \n  Password: '@PWD@', \n  PreserveSourceTransactionBoundary: 'false', \n  BatchPolicy: 'EventCount:1,Interval:60', \n  Tables: 'qatest.%, dbo.% columnmap(opt_type=@USERDATA(OpType),opt_time=@USERDATA(OpTime));' ) \nINPUT FROM admin.sqlreader_cq_out;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use PosTester;\nDROP WACTIONSTORE MerchantActivity;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n\nCREATE SOURCE @APPNAME@_S USING MSSqlReader\n(\n  Compression: false,\n  cdcRoleName: 'STRIIM_READER',\n  DatabaseName: 'QATEST',\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ConnectionPoolSize: 1,\n  FetchTransactionMetadata: false,\n  StartPosition: 'EOF',\n  Username: 'qatest',\n  SendBeforeImage: true,\n  AutoDisableTableCDC: true,\n  ConnectionURL: 'localhost:1433',\n  Tables: 'qatest.test01',\n  adapterName: 'MSSqlReader',\n  Password: 'w3b@ct10n'\n)\nOUTPUT TO @APPNAME@_SS;\n\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(id,name) COLUMNMAP(id=id,col1=col11,col2=col12)',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:10',\nStandardSQL:true\t\n) INPUT FROM @APPNAME@_ss;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING DatabaseReader  (\n  Username: '@READER-UNAME@',\n  Password: '@READER-PASSWORD@',\n  ConnectionURL: '@CDC-READER-URL@',\n  Tables: @SOURCE_TABLE@,\n  sendBeforeImage:'true',\n  FilterTransactionBoundaries: 'true'\n )\nOUTPUT TO @STREAM@;\n\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@2 USING DatabaseReader  (\n  Username: '@READER-UNAME@',\n  Password: '@READER-PASSWORD@',\n  ConnectionURL: '@CDC-READER-URL@',\n  Tables: @SOURCE_TABLE@,\n  sendBeforeImage:'true',\n  FilterTransactionBoundaries: 'true'\n )\nOUTPUT TO @STREAM@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE PROPERTYVARIABLE RetryPolicy='timeOut=00,retryInterval=1,maxRetries=3';\nCREATE OR REPLACE PROPERTYVARIABLE KafkaBrokerAddress='localhost:9099';\nCREATE OR REPLACE PROPERTYVARIABLE KafkaConfig='request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;';\n\nSTOP APPLICATION @Appname@;\nUNDEPLOY APPLICATION @Appname@;\nDROP APPLICATION @Appname@ CASCADE;\nCREATE APPLICATION @Appname@ @Recovery@;\nCREATE FLOW @Appname@AgentFlow;\n--Partitioning source stream with meta (tablename)\nCreate Source @Appname@s1 Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL: '@Connectionurl@',\n Tables:'qatest.oracle_PartitionStream_test%',\n FetchSize:1\n)\nOutput To @Appname@ss1 partition by meta(@Appname@ss1,'TableName');\n-- (id integer,name1 string,name2 string) partition by sleft(name1,1)\n-- select TO_INT(data[0]),TO_STRING(data[1]),TO_STRING(data[2]);\n\n\n--Partitioning source stream using meta (STARTSCN)\nCreate Source @Appname@s2 Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL: '@Connectionurl@',\nTables:'qatest.oracle_PartitionStream_test%',\n FetchSize:1\n)\nOutput To @Appname@ss2 partition by meta(@Appname@ss2,'STARTSCN');\n\n\n\nCREATE TYPE @Appname@OpTableDataType(\n  TableName String,\n  STARTSCN String,\n  data java.util.HashMap\n);\n\n--Partitioning typed stream inline while creation using expr\nCREATE STREAM @Appname@OracleTypedStream OF @Appname@OpTableDataType partition by sright(STARTSCN,2);\nCreate Source @Appname@s3 Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL: '@Connectionurl@',\nTables:'qatest.oracle_PartitionStream_test%',\n FetchSize:1\n)\nOutput To @Appname@ss3 partition by meta(@Appname@ss3,'STARTSCN');\n\nCREATE CQ @Appname@ParseOracleRawStream\n  INSERT INTO @Appname@OracleTypedStream\n  SELECT META(@Appname@ss3, 'TableName').toString(),META(@Appname@ss3, 'STARTSCN').toString(),\n    DATA(@Appname@ss3)\n  FROM @Appname@ss3;\n\n--Partitioning source stream with types defined inline using meta expr\nCreate Source @Appname@s4 Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL: '@Connectionurl@',\nTables:'qatest.oracle_PartitionStream_test%',\n FetchSize:1\n)\nOutput To @Appname@ss4 (id integer,NAME string,COL1 string)\n-- partition by meta(ss4,'STARTSCN')\nselect TO_INT(data[0]),TO_STRING(data[2]),TO_STRING(data[3]);\n\nCREATE CQ @Appname@cqss4\n  INSERT INTO @Appname@ss4new Partition by sright(id,1)\n  SELECT * FROM @Appname@ss4;\n\n--Partitioning source stream -(waevent-s5) with field that has null value\n\nCREATE STREAM @Appname@s5 of Global.WAEvent PARTITION BY meta(@Appname@s5, 'TransactionName');\n\nCreate Source @Appname@src5 Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL: '@Connectionurl@',\nTables:'qatest.oracle_PartitionStream_test%',\n FetchSize:1\n)\nOutput To @Appname@ss5;\n\nEND FLOW @Appname@AgentFlow;\n\n\nCREATE FLOW @Appname@ServerFlow;\n--CREATE STREAM modifyStream OF Global.WAEvent partition by meta(modifyStream,'STARTSCN');\nCREATE STREAM @Appname@modifyStream OF Global.WAEvent partition by sleft(data[0],1);\nCREATE CQ @Appname@modifycq INSERT INTO @Appname@modifyStream\nSELECT * FROM @Appname@ss5\nMODIFY\n(\ndata[0] = (TO_INT(data[0]) * 10000) / 100\n);\n\n\n--Partitioning source stream -(waevent) with modified fields\nCreate Source @Appname@s6 Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL: '@Connectionurl@',\nTables:'qatest.oracle_PartitionStream_test%',\n FetchSize:1\n)\nOutput To @Appname@ss6;\n\nCREATE CQ @Appname@putUserDatacq1\nINSERT INTO @Appname@newss6\nPARTITION BY userdata(@Appname@newss6, 'Modified_Date')\nSELECT\nputUserData(x,'Modified_Date',TO_STRING(DNOW(),'yyyy-MM-dd HH:mm:ss'))\nFROM @Appname@ss6 x;\n\n--Partitioning stream with userdata\nCREATE STREAM @Appname@newss7 of Global.WAEvent PARTITION BY userdata(@Appname@newss7, 'Modified_Date');\n\nCreate Source @Appname@s7 Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL: '@Connectionurl@',\nTables:'qatest.oracle_PartitionStream_test%',\n FetchSize:1\n)\nOutput To @Appname@ss7;\n\nCREATE CQ @Appname@putUserDatacq2\nINSERT INTO @Appname@newss7\nSELECT\nputUserData(x,'Modified_Date',TO_STRING(DNOW(),'yyyy-MM-dd HH:mm:ss'))\nFROM @Appname@ss7 x;\n\n\ncreate Target @Appname@KW1 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'oracle_Expr01',\nMode:'Async',\nKafkaConfig: '$KafkaConfig'\n)\nFORMAT USING jsonFormatter ()\ninput from @Appname@ss1;\n\ncreate Target @Appname@KW2 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'oracle_Expr02',\nMode:'Async',\nKafkaConfig: '$KafkaConfig'\n        )\nFORMAT USING jsonFormatter ()\ninput from @Appname@ss2;\n\ncreate Target @Appname@KW3 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'oracle_Expr03',\nMode:'Async',\nKafkaConfig: '$KafkaConfig'\n        )\nFORMAT USING jsonFormatter ()\ninput from @Appname@OracleTypedStream;\n\ncreate Target @Appname@KW4 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'oracle_Expr04',\nMode:'sync',\nKafkaConfig: '$KafkaConfig'\n        )\nFORMAT USING jsonFormatter ()\ninput from @Appname@ss4new;\n\ncreate Target @Appname@KW5 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'oracle_Expr05',\nMode:'sync',\nKafkaConfig: '$KafkaConfig'\n        )\nFORMAT USING jsonFormatter ()\ninput from @Appname@modifyStream;\n\ncreate Target @Appname@KW6 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'oracle_Expr06',\nMode:'sync',\nKafkaConfig: '$KafkaConfig'\n        )\nFORMAT USING jsonFormatter ()\ninput from @Appname@newss6;\n\ncreate Target @Appname@KW7 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'oracle_Expr07',\nMode:'sync',\nKafkaConfig: '$KafkaConfig'\n        )\nFORMAT USING jsonFormatter ()\ninput from @Appname@newss7;\nEND FLOW @Appname@ServerFlow;\nend application @Appname@;\n--deploy application @Appname@ with @Appname@AgentFlow in Agents, @Appname@ServerFlow in default;\ndeploy application @Appname@;\nstart @Appname@;\n\nstop application @Appname@KR;\nundeploy application @Appname@KR;\ndrop application @Appname@KR cascade;\ncreate application @Appname@KR;\nCREATE STREAM @Appname@KafkaStream of Global.jsonnodeEvent;\nalter stream @Appname@KafkaStream PARTITION BY meta(@Appname@KafkaStream, 'PartitionID');\nCREATE SOURCE @Appname@KafkaSource1 USING KafkaReader Version '2.1.0'\n(\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'oracle_Expr01',\nstartOffset:0\n)\nPARSE USING jsonParser ()\nOUTPUT TO @Appname@KafkaStream;\n\ncreate Target @Appname@t2 using filewriter(\n\tfilename:'%n%',\n\trolloverpolicy:'EventCount:5000000',\n    directory:'FEATURE-DIR/logs/%@metadata(TopicName)%/%@metadata(PartitionID)%'\n)\nformat using jsonFormatter (\n)\ninput from @Appname@KafkaStream;\n\nend application @Appname@KR;\ndeploy application @Appname@KR;\n--start @Appname@KR;\n\nSTOP APPLICATION @Appname@FR;\nUNDEPLOY APPLICATION @Appname@FR;\nDROP APPLICATION @Appname@FR CASCADE;\nCREATE APPLICATION @Appname@FR;\nCREATE SOURCE @Appname@FS0 USING FileReader (\n    directory:'Product/IntegrationTests/TestData/',\n    WildCard:'0',\n\tpositionByEOF:false\n\t)\nPARSE USING jsonParser (\n)OUTPUT TO @Appname@FR_SS0;\n\nCREATE SOURCE @Appname@FS1 USING FileReader (\n    directory:'Product/IntegrationTests/TestData/',\n    WildCard:'0',\n\tpositionByEOF:false\n\t)\nPARSE USING jsonParser (\n)OUTPUT TO @Appname@FR_SS1;\n\nCREATE SOURCE @Appname@FS2 USING FileReader (\n    directory:'Product/IntegrationTests/TestData/',\n    WildCard:'0',\n\tpositionByEOF:false\n\t)\nPARSE USING jsonParser (\n)OUTPUT TO @Appname@FR_SS2;\n\nCREATE SOURCE @Appname@FS3 USING FileReader (\n    directory:'Product/IntegrationTests/TestData/',\n    WildCard:'0',\n\tpositionByEOF:false\n\t)\nPARSE USING jsonParser (\n)OUTPUT TO @Appname@FR_SS3;\n\nCREATE SOURCE @Appname@FS4 USING FileReader (\n    directory:'Product/IntegrationTests/TestData/',\n    WildCard:'0',\n\tpositionByEOF:false\n\t)\nPARSE USING jsonParser (\n)OUTPUT TO @Appname@FR_SS4;\n\nCREATE SOURCE @Appname@FS5 USING FileReader (\n    directory:'Product/IntegrationTests/TestData/',\n    WildCard:'0',\n\tpositionByEOF:false\n\t)\nPARSE USING jsonParser (\n)OUTPUT TO @Appname@FR_SS5;\n\nCREATE SOURCE @Appname@FS6 USING FileReader (\n    directory:'Product/IntegrationTests/TestData/',\n    WildCard:'0',\n\tpositionByEOF:false\n\t)\nPARSE USING jsonParser (\n)OUTPUT TO @Appname@FR_SS6;\nend application @Appname@FR;\ndeploy application @Appname@FR;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE FLOW @STREAM@_SourceFlow;\n\nCREATE SOURCE @SOURCE_NAME@ Using OracleReader\n(\n Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n   _h_ReturnDateTimeAs:'ZonedDateTime'\n) OUTPUT TO @STREAM@;\n\nEND FLOW @STREAM@_SourceFlow;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\nCREATE SOURCE @APPNAME@_S USING OracleReader\n(\n\tUsername: 'qatest',\n\tPassword: 'qatest',\n\tConnectionURL: 'dockerhost:1521:xe',\n\tTables: 'qatest.test01',\n\tFetchSize: '1',\n\tconnectionRetryPolicy:'timeOut=00,retryInterval=1,maxRetries=3'\n)\nOUTPUT TO @APPNAME@_SS;\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(id,name) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:0',\nStandardSQL:true\t\t\n) INPUT FROM @APPNAME@_ss;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\n--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW ;\n\nCREATE OR REPLACE SOURCE DBSource USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: 'QATEST.orac_1000COL',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:1,Interval:60',\n  CommitPolicy: 'EventCount:1,Interval:60',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: 'QATEST.orac_1000COL,test.cassandra_1500col columnmap(field1=f1,field2=f2,field3=f3,field4=f4,field5=f5,field6=f6,field7=f7,field8=f8,field9=f9,field10=f10,field11=f11,field12=f12,field13=f13,field14=f14,field15=f15,field16=f16,field17=f17,field18=f18,field19=f19,field20=f20,field21=f21,field22=f22,field23=f23,field24=f24,field25=f25,field26=f26,field27=f27,field28=f28,field29=f29,field30=f30,field31=f31,field32=f32,field33=f33,field34=f34,field35=f35,field36=f36,field37=f37,field38=f38,field39=f39,field40=f40,field41=f41,field42=f42,field43=f43,field44=f44,field45=f45,field46=f46,field47=f47,field48=f48,field49=f49,field50=f50,field51=f51,field52=f52,field53=f53,field54=f54,field55=f55,field56=f56,field57=f57,field58=f58,field59=f59,field60=f60,field61=f61,field62=f62,field63=f63,field64=f64,field65=f65,field66=f66,field67=f67,field68=f68,field69=f69,field70=f70,field71=f71,field72=f72,field73=f73,field74=f74,field75=f75,field76=f76,field77=f77,field78=f78,field79=f79,field80=f80,field81=f81,field82=f82,field83=f83,field84=f84,field85=f85,field86=f86,field87=f87,field88=f88,field89=f89,field90=f90,field91=f91,field92=f92,field93=f93,field94=f94,field95=f95,field96=f96,field97=f97,field98=f98,field99=f99,field100=f100,field101=f101,field102=f102,field103=f103,field104=f104,field105=f105,field106=f106,field107=f107,field108=f108,field109=f109,field110=f110,field111=f111,field112=f112,field113=f113,field114=f114,field115=f115,field116=f116,field117=f117,field118=f118,field119=f119,field120=f120,field121=f121,field122=f122,field123=f123,field124=f124,field125=f125,field126=f126,field127=f127,field128=f128,field129=f129,field130=f130,field131=f131,field132=f132,field133=f133,field134=f134,field135=f135,field136=f136,field137=f137,field138=f138,field139=f139,field140=f140,field141=f141,field142=f142,field143=f143,field144=f144,field145=f145,field146=f146,field147=f147,field148=f148,field149=f149,field150=f150,field151=f151,field152=f152,field153=f153,field154=f154,field155=f155,field156=f156,field157=f157,field158=f158,field159=f159,field160=f160,field161=f161,field162=f162,field163=f163,field164=f164,field165=f165,field166=f166,field167=f167,field168=f168,field169=f169,field170=f170,field171=f171,field172=f172,field173=f173,field174=f174,field175=f175,field176=f176,field177=f177,field178=f178,field179=f179,field180=f180,field181=f181,field182=f182,field183=f183,field184=f184,field185=f185,field186=f186,field187=f187,field188=f188,field189=f189,field190=f190,field191=f191,field192=f192,field193=f193,field194=f194,field195=f195,field196=f196,field197=f197,field198=f198,field199=f199,field200=f200,field201=f201,field202=f202,field203=f203,field204=f204,field205=f205,field206=f206,field207=f207,field208=f208,field209=f209,field210=f210,field211=f211,field212=f212,field213=f213,field214=f214,field215=f215,field216=f216,field217=f217,field218=f218,field219=f219,field220=f220,field221=f221,field222=f222,field223=f223,field224=f224,field225=f225,field226=f226,field227=f227,field228=f228,field229=f229,field230=f230,field231=f231,field232=f232,field233=f233,field234=f234,field235=f235,field236=f236,field237=f237,field238=f238,field239=f239,field240=f240,field241=f241,field242=f242,field243=f243,field244=f244,field245=f245,field246=f246,field247=f247,field248=f248,field249=f249,field250=f250,field251=f251,field252=f252,field253=f253,field254=f254,field255=f255,field256=f256,field257=f257,field258=f258,field259=f259,field260=f260,field261=f261,field262=f262,field263=f263,field264=f264,field265=f265,field266=f266,field267=f267,field268=f268,field269=f269,field270=f270,field271=f271,field272=f272,field273=f273,field274=f274,field275=f275,field276=f276,field277=f277,field278=f278,field279=f279,field280=f280,field281=f281,field282=f282,field283=f283,field284=f284,field285=f285,field286=f286,field287=f287,field288=f288,field289=f289,field290=f290,field291=f291,field292=f292,field293=f293,field294=f294,field295=f295,field296=f296,field297=f297,field298=f298,field299=f299,field300=f300,field301=f301,field302=f302,field303=f303,field304=f304,field305=f305,field306=f306,field307=f307,field308=f308,field309=f309,field310=f310,field311=f311,field312=f312,field313=f313,field314=f314,field315=f315,field316=f316,field317=f317,field318=f318,field319=f319,field320=f320,field321=f321,field322=f322,field323=f323,field324=f324,field325=f325,field326=f326,field327=f327,field328=f328,field329=f329,field330=f330,field331=f331,field332=f332,field333=f333,field334=f334,field335=f335,field336=f336,field337=f337,field338=f338,field339=f339,field340=f340,field341=f341,field342=f342,field343=f343,field344=f344,field345=f345,field346=f346,field347=f347,field348=f348,field349=f349,field350=f350,field351=f351,field352=f352,field353=f353,field354=f354,field355=f355,field356=f356,field357=f357,field358=f358,field359=f359,field360=f360,field361=f361,field362=f362,field363=f363,field364=f364,field365=f365,field366=f366,field367=f367,field368=f368,field369=f369,field370=f370,field371=f371,field372=f372,field373=f373,field374=f374,field375=f375,field376=f376,field377=f377,field378=f378,field379=f379,field380=f380,field381=f381,field382=f382,field383=f383,field384=f384,field385=f385,field386=f386,field387=f387,field388=f388,field389=f389,field390=f390,field391=f391,field392=f392,field393=f393,field394=f394,field395=f395,field396=f396,field397=f397,field398=f398,field399=f399,field400=f400,field401=f401,field402=f402,field403=f403,field404=f404,field405=f405,field406=f406,field407=f407,field408=f408,field409=f409,field410=f410,field411=f411,field412=f412,field413=f413,field414=f414,field415=f415,field416=f416,field417=f417,field418=f418,field419=f419,field420=f420,field421=f421,field422=f422,field423=f423,field424=f424,field425=f425,field426=f426,field427=f427,field428=f428,field429=f429,field430=f430,field431=f431,field432=f432,field433=f433,field434=f434,field435=f435,field436=f436,field437=f437,field438=f438,field439=f439,field440=f440,field441=f441,field442=f442,field443=f443,field444=f444,field445=f445,field446=f446,field447=f447,field448=f448,field449=f449,field450=f450,field451=f451,field452=f452,field453=f453,field454=f454,field455=f455,field456=f456,field457=f457,field458=f458,field459=f459,field460=f460,field461=f461,field462=f462,field463=f463,field464=f464,field465=f465,field466=f466,field467=f467,field468=f468,field469=f469,field470=f470,field471=f471,field472=f472,field473=f473,field474=f474,field475=f475,field476=f476,field477=f477,field478=f478,field479=f479,field480=f480,field481=f481,field482=f482,field483=f483,field484=f484,field485=f485,field486=f486,field487=f487,field488=f488,field489=f489,field490=f490,field491=f491,field492=f492,field493=f493,field494=f494,field495=f495,field496=f496,field497=f497,field498=f498,field499=f499,field500=f500,field501=f501,field502=f502,field503=f503,field504=f504,field505=f505,field506=f506,field507=f507,field508=f508,field509=f509,field510=f510,field511=f511,field512=f512,field513=f513,field514=f514,field515=f515,field516=f516,field517=f517,field518=f518,field519=f519,field520=f520,field521=f521,field522=f522,field523=f523,field524=f524,field525=f525,field526=f526,field527=f527,field528=f528,field529=f529,field530=f530,field531=f531,field532=f532,field533=f533,field534=f534,field535=f535,field536=f536,field537=f537,field538=f538,field539=f539,field540=f540,field541=f541,field542=f542,field543=f543,field544=f544,field545=f545,field546=f546,field547=f547,field548=f548,field549=f549,field550=f550,field551=f551,field552=f552,field553=f553,field554=f554,field555=f555,field556=f556,field557=f557,field558=f558,field559=f559,field560=f560,field561=f561,field562=f562,field563=f563,field564=f564,field565=f565,field566=f566,field567=f567,field568=f568,field569=f569,field570=f570,field571=f571,field572=f572,field573=f573,field574=f574,field575=f575,field576=f576,field577=f577,field578=f578,field579=f579,field580=f580,field581=f581,field582=f582,field583=f583,field584=f584,field585=f585,field586=f586,field587=f587,field588=f588,field589=f589,field590=f590,field591=f591,field592=f592,field593=f593,field594=f594,field595=f595,field596=f596,field597=f597,field598=f598,field599=f599,field600=f600,field601=f601,field602=f602,field603=f603,field604=f604,field605=f605,field606=f606,field607=f607,field608=f608,field609=f609,field610=f610,field611=f611,field612=f612,field613=f613,field614=f614,field615=f615,field616=f616,field617=f617,field618=f618,field619=f619,field620=f620,field621=f621,field622=f622,field623=f623,field624=f624,field625=f625,field626=f626,field627=f627,field628=f628,field629=f629,field630=f630,field631=f631,field632=f632,field633=f633,field634=f634,field635=f635,field636=f636,field637=f637,field638=f638,field639=f639,field640=f640,field641=f641,field642=f642,field643=f643,field644=f644,field645=f645,field646=f646,field647=f647,field648=f648,field649=f649,field650=f650,field651=f651,field652=f652,field653=f653,field654=f654,field655=f655,field656=f656,field657=f657,field658=f658,field659=f659,field660=f660,field661=f661,field662=f662,field663=f663,field664=f664,field665=f665,field666=f666,field667=f667,field668=f668,field669=f669,field670=f670,field671=f671,field672=f672,field673=f673,field674=f674,field675=f675,field676=f676,field677=f677,field678=f678,field679=f679,field680=f680,field681=f681,field682=f682,field683=f683,field684=f684,field685=f685,field686=f686,field687=f687,field688=f688,field689=f689,field690=f690,field691=f691,field692=f692,field693=f693,field694=f694,field695=f695,field696=f696,field697=f697,field698=f698,field699=f699,field700=f700,field701=f701,field702=f702,field703=f703,field704=f704,field705=f705,field706=f706,field707=f707,field708=f708,field709=f709,field710=f710,field711=f711,field712=f712,field713=f713,field714=f714,field715=f715,field716=f716,field717=f717,field718=f718,field719=f719,field720=f720,field721=f721,field722=f722,field723=f723,field724=f724,field725=f725,field726=f726,field727=f727,field728=f728,field729=f729,field730=f730,field731=f731,field732=f732,field733=f733,field734=f734,field735=f735,field736=f736,field737=f737,field738=f738,field739=f739,field740=f740,field741=f741,field742=f742,field743=f743,field744=f744,field745=f745,field746=f746,field747=f747,field748=f748,field749=f749,field750=f750,field751=f751,field752=f752,field753=f753,field754=f754,field755=f755,field756=f756,field757=f757,field758=f758,field759=f759,field760=f760,field761=f761,field762=f762,field763=f763,field764=f764,field765=f765,field766=f766,field767=f767,field768=f768,field769=f769,field770=f770,field771=f771,field772=f772,field773=f773,field774=f774,field775=f775,field776=f776,field777=f777,field778=f778,field779=f779,field780=f780,field781=f781,field782=f782,field783=f783,field784=f784,field785=f785,field786=f786,field787=f787,field788=f788,field789=f789,field790=f790,field791=f791,field792=f792,field793=f793,field794=f794,field795=f795,field796=f796,field797=f797,field798=f798,field799=f799,field800=f800,field801=f801,field802=f802,field803=f803,field804=f804,field805=f805,field806=f806,field807=f807,field808=f808,field809=f809,field810=f810,field811=f811,field812=f812,field813=f813,field814=f814,field815=f815,field816=f816,field817=f817,field818=f818,field819=f819,field820=f820,field821=f821,field822=f822,field823=f823,field824=f824,field825=f825,field826=f826,field827=f827,field828=f828,field829=f829,field830=f830,field831=f831,field832=f832,field833=f833,field834=f834,field835=f835,field836=f836,field837=f837,field838=f838,field839=f839,field840=f840,field841=f841,field842=f842,field843=f843,field844=f844,field845=f845,field846=f846,field847=f847,field848=f848,field849=f849,field850=f850,field851=f851,field852=f852,field853=f853,field854=f854,field855=f855,field856=f856,field857=f857,field858=f858,field859=f859,field860=f860,field861=f861,field862=f862,field863=f863,field864=f864,field865=f865,field866=f866,field867=f867,field868=f868,field869=f869,field870=f870,field871=f871,field872=f872,field873=f873,field874=f874,field875=f875,field876=f876,field877=f877,field878=f878,field879=f879,field880=f880,field881=f881,field882=f882,field883=f883,field884=f884,field885=f885,field886=f886,field887=f887,field888=f888,field889=f889,field890=f890,field891=f891,field892=f892,field893=f893,field894=f894,field895=f895,field896=f896,field897=f897,field898=f898,field899=f899,field900=f900,field901=f901,field902=f902,field903=f903,field904=f904,field905=f905,field906=f906,field907=f907,field908=f908,field909=f909,field910=f910,field911=f911,field912=f912,field913=f913,field914=f914,field915=f915,field916=f916,field917=f917,field918=f918,field919=f919,field920=f920,field921=f921,field922=f922,field923=f923,field924=f924,field925=f925,field926=f926,field927=f927,field928=f928,field929=f929,field930=f930,field931=f931,field932=f932,field933=f933,field934=f934,field935=f935,field936=f936,field937=f937,field938=f938,field939=f939,field940=f940,field941=f941,field942=f942,field943=f943,field944=f944,field945=f945,field946=f946,field947=f947,field948=f948,field949=f949,field950=f950,field951=f951,field952=f952,field953=f953,field954=f954,field955=f955,field956=f956,field957=f957,field958=f958,field959=f959,field960=f960,field961=f961,field962=f962,field963=f963,field964=f964,field965=f965,field966=f966,field967=f967,field968=f968,field969=f969,field970=f970,field971=f971,field972=f972,field973=f973,field974=f974,field975=f975,field976=f976,field977=f977,field978=f978,field979=f979,field980=f980,field981=f981,field982=f982,field983=f983,field984=f984,field985=f985,field986=f986,field987=f987,field988=f988,field989=f989,field990=f990,field991=f991,field992=f992,field993=f993,field994=f994,field995=f995,field996=f996,field997=f997,field998=f998,field999=f999,field1000=f1000,field1001=f501,field1002=f2,field1003=f3,field1004=f4,field1005=f5,field1006=f6,field1007=f7,field1008=f8,field1009=f9,field1010=f10,field1011=f11,field1012=f12,field1013=f13,field1014=f14,field1015=f15,field1016=f16,field1017=f17,field1018=f18,field1019=f19,field1020=f20,field1021=f21,field1022=f22,field1023=f23,field1024=f24,field1025=f25,field1026=f26,field1027=f27,field1028=f28,field1029=f29,field1030=f30,field1031=f31,field1032=f32,field1033=f33,field1034=f34,field1035=f35,field1036=f36,field1037=f37,field1038=f38,field1039=f39,field1040=f40,field1041=f41,field1042=f42,field1043=f43,field1044=f44,field1045=f45,field1046=f46,field1047=f47,field1048=f48,field1049=f49,field1050=f50,field1051=f51,field1052=f52,field1053=f53,field1054=f54,field1055=f55,field1056=f56,field1057=f57,field1058=f58,field1059=f59,field1060=f60,field1061=f61,field1062=f62,field1063=f63,field1064=f64,field1065=f65,field1066=f66,field1067=f67,field1068=f68,field1069=f69,field1070=f70,field1071=f71,field1072=f72,field1073=f73,field1074=f74,field1075=f75,field1076=f76,field1077=f77,field1078=f78,field1079=f79,field1080=f80,field1081=f81,field1082=f82,field1083=f83,field1084=f84,field1085=f85,field1086=f86,field1087=f87,field1088=f88,field1089=f89,field1090=f90,field1091=f91,field1092=f92,field1093=f93,field1094=f94,field1095=f95,field1096=f96,field1097=f97,field1098=f98,field1099=f99,field1100=f100,field1101=f101,field1102=f102,field1103=f103,field1104=f104,field1105=f105,field1106=f106,field1107=f107,field1108=f108,field1109=f109,field1110=f110,field1111=f111,field1112=f112,field1113=f113,field1114=f114,field1115=f115,field1116=f116,field1117=f117,field1118=f118,field1119=f119,field1120=f120,field1121=f121,field1122=f122,field1123=f123,field1124=f124,field1125=f125,field1126=f126,field1127=f127,field1128=f128,field1129=f129,field1130=f130,field1131=f131,field1132=f132,field1133=f133,field1134=f134,field1135=f135,field1136=f136,field1137=f137,field1138=f138,field1139=f139,field1140=f140,field1141=f141,field1142=f142,field1143=f143,field1144=f144,field1145=f145,field1146=f146,field1147=f147,field1148=f148,field1149=f149,field1150=f150,field1151=f151,field1152=f152,field1153=f153,field1154=f154,field1155=f155,field1156=f156,field1157=f157,field1158=f158,field1159=f159,field1160=f160,field1161=f161,field1162=f162,field1163=f163,field1164=f164,field1165=f165,field1166=f166,field1167=f167,field1168=f168,field1169=f169,field1170=f170,field1171=f171,field1172=f172,field1173=f173,field1174=f174,field1175=f175,field1176=f176,field1177=f177,field1178=f178,field1179=f179,field1180=f180,field1181=f181,field1182=f182,field1183=f183,field1184=f184,field1185=f185,field1186=f186,field1187=f187,field1188=f188,field1189=f189,field1190=f190,field1191=f191,field1192=f192,field1193=f193,field1194=f194,field1195=f195,field1196=f196,field1197=f197,field1198=f198,field1199=f199,field1200=f200,field1201=f201,field1202=f202,field1203=f203,field1204=f204,field1205=f205,field1206=f206,field1207=f207,field1208=f208,field1209=f209,field1210=f210,field1211=f211,field1212=f212,field1213=f213,field1214=f214,field1215=f215,field1216=f216,field1217=f217,field1218=f218,field1219=f219,field1220=f220,field1221=f221,field1222=f222,field1223=f223,field1224=f224,field1225=f225,field1226=f226,field1227=f227,field1228=f228,field1229=f229,field1230=f230,field1231=f231,field1232=f232,field1233=f233,field1234=f234,field1235=f235,field1236=f236,field1237=f237,field1238=f238,field1239=f239,field1240=f240,field1241=f241,field1242=f242,field1243=f243,field1244=f244,field1245=f245,field1246=f246,field1247=f247,field1248=f248,field1249=f249,field1250=f250,field1251=f251,field1252=f252,field1253=f253,field1254=f254,field1255=f255,field1256=f256,field1257=f257,field1258=f258,field1259=f259,field1260=f260,field1261=f261,field1262=f262,field1263=f263,field1264=f264,field1265=f265,field1266=f266,field1267=f267,field1268=f268,field1269=f269,field1270=f270,field1271=f271,field1272=f272,field1273=f273,field1274=f274,field1275=f275,field1276=f276,field1277=f277,field1278=f278,field1279=f279,field1280=f280,field1281=f281,field1282=f282,field1283=f283,field1284=f284,field1285=f285,field1286=f286,field1287=f287,field1288=f288,field1289=f289,field1290=f290,field1291=f291,field1292=f292,field1293=f293,field1294=f294,field1295=f295,field1296=f296,field1297=f297,field1298=f298,field1299=f299,field1300=f300,field1301=f301,field1302=f302,field1303=f303,field1304=f304,field1305=f305,field1306=f306,field1307=f307,field1308=f308,field1309=f309,field1310=f310,field1311=f311,field1312=f312,field1313=f313,field1314=f314,field1315=f315,field1316=f316,field1317=f317,field1318=f318,field1319=f319,field1320=f320,field1321=f321,field1322=f322,field1323=f323,field1324=f324,field1325=f325,field1326=f326,field1327=f327,field1328=f328,field1329=f329,field1330=f330,field1331=f331,field1332=f332,field1333=f333,field1334=f334,field1335=f335,field1336=f336,field1337=f337,field1338=f338,field1339=f339,field1340=f340,field1341=f341,field1342=f342,field1343=f343,field1344=f344,field1345=f345,field1346=f346,field1347=f347,field1348=f348,field1349=f349,field1350=f350,field1351=f351,field1352=f352,field1353=f353,field1354=f354,field1355=f355,field1356=f356,field1357=f357,field1358=f358,field1359=f359,field1360=f360,field1361=f361,field1362=f362,field1363=f363,field1364=f364,field1365=f365,field1366=f366,field1367=f367,field1368=f368,field1369=f369,field1370=f370,field1371=f371,field1372=f372,field1373=f373,field1374=f374,field1375=f375,field1376=f376,field1377=f377,field1378=f378,field1379=f379,field1380=f380,field1381=f381,field1382=f382,field1383=f383,field1384=f384,field1385=f385,field1386=f386,field1387=f387,field1388=f388,field1389=f389,field1390=f390,field1391=f391,field1392=f392,field1393=f393,field1394=f394,field1395=f395,field1396=f396,field1397=f397,field1398=f398,field1399=f399,field1400=f400,field1401=f401,field1402=f402,field1403=f403,field1404=f404,field1405=f405,field1406=f406,field1407=f407,field1408=f408,field1409=f409,field1410=f410,field1411=f411,field1412=f412,field1413=f413,field1414=f414,field1415=f415,field1416=f416,field1417=f417,field1418=f418,field1419=f419,field1420=f420,field1421=f421,field1422=f422,field1423=f423,field1424=f424,field1425=f425,field1426=f426,field1427=f427,field1428=f428,field1429=f429,field1430=f430,field1431=f431,field1432=f432,field1433=f433,field1434=f434,field1435=f435,field1436=f436,field1437=f437,field1438=f438,field1439=f439,field1440=f440,field1441=f441,field1442=f442,field1443=f443,field1444=f444,field1445=f445,field1446=f446,field1447=f447,field1448=f448,field1449=f449,field1450=f450,field1451=f451,field1452=f452,field1453=f453,field1454=f454,field1455=f455,field1456=f456,field1457=f457,field1458=f458,field1459=f459,field1460=f460,field1461=f461,field1462=f462,field1463=f463,field1464=f464,field1465=f465,field1466=f466,field1467=f467,field1468=f468,field1469=f469,field1470=f470,field1471=f471,field1472=f472,field1473=f473,field1474=f474,field1475=f475,field1476=f476,field1477=f477,field1478=f478,field1479=f479,field1480=f480,field1481=f481,field1482=f482,field1483=f483,field1484=f484,field1485=f485,field1486=f486,field1487=f487,field1488=f488,field1489=f489,field1490=f490,field1491=f491,field1492=f492,field1493=f493,field1494=f494,field1495=f495,field1496=f496,field1497=f497,field1498=f498,field1499=f499,field1500=f500)',\n  Password: 'cassandra',\n  Password_encrypted: false\n )\nINPUT FROM Oracle_ChangeDataStream;\n\ncreate Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\nEND APPLICATION DBRTOCW;\ndeploy application DBRTOCW in default;\nstart DBRTOCW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\nCREATE OR REPLACE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCREATE FLOW @APPNAME@_Agent_flow;\n\nCREATE OR REPLACE SOURCE @SourceName@1 USING Global.MSJet (\n  Tables: 'dbo.compsrc',\n    username: 'qatest',\n    DatabaseName: 'qatest',\n    FetchTransactionMetadata: true,\n    filterTransactionBoundaries: true,\n    compression: false,\n    ConnectionURL: '10.211.55.3:1433',\n    CommittedTransactions: true,\n    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',\n    SendBeforeImage: true,\n    password: 'w3b@ct10n' )\nOUTPUT TO @SRCINPUTSTREAM@;\n\nEND FLOW @APPNAME@_Agent_flow;\n\nCREATE FLOW @APPNAME@_Agent_flow1;\n\nCREATE OR REPLACE SOURCE @SourceName@2 USING Global.MSJet (\n  Tables: 'dbo.compsrc',\n    username: 'qatest',\n    DatabaseName: 'qatest',\n    FetchTransactionMetadata: true,\n    filterTransactionBoundaries: true,\n    compression: false,\n    ConnectionURL: '10.211.55.3:1433',\n    CommittedTransactions: true,\n    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',\n    SendBeforeImage: true,\n    password: 'w3b@ct10n' )\nOUTPUT TO @SRCINPUTSTREAM@1;\n\nEND FLOW @APPNAME@_Agent_flow1;\n\nCREATE FLOW @APPNAME@_server_flow;\n\nCREATE OR REPLACE TARGET @targetName@1 USING Global.DatabaseWriter\n(\n  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',\n  BatchPolicy:'EventCount:10,Interval:60',\n  CommitPolicy:'EventCount:10,Interval:60',\n  ParallelThreads:'',\n  CheckPointTable:'CHKPOINT',\n  Password_encrypted:'false',\n  Tables:'qatest.MSJEtsrc1,qatest.MSJEtar1;qatest.MSJEtsrc2,qatest.MSJEtar2;',\n  CDDLAction:'Process',\n  Password:'w3b@ct10n',\n  StatementCacheSize:'50',\n  ConnectionURL:'jdbc:sqlserver://10.211.55.3:1433;databaseName=qatest',\n  DatabaseProviderType:'Default',\n  Username:'qatest',\n  PreserveSourceTransactionBoundary:'false',\n  adapterName:'DatabaseWriter'\n)\nINPUT FROM @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@2 USING Global.DatabaseWriter\n(\n  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',\n  BatchPolicy:'EventCount:10,Interval:60',\n  CommitPolicy:'EventCount:10,Interval:60',\n  ParallelThreads:'',\n  CheckPointTable:'CHKPOINT',\n  Password_encrypted:'false',\n  Tables:'qatest.MSJEtsrc1,qatest.MSJEtar1;qatest.MSJEtsrc2,qatest.MSJEtar2;',\n  CDDLAction:'Process',\n  Password:'w3b@ct10n',\n  StatementCacheSize:'50',\n  ConnectionURL:'jdbc:sqlserver://10.211.55.3:1433;databaseName=qatest',\n  DatabaseProviderType:'Default',\n  Username:'qatest',\n  PreserveSourceTransactionBoundary:'false',\n  adapterName:'DatabaseWriter'\n)\nINPUT FROM @SRCINPUTSTREAM@1;\n\nCREATE TARGET @targetsys@ USING Global.SysOut (\n  name: '@targetsys@' )\nINPUT FROM @SRCINPUTSTREAM@;\n\nEND FLOW @APPNAME@_server_flow;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@ with @APPNAME@_Agent_flow in AGENTS, @APPNAME@_Agent_flow1 in AGENTS ,@APPNAME@_server_flow on any in default;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application ADW1;\nundeploy application ADW1;\ndrop application ADW1 cascade;\nCREATE APPLICATION ADW1;\n\nCREATE  SOURCE SqlServerInitialLoad1 USING DatabaseReader  \n (\n Username:'src_username',\n Password:'src_password',\n ConnectionURL: 'src_url',\n Tables:'@SOURCE-TABLES@',\n FetchSize:2000\n) \nOUTPUT TO InitialLoadStream1;\n\nCREATE TARGET AzureDWInitialLoad1 USING AzureSQLDWHWriter(\nConnectionURL: '@SQLDW-URL@',\n        username: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@',\n        uploadpolicy:'@EVENT-COUNT@'\n)\nINPUT FROM InitialLoadStream1;\n\nEND APPLICATION ADW1;\ndeploy application ADW1;\nstart application ADW1;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION app1;\nSTOP APPLICATION app2;\nSTOP APPLICATION app3;\nSTOP APPLICATION app4;\nSTOP APPLICATION app5;\nSTOP APPLICATION app6;\nSTOP APPLICATION app7;\nUNDEPLOY APPLICATION app1;\nUNDEPLOY APPLICATION app2;\nUNDEPLOY APPLICATION app3;\nUNDEPLOY APPLICATION app4;\nUNDEPLOY APPLICATION app5;\nUNDEPLOY APPLICATION app6;\nUNDEPLOY APPLICATION app7;\nDROP APPLICATION app1 CASCADE;\nDROP APPLICATION app2 CASCADE;\nDROP APPLICATION app3 CASCADE;\nDROP APPLICATION app4 CASCADE;\nDROP APPLICATION app5 CASCADE;\nDROP APPLICATION app6 CASCADE;\nDROP APPLICATION app7 CASCADE;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',\nacks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\nCREATE APPLICATION app1 RECOVERY 1 SECOND INTERVAL;\n\ncreate flow sourceflow;\n\ncreate type type1(\n  id String,\n  name String,\n  city string\n);\n\nCREATE STREAM rawstream OF Global.waevent persist using KafkaPropset;\nCREATE STREAM kps_typedStream OF type1 partition by city persist using KafkaPropset;\nCREATE STREAM sourcestream of Global.waevent;\n\nCREATE OR REPLACE SOURCE s USING oracleReader  (\n  Username:'qatest',\n  Password:'qatest',\n  ConnectionURL:'localhost:1521/xe',\n  Tables:'QATEST.test%',\n  FetchSize:1\n )\nOUTPUT TO rawstream;\n\nend flow sourceflow;\ncreate flow targetflow;\ncreate cq cq1\nINSERT INTO sourcestream\nSELECT * from rawstream;\n\nCREATE CQ cq2\nINSERT INTO kps_typedStream\nSELECT TO_STRING(data[0]),\nTO_STRING(data[1]),\nTO_STRING(data[2])FROM rawstream;\nend flow targetflow;\n\nend application app1;\n-- deploy application app1;\n-- deploy application app1 with sourceflow in AGENTS, targetflow on any in default;\n\nCREATE APPLICATION app2 RECOVERY 1 SECOND INTERVAL;\n\nCREATE TARGET app2_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test01,QATEST.KPS1'\n) INPUT FROM rawstream;\n\n\nend application app2;\ndeploy application app2;\n\n\nCREATE APPLICATION app3 RECOVERY 1 SECOND INTERVAL;\n\nCREATE TARGET app3_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test02,QATEST.KPS2'\n) INPUT FROM rawstream;\n\nend application app3;\ndeploy application app3;\n\n\nCREATE APPLICATION app4 RECOVERY 1 SECOND INTERVAL;\n\nCREATE TARGET app4_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test03,QATEST.KPS3'\n) INPUT FROM rawstream;\n\nend application app4;\ndeploy application app4;\n\n\nCREATE APPLICATION app5 RECOVERY 1 SECOND INTERVAL;\n\nCREATE TARGET app5_target1 USING KafkaWriter VERSION '0.11.0'(\nbrokeraddress:'localhost:9092',\ntopic:'snappy',\nKafkaConfig:'compression.type=snappy'\n)\nFORMAT USING DSVFormatter ()\nINPUT FROM kps_typedStream;\n\nCREATE TARGET app5_target2 USING KafkaWriter VERSION '0.11.0'(\nbrokeraddress:'localhost:9092',\ntopic:'gzip',\nKafkaConfig:'compression.type=gzip'\n)\nFORMAT USING DSVFormatter ()\nINPUT FROM rawstream;\n\nCREATE TARGET app5_target3 USING KafkaWriter VERSION '0.11.0'(\nbrokeraddress:'localhost:9092',\ntopic:'lz4',\nKafkaConfig:'compression.type=lz4'\n)\nFORMAT USING DSVFormatter ()\nINPUT FROM rawstream;\n\n\nend application app5;\ndeploy application app5;\n\nCREATE APPLICATION app6 RECOVERY 1 SECOND INTERVAL;\n\nCREATE TARGET app6_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test03,QATEST.KPS4'\n) INPUT FROM rawstream;\n\nend application app6;\ndeploy application app6;\n\nCREATE APPLICATION app7 RECOVERY 1 SECOND INTERVAL;\n\nCREATE TARGET app7_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test01,QATEST.KPS_NEW1;qatest.test02,QATEST.KPS_NEW2;qatest.test03,QATEST.KPS_NEW3;'\n) INPUT FROM rawstream;\n\nend application app7;\ndeploy application app7;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "/* *******************************************************************\n Ibasis App for monitoring network with pings to different locations \n****************************************************************** */\n\nuse global;\ndrop namespace PingMonitorBatchApp cascade;\ncreate namespace PingMonitorBatchApp;\nuse PingMonitorBatchApp;\n\nCREATE APPLICATION PingMonitorBatchApp;\n\ncreate\tflow PingMonitorBatchAppFlow;\t\n\nCREATE TYPE PingDateEntry(\n    MaxTestDateTime  DateTime KEY\n );\n\n\nCREATE TYPE PINGEVENT(\n        SERVICE_NAME  String,\n        TEST_DATETIME DateTime,\n\tSOURCE_VERIFIER_ALIAS String,\n\tTARGET_VERIFIER_ALIAS  String,\n        DELAY_AVG float,\n\tPACKET_LOST float,\n        JITTER_AVG float,\n        SEQ_NO     long\n);\n\n\nCREATE CACHE PINGEVENTCACHE  using DatabaseReader (\n   ConnectionURL:'jdbc:oracle:thin:@server1204v.ivanet.net:1960:prtld',\n   Username:'xtract',\n   Password:'xtract123',\n   Query:'select SERVICE_NAME,TEST_DATETIME,SOURCE_VERIFIER_ALIAS,TARGET_VERIFIER_ALIAS,NVL(DELAY_AVG,0),NVL(PACKET_LOST,0),NVL(JITTER_AVG,0),SEQ_NO  from ir34_ping_site where TEST_DATETIME > (select NVL(to_date(\\'1970-01-01\\', \\'YYYY-MM-DD\\') + (MAX(MaxTestDateTime)/ 86400000), sysdate -10/24) from IR34SiteLastDate)order by TEST_DATETIME ASC'\n ) QUERY (keytomap:'SOURCE_VERIFIER_ALIAS', refreshinterval: '60 second', publishonrefresh: 'true') OF PINGEVENT;\n\n\ncreate Target trace0 using CSVWriter (filename:'logs/PINGEVENTCACHE.csv') input from PINGEVENTCACHE;\n\nCREATE WACTIONSTORE IR34ElasticSearchWS  \nCONTEXT OF   PingDateEntry \nEVENT TYPES ( PingDateEntry  )\nPERSIST IMMEDIATE\nUSING ( storageProvider: \"elasticsearch\" );\n\n\nCREATE WACTIONSTORE IR34SiteLastDateWS CONTEXT OF PingDateEntry\n \t\tEVENT TYPES ( PingDateEntry )\n \t    PERSIST EVERY 50 second USING (\n \t    JDBC_DRIVER:'oracle.jdbc.driver.OracleDriver',\n \t    JDBC_URL:'jdbc:oracle:thin:@server1204v.ivanet.net:1960:prtld',\n \t    JDBC_USER:'xtract',\n \t    JDBC_PASSWORD:'xtract123',\n \t    DDL_GENERATION:'create-or-extend-tables',\n            CONTEXT_TABLE:'IR34SiteLastDate'\n \t   );\t\n\t   \n\nCREATE CQ populateMaxDate\n INSERT INTO IR34SiteLastDateWS\n select TO_DATE(MAX(TO_LONG(TEST_DATETIME)))\n from  PINGEVENTCACHE\n , heartbeat(interval '20' second, 1 , 1) hb;\n\n\n \n/* create Target trace1 using CSVWriter (filename:'logs/IR34SiteLastDateWS.csv') input from IR34SiteLastDateWS; */\n\n\nCREATE TYPE DELAYTHRESHTYPE(\n        SITE_KEY  STRING,\n        SOURCE_SITE STRING,\n        TARGET_SITE STRING,\n        DELAY_THRESH FLOAT,\n        PROFILE_ID   INT\n);\n\n\nCREATE CACHE DELAYTHRESHCACHE  USING DATABASEREADER (\n   ConnectionURL:'jdbc:oracle:thin:@timstsnap05.ivanet.net:1521:timsrdg2',\n   Username:'network_cfg',\n   Password:'network_cfg',\n   QUERY: 'select src.site_abbreviation || \\'-\\' || tgt.site_abbreviation as SITE_KEY, src.site_abbreviation as SOURCE_SITE, tgt.site_abbreviation as TARGET_SITE,D.DELAY as DELAY_THRESH, d.PROFILE_ID from site_attributes src, geography gsrc, threshold_delay d, site_attributes tgt, geography gtgt where  Src.GEO_ABBREVIATION = Gsrc.GEO_ABBREVIATION  and src.GEO_ABBREVIATION = D.SOURCE_VERIFIER_GEO  and tgt.GEO_ABBREVIATION = D.TARGET_VERIFIER_GEO  and tgt.GEO_ABBREVIATION = Gtgt.GEO_ABBREVIATION  and tgt.PROFILE_ID = D.PROFILE_ID  and d.profile_id = 1'\n ) QUERY (KEYTOMAP:'SITE_KEY', REFRESHINTERVAL: '60 SECOND', publishonrefresh: 'true') OF DELAYTHRESHTYPE;\n\n\nCREATE TARGET TRACE2 USING CSVWRITER (FILENAME:'logs/DELAYTHRESHCACHE.CSV') INPUT FROM DELAYTHRESHCACHE;\n\n\nCREATE TYPE ALERTENTRY(\n\tALERTKEY STRING KEY,\n \tTOLOC STRING,\n \tFROMLOC STRING,\t\n        STATUS STRING,\n\tINTSTATUS INT,\n\tACTUAL_DELAY INT,\n\tDELAY_THRESHOLD INT,\n        TESTTIME DATETIME\n );\n\nCREATE STREAM PingStreamRaw OF PINGEVENT;\n\nCREATE CQ PROCESSSortCQ\n INSERT INTO PingStreamRaw\n select  SERVICE_NAME  ,\n        TEST_DATETIME ,\n        SOURCE_VERIFIER_ALIAS ,\n        TARGET_VERIFIER_ALIAS  ,\n        DELAY_AVG ,\n        PACKET_LOST ,\n        JITTER_AVG ,\n        SEQ_NO     \nfrom PINGEVENTCACHE,  heartbeat(interval '30' second) hb;\n\nCREATE SORTER PingSorterStream OVER\nPingStreamRaw ON TEST_DATETIME OUTPUT TO SortedPingStream\nWITHIN 70 second  \nOUTPUT ERRORS TO fooErrorStream;\n\n\n CREATE STREAM ALERTENTRYSTREAM OF ALERTENTRY;\n\n CREATE CQ PROCESSCQ1\n INSERT INTO ALERTENTRYSTREAM\n SELECT DW.SOURCE_VERIFIER_ALIAS + \" - \" + DW.TARGET_VERIFIER_ALIAS,\n        DW.TARGET_VERIFIER_ALIAS, \n        DW.SOURCE_VERIFIER_ALIAS,         \n        CASE WHEN DW.DELAY_AVG > DT.DELAY_THRESH THEN \"WARNING\" \n             ELSE \"NORMAL\" \n        END,\n        CASE WHEN DW.DELAY_AVG > DT.DELAY_THRESH  THEN 2\n             ELSE 1\n        END,\n        DW.DELAY_AVG,\n        DT.DELAY_THRESH,\n        DW.TEST_DATETIME\n FROM SortedPingStream DW  , DELAYTHRESHCACHE DT \n WHERE DT.SOURCE_SITE = DW.SOURCE_VERIFIER_ALIAS\n   AND DT.TARGET_SITE = DW.TARGET_VERIFIER_ALIAS;\n\nCREATE TARGET TRACE4 USING CSVWRITER (FILENAME:'logs/ALERTENTRIES.CSV') INPUT FROM ALERTENTRYSTREAM;\n\nCREATE WINDOW DELAYWARNWINDOW OVER  ALERTENTRYSTREAM KEEP WITHIN 20 minute ON TESTTIME;\n\nCREATE TYPE ALERTAGGENTRY(\n        ALERTKEY STRING KEY,\n        ALERT_STATUS STRING,\n        NUMBER_OF_VIOLATIONS INT,\n        DELAY_THRESHOLD INT,\n        FIRSTTIME DATETIME\n );\n\n\n\nCREATE STREAM ALERTAGGSTREAM OF ALERTAGGENTRY;\n\n\nCREATE CQ PROCAGGCQ1 \n \tINSERT INTO ALERTAGGSTREAM\n\t\tSELECT  ALERTKEY,\n                        CASE WHEN COUNT(*) > 5 THEN 'CRITICAL'\n                             WHEN COUNT(*) >= 3 AND COUNT(*) <= 5 THEN 'MAJOR'\n                             WHEN COUNT(*) >=1 AND COUNT(*) < 3 THEN 'MINOR'\n                        ELSE 'NORMAL'\n                        END ,\n                        COUNT(*),\n                        DELAY_THRESHOLD,\n                        FIRST(TESTTIME)\n\t\tFROM DELAYWARNWINDOW   \n                WHERE STATUS = 'WARNING'\n\t\tGROUP BY ALERTKEY, STATUS,DELAY_THRESHOLD;\n\n\n-- new waction stores\n\nCREATE WACTIONSTORE PING_DETAILS\n  CONTEXT OF ALERTENTRY\n  EVENT TYPES (ALERTENTRY )\n  PERSIST EVERY 60 second USING (\n  JDBC_DRIVER:'oracle.jdbc.driver.OracleDriver',\n  JDBC_URL:'jdbc:oracle:thin:@timstsnap05.ivanet.net:1521:timsrdg2',\n  JDBC_USER:'network_cfg', JDBC_PASSWORD:'network_cfg',\n  DDL_GENERATION:'create-or-extend-tables',\n--  DDL_GENERATION:'drop-and-create-tables',\n  CONTEXT_TABLE:'PING_DETAILS'\n  );\n\nCREATE CQ STOREDETAILSCQ\n       INSERT INTO PING_DETAILS\n              SELECT * FROM DELAYWARNWINDOW\n              WHERE STATUS <> 'NORMAL';\n\nCREATE WACTIONSTORE PingStore CONTEXT OF ALERTAGGENTRY\n        EVENT TYPES( ALERTAGGENTRY )\n        PERSIST NONE USING ( ) ;\n\nCREATE CQ LoadPingStoreCQ\n                INSERT INTO PingStore\n                SELECT *\n                FROM ALERTAGGSTREAM s;\n\n-- end of stores \n\n\n\nCREATE STREAM AlertStream OF Global.AlertEvent;\n\n\nCREATE CQ GenerateAlerts\nINSERT INTO AlertStream\nSELECT 'FROM TO SITES',      \n       ALERTKEY, \nCASE\nWHEN ALERT_STATUS != 'NORMAL' THEN 'warning'\nELSE 'info' END,\nCASE\nWHEN  (DNOW()  > DADD(FIRSTTIME, DMINS(20)) OR ALERT_STATUS = 'NORMAL')   THEN 'cancel'\nELSE 'raise' END,\nCASE\nWHEN ALERT_STATUS  != 'NORMAL' THEN ALERT_STATUS + ' ALERT FOR ' + ALERTKEY + ' has delay of over threshold ' + TO_STRING(DELAY_THRESHOLD) + ' has ' + TO_STRING(NUMBER_OF_VIOLATIONS) + '  violations over limit over the hour ' + TO_STRING(FIRSTTIME,'MM/dd/yyyy HH:mm:ss')  ELSE ''\nEND\n    FROM ALERTAGGSTREAM \nWHERE NUMBER_OF_VIOLATIONS > 1;\n--    FROM RESULTSSTORE; \n\n\nCREATE TARGET TRACE3 USING CSVWRITER (FILENAME:'logs/ALERTS.CSV') INPUT FROM AlertStream;\n\nCREATE SUBSCRIPTION PingAppEmailAlert\nUSING EmailAdapter (\nsmtpurl:'smtp.nyc.ibasis.net:25',\nsmtp_auth:'false',\nstarttls_enable:'false',\nsubject:\"IR34 Alert from Webaction\", emailList:\"pgopal@ibasis.net,ed@webaction.com\",senderEmail:\"webactionsupport@ibasis.net\"\n)\nINPUT FROM AlertStream;\n\nCREATE SUBSCRIPTION PosAppWebAlert\nUSING WebAlertAdapter( )\nINPUT FROM AlertStream;\n\n\nEND flow PingMonitorBatchAppFlow;\t\n\n/*\t\nCREATE TYPE AlarmThreshold(\n\tprofile int,\n\tthreshholdValue int\n);\n\nCREATE CACHE AlarmThresholdCache using DatabaseReader (\n   ConnectionURL:'jdbc:oracle:thin:@10.1.110.128:1521:orcl',\n   Username:'scott',\n   Password:'tiger',\n   Query:'select * from scott.ALARM_THRESHOLD'\n ) QUERY (keytomap:'profile', refreshinterval: '60000000') OF AlarmThreshold; \n \n\nCREATE TYPE pingEvent(\n\ttoLoc String,\n\tfromLoc String,\t\n        delay float,\n        jitter float,\n\tpackloss float,\n        dateTime DateTime\n);\n\nCREATE STREAM dataStream OF pingEvent;\n\n Create Target trace1 Using Sysout (name:'rawping') input From LCRStream;\n\n \n CREATE CQ renderEvents\n INSERT INTO dataStream\n SELECT\n \tx.TOLOCATION, x.FROMLOCACTION,  TO_FLOAT(x.DELAYAMT), TO_FLOAT(x.JITTERAMT), TO_FLOAT(x.PACKLOSSAMT), DNOW()\n FROM LCRStream1 x;\n \nCREATE  jumping WINDOW  dataWindow OVER dataStream KEEP WITHIN 30 second; \n \n CREATE TYPE AlertEntry(\n\talertKey String KEY,\n \ttoLoc String,\n \tfromLoc String,\t\n    status String,\n\tintStatus int,\n    dateTime DateTime\n );\n\n CREATE STREAM AlertEntryStream OF AlertEntry;\n\n CREATE CQ processCQ1\n INSERT INTO AlertEntryStream\n select dw.toLoc + dw.fromLoc,\n        dw.toLoc, \n        dw.fromLoc,         \n        CASE WHEN dw.delay > c.threshholdValue THEN \"WARNING\" \n             ELSE \"NORMAL\" \n        END,\n        CASE WHEN dw.delay > c.threshholdValue THEN 2\n             ELSE 1\n        END,\n        dw.dateTime\n from dataWindow dw, AlarmThresholdCache c;\n\t  \t \n Create Target trace2 Using Sysout (name:'alert') input From AlertEntryStream;\n*/ \n \nEND APPLICATION PingMonitorBatchApp;\n -- create dashboard using \"ClientApps/Overstock/ProdIndexDash.json\";\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE TYPE @APPNAME@type1 (\n companyName java.lang.String,\n merchantId java.lang.String,\n city java.lang.String);\n\nCREATE STREAM @APPNAME@TypedStream OF @APPNAME@type1 PARTITION BY city;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (\n  wildcard: '',\n  positionByEOF: false,\n  directory: ''\n  )\nPARSE USING DSVParser (\nheader:'true'\n)\nOUTPUT TO @APPNAME@Stream;\n\nCREATE OR REPLACE CQ @APPNAME@CQ\nINSERT INTO @APPNAME@TypedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\") as companyName,\nTO_STRING(data[1]) as merchantID,\nTO_STRING(data[10]) as city\nFROM @APPNAME@Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_jmstrgt USING JMSWriter (\n  QueueName: '',\n  UserName: '',\n  Password: '',\n  Ctx: '',\n  Provider: ''\n  )\nFORMAT USING DSVFormatter()\nINPUT FROM @APPNAME@TypedStream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APPNAME@_app;\nUNDEPLOY APPLICATION @APPNAME@_app;\nDROP APPLICATION @APPNAME@_app CASCADE;\n-- DROP EXCEPTIONSTORE @APPNAME@_exceptionstore;\n\nCREATE APPLICATION @APPNAME@_app RECOVERY 120 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE @APPNAME@_Source USING @SOURCE_ADAPTER@  (\n  Username:'qatest',\n  Password:'qatest',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: '',\n  ) OUTPUT TO @APPNAME@_Stream  ;\n\nCREATE OR REPLACE TARGET @APPNAME@_Target USING BigQueryWriter  (\n  Tables                        : '',\n  projectId                    : '',\n  ServiceAccountKey            : '',\n  Mode                         : 'APPENDONLY',\n  BatchPolicy                  : 'EventCount:1, Interval:60',\n  ) INPUT FROM @APPNAME@_Stream;\n\n-- CREATE OR REPLACE TARGET @APPNAME@_SysOut USING Global.SysOut (name: 'wa') INPUT FROM @APPNAME@_Stream;\n\nEND APPLICATION @APPNAME@_app;\nDEPLOY APPLICATION @APPNAME@_app;\nSTART APPLICATION @APPNAME@_app;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application Postgres_To_PostgresApp;\nundeploy application Postgres_To_PostgresApp;\ndrop application Postgres_To_PostgresApp cascade;\nCREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;\ncreate type pkFlag_type\n(\nTableName String,\nPK_UPDATE String,\nOperationName String\n);\nCREATE STREAM Postgres_TypedStream of pkFlag_type;\nCREATE OR REPLACE SOURCE Postgres_Src USING PostgreSQLReader  ( \n  ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: true,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.postgres_src'\n ) \nOUTPUT TO Postgres_Change_Data_Stream;\ncreate CQ Cqfilter \ninsert into Postgres_TypedStream\nselect \nMETA(u,'TableName').toString(),\nMETA(u,'PK_UPDATE').toString(),\nMETA(u,'OperationName').toString()\nfrom Postgres_Change_Data_Stream u;\nCREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( \n  name: 'postgres_PK_Out'\n ) INPUT FROM Postgres_TypedStream;\nCREATE  TARGET Postgres_FW USING FileWriter  ( \n  filename: 'Postgres_PKOut.log',\n  directory: '/Users/jenniffer/Product2/IntegrationTests/target/test-classes/testNG/PostgreSQLReader/logs'\n ) \nFORMAT USING DSVFormatter  (  ) \nINPUT FROM Postgres_TypedStream;\nend application Postgres_To_PostgresApp;\ndeploy application Postgres_To_PostgresApp;\nstart Postgres_To_PostgresApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create source @SOURCE_NAME@ USING MySQLReader \n(\nUsername: '@READER-UNAME@',\nPassword: '@READER-PASSWORD@',\nConnectionURL: 'jdbc:mysql://127.0.0.1:3306/@DBName@',\nTables: @WATABLES@,\nsendBeforeImage:'true',\nFilterTransactionBoundaries: 'true'\n) \nOUTPUT TO @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@ Using MSSqlReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n DatabaseName:'qatest',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'@SourceTable@',\n ConnectionPoolSize:1,\n StartPosition:'EOF'\n ) Output To @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;\n\n\n CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@SourceTable@,@TargetTable@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE OR REPLACE SOURCE DBSource USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'MINER.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@(\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:100,Interval:120',\n  CommitPolicy: 'EventCount:100,Interval:120',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n )\nINPUT FROM Oracle_ChangeDataStream;\n\n\ncreate Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\nEND APPLICATION DBRTOCW;\n\ndeploy application DBRTOCW on ANY in default;\n\nstart application DBRTOCW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP IS2noder.IS2Node;\nUNDEPLOY APPLICATION IS2noder.IS2Node;\nDROP APPLICATION IS2noder.IS2Node CASCADE;\n\nCREATE APPLICATION IS2Node;\n\nCREATE FLOW ISFLOW1;\n----------------------------------------------------\nCREATE source implicitSOurce USING FileReader (\n      directory:'@TEST-DATA-PATH@',\n      columndelimiter: ',',\n      wildcard:'ISdata.csv',\n      blocksize: 10240,\n      positionByEOF:false\n)\nPARSE USING DSVParser (\n      header:False,\n      trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE Atm(\n  productID String KEY,\n  stateID String,\n  productWeight int,\n  quantity double,\n  size long,\n  currentDate DateTime);\n\nEND FLOW ISFLOW1;\n----------------------------------------------------\n\nCREATE FLOW ISFLOW2;\n\nCREATE CACHE cache1 USING CsvReader(\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'ISdata.csv',\n  header: false,\n  columndelimiter: ',',\n  trimquote: false\n  ) QUERY (keytomap:'productID') OF Atm;\n\n\nCREATE STREAM newStream OF Atm;\n\n\nCREATE CQ newCQ\nINSERT INTO newStream\nSELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]), TO_DATE(data[5]) FROM\nCsvStream;\n\nCREATE WINDOW win1\nOVER newStream\nKEEP 50 rows;\n\n\nCREATE CQ newCQ2\nINSERT INTO newStream2\nSELECT productID as A , stateID AS B, productWeight AS C, quantity AS D, size AS E, currentDate AS F FROM\nnewStream;\n\n\nCREATE CQ newCQ3\nINSERT INTO newStream3 PARTITION BY A\nSELECT A,B,C,D,E,F FROM newStream2 order by C,D\nlink source event;\n\nCREATE CQ newCQ4\nINSERT INTO newStream4\nSELECT count(productID),currentDate FROM newStream ORDER BY currentDate\nlink source event;\n\nCREATE CQ newCQ5\nINSERT INTO newStream5\nSELECT x.*, y.* from cache1 x, newStream y WHERE x.productweight > 6 ORDER BY x.currentDate;\n\n\nCREATE WACTIONSTORE WS1 CONTEXT OF Atm\nEVENT TYPES(Atm );\n\nCREATE CQ newCQ6\nINSERT INTO WS1\nSELECT * FROM newStream WHERE productID = '001';\n\nCREATE CQ newCQ7\nINSERT INTO newStream6\nSELECT aa.productID FROM WS1 [push] aa, cache1 bb;\n\n\nCREATE CQ newCQ8\nINSERT INTO newStream7\nSELECT Sum(X.size) FROM (Select size from win1 where productweight > 5) X;\n\nEND FLOW ISFLOW2;\n----------------------------------------------------\n\nEND APPLICATION IS2Node;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader\n(\nFetchSize:'@FETCHSIZE@',\nQueueSize:'@QUEUESIZE@',\nUsername:'@USERNAME@',\nPassword:'@PASSWORD@',\nConnectionURL:'@URL@',\n_h_useClassic: @CLASSICMODE@,\nTables:'@TABLES@',\nCommittedTransactions:'@COMMITEDT@'\nCompression:'@COMPRESSION@'\n)\nOUTPUT TO @STREAM@ MAP (table: '@SOURCE_SCHEMA@.@SOURCE_TABLE@1')\nSELECT NUM_COL,CHAR_COL,VARCHAR2_COL,LONG_COL,DATE_COL,TIMESTAMP_COL where TO_INT(NUM_COL) > 1;\n\nCREATE TYPE LogType(\nNum_col String key,\nChar_col String,\nVarchar2_col String,\nlong_col String,\nTable String,\nOperation String\n);\n\nCREATE WINDOW CDCWindow\nOVER @STREAM@\nKEEP 1 ROWS;\n\nCREATE WACTIONSTORE CDCWS CONTEXT of LogType\nEVENT TYPES ( LogType )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ ToWactionStore\nINSERT INTO CDCWS\nSELECT * FROM CDCWindow\nLINK SOURCE EVENT;\n\nCREATE TARGET @SOURCE_NAME@_SYS USING SysOut (\n  name: '@SOURCE_NAME@_SYS' )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION SourceRetailApp;\n\nCREATE source RetailDataSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:no,\n  wildcard:'retaildata2M.csv',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO RetailOrders;\n\nCREATE TARGET RetailSourceDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceRetailAppData') input from RetailOrders;\n\nCREATE Source RetailHourlyStoreSales using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'storehourlyData.txt',\n  header: no,\n  columndelimiter: ',',\n  positionByEOF:false\n) OUTPUT TO RetailCacheSource1;\n\nCREATE TARGET RetailCacheDump1 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceRetailCacheData1') input from RetailCacheSource1;\n\nCREATE Source RetailStoreNameLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'StoreNames.csv',\n  header: no,\n  columndelimiter: ',',\n  positionByEOF:false\n) OUTPUT TO RetailCacheSource2;\n\nCREATE TARGET RetailCacheDump2 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceRetailCacheData2') input from RetailCacheSource2;\n\nCREATE Source RetailZipCodeLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'USAddresses.txt',\n  header: no,\n  columndelimiter: '\t',\n  positionByEOF:false\n) OUTPUT TO  RetailCacheSource3;\n\nCREATE TARGET RetailCacheDump3 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceRetailCacheData3') input from RetailCacheSource3;\n\n\nEND APPLICATION SourceRetailApp;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');\n\nCREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.WAEvent PERSIST USING @APPNAME@KafkaPropset;\n\nCREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (\n  ProviderName: '',\n  Provider: '',\n  Ctx: '',\n  QueueName: '',\n  Topic:'',\n  UserName: '',\n  Password: '',\n  EnableTransaction: '',\n  transactionpolicy: ''\n )\nPARSE USING DSVParser ()\nOUTPUT TO @APPNAME@PersistStream@RANDOM@;\n\nCREATE OR REPLACE TARGET @APPNAME@_filetrgt USING FileWriter (\n  filename: '',\n  flushpolicy: '',\n  directory: '',\n  rolloverpolicy: '' )\nFORMAT USING DSVFormatter  (\n  members: 'data' )\nINPUT FROM @APPNAME@PersistStream@RANDOM@;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@;\nCREATE  SOURCE @SourceName@ USING MSSqlReader  ( \n  Username: '@UserName@',\n  Password: '@Password@',\n  DatabaseName: 'qatest',\n  ConnectionURL: '@SourceConnectionURL@',\n  Tables: 'qatest.@SourceTable@',\n  ConnectionPoolSize: 1,\n  StartPosition: 'EOF',\n  ReplicationSlotName: 'null'\n ) \nOUTPUT TO @SRCINPUTSTREAM@;\ncreate Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( \n  ConnectionURL: '@TargetConnectionURL@',\n  Username: '@UserName@',\n  Password: '@Password@',\n  BatchPolicy: 'EventCount:1,Interval:1',\n  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n ) \nINPUT FROM @SRCINPUTSTREAM@;\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nCREATE APPLICATION FileSource;\n\nCREATE  TYPE FileStr2_Type  ( seq java.lang.Integer\n );\n\nCREATE STREAM FileStr2 OF FileStr2_Type;\n\nCREATE OR REPLACE JUMPING WINDOW FileDataAggregation OVER FileStr2 KEEP 1000000 ROWS;\n\nCREATE OR REPLACE SOURCE FileSource USING FileReader  (\n  blocksize: 64,\n  positionbyeof: false,\n  rolloverstyle: 'Default',\n  includesubdirectories: false,\n  adapterName: 'FileReader',\n  directory: '@TEST-DATA-PATH@/Validate-Striim',\n  skipbom: true,\n  wildcard: 'FiletoRead.txt'\n )\n PARSE USING DSVParser  (\n  charset: 'UTF-8',\n  handler: 'com.webaction.proc.DSVParser_1_0',\n  linenumber: '-1',\n  nocolumndelimiter: true,\n  trimwhitespace: false,\n  columndelimiter: ',',\n  columndelimittill: '-1',\n  ignoremultiplerecordbegin: 'true',\n  ignorerowdelimiterinquote: false,\n  parserName: 'DSVParser',\n  separator: ':',\n  blockascompleterecord: false,\n  ignoreemptycolumn: false,\n  rowdelimiter: '\\n',\n  header: false,\n  headerlineno: 0,\n  quoteset: '\\\"',\n  trimquote: true\n )\nOUTPUT TO FileStr1 ;\n\nCREATE OR REPLACE CQ FileDataConvert\nINSERT INTO FileStr2\nSELECT TO_INT(data[0]) as seq\nFROM FileStr1;\n\nCREATE  TYPE FileStr3_Type  ( SUMFileDataAggregationseq java.lang.Long );\n\nCREATE STREAM FileStr3 OF FileStr3_Type;\n\nCREATE OR REPLACE TARGET FileWrite USING FileWriter  (\n  filename: 'SourceResults',\n  rolloveronddl: 'true',\n  flushpolicy: 'eventcount:10000,interval:30',\n  adapterName: 'FileWriter',\n  directory: '@FEATURE-DIR@/logs',\n  rolloverpolicy: 'eventcount:10000,interval:30s'\n )\nFORMAT USING DSVFormatter  (   nullvalue: 'NULL',\n  standard: 'none',\n  handler: 'com.webaction.proc.DSVFormatter',\n  formatterName: 'DSVFormatter',\n  usequotes: 'false',\n  rowdelimiter: '\\n',\n  quotecharacter: '\\\"',\n  header: 'false',\n  columndelimiter: ','\n )\nINPUT FROM FileStr3;\n\nCREATE OR REPLACE CQ SumAggregat\nINSERT INTO FileStr3\nSELECT SUM(FileDataAggregation.seq)\nFROM FileDataAggregation;\n\nEND APPLICATION FileSource;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application AzureApp2;\nundeploy application AzureApp2;\ndrop application AzureApp2 cascade;\n\ncreate application AzureApp2\nRECOVERY 5 second interval;\ncreate source CSVSource2 using FileReader (\n\tdirectory:'@DIR@',\n\tWildCard:'@WILDCARD@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream2;\n\nCreate Type CSVType2 (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream2 of CSVType2;\n\nCREATE CQ CsvToPosData2\nINSERT INTO TypedCSVStream2\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream2;\n\ncreate Target BlobT2 using AzureBlobWriter(\n\taccountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:100000'\n)\nformat using DSVFormatter (\n)\ninput from TypedCSVStream2;\nend application AzureApp2;\ndeploy application AzureApp2 in default;\nstart application AzureApp2;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION DBRTOCW;\nUNDEPLOY APPLICATION DBRTOCW;\nDROP APPLICATION DBRTOCW CASCADE;\nCREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE OR REPLACE SOURCE DBSource USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  --QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream'\n ) OUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n ) INPUT FROM Oracle_ChangeDataStream;\n\n\nCREATE TARGET t2 USING SysOut(name:Foo2) INPUT FROM Oracle_ChangeDataStream;\n\nEND APPLICATION DBRTOCW;\n\nDEPLOY APPLICATION DBRTOCW on ANY in default;\n\nSTART APPLICATION DBRTOCW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n\nCREATE SOURCE @APPNAME@_S USING OracleReader\n(\n\tUsername: 'qatest',\n\tPassword: 'qatest',\n\tConnectionURL: 'dockerhost:1521:xe',\n\tTables: 'qatest.e1ptest%',\n\tFetchSize: '1'\n)\nOUTPUT TO @APPNAME@_SS;\n\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'qatest.e1ptest%,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12)',\nMode:'merge',\noptimizedMerge: 'true',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1000,Interval:2',\nStandardSQL:true\t\t\n) INPUT FROM @APPNAME@_ss;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@;\nCREATE  SOURCE @SourceName@ USING DatabaseReader  ( \n  Username: '@UserName@',\n  Password: '@Password@',\n  DatabaseName: 'qatest',\n  ConnectionURL: '@SourceConnectionURL@',\n  Tables: 'qatest.@SourceTable@',\n  ConnectionPoolSize: 1,\n  StartPosition: 'EOF',\n  ReplicationSlotName: 'null'\n ) \nOUTPUT TO @SRCINPUTSTREAM@;\ncreate Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( \n  ConnectionURL: '@TargetConnectionURL@',\n  Username: '@UserName@',\n  Password: '@Password@',\n  BatchPolicy: 'EventCount:1,Interval:1',\n  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n ) \nINPUT FROM @SRCINPUTSTREAM@;\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 24 with two sources, two sliding time-count windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Sc5a9W  -> CQ1 -> WS\n-- S2 -> Sc6a11W -> CQ2 -> WS\n--\n\nSTOP KStreamRecov24Tester.KStreamRecovTest24;\nUNDEPLOY APPLICATION KStreamRecov24Tester.KStreamRecovTest24;\nDROP APPLICATION KStreamRecov24Tester.KStreamRecovTest24 CASCADE;\nDROP USER KStreamRecov24Tester;\nDROP NAMESPACE KStreamRecov24Tester CASCADE;\nCREATE USER KStreamRecov24Tester IDENTIFIED BY KStreamRecov24Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov24Tester;\nCONNECT KStreamRecov24Tester KStreamRecov24Tester;\n\nCREATE APPLICATION KStreamRecovTest24 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2;\n\nEND APPLICATION KStreamRecovTest24;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION tungstenAppNoComments;\n\nCREATE source CsvDataSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'posdata.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\n\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE CQ CsvToPosData\nINSERT INTO PosDataStream partition by merchantId\nSELECT TO_STRING(data[1]) as merchantId,\n       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,\n       TO_DOUBLE(data[7]) as amount,\n       TO_STRING(data[9]) as zip\nFROM CsvStream;\n\nCREATE JUMPING WINDOW PosData5Minutes\nOVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;\n\nCREATE TYPE MerchantHourlyAve(\n  merchantId String,\n  hourValue integer,\n  hourlyAve integer\n);\nCREATE CACHE HourlyAveLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'hourlyData.txt'\n)\nPARSE USING DSVParser (\n  header: Yes,\n  trimquote:false,\n  trimwhitespace:true\n) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;\n\nCREATE TYPE MerchantTxRate(\n  merchantId String KEY,\n  zip String,\n  startingTime DateTime,\n  count integer,\n  totalAmount double,\n  hourlyAve integer,\n  upperLimit double,\n  lowerLimit double,\n  category String,\n  status String\n);\nCREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;\n\nCREATE CQ GenerateMerchantTxRateOnly\nINSERT INTO MerchantTxRateOnlyStream\nSELECT p.merchantId,\n       FIRST(p.zip),\n       FIRST(p.dateTime),\n       COUNT(p.merchantId),\n       SUM(p.amount),\n       FIRST(l.hourlyAve/12),\n       FIRST(l.hourlyAve/12 * CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       FIRST(l.hourlyAve/12 / CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       '<NOTSET>',\n       '<NOTSET>'\nFROM PosData5Minutes p, HourlyAveLookup l\nWHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue\nGROUP BY p.merchantId;\n\n\nCREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;\n\nCREATE CQ GenerateMerchantTxRateWithStatus\nINSERT INTO MerchantTxRateWithStatusStream\nSELECT merchantId,\n       zip,\n       startingTime,\n       count,\n       totalAmount,\n       hourlyAve,\n       upperLimit,\n       lowerLimit,\n       CASE\n         WHEN count >10000 THEN 'HOT'\n         WHEN count > 800 THEN 'WARM'\n         WHEN count >200 THEN 'COOL'\n         ELSE 'COLD' END,\n       CASE\n         WHEN count > upperLimit THEN 'TOOHIGH'\n         WHEN count < lowerLimit THEN 'TOOLOW'\n         ELSE 'OK' END\nFROM MerchantTxRateOnlyStream;\n\nCREATE TYPE MerchantActivityContext(\n  MerchantId String KEY,\n  StartTime  DateTime,\n  CompanyName String,\n  Category String,\n  Status String,\n  Count integer,\n  HourlyAve integer,\n  UpperLimit double,\n  LowerLimit double,\n  Zip String,\n  City String,\n  State String,\n  LatVal double,\n  LongVal double\n);\n\nCREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext\nEVENT TYPES ( MerchantTxRate )\n@PERSIST-TYPE@\n\n\nCREATE TYPE MerchantNameData(\n  merchantId String KEY,\n  companyName String\n);\n\nCREATE TYPE USAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\nCREATE CACHE NameLookup using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'MerchantNames.csv',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:'yes',\n  trimquote:false,\n  trimwhitespace:true\n) \nQUERY(keytomap:'merchantId') OF MerchantNameData;\n\nCREATE CACHE ZipLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'USAddresses.txt',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header: Yes,\n  columndelimiter: '\\t',\n  trimquote:false,\n  trimwhitespace:true\n) QUERY (keytomap:'zip') OF USAddressData;\n\n\nCREATE CQ GenerateWactionContext\nINSERT INTO MerchantActivity\nSELECT  m.merchantId,\n        m.startingTime,\n        n.companyName,\n        m.category,\n        m.status,\n        m.count,\n        m.hourlyAve,\n        m.upperLimit,\n        m.lowerLimit,\n        m.zip,\n        z.city,\n        z.state,\n        z.latVal,\n        z.longVal\nFROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z\nWHERE m.merchantId = n.merchantId AND m.zip = z.zip\nLINK SOURCE EVENT;\n\nCREATE STREAM AlertStream OF Global.AlertEvent;\n\nCREATE CQ GenerateAlerts\nINSERT INTO AlertStream\nSELECT n.CompanyName,\n       m.MerchantId,\n       CASE\n         WHEN m.Status = 'OK' THEN 'info'\n         ELSE 'warning' END,\n       CASE\n         WHEN m.Status = 'OK' THEN 'cancel'\n         ELSE 'raise' END,\n       CASE\n         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)\n         ELSE ''\n         END\nFROM MerchantTxRateWithStatusStream m, NameLookup n\nWHERE m.merchantId = n.merchantId;\n\nCREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;\n        \nEND APPLICATION tungstenAppNoComments;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "UNDEPLOY APPLICATION admin.BasicAppNoFlow;\nDROP APPLICATION admin.BasicAppNoFlow cascade;\n\nCREATE APPLICATION BasicAppNoFlow;\n\nCREATE SOURCE CsvDataSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard: 'hourlyData.txt',\n  header: Yes,\n  columndelimiter: ',',\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE MerchantHourlyAve(\n  merchantId\tString,\n  hourValue int,\n  hourlyAve int\n);\n\nCREATE STREAM MerchantHourlyStream OF MerchantHourlyAve PARTITION BY merchantId;\n\nCREATE CQ CsvToPosData\nINSERT INTO MerchantHourlyStream\nSELECT data[1], TO_INT(data[2]),\n       TO_INT(data[3])\nFROM CsvStream;\n\n\nEND APPLICATION BasicAppNoFlow;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (\n  StartPosition: '@startPosition@',\n  ConnectionURL: '@sourceURL@',\n  Username: '@userName@',\n  Tables: '@tables@',\n  CheckColumn: '@checkColum@',\n  FetchSize: 1,\n  ReturnDateTimeAs: 'JODA',\n  Password: '@password@',\n  Password_encrypted: 'false',\n  DatabaseProviderType: 'Default',\n  ThreadPoolSize: 5,\n  pollingInterval: '2sec',\n  ConnectionPoolSize: 1 )\nOUTPUT TO @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 5 with Jumping window and partitioned on two node cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS\n--\n\nSTOP APPLICATION N2S2CR5Tester.N2S2CRTest5;\nUNDEPLOY APPLICATION N2S2CR5Tester.N2S2CRTest5;\nDROP APPLICATION N2S2CR5Tester.N2S2CRTest5 CASCADE;\nCREATE APPLICATION N2S2CRTest5 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN2S2CRTest5;\n\nCREATE SOURCE CsvSourceN2S2CRTest5 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN2S2CRTest5;\n\nCREATE FLOW DataProcessingN2S2CRTest5;\n\nCREATE TYPE CsvDataTypeN2S2CRTest5 (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream OF CsvDataTypeN2S2CRTest5 PARTITION BY merchantId;\n\nCREATE CQ CsvToDataN2S2CRTest5\nINSERT INTO DataStream\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE WactionsN2S2CRTest5 CONTEXT OF CsvDataTypeN2S2CRTest5\nEVENT TYPES ( CsvDataTypeN2S2CRTest5 )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWactionN2S2CRTest5\nINSERT INTO WactionsN2S2CRTest5\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND FLOW DataProcessingN2S2CRTest5;\n\nEND APPLICATION N2S2CRTest5;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION MysqltoBQ;\nUNDEPLOY APPLICATION MysqltoBQ;\nDROP APPLICATION MysqltoBQ CASCADE;\nCREATE APPLICATION MysqltoBQ recovery 5 SECOND Interval;\nCREATE OR REPLACE SOURCE MysqltoBQ_Source USING MySQLReader \n(\n  Username:'root',\n  Password:'w@ct10n',\n  connectionURL:'jdbc:mysql://localhost:3306/waction',\n  Tables:'waction.sourceTable',\n  sendBeforeImage:'true',\n  FilterTransactionBoundaries:'true',\n  ExcludedTables:'waction.CHKPOINT',\n  useSSL:true\n) \nOUTPUT TO MysqltoBQ_Stream;\n\nCREATE OR REPLACE TARGET MysqltoBQ_Target USING BigQueryWriter  (\n  ColumnDelimiter: '|', \n  NullMarker: 'NULL', \n  projectId:'striimqa-214712',\n  Encoding: 'UTF-8', \n  BatchPolicy: 'eventCount:5,Interval:120',\n  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', \n  AllowQuotedNewLines: 'false', \n  adapterName: 'BigQueryWriter', \n  optimizedMerge: 'true', \n  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', \n  StandardSQL: 'true', \n  QuoteCharacter: '\\\"', \n  Tables: 'waction.sourceTable,.oratobqtgt',\n  Mode: 'MERGE',\n  StandardSQL: 'true',\n  QuoteCharacter: '\\\"'\n  ) INPUT FROM MysqltoBQ_Stream;\n\nCREATE OR REPLACE TARGET MysqltoBQ_SysOut USING Global.SysOut (name: 'wa') INPUT FROM MysqltoBQ_Stream;\n\nEND APPLICATION MysqltoBQ;\nDEPLOY APPLICATION MysqltoBQ;\nSTART MysqltoBQ;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 28 with two sources, two jumping time-count windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jc5t9W  -> CQ1 -> WS\n--   S2 -> Jc6t11W -> CQ2 -> WS\n--\n\nSTOP Recov28Tester.RecovTest28;\nUNDEPLOY APPLICATION Recov28Tester.RecovTest28;\nDROP APPLICATION Recov28Tester.RecovTest28 CASCADE;\nCREATE APPLICATION RecovTest28 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream6Minutes p;\n\nEND APPLICATION RecovTest28;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 14 with one source, a pattern matching CQ, and one WactionStore\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> PM-CQ -> WS\n--\n\nSTOP Recov14Tester.RecovTest14;\nUNDEPLOY APPLICATION Recov14Tester.RecovTest14;\nDROP APPLICATION Recov14Tester.RecovTest14 CASCADE;\nCREATE APPLICATION RecovTest14 RECOVERY 10 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE CsvData (\n  companyName String,\n  merchantId String,\n  dataCode int KEY,\n  expDate String\n);\n\nCREATE TYPE WactionData (\ncompanyName1 String KEY,\ncompanyName2 String,\ncompanyName3 String,\ndataCode1 int,\ndataCode2 int,\ndataCode3 int\n);\n\nCREATE STREAM DataStream OF CsvData;\n\nCREATE CQ CsvToData\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_INT(data[3]),\n    data[5]\nFROM CsvStream;\n\n\n\n\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\n\nCREATE CQ PatternMatchCQ\nINSERT INTO Wactions\nSELECT A.companyName as companyName1,\n        B.companyName as companyName2,\n        C.companyName as companyName3,\n        A.dataCode as dataCode1,\n        B.dataCode as dataCode2,\n        C.dataCode as dataCode3\nfrom DataStream\nMATCH_PATTERN A E*  B D* C\nDEFINE\nA=DataStream(dataCode = 0),\nE=DataStream(datacode != 1),\nB=DataStream(dataCode = 1),\nD= Datastream(datacode != 2),\nC=DataStream(dataCode = 2);\n\n\n\n\n\n\nCREATE WINDOW RestartFromSpecificLocation\nOVER CsvStream KEEP 8 ROWS;\n\n\n\n\n\nEND APPLICATION RecovTest14;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@;\nCREATE OR REPLACE SOURCE @APP_NAME@_src1 USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream;\n\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt1 USING Global.DeltaLakeWriter (\n  personalAccessToken: '',\n  hostname: 'adb-5292730997167687.7.azuredatabricks.net',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.student',\n  adapterName: 'DeltaLakeWriter',\n  personalAccessToken_encrypted: 'false',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:1,interval:10s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @APP_NAME@_Stream;\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt2 USING Global.DeltaLakeWriter (\n  personalAccessToken: '',\n  hostname: 'adb-5292730997167687.7.azuredatabricks.net',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.student',\n  adapterName: 'DeltaLakeWriter',\n  personalAccessToken_encrypted: 'false',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:1,interval:10s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @APP_NAME@_Stream;\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt3 USING Global.DeltaLakeWriter (\n  personalAccessToken: '',\n  hostname: 'adb-5292730997167687.7.azuredatabricks.net',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.student',\n  adapterName: 'DeltaLakeWriter',\n  personalAccessToken_encrypted: 'false',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:1,interval:10s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @APP_NAME@_Stream;\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt4 USING Global.DeltaLakeWriter (\n  personalAccessToken: '',\n  hostname: 'adb-5292730997167687.7.azuredatabricks.net',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.student',\n  adapterName: 'DeltaLakeWriter',\n  personalAccessToken_encrypted: 'false',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:1,interval:10s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @APP_NAME@_Stream;\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt5 USING Global.DeltaLakeWriter (\n  personalAccessToken: '',\n  hostname: 'adb-5292730997167687.7.azuredatabricks.net',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.student',\n  adapterName: 'DeltaLakeWriter',\n  personalAccessToken_encrypted: 'false',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:1,interval:10s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @APP_NAME@_Stream;\n\nEND APPLICATION @APP_NAME@;\nDEPLOY APPLICATION @APP_NAME@;\nSTART APPLICATION @APP_NAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application JMSWriter.JMS;\nundeploy application JMSWriter.JMS;\ndrop application JMSWriter.JMS cascade;\n\ncreate application JMS;\ncreate source JMSCSVSource using FileReader (\n        directory:'@TEST-DATA-PATH@',\n        WildCard:'AdhocQueryData2.csv',\n        positionByEOF:false,\n        charset:'UTF-8'\n)\nparse using DSVParser (\n        header:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target JmsTarget  using JMSWriter (\n\t\tProvider:'@JMSWRITERPROVIDER@',\n\t\tCtx:'@JMSWRITERCONTEXT@',\n\t\tmessagetype: @MESSAGETYPE@,\n\t\tUserName:'@JMSWRITERUSERNAME@',\n\t\tPassword:'@JMSWRITERPASSWORD@',\n\t\t@DESTINATIONTYPE@)\nformat using @JMSTARGETFORMATTERTYPE@ (\n@JMSTARGETFORMATTERMEMBERS@\n)\ninput from TypedCSVStream;\n\nend Application Jms;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 5 with Jumping window and partitioned on four node all server cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS\n--\n\nSTOP APPLICATION N4S4CR5Tester.N4S4CRTest5;\nUNDEPLOY APPLICATION N4S4CR5Tester.N4S4CRTest5;\nDROP APPLICATION N4S4CR5Tester.N4S4CRTest5 CASCADE;\nCREATE APPLICATION N4S4CRTest5 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN4S4CRTest5;\n\nCREATE SOURCE CsvSourceN4S4CRTest5 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN4S4CRTest5;\n\nCREATE FLOW DataProcessingN4S4CRTest5;\n\nCREATE TYPE CsvDataN4S4CRTest5 (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream OF CsvDataN4S4CRTest5 PARTITION BY merchantId;\n\nCREATE CQ CsvToDataN4S4CRTest5\nINSERT INTO DataStream\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE WactionsN4S4CRTest5 CONTEXT OF CsvDataN4S4CRTest5\nEVENT TYPES ( CsvDataN4S4CRTest5 )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWactionN4S4CRTest5\nINSERT INTO WactionsN4S4CRTest5\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND FLOW DataProcessingN4S4CRTest5;\n\nEND APPLICATION N4S4CRTest5;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application SQLtoRedshift;\nundeploy application SQLtoRedshift;\ndrop application SQLtoRedshift cascade;\nCREATE APPLICATION SQLtoRedshift RECOVERY 1 SECOND INTERVAL;\n\nCREATE  SOURCE SQLSource USING MSSqlReader  ( \n  Username: '@USERNAME@',\n  Password: '@PASSWORD@',\n  DatabaseName: '@DATABASE-NAME@',\n  ConnectionURL: '@URL@',\n  Tables: '@SOURCE-TABLES@'\n ) \nOUTPUT TO sqlstream ;\n\n--CREATE  TARGET t2 USING SysOut  ( name: 'sqltors') INPUT FROM sqlstream;\n\nCREATE TARGET RedshiftTarget USING RedshiftWriter\n\t(\n\t  ConnectionURL: '@TARGET-URL@',\n\t  Username: '@TARGET-UNAME@',\n\t  Password: '@TARGET-PASSWORD@',\n\t  bucketname: '@BUCKETNAME@',\n\t  --accesskeyId: '@ACCESS-KEY-ID@',\n\t  --secretaccesskey: '@SECRET-ACCESS-KEY@',\n\t  S3IAMRole:'@IAMROLE@',\n\t  Tables: '@TARGET-TABLES@',\n\t  uploadpolicy:'eventcount:1,interval:5s',\n\t  Mode:'incremental'\n\t) INPUT FROM sqlstream;\n\nEND APPLICATION SQLtoRedshift;\ndeploy application SQLtoRedshift;\nstart application SQLtoRedshift;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop Quiesce_CDC_BQ_TARGET;\nundeploy application Quiesce_CDC_BQ_TARGET;\nalter application Quiesce_CDC_BQ_TARGET;\nCreate or replace TARGET Quiesce_CDC_BigQueryTrg USING BigQueryWriter (\n  serviceAccountKey: '@SERVICEACCOUNTKEY@',\n  projectId:'@PROJECTID@',\n  BatchPolicy:'Interval:10',\n  _h_maxParallelStreamingRequests: '10',\n  Tables:'QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE1'\n)\nINPUT FROM Quiesce_CDC_OrcStrm;\nalter application Quiesce_CDC_BQ_TARGET recompile;\nDEPLOY APPLICATION Quiesce_CDC_BQ_TARGET;\nstart application Quiesce_CDC_BQ_TARGET;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @WRITERAPPNAME@;\nUNDEPLOY APPLICATION @WRITERAPPNAME@;\nDROP APPLICATION @WRITERAPPNAME@ CASCADE;\ncreate or replace PROPERTYVARIABLE SRC_PASSWORD='@PROP_VAR@';\nCREATE APPLICATION @WRITERAPPNAME@ RECOVERY 10 SECOND INTERVAL;\n-- USE EXCEPTIONSTORE;\n\nCREATE SOURCE @SOURCE@ USING OracleReader\n(\nFetchSize:1,\nUsername:'@SOURCE_USER@',\nPassword:'85d7qFnwTW8=',\n--Password:'$SRC_PASSWORD',\nConnectionURL:'@CONNECTION_URL@',\nTables:'@SOURCE_TABLE@',\npassword_encrypted: 'true'\n)\nOUTPUT TO @STREAM1@;\n\nCREATE OR REPLACE TYPE @TYPE@( \ndatae java.util.HashMap , \nTABLE_NAME java.lang.String , \nOPS_NAME java.lang.String , \nDB_TIMESTAMP java.lang.String  ,\nCOMMITSCN java.lang.String ,\nSCN java.lang.String ,\nREC_INS_TIME java.lang.String );\n\nCREATE CQ @CQ1@\nINSERT INTO @STREAM2@\nSELECT  \nCASE WHEN (META(c,\"OperationName\").toString() == \"DELETE\")\nTHEN putUserData(c, 'isDelete', 'true') \nELSE\nputUserData(c,'isDelete', 'false')\nEND\nFROM @STREAM1@ c;\n\nCREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;\n\nCREATE OR REPLACE CQ @CQ2@ \nINSERT INTO @STREAM3@\nSELECT \ndata(e),\nMETA(e,\"TableName\").toString() as TABLE_NAME,\nMETA(e, \"OperationName\").toString() as OPS_NAME,\nMETA(e, \"TimeStamp\").toString() as DB_TIMESTAMP,\nMETA(e,\"COMMITSCN\").toString() as COMMITSCN ,\nMETA(e,\"SCN\").toString() as  SCN ,\nDNOW().toString() as REC_INS_TIME\nFROM @STREAM1@ e;\n\ncreate Target @TARGET1@ using KafkaWriter VERSION @kafkaAdpVersion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC1',\nParallelThreads:'',\nPartitionKey:'@metadata(TableName)',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING dsvFormatter ()\ninput from @STREAM1@;\n\n\ncreate Target @TARGET2@ using KafkaWriter VERSION @kafkaAdpVersion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC2',\nParallelThreads:'2',\nPartitionKey:'TABLE_NAME',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING jsonFormatter ()\ninput from @STREAM3@;\n\ncreate Target @TARGET3@ using KafkaWriter VERSION @kafkaAdpVersion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC3',\nParallelThreads:'',\nPartitionKey:'@userdata(isDelete)',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING avroFormatter (\nschemaFileName:'KafkaAvroTest.avsc')\ninput from @STREAM2@;\n\nend application @WRITERAPPNAME@;\ndeploy application @WRITERAPPNAME@;\nstart @WRITERAPPNAME@;\nstop application @READERAPPNAME@;\nundeploy application @READERAPPNAME@;\ndrop application @READERAPPNAME@ cascade;\nCREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafkaAdpVersion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC1',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream1;\n\n\nCREATE TARGET kafkaDumpDSV USING FileWriter(\nname:kafkaOuputDSV,\nfilename:'@READERAPPNAME@_RT_DSV')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaReaderStream1;\n\nCREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafkaAdpVersion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC2',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO KafkaReaderStream2;\n\n\nCREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafkaAdpVersion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC3',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'KafkaAvroTest.avsc'\n)\nOUTPUT TO KafkaReaderStream3;\n\n\nend application @READERAPPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@ Using Ojet\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@URL@',\n Tables:'qatest.@SourceTable@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application admin.app1;\nundeploy application admin.app1;\ndrop application admin.app1 cascade;\n\n\nCREATE APPLICATION app1;\n\nCREATE SOURCE S1 USING Global.OracleReader (\n  Tables: 'QATEST.TEST01',\n  TransactionBufferDiskLocation: '.striim/LargeBuffer',\n  OutboundServerProcessName: 'WebActionXStream',\n  Password: 'qatest',\n  Compression: false,\n  ReaderType: 'LogMiner',\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  FetchSize: 1,\n  Password_encrypted: 'false',\n  SupportPDB: false,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CDDLAction: 'Process',\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  DictionaryMode: 'OnlineCatalog',\n  QueueSize: 2048,\n  CommittedTransactions: true,\n  XstreamTimeOut: 600,\n  CDDLCapture: false,\n  TransactionBufferType: 'Disk',\n  Username: 'qatest',\n  TransactionBufferSpilloverSize: '100MB',\n  FilterTransactionBoundaries: true,\n  SendBeforeImage: true,\n  DatabaseRole: 'Primary' )\nOUTPUT TO buffer;\n\nCREATE TARGET t USING Global.DatabaseWriter (\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'qatest',\n  ParallelThreads: '',\n  DatabaseProviderType: 'Oracle',\n  CheckPointTable: 'CHKPOINT',\n  Password_encrypted: 'false',\n  CDDLAction: 'Process',\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.TEST01,QATEST.TEST02',\n  CommitPolicy: 'EventCount:1000,Interval:60',\n  StatementCacheSize: '50',\n  Username: 'qatest',\n  BatchPolicy: 'EventCount:1000,Interval:60',\n  PreserveSourceTransactionBoundary: 'false' )\nINPUT FROM buffer;\n\nEND APPLICATION app1;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION OneAgentMultiSourceCQTester.OneAgentWithMultiSourceCQApp;\nUNDEPLOY APPLICATION OneAgentMultiSourceCQTester.OneAgentWithMultiSourceCQApp;\nDROP APPLICATION OneAgentMultiSourceCQTester.OneAgentWithMultiSourceCQApp cascade;\n\ncreate Application OneAgentWithMultiSourceCQApp;\nCREATE FLOW AgentFlow;\n\ncreate source XMLSource using FileReader (\n  Directory:'@TEST-DATA-PATH@',\n  WildCard:'books.xml',\n  positionByEOF:false\n)\nparse using XMLParser (\n  RootNode:'/catalog/book',\n  columnlist:'book/@id,book/author,book/title,book/genre,book/price,book/publish_date,book/description'\n)\nOUTPUT TO XmlStream;\n\nCREATE TYPE MyTypeXml(\nid String KEY,\nauthor String,\ntitle String,\ngenre String,\nprice String,\npublish_date String,\ndescription String\n);\n\nCREATE STREAM TypedStreamXml of MyTypeXml;\n\nCREATE CQ TypeConversionCQXml\nINSERT INTO TypedStreamXml\nSELECT\ndata[0],\ndata[1],\ndata[2],\ndata[3],\ndata[4],\ndata[5],\ndata[6]\nfrom XmlStream;\n\ncreate source DSVCSVSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'customerdetails-agent.csv',\n  charset: 'UTF-8',\n  positionByEOF:false\n)\nparse using DSVParser (\n  header:'yes',\n  minexpectedcolumns:'8'\n)\nOUTPUT TO DSVCsvStream;\n\nCREATE TYPE MyTypeCsv(\nPAN String,\nFNAME String KEY,\nLNAME String,\nADDRESS String,\nCITY String,\nSTATE String,\nZIP String,\nGENDER String\n);\n\nCREATE STREAM TypedStreamCsv of MyTypeCsv;\n\nCREATE CQ TypeConversionCQCsv\nINSERT INTO TypedStreamCsv\nSELECT\ndata[0],\ndata[1],\ndata[2],\ndata[3],\ndata[4],\ndata[5],\ndata[6],\ndata[7]\nfrom DSVCsvStream;\n\n-- Read from File\n\ncreate source CSVSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'StoreNames.csv',\n  columndelimiter:',',\n  positionByEOF:false\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO CSVStream;\n\nCREATE TYPE MyType (\nStore_Id String KEY,\nStore_Name String\n);\n\nCREATE STREAM TypedStream of MyType;\n\nCREATE CQ TypeConversionCQ\nINSERT INTO TypedStream\nSELECT data[0], data[1]\nfrom CsvStream;\n\nEND FLOW AgentFlow;\n\nCREATE FLOW ServerFlow;\n\nCREATE WACTIONSTORE StoreInfo CONTEXT OF MyType\nEVENT TYPES ( MyType )\n@PERSIST-TYPE@\n\nCREATE CQ StoreWaction\nINSERT INTO StoreInfo\nSELECT * FROM TypedStream\nLINK SOURCE EVENT;\n\nCREATE WACTIONSTORE StoreInfoXml CONTEXT OF MyTypeXml\nEVENT TYPES ( MyTypeXml )\n@PERSIST-TYPE@\n\nCREATE CQ StoreWactionXml\nINSERT INTO StoreInfoXml\nSELECT * FROM TypedStreamXml\nLINK SOURCE EVENT;\n\nCREATE WACTIONSTORE StoreInfoCsv CONTEXT OF MyTypeCsv\nEVENT TYPES ( MyTypeCsv )\n@PERSIST-TYPE@\n\nCREATE CQ StoreWactionCsv\nINSERT INTO StoreInfoCsv\nSELECT * FROM TypedStreamCsv\nLINK SOURCE EVENT;\n\n\nEND FLOW ServerFlow;\nend Application OneAgentWithMultiSourceCQApp;\n\nDEPLOY APPLICATION OneAgentWithMultiSourceCQApp with AgentFlow in AGENTS, ServerFlow on any in default;\nstart OneAgentWithMultiSourceCQApp;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@\n Using Ojet\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'qatest.@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@;\n\nCREATE OR REPLACE TYPE @appname@CQOUT1_Type (\n companyName java.lang.String,\n merchantId java.lang.String,\n dateTime org.joda.time.DateTime,\n hourValue java.lang.String,\n amount java.lang.String,\n zip java.lang.String,\n FileName java.lang.String);\n\nCREATE SOURCE @parquetsrc@ USING S3Reader (\n    bucketname:'',\n    objectnameprefix:''\n     )\nPARSE USING ParquetParser (\n)\nOUTPUT TO @appname@Stream;\n\nCREATE OR REPLACE STREAM @appname@CQOUT1 OF @appname@CQOUT1_Type;\nCREATE OR REPLACE CQ @appname@CQ_PQEvent\nINSERT INTO @appname@CQOUT1\n    Select\n    data.get(\"companyName\").toString(),\n    data.get(\"merchantId\").toString(),\n    TO_DATE(data.get(\"dateTime\").toString()),\n    data.get(\"hourValue\").toString(),\n    data.get(\"amount\").toString(),\n    data.get(\"zip\").toString(),\n    metadata.get(\"FileName\").toString()\n    FROM @appname@Stream p;\n\nCREATE OR REPLACE TARGET @avrotarget@ USING FileWriter (\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:1,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING AvroFormatter  (\nschemaFileName: 'AvroFileSchema'\n)\nINPUT FROM @appname@CQOUT1;\n\ncreate Target @jsontarget@ using FileWriter(\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:1,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nformat using JSONFormatter ()\nINPUT FROM @appname@CQOUT1;\n\ncreate Target @xmltarget@ using FileWriter(\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:1,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nformat using XMLFormatter (\n    rootelement:'',\n    elementtuple:'',\n    charset:'UTF-8'\n)\nINPUT FROM @appname@CQOUT1;\n\ncreate Target @dsvtarget@ using FileWriter(\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:1,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nformat using DSVFormatter ()\nINPUT FROM @appname@CQOUT1;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@@RECOVERY_PROP@;\n\nCREATE OR REPLACE SOURCE @APP_NAME@_src USING FileReader (\ndirectory:'@DIRECTORY@',\nWildCard:'posdata5L.csv',\npositionByEOF:false\n)\nparse using DSVParser (\nheader:true\n)\nOUTPUT TO @APP_NAME@_Stream;\n\n\nCREATE OR REPLACE CQ @APP_NAME@_CQ\nINSERT INTO @APP_NAME@_Stream2\nSELECT data[0] as BUSINESS_NAME,data[1] as MERCHANT_ID,data[2] as PRIMARY_ACCOUNT,\ndata[3] as POS_DATA_CODE,data[4] as DATE_TIME,data[5] as EXP_DATE,data[6] as CURRENCY_CODE,data[7] as AUTH_AMOUNT,\ndata[8] as TERMINAL_ID,data[9] as ZIP,data[10] as CITY\nFROM @APP_NAME@_Stream;\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt USING KuduWriter (\n  kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',\n  pkupdatehandlingmode:'@MODE@',\n  tables: '@TARGET_TABLES@',\n  batchpolicy: 'EventCount:1,Interval:0'\n )\nINPUT FROM @APP_NAME@_Stream2;\n\n\nEND APPLICATION @APP_NAME@;\nDEPLOY APPLICATION @APP_NAME@;\nSTART APPLICATION @APP_NAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n--create flow AgentFlow;\nCREATE SOURCE @APPNAME@_S USING MSSqlReader\n(\n  Compression: false,\n  cdcRoleName: 'STRIIM_READER',\n  DatabaseName: 'QATEST',\n  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',\n  ConnectionPoolSize: 1,\n  FetchTransactionMetadata: false,\n  StartPosition: 'EOF',\n  Username: 'qatest',\n  SendBeforeImage: true,\n  AutoDisableTableCDC: true,\n  ConnectionURL: 'localhost:1433',\n  Tables: 'qatest.test01',\n  adapterName: 'MSSqlReader',\n  Password: 'w3b@ct10n'\n)\nOUTPUT TO @APPNAME@_SS;\n--end flow AgentFlow;\n--create flow serverFlow;\n--create target sys_out using sysout (name:sysoutput) input from ss;\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'NULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:0',\nStandardSQL:true\t,\noptimizedMerge:true\t\n) INPUT FROM @APPNAME@_ss;\n--end flow serverFlow;\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\n--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCreate Source @SourceName@\n Using DatabaseReader\n(\n Username:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@SourceConnectURL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectURL@',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION KafkaWPTester.KWApp;\nSTOP APPLICATION KafkaWPTester.KRApp;\nUNDEPLOY APPLICATION KafkaWPTester.KWApp;\nUNDEPLOY APPLICATION KafkaWPTester.KRApp;\nDROP APPLICATION KafkaWPTester.KWApp CASCADE;\nDROP APPLICATION KafkaWPTester.KRApp CASCADE;\n\nCREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;\nCONNECT KafkaWPTester KafkaWPTester;\n\n\nCREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE SOURCE CSVSource USING FileReader (\n\tdirectory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',\n    WildCard:'access_log_half',\n\tpositionByEOF:false,\n        Charset:'UTF-8'\n)\nPARSE USING DSVParser (\n  columndelimiter:' ',\n  ignoreemptycolumn:'Yes',\n  quoteset:'[]~\"',\n  separator:'~'\n)\nOUTPUT TO FileStream;\n\nCREATE TYPE AccessLogType(\nCol1 String,\nCol2 String,\nCol3 String,\nCol4 String,\nCol5 String,\nCol6 String,\nCol7 String,\nCol8 String,\nCol9 String,\nCol10 String\n);\n\nCREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;\n\nCREATE CQ AceeslogCQ\nINSERT INTO TypedAccessLogStream\nSELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]\nFROM FileStream;\n\ncreate Target DsvWriter using KafkaWriter VERSION '0.9.0' ( \n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V9dsvExistTopic',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream;\n\n\ncreate Target JsonWriter using KafkaWriter VERSION '0.9.0' ( \n\tbrokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n\tTopic:'V9jsonExistTopic',\n\tMode:'Sync'\n    )\n \nFORMAT USING JSONFormatter ()\ninput from TypedAccessLogStream;\n\n\ncreate Target AvroWriter using KafkaWriter VERSION '0.9.0' ( \n\tbrokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n\tTopic:'V9avroExistTopic',\n    Mode:'Sync'\n    )\n \nFORMAT USING AvroFormatter (\n\tschemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')\ninput from TypedAccessLogStream;\n\n\nEND APPLICATION KWApp;\n\nDEPLOY APPLICATION KWApp on any in default;\n\n\n-- Kafka Reader Apps\n\nCREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE TYPE AccessLogType2(\nCol1 String,\nCol2 String,\nCol3 String,\nCol4 String,\nCol5 String,\nCol6 String,\nCol7 String,\nCol8 String,\nCol9 String,\nCol10 String\n);\n\nCREATE Stream KafkaDSVReaderStream of AccessLogType2;\nCREATE Stream KafkaJsonReaderStream of AccessLogType2;\nCREATE Stream KafkaAvroReaderStream of AccessLogType2;\n\n\nCREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '0.9.0' (\n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V9dsvExistTopic',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream1;\n\n\n\n\nCREATE CQ DSVtoKafkaDSVReaderStream\nINSERT INTO KafkaDSVReaderStream\nSELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]\nFROM KafkaReaderStream1;\n\nCREATE TARGET DSVDump USING FileWriter(\nname:testOuput,\nrolloverpolicy:'filesize:500M',\nfilename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V9_RT_DSV')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaDSVReaderStream;\n\nCREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '0.9.0' (\n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V9jsonExistTopic',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO KafkaReaderStream2;\n\n\n\n\nCREATE CQ DSVtoKafkaJsonReaderStream\nINSERT INTO KafkaJsonReaderStream\nSELECT \ndata.get('Col1').textValue(),\ndata.get('Col2').textValue(),\ndata.get('Col3').textValue(),\ndata.get('Col4').textValue(),\ndata.get('Col5').textValue(),\ndata.get('Col6').textValue(),\ndata.get('Col7').textValue(),\ndata.get('Col8').textValue(),\ndata.get('Col9').textValue(),\ndata.get('Col10').textValue()\nFROM KafkaReaderStream2;\n\nCREATE TARGET JSONDump USING FileWriter(\nname:testOuput,\nrolloverpolicy:'filesize:500M',\nfilename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V9_RT_JSON')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaJsonReaderStream;\n\nCREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '0.9.0' (\n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V9avroExistTopic',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'\n)\nOUTPUT TO KafkaReaderStream3;\n\n\n\nCREATE Stream KRTypedStream of AccessLogType;\n\nCREATE CQ DSVtoKafkaAvroReaderStream\nINSERT INTO KafkaAvroReaderStream\nSELECT    \ndata.get(\"Col1\").toString(), \ndata.get(\"Col2\").toString(), \ndata.get(\"Col3\").toString(), \ndata.get(\"Col4\").toString(),\ndata.get(\"Col5\").toString(), \ndata.get(\"Col6\").toString(), \ndata.get(\"Col7\").toString(), \ndata.get(\"Col8\").toString(),\ndata.get(\"Col9\").toString(), \ndata.get(\"Col10\").toString()\nFROM KafkaReaderStream3;\n\nCREATE TARGET AVRODump USING FileWriter(\nname:testOuput,\nrolloverpolicy:'filesize:500M',\nfilename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V9_RT_AVRO')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaAvroReaderStream;\n\nEND APPLICATION KRApp;\nDEPLOY APPLICATION KRApp on any in default;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE OR REPLACE SOURCE DBSource USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',\n  Tables: 'QATEST.OracToCql_alldatatypes1',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE SOURCE DBSource2 USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',\n  Tables: 'QATEST.OracToCql_alldatatypes2',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE SOURCE DBSource3 USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',\n  Tables: 'QATEST.OracToCql_alldatatypes3',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',\n  Tables: '@Tables@',\n  IgnorableExceptionCode: 'PRIMARY KEY',\n  Password: 'cassandra',\n  Password_encrypted: false\n )\nINPUT FROM Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget2 USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',\n  Tables: '@Tables@',\n  IgnorableExceptionCode: 'PRIMARY KEY',\n  Password: 'cassandra',\n  Password_encrypted: false\n )\nINPUT FROM Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget3 USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:10,Interval:60',\n  CommitPolicy: 'EventCount:10,Interval:60',\n  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',\n  Tables: '@Tables@',\n  IgnorableExceptionCode: 'PRIMARY KEY',\n  Password: 'cassandra',\n  Password_encrypted: false\n )\nINPUT FROM Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget4 USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:11,Interval:120',\n  CommitPolicy: 'EventCount:11,Interval:120',\n  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',\n  Tables: '@Tables@',\n  IgnorableExceptionCode: 'PRIMARY KEY',\n  Password: 'cassandra',\n  Password_encrypted: false\n )\nINPUT FROM Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget5 USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:3,Interval:120',\n  CommitPolicy: 'EventCount:3,Interval:120',\n  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',\n  Tables: '@Tables@',\n  IgnorableExceptionCode: 'PRIMARY KEY',\n  ExcludedTables:'QATEST.ORACTOCQL_ALLDATATYPES',\n  Password: '+hbb060plSWQwscvI105cg==',\n  Password_encrypted: true\n )\nINPUT FROM Oracle_ChangeDataStream;\n\nCREATE TARGET FWTarget USING FileWriter(\n\tname:CassandraOuput,\n\tfilename:'OracToFw.log',\n\tflushpolicy : 'interval:120,eventcount:3',\n\trolloverpolicy : 'interval:300s'\n)\nFORMAT USING DSVFormatter()\nINPUT FROM Oracle_ChangeDataStream;\ncreate Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\n\nEND APPLICATION DBRTOCW;\ndeploy application DBRTOCW in default;\nstart DBRTOCW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application PublishAPITester.NS;\nundeploy application PublishAPITester.NS;\ndrop application PublishAPITester.NS cascade;\n\ncreate application NS;\n\ncreate Stream MyStream of Global.WAEvent;\n\nCREATE TYPE bankData\n(\nbankID Integer KEY,\nbankName String\n);\n\n\nCREATE TYPE moreBankData\n(\nbankID Integer KEY,\nbankName String,\nbankRouting long,\nbankAmount double\n);\n\ncreate stream bankStream of bankData;\n\ncreate stream dataStream of moreBankData;\n\n--create Target t3 using SysOut(name:AgentOut) input from MyStream;\n\nend application NS;\n\nDEPLOY APPLICATION NS; \nstart NS;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE TARGET @TARGET_NAME@ USING SnowflakeWriter (\n  ConnectionURL: '@SNOWFLAKE-URL@',\n  username: '@SNOWFLAKE-USERNAME@',\n  password: '@SNOWFLAKE-PASSWORD@',\n  appendOnly:'false',\n  Tables: '@TARGET-TABLES@'\n )\n INPUT FROM @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ recovery 5 second interval;\n\nCREATE  SOURCE @QuerySource@1 USING DatabaseReader  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@1;\ncreate Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;\n CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@1;\n\nCREATE  SOURCE @QuerySource@2 USING DatabaseReader  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@2;\ncreate Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;\n CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@2;\n\nCREATE  SOURCE @QuerySource@3 USING DatabaseReader  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@3;\ncreate Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;\n CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@3;\n\nCREATE  SOURCE @QuerySource@4 USING DatabaseReader  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@4;\ncreate Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;\n CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@4;\n\nCREATE  SOURCE @QuerySource@5 USING DatabaseReader  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@5;\ncreate Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;\n CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@5;\n\nCREATE  SOURCE @QuerySource@6 USING DatabaseReader  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@6;\ncreate Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;\n CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@6;\n\nCREATE  SOURCE @QuerySource@7 USING DatabaseReader  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@7;\ncreate Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;\n CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@7;\n\nCREATE  SOURCE @QuerySource@8 USING DatabaseReader  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@8;\ncreate Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;\n CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@8;\n\nCREATE  SOURCE @QuerySource@9 USING DatabaseReader  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@9;\ncreate Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;\n CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@9;\n\nCREATE  SOURCE @QuerySource@10 USING DatabaseReader  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@10;\ncreate Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;\n CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',\n  Username:'root',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@10;\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application JuniperSaLog;\ncreate source JuniperSaLogSource using FileReader (\n\tdirectory:'@TEST-DATA-PATH@',\n\tWildCard:'juniper-SA*',\n\tcharset:'UTF-8',\n\tpositionByEOF:false\n) PARSE USING JuniperSA2000LogParser (\n\tcolumndelimitTill:5\n) OUTPUT TO JuniperSaLogStream;\ncreate Target JuniperSaDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/junipersa_log',charset:'UTF-8') input from JuniperSaLogStream;\nend application JuniperSaLog;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION EH;\nUNDEPLOY APPLICATION EH;\nDROP APPLICATION EH CASCADE;\nCREATE APPLICATION EH @Recovery@;\nCREATE SOURCE s USING FileReader (\n\tdirectory:'Product/IntegrationTests/TestData/',\n    WildCard:'posdata.csv',\n\tpositionByEOF:false\n\t)\nPARSE USING DSVParser (\n\theader:yes\n)OUTPUT TO ss;\n\nCREATE TYPE userType(\nCol1 String,\nCol2 String,\nCol3 String,\nCol4 String,\nCol5 String\n);\n\nCREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;\n\nCREATE CQ cq1\nINSERT INTO userDefinedTypedStream\nSELECT data[0],data[1],data[2], data[3],data[4]\nFROM ss;\n\ncreate Target t1 using AzureEventHubWriter (\n\tEventHubNamespace:'qatestnov9',\n\tEventHubName:'test_2_partitions',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tPartitionkey:'@metadata(RecordOffset)',\n\t--ParallelThreads:'2',\n\tConsumerGroup:'reader',\n\tE1P:'true',\n\tOperationTimeout:'500000',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using xmlFormatter(\nrootelement:'data')\ninput from ss;\n\ncreate Target t2 using AzureEventHubWriter (\n\tEventHubNamespace:'qatestnov9',\n\tEventHubName:'test_2_partitions',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tPartitionkey:'Col1',\n\t--ParallelThreads:'2',\n\tConsumerGroup:'reader',\n\tE1P:'true',\n\tOperationTimeout:'500000',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using xmlFormatter(\n    rootelement:'document',\n\telementtuple:'Col1:Col2:text=Col1'\n)\ninput from userDefinedTypedStream;\n\nEND APPLICATION EH;\nDEPLOY APPLICATION EH;\nstart application EH;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\ncreate application @appname@ recovery 1 second interval;\n\nCREATE SOURCE @parquetsrc@ USING FileReader (\n  directory: '',\n  positionByEOF: false,\n  WildCard: '' )\nPARSE USING ParquetParser ()\nOUTPUT TO @appname@Streams;\n\nCREATE OR REPLACE CQ @appname@CQOrder3\nINSERT INTO @appname@Stream3\nSELECT\nPUTUSERDATA(s,'schemaName',s.data.getSchema().getName())\nFROM @appname@Streams s;\n\nCREATE TARGET @adlstarget@ USING Global.ADLSGen2Writer (\n    accountname:'',\n  \tsastoken:'',\n  \tfilesystemname:'',\n  \tfilename:'',\n  \tdirectory:'',\n  \tuploadpolicy:'eventcount:10' )\nformat using ParquetFormatter (\nschemaFileName: 'ParquetSchema'\n)\nINPUT FROM @appname@Stream3;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSVSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'posdata.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\n  filename:'TargetPosDataXmlFS',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'FileSizeRollingPolicy,filesize:101M,sequence:00'\n)\nformat using XMLFormatter (\n  rootelement:'document',\n  elementtuple:'MerchantName:merchantid:text=merchantname'\n)\ninput from TypedCSVStream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlFS_actual.log') input from TypedCSVStream;\n\nend application DSV;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nstop application @appname@Out;\nundeploy application @appname@Out;\ndrop application @appname@Out cascade;\n\ndrop stream @appname@KafkaStream;\nCREATE OR REPLACE PROPERTYSET @appname@KafkaPropset (zk.address:@keeper@, bootstrap.brokers:@broker@, partitions:'50');\nCREATE STREAM @appname@KafkaStream OF Global.parquetevent PERSIST USING @appname@KafkaPropset;\n \nCREATE APPLICATION @appname@;\n\nCREATE OR REPLACE SOURCE @parquetsrc@ USING Global.FileReader ( \n  directory: '', \n  wildcard: '',\n  positionbyeof: false ) \nPARSE USING Global.ParquetParser () \nOUTPUT TO @appname@Stream;\n\nCREATE CQ @appname@CQ\nINSERT INTO @appname@KafkaStream\n    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;\n    \nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n\nCREATE APPLICATION @appname@Out;\n\nCREATE OR REPLACE TARGET @filetarget@ USING Global.FileWriter ( \n  directory: '', \n  filename: '' \n)\nFORMAT USING Global.ParquetFormatter  ( \n  schemaFileName: 'parquetSchema' \n) \nINPUT FROM @appname@KafkaStream;\n\nEND APPLICATION @appname@Out;\ndeploy application @appname@Out on all in default;\nstart application @appname@Out;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@ recovery 1 second interval;\n\ncreate source @APPNAME@_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using CobolCopybookParser (\ncopybookFileName : '@TD@/@PROP1@',\n  dataFileFont: '@PROP2@',\n  copybookSplit: '@PROP3@',\n  dataFileOrganization: '@PROP4@',\n  copybookDialect: '@PROP5@', \n  skipIndent:'@PROP6@',\n  DatahandlingScheme:'@PROP7@'\n )\nOUTPUT TO @APPNAME@Stream;\n\ncreate Target @APPNAME@Target using FileWriter(\n    filename :'@FILE@',\n    directory : '@FOLDER@'\n)\nformat using JsonFormatter (\n)\ninput from @APPNAME@Stream;\n\n\nCREATE TYPE test_typeRecov \n(\nnode_new com.fasterxml.jackson.databind.JsonNode,\nnode_name com.fasterxml.jackson.databind.JsonNode,\nnode_addr com.fasterxml.jackson.databind.JsonNode\n);\n\nCreate stream cqAsJSONNodeStreamRecov of test_typeRecov;\n\nCREATE CQ GetPOAsJsonNodesRecov\nINSERT into cqAsJSONNodeStreamRecov\nselect \ndata.get('ACCTS-RECORD'),\ndata.get('ACCTS-RECORD').get('NAME'),\ndata.get('ACCTS-RECORD').get('ADDRESS3')\nfrom @APPNAME@Stream js;\n\ncreate type finaldtypeRecov\n(ACCOUNT_NO int,\nFIRST_NAME String,\nLAST_NAME String,\nADDRESS1 String,\nADDRESS2 String,\nCITY String,\nSTATE String,\nZIP_CODE int);\n\nCREATE STREAM getdataStreamPSRecov OF finaldtypeRecov;\n\nCREATE CQ getdataRecov\nINSERT into getdataStreamPSRecov\nselect JSONGetInteger(x.node_new,\"ACCOUNT-NO\"),\nJSONGetString(x.node_name,\"FIRST-NAME\"),\nJSONGetString(x.node_name,\"LAST-NAME\"),\nJSONGetString(x.node_new,\"ADDRESS1\"),\nJSONGetString(x.node_new,\"ADDRESS2\"),\nJSONGetString(x.node_addr,\"CITY\"),\nJSONGetString(x.node_addr,\"STATE\"),\nJSONGetInteger(x.node_addr,\"ZIP-CODE\")\nfrom cqAsJSONNodeStreamRecov x;\n\ncreate Target @APPNAME@DBTarget using DatabaseWriter(\n  Username: 'qatest',\n  Password: 'qatest',\n  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',\n  BatchPolicy: 'EventCount:1000,Interval:50',\n  CommitPolicy: 'EventCount:1000,Interval:50',\n  Tables: 'QATEST.@table@'\n)\ninput from getdataStreamPSRecov;\n\nend application @APPNAME@;\ndeploy application @APPNAME@ on all in default;\nstart application @APPNAME@;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  --Password_encrypted: false\n )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop Oracle_IRLogWriter;\nUndeploy application Oracle_IRLogWriter;\ndrop application Oracle_IRLogWriter cascade;\n\nCREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;\n\nCREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( \n \n  FetchSize: 5000,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n CheckColumn: 'striim.test01=t1',\n startPosition: '%=-1',\n  PollingInterval: '20sec'\n  )\n  OUTPUT TO data_stream;\ncreate target AzureSQLDWHTarget using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',\n\t\tuploadpolicy:'eventcount:10000,interval:300s'\n) INPUT FROM data_stream;\n  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM data_stream;\n\nEND APPLICATION Oracle_IRLogWriter;\ndeploy application Oracle_IRLogWriter;\nstart Oracle_IRLogWriter;\n\nStop Oracle_IRLogWriter2;\nUndeploy application Oracle_IRLogWriter2;\ndrop application Oracle_IRLogWriter2 cascade;\n\nCREATE APPLICATION Oracle_IRLogWriter2 recovery 5 second interval;\n\nCREATE OR REPLACE SOURCE Oracle_IRSource2 USING IncrementalBatchReader  ( \n \n  FetchSize: 5000,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n CheckColumn: 'striim.test01=t1',\n startPosition: '%=-1',\n  PollingInterval: '20sec'\n  )\n  OUTPUT TO data_stream2;\ncreate target AzureSQLDWHTarget2 using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',\n\t\tuploadpolicy:'eventcount:10000,interval:300s'\n) INPUT FROM data_stream2;\n  CREATE OR REPLACE TARGET TeraSys2 USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM data_stream2;\n\nEND APPLICATION Oracle_IRLogWriter2;\ndeploy application Oracle_IRLogWriter2;\nstart Oracle_IRLogWriter2;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCreate Source @SourceName@\n Using OracleReader\n(\n Username:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@ConnectionURL@',\n Tables:'@SourceTables@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\n\nCREATE CQ @cqName@ INSERT INTO admin.oraclereader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'), 'OpTime',META(x, 'TimeStamp'))) FROM @SRCINPUTSTREAM@ x; ;\n\nCREATE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@ConnectionURL@',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@TableMapping@'\n) INPUT FROM oraclereader_cq_out;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSVSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'posdata.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\n  filename:'JsonTarget',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'FileSizeRollingPolicy,filesize:333M,sequence:00'\n)\nformat using JSONFormatter (\n  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'\n)\n\ninput from TypedCSVStream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizeBig_actual.log') input from TypedCSVStream;\nend application DSV;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@;\nCREATE OR REPLACE SOURCE @APP_NAME@_src1 USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream;\n\nCREATE OR REPLACE SOURCE @APP_NAME@_src2 USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream;\n\nCREATE OR REPLACE SOURCE @APP_NAME@_src3 USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream;\n\nCREATE OR REPLACE SOURCE @APP_NAME@_src4 USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream;\n\nCREATE OR REPLACE SOURCE @APP_NAME@_src5 USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.BQ1',\n  Username: 'qatest',\n  Password: 'qatest'\n) OUTPUT TO @APP_NAME@_Stream;\n\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt1 USING Global.DeltaLakeWriter (\n  personalAccessToken: '',\n  hostname: 'adb-5292730997167687.7.azuredatabricks.net',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.student',\n  adapterName: 'DeltaLakeWriter',\n  personalAccessToken_encrypted: 'false',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:1,interval:10s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @APP_NAME@_Stream;\n\nEND APPLICATION @APP_NAME@;\nDEPLOY APPLICATION @APP_NAME@;\nSTART APPLICATION @APP_NAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nCREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE @APPNAME@_DBSource USING Global.OracleReader (\n  TransactionBufferDiskLocation: '.striim/LargeBuffer',\n  Compression: false,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  Password_encrypted: 'false',\n  SupportPDB: false,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  FetchSize: 1000,\n  CDDLAction: 'Process',\n  ConnectionURL: '192.168.56.3:1521:orcl',\n  DictionaryMode: 'OnlineCatalog',\n  QueueSize: 2048,\n  CommittedTransactions: true,\n  SetConservativeRange: false,\n  CDDLCapture: false,\n  Username: 'fan',\n  Tables: 'FAN.S_BLOB',\n  TransactionBufferType: 'Disk',\n  Password: '9S5GnbGmBQNDD5c/baD0Tw==',\n  TransactionBufferSpilloverSize: '100MB',\n  FilterTransactionBoundaries: true,\n  SendBeforeImage: true,\n  DatabaseRole: 'Primary' )\nOUTPUT TO @APPNAME@_stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_target USING Global.BigQueryWriter (\n  ColumnDelimiter: '|',\n  projectId: 'striim-support',\n  BatchPolicy: 'eventCount:1, Interval:1',\n  NullMarker: 'NULL',\n  streamingUpload: 'false',\n  ServiceAccountKey: '/Users/fzhang/fan/u01/app/product/striim/striim_latest/UploadedFiles/striim-support-286429beb74d.json',\n  Encoding: 'UTF-8',\n  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30',\n  AllowQuotedNewLines: 'false',\n  CDDLAction: 'Process',\n  optimizedMerge: 'false',\n  Tables: 'FAN.S_BLOB,Fan.s_blob columnmap(A=A,B=B,C=C,D=C,E=@metadata(OperationName));',\n  TransportOptions: 'connectionTimeout=300, readTimeout=120',\n  adapterName: 'BigQueryWriter',\n  Mode: 'MERGE',\n  StandardSQL: 'true',\n  includeInsertId: 'true',\n  QuoteCharacter: '\\\"' )\nINPUT FROM @APPNAME@_stream;\n\n\nEND APPLICATION @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 7 with Jumping window and partitioned on two node cluster with one agent\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> KafkaStream -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS\n--\n\nSTOP APPLICATION N2S2CR7Tester.N2S2CRTest7;\nUNDEPLOY APPLICATION N2S2CR7Tester.N2S2CRTest7;\nDROP APPLICATION N2S2CR7Tester.N2S2CRTest7 CASCADE;\nCREATE APPLICATION N2S2CRTest7 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN2S2CRTest7;\n\nCREATE SOURCE CsvSourceN2S2CRTest7 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\n\nCREATE TYPE CsvDataTypeN2S2CRTest7 (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');\nCREATE STREAM KafkaCsvStream of CsvDataTypeN2S2CRTest7 using KafkaProps;\n\nCREATE CQ TransferToKafka\nINSERT INTO KafkaCsvStream\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream;\n\nEND FLOW DataAcquisitionN2S2CRTest7;\n\n\n\n\n\nCREATE FLOW DataProcessingN2S2CRTest7;\n\nCREATE STREAM DataStream OF CsvDataTypeN2S2CRTest7 PARTITION BY merchantId;\n\nCREATE CQ CsvToDataN2S2CRTest7\nINSERT INTO DataStream\nSELECT\n    *\nFROM KafkaCsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE WactionsN2S2CRTest7 CONTEXT OF CsvDataTypeN2S2CRTest7\nEVENT TYPES ( CsvDataTypeN2S2CRTest7 )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWactionN2S2CRTest7\nINSERT INTO WactionsN2S2CRTest7\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND FLOW DataProcessingN2S2CRTest7;\n\nEND APPLICATION N2S2CRTest7;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE FLOW @STREAM@_SourceFlow;\n\nCREATE SOURCE @SOURCE_NAME@ USING MSSqlReader (\n Username:'@READER-UNAME@',\n Password:'@READER-PASSWORD@',\n DatabaseName:'@DB-NAME@',\n ConnectionURL:'@CDC-READER-URL@',\n Tables:@WATABLES@,\n ConnectionPoolSize:2,\n Compression:false,\n StartPosition:'EOF'\n) OUTPUT TO @STREAM@;\n\nEND FLOW @STREAM@_SourceFlow;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop ADW;\nundeploy application ADW;\nDROP APPLICATION ADW CASCADE;\nCREATE APPLICATION ADW recovery 5 second interval;;\n\nCreate Source Ojetsrc1 Using Ojet\n(\n Username:'@ORACLE-USERNAME',\n Password:'@ORACLE-PASSWORD',\n ConnectionURL: '@ORACLE-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'@FETCH-SIZE@'\n) \nOutput To str;\n\nCreate Source Ojetsrc2 Using OracleReader\n(\n Username:'@ORACLE-USERNAME',\n Password:'@ORACLE-PASSWORD',\n ConnectionURL: '@ORACLE-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'@FETCH-SIZE@'\n) \nOutput To str;\n\nCreate Source Ojetsrc3 Using OracleReader\n(\n Username:'@ORACLE-USERNAME',\n Password:'@ORACLE-PASSWORD',\n ConnectionURL: '@ORACLE-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'@FETCH-SIZE@'\n) \nOutput To str;\n\nCreate Source Ojetsrc4 Using OracleReader\n(\n Username:'@ORACLE-USERNAME',\n Password:'@ORACLE-PASSWORD',\n ConnectionURL: '@ORACLE-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'@FETCH-SIZE@'\n) \nOutput To str;\n\nCreate Source Ojetsrc5 Using OracleReader\n(\n Username:'@ORACLE-USERNAME',\n Password:'@ORACLE-PASSWORD',\n ConnectionURL: '@ORACLE-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'@FETCH-SIZE@'\n) \nOutput To str;\n\ncreate target AzureTarget using AzureSQLDWHWriter (\n\t\tConnectionURL: '@SQLDW-URL@',\n        username: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@',  \n        uploadpolicy:'@EVENT-COUNT@'\n) INPUT FROM str;\nEND APPLICATION ADW;\ndeploy application ADW;\nstart application ADW;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nCREATE APPLICATION @AppName@ RECOVERY 1 MINUTE INTERVAL AUTORESUME MAXRETRIES 2 RETRYINTERVAL 60;\n\nCREATE source @AppName@_PosData USING FileReader (\n  WildCard: 'posdata100.csv',\n  directory: '@TestDir@',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n) OUTPUT TO @AppName@_CsvStream;\n\nCREATE OR REPLACE CQ CsvPosAppCq \nINSERT INTO FromCsvPosAppCq \nSELECT TO_STRING(data[1]) as merchantId,\n       TO_DOUBLE(data[7]) as amount,\n       TO_STRING(data[9]) as zip\nFROM @AppName@_CsvStream;\n\n\nCREATE TARGET FileWriterTarget USING Global.FileWriter ( \n \n  flushpolicy: 'EventCount:1000,Interval:30s', \n  directory: '@logs@',\n  filename: '@Filename@', \n  rolloverpolicy: 'EventCount:1000,Interval:30s' ) \nFORMAT USING Global.DSVFormatter  ( \n  quotecharacter: '\\\"', \n  columndelimiter: ',', \n  nullvalue: 'NULL', \n  usequotes: 'false', \n  rowdelimiter: '\\n', \n  standard: 'none', \n  header: 'false' ) \nINPUT FROM FromCsvPosAppCq;\n\nend application @AppName@;\ndeploy application @AppName@;\nstart application @AppName@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 32 with two sources, two sliding attribute windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Sa5W/p -> CQ1 -> WS\n-- S2 -> Sa6W/p -> CQ2 -> WS\n--\n\nSTOP KStreamRecov32Tester.KStreamRecovTest32;\nUNDEPLOY APPLICATION KStreamRecov32Tester.KStreamRecovTest32;\nDROP APPLICATION KStreamRecov32Tester.KStreamRecovTest32 CASCADE;\n\nDROP USER KStreamRecov32Tester;\nDROP NAMESPACE KStreamRecov32Tester CASCADE;\nCREATE USER KStreamRecov32Tester IDENTIFIED BY KStreamRecov32Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov32Tester;\nCONNECT KStreamRecov32Tester KStreamRecov32Tester;\n\nCREATE APPLICATION KStreamRecovTest32 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1\nGROUP BY merchantId;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2\nGROUP BY merchantId;\n\nEND APPLICATION KStreamRecovTest32;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 26 with two sources, two jumping attribute windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Ja5W -> CQ1 -> WS\n-- S2 -> Ja6W -> CQ2 -> WS\n--\n\nSTOP KStreamRecov26Tester.KStreamRecovTest26;\nUNDEPLOY APPLICATION KStreamRecov26Tester.KStreamRecovTest26;\nDROP APPLICATION KStreamRecov26Tester.KStreamRecovTest26 CASCADE;\nDROP USER KStreamRecov26Tester;\nDROP NAMESPACE KStreamRecov26Tester CASCADE;\nCREATE USER KStreamRecov26Tester IDENTIFIED BY KStreamRecov26Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov26Tester;\nCONNECT KStreamRecov26Tester KStreamRecov26Tester;\n\nCREATE APPLICATION KStreamRecovTest26 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE TYPE CsvData (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream6Minutes p;\n\nEND APPLICATION KStreamRecovTest26;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION KafkaWPTester.KWApp;\nSTOP APPLICATION KafkaWPTester.KRApp;\nUNDEPLOY APPLICATION KafkaWPTester.KWApp;\nUNDEPLOY APPLICATION KafkaWPTester.KRApp;\nDROP APPLICATION KafkaWPTester.KWApp CASCADE;\nDROP APPLICATION KafkaWPTester.KRApp CASCADE;\n\nCREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;\nCONNECT KafkaWPTester KafkaWPTester;\n\n\nCREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE SOURCE CSVSource USING FileReader (\n\tdirectory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',\n    WildCard:'access_log_half',\n\tpositionByEOF:false,\n        Charset:'UTF-8'\n)\nPARSE USING DSVParser (\n  columndelimiter:' ',\n  ignoreemptycolumn:'Yes',\n  quoteset:'[]~\"',\n  separator:'~'\n)\nOUTPUT TO FileStream;\n\nCREATE TYPE AccessLogType(\nCol1 String,\nCol2 String,\nCol3 String,\nCol4 String,\nCol5 String,\nCol6 String,\nCol7 String,\nCol8 String,\nCol9 String,\nCol10 String\n);\n\nCREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;\n\nCREATE CQ AceeslogCQ\nINSERT INTO TypedAccessLogStream\nSELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]\nFROM FileStream;\n\ncreate Target DsvWriter using KafkaWriter VERSION '0.11.0' ( \n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V11dsvExistTopic',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream;\n\n\ncreate Target JsonWriter using KafkaWriter VERSION '0.11.0' ( \n\tbrokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n\tTopic:'V11jsonExistTopic',\n\tMode:'Sync'\n    )\n \nFORMAT USING JSONFormatter ()\ninput from TypedAccessLogStream;\n\n\ncreate Target AvroWriter using KafkaWriter VERSION '0.11.0' ( \n\tbrokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n\tTopic:'V11avroExistTopic',\n    Mode:'Sync'\n    )\n \nFORMAT USING AvroFormatter (\n\tschemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')\ninput from TypedAccessLogStream;\n\n\nEND APPLICATION KWApp;\n\nDEPLOY APPLICATION KWApp on any in default;\n\n\n-- Kafka Reader Apps\n\nCREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE TYPE AccessLogType2(\nCol1 String,\nCol2 String,\nCol3 String,\nCol4 String,\nCol5 String,\nCol6 String,\nCol7 String,\nCol8 String,\nCol9 String,\nCol10 String\n);\n\nCREATE Stream KafkaDSVReaderStream of AccessLogType2;\nCREATE Stream KafkaJsonReaderStream of AccessLogType2;\nCREATE Stream KafkaAvroReaderStream of AccessLogType2;\n\n\nCREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '0.11.0' (\n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V11dsvExistTopic',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream1;\n\n\n\n\nCREATE CQ DSVtoKafkaDSVReaderStream\nINSERT INTO KafkaDSVReaderStream\nSELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]\nFROM KafkaReaderStream1;\n\nCREATE TARGET DSVDump USING FileWriter(\nname:testOuput,\nrolloverpolicy:'filesize:500M',\nfilename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V11_RT_DSV')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaDSVReaderStream;\n\nCREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '0.11.0' (\n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V11jsonExistTopic',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO KafkaReaderStream2;\n\n\n\n\nCREATE CQ DSVtoKafkaJsonReaderStream\nINSERT INTO KafkaJsonReaderStream\nSELECT \ndata.get('Col1').textValue(),\ndata.get('Col2').textValue(),\ndata.get('Col3').textValue(),\ndata.get('Col4').textValue(),\ndata.get('Col5').textValue(),\ndata.get('Col6').textValue(),\ndata.get('Col7').textValue(),\ndata.get('Col8').textValue(),\ndata.get('Col9').textValue(),\ndata.get('Col10').textValue()\nFROM KafkaReaderStream2;\n\nCREATE TARGET JSONDump USING FileWriter(\nname:testOuput,\nrolloverpolicy:'filesize:500M',\nfilename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V11_RT_JSON')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaJsonReaderStream;\n\nCREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '0.11.0' (\n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V11avroExistTopic',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'\n)\nOUTPUT TO KafkaReaderStream3;\n\n\n\nCREATE Stream KRTypedStream of AccessLogType;\n\nCREATE CQ DSVtoKafkaAvroReaderStream\nINSERT INTO KafkaAvroReaderStream\nSELECT    \ndata.get(\"Col1\").toString(), \ndata.get(\"Col2\").toString(), \ndata.get(\"Col3\").toString(), \ndata.get(\"Col4\").toString(),\ndata.get(\"Col5\").toString(), \ndata.get(\"Col6\").toString(), \ndata.get(\"Col7\").toString(), \ndata.get(\"Col8\").toString(),\ndata.get(\"Col9\").toString(), \ndata.get(\"Col10\").toString()\nFROM KafkaReaderStream3;\n\nCREATE TARGET AVRODump USING FileWriter(\nname:testOuput,\nrolloverpolicy:'filesize:500M',\nfilename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V11_RT_AVRO')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaAvroReaderStream;\n\nEND APPLICATION KRApp;\nDEPLOY APPLICATION KRApp on any in default;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application SecurityApper.MultiLogDashboard;\nundeploy application SecurityApper.MultiLogDashboard;\ndrop application SecurityApper.MultiLogDashboard cascade;\n\nCREATE APPLICATION MultiLogDashboard;\n\n-- This sample application shows how WebAction could be used monitor and correlate logs\n-- from web and application server logs from the same web application. See the discussion\n-- in the \"Sample Applications\" section of the WebAction documentation for additional\n-- discussion.\n\n\nCREATE FLOW MonitorLogs;\n\n-- MonitorLogs sets up the two log sources used by this application. In a real-world\n--implementation, each source could be reading many logs from many servers.\n\n-- The web server logs are in Apache NCSA extended/ combined log format plus response time:\n-- \"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-agent}i\\\" %D\"\n-- See apache.org for more information.\n\nCREATE SOURCE AccessLogSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'access_log',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  columndelimiter:' ',\n  ignoreemptycolumn:'Yes',\n  quoteset:'[]~\"',\n  separator:'~'\n)\nOUTPUT TO RawAccessStream;\n\nCREATE TYPE AccessLogEntry (\n    srcIp String KEY,\n    userId String,\n    sessionId String,\n    accessTime DateTime,\n    request String,\n    code int,\n    size int,\n    referrer String,\n    userAgent String,\n    responseTime int\n);\nCREATE STREAM AccessStream OF AccessLogEntry;\n\nCREATE CQ ParseAccessLog\nINSERT INTO AccessStream\nSELECT data[0], data[2], MATCH(data[4], \".*jsessionId=(.*) \"),\n       TO_DATE(data[3], \"dd/MMM/yyyy:HH:mm:ss.SSS Z\"), data[4], TO_INT(data[5]), TO_INT(data[6]),\n       data[7], data[8], TO_INT(data[9])\nFROM RawAccessStream;\n\n-- The application server logs are in Apache's Log4J format.\n\nCREATE SOURCE Log4JSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'log4jLog.xml',\n  positionByEOF:false\n)\nPARSE USING XMLParser(\n  rootnode:'/log4j:event',\n  columnlist:'log4j:event/@timestamp,log4j:event/@level,log4j:event/log4j:message,log4j:event/log4j:throwable,log4j:event/log4j:locationInfo/@class,log4j:event/log4j:locationInfo/@method,log4j:event/log4j:locationInfo/@file,log4j:event/log4j:locationInfo/@line'\n)\nOUTPUT TO RawXMLStream;\n\nCREATE TYPE Log4JEntry (\n  logTime DateTime,\n  level String,\n  message String,\n  api String,\n  sessionId String,\n  userId String,\n  sobject String,\n  xception String,\n  className String,\n  method String,\n  fileName String,\n  lineNum String\n);\nCREATE STREAM Log4JStream OF Log4JEntry;\n\nCREATE CQ ParseLog4J\nINSERT INTO Log4JStream\nSELECT TO_DATE(TO_LONG(data[0])), data[1], data[2],\n       MATCH(data[2], '\\\\\\\\[api=([a-zA-Z0-9]*)\\\\\\\\]'),\n       MATCH(data[2], '\\\\\\\\[session=([a-zA-Z0-9\\\\-]*)\\\\\\\\]'),\n       MATCH(data[2], '\\\\\\\\[user=([a-zA-Z0-9\\\\-]*)\\\\\\\\]'),\n       MATCH(data[2], '\\\\\\\\[sobject=([a-zA-Z0-9]*)\\\\\\\\]'),\n       data[3], data[4], data[5], data[6], data[7]\nFROM RawXMLStream;\n\nEND FLOW MonitorLogs;\n\n\nCREATE FLOW ErrorsAndWarnings;\n\n-- ErrorsAndWarnings creates a sliding window (Log4JErrorWarningActivity) containing\n-- the 300 most recent errors and warnings in the application server log. The\n-- ZeroContentCheck and LargeRTCheck flows join events from this window with access log\n-- events.\n\n-- The type Log4JEntry was already defined by the MonitorLogs flow.\nCREATE STREAM Log4ErrorWarningStream OF Log4JEntry;\n\nCREATE CQ GetLog4JErrorWarning\nINSERT INTO Log4ErrorWarningStream\nSELECT l FROM Log4JStream l\nWHERE l.level = 'ERROR' OR l.level = 'WARN';\n\nCREATE WINDOW Log4JErrorWarningActivity\nOVER Log4ErrorWarningStream KEEP 300 ROWS;\n\nEND FLOW ErrorsAndWarnings;\n\n\n-- HackerCheck sends an alert when an access log srcIp value is on a blacklist.\n\nCREATE FLOW HackerCheck;\n\nCREATE TYPE IPEntry (\n    ip String\n);\n\n/* CREATE CACHE BlackListLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'multiLogBlackList.txt',\n  header: No,\n  columndelimiter: ','\n) QUERY (keytomap:'ip') OF IPEntry; */\n\nCREATE CACHE BlackListLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'multiLogBlackList.txt'\n)\nPARSE USING DSVParser ( )\nQUERY (keytomap:'ip') OF IPEntry;\n\n\nCREATE STREAM HackerStream OF AccessLogEntry;\n\nCREATE CQ FindHackers\nINSERT INTO HackerStream\nSELECT ale\nFROM AccessStream ale, BlackListLookup bll\nWHERE ale.srcIp = bll.ip;\n\nCREATE TYPE UnusualContext (\n    typeOfActivity String,\n    accessTime DateTime,\n    accessSessionId String,\n    srcIp String KEY,\n    userId String,\n    country String,\n    city String,\n    lat double,\n    lon double\n);\nCREATE TYPE MergedEntry (\n    accessTime DateTime,\n    accessSessionId String,\n    srcIp String KEY,\n    userId String,\n    request String,\n    code int,\n    size int,\n    referrer String,\n    userAgent String,\n    responseTime int,\n    logTime DateTime,\n    logSessionId String,\n    level String,\n    message String,\n    api String,\n    sobject String,\n    xception String,\n    className String,\n    method String,\n    fileName String,\n    lineNum String\n);\nCREATE WACTIONSTORE UnusualActivity\nCONTEXT OF UnusualContext\nEVENT TYPES (MergedEntry,AccessLogEntry)\nPERSIST NONE USING ( );\n\nCREATE CQ GenerateHackerContext\nINSERT INTO UnusualActivity\nSELECT 'HackAttempt', accessTime, sessionId, srcIp, userId,\n       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)\nFROM HackerStream\nLINK SOURCE EVENT;\n\nCREATE STREAM HackingAlertStream OF Global.AlertEvent;\n\nCREATE CQ SendHackingAlerts\nINSERT INTO HackingAlertStream\nSELECT 'HackingAlert', ''+accessTime, 'warning', 'raise',\n        'Possible Hacking Attempt from ' + srcIp + ' in ' + IP_COUNTRY(srcIp)\nFROM HackerStream;\n\nCREATE SUBSCRIPTION HackingAlertSub USING WebAlertAdapter( ) INPUT FROM HackingAlertStream;\n\nEND FLOW HackerCheck;\n\n\n-- LargeRTCheck sends an alert when an access log responseTime value exceeds 2000\n-- microseconds.\n\nCREATE FLOW LargeRTCheck;\n\nCREATE STREAM LargeRTStream of AccessLogEntry;\n\nCREATE CQ FindLargeRT\nINSERT INTO LargeRTStream\nSELECT ale\nFROM AccessStream ale\nWHERE ale.responseTime > 2000;\n\nCREATE WINDOW LargeRTActivity\nOVER LargeRTStream KEEP 100 ROWS;\n\nCREATE STREAM LargeRTAPIStream OF MergedEntry;\n\nCREATE CQ MergeLargeRTAPI\nINSERT INTO LargeRTAPIStream\nSELECT lrt.accessTime, lrt.sessionId, lrt.srcIp, lrt.userId, lrt.request,\n       lrt.code, lrt.size, lrt.referrer, lrt.userAgent, lrt.responseTime,\n       log4j.logTime, log4j.sessionId, log4j.level, log4j.message, log4j.api, log4j.sobject, log4j.xception,\n       log4j.className, log4j.method, log4j.fileName, log4j.lineNum\nFROM LargeRTActivity lrt, Log4JErrorWarningActivity log4j\nWHERE lrt.sessionId = log4j.sessionId\n      AND lrt.accessTime = log4j.logTime;\n\nCREATE CQ GenerateLargeRTContext\nINSERT INTO UnusualActivity\nSELECT 'LargeResponseTime', accessTime, accessSessionId, srcIp, userId,\n       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)\nFROM LargeRTAPIStream\nLINK SOURCE EVENT;\n\nCREATE STREAM LargeRTAlertStream OF Global.AlertEvent;\n\nCREATE CQ SendLargeRTAlerts\nINSERT INTO LargeRTAlertStream\nSELECT 'LargeRTAlert', ''+accessTime, 'warning', 'raise',\n        'Long response time for call from ' + userId + ' api ' + api + ' message ' + message\nFROM LargeRTAPIStream;\n\nCREATE SUBSCRIPTION LargeRTAlertSub USING WebAlertAdapter( ) INPUT FROM LargeRTAlertStream;\n\nEND FLOW LargeRTCheck;\n\n\n-- ProxyCheck sends an alert when an access log srcIP value is on a list of suspicious\n-- proxies.\n\nCREATE FLOW ProxyCheck;\n\n/* CREATE CACHE ProxyLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'multiLogProxies.txt',\n  header: No,\n  columndelimiter: ','\n) QUERY (keytomap:'ip') OF IPEntry; */\n\nCREATE CACHE ProxyLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'multiLogProxies.txt'\n)\nPARSE USING DSVParser ( )\nQUERY (keytomap:'ip') OF IPEntry;\n\n\nCREATE STREAM ProxyStream OF AccessLogEntry;\n\nCREATE CQ FindProxies\nINSERT INTO ProxyStream\nSELECT ale\nFROM AccessStream ale, ProxyLookup pl\nWHERE ale.srcIp = pl.ip;\n\nCREATE CQ GenerateProxyContext\nINSERT INTO UnusualActivity\nSELECT 'ProxyAccess', accessTime, sessionId, srcIp, userId,\n       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)\nFROM ProxyStream\nLINK SOURCE EVENT;\n\n\nCREATE STREAM ProxyAlertStream OF Global.AlertEvent;\n\nCREATE CQ SendProxyAlerts\nINSERT INTO ProxyAlertStream\nSELECT 'ProxyAlert', ''+accessTime, 'warning', 'raise',\n        'Possible use of Proxy from ' + srcIp + ' in ' + IP_COUNTRY(srcIp) + ' for user ' + userId\nFROM ProxyStream;\n\nCREATE SUBSCRIPTION ProxyAlertSub USING WebAlertAdapter( ) INPUT FROM ProxyAlertStream;\n\nEND FLOW ProxyCheck;\n\n\n-- ZeroContentCheck sends an alert when an access log entry's code value is 200 (that is,\n-- the HTTP request succeeded) but the size value is 0 (the return had no content).\n\nCREATE FLOW ZeroContentCheck;\n\nCREATE STREAM ZeroContentStream of AccessLogEntry;\n\nCREATE CQ FindZeroContent\nINSERT INTO ZeroContentStream\nSELECT ale\nFROM AccessStream ale\nWHERE ale.code = 200 AND ale.size = 0;\n\nCREATE WINDOW ZeroContentActivity\nOVER ZeroContentStream KEEP 100 ROWS;\n\nCREATE STREAM ZeroContentAPIStream OF MergedEntry;\n\nCREATE CQ MergeZeroContentAPI\nINSERT INTO ZeroContentAPIStream\nSELECT zcs.accessTime, zcs.sessionId, zcs.srcIp, zcs.userId, zcs.request,\n       zcs.code, zcs.size, zcs.referrer, zcs.userAgent, zcs.responseTime,\n       log4j.logTime, log4j.sessionId, log4j.level, log4j.message, log4j.api, log4j.sobject, log4j.xception,\n       log4j.className, log4j.method, log4j.fileName, log4j.lineNum\nFROM ZeroContentActivity zcs, Log4JErrorWarningActivity log4j\nWHERE zcs.sessionId = log4j.sessionId\n      AND zcs.accessTime = log4j.logTime;\n\nCREATE CQ GenerateZeroContentContext\nINSERT INTO UnusualActivity\nSELECT 'ZeroContent', accessTime, accessSessionId, srcIp, userId,\n       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)\nFROM ZeroContentAPIStream\nLINK SOURCE EVENT;\n\n\nCREATE TYPE ZeroContentEventListType (\n    srcIp String KEY,\n    code int,\n    size int,\n    level String,\n    message String,\n    xception String);\n\nCREATE WACTIONSTORE ZeroContentEventList\nCONTEXT OF ZeroContentEventListType\nEVENT TYPES (ZeroContentEventListType )\nPERSIST NONE USING ( );\n\n\nCREATE CQ GenerateZeroContentEventList\nINSERT INTO ZeroContentEventList\nSELECT srcIp, code, size, level, message, xception\nFROM ZeroContentAPIStream;\n\n\nCREATE STREAM ZeroContentAlertStream OF Global.AlertEvent;\n\nCREATE CQ SendZeroContentAlerts\nINSERT INTO ZeroContentAlertStream\nSELECT 'ZeroContentAlert', ''+accessTime, 'warning', 'raise',\n        'Zero content returned in call from ' + userId + ' api ' + api + ' message ' + message\nFROM ZeroContentAPIStream;\n\nCREATE SUBSCRIPTION ZeroContentAlertSub USING WebAlertAdapter( ) INPUT FROM ZeroContentAlertStream;\n\nEND FLOW ZeroContentCheck;\n\n\n-- ErrorHandling is functionally identical to ErrorFlow.SaasMonitorApp. It sends an alert\n-- when an error message appears in the application server log.\n\nCREATE FLOW ErrorHandling;\n\nCREATE STREAM ErrorStream OF Log4JEntry;\n\nCREATE CQ GetErrors\nINSERT INTO ErrorStream\nSELECT log4j\nFROM Log4ErrorWarningStream log4j WHERE log4j.level = 'ERROR';\n\nCREATE STREAM ErrorAlertStream OF Global.AlertEvent;\n\nCREATE CQ SendErrorAlerts\nINSERT INTO ErrorAlertStream\nSELECT 'ErrorAlert', ''+logTime, 'error', 'raise', 'Error in log ' + message\nFROM ErrorStream;\n\nCREATE SUBSCRIPTION ErrorAlertSub USING WebAlertAdapter( ) INPUT FROM ErrorAlertStream;\n\nEND FLOW ErrorHandling;\n\n\n-- WarningHandling is a minor variation on WarningFlow.SaasMonitorApp. It sends an alert\n-- once an hour with the count of warnings for each api call for which there has been at\n-- least one alert.\n\nCREATE FLOW WarningHandling;\n\nCREATE STREAM WarningStream OF Log4JEntry;\n\nCREATE CQ GetWarnings\nINSERT INTO WarningStream\nSELECT log4j\nFROM Log4ErrorWarningStream log4j WHERE log4j.level = 'WARN';\n\nCREATE JUMPING WINDOW WarningWindow\nOVER WarningStream KEEP WITHIN 60 MINUTE ON logTime;\n\nCREATE STREAM WarningAlertStream OF Global.AlertEvent;\n\nCREATE CQ SendWarningAlerts\nINSERT INTO WarningAlertStream\nSELECT 'WarningAlert', ''+logTime, 'warning', 'raise',\n        COUNT(logTime) + ' Warnings in log for api ' + api\nFROM WarningWindow\nGROUP BY api\nHAVING count(logTime) > 1;\n\nCREATE SUBSCRIPTION WarningAlertSub USING WebAlertAdapter( ) INPUT FROM WarningAlertStream;\n\nEND FLOW WarningHandling;\n\n\n-- InfoFlow is functionally similar to InfoFlow.SaasMonitorApp. Its output is used by\n-- ApiFlow, CompanyApiFlow, and UserApiFlow.\n\nCREATE FLOW InfoFlow;\n\nCREATE TYPE UserInfo (\n  userId String,\n  userName String,\n  company String,\n  userZip String,\n  companyZip String\n);\n\n/* CREATE CACHE MLogUserLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'multiLogUser.csv',\n  header: No,\n  columndelimiter: ','\n) QUERY (keytomap:'userId') OF UserInfo; */\n\nCREATE CACHE MLogUserLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'multiLogUser.csv'\n)\nPARSE USING DSVParser ( )\nQUERY (keytomap:'userId') OF UserInfo;\n\nCREATE STREAM InfoStream OF Log4JEntry;\n\nCREATE CQ GetInfo\nINSERT INTO InfoStream\nSELECT log4j\nFROM Log4JStream log4j WHERE log4j.level = 'INFO';\n\nCREATE TYPE ApiCall (\n  userId String,\n  api String,\n  sobject String,\n  logTime DateTime,\n  userName String,\n  company String,\n  userZip String,\n  companyZip String\n);\nCREATE STREAM ApiEnrichedStream OF ApiCall;\n\nCREATE CQ GetUserDetails\nINSERT INTO ApiEnrichedStream\nSELECT a.userId, a.api, a.sobject, a.logTime, u.userName, u.company, u.userZip, u.companyZip\nFROM InfoStream a, MLogUserLookup u\nWHERE a.userId = u.userId;\n\nEND FLOW InfoFlow;\n\n\n-- ApiFlow populates the dashboard's Detail - ApiActivity page and the pie chart on the\n-- Overview page.\n\nCREATE FLOW ApiFlow;\n\nCREATE TYPE ApiUsage (\n  api String key,\n  sobject String,\n  count int,\n  logTime DateTime\n);\n\nCREATE TYPE ApiContext (\n  api String key,\n  count int,\n  logTime DateTime\n);\n\nCREATE WACTIONSTORE ApiActivity\nCONTEXT OF ApiContext\nEVENT TYPES (ApiUsage )\nPERSIST NONE USING ( );\n\nCREATE JUMPING WINDOW ApiWindow\nOVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime\nPARTITION BY api;\n\nCREATE STREAM ApiUsageStream OF ApiUsage;\n\nCREATE CQ GetApiUsage\nINSERT INTO ApiUsageStream\nSELECT a.api, a.sobject,\n       COUNT(a.userId), FIRST(a.logTime)\nFROM ApiWindow a\nGROUP BY a.api, a.sobject HAVING FIRST(a.logTime) IS NOT NULL;\n\nCREATE JUMPING WINDOW ApiSummaryWindow\nOVER ApiUsageStream KEEP WITHIN 1 HOUR ON logTime\nPARTITION BY api;\n\nCREATE CQ GetApiSummaryUsage\nINSERT INTO ApiActivity\nSELECT a.api,\n       sum(a.count), first(a.logTime)\nFROM ApiSummaryWindow a\nGROUP BY a.api\nLINK SOURCE EVENT;\n\nEND FLOW ApiFlow;\n\n\n-- CompanyApiFlow populates the dashboard's Detail - CompanyApiActivity page and the bar\n-- chart on the Overview page. It also sends an alert when an API call is used by a\n-- company more than 1500 times during the flow's one-hour jumping window.\n\nCREATE FLOW CompanyApiFlow;\n\nCREATE TYPE CompanyApiUsage (\n  company String key,\n  companyZip String,\n  companyLat double,\n  companyLong double,\n  api String,\n  count int,\n  unusual int,\n  Category String,\n  topObject String,\n  logTime DateTime\n);\n\nCREATE TYPE CompanyApiContext (\n  company String key,\n  companyZip String,\n  companyLat double,\n  companyLong double,\n  count int,\n  unusual int,\n  Category String,\n  topObject String,\n  logTime DateTime\n);\n\nCREATE WACTIONSTORE CompanyApiActivity\nCONTEXT OF CompanyApiContext\nEVENT TYPES ( CompanyApiUsage )\nPERSIST NONE USING ( );\n\nCREATE JUMPING WINDOW CompanyApiWindow\nOVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime\nPARTITION BY company;\n\nCREATE STREAM CompanyApiUsageStream OF CompanyApiUsage;\n\nCREATE TYPE MLogUSAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\n/*CREATE CACHE MLogZipLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'USAddresses.txt',\n  header: Yes,\n  columndelimiter: ','\n) QUERY (keytomap:'zip') OF MLogUSAddressData; */\n\nCREATE CACHE MLogZipLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'USAddresses.txt'\n)\nPARSE USING DSVParser (\n  header: Yes,\n  columndelimiter: '\\t',\n  trimquote:false\n\n)\nQUERY (keytomap:'zip') OF MLogUSAddressData;\n\n\nCREATE CQ GetCompanyApiUsage\nINSERT INTO CompanyApiUsageStream\nSELECT a.company, a.companyZip, z.latVal, z.longVal,\n       a.api, COUNT(a.sobject),\n       CASE WHEN COUNT(a.sobject) > 1500 THEN 1\n            ELSE 0 END,\n       CASE WHEN COUNT(a.sobject) > 1500 THEN 'UNUSUAL'\n            ELSE 'OK' END,\n       MAXOCCURS(a.sobject),\n       FIRST(a.logTime)\nFROM CompanyApiWindow a, MLogZipLookup z\nWHERE a.companyZip = z.zip\nGROUP BY a.company, a.api HAVING FIRST(a.logTime) IS NOT NULL;\n\nCREATE JUMPING WINDOW CompanyWindow\nOVER CompanyApiUsageStream KEEP WITHIN 1 HOUR ON logTime\nPARTITION BY company;\n\nCREATE CQ GetCompanyUsage\nINSERT INTO CompanyApiActivity\nSELECT a.company, a.companyZip, a.companyLat, a.companyLong,\n       SUM(a.count), SUM(a.unusual),\n       CASE WHEN SUM(a.unusual) > 0 THEN 'UNUSUAL'\n            ELSE 'OK' END,\n       MAXOCCURS(a.topObject),\n       FIRST(a.logTime)\nFROM CompanyWindow a\nGROUP BY a.company\nLINK SOURCE EVENT;\n\nCREATE STREAM CompanyAlertStream OF Global.AlertEvent;\n\nCREATE CQ SendCompanyApiAlerts\nINSERT INTO CompanyAlertStream\nSELECT 'CompanyAPIAlert', ''+logTime, 'warning', 'raise',\n       'Company ' + company + ' has used api ' + api + ' ' + count + ' times for ' + topObject\nFROM CompanyApiUsageStream\nWHERE unusual = 1;\n\nCREATE SUBSCRIPTION CompanyAlertSub USING WebAlertAdapter( ) INPUT FROM CompanyAlertStream;\n\nEND FLOW CompanyApiFlow;\n\n\n-- UserApiFlow populates the dashboard's Detail - UserApiActivity page and the US map on\n-- the Overview page. It also sends an alert when an API call is used by a user more than\n-- 125 times during the flow's one-hour window.\n\nCREATE FLOW UserApiFlow;\n\nCREATE TYPE UserApiUsage (\n  userId String key,\n  userName String,\n  userZip String,\n  userLat double,\n  userLong double,\n  company String,\n  api String,\n  count int,\n  unusual int,\n  Category String,\n  topObject String,\n  logTime DateTime\n);\n\nCREATE TYPE UserApiContext (\n  userId String key,\n  userName String,\n  userZip String,\n  userLat double,\n  userLong double,\n  company String,\n  count int,\n  unusual int,\n  Category String,\n  topObject String,\n  logTime DateTime\n);\n\nCREATE WACTIONSTORE UserApiActivity\nCONTEXT OF UserApiContext\nEVENT TYPES (  UserApiUsage )\nPERSIST NONE USING ( );\n\nCREATE JUMPING WINDOW UserApiWindow\nOVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime\nPARTITION BY userId;\n\nCREATE STREAM UserApiUsageStream OF UserApiUsage;\n\nCREATE CQ GetUserApiUsage\nINSERT INTO UserApiUsageStream\nSELECT a.userId, a.userName, a.userZip, z.latVal, z.longVal, a.company,\n       a.api, COUNT(a.sobject),\n       CASE WHEN COUNT(a.sobject) > 125 THEN 1\n            ELSE 0 END,\n       CASE WHEN COUNT(a.sobject) > 125 THEN 'UNUSUAL'\n            ELSE 'OK' END,\n       MAXOCCURS(a.sobject),\n       FIRST(a.logTime)\nFROM UserApiWindow a, MLogZipLookup z\nWHERE a.userZip = z.zip\nGROUP BY a.userId, a.api HAVING FIRST(a.logTime) IS NOT NULL;\n\nCREATE JUMPING WINDOW UserWindow\nOVER UserApiUsageStream KEEP WITHIN 1 HOUR ON logTime\nPARTITION BY userId;\n\nCREATE CQ GetUserUsage\nINSERT INTO UserApiActivity\nSELECT a.userId, a.userName, a.userZip, a.userLat, a.userLong,\n       a.company, SUM(a.count), SUM(a.unusual),\n       CASE WHEN SUM(a.unusual) > 0 THEN 'UNUSUAL'\n            ELSE 'OK' END,\n       MAXOCCURS(a.topObject),\n       FIRST(a.logTime)\nFROM UserWindow a\nGROUP BY a.userId\nLINK SOURCE EVENT;\n\nCREATE STREAM UserAlertStream OF Global.AlertEvent;\n\nCREATE CQ SendUserApiAlerts\nINSERT INTO UserAlertStream\nSELECT 'UserAPIAlert', ''+logTime, 'warning', 'raise',\n       'User ' + userName + ' has used api ' + api + ' ' + count + ' times for ' + topObject\nFROM UserApiUsageStream\nWHERE unusual = 1;\n\nCREATE SUBSCRIPTION UserAlertSub USING WebAlertAdapter( ) INPUT FROM UserAlertStream;\n\nEND FLOW UserApiFlow;\n\n\nCREATE VISUALIZATION MultiLogApp \"@FEATURE-DIR@/tql/MultiLogApp_visualization_settings.json\";\n\nEND APPLICATION MultiLogDashboard;\n\ncreate dashboard using \"@FEATURE-DIR@/tql/MultiLogAppDashboard.json\";\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ recovery 5 second interval;\n\ncreate source @SourceName@ using MySQLReader\n  (ConnectionURL: '@SourceConnectionURL@',\n   Username:'@UserName@',\n   Password:'@Password@',\n   Tables: '@SourceTableName@'\n)\noutput to @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\n   ConnectionURL:'@TargetConnectionURL@',\n   Username:'@UserName@',\n   Password:'@Password@',\n   BatchPolicy:'EventCount:1,Interval:0',\n   Tables: '@SourceTableName@,@TargetTableName@'\n \n ) INPUT FROM @SRCINPUTSTREAM@;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create or replace PROPERTYVARIABLE SRC_PASSWORD='@PROP_VAR@';\nCREATE APPLICATION @WRITERAPPNAME@ RECOVERY 10 SECOND INTERVAL;\n-- USE EXCEPTIONSTORE;\n\nCREATE SOURCE @SOURCE@ USING Ojet\n(\nFetchSize:1,\nUsername:'@SOURCE_USER@',\nPassword:'@pass@',\n--Password:'$SRC_PASSWORD',\nConnectionURL:'@CONNECTION_URL@',\nTables:'@SOURCE_TABLE@',\n)\nOUTPUT TO @STREAM1@;\n\nCREATE OR REPLACE TYPE @TYPE@( \ndatae java.util.HashMap , \nTABLE_NAME java.lang.String , \nOPS_NAME java.lang.String , \nDB_TIMESTAMP java.lang.String  ,\nCOMMITSCN java.lang.String ,\nSCN java.lang.String ,\nREC_INS_TIME java.lang.String );\n\nCREATE CQ @CQ1@\nINSERT INTO @STREAM2@\nSELECT  \nCASE WHEN (META(c,\"OperationName\").toString() == \"DELETE\")\nTHEN putUserData(c, 'isDelete', 'true') \nELSE\nputUserData(c,'isDelete', 'false')\nEND\nFROM @STREAM1@ c;\n\nCREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;\n\nCREATE OR REPLACE CQ @CQ2@ \nINSERT INTO @STREAM3@\nSELECT \ndata(e),\nMETA(e,\"TableName\").toString() as TABLE_NAME,\nMETA(e, \"OperationName\").toString() as OPS_NAME,\nMETA(e, \"TimeStamp\").toString() as DB_TIMESTAMP,\nMETA(e,\"COMMITSCN\").toString() as COMMITSCN ,\nMETA(e,\"SCN\").toString() as  SCN ,\nDNOW().toString() as REC_INS_TIME\nFROM @STREAM1@ e;\n\ncreate Target @TARGET1@ using KafkaWriter VERSION @kafakaversion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC1',\nParallelThreads:'',\nPartitionKey:'@metadata(TableName)',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING dsvFormatter ()\ninput from @STREAM1@;\n\ncreate Target @TARGET2@ using KafkaWriter VERSION @kafakaversion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC2',\nParallelThreads:'2',\nPartitionKey:'TABLE_NAME',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING jsonFormatter ()\ninput from @STREAM3@;\n\ncreate Target @TARGET3@ using KafkaWriter VERSION @kafakaversion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC3',\nParallelThreads:'',\nPartitionKey:'@userdata(isDelete)',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING avroFormatter (\nschemaFileName:'KafkaAvroTest.avsc')\ninput from @STREAM2@;\n\nend application @WRITERAPPNAME@;\n\nCREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;\nCREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafakaversion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC1',\n        startOffset:0       \n)\nPARSE USING DSVParser ()\nOUTPUT TO KafkaReaderStream1;\n\nCREATE TARGET kafkaDumpDSV USING FileWriter(\nname:kafkaOuputDSV,\nfilename:'@READERAPPNAME@_RT_DSV')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaReaderStream1;\n\nCREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafakaversion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC2',\n        startOffset:0          \n)\nPARSE USING JSONParser ()\nOUTPUT TO KafkaReaderStream2;\n\nCREATE TARGET kafkaDumpJSON USING FileWriter(\nfilename:'@READERAPPNAME@_RT_JSON')\nFORMAT USING JSONFormatter()\nINPUT FROM KafkaReaderStream2;\n\nCREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafakaversion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC3',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'KafkaAvroTest.avsc'\n)\nOUTPUT TO KafkaReaderStream3;\n\nCREATE CQ CQAvro_Json \nINSERT INTO Avro_Json  \nSELECT AvroToJSON(u.data) FROM KafkaReaderStream3 u;;\n\nCREATE TARGET kafkaDumpAVRO USING FileWriter(\nfilename:'@READERAPPNAME@_RT_AVRO')\nFORMAT USING Global.JSONFormatter  ()\nINPUT FROM Avro_Json;\n\nend application @READERAPPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION RouterTester.RouterApp;\nUNDEPLOY APPLICATION RouterTester.RouterApp;\nDROP APPLICATION RouterTester.RouterApp CASCADE;\n\n--DROP namespace RouterTester;\n--CREATE namespace RouterTester;\n--USE RouterTester;\n\nCREATE APPLICATION RouterApp RECOVERY 20 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE CsvDataSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'posdata100.csv',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO InputStreamNonTyped;\n\nCREATE TYPE MyDataType (\n  colId String,\n  colStr String,\n  colInt int,\n  colFloat float\n);\n\nCREATE OR REPLACE STREAM InputStreamTyped of MyDataType;\n\nCREATE OR REPLACE CQ  CreateTypedStreamCQ\nINSERT INTO InputStreamTyped (colId, colStr, colInt, colFloat)\nSELECT\n  TO_STRING(src.data[1]) as colId,\n  TO_STRING(src.data[0]) as colStr,\n  TO_INT(src.data[3]) as colInt,\n  TO_FLOAT(src.data[7]) as colFloat\nFROM InputStreamNonTyped src;\n\n-- Router on the stream of Generated type. It tests the predicate with expressions on\n-- integer, float and string types.\nCREATE OR REPLACE ROUTER TestRouterTyped INPUT FROM InputStreamTyped CASE\nWHEN colInt <= 3 and colFloat     < 100  and colStr like 'COMPANY%' THEN ROUTE TO StreamTyped1,\nWHEN colInt <= 6 and colFloat * 3 < 200  and colStr like 'COMPANY%' THEN ROUTE TO StreamTyped2,\nELSE ROUTE TO StreamTypedElse;\n\n-- Router on the stream of Non-generated Type. Predicates with expressions over\n-- integer and float types.\nCREATE OR REPLACE ROUTER TestRouterNonTyped INPUT FROM InputStreamNonTyped CASE\nWHEN TO_INT(data[3]) <= 3 and TO_FLOAT(data[7])     < 100 and TO_STRING(data[0]) like 'COMPANY%' THEN ROUTE TO StreamNonTyped1,\nWHEN TO_INT(data[3]) <= 6 and TO_FLOAT(data[7]) * 3 < 200 and TO_STRING(data[0]) like 'COMPANY%' THEN ROUTE TO StreamNonTyped2,\nELSE ROUTE TO StreamNonTypedElse;\n\n-- Output Streams to populate the waction stores (for non-typed stream only).\nCREATE OR REPLACE STREAM StreamWactions1 of MyDataType;\nCREATE OR REPLACE STREAM StreamWactions2 of MyDataType;\nCREATE OR REPLACE STREAM StreamWactionsElse of MyDataType;\n\nCREATE OR REPLACE CQ ConvertCQNonTyped1\nINSERT INTO StreamWactions1 (colId, colStr, colInt, colFloat)\nSELECT\n  TO_STRING(src.data[1]) as colId,\n  TO_STRING(src.data[0]) as colStr,\n  TO_INT(src.data[3]) as colInt,\n  TO_FLOAT(src.data[7]) * 3 as colFloat\nFROM StreamNonTyped1 src;\n\nCREATE OR REPLACE CQ ConvertCQNonTyped2\nINSERT INTO StreamWactions2 (colId, colStr, colInt, colFloat)\nSELECT\n  TO_STRING(src.data[1]) as colId,\n  TO_STRING(src.data[0]) as colStr,\n  TO_INT(src.data[3]) as colInt,\n  TO_FLOAT(src.data[7]) * 3 as colFloat\nFROM StreamNonTyped2 src;\n\nCREATE OR REPLACE CQ ConvertCQNonTypedElse\nINSERT INTO StreamWactionsElse (colId, colStr, colInt, colFloat)\nSELECT\n  TO_STRING(src.data[1]) as colId,\n  TO_STRING(src.data[0]) as colStr,\n  TO_INT(src.data[3]) as colInt,\n  TO_FLOAT(src.data[7]) * 3 as colFloat\nFROM StreamNonTypedElse src;\n\nCREATE TYPE WactionsTypedType1 (\n  colId  String,\n  colStr String,\n  colInt int,\n  colFloat float\n);\n\nCREATE TYPE WactionsTypedType2 (\n  colId  String,\n  colStr String,\n  colInt int,\n  colFloat float\n);\n\nCREATE TYPE WactionsTypedTypeElse (\n  colId  String,\n  colStr String,\n  colInt int,\n  colFloat float\n);\n\nCREATE TYPE WactionsNonTypedType1 (\n  colId  String,\n  colStr String,\n  colInt int,\n  colFloat float\n);\n\nCREATE TYPE WactionsNonTypedType2 (\n  colId  String,\n  colStr String,\n  colInt int,\n  colFloat float\n);\n\nCREATE TYPE WactionsNonTypedTypeElse (\n  colId  String,\n  colStr String,\n  colInt int,\n  colFloat float\n);\n\n\nCREATE WACTIONSTORE WactionsTyped1 CONTEXT OF WactionsTypedType1\nEVENT TYPES ( WactionsTypedType1 )\n@PERSIST-TYPE@;\n\nCREATE WACTIONSTORE WactionsTyped2 CONTEXT OF WactionsTypedType2\nEVENT TYPES ( WactionsTypedType2 )\n@PERSIST-TYPE@;\n\nCREATE WACTIONSTORE WactionsTypedElse CONTEXT OF WactionsTypedTypeElse\nEVENT TYPES ( WactionsTypedTypeElse )\n@PERSIST-TYPE@;\n\nCREATE WACTIONSTORE WactionsNonTyped1 CONTEXT OF WactionsNonTypedType1\nEVENT TYPES ( WactionsNonTypedType1 )\n@PERSIST-TYPE@;\n\nCREATE WACTIONSTORE WactionsNonTyped2 CONTEXT OF WactionsNonTypedType2\nEVENT TYPES ( WactionsNonTypedType2 )\n@PERSIST-TYPE@;\n\nCREATE WACTIONSTORE WactionsNonTypedElse CONTEXT OF WactionsNonTypedTypeElse\nEVENT TYPES ( WactionsNonTypedTypeElse )\n@PERSIST-TYPE@;\n\nCREATE CQ InsertWactionsTyped1\nINSERT INTO WactionsTyped1\nSELECT *\nFROM StreamTyped1;\n\nCREATE CQ InsertWactionsTyped2\nINSERT INTO WactionsTyped2\nSELECT *\nFROM StreamTyped2;\n\nCREATE CQ InsertWactionsTypedElse\nINSERT INTO WactionsTypedElse\nSELECT *\nFROM StreamTypedElse;\n\nCREATE CQ InsertWactionsNonTyped1\nINSERT INTO WactionsNonTyped1\nSELECT *\nFROM StreamWactions1;\n\nCREATE CQ InsertWactionsNonTyped2\nINSERT INTO WactionsNonTyped2\nSELECT *\nFROM StreamWactions2;\n\nCREATE CQ InsertWactionsNonTypedElse\nINSERT INTO WactionsNonTypedElse\nSELECT *\nFROM StreamWactionsElse;\n\nEND APPLICATION RouterApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Create Source @SOURCE_NAME@ Using OracleReader\n(\n Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: true,\n  FetchSize: 1,\n -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SCHEMANAME@.@TABLENAME@_copy;@SCHEMANAME@.@TABLENAME@_copy',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  StartSCN: 'null',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  SetConservativeRange: true\n) Output To @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP OuterJoinTester.OJApp;\nUNDEPLOY APPLICATION OuterJoinTester.OJApp;\nDROP APPLICATION OuterJoinTester.OJApp cascade;\n\nCREATE APPLICATION OJApp;\n\n\n\nCREATE source wsSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'banks.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:No,\n  trimquote:false\n) OUTPUT TO QaStream;\n\n\n\nCREATE source wsSource2 USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'bankCards.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:No,\n  trimquote:false\n) OUTPUT TO stream2;\n\n\n\nCREATE TYPE bankData\n(\nbankID Integer KEY,\nbankName String\n);\n\nCREATE TYPE cardData\n(\ncardID Integer KEY,\ncardName String\n);\n\n\nCREATE STREAM wsStream OF bankData;\nCREATE STREAM wsStream2 OF cardData;\n\nCREATE JUMPING WINDOW win1 OVER wsStream KEEP 20 rows;\n\n\nCREATE JUMPING WINDOW win2 OVER stream2 KEEP 4 rows;\n\nCREATE WACTIONSTORE oneWS CONTEXT OF bankData\nEVENT TYPES(bankData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE twoWS CONTEXT OF cardData\nEVENT TYPES(cardData )\n@PERSIST-TYPE@\n\n\n--Select data from QaStream and insert into wsStream\n\nCREATE CQ csvTobankData\nINSERT INTO oneWS\nSELECT TO_INT(data[0]), data[1] FROM QaStream;\n\nCREATE CQ csvTobankData2\nINSERT INTO twoWS\nSELECT TO_INT(data[0]), data[1] FROM stream2;\n\n\nEND APPLICATION OJApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION testApp;\nUNDEPLOY APPLICATION testApp;\nDROP APPLICATION testApp CASCADE;\n-- DROP EXCEPTIONSTORE testApp_exceptionstore;\n\nCREATE APPLICATION testApp WITH ENCRYPTION RECOVERY 10 SECOND INTERVAL USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE SOURCE testApp_Source USING GGTrailReader ( \n\n  SupportColumnCharset: false, \n  TrailFilePattern: 'd1*', \n  DefinitionFile: '/Users/gopinaths/Product/IntegrationTests/TestData/OGG/alldatatype/alldtype.def', \n  Compression: false, \n  TrailDirectory: '/Users/gopinaths/Product/IntegrationTests/TestData/OGG/alldatatype', \n  Tables: 'QATEST.ALLDTYPE', \n  FilterTransactionBoundaries: true, \n  ExcludedTables:'waction.CHKPOINT',\n  TrailByteOrder: 'LittleEndian' ) \nOUTPUT TO testApp_Stream;\n\nCREATE OR REPLACE TARGET testApp_Target USING BigQueryWriter  (\n  ColumnDelimiter: '|', \n  NullMarker: 'NULL', \n  projectId:'striimqa-214712',\n  Encoding: 'UTF-8', \n  BatchPolicy: 'eventCount:5,Interval:120',\n  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', \n  AllowQuotedNewLines: 'false', \n  adapterName: 'BigQueryWriter', \n  optimizedMerge: 'true', \n  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', \n  StandardSQL: 'true', \n  QuoteCharacter: '\\\"', \n  Tables: 'QATEST.ALLDTYPE,.oratobqtgt',\n  Mode: 'MERGE',\n  StandardSQL: 'true',\n  QuoteCharacter: '\\\"'\n  ) INPUT FROM testApp_Stream;\n\nCREATE OR REPLACE TARGET testApp_SysOut USING Global.SysOut (name: 'wa') INPUT FROM testApp_Stream;\n\nEND APPLICATION testApp;\nDEPLOY APPLICATION testApp;\nSTART testApp;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION DBWriterTester.DBWriterApp;\nUNDEPLOY APPLICATION DBWriterTester.DBWriterApp;\nDROP APPLICATION DBWriterTester.DBWriterApp CASCADE;\n\nDROP USER DBWriterTester;\nDROP NAMESPACE DBWriterTester CASCADE;\nCREATE USER DBWriterTester IDENTIFIED BY DBWriterTester;\nGRANT create,drop ON deploymentgroup Global.* To user DBWriterTester;\nCONNECT DBWriterTester DBWriterTester;\n\nCREATE APPLICATION DBWriterApp;\nCREATE OR REPLACE SOURCE Source_DBReader USING OracleReader  (\n-- StartTimestamp: '@CDC-STARTUPTIME@',\nUsername: 'miner',\nPassword: 'miner',\nConnectionURL: '//10.1.186.110:1521/orcl',\nTABLES: 'qatest.alltype1;qatest.dbr_marker;',\nFetchSize: '1',\ncommittedtransactions: true,\nCatalogMode: 'Offline',\nBatchPolicy: 'eventCount:10'\n)\nOUTPUT TO DBReaderStrm;\nCREATE OR REPLACE TARGET Target_DBWriter USING DatabaseWriter  (\nTables: 'QATEST.ALLTYPE1,QATEST.ALLTYPE1;QATEST.DBR_MARKER,QATEST.DBR_MARKER;',\nUsername: 'qatest',\nPasSword: 'qatest',\nConnecTionURL: 'jdbc:oracle:thin:@//10.1.110.142:1521/orcl',\nBatchPolicy: 'eventCount:10'\n)\nINPUT FROM DBReaderStrm;\nEND APPLICATION DBWriterApp;\ndeploy application DBWriterApp;\nstart application DBWriterApp;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@;\n\ncreate source @SourceName1@ USING IncrementalBatchReader\n(\n  FetchSize: 10,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: @checkColumn@,\n  startPosition: '@startPosition@',\n  PollingInterval: '20sec'\n)\nOUTPUT TO @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:@targetsys@) input from @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter(\n  ConnectionURL:'@READER-URL@',\n  Username:'@READER-UNAME@',\n  Password:'@READER-PASSWORD@',\n  BatchPolicy:'Eventcount:1,Interval:1',\n  CommitPolicy:'Eventcount:1,Interval:1',\n  Checkpointtable:'RGRN_CHKPOINT',\n  Tables:'@WATABLES@,@WATABLES@_target'\n) INPUT FROM @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\n\nDEPLOY APPLICATION @APPNAME@;\nstart application @APPNAME@;\n\nstop application @APPNAME1@;\nundeploy application @APPNAME1@;\ndrop application @APPNAME1@ cascade;\n\nCREATE APPLICATION @APPNAME1@;\n\ncreate source @SourceName2@ USING IncrementalBatchReader\n(\nFetchSize: 10,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: @checkColumn1@,\n  startPosition: '@startPosition1@',\n  PollingInterval: '20sec'\n)\nOUTPUT TO @SRCINPUTSTREAM1@;\n\ncreate Target @targetsys1@ using SysOut(name:@targetsys1@) input from @SRCINPUTSTREAM1@;\n\nCREATE TARGET @targetName1@ USING DatabaseWriter(\n  ConnectionURL:'@READER-URL@',\n  Username:'@READER-UNAME@',\n  Password:'@READER-PASSWORD@',\n  BatchPolicy:'Eventcount:1,Interval:1',\n  CommitPolicy:'Eventcount:1,Interval:1',\n  Checkpointtable:'RGRN_CHKPOINT',\n  Tables:'@WATABLES_target'\n) INPUT FROM @SRCINPUTSTREAM1@;\n\nEND APPLICATION @APPNAME1@;\n\nDEPLOY APPLICATION @APPNAME1@;\nstart application @APPNAME1@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application HDFSDSV;\nundeploy application HDFSDSV;\ndrop application HDFSDSV cascade;\n\ncreate application HDFSDSV;\ncreate source HDFSCSVSource using HDFSReader (\n\thadoopurl:'@HDFSREADERHADOOPURL@/home/hadoop/input/',\n    WildCard:'posdata.csv',\n\tcharset:'UTF-8',\n    positionByEOF:false\n)\nparse using @HDFSCSVSOURCEFORMATTERTYPE@ (\n\t@HDFSCSVSOURCEFORMATTERMEMBERS@\n)\nOUTPUT TO HDFSCsvStream;\ncreate Target HDFSDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/posdata') input from HDFSCsvStream;\nend application HDFSDSV;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@ RECOVERY 1 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE @SourceName@ USING PostgreSQLReader  ( \n ReplicationSlotName: 'striim_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.postgrestopostgres_src'\n ) \nOUTPUT TO @SRCINPUTSTREAM@ ;\n\nCREATE OR REPLACE SOURCE @SourceName@1 USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.postgrestopostgres_src'\n ) \nOUTPUT TO @SRCINPUTSTREAM@1 ;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy:'EventCount:1000,Interval:60',\nCommitPolicy:'EventCount:1000,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@1 USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy:'EventCount:1000,Interval:60',\nCommitPolicy:'EventCount:1000,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM @SRCINPUTSTREAM@1;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop ADW;\nundeploy application ADW;\nDROP APPLICATION ADW CASCADE;\nCREATE APPLICATION ADW recovery 5 second interval;;\n\nCreate Source Ojetsrc Using Ojet\n(\n Username:'@ORACLE-USERNAME',\n Password:'@ORACLE-PASSWORD',\n ConnectionURL: '@ORACLE-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'@FETCH-SIZE@'\n) \nOutput To str;\n\n\ncreate target AzureTarget1 using AzureSQLDWHWriter (\n\t\tCoNNectionURL: '@SQLDW-URL@',\n        username: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@',  \n        uploadpolicy:'@EVENT-COUNT@'\n) INPUT FROM str;\n\n\ncreate target AzureTarget2 using AzureSQLDWHWriter (\n\t\tConnectionURL: '@SQLDW-URL@',\n        UserName: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@', \n        uploadpolicy:'@EVENT-COUNT@'\n) INPUT FROM str;\n\n\ncreate target AzureTarget3 using AzureSQLDWHWriter (\n\t\tConnectionURL: '@SQLDW-URL@',\n        UserName: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@', \n        uploadpolicy:'@EVENT-COUNT@'\n) INPUT FROM str;\n\ncreate target AzureTarget4 using AzureSQLDWHWriter (\n\t\tConnectionURL: '@SQLDW-URL@',\n        UserName: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@', \n        uploadpolicy:'@EVENT-COUNT@'\n) INPUT FROM str;\n\nEND APPLICATION ADW;\ndeploy application ADW;\nstart application ADW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n--create flow AgentFlow;\nCREATE SOURCE @APPNAME@_S USING MySqlReader\n(\n  Compression: false,\n  FetchSize: 1,\n  SendBeforeImage: true,\n  ConnectionURL: 'mysql://localhost:3306',\n  DatabaseName: 'waction',\n  Tables: 'waction.test01',\n  Password: 'w@ct10n',\n  Password_encrypted: 'false',\n  Username: 'root',\n  connectionRetryPolicy:'retryInterval=1,maxRetries=3'\n)\nOUTPUT TO @APPNAME@_SS;\n--end flow AgentFlow;\n--create flow serverFlow;\n--create target sys_out using sysout (name:sysoutput) input from ss;\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'waction.test01,@DATASET@.% KEYCOLUMNS(id,col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1000,Interval:30',\nStandardSQL:true,\noptimizedMerge:true\t\n) INPUT FROM @APPNAME@_ss;\n\n--end flow serverFlow;\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\n--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_source USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: true,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --startLSN:'758199500005967269518578814280000',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.postgrestopostgres_src',\n  ExcludedTables:'public.postgres_2000target'\n ) \nOUTPUT TO @APPNAME@_Stream ;\n\nCREATE TARGET @APPNAME@_Target USING SpannerWriter (\n\tTables: 'QATEST.%,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tBatchPolicy: 'EventCount: 1; Interval: 1s',\n\tinstanceId: 'qatest'\n) INPUT FROM @APPNAME@_stream;\n\ncreate Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_stream;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;\n\nCreate Source @SourceName@ Using OracleReader\n\n(\n  Username:'c##qatest',\n  Password:'qatest',\n  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',\n  Tables:'CDB$ROOT.\"C##QATEST\".ojet_src;ORCLPDB.QATEST.ojet_src',\n  _h_useClassic:false,\n  Fetchsize:1,\n  Compression: true,\n  SupportPDB:true,\n  ReplicationSlotName:'null'\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter\n(\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',\n  Username:'c##qatest',\n  Password:'qatest',\n  BatchPolicy:'EventCount:1,Interval:1',\n  Tables:'CDB$ROOT.\"C##QATEST\".ojet_src,CDB$ROOT.\"C##QATEST\".ojet_tgt'\n) INPUT FROM @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName1@ USING DatabaseWriter\n(\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orclpdb',\n  Username:'qatest',\n  Password:'qatest',\n  BatchPolicy:'EventCount:1,Interval:1',\n  Tables:'ORCLPDB.QATEST.ojet_src,ORCLPDB.QATEST.ojet_tgt'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application ADW2;\nundeploy application ADW2;\ndrop application ADW2 cascade;\nCREATE APPLICATION ADW2;\n\nCREATE  SOURCE SqlServerInitialLoad2 USING DatabaseReader  \n (\n Username:'src_username',\n Password:'src_password',\n ConnectionURL: 'src_url',\n Tables:'@SOURCE-TABLES@',\n FetchSize:2000\n) \nOUTPUT TO InitialLoadStream2;\n\nCREATE TARGET AzureDWInitialLoad2 USING AzureSQLDWHWriter(\nConnectionURL: '@SQLDW-URL@',\n        username: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@',\n        uploadpolicy:'@EVENT-COUNT@'\n)\nINPUT FROM InitialLoadStream2;\n\nEND APPLICATION ADW2;\ndeploy application ADW2;\nstart application ADW2;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet\n(\n  ConnectionURL: '@CONNECTION_URL@',\n  CommittedTransactions: true,\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OJet',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  Username: '@SOURCE_USER@'\n)OUTPUT TO @STREAM@ (a String, b String ,c String,g String)\n\nSELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]) where TO_String(data[0]) > '1' ;\n\nCREATE TYPE LogType(\nNum_col String key,\nChar_col String,\nVarchar2_col String,\nlong_col String\n);\n\nCREATE WINDOW CDCWindow\nOVER @STREAM@\nKEEP 1 ROWS;\n\nCREATE WACTIONSTORE CDCWS CONTEXT of LogType\nEVENT TYPES ( LogType )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ ToWactionStore\nINSERT INTO CDCWS\nSELECT * FROM CDCWindow\nLINK SOURCE EVENT;\n\nCREATE TARGET @SOURCE_NAME@_SYS USING SysOut (\n  name: '@SOURCE_NAME@_SYS' )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application consoletest.noApp;\nundeploy application consoletest.noApp;\ndrop application consoletest.noApp cascade;\n\nDROP USER consoletest;\nDROP NAMESPACE consoletest CASCADE;\nCREATE USER consoletest IDENTIFIED BY consoletest;\nREVOKE consoletest.admin from user consoletest;\nGRANT create,drop ON deploymentgroup Global.consoletest To user consoletest;\nGRANT all ON namespace Global.consoletest To user consoletest;\nGRANT all ON Application consoletest.noApp To user consoletest;\nREVOKE drop ON Application consoletest.noApp from user consoletest;\nCONNECT consoletest consoletest;\n\ncreate application noApp;\ncreate source CSVSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'posdata.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\ndirectory:'@FEATURE-DIR@/logs/',\nfilename:'PermissionTarget',\nsequence:'00',\nrolloverpolicy:'eventcount:1222001'\n)\nformat using DSVFormatter (\n\n)\ninput from TypedCSVStream;\nend application noApp;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application AzureApp;\nundeploy application AzureApp;\ndrop application AzureApp cascade;\n\ncreate application AzureApp\nRECOVERY 5 second interval;\ncreate source CSVSource using FileReader (\n\tdirectory:'@DIR@',\n\tWildCard:'@WILDCARD@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target BlobT using AzureBlobWriter(\n\taccountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:50'\n)\nformat using DSVFormatter (\n)\ninput from TypedCSVStream;\nend application AzureApp;\ndeploy application AzureApp in default;\nstart application AzureApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION snow2pg;\nUNDEPLOY APPLICATION snow2pg;\nDROP APPLICATION snow2pg CASCADE;\n\n\nCREATE OR REPLACE APPLICATION snow2pg;\n\nCREATE OR REPLACE SOURCE snow_pg USING Global.ServiceNowReader (\n  Mode: 'InitialLoad',\n  ServiceNow.ConnectionTimeOut: 60,\n  ServiceNow.MaxConnections: 20,\n  ServiceNow.FetchSize: 10000,\n  ThreadPoolCount: '10',\n  ServiceNow.ConnectionRetries: 3,\n  PollingInterval: '1',\n  ClientSecret: '6Wa-cv`I7x',\n  Password: '^Pre&$EMO%6O.e_{96h+$R?rJd,=[4Vt=K)Szh?6g<J9D3,3zs8R;hpZqh]-3?C&.u-@GvSakPXH1:2eygbBDI>ou-z#GjBw[u8x',\n  ServiceNow.Tables: 'u_empl',\n  UserName: 'snr',\n  ClientID: 'ce4fd5af894a11103d2c5c3a8fe075e1',\n  adapterName: 'ServiceNowReader',\n  ServiceNow.BatchAPI: false,\n  ServiceNow.ConnectionUrl: 'https://dev84954.service-now.com/' )\nOUTPUT TO sn;\n\nCREATE TARGET pg USING Global.DatabaseWriter (\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'w@ct10n',\n  Tables: 'u_empl,u_empl ColumnMap(name=u_name,age=u_age,address=u_address,sys_id=sys_id)',\n  ParallelThreads: '',\n  CheckPointTable: 'CHKPOINT',\n  CDDLAction: 'Process',\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  CommitPolicy: 'EventCount:1000,Interval:60',\n  StatementCacheSize: '50',\n  Username: 'waction',\n  DatabaseProviderType: 'Postgres',\n  BatchPolicy: 'EventCount:1000,Interval:60',\n  PreserveSourceTransactionBoundary: 'false' )\nINPUT FROM sn;\n\nEND APPLICATION snow2pg;\ndeploy application snow2pg;\nstart snow2pg;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSVSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'smallposdata.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\n  directory:'@FEATURE-DIR@/logs/',\n  filename:'RoundUPPosData',\n  rolloverpolicy:'FileSizeRollingPolicy,filesize:2.5M,sequence:00'\n)\nformat using DSVFormatter (\n\n)\ninput from TypedCSVStream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizeRoundUp_actual.log') input from TypedCSVStream;\n\nend application DSV;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop application dropDeployApp cascade;\ncreate application dropDeployApp;\ndeploy application admin.dropDeployApp on any in default;\ndrop application dropDeployApp;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 27 with two sources, two jumping time windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jt1W -> CQ1 -> WS\n--   S2 -> Jt2W -> CQ2 -> WS\n--\n\nSTOP KStreamRecov27Tester.KStreamRecovTest27;\nUNDEPLOY APPLICATION KStreamRecov27Tester.KStreamRecovTest27;\nDROP APPLICATION KStreamRecov27Tester.KStreamRecovTest27 CASCADE;\nDROP USER KStreamRecov27Tester;\nDROP NAMESPACE KStreamRecov27Tester CASCADE;\nCREATE USER KStreamRecov27Tester IDENTIFIED BY KStreamRecov27Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov27Tester;\nCONNECT KStreamRecov27Tester KStreamRecov27Tester;\n\nCREATE APPLICATION KStreamRecovTest27 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream OF Global.waevent using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE TYPE CsvData (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream;\n\nCREATE JUMPING WINDOW DataStream1Second\nOVER DataStream1 KEEP WITHIN 1 SECOND;\n\nCREATE JUMPING WINDOW DataStream2Second\nOVER DataStream2 KEEP WITHIN 2 SECOND;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data1ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream1Second p;\n\nCREATE CQ Data2ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream2Second p;\n\nEND APPLICATION KStreamRecovTest27;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:100,Interval:3',\n  CommitPolicy: 'EventCount:100,Interval:3',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  --Password_encrypted: false\n )\nINPUT FROM @STREAM@1;\n\nCREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(\n  name: 'CDDL',\n  filename:'WAEventDump.log'\n)INPUT FROM @STREAM@1;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 33 with two sources, two sliding time windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> St1W/p -> CQ1 -> WS\n-- S2 -> St2W/p -> CQ2 -> WS\n--\n\nSTOP KStreamRecov33Tester.KStreamRecovTest33;\nUNDEPLOY APPLICATION KStreamRecov33Tester.KStreamRecovTest33;\nDROP APPLICATION KStreamRecov33Tester.KStreamRecovTest33 CASCADE;\n\nDROP USER KStreamRecov33Tester;\nDROP NAMESPACE KStreamRecov33Tester CASCADE;\nCREATE USER KStreamRecov33Tester IDENTIFIED BY KStreamRecov33Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov33Tester;\nCONNECT KStreamRecov33Tester KStreamRecov33Tester;\n\nCREATE APPLICATION KStreamRecovTest33 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP WITHIN 1 SECOND\nPARTITION BY merchantId;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP WITHIN 2 SECOND\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1\nGROUP BY merchantId;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2\nGROUP BY merchantId;\n\nEND APPLICATION KStreamRecovTest33;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (\n  StartPosition: '@startPosition@',\n  ConnectionURL: '@sourceURL@',\n  Username: '@userName@',\n  Tables: '@tables@',\n  CheckColumn: '@checkColum@',\n  FetchSize: 1,\n  ReturnDateTimeAs: 'JODA',\n  Password: '@password@',\n  Password_encrypted: 'false',\n  DatabaseProviderType: 'Default',\n  ThreadPoolSize: 5,\n  pollingInterval: '2sec',\n  ConnectionPoolSize: 1 )\nOUTPUT TO @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 6 with Jumping window and partitioned on two node cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> KafkaStream -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS\n--\n\nSTOP APPLICATION N2S2CR6Tester.N2S2CRTest6;\nUNDEPLOY APPLICATION N2S2CR6Tester.N2S2CRTest6;\nDROP APPLICATION N2S2CR6Tester.N2S2CRTest6 CASCADE;\nCREATE APPLICATION N2S2CRTest6 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN2S2CRTest6;\n\nCREATE SOURCE CsvSourceN2S2CRTest6 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\n\n\n\nCREATE TYPE CsvDataTypeN2S2CRTest6 (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');\nCREATE STREAM KafkaCsvStream of CsvDataTypeN2S2CRTest6 using KafkaProps;\n\nCREATE CQ TransferToKafka\nINSERT INTO KafkaCsvStream\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream;\n\n\n\n\n\n\n\n\nEND FLOW DataAcquisitionN2S2CRTest6;\n\nCREATE FLOW DataProcessingN2S2CRTest6;\n\nCREATE STREAM DataStream OF CsvDataTypeN2S2CRTest6 PARTITION BY merchantId;\n\nCREATE CQ CsvToDataN2S2CRTest6\nINSERT INTO DataStream\nSELECT\n    *\nFROM KafkaCsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE WactionsN2S2CRTest6 CONTEXT OF CsvDataTypeN2S2CRTest6\nEVENT TYPES ( CsvDataTypeN2S2CRTest6 )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWactionN2S2CRTest6\nINSERT INTO WactionsN2S2CRTest6\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND FLOW DataProcessingN2S2CRTest6;\n\nEND APPLICATION N2S2CRTest6;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSVSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'smallposdata.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\n  filename:'JsonTargetEC',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'eventcount:2000,sequence:00'\n)\nformat using JSONFormatter (\n  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'\n)\n\ninput from TypedCSVStream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetJsonECBig_actual.log') input from TypedCSVStream;\n\nend application DSV;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCreate Source @APPNAME@_src Using OracleReader\n(\n Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream'\n) Output To @APPNAME@_stream;\n\ncreate type @APPNAME@_type(\nid int,\nname String,\ncost float,\nLvalue long,\nPreviousPaid double,\ndateOrder DateTime,\nDwoTime Datetime,\nDeliverDate Datetime,\nTableName string,\nOperationName String\n);\n\ncreate or replace stream @APPNAME@_typed_Stream of @APPNAME@_type;\n\nCreate CQ @APPNAME@_TypedCQ\ninsert into @APPNAME@_typed_Stream\nselect\nto_int(data[0]),data[1],to_float(data[2]),to_long(data[3]),to_double(data[4]),\nto_Date(data[5]),to_Date(data[6]),to_Date(data[7]),\nmeta(@APPNAME@_stream,'TableName'),\nMeta(@APPNAME@_stream,'OperationName') from @APPNAME@_stream;\n\n\ncreate Target @APPNAME@_tgt using GCSWriter(\n    bucketname:'@bucketname@',\n    objectname:'@objectname@',\n    projectId:'@project-id@',\n    uploadPolicy:'eventCount:5',\n    ServiceAccountKey:'@file-path@'\n)\nformat using ParquetFormatter (\n\tschemaFileName:'@FILENAME@',\n\tFormatAs:'@FORMATAS@'\n)\ninput from @APPNAME@_typed_Stream;\n\n\n\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application FileWriterDSVTester.DSV;\nundeploy application FileWriterDSVTester.DSV;\ndrop application FileWriterDSVTester.DSV cascade;\n\ncreate application DSV;\n\ncreate source CSVSource using FileReader (\ndirectory:'@TEST-DATA-PATH@',\nWildCard:'smallposdata.csv',\npositionByEOF:false,\ncharset:'UTF-8'\n)\nparse using DSVParser (\nheader:'yes'\n)\nOUTPUT TO CsvStream;\n\n\nCREATE SOURCE CSVPosDataSource USING FileReader (\n  directory: '@TEST-DATA-PATH@',\n  WildCard: 'posdata.csv',\n  positionByEOF: false,\n  charset: 'UTF-8'\n )\n PARSE USING DSVParser (\n  header: 'yes'\n )\nOUTPUT TO PosDataCsvStream;\n\ncreate source CSVMerchantNamesSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'MerchantNames.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO CsvMerchantNamesStream;\n\ncreate source CSVSmallRetailSource using FileReader (\ndirectory:'@TEST-DATA-PATH@',\nWildCard:'smallretaildata2M.csv',\npositionByEOF:false,\ncharset:'UTF-8'\n)\nparse using DSVParser (\nheader:'yes'\n)\nOUTPUT TO CsvSmallRetailStream;\n\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Type CSVMerchantNamesType (\n  merchantId String,\n  merchantName String\n);\n\nCreate Type CSVSmallRetailType (\nstoreId String,\nnameId String,\ncity String,\nstate String\n);\n\nCreate Stream TypedCSVStream of CSVType;\nCreate Stream TypedPosDataCSVStream of CSVType;\nCreate Stream TypedCSVMerchantNamesStream of CSVMerchantNamesType;\nCreate Stream TypedCSVSmallRetailStream of CSVSmallRetailType;\n\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\n\nCREATE CQ CsvPosData\nINSERT INTO TypedPosDataCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM PosDataCsvStream;\n\n\nCREATE CQ CsvToMerchantNames\nINSERT INTO TypedCSVMerchantNamesStream\nSELECT data[0],\n       data[1]\nFROM CsvMerchantNamesStream;\n\n\nCREATE CQ CsvToSmallRetailData\nINSERT INTO TypedCSVSmallRetailStream\nSELECT data[0],\n       data[1],\n       data[2],\n       data[3]\nFROM CsvSmallRetailStream;\n\n/**\n* 3.4.5.c FileWriter DSV ParserNegativeEC 100\n**/\ncreate Target DSVNegativeEventCount using FileWriter(\nfilename:'EventNCDefault',\ndirectory:'@FEATURE-DIR@/logs/',\nsequence:'00',\n--filelimit: '5',\nrolloverpolicy:'eventcount:-100',\nbuffersize:1\n)\nformat using DSVFormatter (\n\n)\ninput from TypedCSVSmallRetailStream;\n\n\n/**\n* 3.3.1.c FileWriter DSV TimeInterval\n**/\ncreate Target DSVTimeIntervalRollingPolicy using FileWriter(\n  filename:'MerchantTIRP',\n  sequence:'00',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00',\n  buffersize:1\n)\nformat using DSVFormatter (\n\n)\ninput from TypedCSVMerchantNamesStream;\n\n/**\n* 3.4.6.d FileWriter DSV DefaultRP\n**/\ncreate Target DSVDefaultRollingPolicy using FileWriter(\ndirectory:'@FEATURE-DIR@/logs/',\nfilename:'PosData',\nrolloverpolicy:'EventCount:5000000',\nbuffersize:1\n)\nformat using DSVFormatter (\n\n)\ninput from TypedCSVStream;\n\n/**\n* 3.4.5.d FileWriter DSV EventCount 100\n**/\ncreate Target DSVEventCountDecimal using FileWriter(\nfilename:'Events',\ndirectory:'@FEATURE-DIR@/logs/',\nrolloverpolicy:'eventcount:200,sequence:00',\nbuffersize:1\n)\nformat using DSVFormatter (\n\n)\ninput from TypedCSVStream;\n\n/**\n* 3.3.1.a FileWriter DSVFileSize 1MB\n**/\ncreate Target DSVFileSize using FileWriter(\ndirectory:'@FEATURE-DIR@/logs',\nfilename:'PosDataFS',\nrolloverpolicy:'FileSizeRollingPolicy,filesize:1M,sequence:00',\nbuffersize:1\n)\nformat using DSVFormatter (\n\n)\ninput from TypedCSVStream;\n\n/**\n* 3.4.5.a FileWriter DSVFileSizeRoundUp 3MB\n**/\ncreate Target DSVFileSizeDecimal using FileWriter(\n  directory:'@FEATURE-DIR@/logs/',\n  filename:'RoundUPPosData',\n  rolloverpolicy:'FileSizeRollingPolicy,filesize:2.5M,sequence:00',\n  buffersize:1\n)\nformat using DSVFormatter (\n\n)\ninput from TypedCSVStream;\n\n\n/**\n* 3.1.1.b FileWriter DSV EventCount\n**/\n\nCREATE OR REPLACE TARGET DSVEventCount USING FileWriter (\n  filename: 'TargetDefault',\n  directory:'@FEATURE-DIR@/logs/',\n  sequence:'00',\n  flushinterval: '0',\n  rolloverpolicy:'EventCount:5000000',\n  buffersize:1\n )\n format using DSVFormatter (\n\n)\nINPUT FROM TypedPosDataCSVStream;\n\n\ncreate Target TargetSmallPosData using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizePosDataDefault_actual.log') input from TypedCSVStream;\n\nend application DSV;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING CosmosDBWriter  (\n  BatchPolicy: 'EventCount:1000,Interval:1',\n  ServiceEndpoint: 'https://souvik.documents.azure.com:443/',\n  ConnectionPoolSize: '10',\n  AccessKey: 'z1CfmzAy5QwB5MN7bWIinM9gYmJn7zWo1wOaadvaCErqaTCqlb7srpAx7muPYWhJwnYq3plOQoBNENn1xPmkfQ==',\n  adapterName: 'CosmosDBWriter',\n  Collections: 'QATEST.TEST,db2.TEST keycolumns(id)',\n  ConnectionRetryPolicy: 'RetryInterval:1,MaxRetries:0',\n  KeySeparator: ':',\n  AccessKey_encrypted: false\n )\nINPUT FROM @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Kafka Stream with KryoParser and Kafka Reader Recovery Test 1\n-- Bert Hashemi and Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> KS -> WS\n-- S -> K -> CQ -> WS\n\nSTOP KStreamKryoParser1Tester.KStreamKryoParserTest1;\nUNDEPLOY APPLICATION KStreamKryoParser1Tester.KStreamKryoParserTest1;\nDROP APPLICATION KStreamKryoParser1Tester.KStreamKryoParserTest1 CASCADE;\nDROP USER KStreamKryoParser1Tester;\nDROP NAMESPACE KStreamKryoParser1Tester CASCADE;\nCREATE USER KStreamKryoParser1Tester IDENTIFIED BY KStreamKryoParser1Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamKryoParser1Tester;\nCONNECT KStreamKryoParser1Tester KStreamKryoParser1Tester;\n\nCREATE APPLICATION KStreamKryoParserTest1 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'1');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE or REPLACE TYPE KafkaCsvStreamType(\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE Wactions CONTEXT OF KafkaCsvStreamType \nEVENT TYPES ( KafkaCsvStreamType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions\nINSERT INTO Wactions\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream;\n\nCREATE or REPLACE TYPE KafkaStreamType(\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\n--CREATE STREAM KafkaTypedStream OF KafkaStreamType;\n\nCREATE STREAM KafkaStream OF Global.waevent;\n\nCREATE SOURCE KafkaSource USING KafkaReader Version '0.8.0'\n(\n        brokerAddress:'localhost:9092',\n        Topic:'KStreamKryoParser1Tester_KafkaCsvStream',\n        PartitionIDList:'0',\n        startOffset:0\n)\nPARSE USING StriimParser ()\nOUTPUT TO KafkaStream;\n\nCREATE WACTIONSTORE KRWactions CONTEXT OF KafkaStreamType\nEVENT TYPES ( KafkaStreamType )\n@PERSIST-TYPE@\n\nCREATE CQ KRInsertWactions\nINSERT INTO KRWactions\nSELECT TO_STRING(data[1]) as merchantId,\n    TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\n    TO_DOUBLE(data[7]) as amount,\n    TO_STRING(data[10]) as city \nFROM KafkaStream;\n\n/*\nCREATE CQ CQ2KafkaTypedStream\nINSERT INTO KafkaTypedStream\nSELECT TO_STRING(data[1]) as merchantId,\n    TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\n    TO_DOUBLE(data[7]) as amount,\n    TO_STRING(data[10]) as city \nFROM KafkaStream;\n*/\n\nEND APPLICATION KStreamKryoParserTest1;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSVSource using FileReader (\ndirectory:'@TEST-DATA-PATH@',\nWildCard:'smallposdata.csv',\npositionByEOF:false,\ncharset:'UTF-8'\n)\nparse using DSVParser (\nheader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\nfilename:'',\ndirectory:'@FEATURE-DIR@/logs/',\n    sequence:'00',\nrolloverpolicy:'eventcount:200,sequence:00'\n)\nformat using DSVFormatter (\n\n)\ninput from TypedCSVStream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetEventCount_actual.log') input from TypedCSVStream;\n\nend application DSV;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter\n  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  Username: '@TARGET_USER@',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1'\n )\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application reconnect;\nundeploy application reconnect;\ndrop application reconnect cascade;\nCREATE APPLICATION reconnect recovery 1 second interval;\n\nCREATE  SOURCE mssqlsource USING MssqlReader  ( \n  Username: '@USERNAME@',\n  Password: '@PASSWORD@',\n  ConnectionURL: '@URL@',\n  Tables: '@TABLE@',\n  FetchSize: 1\n ) \nOUTPUT TO sqlstream;\n\nCREATE TARGET dbtarget USING DatabaseWriter(\n  ConnectionURL:'@URL@',\n  Username:'@USERNAME@',\n  Password:'@PASSWORD@',\n  ConnectionRetryPolicy: 'retryInterval=15s,maxRetries=2',\n  BatchPolicy:'EventCount:5,Interval:30',\n  CommitPolicy:'EventCount:5,Interval:30',\n  Tables: '@TABLES@'\n ) INPUT FROM sqlstream;\n\n create Target tSysOut using Sysout(name:OrgData) input from sqlstream;\n end application reconnect;\n deploy application reconnect;\n start application reconnect;\n \n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION  @AppName@ RECOVERY 5 SECOND INTERVAL;\nCREATE OR REPLACE SOURCE  @AppName@_Source USING FileReader (\ndirectory:'@dataDir@',\nwildcard:'JsonNodeEvent.json',\npositionByEOF:false\n)\nPARSE USING Global.JSONParser (\n )  OUTPUT TO  @AppName@_rawstream;\n\nCREATE CQ @BuiltinFunc@CQ\nINSERT INTO  @BuiltinFunc@_Stream\nSELECT @BuiltinFunc@(x, 'Sno', data.get(\"_id\"), 'Name', data.get(\"firstname\"))\nFROM @AppName@_rawstream x;\n\nCREATE OR REPLACE CQ cq1\nINSERT INTO ClearUserData_Stream\nSELECT\nclearUserData(s1)\nFROM @BuiltinFunc@_Stream s1;\n\nCREATE OR REPLACE TARGET  @AppName@_FileTarget USING Global.FileWriter (\n  flushpolicy: 'EventCount:10000,Interval:30s',\n  adapterName: 'FileWriter',\n directory: '@logs@',\n  filename: '@BuiltinFunc@_JsonEventClearData',\n  rolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING Global.JSONFormatter  (\n  handler: 'com.webaction.proc.JSONFormatter',\n  jsonMemberDelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true',\n  formatterName: 'JSONFormatter',\n  jsonobjectdelimiter: '\\n' )\nINPUT FROM ClearUserData_Stream;\n\nEnd application  @AppName@;\nDeploy application  @AppName@;\nStart application  @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@;\n\nCREATE SOURCE @parquetsrc@ USING S3Reader (\n    bucketname:'',\n    objectnameprefix:'',\n     )\nPARSE USING ParquetParser (\n)\nOUTPUT TO @appname@Stream;\n\nCREATE CQ @appname@CQ\nINSERT INTO @appname@CqOut\n    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;\n\nCREATE OR REPLACE TARGET @filetarget@ USING FileWriter (\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:1,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING AvroFormatter  (\nschemaFileName: 'AvroFileSchema'\n)\nINPUT FROM @appname@CqOut;\n\ncreate Target @s3target@ using S3Writer(\n    bucketname:'',\n    objectname:'',\n    uploadpolicy:'',\n    foldername:''\n)\nformat using AvroFormatter (\nschemaFileName: 'AvroS3Schema'\n)\ninput from @appname@CqOut;\n\ncreate Target @blobtarget@ using AzureBlobWriter(\n\taccountname:'',\n\taccountaccesskey:'',\n\tcontainername:'',\n    blobname:'',\n\tfoldername:'',\n\tuploadpolicy:'EventCount:10,interval:5s'\n)\nformat using AvroFormatter (\nschemaFileName: 'AvroAzureSchema'\n)\nINPUT FROM @appname@CqOut;\n\nCREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (\n    bucketname:'',\n    objectname:'',\n    foldername:'',\n    projectId:'',\n    uploadPolicy:''\n)\nformat using AvroFormatter (\nschemaFileName: 'AvroGCSSchema'\n)\nINPUT FROM @appname@CqOut;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": ""}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @AppName@;\n\nCREATE OR REPLACE SOURCE ReadHTTPPort USING Global.HTTPReader  ( \ndeferresponsetimeout: '10000ms', \n  keystore: '@dataFile@server.keystore.jks', \n  deferresponse: true, \n  keystorepassword: 'w@ct10n', \n  ipaddress: 'localhost', \n  threadcount: 10, \n  keystorepassword_encrypted: 'false', \n  adapterName: 'HTTPReader', \n  keystoretype: 'JKS', \n  portno: '@port@', \n  authenticateclient: true ) \nOUTPUT TO HTTPReaderOUT  ;\n\nCREATE OR REPLACE TARGET sysout USING Global.SysOut  ( \nname: 'sysout' ) \nINPUT FROM HTTPReaderOUT;\n\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nStart @AppName@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@\n Using OracleReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'qatest.@SourceTable@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCreate Source @APPNAME@_src1 Using OracleReader\n(\n Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n   _h_ReturnDateTimeAs:'ZonedDateTime'\n) Output To @APPNAME@_stream1;\n\nCreate Source @APPNAME@_src2 Using OracleReader\n(\n Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n   _h_ReturnDateTimeAs:'ZonedDateTime'\n) Output To @APPNAME@_stream2;\n\n\ncreate or replace stream @APPNAME@_combined_stream OF Global.WAEvent;\n\nCreate CQ @APPNAME@CQ1\ninsert into @APPNAME@_combined_stream\nselect\n* from @APPNAME@_stream1;\n\nCreate CQ @APPNAME@CQ2\ninsert into @APPNAME@_combined_stream\nselect\n* from @APPNAME@_stream2;\n\n\n\ncreate Target @APPNAME@_tgt1 using FileWriter(\n  filename:'CompressedMerchant.gz',\n  directory:'/logs/',\n  rolloverpolicy:'EventCount:100'\n)\nformat using ParquetFormatter (\n\tschemaFileName:'@FILENAME@',\n\tFormatAs:'@FORMATAS@'\n)\ninput from @APPNAME@_combined_stream;\n\ncreate Target @APPNAME@_tgt2 using FileWriter(\n  filename:'CompressedMerchant.gz',\n  directory:'/logs/',\n  rolloverpolicy:'EventCount:100'\n)\nformat using ParquetFormatter (\n\tschemaFileName:'@FILENAME@',\n\tFormatAs:'@FORMATAS@',\n\tmembers: 'Table=@metadata(TableName),OpName=@metadata(OperationName)'\n)\ninput from @APPNAME@_stream2;\n\ncreate Target @APPNAME@_tgt3 using FileWriter(\n  filename:'CompressedMerchant.gz',\n  directory:'/logs/',\n  rolloverpolicy:'EventCount:100'\n)\nformat using ParquetFormatter (\n\tschemaFileName:'@FILENAME@',\n\tFormatAs:'@FORMATAS@'\n)\ninput from @APPNAME@_stream1;\n\ncreate type @APPNAME@_type(\nid int,\nname String,\ncost float,\nTableName string,\nOperationName String\n);\n\ncreate or replace stream @APPNAME@_typed_Stream of @APPNAME@_type;\n\nCreate CQ @APPNAME@_TypedCQ\ninsert into @APPNAME@_typed_Stream\nselect\nto_int(data[0]),data[1],to_float(data[2]),\nmeta(@APPNAME@_stream2,'TableName'),\nMeta(@APPNAME@_stream2,'OperationName') from @APPNAME@_stream2;\n\n\ncreate Target @APPNAME@_tgt4 using FileWriter(\n  filename:'CompressedMerchant.gz',\n  directory:'/logs/',\n  rolloverpolicy:'EventCount:100'\n)\nformat using ParquetFormatter (\n\tschemaFileName:'@FILENAME@',\n\tFormatAs:'@FORMATAS@'\n)\ninput from @APPNAME@_typed_Stream;\n\n\n\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\nDROP EXCEPTIONSTORE @APP_NAME@_EXCEPTIONSTORE;\n\nCREATE APPLICATION @APP_NAME@ @APP_PROPERTY@ USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE STREAM @APP_NAME@_DataStream OF Global.WAEvent;\n\nCreate Source @APP_NAME@_Source Using @SOURCE_ADAPTER@ (\n\n) OUTPUT TO @APP_NAME@_DataStream;\n\nCREATE TARGET @APP_NAME@_Target USING @TARGET_ADAPTER@( \n\n) INPUT FROM @APP_NAME@_DataStream;\n\nCREATE OR REPLACE TARGET @APP_NAME@_SysOut USING Global.SysOut  ( \n\tname: '@APP_NAME@_SysOutWA' \n) INPUT FROM @APP_NAME@_DataStream;\n\nEND APPLICATION @APP_NAME@;\n\nDEPLOY APPLICATION @APP_NAME@ IN DEFAULT;\nSTART APPLICATION @APP_NAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP OuterJoinTester.OJApp;\nUNDEPLOY APPLICATION OuterJoinTester.OJApp;\nDROP APPLICATION OuterJoinTester.OJApp cascade;\n\nCREATE APPLICATION OJApp;\n\nCREATE source wsSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'banks.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:No,\n  trimquote:false\n) OUTPUT TO QaStream;\n\n\nCREATE source wsSource2 USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'bankCards.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:No,\n  trimquote:false\n) OUTPUT TO stream2;\n\n\n\nCREATE TYPE bankData\n(\nbankID Integer KEY,\nbankName String\n);\n\nCREATE TYPE cardData\n(\ncardID Integer KEY,\ncardName String\n);\n\nCREATE STREAM wsStream OF bankData;\nCREATE STREAM wsStream2 OF cardData;\n\n\nCREATE CACHE cache1 USING CsvReader(\ndirectory: '@TEST-DATA-PATH@',\nwildcard: 'banks.csv',\nheader: No,\ncolumndelimiter: ',',\ntrimquote: false\n) QUERY (keytomap:'bankID') OF bankData;\n\n\nCREATE CACHE cache2 USING CsvReader(\ndirectory: '@TEST-DATA-PATH@',\nwildcard: 'bankCards.csv',\nheader: No,\ncolumndelimiter: ',',\ntrimquote: false\n) QUERY (keytomap:'cardID') OF cardData;\n\n\nCREATE WACTIONSTORE oneWS CONTEXT OF bankData\nEVENT TYPES(bankData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE twoWS CONTEXT OF cardData\nEVENT TYPES(cardData )\n@PERSIST-TYPE@\n\n\n\nCREATE CQ csvTobankData\nINSERT INTO oneWS\nSELECT TO_INT(data[0]), data[1] FROM QaStream;\n\n\n\nCREATE CQ csvTobankData2\nINSERT INTO wsStream\nSELECT TO_INT(data[0]), data[1] FROM QaStream;\n\nCREATE CQ csvTobankData3\nINSERT INTO wsStream2\nSELECT TO_INT(data[0]), data[1] FROM stream2;\n\nCREATE CQ csvTobankData4\nINSERT INTO twoWS\nSELECT TO_INT(data[0]), data[1] FROM stream2;\n\n\nCREATE JUMPING WINDOW win1 OVER wsStream KEEP 20 rows;\n\n\nCREATE JUMPING WINDOW win2 OVER wsStream2 KEEP 4 rows;\n\n\n\nEND APPLICATION OJApp;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application GCSWriterTest;\nundeploy application GCSWriterTest;\ndrop application GCSWriterTest cascade;\ncreate application GCSWriterTest recovery 1 second interval;\ncreate source GCS_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'true'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\ncreate Target GCSTarget using GCSWriter(\n    bucketname:'@bucketname@',\n    objectname:'@objectname@',\n    projectId:'@project-id@',\n    uploadPolicy:'@UPLOAD-SIZE@',\n    ServiceAccountKey:'@file-path@'\n)\nformat using DSVFormatter (\n--members:'data'\n)\ninput from TypedCSVStream;\n\nend application GCSWriterTest;\ndeploy application GCSWriterTest on all in default;\nstart application GCSWriterTest;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP bq;\nUNDEPLOY APPLICATION bq;\nDROP APPLICATION bq CASCADE;\n\nCREATE APPLICATION bq RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE DEV_22964_source USING MSSqlReader\n(\n\tUsername: 'qatest',\n\tPassword: 'w3b@ct10n',\n\tConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',\n\tTables: 'QATEST.BitToBoolean',\n\tFetchTransactionMetadata: true, \n\tFetchSize: '1'\n)\nOUTPUT TO SS;\n\n\nCREATE or replace TARGET DEV_22964_target USING BigQueryWriter (\n\tserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\n\tprojectId: 'bigquerywritertest',\n    Tables:'QATEST.TABLE_TEST_1000001,qatest.% keycolumns(RONUM)',\n    mode:'Appendonly',\n    datalocation: 'US',\n\tnullmarker: 'NULL',\n\tcolumnDelimiter: '|',\n\tBatchPolicy: 'eventCount:100,Interval:10'\t\n) INPUT FROM ss;\n\nEND APPLICATION bq;\nDEPLOY APPLICATION bq;\nSTART APPLICATION bq;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING Global.DeltaLakeWriter (\n  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',\n  hostname: 'adb-5292730997167687.7.azuredatabricks.net',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.testaswin',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:1,interval:10s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@2 USING Global.DeltaLakeWriter (\n  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',\n  hostname: 'adb-5292730997167687.7.azuredatabricks.net',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.testaswin',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:1,interval:10s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@3 USING Global.DeltaLakeWriter (\n  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',\n  hostname: 'adb-5292730997167687.7.azuredatabricks.net',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.testaswin',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:1,interval:10s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@4 USING Global.DeltaLakeWriter (\n  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',\n  hostname: 'adb-5292730997167687.7.azuredatabricks.net',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.testaswin',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:1,interval:10s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @STREAM@;\n\n\nCREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(\n  name: 'CDDL',\n  filename:'WAEventDump.log'\n)INPUT FROM @STREAM@;;\n\ncreate Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application consoleconsoletest.noApp;\nundeploy application consoleconsoletest.noApp;\ndrop application consoleconsoletest.noApp cascade;\n\nDROP USER consoletest;\nDROP NAMESPACE consoletest CASCADE;\nCREATE USER consoletest IDENTIFIED BY consoletest;\n\n--Creating two roles\nCreate role consoletest.R1;\nCreate role consoletest.R2;\n\n--Granting consoletest.dev to new role R1 and granting single role of Drop to R2\nGrant consoletest.dev to role consoletest.R1;\nGrant deploy on application consoletest.* to role consoletest.dev;\nGrant undeploy on application consoletest.* to role consoletest.dev;\nRevoke update on Target consoletest.noApp from role consoletest.dev;\nRevoke select on WACTIONSTORE consoletest.* from role consoletest.dev;\nRevoke consoletest.admin from user consoletest;\nGrant drop on application consoletest.* to role consoletest.R2;\n\n--Granting new roles to the user\nGrant consoletest.R1 to user consoletest;\nGrant consoletest.R2 to user consoletest;\n\n--Creating application\nCreate application consoletest.noApp;\n\nCREATE TYPE consoletest.Atm(\nproductID String KEY,\nstateID String,\nproductWeight int,\nquantity double,\nsize long,\ncurrentDate DateTime);\n\nCREATE source consoletest.implicitSOurce USING FileReader (\ndirectory:'@TEST-DATA-PATH@',\ncolumndelimiter: ',',\nwildcard:'ISdata.csv',\nblocksize: 10240,\npositionByEOF:false\n)\nPARSE USING DSVParser (\nheader:False,\ntrimquote:false\n) OUTPUT TO consoletest.CsvStream;\n\nCREATE TYPE consoletest.wsType(\nquantity double KEY,\ncurrentDate DateTime\n);\n\nCREATE STREAM consoletest.newStream OF consoletest.Atm;\n\nCREATE CQ consoletest.newCQ\nINSERT INTO consoletest.newStream\nSELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]),TO_DATE(data[5]) FROM\nconsoletest.CsvStream;\n\nCREATE WINDOW consoletest.win1\nOVER consoletest.newStream\nkeep within 3 minute;\n\nCREATE STREAM consoletest.newStream2 of consoletest.wsType;\n\nCREATE WACTIONSTORE consoletest.WS1 CONTEXT OF consoletest.wsType\nEVENT TYPES(consoletest.wsType );\n\nCreate cq consoletest.newCQ2\ninsert into consoletest.ws1 (quantity,currentDate)\nselect quantity, currentDate from consoletest.newStream;\n\nCreate Target consoletest.Trace Using Sysout (name: 'Trace') Input From consoletest.newStream;\n\nEnd Application consoletest.noApp;\n\n\nConnect consoletest consoletest;\n\n\nAlter application noApp;\n\ncreate or replace type consoletest.wsType(quantity double KEY, size long, currentDate DateTime);\n\nCreate or replace Target consoletest.Trace1 Using Sysout (name:'Trace1') Input From consoletest.newStream;\n\nend application consoletest.noApp;\nalter application consoletest.noApp recompile;\n\nDeploy application consoletest.noApp;\nStart application consoletest.noApp;\n\n-- The following step should fail as expected \nSelect count (*) from WS1;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop application Component_Disappear cascade;\ndrop cq Test_Subquery_Cq;\ndrop stream Test_Subquery_Cq_out;\n\nCREATE APPLICATION Component_Disappear;\n\ncreate or replace CQ Test_Subquery_Cq\ninsert into Test_Subquery_Cq_out\nSELECT f.topic as topic, sum(f.rawdatacount) as TotalLast24hour, B.rawdatacount as TotalLast1hour FROM JUMP_WND_1EVT_1MIN h\n   join SLIDE_WND_HOURLYTOTALS_KAFKADATA_FILE f on 1=1\n   join (SELECT rawdatacount, topic,timerange from  ET_HOURLYTOTALS_KAFKADATA_FILE,JUMP_WND_1EVT_30SEC where timerange = DHOURS(DNOW())-1) B on B.topic=f.topic\n   Group by f.topic;\n\nEND APPLICATION Component_Disappear;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create Application UdpDsv;\ncreate source UdpDsvCSVSource using UDPReader (\n\tIpAddress:'127.0.0.1',\n\tPortNo:'3546',\n\tcharset: 'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO UdpDsvCsvStream;\ncreate Target UdpDsvDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/userLookup') input from UdpDsvCsvStream;\nend Application UdpDsv;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop tpcc;\nundeploy application tpcc;\ndrop application tpcc cascade;\nCREATE APPLICATION tpcc;\n\nCreate Source oracSource\n Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL:'localhost:1521:orcl',\n Tables:'QATEST.TIMETEST',\n Fetchsize:1\n)\nOutput To DataStream;\n\nCREATE TARGET WriteCDCOracle USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:orcl',\n  Username:'qatest',\n  Password:'qatest',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: 'QATEST.TIMETEST,QATEST.TIMETEST_TGT'\n) INPUT FROM DataStream;\n\ncreate Target t2 using SysOut(name:Foo2) input from DataStream;\n\nEND APPLICATION tpcc;\ndeploy application tpcc in default;\nstart tpcc;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "-- Creating a namespace ensures there won't be conflicts with the regular version of\n-- PosApp. The only difference between this version and the regular version is\n-- that the CQ that parses the source stream includes a PAUSE clauses that introduces a\n-- 40-millisecond pause after each event is read, simulating the way the dashboard would\n-- work with real-time data.\nStop PosAppImplicit.PosAppImplicit;\nundeploy application PosAppImplicit.PosAppImplicit;\ndrop application PosAppImplicit.PosAppImplicit cascade;\n\n\n-- The PosApp sample application demonstrates how a credit card\n-- payment processor might use WebAction to generate reports on current\n-- transaction activity by merchant and send alerts when transaction\n-- counts for a merchant are higher or lower than average for the time\n-- of day.\n\nCREATE Application PosAppImplicit;\n\n-- All CREATE statements between here and the END APPLICATION\n-- statement will create objects in the PosApp application.\n\n-- source CsvDataSource\n\nCREATE source CsvDataSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'posdata.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\n\n-- This is the primary data source for this application.\n-- In a real-world application, it would be real-time data. Here,\n-- the data comes from a comma-delimited file, posdata.csv. The first\n-- two lines of that file are:\n--\n-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY\n-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand\n--\n-- The \"header:Yes\" setting tells WebAction that the first line contains\n-- field labels that should not be treated as data.\n--\n-- The \"positionByEOF:false\" setting tells WebAction to start reading\n-- from the beginning of the file. (In a real-world application\n-- reading real log files, you would typically use the default \"true\"\n-- setting so that the application would read only new data.)\n--\n-- The OUTPUT TO clause automatically creates the stream\n-- CsvStream using the WAEvent type associated with the CSVReader\n-- adapater. The only field from WAEvent used by this application\n-- is \"data\", an array containing the delimited fields.\n\n\n-- CQ CsvToPosData\n--\n-- A stream's type must be declared before the stream, and a CQ's\n-- output stream must be defined before the CQ. Hence type-stream-CQ\n-- sequences like the following are very common.\n\nCREATE CQ CsvToPosData\nINSERT INTO PosDataStream\nSELECT TO_STRING(data[1]) as merchantId,\n       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,\n       TO_DOUBLE(data[7]) as amount,\n       To_String(data[9]) as zip\nFROM CsvStream;\n\n-- Here, \"data\" refers to the array mentioned above, and the number\n-- in brackets specifies a field from the array, counting from zero.\n-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH\n-- AMOUNT, and data[9] is ZIP. These correspond to the merchantId,\n-- dateTime, hourValue, amount, and zip fields in PosDataStream, as\n-- defined by the PosData type.\n--\n-- The DATETIME field from the source is converted to both a DateTime\n-- value, used as the event timestamp by the application, and an int,\n-- which is used to look up historical hourly averages from the\n-- HourlyAveLookup cache, discussed below.\n--\n-- The other six fields are discarded. Thus the first line of data\n-- from posdata.csv has at this point been reduced to five values:\n--\n-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)\n-- 20130312173210 (DateTime)\n\n-- 2.20 (amount)\n-- 41363 (zip)\n\n\n-- CQ GenerateMerchantTxRateOnly\n--\n-- The PosData5Minutes window bounds the data so that the query can\n-- use the COUNT, FIRST, and SUM functions and join data from the\n-- HourlyAveLookup cache. (Aggregate functions cannot be used and\n-- joins cannot be performed on unbound real-time data.)\n--\n-- The HourlyAveLookup cache provides historical average sales\n-- amounts for the current hour for each merchant.\n\nCREATE JUMPING WINDOW PosData5Minutes\nOVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE TYPE MerchantHourlyAve(\n  merchantId String,\n  hourValue int,\n  hourlyAve int\n);\nCREATE CACHE HourlyAveLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'hourlyData.txt'\n)\nPARSE USING DSVParser (\n  header: Yes,\n  trimquote:false\n) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;\n\nCREATE CQ GenerateMerchantTxRateOnly\nINSERT INTO MerchantTxRateOnlyStream\nSELECT p.merchantId as merchantId,\n       p.zip as zip,\n       FIRST(p.dateTime) as startingTime,\n       COUNT(p.merchantId) as count,\n       SUM(p.amount) as totalAmount,\n       l.hourlyAve/12 as hourlyAve,\n       l.hourlyAve/12 * CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END as upperLimit,\n       l.hourlyAve/12 / CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END as lowerLimit,\n       '<NOTSET>' as category,\n       '<NOTSET>' as status\nFROM PosData5Minutes p, HourlyAveLookup l\nWHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue\nGROUP BY p.merchantId;\n\n-- This query aggregates five minutes' worth of data for each\n-- merchant, calculating the total transaction count and amount, and\n-- calculates the upperLimit and lowerLimit values based on the\n-- historical average transaction count for the current hour of the\n-- day from the HourlyAveLookup cache. The category and status fields\n-- are left unset to be populated by the next query.\n\n\n-- CQ GenerateMerchantTxRateWithStatus\n--\n-- This query sets the count values used by the Dashboard map and the\n-- status values used to trigger alerts.\n\nCREATE CQ GenerateMerchantTxRateWithStatus\nINSERT INTO MerchantTxRateWithStatusStream\nSELECT merchantId as merchantId,\n       zip as zip,\n       startingTime as startingTime,\n       count as count,\n       totalAmount as totalAmount,\n       hourlyAve as hourlyAve,\n       upperLimit as upperLimit,\n       lowerLimit as lowerLimit,\n       CASE\n         WHEN count >10000 THEN 'HOT'\n         WHEN count > 800 THEN 'WARM'\n         WHEN count >200 THEN 'COOL'\n         ELSE 'COLD' END as category,\n       CASE\n         WHEN count > upperLimit THEN 'TOOHIGH'\n         WHEN count < lowerLimit THEN 'TOOLOW'\n         ELSE 'OK' END as status\nFROM MerchantTxRateOnlyStream;\n\n\n-- WAction store MerchantActivity\n--\n-- The following group of statements create and populate the MerchantActivity\n-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced\n-- with merchant details from NameLookup cache and with latitude and longitude\n-- values from the USAddressData cache.\n\nCREATE TYPE MerchantActivityContext(\n  MerchantId String KEY,\n  StartTime  DateTime,\n  CompanyName String,\n  Category String,\n  Status String,\n  Count int,\n  HourlyAve int,\n  UpperLimit double,\n  LowerLimit double,\n  Zip String,\n  City String,\n  State String,\n  LatVal double,\n  LongVal double\n);\nCREATE TYPE MerchantTxRate(\n  merchantId String KEY,\n  zip String,\n  startingTime DateTime,\n  count int,\n  totalAmount double,\n  hourlyAve int,\n  upperLimit double,\n  lowerLimit double,\n  category String,\n  status String\n);\n\nCREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext\nEVENT TYPES ( MerchantTxRate )\n@PERSIST-TYPE@\n\nCREATE TYPE MerchantNameData(\n  merchantId String KEY,\n  companyName String\n);\n\nCREATE TYPE USAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\nCREATE CACHE NameLookup using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'MerchantNames.csv',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:'yes',\n  trimquote:false\n)\nQUERY(keytomap:'merchantId') OF MerchantNameData;\n\nCREATE CACHE ZipLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'USAddresses.txt',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header: Yes,\n  columndelimiter: '\\t',\n  trimquote:false\n) QUERY (keytomap:'zip') OF USAddressData;\n\nCREATE CQ GenerateWactionContext\nINSERT INTO MerchantActivity\nSELECT  m.merchantId,\n        m.startingTime,\n        n.companyName,\n        m.category,\n        m.status,\n        m.count,\n        m.hourlyAve,\n        m.upperLimit,\n        m.lowerLimit,\n        m.zip,\n        z.city,\n        z.state,\n        z.latVal,\n        z.longVal\nFROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z\nWHERE m.merchantId = n.merchantId AND m.zip = z.zip\nLINK SOURCE EVENT;\n\n\n-- CQ GenerateAlerts\n--\n-- This CQ sends an alert when a merchant's status value changes to\n-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.\n\n\nCREATE STREAM AlertStream OF Global.AlertEvent;\n\nCREATE CQ GenerateAlerts\nINSERT INTO AlertStream\nSELECT n.CompanyName,\n       m.MerchantId,\n       CASE\n         WHEN m.Status = 'OK' THEN 'info'\n         ELSE 'warning' END,\n       CASE\n         WHEN m.Status = 'OK' THEN 'cancel'\n         ELSE 'raise' END,\n       CASE\n         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)\n         ELSE ''\n         END\nFROM MerchantTxRateWithStatusStream m, NameLookup n\nWHERE m.merchantId = n.merchantId;\n\n\n-- The following statement loads visualization (Dashboard) settings\n-- from a file.\n\n\n--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM AlertStream;\n\nCREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;\n\n\nEND APPLICATION PosAppImplicit;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 7\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> WS\n--\n\nSTOP Recov7Tester.RecovTest7;\nUNDEPLOY APPLICATION Recov7Tester.RecovTest7;\nDROP APPLICATION Recov7Tester.RecovTest7 CASCADE;\nCREATE APPLICATION RecovTest7 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE WactionType (\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE STREAM wacStream OF WactionType;\n\nCREATE CQ InsertWactions\nINSERT INTO wacStream\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE WINDOW waWindow\nOVER wacStream KEEP WITHIN 1 SECOND ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions\nINSERT INTO Wactions\nSELECT *\nFROM waWindow\nLINK SOURCE EVENT;\n\nEND APPLICATION RecovTest7;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application MSSQLTransactionSupportMSSQLToMySQL1;\nundeploy application MSSQLTransactionSupportMSSQLToMySQL1;\ndrop application MSSQLTransactionSupportMSSQLToMySQL1 cascade;\n\nCREATE APPLICATION MSSQLTransactionSupportMSSQLToMySQL1 recovery 1 second interval;\n\nCreate Source ReadFromMSSQL6\nUsing MSSqlReader\n(\nUsername:'@READER-NAME@',\nPassword:'@READER-PASSWORD@',\nDatabaseName:'@SRCDB-NAME@',\nConnectionURL:'@CONN-URL@',\nTables:'@WATABLES-SRC@',\nTransactionSupport: 'true',\nFetchTransactionMetadata:'true',\nFilterTransactionBoundaries: true,\nCompression:'false',\nConnectionPoolSize:1\n)\nOutput To MSSQLTransactionSupportMSSQLToMySQL1Stream;\n\n\nCREATE TARGET WriteToMySQL6 USING DatabaseWriter(\nConnectionURL:'@TGT-URL@',\nUsername:'@WRITER-UNAME@',\nPassword:'@WRITER-PASSWORD@',\nBatchPolicy:'EventCount:5,Interval:5',\nCommitPolicy:'EventCount:1,Interval:1',\nTables: '@WATABLES-SRC@,@WATABLES-TGT@'\n)\nINPUT FROM MSSQLTransactionSupportMSSQLToMySQL1Stream;\n\nCREATE TARGET MSSqlReaderOutput6 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportMSSQLToMySQL1Stream; \n\n\nCREATE OR REPLACE TARGET MSSQLFileOut6 USING FileWriter  ( \n  rolloverpolicy: 'EventCount:10000,Interval:30s',\n  adapterName: 'FileWriter',\n  directory:'@FEATURE-DIR@/logs/',\n  filename: 'TransactionSupportMSSQLToMySQL.txt'\n ) \nFORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true',\n  jsonobjectdelimiter: '\\n'\n ) \nINPUT FROM MSSQLTransactionSupportMSSQLToMySQL1Stream;\n\nEND APPLICATION MSSQLTransactionSupportMSSQLToMySQL1;\ndeploy application MSSQLTransactionSupportMSSQLToMySQL1;\nstart application MSSQLTransactionSupportMSSQLToMySQL1;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION SystemTimeTester.SystemTimeWindows;\nUNDEPLOY APPLICATION SystemTimeTester.SystemTimeWindows;\nDROP APPLICATION SystemTimeTester.SystemTimeWindows cascade;\n\n\nCREATE APPLICATION SystemTimeWindows;\n\nCREATE TYPE RandomData(\nbankNumber int KEY,\nbankName String\n);\n\nCREATE  SOURCE ranDataSource USING StreamReader (\n  OutputType: 'SystemTimeTester.RandomData',\n  noLimit: 'false',\n  isSeeded: 'true',\n  maxRows: 0,\n  iterations: 30,\n  iterationDelay: 1000,\n  StringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',\n  NumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]R'\n )\nOUTPUT TO CSVDataStream;\n\nCREATE STREAM RandomDataStream OF RandomData;\n\nCREATE CQ ParseRandomData\nINSERT INTO RandomDataStream\nSELECT data[0], data[1]\nFROM CSVDataStream;\n\nCREATE @WINDOWTYPE@ WINDOW tierone OVER RandomDataStream keep within 20 second;\n\nCREATE STREAM onetwostream OF RandomData;\n\nCREATE CQ onetwocq\nINSERT INTO onetwostream\nSELECT bankNumber,bankName\nFROM tierone\nwhere  bankNumber >= 300\norder by bankName;\n\nCREATE WACTIONSTORE MyDataActivity  CONTEXT OF RandomData\nEVENT TYPES ( RandomData  )\n@PERSIST-TYPE@\n\nCreate CQ TrackMyDataActivity\nINSERT INTO MyDataActivity\nSELECT bankNumber,bankName from @FROMSTREAM@\nwhere  bankNumber >= 300\norder by bankName\nLINK SOURCE EVENT;\n\nEND APPLICATION SystemTimeWindows;\ndeploy application SystemTimeWindows;\nstart application SystemTimeWindows;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "DROP TYPE T1;\nDROP CACHE C1;\n\n\nCREATE  TYPE T1  ( id String ,\nairport_ref String,\nairport_ident String KEY ,\ntype1 String ,\ndescription String ,\nfrequency_mhz String\n );\n\nCREATE CACHE C1 USING FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: '1MB.csv',\n  charset: 'UTF-8',\n  blockSize: '64',\n  positionbyeof: 'false'\n )\nPARSE USING DSVPARSER (\n  columndelimiter: ',',\n  rowdelimiter: '\\n:\\r',\n  header: 'true'\n )\nQUERY (\n  keytomap: 'id'\n )\n OF T1;\n\n-- Loading Cache C1 which is accessed by DataSourceApp.tql\n\nload cache C1;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@1;\nundeploy application @APPNAME@1;\nstop application @APPNAME@2;\nundeploy application @APPNAME@2;\n\nDROP STREAM @APPNAME@_STREAM;\nDROP APPLICATION @APPNAME@1 CASCADE;\nDROP APPLICATION @APPNAME@2 CASCADE;\n\n\nCREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',\n\t\t\t\t\tacks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\nCREATE OR REPLACE STREAM @APPNAME@_stream OF Global.waevent persist using @APPNAME@_KafkaPropset;\n\nCREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';\ncreate application @APPNAME@1 recovery 5 SECOND INTERVAL;\n\ncreate or replace stream @APPNAME@_stream2 of Global.waevent;\n\nCREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNEC4TION_URL@',\n  Tables: '$table1',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_AGENTFLOW1;\n\nCREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@TABLENAME@2',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_SERVERFLOW1;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;\nCREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x\n WHERE META(x,'TableName').toString() == 'QATEST.ORCTOMSSQLPLATFM_SOURCE4';\n\nCREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n )\nINPUT FROM @STREAM@2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(\n  name: 'CDDL',\n  filename:'WAEventDump.log'\n)INPUT FROM @STREAM@;\n\nCREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo) input from @STREAM@;\n\nEND APPLICATION @APPNAME@1;\n\n\n\nCREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';\nCREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;\n\ncreate or replace stream @APPNAME@_stream3 of Global.waevent;\n\nCREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '$table2',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\n\nEND FLOW @APPNAME@_AGENTFLOW2;\n\nCREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@TABLENAME@4',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_SERVERFLOW2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;\nCREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y\n WHERE META(y,'TableName').toString() == 'QATEST.ORCTOMSSQLPLATFM_SOURCE2';\n\nCREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n )\nINPUT FROM @STREAM@3;\n\nCREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@3;\n\nEND APPLICATION @APPNAME@2;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( \n  Username: '@SRC_USERNAME@',\n  Password: '@SRC_PASSWORD@',\n  ConnectionURL: '@CDC_URL@',\n  Tables: '@Source1Tables@' ) \nOUTPUT TO @APPNAME@cdcStream;\n\nCREATE OR REPLACE EXTERNAL CACHE @APPNAME@1 ( \n  AdapterName:'DatabaseReader',\n  Username: '@SRC_USERNAME@',\n  Password: '@SRC_PASSWORD@',\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  FetchSize: 20,\n  DatabaseProviderType: 'Default',\n  Table: '@Source3Tables@',\n  Columns: 'col1,col2,col3,col4,uniquecol',\n  keytomap: 'uniquecol')  \nOF @APPNAME@cachetype;\n\nCREATE OR REPLACE EXTERNAL CACHE @APPNAME@2 ( \n  AdapterName:'DatabaseReader',\n  Username: '@SRC_USERNAME@',\n  Password: '@SRC_PASSWORD@',\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  FetchSize: 10,\n  DatabaseProviderType: 'Default',\n  Table: '@Source2Tables@',\n  Columns: 'col1,col2,col3,col4,uniquecol',\n  keytomap: 'uniquecol')  \nOF @APPNAME@cachetype;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION LongRunningQueryTester.LongRunningQuery;\nUNDEPLOY APPLICATION LongRunningQueryTester.LongRunningQuery;\nDROP APPLICATION LongRunningQueryTester.LongRunningQuery cascade;\n\nCREATE APPLICATION LongRunningQuery;\n\n\n\n\nCREATE TYPE RandomData(\nmyName String,\nstreetAddress String,\nbankName String,\nbankNumber int KEY,\nbankAmount double\n);\n\n\nCREATE SOURCE ranDataSource using StreamReader(\nOutputType: 'LongRunningQueryTester.RandomData',\nnoLimit: 'false',\nmaxRows: 0,\niterations: 2,\niterationDelay: 1000,\nStringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',\nNumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]G'\n)OUTPUT TO CSVDataStream;\n\nCREATE STREAM RandomDataStream OF RandomData;\n\nCREATE CQ ParseRandomData\nINSERT INTO RandomDataStream\nSELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4])\nFROM CSVDataStream;\n\n\n\n\nCREATE TYPE myData(\nmyName String,\nmyAddress String,\nmyBankName String,\nmyBankNumber int KEY,\nmyBankAmount double\n);\n\nCREATE STREAM myDataStream OF myData;\n\nCREATE CQ GetMyData\nINSERT INTO MyDataStream\nSELECT myName, streetAddress, bankName, bankNumber, bankAmount\nFROM RandomDataStream;\n\n\nCREATE WACTIONSTORE MyDataActivity\nCONTEXT OF MyData\nEVENT TYPES(myData )\n@PERSIST-TYPE@\n\nCreate CQ TrackMyDataActivity\nINSERT INTO MyDataActivity\nSelect * from myDataStream\nLINK SOURCE EVENT;\n\n\nEND APPLICATION LongRunningQuery;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nCREATE APPLICATION ExportApp2_API;\n\nCREATE OR REPLACE SOURCE ExportApp2_API_FileSource USING FileReader ( \n  wildcard: 'posdata.csv', \n  positionByEOF: false, \n  directory: '@testdata_dir@' ) \nPARSE USING DSVParser ( \n ) \nOUTPUT TO ExportApp2_API_SampleStream;\n\nCREATE OR REPLACE TARGET ExportApp2_API_NullTarget USING NullWriter ( \n ) \nINPUT FROM ExportApp2_API_SampleStream;\n\nEND APPLICATION ExportApp2_API;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCREATE SOURCE @SourceName@ USING PostgreSQLReader  ( \nReplicationSlotName: 'test_slot',\nadapterName: 'PostgreSQLReader',\nTransactionSupport: false, \n  FetchTransactionMetadata: false, \n  Compression: false, \n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', \n  Password_encrypted: 'false', \n  ConnectionURL: '@ConnectionURL@', \n  Fetchsize: 0, \n  ConnectionPoolSize: 10, \n  StartPosition: 'EOF', \n  Username: '@UN@', \n  cdcRoleName: 'STRIIM_READER', \n  Password: '@PWD@', \n  Tables: 'qatest.%', \n  FilterTransactionBoundaries: true, \n  SendBeforeImage: true, \n  AutoDisableTableCDC: false ) \nOUTPUT TO @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\n\nCREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'), 'OpTime',META(x, 'TimeStamp'))) FROM @SRCINPUTSTREAM@ x; ;\n\nCREATE TARGET @targetName@ USING DatabaseWriter  ( \nConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', \n  ParallelThreads: '', \n  CheckPointTable: 'CHKPOINT', \n  Password_encrypted: 'false', \n  ConnectionURL: '@ConnectionURL@', \n  CommitPolicy: 'EventCount:1,Interval:60', \n  StatementCacheSize: '50', \n  DatabaseProviderType: 'Default', \n  Username: '@UN@', \n  Password: '@PWD@', \n  PreserveSourceTransactionBoundary: 'false', \n  BatchPolicy: 'EventCount:1,Interval:60', \n  Tables: '@TableMapping@' ) \nINPUT FROM admin.sqlreader_cq_out;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE TYPE PosData(\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION MysqlToDbApp;\nUNDEPLOY APPLICATION MysqlToDbApp;\nDROP APPLICATION MysqlToDbApp CASCADE;\n\n\nCREATE APPLICATION MysqlToDbApp RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE MysqlSource USING MysqlReader\n(\n  Username: '',\n  Password: '',\n  Tables: '',\n  ConnectionURL: '',\n  Password_encrypted: 'false',\n  connectionRetryPolicy: 'retryInterval=30, maxRetries=3'\n)\nOUTPUT TO MysqlReaderOut;\n\n\nCREATE TARGET DbTarget USING DatabaseWriter\n(\n  Username: '',\n  Password: '',\n  Tables: '',\n  ConnectionURL: '',\n  Password_encrypted: 'false',\n  connectionRetryPolicy: 'retryInterval=30, maxRetries=3'\n)\nINPUT FROM MysqlReaderOut;\n\n\n\n\nEND APPLICATION MysqlToDbApp;\nDEPLOY APPLICATION MysqlToDbApp;\nSTART APPLICATION MysqlToDbApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@;\n\nCREATE OR REPLACE SOURCE CCBReader USING FileReader (\n  wildcard: '@WILDCARD@',\n  positionbyeof: false,\n  directory: '@TESTDIR@'\n  )\nPARSE USING Global.CobolCopybookParser (\n  copybookDialect: 'Mainframe',\n  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',\n  GroupPolicy: '',\n  TimeoutPolicy: '1s',\n  dataFileOrganization: 'Default',\n  ProcessCopyBookFileAs: 'SingleEvent',\n  skipIndent: 0,\n  dataFileFont: 'utf8',\n  CopybookFileFormat: 'USE_STANDARD_COLUMNS',\n  copybookSplit: 'None',\n  copybookFileName: '@CCBFILE@'\n   )\nOUTPUT TO CCBStream;\n\nCREATE OR REPLACE TARGET JSONWriter USING FileWriter (\n  filename: '%@metadata(FileName)%',\n  directory: '@DIR@',\n  rolloverpolicy: 'EventCount:10000,Interval:30s',\n  flushpolicy: 'EventCount:1,Interval:30s'\n  )\nFORMAT USING JSONFormatter()\nINPUT FROM CCBStream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@ on all in default;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING IncrementalBatchReader  ( \n  FetchSize: 1,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n CheckColumn: '@CHECKCOLUMN@',\n startPosition: '%=-1',\n  PollingInterval: '5sec'\n  )\n  OUTPUT TO @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @appName@;\nUNDEPLOY APPLICATION @appName@;\nDROP APPLICATION @appName@ CASCADE;\n\nCREATE APPLICATION @appName@;\n\nCREATE OR REPLACE TYPE @EventType@ (\n CC_Number java.lang.String,\n Amount java.lang.String,\n TXN_Type java.lang.String,\n TXN_Timestamp java.lang.String);\n\nCREATE SOURCE @sourceComp@ USING Global.FileReader (\n  rolloverstyle: 'Default',\n  blocksize: 64,\n  wildcard: '@sourceFileName@',\n  skipbom: true,\n  directory: '@sourceFileDir@',\n  includesubdirectories: false,\n  positionbyeof: false )\nPARSE USING Global.DSVParser (\n  trimwhitespace: true,\n  commentcharacter: '',\n  linenumber: '-1',\n  columndelimiter: ',',\n  trimquote: true,\n  columndelimittill: '-1',\n  eventtype: '@EventType@',\n  ignoreemptycolumn: false,\n  separator: ':',\n  quoteset: '\\\"',\n  charset: 'UTF-8',\n  ignoremultiplerecordbegin: 'true',\n  ignorerowdelimiterinquote: false,\n  blockascompleterecord: false,\n  rowdelimiter: '\\n',\n  nocolumndelimiter: false,\n  headerlineno: 0,\n  header: false )\nOUTPUT TO @sourceOutStream@;\n\nCREATE OR REPLACE STREAM @sourceOutStream@ OF @EventType@;\n\nCREATE CQ @match_patternCQ@\nINSERT INTO @patternOutStream@\nSELECT\nLIST(A,B) as events,\nCOUNT(B) as count\nFROM @sourceOutStream@ t\nMATCH_PATTERN T A+ (W|B)\nDEFINE\n\tA = t(TXN_Type = 'AUTH/HOLD'),\n\tB = t(TXN_Type = 'CHARGE'),\n\tT = TIMER(interval 3 minute),\n\tW = WAIT(T)\nPARTITION BY t.CC_Number;;\n\nCREATE CQ @cqForTarget1@\nINSERT INTO @target1InStream@\nSELECT events FROM @patternOutStream@ a\nWHERE a.count > 0;;\n\nCREATE CQ @cqForTarget2@\nINSERT INTO @target2InStream@\nSELECT events FROM @patternOutStream@ a\nWHERE a.count = 0;;\n\nCREATE TARGET @target1@ USING Global.FileWriter (\n  DataEncryptionKeyPassphrase: '',\n  flushpolicy: 'EventCount:10000',\n  rolloveronddl: 'true',\n  encryptionpolicy: '',\n  rolloverpolicy: 'EventCount:250000',\n  directory: '@targetFilesDir@',\n  filename: '@targetFile1@' )\nFORMAT USING Global.JSONFormatter  (\n  jsonMemberDelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true',\n  jsonobjectdelimiter: '\\n' )\nINPUT FROM @target1InStream@;\n\nCREATE TARGET @target2@ USING Global.FileWriter (\n  DataEncryptionKeyPassphrase: '',\n  flushpolicy: 'EventCount:10000',\n  rolloveronddl: 'true',\n  encryptionpolicy: '',\n  directory: '@targetFilesDir@',\n  filename: '@targetFile2@',\n  rolloverpolicy: 'EventCount:250000' )\nFORMAT USING Global.JSONFormatter  (\n  jsonMemberDelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true',\n  jsonobjectdelimiter: '\\n' )\nINPUT FROM @target2InStream@;\n\nEND APPLICATION @appName@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 40 with two sources and two WactionStores. A variety of partitioned windows in between\n-- assure that we are testing a complicated recovery scenario.\n--\n-- NOTE THIS APP IS INCONSISTENT AND NOT COMPATIBLE WITH THE CURRENT VERSION OF RECOVERY BECAUSE IT HAS COMBINING STREAMS\n--\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> JWc2 -> JWc5 -> WS1\n--   S2 -> JWc2 -> JWc7 -> WS2\n--\n\nSTOP Recov40Tester.RecovTest40;\nUNDEPLOY APPLICATION Recov40Tester.RecovTest40;\nDROP APPLICATION Recov40Tester.RecovTest40 CASCADE;\nCREATE APPLICATION RecovTest40 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStreamTop OF CsvData;\n\nCREATE CQ Csv1ToDataStreamTop\nINSERT INTO DataStreamTop\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ Csv2ToDataStreamTop\nINSERT INTO DataStreamTop\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\n\n\n\n\n\n\n\nCREATE JUMPING WINDOW TopJWc2\nOVER DataStreamTop KEEP 2 ROWS;\n\n\n\n\n\n\n\nCREATE STREAM DataStreamLeft OF CsvData;\nCREATE STREAM DataStreamRight OF CsvData;\n\nCREATE CQ DataToLeft\nINSERT INTO DataStreamLeft\nSELECT\n    FIRST(p.merchantId),\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    TO_DOUBLE(FIRST(p.amount))\nFROM TopJWc2 p;\n\nCREATE CQ DataToRight\nINSERT INTO DataStreamRight\nSELECT\n    FIRST(p.merchantId),\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    TO_DOUBLE(FIRST(p.amount))\nFROM TopJWc2 p;\n\n\n\n\n\n\nCREATE JUMPING WINDOW LeftJWc5\nOVER DataStreamLeft KEEP 5 ROWS;\n\nCREATE JUMPING WINDOW RightJWc10\nOVER DataStreamRight KEEP 10 ROWS;\n\n\n\nCREATE WACTIONSTORE WactionsLeft CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE WactionsRight CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ ToWactionsLeft\nINSERT INTO WactionsLeft\nSELECT\n    FIRST(p.merchantId),\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    TO_DOUBLE(FIRST(p.amount))\nFROM LeftJWc5 p;\n\nCREATE CQ ToWactionsRight\nINSERT INTO WactionsRight\nSELECT\n    FIRST(p.merchantId),\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    TO_DOUBLE(FIRST(p.amount))\nFROM RightJWc10 p;\n\nEND APPLICATION RecovTest40;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;\n\nCreate Source @SourceName@ Using Ojet\n\n(\n  Username:'c##qatest',\n  Password:'qatest',\n  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',\n  Tables:'CDB$ROOT.\"C##QATEST\".ojet_src;ORCLPDB.QATEST.ojet_src',\n  _h_useClassic:false,\n  Fetchsize:1,\n  Compression: true,\n  SupportPDB:true,\n  ReplicationSlotName:'null'\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter\n(\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',\n  Username:'c##qatest',\n  Password:'qatest',\n  BatchPolicy:'EventCount:1,Interval:1',\n  Tables:'CDB$ROOT.\"C##QATEST\".ojet_src,CDB$ROOT.\"C##QATEST\".ojet_tgt'\n) INPUT FROM @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName1@ USING DatabaseWriter\n(\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orclpdb',\n  Username:'qatest',\n  Password:'qatest',\n  BatchPolicy:'EventCount:1,Interval:1',\n  Tables:'ORCLPDB.QATEST.ojet_src,ORCLPDB.QATEST.ojet_tgt'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME1@;\nundeploy application @APPNAME1@;\nstop application @APPNAME2@;\nundeploy application @APPNAME2@;\ndrop application @APPNAME1@ cascade;\ndrop application @APPNAME2@ cascade;\n\n\nCREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\nCREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;\nCREATE OR REPLACE SOURCE @SourceName@ Using OracleReader\n(\n  Compression:true,\n  StartTimestamp:'null',\n  CommittedTransactions:true,\n  FilterTransactionBoundaries:true,\n  Password_encrypted:'false',\n  SendBeforeImage:true,\n  XstreamTimeOut:600,\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',\n  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE',\n  adapterName:'OracleReader',\n  Password:'qatest',\n  DictionaryMode:'OfflineCatalog',\n  FilterTransactionState:true,\n  connectionRetryPolicy:'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType:'LogMiner',\n  FetchSize: 1,\n  Username:'qatest',\n  OutboundServerProcessName:'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic:true,\n  CDDLAction:'Quiesce_Cascade',\n  CDDLCapture:'true'\n)OUTPUT TO @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME1@;\nDEPLOY APPLICATION @APPNAME1@;\nSTART APPLICATION @APPNAME1@;\n\nCREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  \n(\n  DatabaseProviderType:'Default',\n  CheckPointTable:'CHKPOINT',\n  PreserveSourceTransactionBoundary:'false',\n  Username:'qatest',\n  BatchPolicy:'EventCount:1',\n  CommitPolicy:'EventCount:1',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',\n  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',\n  Password:'qatest',\n  CDDLAction:'Process'\n) \nINPUT FROM @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME2@;\nDEPLOY APPLICATION @APPNAME2@;\nSTART APPLICATION @APPNAME2@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nSTOP PartiallyJumping1Tester.PartiallyJumping1;\nUNDEPLOY APPLICATION PartiallyJumping1Tester.PartiallyJumping1;\nDROP APPLICATION PartiallyJumping1Tester.PartiallyJumping1 CASCADE;\nCREATE APPLICATION PartiallyJumping1;\n\ncreate source CsvSource1 using FileReader\n(\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'WindowsTest.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n)\n parse using DSVParser\n(\n\theader:'yes',\n\tcolumndelimiter:','\n)\nOUTPUT TO CsvStream1;\n\nCREATE TYPE CsvData (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\nCREATE TYPE CsvData1 (\n  zip double\n);\n\nCREATE TYPE WactionData1 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData2 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData3 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData4 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData5 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData6 (\n  zip double\n);\nCREATE TYPE WactionData7 (\n  zip double\n);\n\nCREATE STREAM DataStream1 OF CsvData;\n\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE STREAM DataStream3 OF CsvData\nPARTITION BY city;\n\nCREATE STREAM DataStream4 OF CsvData;\n\nCREATE STREAM DataStream5 OF CsvData\nPARTITION BY city;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData3\nINSERT INTO DataStream3\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData4\nINSERT INTO DataStream4\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData5\nINSERT INTO DataStream5\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData6\nINSERT INTO DataStream6\nSELECT\n    TO_DOUBLE(data[8])\nFROM CsvStream1;\n\nCREATE CQ CsvToData7\nINSERT INTO DataStream7\nSELECT\n    TO_DOUBLE(data[8])\nFROM CsvStream1;\n\n-- Count based partially jumping window\nCREATE WINDOW DataStreamCount\nOVER DataStream1 KEEP 5 ROWS\nSLIDE 2\nPARTITION BY companyName;\n\n-- Time based partially jumping window\nCREATE WINDOW DataStreamTime OVER DataStream2 KEEP\nwithin 240 second\nSLIDE 15 second;\n\n-- Attribute based partially jumping window\nCREATE WINDOW DataStreamAtrribute\nOVER DataStream3 KEEP\nRANGE 30 SECOND\nON dateTime\nSLIDE 20 second;\n\n-- Count + time based partially jumping window\nCREATE WINDOW DataStreamCountTime\nOVER DataStream4 KEEP\n10 rows\nwithin 150 second\nSLIDE 10;\n\n-- Attribute + time based partially jumping window\nCREATE WINDOW DataStreamAttributeTime\nOVER DataStream5 KEEP\nrange 50 second\nON dateTime\nwithin 25 second\nSLIDE 2 SECOND\nPARTITION BY city;\n\nCREATE WACTIONSTORE Wactions1 CONTEXT OF WactionData1\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions2 CONTEXT OF WactionData2\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions3 CONTEXT OF WactionData3\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions4 CONTEXT OF WactionData4\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions5 CONTEXT OF WactionData5\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions6 CONTEXT OF WactionData6\nEVENT TYPES ( CsvData1 )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions7 CONTEXT OF WactionData7\nEVENT TYPES ( CsvData1 )\n@PERSIST-TYPE@\n\nCREATE CQ Data1ToWaction\nINSERT INTO Wactions1\nSELECT\n    p.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamCount p\ngroup by companyName;\n\nCREATE CQ Data2ToWaction\nINSERT INTO Wactions2\nSELECT\n    p.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamTime p;\n\nCREATE CQ Data3ToWaction\nINSERT INTO Wactions3\nSELECT\n    p.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamAtrribute p;\n\nCREATE CQ Data4ToWaction\nINSERT INTO Wactions4\nSELECT\n    p.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamCountTime p;\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions5\nSELECT\n    p.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamAttributeTime p\ngroup by city;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions6\nSELECT\n    count(*)\nFROM DataStreamCount p\ngroup by companyName;\n\nCREATE CQ Data7ToWaction\nINSERT INTO Wactions7\nSELECT\n    count(*)\nFROM DataStreamTime p;\n\nEND APPLICATION PartiallyJumping1;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING PostgreSQLReader  (\n  ReplicationSlotName: 'Slot_Name',\n  FilterTransactionBoundaries: 'true',\n  Username: 'User_Name',\n  ConnectionURL: 'Connection_URL',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'Password',\n  Tables: 'Tables',\n  PostgresConfig:'{\"ReplicationPluginConfig\": {\"Name\": \"WAL2JSON\", \"Format\": \"2\"}}'\n )\nOUTPUT TO @STREAM@ ;\n\nCREATE TARGET @SOURCE_NAME@_sysout USING Global.SysOut (\n  name: '@SOURCE_NAME@_sysout' )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nCREATE APPLICATION FtoKaf RECOVERY 2 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE FIletoRead USING FileReader  (\n  blocksize: 64,\n  positionbyeof: false,\n  rolloverstyle: 'Default',\n  includesubdirectories: false,\n  adapterName: 'FileReader',\n  directory: '@TEST-DATA-PATH@/Validate-Striim',\n  skipbom: true,\n  wildcard: 'FiletoRead.txt'\n )\n PARSE USING DSVParser  (\n  charset: 'UTF-8',\n  handler: 'com.webaction.proc.DSVParser_1_0',\n  linenumber: '-1',\n  nocolumndelimiter: true,\n  trimwhitespace: false,\n  columndelimiter: ',',\n  columndelimittill: '-1',\n  ignoremultiplerecordbegin: 'true',\n  ignorerowdelimiterinquote: false,\n  parserName: 'DSVParser',\n  separator: ':',\n  blockascompleterecord: false,\n  ignoreemptycolumn: false,\n  rowdelimiter: '\\n',\n  header: false,\n  headerlineno: 0,\n  quoteset: '\\\"',\n  trimquote: true\n )\nOUTPUT TO FtoK1 ;\n\nCREATE  TYPE FtoK2_Type  ( seq java.lang.Integer\n );\n\nCREATE STREAM FtoK2 OF FtoK2_Type;\n\nCREATE OR REPLACE CQ GetData\nINSERT INTO FtoK2\nSELECT TO_INT(data[0]) as seq\nFROM FtoK1;\n\nCREATE OR REPLACE TARGET KafkatoWrite USING KafkaWriter VERSION '0.11.0' (\n  KafkaConfigPropertySeparator: ';',\n  Mode: 'Sync',\n  adapterName: 'KafkaWriter',\n  Topic: 'kafkaTopic7',\n  brokerAddress: 'localhost:9092',\n  KafkaConfigValueSeparator: '=',\n  KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000'\n )\nFORMAT USING DSVFormatter  (   nullvalue: 'NULL',\n  standard: 'none',\n  handler: 'com.webaction.proc.DSVFormatter',\n  formatterName: 'DSVFormatter',\n  usequotes: 'false',\n  rowdelimiter: '\\n',\n  quotecharacter: '\\\"',\n  header: 'false',\n  columndelimiter: ','\n )\nINPUT FROM FtoK2;\n\nEND APPLICATION FtoKaf;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application ManyToManyADLSGen1;\nundeploy application ManyToManyADLSGen1;\ndrop application ManyToManyADLSGen1 cascade;\n\ncreate application ManyToManyADLSGen1 Recovery 5 second interval;\n\n\ncreate type ADLSGen1csv_type(\nid String,\nname String,\nseq String\n);\n\ncreate type ADLSGen1Order_type(\nid String,\nName String,\nCompany String\n);\n\n\n\ncreate source ADLSGen1CSVSource_multi using FileReader (\n        directory:'./Samples/AppData/',\n        WildCard:'Canon1000_All.csv',\n        positionByEOF:false,\n        charset:'UTF-8'\n)\nparse using DSVParser (\n        header:'no'\n)\nOUTPUT TO ADLSGen1CsvStream_user;\n\ncreate source ADLSGen1CSVSource2 using FileReader (\n        directory:'./Samples/AppData/',\n        WildCard:'portfolio.csv',\n        positionByEOF:false,\n        charset:'UTF-8'\n)\nparse using DSVParser (\n        header:'no'\n)\nOUTPUT TO ADLSGen1CsvStream;\n\ncreate source ADLSGen1CSVSource3 using FileReader (\n        directory:'./Samples/AppData/',\n        WildCard:'dynamicdirectory.csv',\n        positionByEOF:false,\n        charset:'UTF-8'\n)\nparse using DSVParser (\n        header:'no'\n)\nOUTPUT TO ADLSGen1CsvStream;\n\n\nCREATE SOURCE ADLSGen1OraSource1 USING OracleReader\n(\n Username:'miner',\n Password:'miner',\n ConnectionURL:'localhost:1521:xe',\n Tables:'QATEST.CUSTOMER1,QATEST.CUSTOMER2,QATEST.CUSTOMER3',\n OnlineCatalog:true,\n FetchSize:10000,\n QueueSize:2148,\n CommittedTransactions:false,\n Compression:true\n)\nOUTPUT TO ADLSGen1OrdersStream;\n\nCREATE SOURCE ADLSGen1OraSource2 USING OracleReader\n(\n Username:'miner',\n Password:'miner',\n ConnectionURL:'localhost:1521:xe',\n Tables:'QATEST.CUSTOMER4,QATEST.CUSTOMER5',\n OnlineCatalog:true,\n FetchSize:10000,\n QueueSize:2148,\n CommittedTransactions:false,\n Compression:true\n)\nOUTPUT TO ADLSGen1OrdersStream;\n\n\ncreate Target ADLSGen1_tgt1 using ADLSGen1Writer(\n        filename:'event_data.csv',\n        directory:'%@metadata(FileName)%',\n        datalakestorename:'striimdls.azuredatalakestore.net',\n        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',\n        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',\n        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',\n\t\trolloverpolicy:'interval:30s'\n)\nFORMAT USING JSONFormatter()\ninput from ADLSGen1CsvStream; \n\ncreate Target ADLSGen1_tgt2 using ADLSGen1Writer(\n        filename:'event_data.csv',\n        directory:'%@metadata(TableName)%',\n        datalakestorename:'striimdls.azuredatalakestore.net',\n        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',\n        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',\n        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',\n\t\trolloverpolicy:'filesize:10M',\n\t\tcompressiontype: 'true'\n)\nformat using DSVFormatter (\n)\ninput from ADLSGen1OrdersStream; \n\ncreate stream ADLSGen1UserdataStream of Global.WAEvent;\n\nCreate CQ ADLSGen1CQUser\ninsert into ADLSGen1UserdataStream\nselect \nputuserdata (data1,'Fileowner',data[1]) from ADLSGen1CsvStream_user data1;\n\ncreate stream ADLSGen1CSVTypedStream1 of csv_type;\ncreate stream ADLSGen1CSVTypedStream2 of csv_type;\ncreate stream ADLSGen1CSVTypedStream3 of csv_type;\n\nCREATE CQ ADLSGen1cq1\nINSERT INTO ADLSGen1CSVTypedStream1\nSELECT data[0],\ndata[1],\ndata[2]\nFROM ADLSGen1UserdataStream\nWHERE USERDATA(ADLSGen1UserdataStream,'Fileowner').toString() == 'Lorem';\n\nCREATE CQ ADLSGen1cq2\nINSERT INTO ADLSGen1CSVTypedStream2\nSELECT data[0],\ndata[1],\ndata[2]\nFROM ADLSGen1UserdataStream\nWHERE USERDATA(ADLSGen1UserdataStream,'Fileowner').toString() == 'doloremque';\n\nCREATE CQ ADLSGen1cq3\nINSERT INTO ADLSGen1CSVTypedStream3\nSELECT data[0],\ndata[1],\ndata[2]\nFROM ADLSGen1UserdataStream\nWHERE USERDATA(ADLSGen1UserdataStream,'Fileowner').toString() == 'accusantium';\n\ncreate Target ADLSGen1_tgt3 using ADLSGen1Writer(\n        filename:'event_data.csv',\n        directory:'1_%name%',\n        datalakestorename:'striimdls.azuredatalakestore.net',\n        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',\n        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',\n        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',\n\trolloverpolicy:'eventcount:74'\n)\nformat using XMLFormatter (\n  elementtuple: 'Eventname:name:id:seq:text=name',\n  formatterName: 'XMLFormatter',\n  rowdelimiter: '\\n',\n  rootelement: 'document'\n)\ninput from ADLSGen1CSVTypedStream1; \n\ncreate Target ADLSGen1_tgt4 using ADLSGen1Writer(\n        filename:'event_data.csv',\n        directory:'2_%name%',\n        datalakestorename:'striimdls.azuredatalakestore.net',\n        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',\n        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',\n        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',\n    rolloverpolicy:'eventcount:4,interval:50s'\n)\nformat using XMLFormatter (\n  elementtuple: 'Eventname:name:id:seq:text=name',\n  formatterName: 'XMLFormatter',\n  rowdelimiter: '\\n',\n  rootelement: 'document'\n)\ninput from ADLSGen1CSVTypedStream2; \n\ncreate Target ADLSGen1_tgt5 using ADLSGen1Writer(\n        filename:'event_data.csv',\n        directory:'3_%name%',\n        datalakestorename:'striimdls.azuredatalakestore.net',\n        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',\n        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',\n        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',\n    rolloverpolicy:'eventcount:12'\n)\nformat using XMLFormatter (\n  elementtuple: 'Eventname:name:id:seq:text=name',\n  formatterName: 'XMLFormatter',\n  rowdelimiter: '\\n',\n  rootelement: 'document'\n)\ninput from ADLSGen1CSVTypedStream3; \n\n\ncreate stream ADLSGen1OrderTypedStream1 of Order_type;\ncreate stream ADLSGen1OrderTypedStream2 of Order_type;\ncreate stream ADLSGen1OrderTypedStream3 of Order_type;\n\nCREATE CQ ADLSGen1cq1_db\nINSERT INTO ADLSGen1OrderTypedStream1\nSELECT data[0],\ndata[1],\ndata[2]\nFROM ADLSGen1OrdersStream\nWHERE META(ADLSGen1OrdersStream,'TableName').toString() == 'QATEST.CUSTOMER1';\n\nCREATE CQ ADLSGen1cq2_db\nINSERT INTO ADLSGen1OrderTypedStream2\nSELECT data[0],\ndata[1],\ndata[2]\nFROM ADLSGen1OrdersStream\nWHERE META(ADLSGen1OrdersStream,'TableName').toString() == 'QATEST.CUSTOMER2';\n\nCREATE CQ ADLSGen1cq3_db\nINSERT INTO ADLSGen1OrderTypedStream3\nSELECT data[0],\ndata[1],\ndata[2]\nFROM ADLSGen1OrdersStream\nWHERE META(ADLSGen1OrdersStream,'TableName').toString() == 'QATEST.CUSTOMER3';\n\n\ncreate Target ADLSGen1_tgt6 using ADLSGen1Writer(\n        filename:'event_data.csv',\n        directory:'Customer1',\n        datalakestorename:'striimdls.azuredatalakestore.net',\n        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',\n        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',\n        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',\n\trolloverpolicy:'eventcount:10000'\n)\nformat using AvroFormatter(\n  formatAs: 'Default',\n  handler: 'com.webaction.proc.AvroFormatter',\n  formatterName: 'AvroFormatter',\n  schemaFileName: '@SCHEMA@'\n)\ninput from ADLSGen1OrderTypedStream1; \n\ncreate Target ADLSGen1_tgt7 using ADLSGen1Writer(\n        filename:'event_data.csv',\n        directory:'Customer2',\n        datalakestorename:'striimdls.azuredatalakestore.net',\n        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',\n        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',\n        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',\n    rolloverpolicy:'eventcount:10000'\n)\nformat using AvroFormatter(\n  formatAs: 'Default',\n  handler: 'com.webaction.proc.AvroFormatter',\n  formatterName: 'AvroFormatter',\n  schemaFileName: '@SCHEMA@'\n)input from ADLSGen1OrderTypedStream2; \n\ncreate Target ADLSGen1_tgt8 using ADLSGen1Writer(\n        filename:'event_data.csv',\n        directory:'Customer3',\n        datalakestorename:'striimdls.azuredatalakestore.net',\n        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',\n        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',\n        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',\n    rolloverpolicy:'eventcount:10000'\n)\nformat using AvroFormatter(\n  formatAs: 'Default',\n  handler: 'com.webaction.proc.AvroFormatter',\n  formatterName: 'AvroFormatter',\n  schemaFileName: '@SCHEMA@'\n)\ninput from ADLSGen1OrderTypedStream3; \n\nend application ManyToManyADLSGen1;\n\ndeploy application ManyToManyADLSGen1;\nstart application ManyToManyADLSGen1;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE PROPERTYVARIABLE RetryPolicy='timeOut=00,retryInterval=1,maxRetries=3';\nCREATE OR REPLACE PROPERTYVARIABLE KafkaBrokerAddress='localhost:9099';\nCREATE OR REPLACE PROPERTYVARIABLE KafkaConfig='request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;';\n\nSTOP @WRITERAPPNAME@@RECOVSTATUS@;\nUNDEPLOY APPLICATION @WRITERAPPNAME@@RECOVSTATUS@;\nDROP APPLICATION @WRITERAPPNAME@@RECOVSTATUS@ CASCADE;\n\nCREATE APPLICATION @WRITERAPPNAME@@RECOVSTATUS@ @Recovery@;\ncreate flow AgentFlow;\nCREATE SOURCE S USING OracleReader\n(\n\tUsername: 'qatest',\n\tPassword: 'qatest',\n\tConnectionURL: 'dockerhost:1521:xe',\n\tTables: 'qatest.oracle_kw_test%',\n\tFetchSize: '1',\n\tconnectionRetryPolicy:'$RetryPolicy'\n)\nOUTPUT TO SS;\nend flow AgentFlow;\ncreate flow serverFlow;\n--create target sys_out using sysout (name:sysoutput) input from ss;\ncreate stream out_cq_select_SS_1 of global.waevent;\ncreate stream out_cq_select_SS_2 of global.waevent;\ncreate stream out_cq_select_SS_3 of global.waevent;\ncreate stream out_cq_select_SS_4 of global.waevent;\ncreate stream out_cq_select_SS_5 of global.waevent;\ncreate stream out_cq_select_SS_6 of global.waevent;\n\nCREATE OR REPLACE CQ cq_select_SS1 \nINSERT INTO out_cq_select_SS_1\nselect *\nfrom ss o\nwhere TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST1';\n\nCREATE OR REPLACE CQ cq_select_SS2 \nINSERT INTO out_cq_select_SS_2\nselect *\nfrom ss o\nwhere TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST2';\n\nCREATE OR REPLACE CQ cq_select_SS3 \nINSERT INTO out_cq_select_SS_3\nselect *\nfrom ss o\nwhere TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST3';\n\nCREATE OR REPLACE CQ cq_select_SS4 \nINSERT INTO out_cq_select_SS_4\nselect *\nfrom ss o\nwhere TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST4';\n\nCREATE OR REPLACE CQ cq_select_SS5 \nINSERT INTO out_cq_select_SS_5\nselect *\nfrom ss o\nwhere TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST5';\n\nCREATE OR REPLACE CQ cq_select_SS6 \nINSERT INTO out_cq_select_SS_6\nselect *\nfrom ss o\nwhere TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST6';\n\ncreate Target TARGET1 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_dsv_sync_CQ',\nParallelThreads:'2',\nPartitionKey:'@metadata(TableName)',\nMode:'sync',\nKafkaConfig: '$KafkaConfig')\nFORMAT USING dsvFormatter ()\ninput from out_cq_select_SS_1;\n\ncreate Target TARGET2 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_json_sync_CQ',\nParallelThreads:'2',\nPartitionKey:'@metadata(TableName)',\nMode:'sync',\nKafkaConfig: '$KafkaConfig')\nFORMAT USING jsonFormatter ()\ninput from out_cq_select_SS_2;\n\ncreate Target TARGET3 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_avro_sync_CQ',\nParallelThreads:'2',\nPartitionKey:'@metadata(TableName)',\nMode:'sync',\nKafkaConfig: '$KafkaConfig')\nFORMAT USING avroFormatter (\nschemaFileName:'KafkaAvroTest_sync_CQ.avsc')\ninput from out_cq_select_SS_3;\n\ncreate Target TARGET4 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_dsv_Async_CQ',\nParallelThreads:'2',\nPartitionKey:'@metadata(ROWID)',\nMode:'Async',\nKafkaConfig: '$KafkaConfig')\nFORMAT USING dsvFormatter ()\ninput from out_cq_select_SS_4;\n\ncreate Target TARGET5 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_json_Async_CQ',\nParallelThreads:'2',\nPartitionKey:'@metadata(ROWID)',\nMode:'Async',\nKafkaConfig: '$KafkaConfig')\nFORMAT USING jsonFormatter ()\ninput from out_cq_select_SS_5;\n\ncreate Target TARGET6 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_avro_Async_CQ',\nParallelThreads:'2',\nPartitionKey:'@metadata(ROWID)',\nMode:'Async',\nKafkaConfig: '$KafkaConfig')\nFORMAT USING avroFormatter (\nschemaFileName:'KafkaAvroTest_Async_CQ.avsc')\ninput from out_cq_select_SS_6;\n\ncreate Target TARGET7 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_dsv_sync',\nParallelThreads:'2',\nPartitionKey:'@metadata(TableName)',\nMode:'sync',\nKafkaConfig: '$KafkaConfig')\nFORMAT USING dsvFormatter ()\ninput from ss;\n\ncreate Target TARGET8 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_json_sync',\nParallelThreads:'2',\nPartitionKey:'@metadata(TableName)',\nMode:'sync',\nKafkaConfig: '$KafkaConfig')\nFORMAT USING jsonFormatter ()\ninput from ss;\n\ncreate Target TARGET9 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_avro_sync',\nParallelThreads:'2',\nPartitionKey:'@metadata(TableName)',\nMode:'sync',\nKafkaConfig: '$KafkaConfig')\nFORMAT USING avroFormatter (\nschemaFileName:'KafkaAvroTest_sync.avsc')\ninput from ss;\n\ncreate Target TARGET10 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_dsv_Async',\nParallelThreads:'2',\nPartitionKey:'@metadata(TableName)',\nMode:'Async',\nKafkaConfig: '$KafkaConfig')\nFORMAT USING dsvFormatter ()\ninput from ss;\n\ncreate Target TARGET11 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_json_Async',\nParallelThreads:'2',\nPartitionKey:'@metadata(TableName)',\nMode:'Async',\nKafkaConfig: '$KafkaConfig')\nFORMAT USING jsonFormatter ()\ninput from ss;\n\ncreate Target TARGET12 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_avro_Async',\nParallelThreads:'2',\nPartitionKey:'@metadata(TableName)',\nMode:'Async',\nKafkaConfig: '$KafkaConfig')\nFORMAT USING avroFormatter (\nschemaFileName:'KafkaAvroTest_Async.avsc')\ninput from ss;\n\nend flow serverFlow;\nend application @WRITERAPPNAME@@RECOVSTATUS@;\ndeploy application @WRITERAPPNAME@@RECOVSTATUS@;\nstart @WRITERAPPNAME@@RECOVSTATUS@;\n\n\n\nstop application @READERAPPNAME@@RECOVSTATUS@;\nundeploy application @READERAPPNAME@@RECOVSTATUS@;\ndrop application @READERAPPNAME@@RECOVSTATUS@ cascade;\nCREATE APPLICATION @READERAPPNAME@@RECOVSTATUS@ RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE SOURCE @SOURCE@_DSV_sync_CQ USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_dsv_sync_CQ',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream1;\n\nCREATE TARGET kafkaDumpDSV USING FileWriter(\nname:kafkaOuputDSV,\nrolloverpolicy:'filesize:500M',\nfilename:'@READERAPPNAME@@RECOVSTATUS@_@SOURCE@_dsv_sync_CQ')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaReaderStream1;\n\nCREATE SOURCE @SOURCE@_JSON_sync_CQ USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_json_sync_CQ',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO KafkaReaderStream2;\n\nCREATE SOURCE @SOURCE@_AVRO_sync_CQ USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_avro_sync_CQ',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'KafkaAvroTest_sync_CQ.avsc'\n)\nOUTPUT TO KafkaReaderStream3;\n\nCREATE SOURCE @SOURCE@_DSV_Async_CQ USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_dsv_Async_CQ',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream4;\n\nCREATE SOURCE @SOURCE@_JSON_Async_CQ USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_json_Async_CQ',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO KafkaReaderStream5;\n\nCREATE SOURCE @SOURCE@_AVRO_Async_CQ USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_avro_Async_CQ',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'KafkaAvroTest_Async_CQ.avsc'\n)\nOUTPUT TO KafkaReaderStream6;\n\nCREATE SOURCE @SOURCE@_DSV_sync USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_dsv_sync',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream7;\n\nCREATE TARGET kafkaDumpDSV_rawstream USING FileWriter(\nname:kafkaOuputDSV_rawstream,\nrolloverpolicy:'filesize:500M',\nfilename:'@READERAPPNAME@@RECOVSTATUS@_@SOURCE@_dsv_sync')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaReaderStream7;\n\nCREATE SOURCE @SOURCE@_JSON_sync USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_json_sync',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO KafkaReaderStream8;\n\nCREATE SOURCE @SOURCE@_AVRO_sync USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_avro_sync',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'KafkaAvroTest_sync.avsc'\n)\nOUTPUT TO KafkaReaderStream9;\n\nCREATE SOURCE @SOURCE@_DSV_Async USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_dsv_Async',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream10;\n\nCREATE SOURCE @SOURCE@_JSON_Async USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_json_Async',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO KafkaReaderStream11;\n\nCREATE SOURCE @SOURCE@_AVRO_Async USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_avro_Async',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'KafkaAvroTest_Async.avsc'\n)\nOUTPUT TO KafkaReaderStream12;\n\nend application @READERAPPNAME@@RECOVSTATUS@;\ndeploy application @READERAPPNAME@@RECOVSTATUS@;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\n\nstop application PCAPTester.PCAPTest;\nundeploy application PCAPTester.PCAPTest;\ndrop application PCAPTester.PCAPTest cascade;\n\nCREATE APPLICATION PCAPTest;\n\n\n    CREATE OR REPLACE TYPE PCAPData (\n      ts DateTime,\n      srcIp String,\n      srcPort String,\n      dstIp String,\n      dstPort String,\n      connection String\n    );\n    \n    CREATE OR REPLACE SOURCE PCAPSource USING PCAPReader ( \n      snaplen: '65536',\n      wildcard: false,\n      directory: '@TEST-DATA-PATH@',\n      file: 'pcapTest.pcap',\n      library: '/usr/lib/libpcap.dylib',\n      live: false \n     )\n    OUTPUT TO PCAPOut;\n    \n    CREATE OR REPLACE STREAM PCAPDataStream OF PCAPData;\n\n    CREATE OR REPLACE CQ GetPCAPData\n    INSERT INTO PCAPDataStream\n    SELECT TO_DATE(ts), \n           srcAddress,\n           TO_STRING(srcPort),\n           dstAddress,\n           TO_STRING(dstPort),\n           transportType + '/' + srcAddress + ':' + srcPort + '/' + dstAddress + ':' + dstPort\n    FROM PCAPOut;\n    \n\n\n\nEND APPLICATION PCAPTest;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Create Source @SOURCE_NAME@\n Using DatabaseReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\n Tables:'QATEST.DATE_TIME',\n --Query:\"SELECT * FROM qatest.oracle_alldatatypes\",\n FetchSize:10000,\n QueueSize:2048,\n) Output To @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSVSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'smallposdata.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\n  filename:'JsonTargetTI',\n  directory:'@FEATURE-DIR@/logs/',\n  sequence:'00',\n  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:10-89s'\n)\nformat using JSONFormatter (\n  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'\n)\n\ninput from TypedCSVStream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetJsonTIAddition_actual.log') input from TypedCSVStream;\n\nend application DSV;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop user dtest;\ndrop namespace dtest cascade;\n\ncreate user dtest identified by test;\ngrant Global.appuser to user dtest;\nconnect dtest test;\ncreate cq permissioncq select * from banker.oneWS where ( bankID = :bI);\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create Target @TARGET_NAME@ using GCSWriter(\n    bucketname:'@bucketname@',\n    objectname:'@objectname@',\n    projectId:'@project-id@',\n    uploadPolicy:'eventCount:1000',\n    ServiceAccountKey:'@file-path@'\n)\nformat using ParquetFormatter (\n\tschemaFileName:'@FILENAME@',\n\tFormatAs:'@FORMATAS@'\n)\ninput from @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@;\n\nCREATE SOURCE @kafkasrc@ USING KafkaReader VERSION @KAFKAVERSION@ (\n  brokerAddress: '',\n  Topic: '',\n  startOffset: '0' )\nPARSE USING AvroParser (\n  schemaregistryurl: 'http://localhost:8081/' )\nOUTPUT TO @appname@Stream2;\n\nCREATE TYPE @appname@ElementsOfNativeRecord1 (\n datarecord com.fasterxml.jackson.databind.JsonNode,\n before com.fasterxml.jackson.databind.JsonNode,\n metadata com.fasterxml.jackson.databind.JsonNode,\n userdata com.fasterxml.jackson.databind.JsonNode,\n datapresenceinfo com.fasterxml.jackson.databind.JsonNode,\n beforepresenceinfo com.fasterxml.jackson.databind.JsonNode);\n\nCREATE TYPE @appname@completeRecord1 (\n completedata com.fasterxml.jackson.databind.JsonNode);\n\nCREATE STREAM @appname@NativeRecordStream1 OF @appname@ElementsOfNativeRecord1;\n\nCREATE STREAM @appname@CompleteRecordInJSONStream1 OF @appname@completeRecord1;\n\nCREATE TARGET @filetarget@ USING FileWriter (\n  filename: 'kafkaout',\n  directory: ''\n  rolloverpolicy: 'EventCount:10' )\nFORMAT USING JSONFormatter  (\n  members: 'datarecord' )\nINPUT FROM @appname@NativeRecordStream1;\n\nCREATE CQ @appname@GetNativeRecordInJSONCQ1\nINSERT INTO @appname@NativeRecordStream1\nSELECT\n completedata.get(\"data\"),\n completedata.get(\"before\"),\n completedata.get(\"metadata\"),\n completedata.get(\"userdata\"),\n completedata.get(\"datapresenceinfo\"),\n completedata.get(\"beforepresenceinfo\")\nFROM @appname@CompleteRecordInJSONStream1;\n\nCREATE CQ @appname@CQ2\nINSERT INTO @appname@CompleteRecordInJSONStream1\nSELECT\n AvroToJson(y.data)\n from @appname@Stream2 y;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 41 with two sources, two jumping count windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jc5W/p -> CQ1 -> WS\n--   S2 -> Jc6W/p -> CQ2 -> WS\n--\n\nSTOP KStreamRecov41Tester.KStreamRecovTest41;\nUNDEPLOY APPLICATION KStreamRecov41Tester.KStreamRecovTest41;\nDROP APPLICATION KStreamRecov41Tester.KStreamRecovTest41 CASCADE;\nDROP USER KStreamRecov41Tester;\nDROP NAMESPACE KStreamRecov41Tester CASCADE;\nCREATE USER KStreamRecov41Tester IDENTIFIED BY KStreamRecov41Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov41Tester;\nCONNECT KStreamRecov41Tester KStreamRecov41Tester;\n\nCREATE APPLICATION KStreamRecovTest41 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP 5 ROWS\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP 6 ROWS\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nEND APPLICATION KStreamRecovTest41;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');\n\nCREATE STREAM @APPNAME@PersistStream1@RANDOM@ OF Global.waevent PERSIST USING @APPNAME@KafkaPropset;\n\nCREATE OR REPLACE SOURCE @APPNAME@_jmssrc1 USING JMSReader (\n  ProviderName: '',\n  UserName: '',\n  transactionpolicy: '',\n  Provider: '',\n  Ctx: '',\n  EnableTransaction: '',\n  QueueName: '',\n  Topic:'',\n  Password: '' )\nPARSE USING DSVParser ()\nOUTPUT TO @APPNAME@PersistStream1@RANDOM@;\n\nCREATE OR REPLACE TARGET @APPNAME@_filetrgt1 USING Global.FileWriter (\n  filename: '',\n  flushpolicy: '',\n  directory: '',\n  rolloverpolicy: '' )\nFORMAT USING Global.DSVFormatter  (\n  members: 'data' )\nINPUT FROM @APPNAME@PersistStream1@RANDOM@;\n\nCREATE STREAM @APPNAME@PersistStream2@RANDOM@ OF Global.JSONNodeEvent PERSIST USING @APPNAME@KafkaPropset;\n\nCREATE OR REPLACE SOURCE @APPNAME@_jmssrc2 USING JMSReader (\n  ProviderName: '',\n  UserName: '',\n  transactionpolicy: '',\n  Provider: '',\n  Ctx: '',\n  EnableTransaction: '',\n  QueueName: '',\n  Topic:'',\n  Password: '' )\nPARSE USING JSONParser ()\nOUTPUT TO @APPNAME@PersistStream2@RANDOM@;\n\nCREATE OR REPLACE TARGET @APPNAME@_filetrgt2 USING Global.FileWriter (\n  filename: '',\n  flushpolicy: '',\n  directory: '',\n  rolloverpolicy: '' )\nFORMAT USING Global.JSONFormatter  (\n  members: 'data' )\nINPUT FROM @APPNAME@PersistStream2@RANDOM@;\n\nCREATE STREAM @APPNAME@PersistStream3@RANDOM@ OF Global.waevent PERSIST USING @APPNAME@KafkaPropset;\n\nCREATE OR REPLACE SOURCE @APPNAME@_jmssrc3 USING JMSReader (\n  ProviderName: '',\n  UserName: '',\n  transactionpolicy: '',\n  Provider: '',\n  Ctx: '',\n  EnableTransaction: '',\n  QueueName: '',\n  Topic:'',\n  Password: '' )\nPARSE USING DSVParser ()\nOUTPUT TO @APPNAME@PersistStream3@RANDOM@;\n\nCREATE OR REPLACE TARGET @APPNAME@_filetrgt3 USING Global.FileWriter (\n  filename: '',\n  flushpolicy: '',\n  directory: '',\n  rolloverpolicy: ''  )\nFORMAT USING Global.JSONFormatter  (\n  members: 'data' )\nINPUT FROM @APPNAME@PersistStream3@RANDOM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ on all in default;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop IR;\nundeploy application IR;\ndrop application IR cascade;\nCREATE APPLICATION IR recovery 5 second interval;\n\n CREATE OR REPLACE SOURCE TS USING IncrementalBatchReader  ( \n  FetchSize: 100,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.test01;striim.test02;striim.test03',\n  adapterName: 'IncrementalBatchReader',\n CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',\n  PollingInterval: '5sec'\n  )\n  OUTPUT TO data_stream ;\n\n  CREATE OR REPLACE TARGET sys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM data_stream;\n\ncreate target T using AzureSQLDWHWriter(\n\t\tConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        accountname: 'striimqatestdonotdelete',\n        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',\n        uploadpolicy:'eventcount:1,interval:10s'\n) INPUT FROM data_stream;\n\n\nCREATE TARGET log USING LOGWRITER\n(\n name:log,\n filename:'LOGFILENAME'\n)\nINPUT FROM data_stream;\n\n\nEND APPLICATION IR;\n\ndeploy application IR;\nstart IR;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application logminer;\nundeploy application logminer;\ndrop application logminer cascade;\n\ncreate application logminer;\n\nCreate Source Rac11g Using OracleReader\n(\n --StartTimestamp:'15-JAN-2015 13:00:40',\n Username:'miner',\n Password:'miner',\n ConnectionURL:'10.1.110.128:1521:orcl',\n --Tables:'SCOTT.SIMPLETEST',\n Tables:'QATEST.SAMPLETEST2',\n OnlineCatalog:true,\n FetchSize:1,\n QueueSize:2148,\n CommittedTransactions:false,\n Compression:false,\n FilterTransactionState:false\n)\nOutput To LCRStream;\n\ncreate target myout using sysout(name: logminer) input from LCRStream;\ncreate Target y using logwriter(name:GitCommitInfo,filename:logminer) input from LCRStream;\n\nend application logminer;\ndeploy application logminer;\nstart application logminer;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@\n Using OracleReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'qatest.@SourceTable@',\n _h_returnDateTimeAs: 'ZonedDateTime',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop @APPNAME@_App;\nundeploy application @APPNAME@_App;\ndrop application @APPNAME@_App cascade;\n\nCREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;\nCREATE SOURCE @APPNAME@_Src USING Global.OracleReader(\n  FetchSize:'1',\n  Username:'@SOURCE_USER@',\n  Password:'@SOURCE_PASSWORD@',\n  ConnectionURL:'@SOURCE_URL@',\n  Tables:'@SOURCE_TABLES@',\n  _h_useClassic:'true',\n  ConnectionRetryPolicy:'@AUTO_CONNECTION_RETRY@'\n)\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE TARGET @APPNAME@_SYS USING SysOut (\n  name: '@APPNAME@_SYS' )\nINPUT FROM @APPNAME@_Stream;\n\nEnd APPLICATION @APPNAME@_App;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION  @AppName@ RECOVERY 5 SECOND INTERVAL;\nCREATE OR REPLACE SOURCE  @AppName@_Source USING FileReader (\ndirectory:'@dataDir@',\nwildcard:'JsonNodeEvent.json',\npositionByEOF:false\n)\nPARSE USING Global.JSONParser (\n )  OUTPUT TO  @AppName@_rawstream;\n\n\nCREATE CQ @BuiltinFunc@CQ\nINSERT INTO  @BuiltinFunc@_Stream\nSELECT @BuiltinFunc@(x, 'Sno', data.get(\"_id\"), 'Name', data.get(\"firstname\"))\nFROM @AppName@_rawstream x;\n\nCREATE OR REPLACE TARGET  @AppName@_FileTarget USING Global.FileWriter (\n  flushpolicy: 'EventCount:10000,Interval:30s',\n  adapterName: 'FileWriter',\n directory: '@logs@',\n  filename: '@BuiltinFunc@_JsonNodeEventData',\n  rolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING Global.JSONFormatter  (\n  handler: 'com.webaction.proc.JSONFormatter',\n  jsonMemberDelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true',\n  formatterName: 'JSONFormatter',\n  jsonobjectdelimiter: '\\n' )\nINPUT FROM @BuiltinFunc@_Stream;\n\nEnd application  @AppName@;\nDeploy application  @AppName@;\nStart application  @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP OuterJoinTester.OJApp;\nUNDEPLOY APPLICATION OuterJoinTester.OJApp;\nDROP APPLICATION OuterJoinTester.OJApp cascade;\n\nCREATE APPLICATION OJApp;\n\n\nCREATE source wsSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'banks.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:No,\n  trimquote:false\n) OUTPUT TO QaStream;\n\n\nCREATE source wsSource2 USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'bankCards.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:No,\n  trimquote:false\n) OUTPUT TO stream2;\n\n\n\nCREATE TYPE bankData\n(\nbankID Integer KEY,\nbankName String\n);\n\nCREATE TYPE cardData\n(\ncardID Integer KEY,\ncardName String\n);\n\n\nCREATE STREAM wsStream OF bankData;\nCREATE STREAM wsStream2 OF cardData;\n\n\n--Select data from QaStream and insert into wsStream\n\nCREATE CQ csvTobankData\nINSERT INTO wsStream\nSELECT TO_INT(data[0]), data[1] FROM QaStream;\n\nCREATE CQ csvTobankData2\nINSERT INTO wsStream2\nSELECT TO_INT(data[0]), data[1] FROM stream2;\n\nCREATE JUMPING WINDOW win1 OVER wsStream KEEP 20 rows;\n\n\nCREATE JUMPING WINDOW win2 OVER wsStream2 KEEP 4 rows;\n\nEND APPLICATION OJApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet\n(\n  ConnectionURL: '@CONNECTION_URL@',\n  CommittedTransactions: true,\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OJet',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  Username: '@SOURCE_USER@'\n)OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)\nSELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[2]) = 'Null' ;\n\nCREATE TYPE LogType(\nNum_col String key,\nChar_col String,\nVarchar2_col String,\nlong_col String,\nTable String,\nOperation String\n);\n\nCREATE WINDOW CDCWindow\nOVER @STREAM@\nKEEP 1 ROWS;\n\nCREATE WACTIONSTORE CDCWS CONTEXT of LogType\nEVENT TYPES ( LogType )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ ToWactionStore\nINSERT INTO CDCWS\nSELECT * FROM CDCWindow\nLINK SOURCE EVENT;\n\nCREATE TARGET @SOURCE_NAME@_SYS USING SysOut (\n  name: '@SOURCE_NAME@_SYS' )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCreate Source @SourceName@ Using Ojet\n(\nUsername:'@OJET-UNAME@',\nPassword:'@OJET-PASSWORD@',\nConnectionURL:'@OCI-URL@',\nTables:'@SourceTable@',\n)\nOutput To @SRCINPUTSTREAM@;\nCREATE OR REPLACE STREAM Rstream1 OF Global.WAEvent;\nCREATE OR REPLACE OPEN PROCESSOR Open_Processor1 USING PartialRecordPolicy\n(\nUsername: 'qatest_ojet',\nPassword: 'qatest_ojet',\nConnectionURL: 'jdbc:oracle:oci:@//localhost:1525/orcl',\nTables: 'QATEST.Ojetsrc(ROWIDCOL)'\n)\nINSERT INTO Rstream1\nFROM @SRCINPUTSTREAM@;\nCREATE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\nUsername:'@UN@',\nPassword:'@PWD@',\nBatchPolicy:'EventCount:1,Interval:1',\nTables: '@Tablemapping@'\n)INPUT FROM Rstream1;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 65\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ1 -> JWc10 -> CQ(aggregate) -> WS1\n-- S -> CQ2 -> JWc11-> CQ(aggregate) -> WS2\n--\n\nSTOP Recov65Tester.RecovTest65;\nUNDEPLOY APPLICATION Recov65Tester.RecovTest65;\nDROP APPLICATION Recov65Tester.RecovTest65 CASCADE;\nCREATE APPLICATION RecovTest65 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE WactionType (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionType;\n\nCREATE CQ CsvToWaction\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStreamSize10\nOVER DataStream KEEP 10 ROWS;\n\nCREATE JUMPING WINDOW DataStreamSize11\nOVER DataStream KEEP 11 ROWS;\n\nCREATE WACTIONSTORE Wactions1 CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions2 CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions1\nINSERT INTO Wactions1\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.merchantId),\n    TO_DATE(FIRST(p.dateTime)),\n    TO_DOUBLE(FIRST(p.amount)),\n    FIRST(p.city)\nFROM DataStreamSize10 p;\n\nCREATE CQ InsertWactions2\nINSERT INTO Wactions2\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.merchantId),\n    TO_DATE(FIRST(p.dateTime)),\n    TO_DOUBLE(FIRST(p.amount)),\n    FIRST(p.city)\nFROM DataStreamSize11 p;\n\ncreate Target t1 using logwriter(name:Foo1, filename: output1) input from DataStreamSize10;\ncreate Target t2 using logwriter(name:Foo2, filename: output2) input from DataStreamSize11;\n\nEND APPLICATION RecovTest65;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Canon Test W20\n-- Nicholas Keene, WebAction, Inc.\n--\n-- Basic test for an unpartitioned sliding attribute window\n--\n-- S -> SWa5u -> CQ -> WS\n--\n\n\nUNDEPLOY APPLICATION NameW20.W20;\nDROP APPLICATION NameW20.W20 CASCADE;\nCREATE APPLICATION W20 RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE FLOW DataAcquisitionW20;\n\nCREATE SOURCE CsvSourceW20 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'Canon1000.csv',\n  columndelimiter:',',\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStreamW20;\n\nEND FLOW DataAcquisitionW20;\n\n\n\nCREATE FLOW DataProcessingW20;\n\nCREATE TYPE DataTypeW20 (\n    eventID String,\n    word String KEY,\n    dateTime DateTime\n);\n\nCREATE STREAM DataStreamW20 OF DataTypeW20;\n\nCREATE CQ CSVStreamW20_to_DataStreamW20\nINSERT INTO DataStreamW20\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[2],'yyyyMMddHHmmss')\nFROM CsvStreamW20;\n\nCREATE WINDOW SWa5uW20\nOVER DataStreamW20\nKEEP WITHIN 5 SECOND ON dateTime;\n\nCREATE WACTIONSTORE WactionStoreW20 CONTEXT OF DataTypeW20\nEVENT TYPES ( DataTypeW20 KEY(word) )\n@PERSIST-TYPE@\n\nCREATE CQ SWa5uW20_to_WactionStoreW20\nINSERT INTO WactionStoreW20\nSELECT\n    FIRST(eventID),\n    FIRST(word),\n    FIRST(dateTime)\nFROM SWa5uW20;\n\nEND FLOW DataProcessingW20;\n\n\n\nEND APPLICATION W20;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nCREATE source CsvDataSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'smallposdata10rows.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\n\n@@FEATURE-DIR@/tql/TQLwithinTQL3.tql;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@1;\nundeploy application @APPNAME@1;\nstop application @APPNAME@2;\nundeploy application @APPNAME@2;\n\nDROP STREAM @APPNAME@_STREAM;\nDROP APPLICATION @APPNAME@1 CASCADE;\nDROP APPLICATION @APPNAME@2 CASCADE;\n\ndrop propertyset OrcToOrcPlatfm_App_KafkaPropset;\ndrop stream  OrcToOrcPlatfm_Stream CASCADE;\n\n\n--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\n--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;\n\nCREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';\ncreate application @APPNAME@1 recovery 5 SECOND INTERVAL;\n\ncreate or replace stream @STREAM@2 of Global.waevent;\n\nCREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNEC4TION_URL@',\n  Tables: '$table1',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_AGENTFLOW1;\n\nCREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@TABLENAME@2',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_SERVERFLOW1;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@1 USING AzureSQLDWHWriter  (\n\t\tConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',\n\t\tuploadpolicy:'eventcount:10,interval:30s'\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;\nCREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x\n WHERE META(x,'TableName').toString() == 'QATEST.ORCTOORCPLATFM_SOURCE4';\n\nCREATE OR REPLACE TARGET @TARGET_NAME@2 USING AzureSQLDWHWriter  (\n\t\tConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',\n\t\tuploadpolicy:'eventcount:10,interval:30s'\n )\nINPUT FROM @STREAM@2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(\n  name: 'CDDL',\n  filename:'WAEventDump.log'\n)INPUT FROM @STREAM@;\n\nCREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;\n\nEND APPLICATION @APPNAME@1;\n\n\n\nCREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';\nCREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;\n\ncreate or replace stream @STREAM@3 of Global.waevent;\n\nCREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '$table2',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\n\nEND FLOW @APPNAME@_AGENTFLOW2;\n\nCREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@TABLENAME@4',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_SERVERFLOW2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@3 USING AzureSQLDWHWriter  (\n\t\tConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',\n\t\tuploadpolicy:'eventcount:10,interval:30s'\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;\nCREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y\n WHERE META(y,'TableName').toString() == 'QATEST.ORCTOORCPLATFM_SOURCE2';\n\nCREATE OR REPLACE TARGET @TARGET_NAME@4 USING AzureSQLDWHWriter  (\n\t\tConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',\n\t\tuploadpolicy:'eventcount:10,interval:30s'\n )\nINPUT FROM @STREAM@3;\n\nCREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;\n\nEND APPLICATION @APPNAME@2;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@ recovery 5 second iNTERVAL;\ncreate source @SOURCE@ using FileReader (\n        directory:'./Samples/AppData/',\n        WildCard:'dynamicdirectory.csv',\n        positionByEOF:false,\n        charset:'UTF-8'\n)\nparse using DSVParser (\n        header:'no'\n)\nOUTPUT TO @STREAM@;\n\nCreate Type CSVType_@APPNAME@ (\n  id String,\n  name String,\n  department String,\n  yoj String,\n  moj String,\n  doj int\n);\n\nCreate Stream Typed@STREAM@ of CSVType_@APPNAME@;\n\nCREATE CQ CsvToPosData@APPNAME@\nINSERT INTO Typed@STREAM@\nSELECT data[0],\n       data[1],\n       data[2],\n       data[3],\n       data[4],\n       TO_INT(data[5])\nFROM @STREAM@;\n\ncreate Target @TARGET@ using ADLSGen2Writer(\n    accountname:'',\n\tsastoken:'',\n\tfilesystemname:'',\n\tfilename:'',\n\tdirectory:'',\n\tuploadpolicy:'eventcount:5'\n\n)format using DSVFormatter (\n)\ninput from Typed@STREAM@;\n\nend application @APPNAME@;\n\ndeploy application @APPNAME@;\nstart application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application GCSWriterTest;\nundeploy application GCSWriterTest;\ndrop application GCSWriterTest cascade;\ncreate application GCSWriterTest recovery 1 second interval;\ncreate source GS Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'false'\n)\nOUTPUT TO CsvStream;\n\ncreate Target T using GCSWriter(\n    bucketname:'@bucketname@',\n    objectname:'@objectname@',\n    projectId:'@project-id@',\n    uploadPolicy:'@UPLOAD-SIZE@',\n    ServiceAccountKey:'@file-path@'\n)\nformat using DSVFormatter (\nmembers:'data'\n)\ninput from CsvStream;\n\nend application GCSWriterTest;\ndeploy application GCSWriterTest on all in default;\nstart application GCSWriterTest;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@;\n\nCREATE OR REPLACE SOURCE @parquetsrc@ USING Global.HDFSReader (\n  wildcard: '',\n  directory: '',\n  hadoopurl: '',\n  hadoopconfigurationpath: '',\n  positionbyeof: false )\n  PARSE USING ParquetParser (\n   )\nOUTPUT TO @appname@Stream;\n\nCREATE CQ @appname@CQ\nINSERT INTO @appname@CqOut\n    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;\n\nCREATE OR REPLACE TARGET @filetarget@ USING FileWriter (\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:1,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING AvroFormatter  (\nschemaFileName: 'AvroFileSchema'\n)\nINPUT FROM @appname@CqOut;\n\ncreate Target @s3target@ using S3Writer(\n    bucketname:'',\n    objectname:'',\n    uploadpolicy:'',\n    foldername:''\n)\nformat using AvroFormatter (\nschemaFileName: 'AvroS3Schema'\n)\ninput from @appname@CqOut;\n\ncreate Target @blobtarget@ using AzureBlobWriter(\n\taccountname:'',\n\taccountaccesskey:'',\n\tcontainername:'',\n    blobname:'',\n\tfoldername:'',\n\tuploadpolicy:'EventCount:10,interval:5s'\n)\nformat using AvroFormatter (\nschemaFileName: 'AvroAzureSchema'\n)\nINPUT FROM @appname@CqOut;\n\nCREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (\n    bucketname:'',\n    objectname:'',\n    foldername:'',\n    projectId:'',\n    uploadPolicy:''\n)\nformat using AvroFormatter (\nschemaFileName: 'AvroGCSSchema'\n)\nINPUT FROM @appname@CqOut;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCreate Source @SourceName@ USING Ojet\n(\n Username:'@OJET-UNAME@',\nPassword:'@OJET-PASSWORD@',\nConnectionURL:'@OCI-URL@',\nTables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectURL@',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application MSSQLTransactionSupportAutoDisableCdcTrue;\nundeploy application MSSQLTransactionSupportAutoDisableCdcTrue;\ndrop application MSSQLTransactionSupportAutoDisableCdcTrue cascade;\n\nCREATE APPLICATION MSSQLTransactionSupportAutoDisableCdcTrue recovery 1 second interval;\n\nCreate Source ReadFromMSSQL3\nUsing MSSqlReader\n(\nUsername:'@READER-NAME@',\nPassword:'@READER-PASSWORD@',\nDatabaseName:'@SRCDB-NAME@',\nConnectionURL:'@CONN-URL@',\nTables:'@WATABLES-SRC@',\nTransactionSupport: 'true',\nAutoDisableTableCDC:'true',\nFetchTransactionMetadata:'true',\nFilterTransactionBoundaries: false,\nCompression:'false',\nConnectionPoolSize:1\n)\nOutput To MSSQLTransactionSupportAutoDisableCdcTrueStream;\n\n\nCREATE TARGET WriteToMSSQL3 USING DatabaseWriter(\nConnectionURL:'@TGT-URL@',\nUsername:'@WRITER-UNAME@',\nPassword:'@WRITER-PASSWORD@',\nBatchPolicy:'EventCount:5,Interval:5',\nCommitPolicy:'EventCount:1,Interval:1',\nTables: '@WATABLES-SRC,@@WATABLES-TGT@'\n)\nINPUT FROM MSSQLTransactionSupportAutoDisableCdcTrueStream;\n\nCREATE TARGET MSSqlReaderOutput3 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportAutoDisableCdcTrueStream; \n\n\nCREATE OR REPLACE TARGET MSSQLFileOut3 USING FileWriter  ( \n  rolloverpolicy: 'EventCount:10000,Interval:30s',\n  adapterName: 'FileWriter',\n  directory:'@FEATURE-DIR@/logs/',\n  filename: 'TransactionSupportAutoDisableTableCdcTrue.txt'\n ) \nFORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true',\n  jsonobjectdelimiter: '\\n'\n ) \nINPUT FROM MSSQLTransactionSupportAutoDisableCdcTrueStream;\n\nEND APPLICATION MSSQLTransactionSupportAutoDisableCdcTrue;\ndeploy application MSSQLTransactionSupportAutoDisableCdcTrue;\nstart application MSSQLTransactionSupportAutoDisableCdcTrue;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "-- The PosAppAgent sample application demonstrates how a credit card\n-- payment processor might use Striim to generate reports on current\n-- transaction activity by merchant and send alerts when transaction\n-- counts for a merchant are higher or lower than average for the time\n-- of day.\n\n\nCREATE APPLICATION PosAppAgent;\n\n-- All CREATE statements between here and the END APPLICATION\n-- statement will create objects in the PosAppAgent application.\n\n-- source CsvAgentDataSource\n\nCREATE FLOW AgentFlow;\n\nCREATE source CsvAgentDataSource USING FileReader (\n  directory: 'C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData',\n  wildcard:'posdata.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n) OUTPUT TO CsvAgentStream;\n\nEND FLOW AgentFlow;\n\n-- This is the primary data source for this application.\n-- In a real-world application, it would be real-time data. Here,\n-- the data comes from a comma-delimited file, posdata.csv. The first\n-- two lines of that file are:\n--\n-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY\n-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand\n--\n-- The \"header:Yes\" setting tells Striim that the first line contains\n-- field labels that should not be treated as data.\n--\n-- The \"positionByEOF:false\" setting tells Striim to start reading\n-- from the beginning of the file. (In a real-world application\n-- reading real log files, you would typically use the default \"true\"\n-- setting so that the application would read only new data.)\n--\n-- The OUTPUT TO clause automatically creates the stream\n-- CsvAgentStream using the WAEvent type associated with the CSVReader\n-- adapater. The only field from WAEvent used by this application\n-- is \"data\", an array containing the delimited fields.\n\n\n-- CQ CsvAgentToPosDataCq\n\nCREATE FLOW ProcessFlow;\n\nCREATE CQ CsvAgentToPosDataCq\nINSERT INTO PosDataAgentStream partition by merchantId\nSELECT TO_STRING(data[1]) as merchantId,\n       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,\n       TO_DOUBLE(data[7]) as amount,\n       TO_STRING(data[9]) as zip\nFROM CsvAgentStream;\n\n-- Here, \"data\" refers to the array mentioned above, and the number\n-- in brackets specifies a field from the array, counting from zero.\n-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH\n-- AMOUNT, and data[9] is ZIP.\n--\n-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as\n-- the types to be used in PosDataAgentStream, which is created automatically.\n\n-- The DATETIME field from the source is converted to both a dateTime\n-- value, used as the event timestamp by the application, and (via the\n-- function) an integer hourValue, which is used to look up\n-- historical hourly averages from the HourlyAgentAveLookup cache,\n-- discussed below.\n--\n-- The other six fields are discarded. Thus the first line of data\n-- from posdata.csv has at this point been reduced to five values:\n--\n-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)\n-- 20130312173210 (DateTime)\n-- 17 (hourValue)\n-- 2.20 (amount)\n-- 41363 (zip)\n\n\n-- CQ GenerateMerchantTxRateAgentOnly\n--\n-- The AgentPosData5Minutes window bounds the data so that the query can\n-- use the COUNT, FIRST, and SUM functions and join data from the\n-- HourlyAgentAveLookup cache. (Aggregate functions cannot be used and\n-- joins cannot be performed on unbound real-time data.)\n--\n-- The HourlyAgentAveLookup cache provides historical average sales\n-- amounts for the current hour for each merchant.\n\nCREATE JUMPING WINDOW AgentPosData5Minutes\nOVER PosDataAgentStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;\n\nCREATE TYPE MerchantAgentHourlyAve(\n  merchantId String,\n  hourValue integer,\n  hourlyAve integer\n);\nCREATE CACHE HourlyAgentAveLookup using FileReader (\n  directory: 'C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData',\n  wildcard: 'hourlyData.txt'\n)\nPARSE USING DSVParser (\n  header: Yes,\n  trimquote:false,\n  trimwhitespace:true\n) QUERY (keytomap:'merchantId') OF MerchantAgentHourlyAve;\n\nCREATE TYPE MerchantTxRateAgent(\n  merchantId String KEY,\n  zip String,\n  startTime DateTime,\n  count integer,\n  totalAmount double,\n  hourlyAve integer,\n  upperLimit double,\n  lowerLimit double,\n  category String,\n  status String\n);\nCREATE STREAM MerchantTxRateAgentOnlyStream OF MerchantTxRateAgent PARTITION BY merchantId;\n\nCREATE CQ GenerateMerchantTxRateAgentOnly\nINSERT INTO MerchantTxRateAgentOnlyStream\nSELECT p.merchantId,\n       FIRST(p.zip),\n       FIRST(p.dateTime),\n       COUNT(p.merchantId),\n       SUM(p.amount),\n       FIRST(l.hourlyAve/12),\n       FIRST(l.hourlyAve/12 * CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       FIRST(l.hourlyAve/12 / CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       '<NOTSET>',\n       '<NOTSET>'\nFROM AgentPosData5Minutes p, HourlyAgentAveLookup l\nWHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue\nGROUP BY p.merchantId;\n\n-- This query aggregates five minutes' worth of data for each\n-- merchant, calculating the total transaction count and amount, and\n-- calculates the upperLimit and lowerLimit values based on the\n-- historical average transaction count for the current hour of the\n-- day from the HourlyAgentAveLookup cache. The category and status fields\n-- are left unset to be populated by the next query.\n\n\n-- CQ GenerateMerchantTxRateAgentWithStatus\n--\n-- This query sets the count values used by the Dashboard map and the\n-- status values used to trigger alerts.\n\nCREATE STREAM MerchantTxRateAgentWithStatusStream OF MerchantTxRateAgent;\n\nCREATE CQ GenerateMerchantTxRateAgentWithStatus\nINSERT INTO MerchantTxRateAgentWithStatusStream\nSELECT merchantId,\n       zip,\n       startTime,\n       count,\n       totalAmount,\n       hourlyAve,\n       upperLimit,\n       lowerLimit,\n       CASE\n         WHEN count >10000 THEN 'HOT'\n         WHEN count > 800 THEN 'WARM'\n         WHEN count >200 THEN 'COOL'\n         ELSE 'COLD' END,\n       CASE\n         WHEN count > upperLimit THEN 'TOOHIGH'\n         WHEN count < lowerLimit THEN 'TOOLOW'\n         ELSE 'OK' END\nFROM MerchantTxRateAgentOnlyStream;\n\n\n-- WAction store MerchantActivityAgent\n--\n-- The following group of statements create and populate the MerchantActivityAgent\n-- WAction store. Data from the MerchantTxRateAgentWithStatusStream is enhanced\n-- with merchant details from NameLookupAgent cache and with latitude and longitude\n-- values from the USAddressDataAgent cache.\n\nCREATE TYPE MerchantActivityAgentContext(\n  MerchantId String KEY,\n  StartTime  DateTime,\n  CompanyName String,\n  Category String,\n  Status String,\n  Count integer,\n  HourlyAve integer,\n  UpperLimit double,\n  LowerLimit double,\n  Zip String,\n  City String,\n  State String,\n  LatVal double,\n  LongVal double\n);\n\nCREATE WACTIONSTORE MerchantActivityAgent CONTEXT OF MerchantActivityAgentContext\nEVENT TYPES ( MerchantTxRateAgent )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\n\nCREATE TYPE MerchantAgentNameData(\n  merchantId String KEY,\n  companyName String\n);\n\nCREATE TYPE USAddressDataAgent(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\nCREATE CACHE NameLookupAgent using FileReader (\n  directory:'C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData',\n  WildCard:'MerchantNames.csv',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:'yes',\n  trimquote:false,\n  trimwhitespace:true\n)\nQUERY(keytomap:'merchantId') OF MerchantAgentNameData;\n\nCREATE CACHE ZipLookupAgent using FileReader (\n  directory: 'C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData',\n  wildcard: 'USAddresses.txt',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header: Yes,\n  columndelimiter: '\\t',\n  trimquote:false,\n  trimwhitespace:true\n) QUERY (keytomap:'zip') OF USAddressDataAgent;\n\n\nCREATE CQ GenerateWactionAgentContext\nINSERT INTO MerchantActivityAgent\nSELECT  m.merchantId,\n        m.startTime,\n        n.companyName,\n        m.category,\n        m.status,\n        m.count,\n        m.hourlyAve,\n        m.upperLimit,\n        m.lowerLimit,\n        m.zip,\n        z.city,\n        z.state,\n        z.latVal,\n        z.longVal\nFROM MerchantTxRateAgentWithStatusStream m, NameLookupAgent n, ZipLookupAgent z\nWHERE m.merchantId = n.merchantId AND m.zip = z.zip\nLINK SOURCE EVENT;\n\n-- CQ GenerateAgentAlerts\n--\n-- This CQ sends an alert when a merchant's status value changes to\n-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.\n\n\nCREATE STREAM AlertAgentStream OF Global.AlertEvent;\n\nCREATE CQ GenerateAgentAlerts\nINSERT INTO AlertAgentStream\nSELECT n.CompanyName,\n       m.MerchantId,\n       CASE\n         WHEN m.Status = 'OK' THEN 'info'\n         ELSE 'warning' END,\n       CASE\n         WHEN m.Status = 'OK' THEN 'cancel'\n         ELSE 'raise' END,\n       CASE\n         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)\n         ELSE ''\n         END\nFROM MerchantTxRateAgentWithStatusStream m, NameLookupAgent n\nWHERE m.merchantId = n.merchantId;\n\nCREATE SUBSCRIPTION AgentAlertSub USING WebAlertAdapter( ) INPUT FROM AlertAgentStream;\n\nEND FLOW ProcessFlow;\n\nEND APPLICATION PosAppAgent;\n\nDEPLOY APPLICATION PosAppAgent with AgentFlow in AGENTS, ProcessFlow in default;\n\n-- CREATE DASHBOARD USING \"C:/Users/Administrator/Desktop/AutoInstallDirectory/Installer/Striim/Samples/PosApp/appData/PosAppDashboard.json\";\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\nCREATE APPLICATION @APPNAME@ WITH ENCRYPTION @Recovery@ USE EXCEPTIONSTORE;\nCREATE SOURCE @APPNAME@_s USING FileReader(\n  directory:'Samples/AppData',\n  wildcard:'PO.JSON',\n  positionByEOF:false\n)\nparse using JSONParser (\n) OUTPUT TO @APPNAME@_ss1;\n\n--Using JSONNew() to create empty json node (j1)\nCREATE CQ @APPNAME@_cq1 INSERT INTO @APPNAME@_ss2 SELECT JSONNew() j1,* FROM @APPNAME@_ss1 ;\n--Using JSONSet() to set value to the empty json node (j1) and naming it as (j2)\nCREATE CQ @APPNAME@_cq2 INSERT INTO @APPNAME@_ss3 SELECT JSONSet(j1, \"Mission\",\"Test\") j2,* FROM @APPNAME@_ss2 ;\n--Using JSONFrom() to create new json node (j3) using string\nCREATE CQ @APPNAME@_cq3 INSERT INTO @APPNAME@_ss4 SELECT JSONFrom('{\"PONumber\":1600,\"Reference\":\"ABULL-20140421\",\"Requestor\":\"Alexis Bull\",\"User\":\"ABULL\",\"CostCenter\":\"A50\",\"ShippingInstructions\":{\"name\":\"Alexis Bull\",\"Address\":{\"street\":\"200 Sporting Green\",\"city\":\"South San Francisco\",\"state\":\"CA\",\"zipCode\":99236,\"country\":\"United States of America\"},\"Phone\":[{\"type\":\"Office\",\"number\":\"909-555-7307\"},{\"type\":\"Mobile\",\"number\":\"415-555-1234\"}]},\"Special Instructions\":null,\"AllowPartialShipment\":false,\"LineItems\":[{\"ItemNumber\":1,\"Part\":{\"Description\":\"One Magic Christmas\",\"UnitPrice\":19.95,\"UPCCode\":13131092899},\"Quantity\":9,\"Barcodes\":[{\"type\":\"Home\",\"number\":\"9-555-7307\"},{\"type\":\"Office\",\"number\":\"9-555-1234\"}]},{\"ItemNumber\":2,\"Part\":{\"Description\":\"Lethal Weapon\",\"UnitPrice\":19.95,\"UPCCode\":85391628927},\"Quantity\":5,\"Barcodes\":[{\"type\":\"Home\",\"number\":\"10-555-7307\"},{\"type\":\"Office\",\"number\":\"10-555-1234\"}]}]}')j3,* FROM @APPNAME@_ss3 ;\n--Using JSONSet() to set new element to already existing node (data) - naming it as j4\nCREATE CQ @APPNAME@_cq4 INSERT INTO @APPNAME@_ss5 SELECT JSONSet(data, \"FileName\",\"Dummy\") j4,* FROM @APPNAME@_ss4 ;\n--Using JSONArrayAdd() to Add element to existing JsonArray - naming it as j5\nCREATE CQ @APPNAME@_cq5 INSERT INTO @APPNAME@_ss6 SELECT JSONArrayAdd(data.get(\"ShippingInstructions\").get(\"Phone\"),\"work\") j5,* FROM @APPNAME@_ss5 ;\n--Using JSONArrayInsert() to Add element to existing JsonArray by specifying Index - naming it as j6\nCREATE CQ @APPNAME@_cq6 INSERT INTO @APPNAME@_ss7 SELECT JSONArrayInsert(data.get(\"ShippingInstructions\").get(\"Phone\"),1,\"residence\") j6,* FROM @APPNAME@_ss6 ;\n--Using JSONRemove() to Remove element from existing Json node 'data' - naming it as j7\nCREATE CQ @APPNAME@_cq7 INSERT INTO @APPNAME@_ss8 SELECT JSONRemove(data,\"Requestor\") j7,* FROM @APPNAME@_ss7 ;\n--Using JSONSetAll() to Add collection of objects (from metadata) to existing Json node 'data' - naming it as j8\nCREATE CQ @APPNAME@_cq8 INSERT INTO @APPNAME@_ss9 SELECT JSONSetAll(j4, metadata) j8,* FROM @APPNAME@_ss8;\n--Using JSONSetAll() in empty json node j1 - naming it as j9\nCREATE CQ @APPNAME@_cq9 INSERT INTO @APPNAME@_ss10 SELECT JSONSetAll(j1, metadata) j9,* FROM @APPNAME@_ss9;\n--Using JSONSetAll() with simple json node j2 - naming it as j10\nCREATE CQ @APPNAME@_cq10 INSERT INTO @APPNAME@_ss11 SELECT JSONSetAll(j2, metadata) j10,* FROM @APPNAME@_ss10;\n--Using JSONSetAll() with json node which has the same field/values to verify overriding\nCREATE CQ @APPNAME@_cq11 INSERT INTO @APPNAME@_ss12 SELECT JSONSetAll(j10, metadata) j11,* FROM @APPNAME@_ss11;\ncreate target @APPNAME@_t using sysout (name:ss1) input from @APPNAME@_ss12;\n\nCREATE Stream @APPNAME@_str1 (\n     PODetails com.fasterxml.jackson.databind.JsonNode,\n     Phone com.fasterxml.jackson.databind.JsonNode,\n     LineItems com.fasterxml.jackson.databind.JsonNode,\n     BarCodes com.fasterxml.jackson.databind.JsonNode,\n     Addr com.fasterxml.jackson.databind.JsonNode\n);\n\nCREATE CQ @APPNAME@_cq12\nINSERT into @APPNAME@_str1\n    select\n    j8,\n    j8.get('ShippingInstructions').get('Phone'),\n    j8.get('LineItems'),\n    j8.get('LineItems').get('Barcodes'),\n    j8.get('ShippingInstructions').get('Address')\nfrom @APPNAME@_ss12;\n\nCREATE Stream @APPNAME@_str2(\n     PONumber Integer,\n     Reference String,\n     Usr String,\n     CostCenter String,\n     Name String,\n     street String,\n     city String,\n     state String,\n     zipCode Integer,\n     country String,\n     officephone String,\n     mobilephone String,\n     SplInstruction String,\n     AllowPartialShipment boolean,\n     PhoneType String,\n     PhoneNo String,\n     ItemNumber Integer,\n     ItemDesc String,\n     UnitProce Double,\n     UPCCode Double,\n     Quantity Integer,\n     BarcodeType String,\n     BarcodeNo String\n);\n\nCREATE CQ @APPNAME@_cq13\nINSERT into @APPNAME@_str2\nSELECT\n\n    /* PO Details */\n    JSONGetInteger(x.PODetails,\"PONumber\"),\n    JSONGetString(x.PODetails,\"Reference\"),\n    JSONGetString(x.PODetails,\"User\"),\n    JSONGetString(x.PODetails,\"CostCenter\"),\n\n    /* Shipping Details */\n    JSONGetString(x.PODetails.get('ShippingInstructions'),\"name\"),\n    JSONGetString(x.Addr,\"street\"),\n    JSONGetString(x.Addr,\"city\"),\n    JSONGetString(x.Addr,\"state\"),\n    JSONGetInteger(x.Addr,\"zipCode\"),\n    JSONGetString(x.Addr,\"country\"),\n\n    /* Phone nos by array position*/\n    JSONGetString(x.Phone.get(0),\"number\"),\n    JSONGetString(x.Phone.get(1),\"number\"),\n\n    /*Others*/\n    JSONGetString(x.PODetails,\"Special Instructions\"),\n    JSONGetboolean(x.PODetails,\"AllowPartialShipment\"),\n\n    /* Phone nos using iterator*/\n    JSONGetString(pho,\"type\"),\n    JSONGetString(pho,\"number\"),\n\n    /*Item Specific one */\n    JSONGetInteger(Items,\"ItemNumber\"),\n    JSONGetString(Items.get('Part'),\"Description\"),\n    JSONGetDouble(Items.get('Part'),\"UnitPrice\"),\n    JSONGetDouble(Items.get('Part'),\"UPCCode\"),\n    JSONGetInteger(Items,\"Quantity\"),\n\n    /*Barcode specific values */\n    JSONGetString(Barcode,\"type\"),\n    JSONGetString(Barcode,\"number\")\n\nfrom @APPNAME@_str1 x, iterator(x.LineItems) Items, iterator(x.Phone) pho, iterator(Items.Barcodes) Barcode;\n\ncreate Target @APPNAME@_JSONNew_T using FileWriter (\nfilename:'@APPNAME@_JSONNew_RT',\ndirectory:'@LOGDIR@',\nrolloverpolicy:'eventcount:5000000,interval:1s'\n)\nformat using jsonFormatter(members:j1)\ninput from @APPNAME@_ss2;\n\ncreate Target @APPNAME@_JSONSet1_T using FileWriter (\nfilename:'@APPNAME@_JSONSet1_RT',\ndirectory:'@LOGDIR@',\nrolloverpolicy:'eventcount:5000000,interval:1s'\n)\nformat using jsonFormatter(members:j2)\ninput from @APPNAME@_ss3;\n\ncreate Target @APPNAME@_JSONFrom_T using FileWriter (\nfilename:'@APPNAME@_JSONFrom_RT',\ndirectory:'@LOGDIR@',\nrolloverpolicy:'eventcount:5000000,interval:1s'\n)\nformat using jsonFormatter(members:j3)\ninput from @APPNAME@_ss4;\n\ncreate Target @APPNAME@_JSONSet2_T using FileWriter (\nfilename:'@APPNAME@_JSONSet2_RT',\ndirectory:'@LOGDIR@',\nrolloverpolicy:'eventcount:5000000,interval:1s'\n)\nformat using jsonFormatter(members:j4)\ninput from @APPNAME@_ss5;\n\ncreate Target @APPNAME@_JSONArrayAdd_T using FileWriter (\nfilename:'@APPNAME@_JSONArrayAdd_RT',\ndirectory:'@LOGDIR@',\nrolloverpolicy:'eventcount:5000000,interval:1s'\n)\nformat using jsonFormatter(members:j5)\ninput from @APPNAME@_ss6;\n\ncreate Target @APPNAME@_JSONArrayInsert_T using FileWriter (\nfilename:'@APPNAME@_JSONArrayInsert_RT',\ndirectory:'@LOGDIR@',\nrolloverpolicy:'eventcount:5000000,interval:1s'\n)\nformat using jsonFormatter(members:j6)\ninput from @APPNAME@_ss7;\n\ncreate Target @APPNAME@_JSONRemove_T using FileWriter (\nfilename:'@APPNAME@_JSONRemove_RT',\ndirectory:'@LOGDIR@',\nrolloverpolicy:'eventcount:5000000,interval:1s'\n)\nformat using jsonFormatter(members:j7)\ninput from @APPNAME@_ss8;\n\ncreate Target @APPNAME@_JSONSetAll1_T using FileWriter (\nfilename:'@APPNAME@_JSONSetAll1_RT',\ndirectory:'@LOGDIR@',\nrolloverpolicy:'eventcount:5000000,interval:1s'\n)\nformat using jsonFormatter(members:j8)\ninput from @APPNAME@_ss9;\n\ncreate Target @APPNAME@_JSONSetAll2_T using FileWriter (\nfilename:'@APPNAME@_JSONSetAll2_RT',\ndirectory:'@LOGDIR@',\nrolloverpolicy:'eventcount:5000000,interval:1s'\n)\nformat using jsonFormatter(members:j9)\ninput from @APPNAME@_ss10;\n\ncreate Target @APPNAME@_JSONSetAll3_T using FileWriter (\nfilename:'@APPNAME@_JSONSetAll3_RT',\ndirectory:'@LOGDIR@',\nrolloverpolicy:'eventcount:5000000,interval:1s'\n)\nformat using jsonFormatter(members:j10)\ninput from @APPNAME@_ss11;\n\ncreate Target @APPNAME@_JSONSetAll4_T using FileWriter (\nfilename:'@APPNAME@_JSONSetAll4_RT',\ndirectory:'@LOGDIR@',\nrolloverpolicy:'eventcount:5000000,interval:1s'\n)\nformat using jsonFormatter(members:j11)\ninput from @APPNAME@_ss12;\n\ncreate Target @APPNAME@_PO_T using FileWriter (\nfilename:'@APPNAME@_PO_RT',\ndirectory:'@LOGDIR@',\nrolloverpolicy:'eventcount:5000000'\n)\nformat using dsvFormatter()\ninput from @APPNAME@_str2;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION SystemTimeTester.SystemTimeWindows;\nUNDEPLOY APPLICATION SystemTimeTester.SystemTimeWindows;\nDROP APPLICATION SystemTimeTester.SystemTimeWindows cascade;\n\nCREATE APPLICATION SystemTimeWindows;\n\nCREATE TYPE RandomData(\nbankNumber int KEY,\nbankName String\n);\n\n\nCREATE SOURCE ranDataSource using StreamReader(\nOutputType: 'SystemTimeTester.RandomData',\nnoLimit: 'false',\nisSeeded: 'true',\nmaxRows: 0,\niterations: 30,\niterationDelay: 1000,\nStringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',\nNumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]R'\n)OUTPUT TO CSVDataStream;\n\nCREATE STREAM RandomDataStream OF RandomData;\n\nCREATE CQ ParseRandomData\nINSERT INTO RandomDataStream\nSELECT (data[0]), data[1]\nFROM CSVDataStream;\n\nCREATE @WINDOWTYPE@ WINDOW tierone OVER RandomDataStream keep within 20 second;\n\nCREATE STREAM onetwostream OF RandomData;\n\nCREATE CQ onetwocq\nINSERT INTO onetwostream\nSELECT bankNumber,bankName\nFROM tierone\nwhere bankName ='bofa'\norder by bankNumber;\n\nCREATE @WINDOWTYPE@ WINDOW tiertwo OVER onetwostream keep within 40 second;\n\nCREATE STREAM twothreestream OF RandomData;\n\nCREATE CQ twothreecq\nINSERT INTO twothreestream\nSELECT bankNumber,bankName\nFROM tierTwo\nwhere bankName ='bofa'\norder by bankNumber;\n\nCREATE @WINDOWTYPE@ WINDOW tierthree OVER twothreestream keep within 1 minute;\n\nCREATE WACTIONSTORE MyDataActivity\nCONTEXT OF RandomData\nEVENT TYPES(RandomData )\n@PERSIST-TYPE@\n\nCreate CQ TrackMyDataActivity\nINSERT INTO MyDataActivity\nSELECT bankNumber,bankName from @FROMSTREAM@\nwhere bankName ='bofa'\norder by bankNumber\nLINK SOURCE EVENT;\n\n\nEND APPLICATION SystemTimeWindows;\ndeploy application SystemTimeWindows;\nstart application SystemTimeWindows;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 6 with sliding window and partitioned feature\n-- Nicholas Keene, Bert Hashemi WebAction, Inc.\n--\n-- S -> CQ -> SW(partitioned) -> CQ(no aggregate) -> WS\n--\n\nSTOP KStreamRecov6Tester.KStreamRecovTest6;\nUNDEPLOY APPLICATION KStreamRecov6Tester.KStreamRecovTest6;\nDROP APPLICATION KStreamRecov6Tester.KStreamRecovTest6 CASCADE;\nDROP USER KStreamRecov6Tester;\nDROP NAMESPACE KStreamRecov6Tester CASCADE;\nCREATE USER KStreamRecov6Tester IDENTIFIED BY KStreamRecov6Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov6Tester;\nCONNECT KStreamRecov6Tester KStreamRecov6Tester;\n\nCREATE APPLICATION KStreamRecovTest6 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream OF CsvData PARTITION BY merchantId;\n\nCREATE CQ CsvToData\nINSERT INTO DataStream\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream;\n\nCREATE WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND APPLICATION KStreamRecovTest6;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE CCBReader USING FileReader (\n  wildcard: '@WILDCARD@',\n  positionbyeof: false,\n  directory: '@TESTDIR@'\n  )\nPARSE USING Global.CobolCopybookParser (\n  copybookDialect: 'Mainframe',\n  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',\n  GroupPolicy: '@GRPPOLICY@',\n  TimeoutPolicy: '1s',\n  dataFileOrganization: 'FixedLength',\n  ProcessCopyBookFileAs: 'MultipleEvents',\n  skipIndent: 0,\n  dataFileFont: 'utf8',\n  CopybookFileFormat: 'USE_STANDARD_COLUMNS',\n  copybookSplit: 'None',\n  copybookFileName: '@CCBFILE@'\n   )\nOUTPUT TO CCBStream;\n\nCREATE OR REPLACE TARGET JSONWriter USING FileWriter (\n  filename: '%@metadata(FileName)%',\n  directory: '@DIR@',\n  rolloverpolicy: 'EventCount:10000,Interval:30s',\n  flushpolicy: 'EventCount:1,Interval:30s'\n  )\nFORMAT USING JSONFormatter()\nINPUT FROM CCBStream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@ on all in default;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop @APPNAME@_App;\nundeploy application @APPNAME@_App;\ndrop application @APPNAME@_App cascade;\n\nCREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;\nCREATE SOURCE @APPNAME@_Source USING Global.Ojet\n\n(\n  Username:'@SOURCE_USER@',\n  Password:'@SOURCE_PASSWORD@',\n  ConnectionURL:'@SOURCE_URL@',\n  Tables:'@SOURCE_TABLES@'\n)\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE TARGET @APPNAME@_SYS USING SysOut (\n  name: '@APPNAME@_SYS' )\nINPUT FROM @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_CDB_Target USING DatabaseWriter\n\n(\n  Username:'@TARGET_CDB_USER@',\n  ConnectionURL:'@TARGET_CDB_URL@',\n  Tables:'@TARGET_CDB_TABLES@',\n  Password:'@TARGET_CDB_PASSWORD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n  CommitPolicy:'EventCount:1,Interval:1'\n)\nINPUT FROM @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_PDB_Target USING DatabaseWriter\n\n(\n  Username:'@TARGET_PDB_USER@',\n  ConnectionURL:'@TARGET_PDB_URL@',\n  Tables:'@TARGET_PDB_TABLES@',\n  Password:'@TARGET_PDB_PASSWORD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n  CommitPolicy:'EventCount:1,Interval:1'\n)\nINPUT FROM @APPNAME@_Stream;\n\nEnd APPLICATION @APPNAME@_App;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@;\n\nCREATE SOURCE @parquetsrc@ USING FileReader (\nwildcard: '',\ndirectory: '',\npositionbyeof: false )\nPARSE USING ParquetParser (\n)\nOUTPUT TO @appname@Stream;\n\nCREATE CQ @appname@CQ\nINSERT INTO @appname@CqOut\n    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;\n\nCREATE OR REPLACE TARGET @filetarget@ USING FileWriter (\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:1,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING ParquetFormatter  (\nschemaFileName: ''\n)\nINPUT FROM @appname@CqOut;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@;\n\nCREATE SOURCE @parquetsrc@ USING FILEReader (\n    wildcard: '',\n    directory: '',\n    positionbyeof: false\n )\nPARSE USING ParquetParser (\n)\nOUTPUT TO @appname@Stream;\n\nCREATE CQ @appname@CQ\nINSERT INTO @appname@CqOut\n    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;\n\nCREATE OR REPLACE TARGET @filetarget@ USING FileWriter (\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:10,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING ParquetFormatter  (\nschemaFileName: ''\n)\nINPUT FROM @appname@CqOut;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@ recovery 1 second interval;\n\ncreate source @APPNAME@_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using CobolCopybookParser (\ncopybookFileName : '@TD@/@PROP1@',\n  dataFileFont: '@PROP2@',\n  copybookSplit: '@PROP3@',\n  dataFileOrganization: '@PROP4@',\n  copybookDialect: '@PROP5@', \n  skipIndent:'@PROP6@',\n  DatahandlingScheme:'@PROP7@'\n  --recordSelector: '@PROP8@'\n)\nOUTPUT TO @APPNAME@Stream;\n\ncreate Target @APPNAME@Target using FileWriter(\n    filename :'@FILE@',\n    directory : '@FOLDER@'\n)\nformat using JsonFormatter (\n)\ninput from @APPNAME@Stream;\n\n/*\ncreate Target @APPNAME@DBTarget using DatabaseWriter(\n  Username: 'qatest',\n  Password: 'qatest',\n  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',\n  BatchPolicy: 'EventCount:1,Interval:5',\n  CommitPolicy: 'EventCount:1,Interval:5',\n  Tables: 'QATEST.@table@'\n)\ninput from @APPNAME@Stream;*/\nend application @APPNAME@;\ndeploy application @APPNAME@ on all in default;\nstart application @APPNAME@;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ Recovery 5 second interval;\n--create application @APPNAME@;\n\n--CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaProps(zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11');\n--CREATE STREAM @APPNAME@kperststream of Global.WAEvent PERSIST USING @APPNAME@KafkaProps;\n\ncreate type @APPNAME@employee\n(\nid integer,\nname String,\ncompany String\n);\nCREATE STREAM @APPNAME@Hana_TypedStream of @APPNAME@employee;\n\nCREATE OR REPLACE SOURCE @APPNAME@OnPrem_Oracle USING OracleReader  (\n  Compression: false,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  TransactionBufferDiskLocation: '.striim/LargeBuffer',\n  FetchSize: 1,\n  DDLCaptureMode: 'All',\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'localhost:1521:xe',\n Tables: 'QATEST.EMP%',\n-- Tables: 'QATEST.OracToCql_alldatatypes',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  TransactionBufferType: 'Memory',\n  DictionaryMode: 'OnlineCatalog',\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  TransactionBufferSpilloverSize: '1MB'\n )\nOUTPUT TO @APPNAME@kperststream ;\n\ncreate stream @APPNAME@UserdataStream1 of Global.WAEvent;\n\nCreate CQ @APPNAME@CQUser\ninsert into @APPNAME@UserdataStream1\nselect putuserdata (data1,'OperationName',META(data1,'OperationName').toString()) from @APPNAME@kperststream data1;\n\nCreate CQ @APPNAME@CQUser_typed\ninsert into @APPNAME@Hana_TypedStream\nselect \nto_int(data[0]),\ndata[1],\ndata[2]\nfrom @APPNAME@UserdataStream1 u \nwhere USERDATA(u,'OperationName').toString()=='INSERT' and meta(u,'TableName').toString()=\"QATEST.EMP3\";\n\n\nCREATE OR REPLACE TARGET @APPNAME@DBTarget USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'SYSTEM',\n  Password_encrypted: 'false',\n  --ParallelThreads: '4',\n  BatchPolicy: 'EventCount:1000,Interval:60',\n  CommitPolicy: 'EventCount:100,Interval:60',\n  --ExcludedTables: 'QATEST.EMP2;QATEST.EMP3',\n  ConnectionURL: 'jdbc:sap://10.77.21.116:39013/?databaseName=striim&currentSchema=QA',\n  Tables: 'QA.EMP3',\n  adapterName: 'DatabaseWriter',\n  Password: 'XgsL2qpACEIHrXXh4SueCg=='\n ) \nINPUT FROM @APPNAME@Hana_TypedStream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCreate Source @SourceName@ Using Ojet\n(\n Username:'@OJET-UNAME@',\n Password:'@OJET-PASSWORD@',\n ConnectionURL:'@OCI-URL@',\n Tables:'@SourceTable@'\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application JMSWriter.JMS;\nundeploy application JMSWriter.JMS;\ndrop application JMSWriter.JMS cascade;\n\ncreate application JMS;\ncreate source JMSCSVSource using FileReader (\n        directory:'@TEST-DATA-PATH@',\n        WildCard:'AdhocQueryData2.csv',\n        positionByEOF:false,\n        charset:'UTF-8'\n)\nparse using DSVParser (\n        header:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target JmsTarget  using JMSWriter (\n\t\tProvider:'@PROVIDER@',\n\t\tCtx:'@CONTEXT@',\n\t\tmessagetype: @MESSAGETYPE@,\n\t\tUserName:'@USERNAME@',\n\t\tPassword:'@PASSWORD@',\n\t\tQueuename:'dynamicQueues/Test.bar')\nformat using dsvformatter (\n)\ninput from TypedCSVStream;\n\nend Application Jms;\ndeploy application jms;\n\nstart jms;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 4\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS\n-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS\n--\n\nSTOP KStreamRecov4Tester.KStreamRecovTest4;\nUNDEPLOY APPLICATION KStreamRecov4Tester.KStreamRecovTest4;\nDROP APPLICATION KStreamRecov4Tester.KStreamRecovTest4 CASCADE;\nDROP USER KStreamRecov4Tester;\nDROP NAMESPACE KStreamRecov4Tester CASCADE;\nCREATE USER KStreamRecov4Tester IDENTIFIED BY KStreamRecov4Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov4Tester;\nCONNECT KStreamRecov4Tester KStreamRecov4Tester;\n\nCREATE APPLICATION KStreamRecovTest4 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE TYPE CsvData (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\n\nCREATE STREAM DataStream OF CsvData;\n\nCREATE CQ CsvToData\nINSERT INTO DataStream\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream6Minutes p;\n\nEND APPLICATION KStreamRecovTest4;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME1@;\nundeploy application @APPNAME1@;\nstop application @APPNAME2@;\nundeploy application @APPNAME2@;\ndrop application @APPNAME1@ cascade;\ndrop application @APPNAME2@ cascade;\n\n\nCREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\nCREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;\nCREATE OR REPLACE SOURCE @SourceName@ Using PostgreSQLReader\n(\n   adapterName: PostgreSQLReader,\n   CDDLAction: Quiesce_Cascade,\n   CDDLCapture: true,\n   CDDLTrackingTable:'striim.ddlcapturetable',\n   ConnectionURL: jdbc:postgresql://localhost:5432/qatest,\n   FilterTransactionBoundaries: true,\n   Password: w@ct10n,\n   ReplicationSlotName:'test_slot',\n   Tables: public.PGMultiDownstream_src,\n   Username: sa,\n)OUTPUT TO @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME1@;\nDEPLOY APPLICATION @APPNAME1@;\nSTART APPLICATION @APPNAME1@;\n\nCREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  \n(\n  DatabaseProviderType:'Default',\n  CheckPointTable:'CHKPOINT',\n  PreserveSourceTransactionBoundary:'false',\n  Username:'qatest',\n  BatchPolicy:'EventCount:1',\n  CommitPolicy:'EventCount:1',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',\n  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',\n  Password:'qatest',\n  CDDLAction:'Process'\n) \nINPUT FROM @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME2@;\nDEPLOY APPLICATION @APPNAME2@;\nSTART APPLICATION @APPNAME2@;\n\nCREATE APPLICATION @APPNAME3@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE TARGET @targetName1@ USING DatabaseWriter  \n(\n  DatabaseProviderType:'Default',\n  CheckPointTable:'CHKPOINT',\n  PreserveSourceTransactionBoundary:'false',\n  Username:'qatest',\n  BatchPolicy:'EventCount:1',\n  CommitPolicy:'EventCount:1',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',\n  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',\n  Password:'qatest',\n  CDDLAction:'Process'\n) \nINPUT FROM @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME3@;\nDEPLOY APPLICATION @APPNAME3@;\nSTART APPLICATION @APPNAME3@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "-- The PosAppAgent sample application demonstrates how a credit card\n-- payment processor might use Striim to generate reports on current\n-- transaction activity by merchant and send alerts when transaction\n-- counts for a merchant are higher or lower than average for the time\n-- of day.\n\n\nCREATE APPLICATION PosAppAgent;\n\n-- All CREATE statements between here and the END APPLICATION\n-- statement will create objects in the PosAppAgent application.\n\n-- source CsvAgentDataSource\n\nCREATE FLOW AgentFlow;\n\nCREATE source CsvAgentDataSource USING FileReader (\n  directory:'/opt/striim/Samples/PosApp/appData',\n  wildcard:'posdata.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n) OUTPUT TO CsvAgentStream;\n\nEND FLOW AgentFlow;\n\n-- This is the primary data source for this application.\n-- In a real-world application, it would be real-time data. Here,\n-- the data comes from a comma-delimited file, posdata.csv. The first\n-- two lines of that file are:\n--\n-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY\n-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand\n--\n-- The \"header:Yes\" setting tells Striim that the first line contains\n-- field labels that should not be treated as data.\n--\n-- The \"positionByEOF:false\" setting tells Striim to start reading\n-- from the beginning of the file. (In a real-world application \n-- reading real log files, you would typically use the default \"true\"\n-- setting so that the application would read only new data.)\n--\n-- The OUTPUT TO clause automatically creates the stream\n-- CsvAgentStream using the WAEvent type associated with the CSVReader\n-- adapater. The only field from WAEvent used by this application\n-- is \"data\", an array containing the delimited fields.\n\n\n-- CQ CsvAgentToPosDataCq\n\nCREATE FLOW ProcessFlow;\n\nCREATE CQ CsvAgentToPosDataCq\nINSERT INTO PosDataAgentStream partition by merchantId\nSELECT TO_STRING(data[1]) as merchantId,\n       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,\n       TO_DOUBLE(data[7]) as amount,\n       TO_STRING(data[9]) as zip\nFROM CsvAgentStream;\n\n-- Here, \"data\" refers to the array mentioned above, and the number\n-- in brackets specifies a field from the array, counting from zero.\n-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH\n-- AMOUNT, and data[9] is ZIP.\n--\n-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as \n-- the types to be used in PosDataAgentStream, which is created automatically.\n\n-- The DATETIME field from the source is converted to both a dateTime \n-- value, used as the event timestamp by the application, and (via the \n-- function) an integer hourValue, which is used to look up \n-- historical hourly averages from the HourlyAgentAveLookup cache, \n-- discussed below.\n--\n-- The other six fields are discarded. Thus the first line of data\n-- from posdata.csv has at this point been reduced to five values:\n--\n-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)\n-- 20130312173210 (DateTime)\n-- 17 (hourValue)\n-- 2.20 (amount)\n-- 41363 (zip)\n\n\n-- CQ GenerateMerchantTxRateAgentOnly\n--\n-- The AgentPosData5Minutes window bounds the data so that the query can\n-- use the COUNT, FIRST, and SUM functions and join data from the\n-- HourlyAgentAveLookup cache. (Aggregate functions cannot be used and \n-- joins cannot be performed on unbound real-time data.)\n--\n-- The HourlyAgentAveLookup cache provides historical average sales\n-- amounts for the current hour for each merchant.\n\nCREATE JUMPING WINDOW AgentPosData5Minutes\nOVER PosDataAgentStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;\n\nCREATE TYPE MerchantAgentHourlyAve(\n  merchantId String,\n  hourValue integer,\n  hourlyAve integer\n);\nCREATE CACHE HourlyAgentAveLookup using FileReader (\n  directory: 'Samples/PosApp/appData',\n  wildcard: 'hourlyData.txt'\n)\nPARSE USING DSVParser (\n  header: Yes,\n  trimquote:false,\n  trimwhitespace:true\n) QUERY (keytomap:'merchantId') OF MerchantAgentHourlyAve;\n\nCREATE TYPE MerchantTxRateAgent(\n  merchantId String KEY,\n  zip String,\n  startTime DateTime,\n  count integer,\n  totalAmount double,\n  hourlyAve integer,\n  upperLimit double,\n  lowerLimit double,\n  category String,\n  status String\n);\nCREATE STREAM MerchantTxRateAgentOnlyStream OF MerchantTxRateAgent PARTITION BY merchantId;\n\nCREATE CQ GenerateMerchantTxRateAgentOnly\nINSERT INTO MerchantTxRateAgentOnlyStream\nSELECT p.merchantId,\n       FIRST(p.zip),\n       FIRST(p.dateTime),\n       COUNT(p.merchantId),\n       SUM(p.amount),\n       FIRST(l.hourlyAve/12),\n       FIRST(l.hourlyAve/12 * CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       FIRST(l.hourlyAve/12 / CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       '<NOTSET>',\n       '<NOTSET>'\nFROM AgentPosData5Minutes p, HourlyAgentAveLookup l\nWHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue\nGROUP BY p.merchantId;\n\n-- This query aggregates five minutes' worth of data for each\n-- merchant, calculating the total transaction count and amount, and\n-- calculates the upperLimit and lowerLimit values based on the\n-- historical average transaction count for the current hour of the\n-- day from the HourlyAgentAveLookup cache. The category and status fields\n-- are left unset to be populated by the next query.\n\n\n-- CQ GenerateMerchantTxRateAgentWithStatus\n--\n-- This query sets the count values used by the Dashboard map and the\n-- status values used to trigger alerts.\n\nCREATE STREAM MerchantTxRateAgentWithStatusStream OF MerchantTxRateAgent;\n\nCREATE CQ GenerateMerchantTxRateAgentWithStatus\nINSERT INTO MerchantTxRateAgentWithStatusStream\nSELECT merchantId,\n       zip,\n       startTime,\n       count,\n       totalAmount,\n       hourlyAve,\n       upperLimit,\n       lowerLimit,\n       CASE\n         WHEN count >10000 THEN 'HOT'\n         WHEN count > 800 THEN 'WARM'\n         WHEN count >200 THEN 'COOL'\n         ELSE 'COLD' END,\n       CASE\n         WHEN count > upperLimit THEN 'TOOHIGH'\n         WHEN count < lowerLimit THEN 'TOOLOW'\n         ELSE 'OK' END\nFROM MerchantTxRateAgentOnlyStream;\n\n\n-- WAction store MerchantActivityAgent\n--\n-- The following group of statements create and populate the MerchantActivityAgent\n-- WAction store. Data from the MerchantTxRateAgentWithStatusStream is enhanced\n-- with merchant details from NameLookupAgent cache and with latitude and longitude\n-- values from the USAddressDataAgent cache.\n\nCREATE TYPE MerchantActivityAgentContext(\n  MerchantId String KEY,\n  StartTime  DateTime,\n  CompanyName String,\n  Category String,\n  Status String,\n  Count integer,\n  HourlyAve integer,\n  UpperLimit double,\n  LowerLimit double,\n  Zip String,\n  City String,\n  State String,\n  LatVal double,\n  LongVal double\n);\n\nCREATE WACTIONSTORE MerchantActivityAgent CONTEXT OF MerchantActivityAgentContext\nEVENT TYPES ( MerchantTxRateAgent )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\n\nCREATE TYPE MerchantAgentNameData(\n  merchantId String KEY,\n  companyName String\n);\n\nCREATE TYPE USAddressDataAgent(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\nCREATE CACHE NameLookupAgent using FileReader (\n  directory:'Samples/PosApp/appData',\n  WildCard:'MerchantNames.csv',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:'yes',\n  trimquote:false,\n  trimwhitespace:true\n) \nQUERY(keytomap:'merchantId') OF MerchantAgentNameData;\n\nCREATE CACHE ZipLookupAgent using FileReader (\n  directory: 'Samples/PosApp/appData',\n  wildcard: 'USAddresses.txt',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header: Yes,\n  columndelimiter: '\\t',\n  trimquote:false,\n  trimwhitespace:true\n) QUERY (keytomap:'zip') OF USAddressDataAgent;\n\n\nCREATE CQ GenerateWactionAgentContext\nINSERT INTO MerchantActivityAgent\nSELECT  m.merchantId,\n        m.startTime,\n        n.companyName,\n        m.category,\n        m.status,\n        m.count,\n        m.hourlyAve,\n        m.upperLimit,\n        m.lowerLimit,\n        m.zip,\n        z.city,\n        z.state,\n        z.latVal,\n        z.longVal\nFROM MerchantTxRateAgentWithStatusStream m, NameLookupAgent n, ZipLookupAgent z\nWHERE m.merchantId = n.merchantId AND m.zip = z.zip\nLINK SOURCE EVENT;\n\n-- CQ GenerateAgentAlerts\n--\n-- This CQ sends an alert when a merchant's status value changes to\n-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.\n\n\nCREATE STREAM AlertAgentStream OF Global.AlertEvent;\n\nCREATE CQ GenerateAgentAlerts\nINSERT INTO AlertAgentStream\nSELECT n.CompanyName,\n       m.MerchantId,\n       CASE\n         WHEN m.Status = 'OK' THEN 'info'\n         ELSE 'warning' END,\n       CASE\n         WHEN m.Status = 'OK' THEN 'cancel'\n         ELSE 'raise' END,\n       CASE\n         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)\n         ELSE ''\n         END\nFROM MerchantTxRateAgentWithStatusStream m, NameLookupAgent n\nWHERE m.merchantId = n.merchantId;\n\nCREATE SUBSCRIPTION AgentAlertSub USING WebAlertAdapter( ) INPUT FROM AlertAgentStream;\n\nEND FLOW ProcessFlow;\n\nEND APPLICATION PosAppAgent;\n\nDEPLOY APPLICATION PosAppAgent with AgentFlow in AGENTS, ProcessFlow in default;\n\n-- CREATE DASHBOARD USING \"Samples/PosApp/PosAppDashboard.json\";\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use PosTester;\nalter application PosApp;\n\nCREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext\nEVENT TYPES ( MerchantTxRate )\n@PERSIST-TYPE@\n\nend application PosApp;\n\nalter application PosApp recompile;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');\n\nCREATE FLOW @APPNAME@AgentFlow;\n\nCREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.waevent PERSIST USING @APPNAME@KafkaPropset;\n\nCREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (\n  ProviderName: '',\n  UserName: '',\n  transactionpolicy: '',\n  Provider: '',\n  Ctx: '',\n  EnableTransaction: '',\n  QueueName: '',\n  Password: '' )\nPARSE USING DSVParser ()\nOUTPUT TO @APPNAME@PersistStream@RANDOM@;\nEND FLOW @APPNAME@AgentFlow;\n\nCREATE FLOW @APPNAME@serverFlow;\nCREATE OR REPLACE TARGET @APPNAME@_filetrgt USING FileWriter (\n  filename: '',\n  flushpolicy: '',\n  directory: '',\n  rolloverpolicy: '' )\nFORMAT USING JSONFormatter  (\n  members: 'data' )\nINPUT FROM @APPNAME@PersistStream@RANDOM@;\nEND FLOW @APPNAME@serverFlow;\n\nEND APPLICATION @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop Application zoneDateTime_CaseSensistive_CQ;\nundeploy application zoneDateTime_CaseSensistive_CQ;\ndrop application zoneDateTime_CaseSensistive_CQ cascade;\n\ncreate application zoneDateTime_CaseSensistive_CQ;\n\nCREATE OR REPLACE SOURCE csvinput USING FileReader  ( \n  blocksize: 64,\n  positionbyeof: false,\n  rolloverstyle: 'Default',\n  includesubdirectories: false,\n  adapterName: 'FileReader',\n  directory: '@ResourceDirectory@',\n  skipbom: true,\n  wildcard: 'ZonedDateTime_data.csv'\n ) \n PARSE USING DSVParser  ( \n  _h_ReturnDateTimeAs: 'ZonedDateTime'\n ) \nOUTPUT TO csvinput_out ;\n\nCREATE  TYPE cq_zonedtime_out_Type  ( \na java.lang.Integer , \nb string,\nc string,\nd string,\ne string,\nf string\n--b java.time.ZonedDateTime , \n--c java.time.ZonedDateTime ,\n--d java.time.ZonedDateTime , \n--e java.time.ZonedDateTime ,\n--f java.time.ZonedDateTime   \n );\n\nCREATE STREAM cq_zonedtime_out OF cq_zonedtime_out_Type;\n\nCREATE OR REPLACE   CQ cq_zonedtime \nINSERT INTO cq_zonedtime_out\nSELECT TO_INT(data[0]) as a,\nTO_STRING(TO_ZONEDDATETIME(data[1],\"dd-MMM-yy hh.mm.ss.SSSSSSSSS a\"),\"dd-MMM-yy hh.mm.ss.SSSSSSSSS a\") as b,\nTO_STRING(TO_ZONEDDATETIME(data[2], \"dd-MMM-yy hh.mm.ss.SSSSSSSSS a z\"),\"dd-MMM-yy hh.mm.ss.SSSSSSSSS a z\") as c,\nTO_STRING(TO_ZONEDDATETIME(data[3], \"EEEEE dd-MMM-yy hh.mm.ss.SSSSSSSSS a\"),\"EEEEE dd-MMM-yy hh.mm.ss.SSSSSSSSS a\") as d,\nTO_STRING(TO_ZONEDDATETIME(data[4], \"E dd-MMM-yy hh.mm.ss.SSSSSSSSS a z\"),\"E dd-MMM-yy hh.mm.ss.SSSSSSSSS a z\") as e,\nTO_STRING(TO_ZONEDDATETIME(data[5], \"EEEE dd-MMMM-yy hh.mm.ss.SSSSSSSSS a z\"),\"EEEE dd-MMMM-yy hh.mm.ss.SSSSSSSSS a z\") as f\nFROM csvinput_out c;\n\ncreate or replace target sample_out using Sysout(\n  name:'Foo'\n)INPUT FROM cq_zonedtime_out;\n\ncreate Target File_tgt using FileWriter(\n  filename:'ZonedDateTime_ActualOutput.csv',\n  directory:'@OutputDirectory@',\n  rolloverpolicy:'eventcount:1000'\n)\nformat using DSVFormatter (\n\n)\ninput from cq_zonedtime_out;\n\nEND APPLICATION zoneDateTime_CaseSensistive_CQ;\n\ndeploy application zoneDateTime_CaseSensistive_CQ;\n\nstart application zoneDateTime_CaseSensistive_CQ;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop MySQLToAzure;\nundeploy application MySQLToAzure;\nDROP APPLICATION MySQLToAzure CASCADE;\nCREATE APPLICATION MySQLToAzure recovery 5 second interval;;\n\nCreate Source MySQLSOURCE Using MySQLReader\n(\n  Username: '@MYSQL-USERNAME@',\n  Password: '@MYSQL-PASSWORD@',\n  ConnectionURL: '@MYSQL-URL@',\n  Database:'@MYSQL-DATABASE@',\n  Tables: '@SOURCE_TABLES@'\n) \nOutput To str;\n\n\nCREATE  TARGET t4 USING SysOut  ( \n  name: 'sqltors'\n ) \nINPUT FROM str;\n\ncreate target MySQLAzureTarget using AzureSQLDWHWriter (\n\t\tConnectionURL: '@SQLDW-URL@',\n        username: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@',\n        uploadpolicy:'@EVENT-COUNT@'\n) INPUT FROM str;\n\nEND APPLICATION MySQLToAzure;\ndeploy application MySQLToAzure;\nstart application MySQLToAzure;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Create Source @SOURCE_NAME@ Using MSSqlReader\n(\n Username:'@READER-UNAME@',\n Password:'@READER-PASSWORD@',\n DatabaseName:'qatest',\n ConnectionURL:'@CDC-READER-URL@',\n Tables:@WATABLES@,\n ConnectionPoolSize:2,\n Compression:false,\n StartPosition:'EOF'\n)\nOutput To @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION EH;\nUNDEPLOY APPLICATION EH;\nDROP APPLICATION EH CASCADE;\nCREATE APPLICATION EH @Recovery@;\nCREATE SOURCE s USING FileReader (\n\tdirectory:'Product/IntegrationTests/TestData/',\n    WildCard:'posdata.csv',\n\tpositionByEOF:false\n\t)\nPARSE USING DSVParser (\n\theader:yes\n)OUTPUT TO ss;\n\nCREATE TYPE userType(\nCol1 String,\nCol2 String,\nCol3 String,\nCol4 String,\nCol5 String\n);\n\nCREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;\n\nCREATE CQ cq1\nINSERT INTO userDefinedTypedStream\nSELECT data[0],data[1],data[2], data[3],data[4]\nFROM ss;\n\ncreate Target t1 using AzureEventHubWriter (\n\tEventHubNamespace:'qatestnov9',\n\tEventHubName:'test_2_partitions',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\t--Partitionkey:'@metadata(RecordOffset)',\n\tParallelThreads:'2',\n\tConsumerGroup:'reader',\n\tE1P:'true',\n\tOperationTimeout:'500000',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\njsonMemberDelimiter: '\\n',\njsonobjectdelimiter: '\\n',\nEventsAsArrayOfJsonObjects: 'true')\ninput from ss;\n\ncreate Target t2 using AzureEventHubWriter (\n\tEventHubNamespace:'qatestnov9',\n\tEventHubName:'test_2_partitions',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\t--Partitionkey:'Col1',\n\tParallelThreads:'2',\n\tConsumerGroup:'reader',\n\tE1P:'true',\n\tOperationTimeout:'500000',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\njsonMemberDelimiter: '\\n',\njsonobjectdelimiter: '\\n',\nEventsAsArrayOfJsonObjects: 'true')\ninput from userDefinedTypedStream;\n\nEND APPLICATION EH;\nDEPLOY APPLICATION EH;\nstart application EH;\n\nSTOP APPLICATION ER;\nUNDEPLOY APPLICATION ER;\nDROP APPLICATION ER CASCADE;\nCREATE APPLICATION ER;\nCREATE SOURCE ER_S1 USING AzureEventHubReader (\n    EventHubNamespace:'qatestnov9',\n\tEventHubName:'test_2_partitions',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'Default',\n\tstartSeqNo:'0'\t\n\t)\nPARSE USING jsonParser (\n)OUTPUT TO ER_SS1;\n\n\nCREATE SOURCE ER_S2 USING AzureEventHubReader (\n    EventHubNamespace:'qatestnov9',\n\tEventHubName:'test_2_partitions',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'Default',\n\tstartSeqNo:'0'\t\n\t)\nPARSE USING jsonParser (\n)OUTPUT TO ER_SS2;\n\ncreate Type CustType1 \n(writerdata com.fasterxml.jackson.databind.JsonNode\n--TopicName java.lang.String,\n--PartitionID java.lang.String\n);\n\nCreate Stream datastream1 of CustType1;\n\ncreate Type CustType2\n(Col1 String,\nCol2 String,\nCol3 String,\nCol4 String,\nCol5 String\n--TopicName String,\n--PartitionID String\n);\n\nCreate Stream datastream2 of CustType2;\n\nCREATE CQ CustCQ1\nINSERT INTO datastream1\nSELECT data.data\n--metadata.get(\"TopicName\").toString() AS TopicName,\n--metadata.get(\"PartitionID\").toString() AS PartitionID\nFROM ER_SS1;\n\nCREATE CQ CustCQ2\nINSERT INTO datastream2\nSELECT \ndata.get('Col1').textValue(),\ndata.get('Col2').textValue(),\ndata.get('Col3').textValue(),\ndata.get('Col4').textValue(),\ndata.get('Col5').textValue()\n--metadata.get(\"TopicName\").toString() AS TopicName,\n--metadata.get(\"PartitionID\").toString() AS PartitionID\nFROM ER_SS2;\n\ncreate Target ER_t1 using FileWriter (\nfilename:'FT1_5L_JSON_RT',\ndirectory:'FEATURE-DIR/logs/',\nrolloverpolicy:'eventcount:5000000'\t\n)\nformat using dsvFormatter()\ninput from datastream1;\n\ncreate Target ER_t2 using FileWriter (\nfilename:'FT2_5L_JSON_RT',\ndirectory:'FEATURE-DIR/logs/',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000'\t\n)\nformat using dsvFormatter()\ninput from datastream2;\nend application ER;\ndeploy application ER;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION VerticaTester.VW;\nUNDEPLOY APPLICATION VerticaTester.VW;\nDROP APPLICATION VerticaTester.VW CASCADE;\nCREATE APPLICATION VW;\n\nCREATE SOURCE CSVSource USING CSVReader\n(\n  directory:'@TEST-DATA-PATH@',\n  header:No,\n  wildcard:'trade.csv',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO CsvStream;\n\n\nCREATE TYPE VerticaStream_t\n(\n  NameField   String,\n  ValueField  String\n);\n\nCREATE STREAM VerticaStream OF VerticaStream_t;\n\nCREATE CQ VW_q\n  INSERT INTO VerticaStream\n  SELECT DATA[0],\n         DATA[1]\n  FROM CsvStream;\n\nCREATE TARGET WriteToVertica USING VerticaWriter\n(\n  dbHostName:'@HOST@',\n  dbUser:'@USER@',\n  dbPassword: '@PASS@',\n  dbName: '@DBNAME@',\n  tableName: '@SCHEMA@.TRADE'\n)\nINPUT FROM VerticaStream;\n\nEND APPLICATION VW;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP ttlTester.ttlApp;\nUNDEPLOY APPLICATION ttlTester.ttlApp;\nDROP APPLICATION ttlTester.ttlApp cascade;\n\nCREATE APPLICATION ttlApp;\n\n\nCREATE source wsSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'banks.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:No,\n  trimquote:false\n) OUTPUT TO QaStream;\n\n\nCREATE TYPE wsData\n(\nbankID Integer KEY,\nbankName String\n);\n\n\nCREATE STREAM wsStream OF wsData;\n\n--Select data from QaStream and insert into wsStream\n\nCREATE CQ csvTowsData\nINSERT INTO wsStream\nSELECT TO_INT(data[0]),data[1] FROM QaStream;\n\n--create jumping window over data in wsStream\n\nCREATE WACTIONSTORE oneWS\nCONTEXT OF wsData\nEVENT TYPES(wsData )\nPERSIST IMMEDIATE USING ( storageProvider: 'elasticsearch', elasticsearch.time_to_live: '10000ms' ) ;\n\n--get data from wsStream and place into wactionStore oneWS\nCREATE CQ wsToWaction\nINSERT INTO oneWS\nSELECT * FROM wsStream\nLINK SOURCE EVENT;\n\n\nEND APPLICATION ttlApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE FLOW @APPNAME@AgentFlow;\nCREATE OR REPLACE SOURCE @APPNAME@_src USING Global.GCSReader ()\nPARSE USING Global.DSVParser ()\nOUTPUT TO @APPNAME@_Stream;\nEND FLOW @APPNAME@AgentFlow;\n\nCREATE FLOW @APPNAME@serverFlow;\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer ()\nFORMAT USING DSVFormatter (\nmembers:'data')\nINPUT FROM @APPNAME@_Stream;\nEND FLOW @APPNAME@serverFlow;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ with @APPNAME@AgentFlow in Agents, @APPNAME@ServerFlow in default;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING PostgreSQLReader  (\n  ReplicationSlotName: 'Slot_Name',\n  FilterTransactionBoundaries: 'true',\n  Username: 'User_Name',\n  ConnectionURL: 'Connection_URL',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'Password',\n  Tables: 'Tables'\n )\nOUTPUT TO @STREAM@ ;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@2 USING PostgreSQLReader  (\n  ReplicationSlotName: 'Slot_Name',\n  FilterTransactionBoundaries: 'true',\n  Username: 'User_Name',\n  ConnectionURL: 'Connection_URL',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'Password',\n  Tables: 'Tables'\n )\nOUTPUT TO @STREAM@ ;\n\nCREATE TARGET @SOURCE_NAME@_sysout USING Global.SysOut (\n  name: '@SOURCE_NAME@_sysout' )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@ Using DatabaseReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'qatest.@SourceTable@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop APPLICATION @AppName@;\nUndeploy APPLICATION @AppName@;\ndrop APPLICATION @AppName@ cascade;\nCREATE OR REPLACE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW @AgentFlow@;\n\nCREATE OR REPLACE SOURCE @SourceName@ USING Global.MSJet (\n  Tables: 'dbo.compsrc',\n  username: 'qatest',\n  DatabaseName: 'qatest',\n  FetchTransactionMetadata: true,\n  filterTransactionBoundaries: true,\n  compression: false,\n   Mode: '@mode@',\n  ConnectionURL: '@ConnectionURL@',\n  CommittedTransactions: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',\n  SendBeforeImage: true,\n  password: 'w3b@ct10n' )\nOUTPUT TO @StreamName@;\nEND FLOW @AgentFlow@;\n\nCREATE TARGET @SysTarget@ USING Global.SysOut (\n  name: 'MS_CDC_SYSOUT' )\nINPUT FROM @StreamName@;\n\nCREATE FLOW @ServerFlow@;\n\nCREATE TARGET @TargetName@ USING Global.DatabaseWriter (\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  ParallelThreads: '',\n  CheckPointTable: 'CHKPOINT',\n  CDDLAction: 'Process',\n  CommitPolicy: 'EventCount:1000,Interval:60',\n  ConnectionURL: '@ConnectionURL@',\n  StatementCacheSize: '50',\n  DatabaseProviderType: 'Default',\n  Username: 'qatest',\n  Tables: 'dbo.compsrc,dbo.comptar',\n  Password: 'w3b@ct10n',\n  PreserveSourceTransactionBoundary: 'false',\n  BatchPolicy: 'EventCount:1,Interval:60' )\nINPUT FROM @StreamName@;\n\ncreate Target @TargetName@_File using FileWriter(\n  filename:'CompressedMerchant.gz',\n  directory:'',\n  rolloverpolicy:'EventCount:10000,Interval:30s'\n)\nformat using DSVFormatter (\n\n)\nINPUT FROM @StreamName@;\n\nEND FLOW @ServerFlow@;\n\nEND APPLICATION @AppName@;\nDEPLOY APPLICATION @AppName@ with @AgentFlow@ in AGENTS ,@ServerFlow@ on any in default;\nSTART APPLICATION @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application FileWriterTest;\nundeploy application FileWriterTest;\ndrop application FileWriterTest cascade;\ncreate application FileWriterTest recovery 1 second interval;\ncreate source File_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'false'\n)\nOUTPUT TO CsvStream;\n\ncreate Target FileTarget using FileWriter(\n    rolloverpolicy:'@UPLOAD-SIZE@',\n    filename :'@FILE@',\n    directory : '@FOLDER@'\n)\nformat using XMLFormatter (\ncharset:'@charset@',\nrootelement:'@mem@'\n)\ninput from CsvStream;\n\nend application FileWriterTest;\ndeploy application FileWriterTest on all in default;\nstart application FileWriterTest;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@ Using MySQLReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'qatest.@SourceTable@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n\nCREATE SOURCE @APPNAME@_S USING MSSqlReader\n(\n  Compression: false,\n  cdcRoleName: 'STRIIM_READER',\n  DatabaseName: 'QATEST',\n  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',\n  ConnectionPoolSize: 1,\n  FetchTransactionMetadata: false,\n  StartPosition: 'EOF',\n  Username: 'qatest',\n  SendBeforeImage: true,\n  AutoDisableTableCDC: true,\n  ConnectionURL: 'localhost:1433',\n  Tables: 'qatest.test01',\n  adapterName: 'MSSqlReader',\n  Password: 'w3b@ct10n'\n)\nOUTPUT TO @APPNAME@_SS;\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'NULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:0',\nStandardSQL:true\t\n) INPUT FROM @APPNAME@_ss;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\n--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop OracleToKudu;\nundeploy application OracleToKudu;\ndrop application OracleToKudu cascade;\n\nCREATE APPLICATION OracleToKudu;\nCreate Source OS Using OracleReader\n(\n Username:'@LOGMINER-UNAME@',\n Password:'@LOGMINER-PASSWORD@',\n ConnectionURL:'@LOGMINER-URL@',\n Tables:'@SOURCE_TABLES@',\n OnlineCatalog:true,\n FetchSize:1\n) Output To DataStream;\n\nCREATE TARGET T using KuduWriter (\nkuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',\npkupdatehandlingmode:'@MODE@',\ntables: '@TARGET_TABLES@',\nbatchpolicy: 'EventCount:1,Interval:0')\nINPUT FROM DataStream;\n\nEND APPLICATION OracleToKudu;\ndeploy application OracleToKudu in default;\nstart OracleToKudu;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING DatabaseReader  (\n  Username: '@READER-UNAME@',\n  Password: '@READER-PASSWORD@',\n  ConnectionURL: '@CDC-READER-URL@',\n  Tables: @SOURCE_TABLE@,\n  sendBeforeImage:'true',\n  FilterTransactionBoundaries: 'true'\n )\nOUTPUT TO @STREAM@;\n\nCREATE ROUTER @SOURCE_NAME@_ROUTER INPUT FROM @STREAM@ cs CASE\nWHEN meta(cs,\"OperationName\").toString()='SELECT' THEN ROUTE TO @STREAM@1, ELSE ROUTE TO @STREAM@2;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE OR REPLACE SOURCE @SOURCE@ USING MSSQLReader  ( \n  FilterTransactionBoundaries: true,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  Username: '@SOURCE_USER@',\n  DatabaseName: 'qatest',\n  ConnectionPoolSize: 1,\n  StartPosition: 'EOF',\n  ReplicationSlotName: 'null'\n ) \nOUTPUT TO @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET@ USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n ) \nINPUT FROM @STREAM@;\n\n\nEND APPLICATION @APPNAME@;\n\ndeploy application @APPNAME@ on ANY in default;\n\nstart application @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\ncreate application @appname@;\n\nCREATE SOURCE @parquetsrc@ USING FileReader (\n  directory: '',\n  positionByEOF: false,\n  WildCard: '')\nPARSE USING ParquetParser ()\nOUTPUT TO @appname@Streams;\n\nCREATE CQ @appname@CQ1\nINSERT INTO @appname@out\nSELECT convertParquetEventToWAEvent(o) FROM @appname@Streams o;\n\nCREATE TARGET @filetarget@ USING Global.FileWriter (\n  directory: '',\n  DataEncryptionKeyPassphrase: '',\n  flushpolicy: 'EventCount:10000,Interval:30s',\n  rolloveronddl: 'true',\n  encryptionpolicy: '',\n  filename: '',\n  DataEncryptionKeyPassphrase_encrypted: 'true',\n  rolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING Global.JSONFormatter  (\n  jsonMemberDelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true',\n  members: 'data',\n  jsonobjectdelimiter: '\\n' )\nINPUT FROM @appname@out;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ recovery 5 second interval;\n\nCREATE  SOURCE @QuerySource@1 USING DatabaseReader  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  DatabaseName:'qatest',\n  StartPosition:'EOF',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@1;\ncreate Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;\n CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@1;\n\nCREATE  SOURCE @QuerySource@2 USING DatabaseReader  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  DatabaseName:'qatest',\n  StartPosition:'EOF',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@2;\ncreate Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;\n CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@2;\n\nCREATE  SOURCE @QuerySource@3 USING DatabaseReader  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  DatabaseName:'qatest',\n  StartPosition:'EOF',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@3;\ncreate Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;\n CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@3;\n\nCREATE  SOURCE @QuerySource@4 USING DatabaseReader  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  DatabaseName:'qatest',\n  StartPosition:'EOF',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@4;\ncreate Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;\n CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@4;\n\nCREATE  SOURCE @QuerySource@5 USING DatabaseReader  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  DatabaseName:'qatest',\n  StartPosition:'EOF',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@5;\ncreate Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;\n CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@5;\n\nCREATE  SOURCE @QuerySource@6 USING DatabaseReader  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  DatabaseName:'qatest',\n  StartPosition:'EOF',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@6;\ncreate Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;\n CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@6;\n\nCREATE  SOURCE @QuerySource@7 USING DatabaseReader  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  DatabaseName:'qatest',\n  StartPosition:'EOF',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@7;\ncreate Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;\n CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@7;\n\nCREATE  SOURCE @QuerySource@8 USING DatabaseReader  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  DatabaseName:'qatest',\n  StartPosition:'EOF',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@8;\ncreate Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;\n CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@8;\n\nCREATE  SOURCE @QuerySource@9 USING DatabaseReader  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  DatabaseName:'qatest',\n  StartPosition:'EOF',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@9;\ncreate Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;\n CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@9;\n\nCREATE  SOURCE @QuerySource@10 USING DatabaseReader  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  DatabaseName:'qatest',\n  StartPosition:'EOF',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@10;\ncreate Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;\n CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',,\n  Username: 'qatest',\n  Password:'w3b@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@10;\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application AzureApp;\nundeploy application AzureApp;\ndrop application AzureApp cascade;\n\ncreate application AzureApp\nRECOVERY 10 second interval;\nCREATE SOURCE OracleSource USING OracleReader\n(\n    Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n Tables:'@TABLES@',\n    FetchSize: 1\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId int,\n  curr String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT TO_INT(data[0]),\n       data[1]\nFROM CsvStream;\n\ncreate Target BlobT using AzureBlobWriter(\n\taccountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:5,interval:5s'\n)\nformat using AvroFormatter (\n)\ninput from TypedCSVStream;\nend application AzureApp;\ndeploy application AzureApp in default;\nstart application AzureApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@ recovery 5 SECOND Interval;\nCREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: '@ORACLE-URL@',\n  Tables: '@SOURCE-TABLES@',\n  Username: '@ORACLE-USERNAME@',\n  Password: '@ORACLE-PASSWORD@'\n) OUTPUT TO @APP_NAME@_Stream;\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt USING SnowflakeWriter\n\n(\n  ConnectionURL:'@SNOWFLAKE-URL@',\n  username:'@SNOWFLAKE-USERNAME@',\n  appendOnly:'false',\n  Tables:'@TARGET-TABLES@',\n  uploadpolicy:'eventcount:3,interval:10s',\n  externalStageType:'local'\n)\nINPUT FROM @APP_NAME@_Stream;\n\n\nEND APPLICATION @APP_NAME@;\nDEPLOY APPLICATION @APP_NAME@;\nSTART APPLICATION @APP_NAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application APP_KAFKA_DATASOURCES;\nundeploy application APP_KAFKA_DATASOURCES;\nalter application APP_KAFKA_DATASOURCES;\n\nCREATE OR REPLACE SOURCE SRC_FR_KAFKA_HOURLYTOTALS USING Global.FileReader (\n  adapterName: 'FileReader',\n  rolloverstyle: 'Default',\n  blocksize: 64,\n  skipbom: true,\n  wildcard: 'kafka_hourly_total_20210316.txt',\n  directory: '@confDir@',\n  includesubdirectories: false,\n  positionbyeof: false ) \nPARSE USING Global.DSVParser (\n  trimwhitespace: false,\n  linenumber: '-1',\n  columndelimittill: '-1',\n  trimquote: true,\n  ignoreemptycolumn: false,\n  parserName: 'DSVParser',\n  quoteset: '\\\"',\n  handler: 'com.webaction.proc.DSVParser_1_0',\n  charset: 'UTF-8',\n  columndelimiter: ':',\n  ignoremultiplerecordbegin: 'true',\n  ignorerowdelimiterinquote: false,\n  separator: ',',\n  header: false,\n  blockascompleterecord: false,\n  rowdelimiter: '\\n',\n  nocolumndelimiter: false,\n  headerlineno: 0 )\nOUTPUT TO STREAM_SRC_FR_KAFKA_HOURLYTOTALS;\n\nalter application APP_KAFKA_DATASOURCES recompile;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ recovery 5 second interval;\nCREATE  SOURCE @SourceName@ USING MSSqlReader  ( \n  Username: '@Username@',\n  Password: '@Password@',\n  DatabaseName: 'qatest',\n  connectionRetryPolicy: @ConnectionRetryPolicy@,\n  ConnectionURL: '@ConnectionURL@',\n  Tables: '@SourceTables@',\n  ConnectionPoolSize: 1,\n  StartPosition: 'EOF',\n  ReplicationSlotName: 'null'\n ) \nOUTPUT TO @SRCINPUTSTREAM@;\ncreate Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING AzureBlobWriter(\n\taccountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:@EC@'\n)\nformat using DSVFormatter (\n)\ninput from @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE @APPNAME@_Source USING Global.MSSqlReader (\n  TransactionSupport: false,\n  _h_returnNumericAs: 'Double',\n  Tables: 'qatest.T27342_Source',\n  FetchTransactionMetadata: false,\n  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',\n  Compression: false,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  Password_encrypted: 'false',\n  Password: 'w3b@ct10n',\n  StartPosition: 'NOW',\n  adapterName: 'MSSqlReader',\n  cdcRoleName: 'STRIIM_READER',\n  DatabaseName: 'qatest',\n  Username: 'qatest',\n  FetchSize: 0,\n  IntegratedSecurity: false,\n  FilterTransactionBoundaries: true,\n  ConnectionPoolSize: 2,\n  SendBeforeImage: true,\n  AutoDisableTableCDC: false )\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_Target USING Global.AzureSQLDWHWriter (\n  AccountName: 'testaswin',\n  Tables: 'qatest.T27342_Source,dbo.test',\n  Password_encrypted: 'false',\n  AccountAccessKey_encrypted: 'false',\n  CDDLAction: 'Process',\n  StorageAccessDriverType: 'WASBS',\n  ConnectionURL: 'jdbc:sqlserver://testaswin.database.windows.net:1433;database=testaswin',\n  Username: 'testaswin',\n  columnDelimiter: '|',\n  Mode: 'MERGE',\n  AccountAccessKey: 'MmlfH35Vc2mcOScbY2wnOyXol6deT8gtGA4XW3C5EXwwdFQEukP37RfHGWeUgMhfKsIvDvCHF/v3GF6frXGdYg==',\n  Password: 'W3b@ct10n2020',\n  uploadpolicy: 'eventcount:1,interval:5m' )\nINPUT FROM @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_sysout USING Global.SysOut (\n  name: 'sysout_1' )\nINPUT FROM @APPNAME@_Stream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSV1Source using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'MerchantNames.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO Csv1Stream;\n\nCreate Type CSV1Type (\n  merchantId String,\n  merchantName String\n);\n\nCreate Stream TypedCSV1Stream of CSV1Type;\n\nCREATE CQ CsvToMerchantNames\nINSERT INTO TypedCSV1Stream\nSELECT data[0],\n       data[1]\nFROM Csv1Stream;\n\ncreate Target t using FileWriter(\n  filename:'CompressedMerchant.gz',\n  directory:'@FEATURE-DIR@/logs/',\n  compressiontype : 'garbage',\n  rolloverpolicy:'EventCount:10000,Interval:30s'\n)\nformat using DSVFormatter (\n\n)\ninput from TypedCSV1Stream;\n\nend application DSV;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "-- Wactionstore has been moved to DSWaction.tql\n\nCREATE APPLICATION MyPosApp;\n\nCREATE TYPE USAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\nCREATE CACHE ZipLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'USAddresses.txt',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header: Yes,\n  columndelimiter: '\\t',\n  trimquote:false\n) QUERY (keytomap:'zip') OF USAddressData;\n\nCREATE source CsvDataSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'posdata.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\n\nCREATE TYPE PosData(\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\nCREATE STREAM PosDataStream OF PosData PARTITION BY merchantId;\n\nCREATE CQ CsvToPosData\nINSERT INTO PosDataStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW PosData5Minutes\nOVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE TYPE MerchantHourlyAve(\n  merchantId String,\n  hourValue int,\n  hourlyAve int\n);\nCREATE CACHE HourlyAveLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'hourlyData.txt'\n)\nPARSE USING DSVParser (\n  header: Yes,\n  trimquote:false\n) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;\n\n\nCREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;\n\nCREATE CQ GenerateMerchantTxRateOnly\nINSERT INTO MerchantTxRateOnlyStream\nSELECT p.merchantId,\n       p.zip,\n       FIRST(p.dateTime),\n       COUNT(p.merchantId),\n       SUM(p.amount),\n       FIRST(l.hourlyAve/12),\n       FIRST(l.hourlyAve/12 * CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       FIRST(l.hourlyAve/12 / CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       '<NOTSET>',\n       '<NOTSET>'\nFROM PosData5Minutes p, HourlyAveLookup l\nWHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue\nGROUP BY p.merchantId;\n\nCREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate PARTITION BY merchantId;\n\nCREATE CQ GenerateMerchantTxRateWithStatus\nINSERT INTO MerchantTxRateWithStatusStream\nSELECT merchantId,\n       zip,\n       startTime,\n       count,\n       totalAmount,\n       hourlyAve,\n       upperLimit,\n       lowerLimit,\n       CASE\n         WHEN count >10000 THEN 'HOT'\n         WHEN count > 800 THEN 'WARM'\n         WHEN count >200 THEN 'COOL'\n         ELSE 'COLD' END,\n       CASE\n         WHEN count > upperLimit THEN 'TOOHIGH'\n         WHEN count < lowerLimit THEN 'TOOLOW'\n         ELSE 'OK' END\nFROM MerchantTxRateOnlyStream;\n\n\nCREATE TYPE MerchantNameData(\n  merchantId String KEY,\n  companyName String\n);\n\nCREATE CACHE NameLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  WildCard:'MerchantNames.csv',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:'yes',\n  trimquote:false\n)\nQUERY(keytomap:'merchantId') OF MerchantNameData;\n\nCREATE CQ GenerateWactionContext\nINSERT INTO MerchantActivity\nSELECT  m.merchantId,\n        m.startTime,\n        n.companyName,\n        m.category,\n        m.status,\n        m.count,\n        m.hourlyAve,\n        m.upperLimit,\n        m.lowerLimit,\n        m.zip,\n        z.city,\n        z.state,\n        z.latVal,\n        z.longVal\nFROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z\nWHERE m.merchantId = n.merchantId AND m.zip = z.zip\nLINK SOURCE EVENT;\n\nCREATE STREAM AlertStream OF Global.AlertEvent;\n\nCREATE CQ GenerateAlerts\nINSERT INTO AlertStream\nSELECT n.CompanyName,\n       m.MerchantId,\n       CASE\n         WHEN m.Status = 'OK' THEN 'info'\n         ELSE 'warning' END,\n       CASE\n         WHEN m.Status = 'OK' THEN 'cancel'\n         ELSE 'raise' END,\n       CASE\n         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)\n         ELSE ''\n         END\nFROM MerchantTxRateWithStatusStream m, NameLookup n\nWHERE m.merchantId = n.merchantId;\n\nCREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;\n\nEND APPLICATION MyPosApp;\ndeploy application MyPosApp;\nstart application MyPosApp;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n\nCREATE SOURCE @APPNAME@_S USING MySqlReader\n(\n  Compression: false,\n  FetchSize: 1,\n  SendBeforeImage: true,\n  ConnectionURL: 'mysql://localhost:3306',\n  DatabaseName: 'waction',\n  Tables: 'waction.test01',\n  Password: 'w@ct10n',\n  Password_encrypted: 'false',\n  Username: 'root'\n)\nOUTPUT TO @APPNAME@_SS;\n\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'waction.test01,@DATASET@.% KEYCOLUMNS(id,name) COLUMNMAP(id=id,col1=col11,col2=col12)',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:10',\nStandardSQL:true\t\n) INPUT FROM @APPNAME@_ss;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\nSTART APPLICATION @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n--create application @APPNAME@ Recovery 5 Second Interval;\ncreate application @APPNAME@;\n\ncreate or replace type @APPNAME@emp_type(\nSno integer,\nEmpname string,\nDoj string,\nCountry string,\nCompanyName string\n);\n\nCREATE OR REPLACE SOURCE @APPNAME@File_Source1 using Filereader(\n\tdirectory:'@DIRECTORY@',\n  wildcard:'File_empdata.csv',\n  positionByEOF:false\n)parse using dsvParser(\n    header:'yes'\n)\nOUTPUT TO @APPNAME@FileSource_Stream1,\nOUTPUT TO @APPNAME@FileSource_Stream1_automap MAP(filename:'File_empdata.csv');\n\nCREATE OR REPLACE SOURCE @APPNAME@Init_Source1 USING DatabaseReader  (\n  Username: 'qatest',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.EMP_INIT',\n  adapterName: 'DatabaseReader',\n  FetchSize: 100,\n  Password: 'qatest'\n )\nOUTPUT TO \t@APPNAME@InitialLoad_Stream1;\n\nCREATE OR REPLACE SOURCE @APPNAME@CDC_Source1 USING OracleReader  (\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  TransactionBufferDiskLocation: '.striim/LargeBuffer',\n  FetchSize: 1,\n  DDLCaptureMode: 'All',\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.EMP',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  TransactionBufferType: 'Memory',\n  DictionaryMode: 'OnlineCatalog',\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  StartSCN: null,\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  TransactionBufferSpilloverSize: '1MB',\n  compression: true,\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO @APPNAME@CDC_Stream1 ;\n\ncreate or replace stream @APPNAME@FileSource_cdc_init_TypedStream of @APPNAME@emp_type;\ncreate or replace cq @APPNAME@file_typed_streamcq\ninsert into @APPNAME@FileSource_cdc_init_TypedStream\nSELECT to_int(data[0]),\ndata[1],\ndata[2],\ndata[3],\ndata[4]\nfrom @APPNAME@FileSource_Stream1;\n\ncreate or replace cq @APPNAME@cdc_typed_streamcq\ninsert into @APPNAME@FileSource_cdc_init_TypedStream\nSELECT to_int(data[0]),\ndata[1],\ndata[2],\ndata[3],\ndata[4]\nfrom @APPNAME@CDC_Stream1;\n\ncreate or replace cq @APPNAME@init_typed_streamcq\ninsert into @APPNAME@FileSource_cdc_init_TypedStream\nSELECT to_int(data[0]),\ndata[1],\ndata[2],\ndata[3],\ndata[4]\nfrom @APPNAME@InitialLoad_Stream1;\n\nCREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target1 USING CassandraCosmosDBWriter  (\n  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --CheckPointTable: '',\n  FlushPolicy: 'EventCount:1000,Interval:60',\n  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',\n  AccountKey:'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',\n  KeySpace: 'test',\n  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',\n  --ConsistencyLevel:'fh',\n  --port:'10505',\n  Tables: 'test.file_emp',\n  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',\n  adapterName: 'CassandraCosmosDBWriter'\n )\nINPUT FROM @APPNAME@FileSource_Stream1_automap;\n\nCREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target2 USING CassandraCosmosDBWriter  (\n  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --CheckPointTable: '',\n  FlushPolicy: 'EventCount:1000,Interval:60',\n  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',\n  AccountKey: 'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',\n  KeySpace: 'test',\n  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',\n  --ConsistencyLevel:'fh',\n  --port:'10505',\n  Tables: 'QATEST.EMP,test.cdc_emp',\n  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',\n  adapterName: 'CassandraCosmosDBWriter'\n )\nINPUT FROM @APPNAME@CDC_Stream1;\n\nCREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target3 USING CassandraCosmosDBWriter  (\n  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --CheckPointTable: '',\n  FlushPolicy: 'EventCount:1000,Interval:60',\n  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',\n  AccountKey: 'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',\n  KeySpace: 'test',\n  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',\n  --ConsistencyLevel:'fh',\n  --port:'10505',\n  Tables: 'QATEST.EMP_INIT,test.initialload_emp',\n  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',\n  adapterName: 'CassandraCosmosDBWriter'\n )\nINPUT FROM @APPNAME@InitialLoad_Stream1;\n\n\nCREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target4 USING CassandraCosmosDBWriter  (\n  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --CheckPointTable: '',\n  FlushPolicy: 'EventCount:1000,Interval:60',\n  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',\n  AccountKey: 'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',\n  KeySpace: 'test',\n  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',\n  --ConsistencyLevel:'fh',\n  --port:'10505',\n  Tables: 'test.file_cdc_emp',\n  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',\n  adapterName: 'CassandraCosmosDBWriter'\n )\nINPUT FROM @APPNAME@FileSource_cdc_init_TypedStream;\n\ncreate or replace target @APPNAME@sys_file_tgt using sysout(\nname:'foo_file'\n)input from @APPNAME@FileSource_Stream1;\n\ncreate or replace target @APPNAME@sys_cdc_tgt using sysout(\nname:'foo_cdc'\n)input from @APPNAME@CDC_Stream1;\n\ncreate or replace target @APPNAME@sys_init_tgt using sysout(\nname:'foo_init'\n)input from @APPNAME@InitialLoad_Stream1;\n\nEnd Application @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 42 with two sources and two WactionStores. A variety of partitioned windows in between\n-- assure that we are testing a complicated recovery scenario.\n--\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Stream -> JWc5 -> WS1\n--   S2 -> Stream -> JWc10 -> WS2\n--\n\nSTOP Recov42Tester.RecovTest42;\nUNDEPLOY APPLICATION Recov42Tester.RecovTest42;\nDROP APPLICATION Recov42Tester.RecovTest42 CASCADE;\nCREATE APPLICATION RecovTest42 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10242,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10242,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');\nCREATE STREAM DataStreamTop OF CsvData using KafkaProps;\n\nCREATE CQ Csv1ToDataStreamTop\nINSERT INTO DataStreamTop\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ Csv2ToDataStreamTop\nINSERT INTO DataStreamTop\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\n\n\n\n\n\n\n\n\n\nCREATE JUMPING WINDOW LeftJWc5\nOVER DataStreamTop KEEP 5 ROWS;\n\nCREATE JUMPING WINDOW RightJWc10\nOVER DataStreamTop KEEP 10 ROWS;\n\n\n\nCREATE WACTIONSTORE WactionsLeft CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE WactionsRight CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ ToWactionsLeft\nINSERT INTO WactionsLeft\nSELECT\n    FIRST(p.merchantId),\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    TO_DOUBLE(FIRST(p.amount))\nFROM LeftJWc5 p;\n\nCREATE CQ ToWactionsRight\nINSERT INTO WactionsRight\nSELECT\n    FIRST(p.merchantId),\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    TO_DOUBLE(FIRST(p.amount))\nFROM RightJWc10 p;\n\nEND APPLICATION RecovTest42;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application admin.@APPNAME@;\nundeploy application admin.@APPNAME@;\nalter application admin.@APPNAME@ AUTORESUME MAXRETRIES 5 RETRYINTERVAL 5;\nAlter application admin.@APPNAME@ RECOMPILE;\nAlter application admin.@APPNAME@;\ncreate or replace source admin.@APPNAME@s using FileReader (\n        directory:'Product/IntegrationTests/TestData/',\n        wildcard:'posdata5L.csv',\n        positionByEOF:false,\n        Charset:'UTF-8'\n)\nPARSE USING DSVParser (\n  columndelimiter:',',\n  ignoreemptycolumn:'Yes',\n  quoteset:'[]~\"',\n  header: true,\n  separator:'~'\n)\nOUTPUT TO admin.@APPNAME@in_memory_rawStream;\n\nAlter application admin.@APPNAME@ RECOMPILE;\ndeploy application admin.@APPNAME@;\nstart admin.@APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE FLOW ServerFlow;\n\nCREATE TARGET @TARGET_NAME@_sysout USING Global.SysOut (\n  name: '@TARGET_NAME@_SysOut' )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  Username: '@TARGET_USER@',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1'\n )\nINPUT FROM @STREAM@;\nEND FLOW ServerFlow;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION DSV RECOVERY 1 SECOND INTERVAL;\n\nCREATE FLOW agentflow;\n\nCREATE OR REPLACE SOURCE CSVSource USING Global.FileReader (\n  charset: 'UTF-8',\n  adapterName: 'FileReader',\n  rolloverstyle: 'Default',\n  positionByEOF: false,\n  WildCard: 'posdata.csv',\n  blocksize: 64,\n  skipbom: true,\n  includesubdirectories: false,\n  directory: 'Samples/AppData' )\nPARSE USING Global.DSVParser (\n  trimwhitespace: false,\n  linenumber: '-1',\n  columndelimiter: ',',\n  columndelimittill: '-1',\n  trimquote: true,\n  ignoreemptycolumn: false,\n  separator: ':',\n  parserName: 'DSVParser',\n  quoteset: '\\\"',\n  handler: 'com.webaction.proc.DSVParser_1_0',\n  charset: 'UTF-8',\n  ignoremultiplerecordbegin: 'true',\n  ignorerowdelimiterinquote: false,\n  blockascompleterecord: false,\n  rowdelimiter: '\\n',\n  nocolumndelimiter: false,\n  headerlineno: 0,\n  header: true )\nOUTPUT TO CsvStream;\n\nEND FLOW agentflow;\n\nCREATE OR REPLACE TARGET t USING Global.FileWriter (\n  directory: '@TEST-DATA-PATH@',\n  flushpolicy: 'EventCount:10000,Interval:30s',\n  members: 'data',\n  rolloveronddl: 'true',\n  adapterName: 'FileWriter',\n  rolloverpolicy: 'TimeIntervalRollingPolicy,rotationinterval:5s',\n  filename: 'Foo' )\nFORMAT USING Global.DSVFormatter  (\n  quotecharacter: '\\\"',\n  handler: 'com.webaction.proc.DSVFormatter',\n  columndelimiter: ',',\n  formatterName: 'DSVFormatter',\n  nullvalue: 'NULL',\n  usequotes: 'false',\n  rowdelimiter: '\\n',\n  standard: 'none',\n  header: 'false' )\nINPUT FROM CsvStream;\n\nEND APPLICATION DSV;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@ recovery 5 Second interval;\ncreate source @APPNAME@_Source using FileReader (\n        directory:'./Samples/AppData/',\n        WildCard:'dynamicdirectory.csv',\n        positionByEOF:false,\n        charset:'UTF-8'\n)\nparse using DSVParser (\n        header:'no'\n)\nOUTPUT TO @APPNAME@_Stream;\n\nCreate Type @APPNAME@_type (\n  id String,\n  name String,\n  department String,\n  yoj String,\n  moj String,\n  doj int\n);\n\nCreate Stream @APPNAME@_typedstream of @APPNAME@_type;\n\nCREATE CQ @APPNAME@_cq\nINSERT INTO @APPNAME@_typedstream\nSELECT data[0],\n       data[1],\n       data[2],\n       data[3],\n       data[4],\n       TO_INT(data[5])\nFROM @APPNAME@_stream;\n\ncreate Target @APPNAME@_target using ADLSGen1Writer(\n        filename:'',\n        directory:'',\n        datalakestorename:'',\n        clientid:'',\n        authtokenendpoint:'',\n        clientkey:'',\n\t\trolloverpolicy:'eventcount:5'\n)\nformat using DSVFormatter (\n)\ninput from @APPNAME@_typedstream; \n\nend application @APPNAME@;\n\ndeploy application @APPNAME@;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE OR REPLACE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;\n\nCREATE OR REPLACE SOURCE @SOURCE@ USING SalesForceReader (\n  autoAuthTokenRenewal: 'true',\n  Username: '@userName@',\n  securityToken: '@securityToken@',\n  sObjects: '@srcObjectName@',\n  pollingInterval: '1 min',\n  Password_encrypted: 'false',\n  securityToken_encrypted: 'false',\n  customObjects: 'False',\n  consumerKey: '@consumerKey@',\n  startTimestamp: '',\n  apiEndPoint: 'https://ap2.salesforce.com',\n  mode: 'Incremental',\n  consumerSecret: '@consumerSecert@',\n  consumerSecret_encrypted: 'false',\n  Password: '@Password@' )\nOUTPUT TO @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET@ USING Global.BigQueryWriter (\n  ColumnDelimiter: '|',\n  NullMarker: 'NULL',\n  ConnectionRetryPolicy: 'retryInterval=30,\\n maxRetries=3',\n  streamingUpload: 'false',\n  Mode: 'Merge',\n  projectId: '@ProjectId@',\n  Encoding: 'UTF-8',\n  TransportOptions: 'connectionTimeout=300,\\n readTimeout=120',\n  Tables: '@srcObjectName@,@TargetTableName@ columnmap(ID=ID,checkbool__c=checkbool__c,dt__c=dt__c,percnt__c=percnt__c,phn__c=phn__c,txtlong__c=txtlong__c,url1__c=url1__c);',\n  AllowQuotedNewlines: 'false',\n  CDDLAction: 'Process',\n  adapterName: 'BigQueryWriter',\n  serviceAccountKey: '@GCS-AuthPath@',\n  optimizedMerge: 'true',\n  StandardSQL: 'true',\n  includeInsertId: 'true',\n  QuoteCharacter: '\\\"',\n  BatchPolicy: 'eventCount:1000,Interval:10' )\nINPUT FROM @STREAM@;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\n\nSTOP APPLICATION TQLwithinTqlTester.TQLwithinTqlApp;\nUNDEPLOY APPLICATION TQLwithinTqlTester.TQLwithinTqlApp;\nDROP APPLICATION TQLwithinTqlTester.TQLwithinTqlApp CASCADE;\n\nCREATE APPLICATION TQLwithinTqlApp;\n\n@@FEATURE-DIR@/tql/TQLwithinTQL2.tql;\n@@FEATURE-DIR@/tql/TQLwithinTQL4.tql;\n\nEND APPLICATION TQLwithinTqlApp;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application SybaseJaguarApp;\n\ncreate source SybaseJaguarSource using FileReader (\n        directory:'@TEST-DATA-PATH@',\n        WildCard:'SybaseJaguar.log',\n        charset:'ISO-8859-1',\n        positionByEOF:false\n)\nparse using FreeformTextParser (\n        -- TimeStamp:'%E %mon %d %H:%M:%S %y %z',\n        RecordBegin:'%H:%M:%S:%sss %yyyy%mm%d:1234567890:\\n[TRACE]',\n        RecordEnd:')\\n',\n\tIgnoreMultipleRecordBegin: false,\t\n        regex:'((?<=logonname\\\\()[a-z,0-9]*)|((?<=localHostName\\\\()[a-z,A-Z,0-9]*)|((?<=localIP\\\\()[0-9,.]*)',\n        separator:'~'\n)\nOUTPUT TO SybaseJaguarStream;\n\nCREATE TYPE LoginInfo (\n        userName String,\n        hostName String,\n        hostIP  String,\n        eventTime long\n);\n\ncreate stream LoginInfoStream of LoginInfo;\n\ncreate cq LoginInfoCQ\ninsert into LoginInfoStream\nselect\n        data[0],\n        data[1],\n        data[2],\n        TO_LONG(META(x,'OriginTimestamp'))\nfrom SybaseJaguarStream x;\n\n\ncreate Target SybaseJaguarDump using CSVWriter(filename:'@FEATURE-DIR@/logs/SybaseJaguar.log') input from LoginInfoStream;\nend application SybaseJaguarApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE PROPERTYVARIABLE Mode='sync';\nCREATE OR REPLACE PROPERTYVARIABLE BatchPolicy='Size:900000,Interval:1';\ncreate application KinesisTest;\ncreate source CSVSource using FileReader (\n\tdirectory:'/home/dz/src/product/Samples/AppData',\n\tWildCard:'posdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  companyName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0], data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate or replace Target t using KinesisWriter (\n\tregionName:'TARGET_REGION',\n\tstreamName:'TARGET_STREAM',\n\tBatchPolicy: '$BatchPolicy',\n    Mode: '$Mode'\n)\nformat using XMLFormatter (\n\trootelement:'document',\n\telementtuple:'CompanyName:merchantid:text=companyname,ZipCode:text=zip,Amount:text=amount'\n)\ninput from TypedCSVStream;\nend application KinesisTest;\ndeploy application KinesisTest in default;\nstart application KinesisTest;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop application Postgres_To_PostgresApp;\nundeploy application Postgres_To_PostgresApp;\ndrop application Postgres_To_PostgresApp cascade;\n\nCREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE Postgres_Src1 USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot_1',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.tablename0'\n ) \nOUTPUT TO Change_Data_Stream ;\n\nCREATE OR REPLACE SOURCE Postgres_Src2 USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot_2',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.tablename1'\n ) \nOUTPUT TO Change_Data_Stream ;\n\n\nCREATE OR REPLACE SOURCE Postgres_Src3 USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot_3',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.tablename2'\n ) \nOUTPUT TO Change_Data_Stream ;\n\nCREATE OR REPLACE SOURCE Postgres_Src4 USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot_4',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.tablename3'\n ) \nOUTPUT TO Change_Data_Stream ;\n\n\nCREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM Change_Data_Stream;\n\nCREATE OR REPLACE TARGET PostgreSQL_Tgt1 USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy: 'EventCount:1,Interval:60',\nCommitPolicy: 'EventCount:1,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.tablename0, public.target0;public.tablename1, public.target0;public.tablename2, public.target0;public.tablename3, public.target0;',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM Change_Data_Stream;\n\nCREATE OR REPLACE TARGET PostgreSQL_Tgt2 USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy: 'EventCount:1,Interval:60',\nCommitPolicy: 'EventCount:1,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.tablename0, public.target1;public.tablename1, public.target1;public.tablename2, public.target1;public.tablename3, public.target1;',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM Change_Data_Stream;\n\nCREATE OR REPLACE TARGET PostgreSQL_Tgt3 USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy: 'EventCount:1,Interval:60',\nCommitPolicy: 'EventCount:1,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.tablename0, public.target2;public.tablename1, public.target2;public.tablename2, public.target2;public.tablename3, public.target2;',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM Change_Data_Stream;\n\nCREATE OR REPLACE TARGET PostgreSQL_Tgt4 USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy: 'EventCount:1,Interval:60',\nCommitPolicy: 'EventCount:1,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.tablename0, public.target3;public.tablename1, public.target3;public.tablename2, public.target3;public.tablename3, public.target3;',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM Change_Data_Stream;\n\n\nend application Postgres_To_PostgresApp;\ndeploy application Postgres_To_PostgresApp;\nstart Postgres_To_PostgresApp;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');\n\nCREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.WAEvent PERSIST USING @APPNAME@KafkaPropset;\n\nCREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader ()\nPARSE USING DSVParser ()\nOUTPUT TO @APPNAME@PersistStream@RANDOM@;\n\nCREATE OR REPLACE TARGET @APPNAME@_filetrgt USING Global.FileWriter ()\nFORMAT USING Global.JSONFormatter  (\n  members: 'data'\n)\nINPUT FROM @APPNAME@PersistStream@RANDOM@;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use consoletest;\nalter application noApp;\n\nCREATE source CsvDataSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'posdata.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\n\nend application noApp;\n\nalter application noApp recompile;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 99\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> WS\n--\n\nSTOP Recov99Tester.RecovTest99;\nUNDEPLOY APPLICATION Recov99Tester.RecovTest99;\nDROP APPLICATION Recov99Tester.RecovTest99 CASCADE;\nCREATE APPLICATION RecovTest99 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW BasicComponentTestsFlow;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE WactionType (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionType;\n\nCREATE CQ CsvToWaction\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\n-- unpartitioned windows\n\nCREATE WINDOW DataStreamSliding3U\nOVER DataStream KEEP 3 ROWS;\n\nCREATE WINDOW DataStreamSliding11U\nOVER DataStream KEEP 11 ROWS;\n\nCREATE JUMPING WINDOW DataStreamJumping5U\nOVER DataStream KEEP 5 ROWS;\n\nCREATE JUMPING WINDOW DataStreamJumping7U\nOVER DataStream KEEP 7 ROWS;\n\nCREATE JUMPING WINDOW DataStream5MinutesU\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE JUMPING WINDOW DataStream6MinutesU\nOVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;\n\n\n-- partitioned windows\n\nCREATE WINDOW DataStreamSliding3P\nOVER DataStream KEEP 3 ROWS\nPARTITION BY merchantId;\n\nCREATE WINDOW DataStreamSliding11P\nOVER DataStream KEEP 11 ROWS\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStreamJumping5P\nOVER DataStream KEEP 5 ROWS\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStreamJumping7P\nOVER DataStream KEEP 7 ROWS\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream5MinutesP\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6MinutesP\nOVER DataStream KEEP WITHIN 6 MINUTE ON dateTime\nPARTITION BY merchantId;\n\n\n\n\n\n-- THE TEST OUTPUTS\n\n\n-- Wactions01 receives data straight through from the Source\n-- should contain the exact data emitted\nCREATE WACTIONSTORE Wactions_S1 CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions01\nINSERT INTO Wactions01\nSELECT *\nFROM DataStream;\n\n\n\n\n\n\n\n\n-- test test boundaries for jumping/sliding partitioned/unpartitioned aggregate/non-aggregate\n\n-- SLIDING\n\n-- Wactions_SW3UN data goes through an unpartitioned sliding window then a non-aggregate CQ\n\nCREATE WACTIONSTORE Wactions_SW3UN CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions_SW3UN\nINSERT INTO Wactions_SW3UN\nSELECT *\nFROM DataStreamSliding3U;\n\n\n-- Wactions_SW3UN data goes through a partitioned sliding window then a non-aggregate CQ\n\nCREATE WACTIONSTORE Wactions_SW3PN CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions_SW3PN\nINSERT INTO Wactions_SW3PN\nSELECT *\nFROM DataStreamSliding3P;\n\n\n-- Wactions_SW3UN data goes through an unpartitioned sliding window then an aggregate CQ\n\nCREATE WACTIONSTORE Wactions_SW3UA CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions_SW3UA\nINSERT INTO Wactions_SW3UA\nSELECT FIRST(*)\nFROM DataStreamSliding3U;\n\n\n-- Wactions_SW3UN data goes through a partitioned sliding window then an aggregate CQ\n\nCREATE WACTIONSTORE Wactions_SW3PA CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions_SW3PA\nINSERT INTO Wactions_SW3PA\nSELECT FIRST(*)\nFROM DataStreamSliding3P;\n\n\n\n-- JUMPING\n\n\n-- Wactions_JW3UN data goes through an unpartitioned jumping window then a non-aggregate CQ\n\nCREATE WACTIONSTORE Wactions_JW5UN CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions_JW5UN\nINSERT INTO Wactions_JW5UN\nSELECT *\nFROM DataStreamJumping5U;\n\n\n-- Wactions_JW5UN data goes through a partitioned sliding jumping then a non-aggregate CQ\n\nCREATE WACTIONSTORE Wactions_JW5PN CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions_JW5PN\nINSERT INTO Wactions_JW5PN\nSELECT *\nFROM DataStreamJumping5P;\n\n\n-- Wactions_JW5UN data goes through an unpartitioned jumping window then an aggregate CQ\n\nCREATE WACTIONSTORE Wactions_JW5UA CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions_JW5UA\nINSERT INTO Wactions_JW5UA\nSELECT FIRST(*)\nFROM DataStreamJumping5U;\n\n\n-- Wactions_JW5UN data goes through a partitioned jumping window then an aggregate CQ\n\nCREATE WACTIONSTORE Wactions_JW5PA CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions_JW5PA\nINSERT INTO Wactions_JW5PA\nSELECT FIRST(*)\nFROM DataStreamJumping5P;\n\n\n\n\n\n\n\nEND FLOW BasicComponentTestsFlow;\n\n\n\n\n\n\n\n\n\n\nCREATE FLOW ComplexScenariosFlow;\n\n\n\n\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\n\n\n\n\n\n\n\n-- Wactions03 data goes through an unpartitioned jumping window then an aggregate CQ\n-- should contain one waction for every 5 events\nCREATE WACTIONSTORE Wactions03 CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions03\nINSERT INTO Wactions03\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.merchantId),\n    FIRST(p.dateTime),\n    SUM(p.amount),\n    FIRST(p.city)\nFROM DataStreamJumping5 p;\n\n\n-- Wactions04 data goes through a size-6 unpartitioned jumping window then an aggregate CQ\n-- should contain one waction for every 6 events\nCREATE WACTIONSTORE Wactions04 CONTEXT OF WactionType\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data6ToWactios04\nINSERT INTO Wactions04\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.merchantId),\n    FIRST(p.dateTime),\n    SUM(p.amount),\n    FIRST(p.city)\nFROM DataStream6Minutes p;\n\n\n\n\n\nEND FLOW ComplexScenariosFlow;\n\n\n\n\n\n\nEND APPLICATION RecovTest99;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n--create flow AgentFlow;\nCREATE SOURCE @APPNAME@_S USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.test01',\n  connectionRetryPolicy:'retryInterval=1,maxRetries=3'\n ) \nOUTPUT TO @APPNAME@_SS;\n--end flow AgentFlow;\n--create flow serverFlow;\n--create target sys_out using sysout (name:sysoutput) input from ss;\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'public.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:0',\nStandardSQL:true\t\n) INPUT FROM @APPNAME@_ss;\n--end flow serverFlow;\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\n--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING DatabaseReader (\n  Tables: '',\n  ConnectionURL: '',\n  Password: '',\n  Username: ''\n  )\nOUTPUT TO @APPNAME@stream;\n\nCREATE CQ @APPNAME@CQ\nINSERT INTO @APPNAME@CQStream\nSELECT putUserData(x, 'city',data[6])\nFROM @APPNAME@stream x;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@CQStream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION SystemTimeTester.SystemTimeWindows;\nUNDEPLOY APPLICATION SystemTimeTester.SystemTimeWindows;\nDROP APPLICATION SystemTimeTester.SystemTimeWindows cascade;\n\nCREATE APPLICATION SystemTimeWindows;\n\nCREATE TYPE RandomData(\nmyName String,\nstreetAddress String,\nbankName String,\nbankNumber int KEY,\nbankAmount double\n);\n\n\nCREATE SOURCE ranDataSource using StreamReader(\nOutputType: 'SystemTimeTester.RandomData',\nnoLimit: 'false',\nisSeeded: 'true',\nmaxRows: 0,\niterations: 30,\niterationDelay: 1000,\nStringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',\nNumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]R'\n)OUTPUT TO CSVDataStream;\n\nCREATE STREAM RandomDataStream OF RandomData;\n\nCREATE CQ ParseRandomData\nINSERT INTO RandomDataStream\nSELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4])\nFROM CSVDataStream;\n\nCREATE @WINDOWTYPE@ WINDOW tierone OVER RandomDataStream keep within 20 second;\n\nCREATE STREAM onetwostream OF RandomData;\n\nCREATE CQ onetwocq\nINSERT INTO onetwostream\nSELECT *\nFROM tierone;\n\nCREATE @WINDOWTYPE@ WINDOW tiertwo OVER onetwostream keep within 40 second;\n\nCREATE STREAM twothreestream OF RandomData;\n\nCREATE CQ twothreecq\nINSERT INTO twothreestream\nSELECT *\nFROM tiertwo;\n\nCREATE @WINDOWTYPE@ WINDOW tierthree OVER twothreestream keep within 1 minute;\n\nCREATE WACTIONSTORE MyDataActivity\nCONTEXT OF RandomData\nEVENT TYPES(RandomData )\n@PERSIST-TYPE@\n\nCreate CQ TrackMyDataActivity\nINSERT INTO MyDataActivity\nSelect * from @FROMSTREAM@\nLINK SOURCE EVENT;\n\n\nEND APPLICATION SystemTimeWindows;\ndeploy application SystemTimeWindows;\nstart application SystemTimeWindows;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 5 with Jumping window and partitioned\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS\n--\n\nSTOP KStreamRecov5Tester.KStreamRecovTest5;\nUNDEPLOY APPLICATION KStreamRecov5Tester.KStreamRecovTest5;\nDROP APPLICATION KStreamRecov5Tester.KStreamRecovTest5 CASCADE;\nDROP USER KStreamRecov5Tester;\nDROP NAMESPACE KStreamRecov5Tester CASCADE;\nCREATE USER KStreamRecov5Tester IDENTIFIED BY KStreamRecov5Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov5Tester;\nCONNECT KStreamRecov5Tester KStreamRecov5Tester;\n\nCREATE APPLICATION KStreamRecovTest5 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream OF CsvData PARTITION BY merchantId;\n\nCREATE CQ CsvToData\nINSERT INTO DataStream\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND APPLICATION KStreamRecovTest5;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@app3;\nundeploy application @APPNAME@app3;\nalter application @APPNAME@app3;\nCREATE or replace TARGET @APPNAME@app3_targetNew USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test01,QATEST.KPS3_Alter'\n) INPUT FROM @APPNAME@sourcestream;\nalter application @APPNAME@app3 recompile;\ndeploy application @APPNAME@app3;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet\n(\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OJet',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  Username: '@SOURCE_USER@'\n)OUTPUT TO @STREAM@;\n\ncreate TYPE CountTYPE(numcol INT);\n\nCREATE JUMPING WINDOW nEvents OVER @STREAM@ KEEP 10 ROWS;\n\nCREATE STREAM TypedCountStream of CountTYPE;\n\nCREATE CQ CountCQ INSERT INTO TypedCountStream SELECT TO_INT(data[0]) FROM nEvents;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@;\n\nCREATE SOURCE @SourceName@ USING OracleReader  (\nReaderType: 'LogMiner',\n  Password_encrypted: 'false',\n  SupportPDB: false,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  QueueSize: 2048,\n  CommittedTransactions: true,\n  Username: '@UserName@',\n  TransactionBufferType: 'Memory',\n  _h_ReturnDateTimeAs: 'ZonedDateTime',\n  TransactionBufferDiskLocation: '.striim/LargeBuffer',\n  OutboundServerProcessName: 'WebActionXStream',\n  Password: '@Password@',\n  DDLCaptureMode: 'All',\n  Compression: false,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  FetchSize: 1,\n  Tables: '@SourceTables@',\n  DictionaryMode: 'OnlineCatalog',\n  XstreamTimeOut: 600,\n  TransactionBufferSpilloverSize: '1MB',\n  StartTimestamp: 'null',\n  FilterTransactionBoundaries: true,\n  StartSCN: null,\n  ConnectionURL: '@ConnectionURL@',\n  SendBeforeImage: true )\nOUTPUT TO @AppStream@  ;\n\nCREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream SELECT data[2], to_string(data[2], \"dd-MMM-yy hh.mm.ss\") FROM @AppStream@ o ;\n\nCREATE  TARGET @targetsys@ USING Global.SysOut  (\nname: 'ora1_sys' )\nINPUT FROM admin.ZDT_cq_stream;\n\ncreate Target @TargetFile@ using FileWriter(\n  filename:'toStringOut.log',\n  directory:'@FilePath@',\n  rolloverpolicy:'eventcount:1000'\n)\nformat using DSVFormatter (\n\n)\ninput from admin.ZDT_cq_stream;\n\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @appname@;\nundeploy application @appname@;\nDROP APPLICATION @appname@ CASCADE;\nCREATE APPLICATION @appname@;\n\nCREATE SOURCE @appname@_src USING databaseReader  (\n  Username: '@@',\n  Password: '@@',\n  ConnectionURL: '@@',\n  Tables: '@@',\n  FetchSize: '100'\n )\nOUTPUT TO @appname@_ss;\n\n--CREATE JUMPING WINDOW @appname@_win OVER @appname@_ss KEEP @winsize@ ROWS;\n\nCREATE TYPE @appname@_MapType\n    (   \n       id INTEGER,\n        name STRING,\n        city  STRING\n    );\n    \nCREATE EXTERNAL CACHE @appname@_cach (\n  AdapterName: 'DatabaseReader',\n    ConnectionURL: '@url@',\n    UserName: '@uname@',\n    Password: '@pwd@',\n   Table: '@tablename@',\n  FetchSize: 100,\n  Columns: 'id,name,city',\n  trimquote: false,\n  KeyToMap: '@key@'\n )\n OF @appname@_MapType;\n \nCREATE TYPE @appname@_MapTypenew\n    (   id_t            INTEGER,\n        name_t           STRING,\n        city_t            STRING,\n        id_c            INTEGER,\n        name_c            STRING,\n        city_c            STRING\n    );\n    \nCREATE STREAM @appname@_JoinedData OF @appname@_MapTypenew;\n\nCREATE CQ @appname@_JoinDataCQ\nINSERT INTO @appname@_JoinedData\nSELECT  TO_INT(f.data[0]),\n        TO_STRING(f.data[1]),\n        TO_STRING(f.data[2]),\n        z.id,\n        z.name,\n        z.city\nFROM @appname@_ss f, @appname@_cach z\nwhere TO_INT(f.data[0]) = z.id\n@Ex@;\n\nCREATE TARGET @appname@_tgt USING DatabaseWriter\n(\n  ConnectionURL:'@@',\n  Username:'@@',\n  Password:'@@',\n  BatchPolicy:'Eventcount:10000,Interval:1',\n  CommitPolicy:'Interval:1,Eventcount:10000',\n  Tables:'@@'\n) \nINPUT FROM @appname@_JoinedData;\n\nEND APPLICATION @appname@;\ndeploy application @appname@;\nstart @appname@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application CDCTester.CDCTest;\nundeploy application CDCTester.CDCTest;\ndrop application CDCTester.CDCTest cascade;\n\ncreate application CDCTest;\n\nCreate Source Rac11g Using OracleReader\n(\n Username:'@LOGMINER-UNAME@',\n Password:'@LOGMINER-PASSWORD@',\n ConnectionURL:'@LOGMINER-URL@',\n Tables:$Tables,\n FetchSize:$FetchSize,\n QueueSize:$QueueSize\n\n)\nOutput To LCRStream;\n\n\nend application CDCTest;\ndeploy application CDCTest;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE TYPE @APPNAME@type1 (\n companyName java.lang.String,\n merchantId java.lang.String\n );\n\nCREATE STREAM @APPNAME@TypedStream OF @APPNAME@type1;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (\n  wildcard: '',\n  positionByEOF: false,\n  directory: ''\n  )\nPARSE USING DSVParser (\nheader:'true'\n)\nOUTPUT TO @APPNAME@Stream;\n\nCREATE OR REPLACE CQ @APPNAME@CQ\nINSERT INTO @APPNAME@TypedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\") as companyName,\nTO_STRING(data[1]) as merchantId\nFROM @APPNAME@Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_jmstrgt USING JMSWriter (\n  QueueName: '',\n  UserName: '',\n  Password: '',\n  Ctx: '',\n  Provider: ''\n  )\nFORMAT USING XMLFormatter(\n  rootelement:'JMSXMLIN',\n  elementtuple:'companyName:merchantId:text=companyName'\n)\nINPUT FROM @APPNAME@TypedStream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop DataGenSampleApp;\nundeploy application DataGenSampleApp;\ndrop application DataGenSampleApp cascade;\n\n\nCREATE APPLICATION DataGenSampleApp;\n\nCreate Source dataGenSrc USING OracleReader  ( \n  FetchSize: 1,\n  QueueSize: 2048,\n  CommittedTransactions: true,\n  Compression: false,\n  Username: 'qatest',\n  Password: 'r+g6o0bDETs=',\n  ConnectionURL: 'localhost:1521:xe',\n  FilterTransactionState: true,\n  DictionaryMode: 'OnlineCatalog',\n  ReaderType: 'LogMiner',\n  Tables: '@tableNames@',\n  Password_encrypted: true\n ) \n Output To LCRStream;\n\ncreate Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;\n\nEND APPLICATION DataGenSampleApp;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application FileWriterTest;\nundeploy application FileWriterTest;\ndrop application FileWriterTest cascade;\ncreate application FileWriterTest recovery 1 second interval;\ncreate source File_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tcharset:'@charset@'\n)\nparse using JSONParser (\n\teventType:'@evty@',\n\tfieldName:'@fname@'\n)\nOUTPUT TO CsvStream;\n\ncreate Target FileTarget using FileWriter(\n    rolloverpolicy:'eventcount:100',\n    filename :'@FILE@',\n    directory : '@FOLDER@'\n)\nformat using JSONFormatter (\n)\ninput from CsvStream;\n\nend application FileWriterTest;\ndeploy application FileWriterTest on all in default;\nstart application FileWriterTest;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE CQ @CQ_NAME@\nINSERT INTO @EMB_STREAM@\n@SELECT_QUERY@\nFROM @STREAM@ e;\n\nCREATE TARGET @CQ_NAME@_sysout USING Global.SysOut (\n  name: '@CQ_NAME@_sysout' )\nINPUT FROM @EMB_STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Kafka Stream Recovery Test 1\n-- Bert Hashemi and Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> KS -> WS\n\nSTOP KStreamRecov1Tester.KStreamRecovTest1;\nUNDEPLOY APPLICATION KStreamRecov1Tester.KStreamRecovTest1;\nDROP APPLICATION KStreamRecov1Tester.KStreamRecovTest1 CASCADE;\nDROP USER KStreamRecov1Tester;\nDROP NAMESPACE KStreamRecov1Tester CASCADE;\nCREATE USER KStreamRecov1Tester IDENTIFIED BY KStreamRecov1Tester;\n-- GRANT 'Global:create,drop:deploymentgroup:*' TO USER KStreamRecov1Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov1Tester;\nCONNECT KStreamRecov1Tester KStreamRecov1Tester;\n\nCREATE APPLICATION KStreamRecovTest1 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE or REPLACE TYPE KafkaCsvStreamType(\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE Wactions CONTEXT OF KafkaCsvStreamType \nEVENT TYPES ( KafkaCsvStreamType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions\nINSERT INTO Wactions\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream;\n\nEND APPLICATION KStreamRecovTest1;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application sorted;\n\ncreate source AALSortedSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'sorted_log',\n  charset:'UTF-8',\n  positionByEOF:false\n) PARSE USING AALParser (\n  columndelimiter:' ',\n  IgnoreEmptyColumn:'Yes',\n  columndelimittill:5\n) OUTPUT TO AalSortedStream;\n\ncreate Target AALSortedDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/sorted_log') input from AalSortedStream;\n\nend application sorted;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION admin.PosApp;\nUNDEPLOY APPLICATION admin.PosApp;\nDROP APPLICATION admin.PosApp cascade;\ndrop namedquery admin.PosAppMainPageMap;\n\ndrop namedquery admin.PosAppMainPageBar;\n\ndrop namedquery admin.PosAppMainPageBar2;\ndrop namedquery admin.PosAppMainPageScatter;\n\ndrop namedquery admin.PosAppHeatMapDrilldownHeatMap;\ndrop namedquery admin.PosAppHeatMapDrilldownDonuts;\ndrop namedquery admin.PosAppMainPageHeatMap;\ndrop namedquery admin.PosAppCompanyDrilldownCharts;\ndrop namedquery admin.PosAppHeatMapDrilldownMap;\ndrop namedquery admin.PosAppMainSearchBox;\ndrop namedquery admin.PosAppMcount;\n\ndrop user PosTester;\n\ndrop namespace PosTester cascade;\ndrop dashboard admin.PosAppDash;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE TYPE @APPNAME@_completeRecord(\ncompletedata com.fasterxml.jackson.databind.JsonNode);\nCREATE stream @APPNAME@_CompleteRecordInJSONStream of @APPNAME@_completeRecord;\n\nCREATE SOURCE @APPNAME@_src USING KafkaReader VERSION @KAFKA_VERSION@ ()\nPARSE USING AvroParser (\nschemaFileName: 'avroSchema'\n)\nOUTPUT TO @APPNAME@_Stream2;\n\nCREATE CQ @APPNAME@_CQ1\n INSERT INTO @APPNAME@_CompleteRecordInJSONStream\n SELECT\n AvroToJson(y.data)\n from @APPNAME@_Stream2 y;\n\nCREATE CQ @APPNAME@_GetNativeRecordInJSONCQ\nINSERT INTO @APPNAME@_NativeRecordStream\nSELECT\n completedata.get(\"Facility\").toString() as Facility,\n completedata.get(\"MsgTime\").toString() as MsgTime,\n completedata.get(\"PatientID\").toString() as PatientID,\n completedata.get(\"OrderIdentifier\").toString() as OrderIdentifier,\n completedata.get(\"OrderText\").toString() as OrderText\nFROM @APPNAME@_CompleteRecordInJSONStream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING Global.FileWriter ()\nFORMAT USING JSONFormatter()\nINPUT FROM @APPNAME@_NativeRecordStream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE OR REPLACE SOURCE DBSource USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n )\nINPUT FROM Oracle_ChangeDataStream;\n\n\ncreate Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\nEND APPLICATION DBRTOCW;\n\ndeploy application DBRTOCW on ANY in default;\n\nstart application DBRTOCW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @appname@;\nundeploy application @appname@;\nDROP APPLICATION @appname@ CASCADE;\nCREATE APPLICATION @appname@;\n\nCREATE SOURCE @appname@_src USING databaseReader  (\n  Username: '@@',\n  Password: '@@',\n  ConnectionURL: '@@',\n  Tables: '@@',\n  FetchSize: '100'\n )\nOUTPUT TO @appname@_ss;\n\n----1st set of window and cache\n\nCREATE JUMPING WINDOW @appname@_win OVER @appname@_ss KEEP @winsize@ ROWS;\n\nCREATE TYPE @appname@_MapType\n    (   \n       id INTEGER,\n        name STRING,\n        city  STRING\n    );\n    \nCREATE EXTERNAL CACHE @appname@_cach (\n  AdapterName: 'DatabaseReader',\n    ConnectionURL: '@url@',\n    UserName: '@uname@',\n    Password: '@pwd@',\n   Table: '@tablename@2',\n  FetchSize: 100,\n  Columns: 'id,name,city',\n  trimquote: false,\n  KeyToMap: '@key@'\n )\n OF @appname@_MapType;   \n \nCREATE TYPE @appname@_MapTypenew\n    (   id_t            INTEGER,\n        name_t           STRING,\n        city_t            STRING,\n        id_c            INTEGER,\n        name_c            STRING,\n        city_c            STRING\n    );\n    \nCREATE STREAM @appname@_JoinedData OF @appname@_MapTypenew;\n\nCREATE CQ @appname@_JoinDataCQ\nINSERT INTO @appname@_JoinedData\nSELECT  TO_INT(f.data[0]),\n        TO_STRING(f.data[1]),\n        TO_STRING(f.data[2]),\n        z.id,\n        z.name,\n        z.city\nFROM @appname@_win f, @appname@_cach z\nwhere TO_INT(f.data[0]) = z.id\n@Ex@;\n\n----2nd set of window and cache\n\nCREATE JUMPING WINDOW @appname@_win2 OVER @appname@_ss KEEP @winsize@ ROWS;\n \n CREATE EXTERNAL CACHE @appname@_cach2 (\n  AdapterName: 'DatabaseReader',\n    ConnectionURL: '@url@',\n    UserName: '@uname@',\n    Password: '@pwd@',\n   Table: '@tablename@3',\n  FetchSize: 100,\n  Columns: 'id,name,city',\n  trimquote: false,\n  KeyToMap: '@key@'\n )\n OF @appname@_MapType;\n \nCREATE CQ @appname@_JoinDataCQ2\nINSERT INTO @appname@_JoinedData\nSELECT  TO_INT(f.data[0]),\n        TO_STRING(f.data[1]),\n        TO_STRING(f.data[2]),\n        z.id,\n        z.name,\n        z.city\nFROM @appname@_win2 f, @appname@_cach2 z\nwhere TO_INT(f.data[0]) = z.id\n@Ex@;\n\n\n----3rd set of window and cache\n\nCREATE JUMPING WINDOW @appname@_win3 OVER @appname@_ss KEEP @winsize@ ROWS;\n \n CREATE EXTERNAL CACHE @appname@_cach3 (\n  AdapterName: 'DatabaseReader',\n    ConnectionURL: '@url@',\n    UserName: '@uname@',\n    Password: '@pwd@',\n   Table: '@tablename@4',\n  FetchSize: 100,\n  Columns: 'id,name,city',\n  trimquote: false,\n  KeyToMap: '@key@'\n )\n OF @appname@_MapType;\n \nCREATE CQ @appname@_JoinDataCQ3\nINSERT INTO @appname@_JoinedData\nSELECT  TO_INT(f.data[0]),\n        TO_STRING(f.data[1]),\n        TO_STRING(f.data[2]),\n        z.id,\n        z.name,\n        z.city\nFROM @appname@_win3 f, @appname@_cach3 z\nwhere TO_INT(f.data[0]) = z.id\n@Ex@;\n\n\nCREATE TARGET @appname@_tgt USING DatabaseWriter\n(\n  ConnectionURL:'@@',\n  Username:'@@',\n  Password:'@@',\n  BatchPolicy:'Eventcount:10000,Interval:1',\n  CommitPolicy:'Interval:1,Eventcount:10000',\n  Tables:'@@'\n) \nINPUT FROM @appname@_JoinedData;\n\nEND APPLICATION @appname@;\ndeploy application @appname@;\nstart @appname@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION BooleanExpressionsTester.BooleanExpressionsApp;\nUNDEPLOY APPLICATION BooleanExpressionsTester.BooleanExpressionsApp;\nDROP APPLICATION BooleanExpressionsTester.BooleanExpressionsApp CASCADE;\n\nCREATE APPLICATION BooleanExpressionsApp;\n\nCREATE source wsSource USING FileReader (\ndirectory:'@TEST-DATA-PATH@',\nwildcard:'bool.csv',\nblocksize: 10240,\npositionByEOF:false\n)\nPARSE USING DSVParser (\nheader:No,\ntrimquote:false\n) OUTPUT TO QaStream;\n\nCREATE TYPE wsData\n(\nbankbool boolean,\nbankID integer key\n);\n\n\nCREATE WACTIONSTORE oneWS\nCONTEXT OF wsData\nEVENT TYPES(wsData )\n@PERSIST-TYPE@\n\n\n\nCREATE CQ wsToWaction\nINSERT INTO oneWS\nSELECT To_boolean(data[0]),to_int(data[1]) FROM QaStream\nLINK SOURCE EVENT;\n\n\nEND APPLICATION BooleanExpressionsApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE  TARGET @TARGET_NAME@ USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @STREAM@1;\n\n CREATE  TARGET @TARGET_NAME@2 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @STREAM@2;\n\n CREATE  TARGET @TARGET_NAME@3 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @STREAM@3;\n\n CREATE  TARGET @TARGET_NAME@4 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @STREAM@4;\n\n CREATE  TARGET @TARGET_NAME@5 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @STREAM@5;\n\n CREATE  TARGET @TARGET_NAME@6 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @STREAM@6;\n\n CREATE  TARGET @TARGET_NAME@7 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @STREAM@7;\n\n CREATE  TARGET @TARGET_NAME@8 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @STREAM@8;\n\n CREATE  TARGET @TARGET_NAME@9 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @STREAM@9;\n\n CREATE  TARGET @TARGET_NAME@10 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @STREAM@10;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application FileWriterTest;\nundeploy application FileWriterTest;\ndrop application FileWriterTest cascade;\ncreate application FileWriterTest recovery 1 second interval;\ncreate source File_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'false'\n)\nOUTPUT TO CsvStream;\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream; \n\n\ncreate Target FileTarget using FileWriter(\n    rolloverpolicy:'@UPLOAD-SIZE@',\n    filename :'@FILE@',\n    directory : '@FOLDER@'\n)\nformat using AvroFormatter (\nschemafilename:'@charset@',\nformatAs:'@mem@',\nschemaregistryurl:'@head@'\n\n)\ninput from TypedCSVStream;\n\nend application FileWriterTest;\ndeploy application FileWriterTest on all in default;\nstart application FileWriterTest;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--use global;\nCREATE OR REPLACE PROPERTYSET LDAP1 ( PROVIDER_URL:\"@LDAP_URL@\", SECURITY_AUTHENTICATION:@LDAP_AUTH@, SECURITY_PRINCIPAL: \"@LDAP_PRINCIPAL@\" , SECURITY_CREDENTIALS:@LDAP_CRED@, USER_BASE_DN:\"@LDAP_DN@\", User_userId:@LDAP_USERID@ );\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()\nPARSE USING AvroParser ()\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE CQ @APPNAME@_CQ\nINSERT INTO @APPNAME@_CQOut\nSELECT AvroToJson(data,false) FROM @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()\nFORMAT USING JSONFormatter ()\nINPUT FROM @APPNAME@_CQOut;\n\nEND APPLICATION @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop IR;\nundeploy application IR;\ndrop application IR cascade;\nCREATE APPLICATION IR recovery 5 second interval;\n\n CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( \n  FetchSize: 100,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.test01;striim.test02;striim.test03',\n  adapterName: 'IncrementalBatchReader',\n CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',\n startPosition: 'striim.test01=1;striim.test02=-1;%=0',\n  PollingInterval: '5sec'\n  )\n  OUTPUT TO data_stream ;\n\n  CREATE OR REPLACE TARGET sys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM data_stream;\n\ncreate target AzureSQLDWHTarget using AzureSQLDWHWriter(\n\t\tConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        accountname: 'striimqatestdonotdelete',\n        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',\n        uploadpolicy:'eventcount:1,interval:10s'\n) INPUT FROM data_stream;\n\n\nCREATE TARGET log USING LOGWRITER\n(\n name:log,\n filename:'LOGFILENAME'\n)\nINPUT FROM data_stream;\n\n\nEND APPLICATION IR;\n\ndeploy application IR;\nstart IR;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@  RECOVERY 1 SECOND INTERVAL;\nCREATE SOURCE @srcName@ USING OracleReader (\n Username: '@srcusername@',\n  Password: '@srcpassword@',\n  ConnectionURL: '@srcurl@',\n  Tables: '@srcschema@.@srctable@'\n)\nOUTPUT TO @outstreamname@;\n\nCREATE TARGET @tgtName@ USING AzureEventHubWriter (\n  SASKey:'@saasKey@',\n  EventHubNamespace:'@eventhubNamespace@',\n  ConsumerGroup:'@consumerGrp@',\n  SASPolicyName:'RootManageSharedAccessKey',\n  E1P:'true',\n  OperationTimeout:'1m',\n  ConnectionRetryPolicy:'Retries:0,RetryBackOff:1m',\n  EventHubName:'@eventhub@',\n  BatchPolicy:'Size:1000000,Interval:1m'\n)\nFORMAT USING JSONFormatter  (\n  jsonMemberDelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true',\n  members: 'data',\n  jsonobjectdelimiter: '\\n' )\nINPUT FROM @instreamname@;\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\nCREATE APPLICATION DSV;\n\nCreate type ScanResultType (\n  timestamp1 String,\n  rssi String\n);\n\nCreate Stream ScanResultStream of ScanResultType;\n\nCREATE  SOURCE CSVSource USING FileReader (\n  directory: '@TEST-DATA-PATH@',\n  WildCard: 'sample.json',\n  positionByEOF: false\n )\n PARSE USING JSONParser (\n  eventType: 'RollOverTester.ScanResultType',\n  fieldName: 'scanresult'\n )\nOUTPUT TO ScanResultStream;\n\nCREATE TARGET KafkaSYSOUT USING FileWriter (\n  filename: 'EventType',\n  flushinterval: '0',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy: 'eventcount:1,sequence:00'\n )\nformat using JSONFormatter (\n  members:'timestamp1,rssi'\n)\nINPUT FROM ScanResultStream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/EventType_actual.log') input from ScanResultStream;\n\nEND APPLICATION DSV;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (\n  Tables: '',\n  ConnectionURL: '',\n  Password: '',\n  Username: ''\n  )\nOUTPUT TO @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\tSTOP APPLICATION orrs;\n\tUNDEPLOY APPLICATION orrs;\n\tDROP APPLICATION orrs CASCADE;\n\tCREATE APPLICATION orrs;\n\tCreate Source OraSource Using OracleReader \n\t(\n\t Username:'user-name',\t\n\t Password:'password',\n\t ConnectionURL: 'src_url',\n\t Tables:'src_table',\n\t FilterTransactionBoundaries:true,\n\t FetchSize:'fetch-size'\n\t) Output To LCRStream;\n\t\n\tCREATE TARGET RSTarget USING RedshiftWriter\n\t(\n\t  ConnectionURL: 'tgt_url',\n\t  Username: 'tgt_username',\n\t  Password: 'tgt_pwrd',\n\t  bucketname: 'bucket_name',\n\t  --accesskeyId: '@ACCESS-KEY-ID@',\n\t  --secretaccesskey: '@SECRET-ACCESS-KEY@',\n\t  S3IAMRole:'@IAMROLE@',\n\t  Tables: 'tgt_table',\n\t  uploadpolicy:'eventcount:300,interval:1m'\n\t) INPUT FROM LCRStream;\n\t\n\tEND APPLICATION orrs;\n\tdeploy application orrs;\n\tSTART application orrs;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application app2PS;\nundeploy application app2PS;\ndrop application app2PS cascade;\n\ncreate application app2PS;\n\ncreate target File_TargerPS2 using FileWriter\n(\ndirectory : '',\nfilename : ''\n)\nformat using DSVFormatter()\ninput from Recoveryss2;\n\nend application app2PS;\n\ndeploy application app2PS;\nstart application app2PS;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": ""}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION @AppName@;\n\nCreate Source @AppName@_source Using OracleReader(\n  Username:'@username@',\n  Password:'@password@',\n  ConnectionURL:'@url@',\n  Tables:'@srctableName@',\n  Fetchsize:1\n)\nOutput To @AppName@_Stream;\n\nCREATE OR REPLACE TARGET @AppName@_DB_Target USING Global.DeltaLakeWriter (\nconnectionProfileName: 'admin.@DBCP@',\n   useConnectionProfile: 'true',\n  Tables: '@srctableName@,@trgtableName@',\n  uploadPolicy: 'eventcount:100000,interval:60s'\n)\n\nINPUT FROM @AppName@_Stream;\n\nCREATE OR REPLACE TARGET @AppName@_DB_Target2 USING Global.DeltaLakeWriter (\nconnectionProfileName: 'admin.@DBCP@',\n   useConnectionProfile: 'true',\n  Tables: '@srctableName@,@trgtableName@',\n  uploadPolicy: 'eventcount:100000,interval:60s'\n)\n\nINPUT FROM @AppName@_Stream;\n\nEND APPLICATION @AppName@;\nDeploy application @AppName@;\nStart application @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION OneAgentEncryptionTester.CSV;\nUNDEPLOY APPLICATION OneAgentEncryptionTester.CSV;\nDROP APPLICATION OneAgentEncryptionTester.CSV cascade;\n\ncreate application CSV WITH ENCRYPTION;\n\nCREATE FLOW AgentFlow;\ncreate source CSVSource using CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'customerdetails-agent.csv',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO CsvStream1;\n\n\nCREATE TYPE MyTypeCsv(\nPAN String,\nFNAME String KEY,\nLNAME String,\nADDRESS String,\nCITY String,\nSTATE String,\nZIP String,\nGENDER String\n);\n\nCREATE STREAM TypedStreamCsv of MyTypeCsv;\n\nCREATE CQ TypeConversionCQCsv\nINSERT INTO TypedStreamCsv\nSELECT\ndata[0],\ndata[1],\ndata[2],\ndata[3],\ndata[4],\ndata[5],\ndata[6],\ndata[7]\nfrom CsvStream1;\n\nEND FLOW AgentFlow;\n\nCREATE FLOW ServerFlow;\n\nCREATE WACTIONSTORE StoreInfoCsv CONTEXT OF MyTypeCsv\nEVENT TYPES ( MyTypeCsv )\n@PERSIST-TYPE@\n\nCREATE CQ StoreWactionCsv\nINSERT INTO StoreInfoCsv\nSELECT * FROM TypedStreamCsv\nLINK SOURCE EVENT;\n\n\nEND FLOW ServerFlow;\n\nend application CSV;\n\nDEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;\nSTART CSV;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION routerApp;\nUNDEPLOY APPLICATION routerApp;\nDROP APPLICATION routerApp CASCADE;\n\n\nCREATE APPLICATION routerApp;\n\nCREATE  SOURCE OraSource USING OracleReader  (\nUsername: 'qatest',\nPassword: 'qatest',\nConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',\nTables: 'QATEST.TGT_T%',\n FetchSize:'100'\n)\nOUTPUT TO MasterStream1;\n\nCREATE OR REPLACE ROUTER tablerouter1 INPUT FROM MasterStream1 s CASE\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T1' THEN ROUTE TO ss1,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T2' THEN ROUTE TO ss2,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T3' THEN ROUTE TO ss3,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T4' THEN ROUTE TO ss4,\nELSE ROUTE TO ss_else;\n\ncreate Target FileTarget_1 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n)\nFORMAT USING dsvFormatter ()\ninput from ss1;\n\ncreate Target FileTarget_2 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n)\nFORMAT USING dsvFormatter ()\ninput from ss2;\n\ncreate Target FileTarget_3 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n\n)\nFORMAT USING dsvFormatter ()\ninput from ss3;\n\ncreate Target FileTarget_4 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n\n)\nFORMAT USING dsvFormatter ()\ninput from ss4;\n\n\ncreate Target FileTarget_5 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n\n)\nFORMAT USING dsvFormatter ()\ninput from ss_else;\n\n\nend application routerApp;\ndeploy application routerApp;\nstart routerApp;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop APPLICATION RollOverTester.DSV;\n\nUNDEPLOY APPLICATION RollOverTester.DSV;\nDROP APPLICATION RollOverTester.DSV CASCADE;\nCREATE APPLICATION DSV;\n\ncreate source JsonSource using FileReader (\n        directory:'@TEST-DATA-PATH@',\n        WildCard:'TextInput.txt',\n        positionByEOF:false,\n        charset:'UTF-8'\n)\n PARSE USING JSONParser ( \n  eventType: ''\n ) \nOUTPUT TO activeguard_rawstream;\n\n\nCREATE OR REPLACE TYPE activeguard_parsed (\n\t-- Enriched, or not in AGI left navigation\n\thash_key String KEY,\n\tclient_id Long,\n\tevent_type_id Long,\n\ttype_code String,\n\trule_id Long,\n\trule_name String,\n\trule_action String,\n\tsce_id  Long,\n\tmessage String,\n\tref_list_match String,\n\tevent_date DateTime,\n\tlog_count Long,\n\t\n\t-- As seen in AGI\n\t-- frequently used, but not duplicated\n\treceived_date DateTime,\n\tevent_sub_source String,\n\t\t\t\t\t\n\t-- database\n\tsol_action String,\n\tcomment String,\n\tcomment_text String,\n\tdatabase_name String,\n\tdatabase_user_name String,\n\tentry_id String,\n\tobj_name String,\n\tos_user_name String,\n\towner String,\n\tpriv_used String,\n\treturn_code String,\n\tsession_id String,\n\t\n\t-- geo\n\tdestination_country_code String,\n\tsource_country_code String,\n\t\n\t-- network\n\tdata_size String,\n\tdestination String,\n\tdestination_object String,\n\tdestination_port String,\n\tsol_domain String,\n\tevent_manager String,\n\tevent_path String,\n\tevent_source String,\n\tmsg_no String,\n\tprotocol_id String,\n\tservice String,\n\tsol_source String,\n\tsource_mac_address String,\n\tsource_object String,\n\tsource_port String,\n\tstatus_code String,\n\ttarget_path String,\n\t\n\t-- other\n\tcommand String,\n\t\n\t-- security\n\taffected_file String,\n\tobject_action String,\n\tobject_action_result String,\n\tobject_id String,\n\tobject_name String,\n\tobject_path String,\n\tprocess_name String,\n\t\n\t-- user\n\tcaller_user_name String,\n\tlogon_type String,\n\ttarget_user_name String,\n\tuser_agent String,\n\tuser_name String,\n\tworkstation String,\n\t\n\t-- web\n\tweb_file String,\n\tweb_method String,\n\tweb_page String,\n\tweb_protocol String\n);\n\n\nCREATE OR REPLACE STREAM activeguard_stream OF activeguard_parsed;\n\nCREATE CQ Parseactiveguard_stream \nINSERT INTO activeguard_stream\nSELECT\n\n\t-- Enriched, or not in AGI left navigation\n\tdata.get('hash_key').textValue() as hash_key,\n\tdata.get('client_id').longValue() as client_id,\n\tdata.get('event_type_id').longValue() as event_type_id,\n\tdata.get('type_code').textValue() as type_code,\n\tdata.get('rule_id').longValue() as rule_id,\n\tdata.get('rule_name').textValue() as rule_name,\n\tdata.get('rule_action').textValue() as rule_action,\n\tdata.get('sce_id').longValue() as sce_id,\n\tdata.get('message').textValue() as message,\n\tdata.get('ref_list_match').textValue() as ref_list_match,\n\tTO_DATE(data.get('event_date').textValue()) as event_date,\n\tdata.get('log_count').longValue() as log_count,\n\t\n\t-- As seen in AGI\n\t-- frequently used, but not duplicated\n\tTO_DATE(data.get('received_date').textValue()) as received_date,\n\tdata.get('event_sub_source').textValue() as event_sub_source,\n\t\t\t\t\t\n\t-- database\n\tdata.get('sol_action').textValue() as sol_action,\n\tdata.get('comment').textValue() as comment,\n\tdata.get('comment_text').textValue() as comment_text,\n\tdata.get('database_name').textValue() as database_name,\n\tdata.get('database_user_name').textValue() as database_user_name,\n\tdata.get('entry_id').textValue() as entry_id,\n\tdata.get('obj_name').textValue() as obj_name,\n\tdata.get('os_user_name').textValue() as os_user_name,\n\tdata.get('owner').textValue() as owner,\n\tdata.get('priv_used').textValue() as priv_used,\n\tdata.get('return_code').textValue() as return_code,\n\tdata.get('session_id').textValue() as session_id,\n\t\n\t-- geo\n\tdata.get('destination_country_code').textValue() as destination_country_code,\n\tdata.get('source_country_code').textValue() as source_country_code,\n\t\n\t-- network\n\tdata.get('data_size').textValue() as data_size,\n\tdata.get('destination').textValue() as destination,\n\tdata.get('destination_object').textValue() as destination_object,\n\tdata.get('destination_port').textValue() as destination_port,\n\tdata.get('sol_domain').textValue() as sol_domain,\n\tdata.get('event_manager').textValue() as event_manager,\n\tdata.get('event_path').textValue() as event_path,\n\tdata.get('event_source').textValue() as event_source,\n\tdata.get('msg_no').textValue() as msg_no,\n\tdata.get('protocol_id').textValue() as protocol_id,\n\tdata.get('service').textValue() as service,\n\tdata.get('sol_source').textValue() as sol_source,\n\tdata.get('source_mac_address').textValue() as source_mac_address,\n\tdata.get('source_object').textValue() as source_object,\n\tdata.get('source_port').textValue() as source_port,\n\tdata.get('status_code').textValue() as status_code,\n\tdata.get('target_path').textValue() as target_path,\n\t\n\t-- other\n\tdata.get('command').textValue() as command,\n\t\n\t-- security\n\tdata.get('affected_file').textValue() as affected_file,\n\tdata.get('object_action').textValue() as object_action,\n\tdata.get('object_action_result').textValue() as object_action_result,\n\tdata.get('object_id').textValue() as object_id,\n\tdata.get('object_name').textValue() as object_name,\n\tdata.get('object_path').textValue() as object_path,\n\tdata.get('process_name').textValue() as process_name,\n\t\n\t-- user\n\tdata.get('caller_user_name').textValue() as caller_user_name,\n\tdata.get('logon_type').textValue() as logon_type,\n\tdata.get('target_user_name').textValue() as target_user_name,\n\tdata.get('user_agent').textValue() as user_agent,\n\tdata.get('user_name').textValue() as user_name,\n\tdata.get('workstation').textValue() as workstation,\n\t\n\t-- web\n\tdata.get('web_file').textValue() as web_file,\n\tdata.get('web_method').textValue() as web_method,\n\tdata.get('web_page').textValue() as web_page,\n\tdata.get('web_protocol').textValue() as web_protocol\n\t\nFROM activeguard_rawstream;\n\n\nCREATE TARGET alertf USING FileWriter ( directory: '@FEATURE-DIR@/logs', filename: 'TestOutput.txt', rolloverpolicy:'EventCount:10000,Interval:30s' )\n FORMAT USING JSONFormatter (\n\t\tmembers:'hash_key,client_id,event_type_id,type_code,rule_id,rule_name,rule_action,sce_id,message,ref_list_match,event_date,log_count,received_date,event_sub_source,sol_action,comment,comment_text,database_name,database_user_name,entry_id,obj_name,os_user_name,owner,priv_used,return_code,session_id,destination_country_code,source_country_code,data_size,destination,destination_object,destination_port,sol_domain,event_manager,event_path,event_source,msg_no,protocol_id,service,sol_source,source_mac_address,source_object,source_port,status_code,target_path,command,affected_file,object_action,object_action_result,object_id,object_name,object_path,process_name,caller_user_name,logon_type,target_user_name,user_agent,user_name,workstation,web_file,web_method,web_page,web_protocol',\n \t\trowdelimiter:'\\n'\n )\n--format using dsvformatter()\n INPUT FROM activeguard_stream;\n\n \nEND APPLICATION DSV;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appName@;\nundeploy application @appName@;\ndrop application @appName@ cascade;\n\nCREATE OR REPLACE APPLICATION @appName@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE @appName@_Source USING PostgreSQLReader (\nUsername: '@Username@',\nConnectionURL: '@ConnectionURL@',\nReplicationSlotName: '@ReplicationSlotName@',\nTables: '@srcTable@',\nPassword: '@Password@' )\nOUTPUT TO @appName@_Stream;\n\nCREATE OR REPLACE TARGET @appName@_Target USING Global.BigQueryWriter (\nColumnDelimiter: '|',\nNullMarker: 'NULL',\nstreamingUpload: 'false',\nprojectId: '@projectId@',\nEncoding: 'UTF-8',\nConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 ,\nmaxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30',\nAllowQuotedNewLines: 'false',\nCDDLAction: 'Process',\nTables: '@srcTable@,@tgtTable@',\noptimizedMerge: 'false',\nTransportOptions: 'connectionTimeout=300,\nreadTimeout=120',\nadapterName: 'BigQueryWriter',\nMode: 'APPENDONLY',   StandardSQL: 'true',\nServiceAccountKey: '@GCS-AuthPath@',\nBatchPolicy: 'eventCount:100',\nQuoteCharacter: '\\\"' )\nINPUT FROM @appName@_Stream;\nEND APPLICATION @appName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP TestAlertsEmail.TestAlertsEmailApp;\nUNDEPLOY APPLICATION TestAlertsEmail.TestAlertsEmailApp;\nDROP APPLICATION TestAlertsEmail.TestAlertsEmailApp CASCADE;\n\nCREATE APPLICATION TestAlertsEmailApp;\n\nCREATE source rawSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:No,\n  wildcard:'@TESTDATAFILE@',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO rawStream;\n\nCREATE STREAM MyAlertStream OF Global.AlertEvent;\nCREATE CQ GenerateMyAlerts\nINSERT INTO MyAlertStream (name, keyVal, severity, flag, message)\nSELECT \"Testing Alerts\", data[0], data[1], data[2], data[3]\nFROM rawStream s;\nCREATE TARGET output2 USING SysOut(name : alertsrecevied) input FROM MyAlertStream;\n\nCREATE SUBSCRIPTION alertSubscription USING EmailAdapter\n(\nSMTPUSER:'@Alerts_Smtpuser@',\n--, ${alerts.smtpuser}\nSMTPUSER:' ${alerts.smtpuser}',\nSMTPPASSWORD:'@Alerts_Smtppassword@',\nsmtpurl:\"@Alerts_Smtpurl@\",\nstarttls_enable:\"@Alerts_Starttls_enable@\",\nsmtp_auth:\"@Alerts_Smtp_auth@\",\nsubject:\"@Alerts_Subject@\",\nemailList:\"@Alerts_Emaillist@\",\nuserIds:\"@Alerts_UserId@\",\nthreadCount:\"@Alerts_Threadcount@\",\n@CONTENTTYPE@\nsenderEmail:\"@Alerts_SenderEmail@\",\n)\nINPUT FROM MyAlertStream;\n\nEND APPLICATION TestAlertsEmailApp;\nDEPLOY APPLICATION TestAlertsEmail.TestAlertsEmailApp;\nSTART TestAlertsEmail.TestAlertsEmailApp;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop OracleToKudu;\nundeploy application OracleToKudu;\ndrop application OracleToKudu cascade;\n\nCREATE APPLICATION OracleToKudu;\n\nCREATE FLOW OracleTokudu1;\n\tCreate Source oracSource1\n\t Using OracleReader\n\t(\n\t Username:'@LOGMINER-UNAME@',\n \tPassword:'@LOGMINER-PASSWORD@',\n \tConnectionURL:'@LOGMINER-URL@',\n \tTables:'@SOURCE_TABLES@',\n \tOnlineCatalog:true,\n \tFetchSize:1\n\t) Output To DataStream1;\n\n\tCREATE TARGET WriteintoKudu1 using KuduWriter (\n\tkuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',\n\tpkupdatehandlingmode:'@MODE@',\n\ttables: '@TARGET_TABLES@',\n\tbatchpolicy: 'EventCount:1,Interval:0')\n\tINPUT FROM DataStream1;\nEND FLOW OracleTokudu1;\n\nCREATE FLOW OracleTokudu2;\n\tCreate Source oracSource2\n\t Using OracleReader\n\t(\n\t Username:'@LOGMINER-UNAME@',\n \tPassword:'@LOGMINER-PASSWORD@',\n \tConnectionURL:'@LOGMINER-URL@',\n \tTables:'@SOURCE_TABLES@',\n \tOnlineCatalog:true,\n \tFetchSize:1\n\t) Output To DataStream2;\n\n\tCREATE TARGET WriteintoKudu2 using KuduWriter (\n\tkuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',\n\tpkupdatehandlingmode:'@MODE@',\n\ttables: '@TARGET_TABLES@',\n\tbatchpolicy: 'EventCount:10,Interval:10')\n\tINPUT FROM DataStream2;\nEND FLOW OracleTokudu2;\n\nCREATE FLOW OracleTokudu3;\n\tCreate Source oracSource3\n\t Using OracleReader\n\t(\n\t Username:'@LOGMINER-UNAME@',\n \tPassword:'@LOGMINER-PASSWORD@',\n \tConnectionURL:'@LOGMINER-URL@',\n \tTables:'@SOURCE_TABLES@',\n \tOnlineCatalog:true,\n \tFetchSize:1\n\t) Output To DataStream3;\n\n\tCREATE TARGET WriteintoKudu3 using KuduWriter (\n\tkuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',\n\tpkupdatehandlingmode:'@MODE@',\n\ttables: '@TARGET_TABLES@',\n\tbatchpolicy: 'EventCount:5,Interval:120')\n\tINPUT FROM DataStream3;\nEND FLOW OracleTokudu3;\n\nEND APPLICATION OracleToKudu;\ndeploy application OracleToKudu in default;\nstart OracleToKudu;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION LoadMarketsAppTester.LoadMarketsApp;\nUNDEPLOY APPLICATION LoadMarketsAppTester.LoadMarketsApp;\nDROP APPLICATION LoadMarketsAppTester.LoadMarketsApp CASCADE;\n\nCREATE APPLICATION LoadMarketsApp;\n\nCREATE OR REPLACE SOURCE MarketSource USING FileReader (\n directory:'@TEST-DATA-PATH@',\n  wildcard: 'marketlocations.json',\n  blocksize: '64',\n  charset: 'UTF-8',\n  positionbyeof: 'false',\n  rolloverpolicy: 'DefaultFileComparator',\n  skipbom: 'true'\n )\n PARSE USING JSONParser (\n  fieldName: 'locations'\n )\nOUTPUT TO RawMarketLocationsStream;\n\n\nCREATE OR REPLACE CQ ConvertRawShapes\nINSERT INTO MarketStream\nSELECT e.data.get(\"marketid\").textValue() as id,\n       e.data.get(\"marketname\").textValue() as name,\n       e.data.withArray(\"geopoints\") as points,\n       DNOW() as lastUpdated\nFROM RawMarketLocationsStream e;\n\nCREATE OR REPLACE CQ SplitLatLon\nINSERT INTO PontStream\nSELECT\n  s.id as id,\n  s.name as name,\n  lat as lat ,\n  lon as lon,\n  s.lastUpdated as lastUpdated\nFROM MarketStream s, iterator(s.points, (lat String, lon String)) a;\n\nCREATE OR REPLACE WINDOW MarketWindow OVER MarketStream KEEP WITHIN 1 HOUR ON lastUpdated;\n\n\n-- added by nambi ,Reason - for filtering out \"lastUpdated\", which has current time, is to have static expected results\n\nCREATE OR REPLACE CQ SplitLatLon2\nINSERT INTO PontStream2\nSELECT\n  s.id as id,\n  s.name as name,\n  lat as lat ,\n  lon as lon\nFROM MarketStream s, iterator(s.points, (lat String, lon String)) a;\n\nCREATE TYPE wsData\n(\n  id String,\n  name String,\n  lat String,\n  lon String\n);\n\n\nCREATE WACTIONSTORE oneWS\nCONTEXT OF wsData\nEVENT TYPES(wsData )\n@PERSIST-TYPE@\n\nCREATE CQ wsToWaction\nINSERT INTO oneWS\nSELECT\n  id,\n  name,\n  lat ,\n  lon\nFROM PontStream2\nLINK SOURCE EVENT;\n\nEND APPLICATION LoadMarketsApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (\n  positionByEOF: false,\n  wildcard: '',\n  directory: '' )\nPARSE USING XMLParserV2 (\n  rootnode: 'document/MerchantName' )\nOUTPUT TO @APPNAME@Stream;\n\nCREATE CQ @APPNAME@CQ\nINSERT INTO @APPNAME@CQStream\nSELECT\ndata.attributeValue(\"merchantid\") as merchantid,\ndata.getText() as MerchantName\nFROM @APPNAME@Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter ()\nINPUT FROM @APPNAME@CQStream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@ Using DatabaseReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'qatest.@SourceTable@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCreate Source @SOURCE@\n        Using OracleReader\n(\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: 'qatest.test77',\n  FetchSize:1,\n  QueueSize:2000,\n  CommittedTransactions:true,\n  Compression:false\n)\nOutput To @STREAM@;\n\ncreate Target @TARGET@ using S3Writer(\n    bucketname:'@BUCKET@',\n   objectname:'upgradeData.csv',\n   foldername:'upgradefolder',\n  uploadpolicy:'EventCount : 10000,Interval :1m '\n)\nformat using DSVFormatter (\n)\ninput from @STREAM@;\n\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@ recovery 1 second interval;\n\ncreate source @APPNAME@_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using CobolCopybookParser (\ncopybookFileName : '@TD@/@PROP1@',\n  dataFileFont: '@PROP2@',\n  copybookSplit: '@PROP3@',\n  dataFileOrganization: '@PROP4@',\n  copybookDialect: '@PROP5@', \n  skipIndent:'@PROP6@',\n  DatahandlingScheme:'@PROP7@',\n  CopybookFileFormat:'USE_COLS_6_TO_80'\n  --recordSelector: '@PROP8@'\n)\nOUTPUT TO @APPNAME@Stream;\n\ncreate Target @APPNAME@Target using FileWriter(\n    filename :'@FILE@',\n    directory : '@FOLDER@'\n)\nformat using JsonFormatter (\n)\ninput from @APPNAME@Stream;\n\n/*\ncreate Target @APPNAME@DBTarget using DatabaseWriter(\n  Username: 'qatest',\n  Password: 'qatest',\n  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',\n  BatchPolicy: 'EventCount:1,Interval:5',\n  CommitPolicy: 'EventCount:1,Interval:5',\n  Tables: 'QATEST.@table@'\n)\ninput from @APPNAME@Stream;*/\nend application @APPNAME@;\ndeploy application @APPNAME@ on all in default;\nstart application @APPNAME@;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@ Using OracleReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'qatest.@SourceTable@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application ADW;\nundeploy application ADW;\ndrop application ADW cascade;\nCREATE APPLICATION ADW;\n\nCREATE  SOURCE OjetIL USING DatabaseReader  \n (\n Username:'src_username',\n Password:'src_password',\n ConnectionURL: 'src_url',\n Tables:'@SOURCE-TABLES@',\n FetchSize:2000\n) \nOUTPUT TO InitialLoadStream;\n\nCREATE TARGET AzureDWInitialLoad USING AzureSQLDWHWriter(\nConnectionURL: '@SQLDW-URL@',\n        username: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@',\n        uploadpolicy:'@EVENT-COUNT@'\n)\nINPUT FROM InitialLoadStream;\n\nEND APPLICATION ADW;\ndeploy application ADW;\nstart application ADW;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create Target @TARGET_NAME@ using ADLSGen1Writer(\n        filename:'event_data.csv',\n        directory:'%@metadata(TableName)%',\n        datalakestorename:'striimdls.azuredatalakestore.net',\n        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',\n        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',\n        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',\n\t\trolloverpolicy:'filesize:10M',\n\t\tcompressiontype: 'false'\n)\nformat using DSVFormatter (\n)\ninput from @STREAM@; "}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');\n\nCREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.JsonNodeEvent PERSIST USING @APPNAME@KafkaPropset;\n\nCREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (\n  ProviderName: '',\n  Provider: '',\n  Ctx: '',\n  QueueName: '',\n  Topic:'',\n  UserName: '',\n  Password: '',\n  EnableTransaction: '',\n  transactionpolicy: ''\n )\nPARSE USING JSONParser ()\nOUTPUT TO @APPNAME@PersistStream@RANDOM@;\n\nCREATE OR REPLACE TARGET @APPNAME@_filetrgt USING FileWriter (\n  filename: '',\n  flushpolicy: '',\n  directory: '',\n  rolloverpolicy: '' )\nFORMAT USING JSONFormatter  (\n  members: 'data' )\nINPUT FROM @APPNAME@PersistStream@RANDOM@;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Canon Test W32\n-- Nicholas Keene, WebAction, Inc.\n--\n-- Test having one source leading to two data paths with unpartitioned\n-- jumping count windows leading to one waction store.\n--\n-- S -> JWc5u -> CQ5 -> WS\n-- S -> JWc2u -> CQ2 -> WS\n--\n\n\nUNDEPLOY APPLICATION NameW32.W32;\nDROP APPLICATION NameW32.W32 CASCADE;\nCREATE APPLICATION W32 RECOVERY 5 SECOND INTERVAL;\n\n\n\n\nCREATE FLOW DataAcquisitionW32;\n\n\nCREATE SOURCE CsvSourceW32 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'Canon1000.csv',\n  columndelimiter:',',\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStreamW32;\n\n\nEND FLOW DataAcquisitionW32;\n\n\n\n\nCREATE FLOW DataProcessingW32;\n\nCREATE TYPE DataTypeW32 (\n    eventID String,\n    word String KEY,\n    dateTime DateTime\n);\n\nCREATE STREAM DataStreamW32 OF DataTypeW32;\n\nCREATE CQ CSVStreamW32_to_DataStreamW32\nINSERT INTO DataStreamW32\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[2],'yyyyMMddHHmmss')\nFROM CsvStreamW32;\n\nCREATE JUMPING WINDOW JWc5uW32\nOVER DataStreamW32\nKEEP 5 ROWS;\n\nCREATE JUMPING WINDOW JWc2uW32\nOVER DataStreamW32\nKEEP 2 ROWS;\n\nCREATE WACTIONSTORE WactionStoreW32 CONTEXT OF DataTypeW32\nEVENT TYPES ( DataTypeW32 KEY(word) )\n@PERSIST-TYPE@\n\nCREATE CQ JWc5uW32_to_WactionStoreW32\nINSERT INTO WactionStoreW32\nSELECT\n    FIRST(eventID),\n    FIRST(word),\n    FIRST(dateTime)\nFROM JWc5uW32;\n\nCREATE CQ JWc2uW32_to_WactionStoreW32\nINSERT INTO WactionStoreW32\nSELECT\n    FIRST(eventID),\n    FIRST(word),\n    FIRST(dateTime)\nFROM JWc2uW32;\n\nEND FLOW DataProcessingW32;\n\n\n\nEND APPLICATION W32;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\nCREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\n\nCreate Source @SourceName@ Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL:'jdbc:oracle:thin:@//192.168.124.25:1522/orcl',\n Tables:'qatest.sourcetable',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\n\nCREATE TARGET @targetsys@ USING Global.SysOut (\n  name: 'sysout' )\nINPUT FROM @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING Global.DatabaseWriter (\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',\n  BatchPolicy: 'EventCount:10000,Interval:30',\n  CheckPointTable: 'CHKPOINT',\n  CDDLAction: 'Process',\n  CommitPolicy: 'EventCount:10000,Interval:100',\n  StatementCacheSize: '50',\n  Password: 'w3b@ct10n',\n  Username: 'qatest',\n  IgnorableExceptionCode: '547,DUPLICATE_ROW_EXISTS,NO_OP_UPDATE,NO_OP_DELETE,NO_OP_PKUPDATE',\n  DatabaseProviderType: 'SQLServer',\n  PreserveSourceTransactionBoundary: 'false',\n  Tables: 'dbo.sourceTable,dbo.targetTable',\n  VendorConfiguration: 'enableIdentityInsert=true',\n  adapterName: 'DatabaseWriter' )\nINPUT FROM @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\n\nCREATE STREAM PosDataStream OF PosData;\n\nCREATE CQ CsvToPosData\nINSERT INTO PosDataStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\n\nCREATE TARGET DumpFileStream USING LogWriter(\nname:testOuput1,\nfilename:'@FEATURE-DIR@/logs/TQLwithinTQL-console-out-RT.log'\n) INPUT FROM PosDataStream;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING Global.FabricDataWarehouseWriter (\n  Tables: '',\n  StorageAccessDriverType: 'WASBS',\n  ConnectionURL: '',\n  Username: '',\n  AccountAccessKey: '',\n  Mode: 'APPENDONLY',\n  Password: '',\n  uploadpolicy: 'eventcount:1,interval:10s',\n  AccountName: '')\nINPUT FROM @STREAM@;\n\nCREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(\n  name: 'CDDL',\n  filename:'WAEventDump.log'\n)INPUT FROM @STREAM@;;\n\ncreate Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Kafka Stream Recovery Test 1 New with internal test data generation\n-- Bert Hashemi and Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> KS -> WS\n\nSTOP KStreamRecovTester.KStreamRecovTest;\nUNDEPLOY APPLICATION KStreamRecovTester.KStreamRecovTest;\nDROP APPLICATION KStreamRecovTester.KStreamRecovTest CASCADE;\nDROP USER KStreamRecovTester;\nDROP NAMESPACE KStreamRecovTester CASCADE;\nCREATE USER KStreamRecovTester IDENTIFIED BY KStreamRecovTester;\n-- GRANT 'Global:create,drop:deploymentgroup:*' TO USER KStreamRecov1Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecovTester;\nCONNECT KStreamRecovTester KStreamRecovTester;\n\nCREATE APPLICATION KStreamRecovTest RECOVERY 5 SECOND INTERVAL;\n\nCREATE or REPLACE TYPE KafkaType(\n  value java.lang.Long KEY\n);\n\nCREATE SOURCE KafkaSource USING NumberSource (\n  lowValue: '1',\n  highValue: '1003',\n  delayMillis: '10',\n  delayNanos: '0',\n  repeat: 'false'\n )\nOUTPUT TO NumberSourceOut;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaStream OF KafkaType persist using KafkaPropset;\n\nCREATE OR REPLACE CQ KafkaStreamPopulate\nINSERT INTO KafkaStream\nSELECT data[1]\nFROM NumberSourceOut;\n\nCREATE WACTIONSTORE Wactions CONTEXT of KafkaType\n@PERSIST-TYPE@\n\nCREATE CQ WactionsPopulate\nINSERT INTO Wactions\nSELECT * FROM KafkaStream;\n\nEND APPLICATION KStreamRecovTest;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.GGTrailReader (\n  Tables:'@TABLES@',\n  CDDLCapture: false,\n  TrailDirectory: '@TRAIL_FILE_DIR@',\n  TrailFilePattern: '@WILDCARD@',\n  Compression: false,\n  SupportColumnCharset: false,\n  CDDLAction: 'Process',\n  FilterTransactionBoundaries: true,\n  adapterName: 'GGTrailReader',\n  TrailByteOrder: '@ENDIAN@' )\nOUTPUT TO @STREAM@;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@2 USING Global.GGTrailReader (\n  Tables:'@TABLES@',\n  CDDLCapture: false,\n  TrailDirectory: '@TRAIL_FILE_DIR@',\n  TrailFilePattern: '@WILDCARD@',\n  Compression: false,\n  SupportColumnCharset: false,\n  CDDLAction: 'Process',\n  FilterTransactionBoundaries: true,\n  adapterName: 'GGTrailReader',\n  TrailByteOrder: '@ENDIAN@' )\nOUTPUT TO @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 1\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> WS\n--\n\nSTOP Recov1Tester.RecovTest1;\nUNDEPLOY APPLICATION Recov1Tester.RecovTest1;\nDROP APPLICATION Recov1Tester.RecovTest1 CASCADE;\nCREATE APPLICATION RecovTest1 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE WactionType (\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions\nINSERT INTO Wactions\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nEND APPLICATION RecovTest1;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "-- CacheTestWithDB.tql\n-- Author: Gopi Putty; Date: Oct,11, 2016. \n-- A cache backed by db table.\n/*\nOracle:\nCREATE TABLE PRODUCT_INV ( SKU INT PRIMARY KEY NOT NULL,  NAME varchar2(20) );\nINSERT INTO PRODUCT_INV (SKU, NAME)\n*/\n\nUSE ADMIN;\nSTOP APPLICATION CACHETEST2.CACHEBACKEDWITHDB;\nUNDEPLOY APPLICATION CACHETEST2.CACHEBACKEDWITHDB;\nDROP APPLICATION CACHETEST2.CACHEBACKEDWITHDB CASCADE;\nDROP NAMESPACE CACHETEST2 CASCADE; \nCREATE NAMESPACE CACHETEST2;\nUSE CACHETEST2; \n\nCREATE APPLICATION CACHEBACKEDWITHDB;\n\nCREATE TYPE CTYPE(\n  SKU int,\n  Name String\n);\n\nCREATE STREAM skuStream OF CTYPE;\n\nCREATE OR REPLACE CACHE BKRequestLookup USING DatabaseReader (\nConnectionURL:'jdbc:oracle:thin:@10.1.186.105:1521:orcl',\nUsername:'GOPI',\nPassword:'gopi',\nQuery: \"\nSELECT  PI.SKU, PI.NAME FROM GOPI.PRODUCT_INV PI\"\n) QUERY (keytomap:'SKU', refreshinterval: '60000000',charset:'UTF-8',replicas:1) OF CTYPE;\n\nEND APPLICATION CACHEBACKEDWITHDB;\nDEPLOY APPLICATION CACHEBACKEDWITHDB;\n\nUSE ADMIN;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @Appname@;\nUNDEPLOY APPLICATION @Appname@;\nDROP APPLICATION @Appname@ CASCADE;\n\nCREATE APPLICATION @Appname@ RECOVERY 10 SECOND INTERVAL;\n\nCREATE  SOURCE @Appname@source USING MySQLReader\n(\nUsername: '',\nPassword: '',\nConnectionURL: '',\nTables: 'waction.TABLE_TEST_%',\nFetchSize:1\n)\nOUTPUT TO @Appname@MasterStream;\n\nCREATE OR REPLACE ROUTER @Appname@Rs1 INPUT FROM @Appname@MasterStream s CASE\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_1' THEN ROUTE TO @Appname@Typed1,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_2' THEN ROUTE TO @Appname@Typed2,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_3' THEN ROUTE TO @Appname@Typed3,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_4' THEN ROUTE TO @Appname@Typed4,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_5' THEN ROUTE TO @Appname@Typed5,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_6' THEN ROUTE TO @Appname@Typed6,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_7' THEN ROUTE TO @Appname@Typed7,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_8' THEN ROUTE TO @Appname@Typed8,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_9' THEN ROUTE TO @Appname@Typed9,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_10' THEN ROUTE TO @Appname@Typed10,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_11' THEN ROUTE TO @Appname@Typed11,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_12' THEN ROUTE TO @Appname@Typed12,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_13' THEN ROUTE TO @Appname@Typed13,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_14' THEN ROUTE TO @Appname@Typed14,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_15' THEN ROUTE TO @Appname@Typed15,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_16' THEN ROUTE TO @Appname@Typed16,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_17' THEN ROUTE TO @Appname@Typed17,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_18' THEN ROUTE TO @Appname@Typed18,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_19' THEN ROUTE TO @Appname@Typed19,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_20' THEN ROUTE TO @Appname@Typed20,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_21' THEN ROUTE TO @Appname@Typed21,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_22' THEN ROUTE TO @Appname@Typed22,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_23' THEN ROUTE TO @Appname@Typed23,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_24' THEN ROUTE TO @Appname@Typed24,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_25' THEN ROUTE TO @Appname@Typed25,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_26' THEN ROUTE TO @Appname@Typed26,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_27' THEN ROUTE TO @Appname@Typed27,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_28' THEN ROUTE TO @Appname@Typed28,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_29' THEN ROUTE TO @Appname@Typed29,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_30' THEN ROUTE TO @Appname@Typed30,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_31' THEN ROUTE TO @Appname@Typed31,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_32' THEN ROUTE TO @Appname@Typed32,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_33' THEN ROUTE TO @Appname@Typed33,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_34' THEN ROUTE TO @Appname@Typed34,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_35' THEN ROUTE TO @Appname@Typed35,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_36' THEN ROUTE TO @Appname@Typed36,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_37' THEN ROUTE TO @Appname@Typed37,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_38' THEN ROUTE TO @Appname@Typed38,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_39' THEN ROUTE TO @Appname@Typed39,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_40' THEN ROUTE TO @Appname@Typed40,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_41' THEN ROUTE TO @Appname@Typed41,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_42' THEN ROUTE TO @Appname@Typed42,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_43' THEN ROUTE TO @Appname@Typed43,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_44' THEN ROUTE TO @Appname@Typed44,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_45' THEN ROUTE TO @Appname@Typed45,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_46' THEN ROUTE TO @Appname@Typed46,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_47' THEN ROUTE TO @Appname@Typed47,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_48' THEN ROUTE TO @Appname@Typed48,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_49' THEN ROUTE TO @Appname@Typed49,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_50' THEN ROUTE TO @Appname@Typed50,\n/*WHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_51' THEN ROUTE TO @Appname@Typed51,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_52' THEN ROUTE TO @Appname@Typed52,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_53' THEN ROUTE TO @Appname@Typed53,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_54' THEN ROUTE TO @Appname@Typed54,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_55' THEN ROUTE TO @Appname@Typed55,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_56' THEN ROUTE TO @Appname@Typed56,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_57' THEN ROUTE TO @Appname@Typed57,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_58' THEN ROUTE TO @Appname@Typed58,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_59' THEN ROUTE TO @Appname@Typed59,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_60' THEN ROUTE TO @Appname@Typed60,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_61' THEN ROUTE TO @Appname@Typed61,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_62' THEN ROUTE TO @Appname@Typed62,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_63' THEN ROUTE TO @Appname@Typed63,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_64' THEN ROUTE TO @Appname@Typed64,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_65' THEN ROUTE TO @Appname@Typed65,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_66' THEN ROUTE TO @Appname@Typed66,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_67' THEN ROUTE TO @Appname@Typed67,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_68' THEN ROUTE TO @Appname@Typed68,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_69' THEN ROUTE TO @Appname@Typed69,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_70' THEN ROUTE TO @Appname@Typed70,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_71' THEN ROUTE TO @Appname@Typed71,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_72' THEN ROUTE TO @Appname@Typed72,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_73' THEN ROUTE TO @Appname@Typed73,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_74' THEN ROUTE TO @Appname@Typed74,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_75' THEN ROUTE TO @Appname@Typed75,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_76' THEN ROUTE TO @Appname@Typed76,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_77' THEN ROUTE TO @Appname@Typed77,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_78' THEN ROUTE TO @Appname@Typed78,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_79' THEN ROUTE TO @Appname@Typed79,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_80' THEN ROUTE TO @Appname@Typed80,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_81' THEN ROUTE TO @Appname@Typed81,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_82' THEN ROUTE TO @Appname@Typed82,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_83' THEN ROUTE TO @Appname@Typed83,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_84' THEN ROUTE TO @Appname@Typed84,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_85' THEN ROUTE TO @Appname@Typed85,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_86' THEN ROUTE TO @Appname@Typed86,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_87' THEN ROUTE TO @Appname@Typed87,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_88' THEN ROUTE TO @Appname@Typed88,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_89' THEN ROUTE TO @Appname@Typed89,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_90' THEN ROUTE TO @Appname@Typed90,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_91' THEN ROUTE TO @Appname@Typed91,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_92' THEN ROUTE TO @Appname@Typed92,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_93' THEN ROUTE TO @Appname@Typed93,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_94' THEN ROUTE TO @Appname@Typed94,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_95' THEN ROUTE TO @Appname@Typed95,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_96' THEN ROUTE TO @Appname@Typed96,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_97' THEN ROUTE TO @Appname@Typed97,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_98' THEN ROUTE TO @Appname@Typed98,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_99' THEN ROUTE TO @Appname@Typed99,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_1000100' THEN ROUTE TO @Appname@Typed100,*/\nELSE ROUTE TO @Appname@TypedElse;\n\n\n\nCREATE OR REPLACE cq @Appname@cq1\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed1 s;\n\nCREATE OR REPLACE cq @Appname@cq2\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed2 s;\n\nCREATE OR REPLACE cq @Appname@cq3\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed3 s;\n\nCREATE OR REPLACE cq @Appname@cq4\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed4 s;\n\nCREATE OR REPLACE cq @Appname@cq5\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed5 s;\n\nCREATE OR REPLACE cq @Appname@cq6\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed6 s;\n\nCREATE OR REPLACE cq @Appname@cq7\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed7 s;\n\nCREATE OR REPLACE cq @Appname@cq8\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed8 s;\n\nCREATE OR REPLACE cq @Appname@cq9\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed9 s;\n\nCREATE OR REPLACE cq @Appname@cq10\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed10 s;\n\n\nCREATE OR REPLACE cq @Appname@cq11\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed11 s;\n\nCREATE OR REPLACE cq @Appname@cq12\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed12 s;\n\nCREATE OR REPLACE cq @Appname@cq13\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed13 s;\n\nCREATE OR REPLACE cq @Appname@cq14\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed14 s;\n\nCREATE OR REPLACE cq @Appname@cq15\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed15 s;\n\nCREATE OR REPLACE cq @Appname@cq16\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed16 s;\n\nCREATE OR REPLACE cq @Appname@cq117\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed17 s;\n\nCREATE OR REPLACE cq @Appname@cq18\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed18 s;\n\nCREATE OR REPLACE cq @Appname@cq19\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed19 s;\n\nCREATE OR REPLACE cq @Appname@cq20\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed20 s;\n\nCREATE OR REPLACE cq @Appname@cq21\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed21 s;\n\nCREATE OR REPLACE cq @Appname@cq22\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed22 s;\n\nCREATE OR REPLACE cq @Appname@cq23\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed23 s;\n\nCREATE OR REPLACE cq @Appname@cq24\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed24 s;\n\nCREATE OR REPLACE cq @Appname@cq25\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed25 s;\n\nCREATE OR REPLACE cq @Appname@cq26\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed26 s;\n\nCREATE OR REPLACE cq @Appname@cq27\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed27 s;\n\nCREATE OR REPLACE cq @Appname@cq28\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed28 s;\n\nCREATE OR REPLACE cq @Appname@cq29\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed29 s;\n\nCREATE OR REPLACE cq @Appname@cq30\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed30 s;\n\nCREATE OR REPLACE cq @Appname@cq31\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed31 s;\n\nCREATE OR REPLACE cq @Appname@cq32\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed32 s;\n\nCREATE OR REPLACE cq @Appname@cq33\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed33 s;\n\nCREATE OR REPLACE cq @Appname@cq34\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed34 s;\n\nCREATE OR REPLACE cq @Appname@cq35\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed35 s;\n\nCREATE OR REPLACE cq @Appname@cq36\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed36 s;\n\nCREATE OR REPLACE cq @Appname@cq37\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed37 s;\n\nCREATE OR REPLACE cq @Appname@cq38\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed38 s;\n\nCREATE OR REPLACE cq @Appname@cq39\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed39 s;\n\nCREATE OR REPLACE cq @Appname@cq40\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed40 s;\n\nCREATE OR REPLACE cq @Appname@cq41\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed41 s;\n\nCREATE OR REPLACE cq @Appname@cq42\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed42 s;\n\nCREATE OR REPLACE cq @Appname@cq43\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed43 s;\n\nCREATE OR REPLACE cq @Appname@cq44\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed44 s;\n\nCREATE OR REPLACE cq @Appname@cq45\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed45 s;\n\nCREATE OR REPLACE cq @Appname@cq46\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed46 s;\n\nCREATE OR REPLACE cq @Appname@cq47\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed47 s;\n\nCREATE OR REPLACE cq @Appname@cq48\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed48 s;\n\nCREATE OR REPLACE cq @Appname@cq49\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed49 s;\n\nCREATE OR REPLACE cq @Appname@cq50\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed50 s;\n/*\nCREATE OR REPLACE cq @Appname@cq51\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed51 s;\n\nCREATE OR REPLACE cq @Appname@cq52\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed52 s;\n\nCREATE OR REPLACE cq @Appname@cq53\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed53 s;\n\nCREATE OR REPLACE cq @Appname@cq54\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed54 s;\n\nCREATE OR REPLACE cq @Appname@cq55\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed55 s;\n\nCREATE OR REPLACE cq @Appname@cq56\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed56 s;\n\nCREATE OR REPLACE cq @Appname@cq57\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed57 s;\n\nCREATE OR REPLACE cq @Appname@cq58\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed58 s;\n\nCREATE OR REPLACE cq @Appname@cq59\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed59 s;\n\nCREATE OR REPLACE cq @Appname@cq60\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed60 s;\n\nCREATE OR REPLACE cq @Appname@cq61\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed61 s;\n\nCREATE OR REPLACE cq @Appname@cq62\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed62 s;\n\nCREATE OR REPLACE cq @Appname@cq63\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed63 s;\n\nCREATE OR REPLACE cq @Appname@cq64\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed64 s;\n\nCREATE OR REPLACE cq @Appname@cq65\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed65 s;\n\nCREATE OR REPLACE cq @Appname@cq66\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed66 s;\n\nCREATE OR REPLACE cq @Appname@cq67\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed67 s;\n\nCREATE OR REPLACE cq @Appname@cq68\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed68 s;\n\nCREATE OR REPLACE cq @Appname@cq69\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed69 s;\n\nCREATE OR REPLACE cq @Appname@cq70\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed70 s;\n\nCREATE OR REPLACE cq @Appname@cq71\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed71 s;\n\nCREATE OR REPLACE cq @Appname@cq72\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed72 s;\n\nCREATE OR REPLACE cq @Appname@cq73\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed73 s;\n\nCREATE OR REPLACE cq @Appname@cq74\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed74 s;\n\nCREATE OR REPLACE cq @Appname@cq75\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed75 s;\n\nCREATE OR REPLACE cq @Appname@cq76\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed76 s;\n\nCREATE OR REPLACE cq @Appname@cq77\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed77 s;\n\nCREATE OR REPLACE cq @Appname@cq78\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed78 s;\n\nCREATE OR REPLACE cq @Appname@cq79\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed79 s;\n\nCREATE OR REPLACE cq @Appname@cq80\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed80 s;\n\nCREATE OR REPLACE cq @Appname@cq81\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed81 s;\n\nCREATE OR REPLACE cq @Appname@cq82\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed82 s;\n\nCREATE OR REPLACE cq @Appname@cq83\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed83 s;\n\nCREATE OR REPLACE cq @Appname@cq84\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed84 s;\n\nCREATE OR REPLACE cq @Appname@cq85\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed85 s;\n\nCREATE OR REPLACE cq @Appname@cq86\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed86 s;\n\nCREATE OR REPLACE cq @Appname@cq87\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed87 s;\n\nCREATE OR REPLACE cq @Appname@cq88\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed88 s;\n\nCREATE OR REPLACE cq @Appname@cq89\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed89 s;\n\nCREATE OR REPLACE cq @Appname@cq90\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed90 s;\n\nCREATE OR REPLACE cq @Appname@cq91\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed91 s;\n\nCREATE OR REPLACE cq @Appname@cq92\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed92 s;\n\nCREATE OR REPLACE cq @Appname@cq93\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed93 s;\n\nCREATE OR REPLACE cq @Appname@cq94\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed94 s;\n\nCREATE OR REPLACE cq @Appname@cq95\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed95 s;\n\nCREATE OR REPLACE cq @Appname@cq96\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed96 s;\n\nCREATE OR REPLACE cq @Appname@cq97\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed97 s;\n\nCREATE OR REPLACE cq @Appname@cq98\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed98 s;\n\nCREATE OR REPLACE cq @Appname@cq99\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed99 s;\n\nCREATE OR REPLACE cq @Appname@cq100\nINSERT INTO combinedstream\nselect s FROM @Appname@Typed100 s;\n\nCREATE OR REPLACE cq @Appname@cq101\nINSERT INTO combinedstream\nselect s FROM @Appname@TypedElse s;*/\n\nCREATE OR REPLACE ROUTER Rs2 INPUT FROM combinedstream s CASE\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_1' THEN ROUTE TO New@Appname@Typed1,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_2' THEN ROUTE TO New@Appname@Typed2,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_3' THEN ROUTE TO New@Appname@Typed3,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_4' THEN ROUTE TO New@Appname@Typed4,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_5' THEN ROUTE TO New@Appname@Typed5,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_6' THEN ROUTE TO New@Appname@Typed6,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_7' THEN ROUTE TO New@Appname@Typed7,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_8' THEN ROUTE TO New@Appname@Typed8,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_9' THEN ROUTE TO New@Appname@Typed9,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_10' THEN ROUTE TO New@Appname@Typed10,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_11' THEN ROUTE TO New@Appname@Typed11,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_12' THEN ROUTE TO New@Appname@Typed12,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_13' THEN ROUTE TO New@Appname@Typed13,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_14' THEN ROUTE TO New@Appname@Typed14,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_15' THEN ROUTE TO New@Appname@Typed15,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_16' THEN ROUTE TO New@Appname@Typed16,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_17' THEN ROUTE TO New@Appname@Typed17,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_18' THEN ROUTE TO New@Appname@Typed18,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_19' THEN ROUTE TO New@Appname@Typed19,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_20' THEN ROUTE TO New@Appname@Typed20,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_21' THEN ROUTE TO New@Appname@Typed21,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_22' THEN ROUTE TO New@Appname@Typed22,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_23' THEN ROUTE TO New@Appname@Typed23,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_24' THEN ROUTE TO New@Appname@Typed24,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_25' THEN ROUTE TO New@Appname@Typed25,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_26' THEN ROUTE TO New@Appname@Typed26,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_27' THEN ROUTE TO New@Appname@Typed27,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_28' THEN ROUTE TO New@Appname@Typed28,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_29' THEN ROUTE TO New@Appname@Typed29,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_30' THEN ROUTE TO New@Appname@Typed30,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_31' THEN ROUTE TO New@Appname@Typed31,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_32' THEN ROUTE TO New@Appname@Typed32,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_33' THEN ROUTE TO New@Appname@Typed33,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_34' THEN ROUTE TO New@Appname@Typed34,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_35' THEN ROUTE TO New@Appname@Typed35,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_36' THEN ROUTE TO New@Appname@Typed36,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_37' THEN ROUTE TO New@Appname@Typed37,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_38' THEN ROUTE TO New@Appname@Typed38,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_39' THEN ROUTE TO New@Appname@Typed39,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_40' THEN ROUTE TO New@Appname@Typed40,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_41' THEN ROUTE TO New@Appname@Typed41,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_42' THEN ROUTE TO New@Appname@Typed42,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_43' THEN ROUTE TO New@Appname@Typed43,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_44' THEN ROUTE TO New@Appname@Typed44,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_45' THEN ROUTE TO New@Appname@Typed45,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_46' THEN ROUTE TO New@Appname@Typed46,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_47' THEN ROUTE TO New@Appname@Typed47,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_48' THEN ROUTE TO New@Appname@Typed48,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_49' THEN ROUTE TO New@Appname@Typed49,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_50' THEN ROUTE TO New@Appname@Typed50,\n/*WHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_51' THEN ROUTE TO New@Appname@Typed51,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_52' THEN ROUTE TO New@Appname@Typed52,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_53' THEN ROUTE TO New@Appname@Typed53,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_54' THEN ROUTE TO New@Appname@Typed54,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_55' THEN ROUTE TO New@Appname@Typed55,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_56' THEN ROUTE TO New@Appname@Typed56,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_57' THEN ROUTE TO New@Appname@Typed57,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_58' THEN ROUTE TO New@Appname@Typed58,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_59' THEN ROUTE TO New@Appname@Typed59,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_60' THEN ROUTE TO New@Appname@Typed60,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_61' THEN ROUTE TO New@Appname@Typed61,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_62' THEN ROUTE TO New@Appname@Typed62,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_63' THEN ROUTE TO New@Appname@Typed63,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_64' THEN ROUTE TO New@Appname@Typed64,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_65' THEN ROUTE TO New@Appname@Typed65,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_66' THEN ROUTE TO New@Appname@Typed66,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_67' THEN ROUTE TO New@Appname@Typed67,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_68' THEN ROUTE TO New@Appname@Typed68,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_69' THEN ROUTE TO New@Appname@Typed69,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_70' THEN ROUTE TO New@Appname@Typed70,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_71' THEN ROUTE TO New@Appname@Typed71,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_72' THEN ROUTE TO New@Appname@Typed72,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_73' THEN ROUTE TO New@Appname@Typed73,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_74' THEN ROUTE TO New@Appname@Typed74,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_75' THEN ROUTE TO New@Appname@Typed75,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_76' THEN ROUTE TO New@Appname@Typed76,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_77' THEN ROUTE TO New@Appname@Typed77,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_78' THEN ROUTE TO New@Appname@Typed78,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_79' THEN ROUTE TO New@Appname@Typed79,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_80' THEN ROUTE TO New@Appname@Typed80,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_81' THEN ROUTE TO New@Appname@Typed81,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_82' THEN ROUTE TO New@Appname@Typed82,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_83' THEN ROUTE TO New@Appname@Typed83,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_84' THEN ROUTE TO New@Appname@Typed84,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_85' THEN ROUTE TO New@Appname@Typed85,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_86' THEN ROUTE TO New@Appname@Typed86,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_87' THEN ROUTE TO New@Appname@Typed87,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_88' THEN ROUTE TO New@Appname@Typed88,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_89' THEN ROUTE TO New@Appname@Typed89,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_90' THEN ROUTE TO New@Appname@Typed90,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_91' THEN ROUTE TO New@Appname@Typed91,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_92' THEN ROUTE TO New@Appname@Typed92,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_93' THEN ROUTE TO New@Appname@Typed93,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_94' THEN ROUTE TO New@Appname@Typed94,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_95' THEN ROUTE TO New@Appname@Typed95,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_96' THEN ROUTE TO New@Appname@Typed96,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_97' THEN ROUTE TO New@Appname@Typed97,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_98' THEN ROUTE TO New@Appname@Typed98,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_99' THEN ROUTE TO New@Appname@Typed99,\nWHEN meta(s,\"TableName\").toString()='waction.TABLE_TEST_1000100' THEN ROUTE TO New@Appname@Typed100,*/\nELSE ROUTE TO New@Appname@TypedElse;\n\nCREATE OR REPLACE cq New@Appname@cq1\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed1 s;\n\nCREATE OR REPLACE cq New@Appname@cq2\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed2 s;\n\nCREATE OR REPLACE cq New@Appname@cq3\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed3 s;\n\nCREATE OR REPLACE cq New@Appname@cq4\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed4 s;\n\nCREATE OR REPLACE cq New@Appname@cq5\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed5 s;\n\nCREATE OR REPLACE cq New@Appname@cq6\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed6 s;\n\nCREATE OR REPLACE cq New@Appname@cq7\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed7 s;\n\nCREATE OR REPLACE cq New@Appname@cq8\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed8 s;\n\nCREATE OR REPLACE cq New@Appname@cq9\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed9 s;\n\nCREATE OR REPLACE cq New@Appname@cq10\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed10 s;\n\n\nCREATE OR REPLACE cq New@Appname@cq11\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed11 s;\n\nCREATE OR REPLACE cq New@Appname@cq12\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed12 s;\n\nCREATE OR REPLACE cq New@Appname@cq13\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed13 s;\n\nCREATE OR REPLACE cq New@Appname@cq14\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed14 s;\n\nCREATE OR REPLACE cq New@Appname@cq15\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed15 s;\n\nCREATE OR REPLACE cq New@Appname@cq16\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed16 s;\n\nCREATE OR REPLACE cq New@Appname@cq117\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed17 s;\n\nCREATE OR REPLACE cq New@Appname@cq18\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed18 s;\n\nCREATE OR REPLACE cq New@Appname@cq19\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed19 s;\n\nCREATE OR REPLACE cq New@Appname@cq20\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed20 s;\n\nCREATE OR REPLACE cq New@Appname@cq21\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed21 s;\n\nCREATE OR REPLACE cq New@Appname@cq22\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed22 s;\n\nCREATE OR REPLACE cq New@Appname@cq23\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed23 s;\n\nCREATE OR REPLACE cq New@Appname@cq24\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed24 s;\n\nCREATE OR REPLACE cq New@Appname@cq25\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed25 s;\n\nCREATE OR REPLACE cq New@Appname@cq26\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed26 s;\n\nCREATE OR REPLACE cq New@Appname@cq27\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed27 s;\n\nCREATE OR REPLACE cq New@Appname@cq28\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed28 s;\n\nCREATE OR REPLACE cq New@Appname@cq29\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed29 s;\n\nCREATE OR REPLACE cq New@Appname@cq30\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed30 s;\n\nCREATE OR REPLACE cq New@Appname@cq31\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed31 s;\n\nCREATE OR REPLACE cq New@Appname@cq32\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed32 s;\n\nCREATE OR REPLACE cq New@Appname@cq33\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed33 s;\n\nCREATE OR REPLACE cq New@Appname@cq34\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed34 s;\n\nCREATE OR REPLACE cq New@Appname@cq35\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed35 s;\n\nCREATE OR REPLACE cq New@Appname@cq36\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed36 s;\n\nCREATE OR REPLACE cq New@Appname@cq37\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed37 s;\n\nCREATE OR REPLACE cq New@Appname@cq38\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed38 s;\n\nCREATE OR REPLACE cq New@Appname@cq39\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed39 s;\n\nCREATE OR REPLACE cq New@Appname@cq40\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed40 s;\n\nCREATE OR REPLACE cq New@Appname@cq41\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed41 s;\n\nCREATE OR REPLACE cq New@Appname@cq42\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed42 s;\n\nCREATE OR REPLACE cq New@Appname@cq43\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed43 s;\n\nCREATE OR REPLACE cq New@Appname@cq44\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed44 s;\n\nCREATE OR REPLACE cq New@Appname@cq45\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed45 s;\n\nCREATE OR REPLACE cq New@Appname@cq46\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed46 s;\n\nCREATE OR REPLACE cq New@Appname@cq47\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed47 s;\n\nCREATE OR REPLACE cq New@Appname@cq48\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed48 s;\n\nCREATE OR REPLACE cq New@Appname@cq49\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed49 s;\n\nCREATE OR REPLACE cq New@Appname@cq50\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed50 s;\n/*\nCREATE OR REPLACE cq New@Appname@cq51\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed51 s;\n\nCREATE OR REPLACE cq New@Appname@cq52\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed52 s;\n\nCREATE OR REPLACE cq New@Appname@cq53\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed53 s;\n\nCREATE OR REPLACE cq New@Appname@cq54\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed54 s;\n\nCREATE OR REPLACE cq New@Appname@cq55\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed55 s;\n\nCREATE OR REPLACE cq New@Appname@cq56\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed56 s;\n\nCREATE OR REPLACE cq New@Appname@cq57\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed57 s;\n\nCREATE OR REPLACE cq New@Appname@cq58\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed58 s;\n\nCREATE OR REPLACE cq New@Appname@cq59\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed59 s;\n\nCREATE OR REPLACE cq New@Appname@cq60\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed60 s;\n\nCREATE OR REPLACE cq New@Appname@cq61\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed61 s;\n\nCREATE OR REPLACE cq New@Appname@cq62\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed62 s;\n\nCREATE OR REPLACE cq New@Appname@cq63\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed63 s;\n\nCREATE OR REPLACE cq New@Appname@cq64\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed64 s;\n\nCREATE OR REPLACE cq New@Appname@cq65\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed65 s;\n\nCREATE OR REPLACE cq New@Appname@cq66\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed66 s;\n\nCREATE OR REPLACE cq New@Appname@cq67\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed67 s;\n\nCREATE OR REPLACE cq New@Appname@cq68\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed68 s;\n\nCREATE OR REPLACE cq New@Appname@cq69\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed69 s;\n\nCREATE OR REPLACE cq New@Appname@cq70\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed70 s;\n\nCREATE OR REPLACE cq New@Appname@cq71\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed71 s;\n\nCREATE OR REPLACE cq New@Appname@cq72\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed72 s;\n\nCREATE OR REPLACE cq New@Appname@cq73\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed73 s;\n\nCREATE OR REPLACE cq New@Appname@cq74\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed74 s;\n\nCREATE OR REPLACE cq New@Appname@cq75\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed75 s;\n\nCREATE OR REPLACE cq New@Appname@cq76\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed76 s;\n\nCREATE OR REPLACE cq New@Appname@cq77\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed77 s;\n\nCREATE OR REPLACE cq New@Appname@cq78\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed78 s;\n\nCREATE OR REPLACE cq New@Appname@cq79\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed79 s;\n\nCREATE OR REPLACE cq New@Appname@cq80\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed80 s;\n\nCREATE OR REPLACE cq New@Appname@cq81\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed81 s;\n\nCREATE OR REPLACE cq New@Appname@cq82\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed82 s;\n\nCREATE OR REPLACE cq New@Appname@cq83\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed83 s;\n\nCREATE OR REPLACE cq New@Appname@cq84\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed84 s;\n\nCREATE OR REPLACE cq New@Appname@cq85\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed85 s;\n\nCREATE OR REPLACE cq New@Appname@cq86\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed86 s;\n\nCREATE OR REPLACE cq New@Appname@cq87\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed87 s;\n\nCREATE OR REPLACE cq New@Appname@cq88\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed88 s;\n\nCREATE OR REPLACE cq New@Appname@cq89\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed89 s;\n\nCREATE OR REPLACE cq New@Appname@cq90\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed90 s;\n\nCREATE OR REPLACE cq New@Appname@cq91\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed91 s;\n\nCREATE OR REPLACE cq New@Appname@cq92\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed92 s;\n\nCREATE OR REPLACE cq New@Appname@cq93\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed93 s;\n\nCREATE OR REPLACE cq New@Appname@cq94\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed94 s;\n\nCREATE OR REPLACE cq New@Appname@cq95\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed95 s;\n\nCREATE OR REPLACE cq New@Appname@cq96\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed96 s;\n\nCREATE OR REPLACE cq New@Appname@cq97\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed97 s;\n\nCREATE OR REPLACE cq New@Appname@cq98\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed98 s;\n\nCREATE OR REPLACE cq New@Appname@cq99\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed99 s;\n\nCREATE OR REPLACE cq New@Appname@cq100\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@Typed100 s;\n\nCREATE OR REPLACE cq New@Appname@cq101\nINSERT INTO Newcombinedstream\nselect s FROM New@Appname@TypedElse s;*/\n\ncreate target systar using sysout(name:'out')INPUT FROM Newcombinedstream;\n\nCREATE OR REPLACE TARGET @Appname@Target USING DatabaseWriter (\nConnectionURL:'jdbc:mysql://162.222.179.3:3306/waction',\nUsername:'root',\nPassword:'w@ct10n',\nTables:'waction.TABLE_TEST_%;waction.RESULTTABLE',\nIgnorableExceptioncode : 'NO_OP_DELETE,NO_OP_UPDATE,NO_OP_PKUPDATE',\nCommitPolicy:'Eventcount:1000,Interval:3600',\nBatchPolicy:'eventCount:1000,Interval:3600'\n)\nINPUT FROM Newcombinedstream;\n\nend application @Appname@;\ndeploy application @Appname@;\nstart application @Appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@ @APP_PROPERTY@ USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE STREAM @APP_NAME@_DataStreamFromCQ OF Global.WAEvent;\n\nCreate Source @APP_NAME@_Source Using @SOURCE_ADAPTER@ (\n\n) OUTPUT TO @APP_NAME@_DataStream;\n\nCREATE OR REPLACE CQ @APP_NAME@_CQ INSERT INTO @APP_NAME@_DataStreamFromCQ SELECT * FROM @APP_NAME@_DataStream s where META(s,\"TableName\") is not null AND META(s,\"TableName\").toString().trim().isEmpty() == false;\n\nCREATE TARGET @APP_NAME@_Target USING @TARGET_ADAPTER@ ( \n\n) INPUT FROM @APP_NAME@_DataStreamFromCQ;\n\nCREATE OR REPLACE TARGET @APP_NAME@_SysOut_ReadFromSource USING Global.SysOut ( \n\tname: '@APP_NAME@_SysOutWA_Source' \n) INPUT FROM @APP_NAME@_DataStream;\n\nCREATE OR REPLACE TARGET @APP_NAME@_SysOut_WriteToTarget USING Global.SysOut ( \n\tname: '@APP_NAME@_SysOutWA_Target' \n) INPUT FROM @APP_NAME@_DataStreamFromCQ;\n\nEND APPLICATION @APP_NAME@;\n\nDEPLOY APPLICATION @APP_NAME@ IN DEFAULT;\nSTART APPLICATION @APP_NAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "UNDEPLOY APPLICATION RollOverTester.DSV;\nDROP APPLICATION RollOverTester.DSV CASCADE;\nCREATE APPLICATION DSV;\n\n\ncreate source JsonSource using FileReader (\n        directory:'@TEST-DATA-PATH@',\n        WildCard:'TestInput.txt',\n        positionByEOF:false,\n        charset:'UTF-8'\n)\n PARSE USING JSONParser (\n  eventType: ''\n )\nOUTPUT TO activeguard_rawstream;\n\n\nCREATE OR REPLACE TYPE activeguard_parsed (\n\t-- Enriched, or not in AGI left navigation\n\thash_key String KEY,\n\tclient_id Long,\n\tevent_type_id Long,\n\ttype_code String,\n\trule_id Long,\n\trule_name String,\n\trule_action String,\n\tsce_id  Long,\n\tmessage String,\n\tref_list_match String,\n\tevent_date DateTime,\n\tlog_count Long,\n\n\t-- As seen in AGI\n\t-- frequently used, but not duplicated\n\treceived_date DateTime,\n\tevent_sub_source String,\n\n\t-- database\n\tsol_action String,\n\tcomment String,\n\tcomment_text String,\n\tdatabase_name String,\n\tdatabase_user_name String,\n\tentry_id String,\n\tobj_name String,\n\tos_user_name String,\n\towner String,\n\tpriv_used String,\n\treturn_code String,\n\tsession_id String,\n\n\t-- geo\n\tdestination_country_code String,\n\tsource_country_code String,\n\n\t-- network\n\tdata_size String,\n\tdestination String,\n\tdestination_object String,\n\tdestination_port String,\n\tsol_domain String,\n\tevent_manager String,\n\tevent_path String,\n\tevent_source String,\n\tmsg_no String,\n\tprotocol_id String,\n\tservice String,\n\tsol_source String,\n\tsource_mac_address String,\n\tsource_object String,\n\tsource_port String,\n\tstatus_code String,\n\ttarget_path String,\n\n\t-- other\n\tcommand String,\n\n\t-- security\n\taffected_file String,\n\tobject_action String,\n\tobject_action_result String,\n\tobject_id String,\n\tobject_name String,\n\tobject_path String,\n\tprocess_name String,\n\n\t-- user\n\tcaller_user_name String,\n\tlogon_type String,\n\ttarget_user_name String,\n\tuser_agent String,\n\tuser_name String,\n\tworkstation String,\n\n\t-- web\n\tweb_file String,\n\tweb_method String,\n\tweb_page String,\n\tweb_protocol String\n);\n\n\nCREATE OR REPLACE STREAM activeguard_stream OF activeguard_parsed;\n\nCREATE CQ Parseactiveguard_stream\nINSERT INTO activeguard_stream\nSELECT\n\n\t-- Enriched, or not in AGI left navigation\n\tdata.get('hash_key').textValue() as hash_key,\n\tdata.get('client_id').longValue() as client_id,\n\tdata.get('event_type_id').longValue() as event_type_id,\n\tdata.get('type_code').textValue() as type_code,\n\tdata.get('rule_id').longValue() as rule_id,\n\tdata.get('rule_name').textValue() as rule_name,\n\tdata.get('rule_action').textValue() as rule_action,\n\tdata.get('sce_id').longValue() as sce_id,\n\tdata.get('message').textValue() as message,\n\tdata.get('ref_list_match').textValue() as ref_list_match,\n\tTO_DATE(data.get('event_date').textValue()) as event_date,\n\tdata.get('log_count').longValue() as log_count,\n\n\t-- As seen in AGI\n\t-- frequently used, but not duplicated\n\tTO_DATE(data.get('received_date').textValue()) as received_date,\n\tdata.get('event_sub_source').textValue() as event_sub_source,\n\n\t-- database\n\tdata.get('sol_action').textValue() as sol_action,\n\tdata.get('comment').textValue() as comment,\n\tdata.get('comment_text').textValue() as comment_text,\n\tdata.get('database_name').textValue() as database_name,\n\tdata.get('database_user_name').textValue() as database_user_name,\n\tdata.get('entry_id').textValue() as entry_id,\n\tdata.get('obj_name').textValue() as obj_name,\n\tdata.get('os_user_name').textValue() as os_user_name,\n\tdata.get('owner').textValue() as owner,\n\tdata.get('priv_used').textValue() as priv_used,\n\tdata.get('return_code').textValue() as return_code,\n\tdata.get('session_id').textValue() as session_id,\n\n\t-- geo\n\tdata.get('destination_country_code').textValue() as destination_country_code,\n\tdata.get('source_country_code').textValue() as source_country_code,\n\n\t-- network\n\tdata.get('data_size').textValue() as data_size,\n\tdata.get('destination').textValue() as destination,\n\tdata.get('destination_object').textValue() as destination_object,\n\tdata.get('destination_port').textValue() as destination_port,\n\tdata.get('sol_domain').textValue() as sol_domain,\n\tdata.get('event_manager').textValue() as event_manager,\n\tdata.get('event_path').textValue() as event_path,\n\tdata.get('event_source').textValue() as event_source,\n\tdata.get('msg_no').textValue() as msg_no,\n\tdata.get('protocol_id').textValue() as protocol_id,\n\tdata.get('service').textValue() as service,\n\tdata.get('sol_source').textValue() as sol_source,\n\tdata.get('source_mac_address').textValue() as source_mac_address,\n\tdata.get('source_object').textValue() as source_object,\n\tdata.get('source_port').textValue() as source_port,\n\tdata.get('status_code').textValue() as status_code,\n\tdata.get('target_path').textValue() as target_path,\n\n\t-- other\n\tdata.get('command').textValue() as command,\n\n\t-- security\n\tdata.get('affected_file').textValue() as affected_file,\n\tdata.get('object_action').textValue() as object_action,\n\tdata.get('object_action_result').textValue() as object_action_result,\n\tdata.get('object_id').textValue() as object_id,\n\tdata.get('object_name').textValue() as object_name,\n\tdata.get('object_path').textValue() as object_path,\n\tdata.get('process_name').textValue() as process_name,\n\n\t-- user\n\tdata.get('caller_user_name').textValue() as caller_user_name,\n\tdata.get('logon_type').textValue() as logon_type,\n\tdata.get('target_user_name').textValue() as target_user_name,\n\tdata.get('user_agent').textValue() as user_agent,\n\tdata.get('user_name').textValue() as user_name,\n\tdata.get('workstation').textValue() as workstation,\n\n\t-- web\n\tdata.get('web_file').textValue() as web_file,\n\tdata.get('web_method').textValue() as web_method,\n\tdata.get('web_page').textValue() as web_page,\n\tdata.get('web_protocol').textValue() as web_protocol\n\nFROM activeguard_rawstream;\n\n\n\nCREATE TARGET alertf USING FileWriter (\n  filename: 'JParser',\n  flushinterval: '0',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy: 'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00'\n )\n FORMAT USING JSONFormatter (\n\t\tmembers:'hash_key,client_id,event_type_id,type_code,rule_id,rule_name,rule_action,sce_id,message,ref_list_match,event_date,log_count,received_date,event_sub_source,sol_action,comment,comment_text,database_name,database_user_name,entry_id,obj_name,os_user_name,owner,priv_used,return_code,session_id,destination_country_code,source_country_code,data_size,destination,destination_object,destination_port,sol_domain,event_manager,event_path,event_source,msg_no,protocol_id,service,sol_source,source_mac_address,source_object,source_port,status_code,target_path,command,affected_file,object_action,object_action_result,object_id,object_name,object_path,process_name,caller_user_name,logon_type,target_user_name,user_agent,user_name,workstation,web_file,web_method,web_page,web_protocol',\n \t\trowdelimiter:'\\n'\n )\nINPUT FROM activeguard_stream;\n\ncreate Target z using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/Target_JParser_actual.log') input from activeguard_stream;\n\n\nEND APPLICATION DSV;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION SystemTimeTester.SystemTimeWindows;\nUNDEPLOY APPLICATION SystemTimeTester.SystemTimeWindows;\nDROP APPLICATION SystemTimeTester.SystemTimeWindows cascade;\n\n\nCREATE APPLICATION SystemTimeWindows;\n\nCREATE TYPE RandomData(\nbankNumber int KEY,\nbankName String\n);\n\nCREATE  SOURCE ranDataSource USING StreamReader (\n  OutputType: 'SystemTimeTester.RandomData',\n  noLimit: 'false',\n  isSeeded: 'true',\n  maxRows: 0,\n  iterations: 30,\n  iterationDelay: 1000,\n  StringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',\n  NumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]R'\n )\nOUTPUT TO CSVDataStream;\n\nCREATE STREAM RandomDataStream OF RandomData;\n\nCREATE CQ ParseRandomData\nINSERT INTO RandomDataStream\nSELECT data[0], data[1]\nFROM CSVDataStream;\n\nCREATE @WINDOWTYPE@ WINDOW tierone OVER RandomDataStream keep within 20 second;\n\nCREATE STREAM onetwostream OF RandomData;\n\nCREATE CQ onetwocq\nINSERT INTO onetwostream\nSELECT bankNumber,bankName\nFROM tierone\nwhere  bankName LIKE 'bofa'\norder by bankName;\n\nCREATE WACTIONSTORE MyDataActivity  CONTEXT OF RandomData\nEVENT TYPES ( RandomData  )\n@PERSIST-TYPE@\n\nCreate CQ TrackMyDataActivity\nINSERT INTO MyDataActivity\nSELECT bankNumber,bankName from @FROMSTREAM@\nwhere  bankName LIKE '%fa%'\norder by bankName\nLINK SOURCE EVENT;\n\nEND APPLICATION SystemTimeWindows;\ndeploy application SystemTimeWindows;\nstart application SystemTimeWindows;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@ recovery 1 second interval;\n\ncreate source @APPNAME@_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using CobolCopybookParser (\ncopybookFileName : '@TD@/@PROP1@',\n  dataFileFont: '@PROP2@',\n  copybookSplit: '@PROP3@',\n  dataFileOrganization: '@PROP4@',\n  copybookDialect: '@PROP5@', \n  skipIndent:'@PROP6@',\n  DatahandlingScheme:'@PROP7@',\n  recordSelector: 'OH:MOH-SEG-ID=OH,OH2:OH2-SEG-ID=OH2,OHU:OHU-SEG-ID=OHU,OR1:OR1-SEG-ID=OR1,OR2:OR2-SEG-ID=OR2,OR3:OR3-SEG-ID=OR3,OR3:OR3-SEG-ID=OR3,OHM:OHM-SEG-ID=OR1,OD:OD-SEG-ID=OD,ODU:ODU-SEG-ID=ODU,OD1:OD1-SEG-ID=OD1,ODM:ODM-SEG-ID=ODM,OT:OT-SEG-ID=OT'\n)\nOUTPUT TO @APPNAME@Stream;\n\ncreate Target @APPNAME@Target using FileWriter(\n    filename :'@FILE@',\n    directory : '@FOLDER@'\n)\nformat using JsonFormatter (\n)\ninput from @APPNAME@Stream;\n\n/*\ncreate Target @APPNAME@DBTarget using DatabaseWriter(\n  Username: 'qatest',\n  Password: 'qatest',\n  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',\n  BatchPolicy: 'EventCount:1,Interval:5',\n  CommitPolicy: 'EventCount:1,Interval:5',\n  Tables: 'QATEST.@table@'\n)\ninput from @APPNAME@Stream;*/\nend application @APPNAME@;\ndeploy application @APPNAME@ on all in default;\nstart application @APPNAME@;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop IR;\nundeploy application IR;\ndrop application IR cascade;\nCREATE APPLICATION IR;\n\nCreate Source s1 Using IncrementalBatchReader (\n FetchSize: 1,\n  Username: 'striim',\n  Password: 'o4l1uMpwIDQ=',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',\n  Tables: 'striim.autotest01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.autotest01=id',\n  startPosition: '%=0'\n )\nOUTPUT TO data_stream1;\n\ncreate source s2 using IncrementalBatchReader (\nFetchSize: 1,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',\n  Tables: 'striim.autotest02',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.autotest02=id',\n  startPosition: '%=0'\n )\nOUTPUT TO data_stream2;\n\ncreate source s3 using IncrementalBatchReader (\nFetchSize: 1,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',\n  Tables: 'striim.autotest03',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.autotest03=id',\n  startPosition: '%=0'\n )\nOUTPUT TO data_stream3;\n\nCreate Type EventType (\nID int,\nPIN int\n);\n\nCREATE STREAM insertData1  of EventType;\nCREATE STREAM deleteData1 of EventType;\nCREATE STREAM joinData1 of EventType;\nCREATE STREAM joinData2 of EventType;\nCREATE STREAM deleteData2 of EventType;\nCREATE STREAM OutStream of EventType;\n\nCREATE CQ cq1 INSERT INTO insertData1  SELECT TO_INT(data[0]),TO_INT(data[1]) FROM data_stream1;\n\nCREATE CQ cq2 INSERT INTO deleteData1 SELECT TO_INT(data[0]),TO_INT(data[1]) FROM data_stream2;\n\nCREATE CQ cq3 INSERT INTO joinData1 SELECT TO_INT(data[0]),TO_INT(data[1]) FROM data_stream3;\n\nCREATE JUMPING WINDOW DataWin1 OVER deleteData1 KEEP 1 ROWS;\n\nCREATE CQ TypedEventKeyCQ6 INSERT INTO deleteData2 SELECT * from DataWin1;\n\nCREATE JUMPING WINDOW DataWin2 OVER joinData1 KEEP 1 ROWS;\n\nCREATE CQ TypedEventKeyCQ5 INSERT INTO joinData2 SELECT * from DataWin2;\n\nCREATE EVENTTABLE ETABLE1 using STREAM ( NAME: 'insertData1 ' )\nDELETE using STREAM ( NAME: 'deleteData1')\nQUERY (keytomap:\"ID\", persistPolicy: 'true') OF EventType;\n\nCREATE CQ cq4 INSERT INTO OutStream SELECT B.ID,B.PIN FROM joinData2 A, ETABLE1 B where A.ID=B.ID;\n\nCREATE TARGET EventTableFW USING FileWriter\n(filename:'BasicIR_RT.log',\n rolloverpolicy: 'EventCount:1000000')\nFORMAT USING DSVFormatter () INPUT FROM OutStream;\n\ncreate target Target_Azure using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'STRIIM',\n        password: 'W3b@ct10n',\n        AccountName: 'striimqatestdonotdelete',\n        accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables:'dbo.autotest01',\n        uploadpolicy:'eventcount:1,interval:10s'\n) INPUT FROM OutStream;\n\nEND APPLICATION IR;\ndeploy application IR in default;\nstart IR;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@ recovery 5 second Interval;\nCREATE SOURCE @srcName@ USING PostgreSQLReader  \n\n(\n  ReplicationSlotName:'@srcreplicationslot@',\n  FilterTransactionBoundaries:'true',\n  Username:'@srcusername@',\n  Password_encrypted:false,\n  ConnectionURL: '@srcurl@',\n  adapterName:'PostgreSQLReader',\n  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',\n  Password:'@srcpassword@',\n  Tables:'@srcschema@.@srctable@'\n) \nOUTPUT TO @outstreamname@ ;\n\nCREATE OR REPLACE TARGET @tgtName@ USING DatabaseWriter  \n\n(\n  DatabaseProviderType:'Default',\n  CheckPointTable:'CHKPOINT',\n  PreserveSourceTransactionBoundary:'false',\n  Username:'@tgtusername@',\n  BatchPolicy:'EventCount:1,Interval:0',\n  CommitPolicy:'EventCount:1,Interval:0',\n  ConnectionURL:'@tgturl@',\n  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',\n  Password:'@tgtpassword@'\n) \nINPUT FROM @instreamname@;\nEnd APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application DualGen;\nundeploy application DualGen;\ndrop application DualGen cascade;\nCREATE APPLICATION DualGen;\n\nCREATE OR REPLACE TYPE DualEvent (\n    Dummy DateTime,\n    PhoneNo java.lang.String\n);\n\nCREATE OR REPLACE STREAM DualEvents OF DualEvent;\n\nCREATE OR REPLACE CQ GenDual \nINSERT INTO DualEvents\nSELECT\n  TO_DATEF('28-FEB-22',\"dd-MMM-yy\"),maskPhoneNumber('44 844 493 0787', \"(\\\\\\\\d{0,4}\\\\\\\\s)(\\\\\\\\d{0,4}\\\\\\\\s)([0-9 ]+)\", 1, 2)  as Dummy\nFROM\n   heartbeat(interval @INTERVAL@ second) h;\n\n\nCREATE OR REPLACE TARGET DualSys USING SysOut  ( \n  name: 'heartbeat_out'\n ) \nINPUT FROM DualEvents;\n\n\nCREATE TARGET DSVFormatterOut using FileWriter(\n filename:'HeartBeat_Output.log',\n flushpolicy:'EventCount:6',\n rolloverpolicy:'interval:97s')\nFORMAT USING DSVFormatter ()\nINPUT FROM DualEvents;\n\nEND APPLICATION DualGen;\ndeploy application DualGen;\nstart application DualGen;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE @SourceName@ USING PostgreSQLReader  (\n  ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: '@UserName@',\n  Password_encrypted: false,\n  ConnectionURL: '@SourceConnectionURL@',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: '@Password@',\n  Tables: '@SourceTable@'\n ) \nOUTPUT TO @SRCINPUTSTREAM@;\n\n\nCREATE OR REPLACE TARGET @targetsys@ USING SysOut (name: 'ora12_out') INPUT FROM @SRCINPUTSTREAM@;\n\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@UserName@',\n  Password_encrypted: 'false',\n  BatchPolicy: 'EventCount:1,Interval:60',\n  CommitPolicy: 'EventCount:1,Interval:60',\n  ConnectionURL: '@TargetConnectionURL@',\n  Tables: '@SourceTable@,@TargetTable@',\n\n  adapterName: 'PostgreSQLReader',\n  Password: '@Password@'\n )\nINPUT FROM @SRCINPUTSTREAM@;\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop APPLICATION OrcToDWH;\nundeploy APPLICATION OrcToDWH;\nDROP APPLICATION OrcToDWH CASCADE;\nCREATE APPLICATION OrcToDWH recovery 5 second interval;\nCreate Source OracleSource Using OracleReader\n(\n Username:'@ORACLE-USERNAME',\n Password:'@ORACLE-PASSWORD',\n ConnectionURL: '@ORACLE-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'@FETCH-SIZE@'\n) \nOutput To str;\n\n\ncreate target AzureTarget using AzureSQLDWHWriter (\n\t\tConnectionURL: '@SQLDW-URL@',\n        username: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@',\n        uploadpolicy:'@EVENT-COUNT@'\n        _h_TransportOptions:'connectionTimeout=30s, readTimeout=12s',\n) INPUT FROM str;\n\nEND APPLICATION OrcToDWH;\ndeploy APPLICATION OrcToDWH;\nstart APPLICATION OrcToDWH;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "DROP TYPE MerchantActivityContext;\nDROP TYPE MerchantTxRate;\nDROP WACTIONSTORE MerchantActivity;\n\nCREATE TYPE MerchantActivityContext(\n  MerchantId String KEY,\n  StartTime  DateTime,\n  CompanyName String,\n  Category String,\n  Status String,\n  Count int,\n  HourlyAve int,\n  UpperLimit double,\n  LowerLimit double,\n  Zip String,\n  City String,\n  State String,\n  LatVal double,\n  LongVal double\n);\n\nCREATE TYPE MerchantTxRate(\n  merchantId String KEY,\n  zip String,\n  startingTime DateTime,\n  count int,\n  totalAmount double,\n  hourlyAve int,\n  upperLimit double,\n  lowerLimit double,\n  category String,\n  status String\n);\n\n-- Loading WACTIONSTORE MerchantActivity to be accessed by PosAppWS.tql\n\nCREATE WACTIONSTORE MerchantActivity CONTEXT OF DS.MerchantActivityContext\nEVENT TYPES ( DS.MerchantTxRate )\n@PERSIST-TYPE@\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 2\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS\n--\n\n\nSTOP KStreamRecov2Tester.KStreamRecovTest2;\nUNDEPLOY APPLICATION KStreamRecov2Tester.KStreamRecovTest2;\nDROP APPLICATION KStreamRecov2Tester.KStreamRecovTest2 CASCADE;\nDROP USER KStreamRecov2Tester;\nDROP NAMESPACE KStreamRecov2Tester CASCADE;\nCREATE USER KStreamRecov2Tester IDENTIFIED BY KStreamRecov2Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov2Tester;\nCONNECT KStreamRecov2Tester KStreamRecov2Tester;\n\nCREATE APPLICATION KStreamRecovTest2 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE TYPE WactionType (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionType;\n\nCREATE CQ CsvToWaction\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND APPLICATION KStreamRecovTest2;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application BigqueryBulkLoadMonMetrics;\nundeploy application BigqueryBulkLoadMonMetrics;\ndrop application BigqueryBulkLoadMonMetrics cascade;\n\nCREATE APPLICATION BigqueryBulkLoadMonMetrics;\n\nCREATE FLOW BigqueryBulkLoadMonMetrics_SourceFlow;\n\nCREATE SOURCE BigqueryBulkLoadMonMetrics_DBSource USING DatabaseReader ( \n  Username: 'qatest', \n  DatabaseProviderType: 'ORACLE', \n  FetchSize: 10000, \n  Password_encrypted: 'false', \n  QuiesceOnILCompletion: 'true', \n  Password: 'JVaLv3ZpgQDY8R2ZxS38xg==', \n  Tables: 'QATEST.EMPLOYEE', \n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe' ) \nOUTPUT TO BigqueryBulkLoadMonMetrics_OutputStream;\n\nEND FLOW BigqueryBulkLoadMonMetrics_SourceFlow;\n\nCREATE OR REPLACE TARGET BigqueryBulkLoadMonMetrics_BigQueryTarget1 USING BigQueryWriter ( \n  ColumnDelimiter: '|', \n  NullMarker: 'NULL', \n  streamingUpload: 'false', \n  projectId: 'striimqa-214712', \n  Encoding: 'UTF-8', \n  BatchPolicy: 'eventCount:1000000, Interval:90', \n  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30', \n  AllowQuotedNewLines: 'false', \n  optimizedMerge: 'false', \n  TransportOptions: 'connectionTimeout=300, readTimeout=120', \n  adapterName: 'BigQueryWriter', \n  Mode: 'MERGE', \n  Tables: 'QATEST.EMPLOYEE,DEV22862jen.sample', \n  StandardSQL: 'true', \n  includeInsertId: 'true', \n  QuoteCharacter: '\\\"', \n  ServiceAccountKey: '/Users/jenniffer/Product2/IntegrationTests/TestData/google-gcs.json' ) \nINPUT FROM BigqueryBulkLoadMonMetrics_OutputStream;\n\nEND APPLICATION BigqueryBulkLoadMonMetrics;\n\ndeploy application BigqueryBulkLoadMonMetrics;\nstart application BigqueryBulkLoadMonMetrics;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@app4;\nundeploy application @APPNAME@app4;\nalter application @APPNAME@app4;\nCREATE or replace TARGET @APPNAME@app4_targetNew USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test01,QATEST.KPS4_Alter'\n) INPUT FROM @APPNAME@sourcestream;\nalter application @APPNAME@app4 recompile;\ndeploy application @APPNAME@app4;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP OuterJoinTester.OJApp;\nUNDEPLOY APPLICATION OuterJoinTester.OJApp;\nDROP APPLICATION OuterJoinTester.OJApp cascade;\n\nCREATE APPLICATION OJApp;\n\n\nCREATE TYPE bankData\n(\nucID Integer,\nucLong long,\nucDate DateTime,\nucDouble Double KEY\n);\n\n\nCREATE STREAM wsStream OF bankData;\n\nCREATE CACHE cache1 USING CsvReader(\ndirectory: '@TEST-DATA-PATH@',\nwildcard: 'ucData.csv',\nheader: No,\ncolumndelimiter: ',',\ntrimquote: false\n) QUERY (keytomap:'ucDouble') OF bankData;\n\n\n\nEND APPLICATION OJApp;\n\n\n\n\n\n\n\n\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use PosTester;\nalter application PosApp;\n\nCREATE JUMPING WINDOW PosData5Minutes\nOVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nend application PosApp;\n\nalter application PosApp recompile;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "undeploy application reconnect;\nalter application reconnect;\n\ncreate or replace TARGET dbtarget USING DatabaseWriter  (\n  ConnectionURL:'@URL@',\n  Username:'@USERNAME@',\n  Password:'@PASSWORD@',\n  ConnectionRetryPolicy: 'retryInterval=20s, maxRetries=3',\n  BatchPolicy:'EventCount:5,Interval:30',\n  CommitPolicy:'EventCount:5,Interval:30',\n  Tables: '@TABLES@'\n )\nINPUT FROM sqlstream;\n\nalter application reconnect recompile;\ndeploy application reconnect;\nstart application reconnect;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCreate Source @APPNAME@_src Using OracleReader\n(\n Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n   _h_ReturnDateTimeAs:'ZonedDateTime'\n) Output To @APPNAME@_stream;\n\ncreate Target @APPNAME@_tgt using FileWriter(\n  filename:'CompressedMerchant.gz',\n  directory:'/logs/',\n  rolloverpolicy:'EventCount:10000'\n)\nformat using ParquetFormatter (\n\tschemaFileName:'@FILENAME@',\n\tFormatAs:'@FORMATAS@'\n)\ninput from @APPNAME@_stream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE @SOURCE@ USING Ojet  ( \n  FilterTransactionBoundaries: true,\n  ConnectionURL: '@OCI-URL@',\n  Tables: '@SOURCE_TABLE@',\n  Password: '@OJET-PASSWORD@',\n  fetchsize: 1,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  Username: '@OJET-UNAME@'\n ) \nOUTPUT TO @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET@1 USING BigQueryWriter ( \n  ColumnDelimiter: '|', \n  NullMarker: 'NULL', \n  streamingUpload: 'false', \n  BatchPolicy: 'eventCount:1000000, Interval:90', \n  Tables: 'sample.tab', \n  ServiceAccountKey: '', \n  projectId: 'striimqa-214712', \n  Encoding: 'UTF-8', \n  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30', \n  AllowQuotedNewLines: 'false', \n  CDDLAction: 'Process', \n  optimizedMerge: 'false', \n  TransportOptions: 'connectionTimeout=300, readTimeout=120', \n  Mode: 'MERGE', \n  StandardSQL: 'true', \n  includeInsertId: 'true', \n  QuoteCharacter: '\\\"' ) \nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET@2 USING BigQueryWriter ( \n  ColumnDelimiter: '|', \n  NullMarker: 'NULL', \n  streamingUpload: 'false', \n  BatchPolicy: 'eventCount:1000000, Interval:90', \n  Tables: 'sample.tab', \n  ServiceAccountKey: '', \n  projectId: 'striimqa-214712', \n  Encoding: 'UTF-8', \n  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30', \n  AllowQuotedNewLines: 'false', \n  CDDLAction: 'Process', \n  optimizedMerge: 'true', \n  TransportOptions: 'connectionTimeout=300, readTimeout=120', \n  Mode: 'MERGE', \n  StandardSQL: 'true', \n  includeInsertId: 'true', \n  QuoteCharacter: '\\\"' ) \nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET@3 USING BigQueryWriter ( \n  ColumnDelimiter: '|', \n  NullMarker: 'NULL', \n  streamingUpload: 'false', \n  BatchPolicy: 'eventCount:1000000, Interval:90', \n  Tables: 'sample.tab', \n  ServiceAccountKey: '', \n  projectId: 'striimqa-214712', \n  Encoding: 'UTF-8', \n  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30', \n  AllowQuotedNewLines: 'false', \n  CDDLAction: 'Process', \n  optimizedMerge: 'false', \n  TransportOptions: 'connectionTimeout=300, readTimeout=120', \n  Mode: 'APPENDONLY', \n  StandardSQL: 'true', \n  includeInsertId: 'true', \n  QuoteCharacter: '\\\"' ) \nINPUT FROM @STREAM@;\n\nEND APPLICATION @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Canon Test W30\n-- Nicholas Keene, WebAction, Inc.\n--\n-- Basic test for an unpartitioned jumping count window\n--\n-- S -> JWc5u -> CQ -> WS\n--\n\n\nUNDEPLOY APPLICATION NameW30.W30;\nDROP APPLICATION NameW30.W30 CASCADE;\nCREATE APPLICATION W30 RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE FLOW DataAcquisitionW30;\n\n\nCREATE SOURCE CsvSourceW30 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'Canon1000.csv',\n  columndelimiter:',',\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStreamW30;\n\n\nEND FLOW DataAcquisitionW30;\n\n\n\nCREATE FLOW DataProcessingW30;\n\nCREATE TYPE DataTypeW30 (\n    eventID String,\n    word String KEY,\n    dateTime DateTime\n);\n\nCREATE STREAM DataStreamW30 OF DataTypeW30;\n\nCREATE CQ CSVStreamW30_to_DataStreamW30\nINSERT INTO DataStreamW30\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[2],'yyyyMMddHHmmss')\nFROM CsvStreamW30;\n\nCREATE JUMPING WINDOW JWc5uW30\nOVER DataStreamW30\nKEEP 5 ROWS;\n\nCREATE WACTIONSTORE WactionStoreW30 CONTEXT OF DataTypeW30\nEVENT TYPES ( DataTypeW30 KEY(word) )\n@PERSIST-TYPE@\n\nCREATE CQ JWc5uW30_to_WactionStoreW30\nINSERT INTO WactionStoreW30\nSELECT\n    FIRST(eventID),\n    FIRST(word),\n    FIRST(dateTime)\nFROM JWc5uW30;\n\nEND FLOW DataProcessingW30;\n\n\n\nEND APPLICATION W30;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE TARGET @TARGET_NAME@ USING SpannerWriter (\n\tTables: 'QATEST.%,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tinstanceId: 'qatest',\n\tbatchPolicy: 'EventCount: 10000,Interval:600s'\n) INPUT FROM @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\ncreate flow @APPNAME@_agentflow;\n\nCreate Source @SOURCE@\n        Using OracleReader\n(\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: 'qatest.test77',\n  FetchSize:1,\n  QueueSize:2000,\n  CommittedTransactions:true,\n  Compression:false\n)\nOutput To @STREAM@;\n\nend flow @APPNAME@_agentflow;\n\ncreate flow @APPNAME@_serverflow;\n\ncCREATE TARGET @TARGET@ USING RedshiftWriter\n\t(\n\t  ConnectionURL: 'tgt_url',\n\t  Username: 'tgt_username',\n\t  Password: 'tgt_pwrd',\n\t  bucketname: 'bucket_name',\n\t  --accesskeyId: 'access_key',\n\t  --secretaccesskey: 'secret_access',\n\t  Tables: 'QATEST.%,QATEST.%',\n\t   S3IAMRole:'@IAMROLE@',\n\tuploadpolicy:'EventCount:7'\n\t) INPUT FROM @STREAM@;\n\t\nend flow @APPNAME@_serverflow;\n\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "-- The PosApp sample application demonstrates how a credit card\n-- payment processor might use Striim to generate reports on current\n-- transaction activity by merchant and send alerts when transaction\n-- counts for a merchant are higher or lower than average for the time\n-- of day.\n\n\nSTOP admin.PosApp;\nUNDEPLOY APPLICATION admin.PosApp;\nDrop Application admin.PosApp cascade;\nCREATE APPLICATION PosApp;\n\n-- All CREATE statements between here and the END APPLICATION\n-- statement will create objects in the PosApp application.\n\n-- source CsvDataSource\n\nCREATE source CsvDataSource USING FileReader (\n  directory:'/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/appData',\n  wildcard:'posdata.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\n\n-- This is the primary data source for this application.\n-- In a real-world application, it would be real-time data. Here,\n-- the data comes from a comma-delimited file, posdata.csv. The first\n-- two lines of that file are:\n--\n-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY\n-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand\n--\n-- The \"header:Yes\" setting tells Striim that the first line contains\n-- field labels that should not be treated as data.\n--\n-- The \"positionByEOF:false\" setting tells Striim to start reading\n-- from the beginning of the file. (In a real-world application\n-- reading real log files, you would typically use the default \"true\"\n-- setting so that the application would read only new data.)\n--\n-- The OUTPUT TO clause automatically creates the stream\n-- CsvStream using the WAEvent type associated with the CSVReader\n-- adapater. The only field from WAEvent used by this application\n-- is \"data\", an array containing the delimited fields.\n\n\n-- CQ CsvToPosData\n\nCREATE CQ CsvToPosData\nINSERT INTO PosDataStream partition by merchantId\nSELECT TO_STRING(data[1]) as merchantId,\n       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,\n       TO_DOUBLE(data[7]) as amount,\n       TO_STRING(data[9]) as zip\nFROM CsvStream;\n\n-- Here, \"data\" refers to the array mentioned above, and the number\n-- in brackets specifies a field from the array, counting from zero.\n-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH\n-- AMOUNT, and data[9] is ZIP.\n--\n-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as\n-- the types to be used in PosDataStream, which is created automatically.\n\n-- The DATETIME field from the source is converted to both a dateTime\n-- value, used as the event timestamp by the application, and (via the\n-- function) an integer hourValue, which is used to look up\n-- historical hourly averages from the HourlyAveLookup cache,\n-- discussed below.\n--\n-- The other six fields are discarded. Thus the first line of data\n-- from posdata.csv has at this point been reduced to five values:\n--\n-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)\n-- 20130312173210 (DateTime)\n-- 17 (hourValue)\n-- 2.20 (amount)\n-- 41363 (zip)\n\n\n-- CQ GenerateMerchantTxRateOnly\n--\n-- The PosData5Minutes window bounds the data so that the query can\n-- use the COUNT, FIRST, and SUM functions and join data from the\n-- HourlyAveLookup cache. (Aggregate functions cannot be used and\n-- joins cannot be performed on unbound real-time data.)\n--\n-- The HourlyAveLookup cache provides historical average sales\n-- amounts for the current hour for each merchant.\n\nCREATE JUMPING WINDOW PosData5Minutes\nOVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;\n\nCREATE TYPE MerchantHourlyAve(\n  merchantId String,\n  hourValue integer,\n  hourlyAve integer\n);\nCREATE CACHE HourlyAveLookup using FileReader (\n  directory: '/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/appData',\n  wildcard: 'hourlyData.txt'\n)\nPARSE USING DSVParser (\n  header: Yes,\n  trimquote:false,\n  trimwhitespace:true\n) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;\n\nCREATE TYPE MerchantTxRate(\n  merchantId String KEY,\n  zip String,\n  startTime DateTime,\n  count integer,\n  totalAmount double,\n  hourlyAve integer,\n  upperLimit double,\n  lowerLimit double,\n  category String,\n  status String\n);\nCREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;\n\nCREATE CQ GenerateMerchantTxRateOnly\nINSERT INTO MerchantTxRateOnlyStream\nSELECT p.merchantId,\n       FIRST(p.zip),\n       FIRST(p.dateTime),\n       COUNT(p.merchantId),\n       SUM(p.amount),\n       FIRST(l.hourlyAve/12),\n       FIRST(l.hourlyAve/12 * CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       FIRST(l.hourlyAve/12 / CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       '<NOTSET>',\n       '<NOTSET>'\nFROM PosData5Minutes p, HourlyAveLookup l\nWHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue\nGROUP BY p.merchantId;\n\n-- This query aggregates five minutes' worth of data for each\n-- merchant, calculating the total transaction count and amount, and\n-- calculates the upperLimit and lowerLimit values based on the\n-- historical average transaction count for the current hour of the\n-- day from the HourlyAveLookup cache. The category and status fields\n-- are left unset to be populated by the next query.\n\n\n-- CQ GenerateMerchantTxRateWithStatus\n--\n-- This query sets the count values used by the Dashboard map and the\n-- status values used to trigger alerts.\n\nCREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;\n\nCREATE CQ GenerateMerchantTxRateWithStatus\nINSERT INTO MerchantTxRateWithStatusStream\nSELECT merchantId,\n       zip,\n       startTime,\n       count,\n       totalAmount,\n       hourlyAve,\n       upperLimit,\n       lowerLimit,\n       CASE\n         WHEN count >10000 THEN 'HOT'\n         WHEN count > 800 THEN 'WARM'\n         WHEN count >200 THEN 'COOL'\n         ELSE 'COLD' END,\n       CASE\n         WHEN count > upperLimit THEN 'TOOHIGH'\n         WHEN count < lowerLimit THEN 'TOOLOW'\n         ELSE 'OK' END\nFROM MerchantTxRateOnlyStream;\n\n\n-- WAction store MerchantActivity\n--\n-- The following group of statements create and populate the MerchantActivity\n-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced\n-- with merchant details from NameLookup cache and with latitude and longitude\n-- values from the USAddressData cache.\n\nCREATE TYPE MerchantActivityContext(\n  MerchantId String KEY,\n  StartTime  DateTime,\n  CompanyName String,\n  Category String,\n  Status String,\n  Count integer,\n  HourlyAve integer,\n  UpperLimit double,\n  LowerLimit double,\n  Zip String,\n  City String,\n  State String,\n  LatVal double,\n  LongVal double\n);\n\nCREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext\nEVENT TYPES ( MerchantTxRate )\nPERSIST EVERY 10 second USING ( JDBC_DRIVER:'org.apache.derby.jdbc.ClientDriver',  JDBC_URL:'jdbc:derby://192.168.1.5:1527/wactionrepos', JDBC_USER:'waction', JDBC_PASSWORD:'w@ct10n', pu_name:derby, DDL_GENERATION:'create-or-extend-tables',  LOGGING_LEVEL:'SEVERE' );\n\n\nCREATE TYPE MerchantNameData(\n  merchantId String KEY,\n  companyName String\n);\n\nCREATE TYPE USAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\nCREATE CACHE NameLookup using FileReader (\n  directory:'/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/appData',\n  WildCard:'MerchantNames.csv',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:'yes',\n  trimquote:false,\n  trimwhitespace:true\n) \nQUERY(keytomap:'merchantId') OF MerchantNameData;\n\nCREATE CACHE ZipLookup using FileReader (\n  directory: '/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/appData',\n  wildcard: 'USAddresses.txt',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header: Yes,\n  columndelimiter: '\\t',\n  trimquote:false,\n  trimwhitespace:true\n) QUERY (keytomap:'zip') OF USAddressData;\n\n\nCREATE CQ GenerateWactionContext\nINSERT INTO MerchantActivity\nSELECT  m.merchantId,\n        m.startTime,\n        n.companyName,\n        m.category,\n        m.status,\n        m.count,\n        m.hourlyAve,\n        m.upperLimit,\n        m.lowerLimit,\n        m.zip,\n        z.city,\n        z.state,\n        z.latVal,\n        z.longVal\nFROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z\nWHERE m.merchantId = n.merchantId AND m.zip = z.zip\nLINK SOURCE EVENT;\n\n-- CQ GenerateAlerts\n--\n-- This CQ sends an alert when a merchant's status value changes to\n-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.\n\n\nCREATE STREAM AlertStream OF Global.AlertEvent;\n\nCREATE CQ GenerateAlerts\nINSERT INTO AlertStream\nSELECT n.CompanyName,\n       m.MerchantId,\n       CASE\n         WHEN m.Status = 'OK' THEN 'info'\n         ELSE 'warning' END,\n       CASE\n         WHEN m.Status = 'OK' THEN 'cancel'\n         ELSE 'raise' END,\n       CASE\n         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)\n         ELSE ''\n         END\nFROM MerchantTxRateWithStatusStream m, NameLookup n\nWHERE m.merchantId = n.merchantId;\n\nCREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;\n\nEND APPLICATION PosApp;\n\nCREATE DASHBOARD USING \"/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/PosAppDashboard.json\";\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nCREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_DBSource USING Global.OracleReader (\n  TransactionBufferDiskLocation: '.striim/LargeBuffer',\n  Compression: false,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  Password_encrypted: 'false',\n  SupportPDB: false,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  FetchSize: 1000,\n  CDDLAction: 'Process',\n  ConnectionURL: '192.168.56.3:1521:orcl',\n  DictionaryMode: 'OnlineCatalog',\n  QueueSize: 2048,\n  CommittedTransactions: true,\n  SetConservativeRange: false,\n  CDDLCapture: false,\n  Username: 'fan',\n  Tables: 'FAN.S_BLOB',\n  TransactionBufferType: 'Disk',\n  Password: '9S5GnbGmBQNDD5c/baD0Tw==',\n  TransactionBufferSpilloverSize: '100MB',\n  FilterTransactionBoundaries: true,\n  SendBeforeImage: true,\n  DatabaseRole: 'Primary' )\nOUTPUT TO @APPNAME@_stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_target USING Global.BigQueryWriter (\n  ColumnDelimiter: '|',\n  projectId: 'striim-support',\n  BatchPolicy: 'eventCount:1, Interval:1',\n  NullMarker: 'NULL',\n  streamingUpload: 'false',\n  ServiceAccountKey: '/Users/fzhang/fan/u01/app/product/striim/striim_latest/UploadedFiles/striim-support-286429beb74d.json',\n  Encoding: 'UTF-8',\n  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30',\n  AllowQuotedNewLines: 'false',\n  CDDLAction: 'Process',\n  optimizedMerge: 'false',\n  Tables: 'FAN.S_BLOB,Fan.s_blob columnmap(A=A,B=B,C=C,D=C,E=@metadata(OperationName));',\n  TransportOptions: 'connectionTimeout=300, readTimeout=120',\n  adapterName: 'BigQueryWriter',\n  Mode: 'APPENDONLY',\n  StandardSQL: 'true',\n  includeInsertId: 'true',\n  QuoteCharacter: '\\\"' )\nINPUT FROM @APPNAME@_stream;\n\n\nEND APPLICATION @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application DhcpLog;\ncreate source DHCPLogSource using FileReader (\n\tdirectory:'@TEST-DATA-PATH@',\n\tWildCard:'Dhcp*',\n\tcharset:'UTF-8',\n\tpositionByEOF:false\n) PARSE USING DHCPLogParser (\n\trowdelimiter:'\\r\\n',\n\tLineNumber:33\n)\nOUTPUT TO DHCPLogStream;\ncreate Target DHCPDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/dhcp_log') input from DHCPLogStream;\nend application DhcpLog;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@;\n\nCREATE FLOW @appname@AgentFlow;\n    CREATE SOURCE @parquetsrc@ USING FileReader (\n    wildcard: '',\n    directory: '',\n    positionbyeof: false )\n    PARSE USING ParquetParser (\n    )\n    OUTPUT TO @appname@Stream;\nEND FLOW @appname@AgentFlow;\n\nCREATE FLOW @appname@serverFlow;\n    CREATE CQ @appname@CQ\n    INSERT INTO @appname@CqOut\n        SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;\n    \n    CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (\n    filename: '',\n    directory: '',\n    flushpolicy: 'EventCount:1,Interval:30s',\n    rolloverpolicy: 'EventCount:10000,Interval:30s' )\n    FORMAT USING ParquetFormatter  (\n    schemaFileName: ''\n    )\n    INPUT FROM @appname@CqOut;\nEND FLOW @appname@serverFlow;\n\nEND APPLICATION @appname@;\nDEPLOY APPLICATION @appname@ with @appname@AgentFlow in Agents, @appname@ServerFlow in default;\nstart application @appname@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application KinesisTest;\nCREATE OR REPLACE SOURCE OS USING OracleReader (\n  Username: 'miner',\n  Password: 'miner',\n  ConnectionURL: '192.168.1.113:1521:ORCL',\n  TABLES: 'QATEST.H_REGION;QATEST.H_NATION;QATEST.H_CUSTOMER',\n  FetchSize: '1'\n )\nOUTPUT TO DDLCDCStream;\n\ncreate or replace Target t using KinesisWriter (\n\tregionName:'TARGET_REGION',\n\tstreamName:'TARGET_STREAM',\n\taccesskeyid:'ACCESS_KEY',\n\tsecretaccesskey:'SECRET_KEY'\n)\nformat using JSONFormatter (\n)\ninput from DDLCDCStream;\n\nend application KinesisTest;\ndeploy application KinesisTest in default;\nstart application KinesisTest;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP TestAlertsSMS.TestAlertsSmsApp;\nUNDEPLOY APPLICATION TestAlertsSMS.TestAlertsSmsApp;\nDROP APPLICATION TestAlertsSMS.TestAlertsSmsApp CASCADE;\n\nCREATE APPLICATION TestAlertsSmsApp;\n\nCREATE source rawSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:No,\n  wildcard:'alerts_csv.txt',\n  coldelimiter:' ',\n  positionByEOF:false\n) OUTPUT TO rawStream;\n\nCREATE STREAM MyAlertStream OF Global.AlertEvent;\n\nCREATE CQ GenerateMyAlerts\nINSERT INTO MyAlertStream (name, keyVal, severity, flag, message)\nSELECT \"Testing Alerts\", trimStr(data[0]), trimStr(data[1]), trimStr(data[2]), trimStr(data[3])\nFROM rawStream s;\n\nCREATE TARGET output2 USING SysOut(name : alertsrecevied) input FROM MyAlertStream;\n\nCREATE SUBSCRIPTION smsAlertSubscription4 USING ClickatellSMSAdapter\n(\nclickatelluserName:\"@Alerts_smsuser@\",\nCLICKaTELLpasSWORD:\"@Alerts_smspassword@\",\nclickatellapiID:\"@Alerts_smsid@\",\nuserIds:\"@Alerts_smsuserid@\",\nthreadCount:\"@Alerts_smsthreadcount@\",\nsenderNumber:\"@Alerts_smssenderno@\",\nphoneNumberList:\"@Alerts_smsphonelist@\" ) INPUT FROM MyAlertStream;\n\nEND APPLICATION TestAlertsSmsApp;\nDEPLOY APPLICATION TestAlertsSMS.TestAlertsSmsApp;\nSTART TestAlertsSMS.TestAlertsSmsApp;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application APP_KAFKA_DATASOURCES;\nundeploy application APP_KAFKA_DATASOURCES;\ndrop application APP_KAFKA_DATASOURCES cascade;\n\nCREATE APPLICATION APP_KAFKA_DATASOURCES;\n\nCREATE OR REPLACE SOURCE SRC_FR_KAFKA_HOURLYTOTALS USING Global.FileReader (\n  adapterName: 'FileReader',\n  rolloverstyle: 'Default',\n  blocksize: 64,\n  skipbom: true,\n  wildcard: 'kafka_hourly_total*.txt',\n  directory: '@confDir@',\n  includesubdirectories: false,\n  positionbyeof: false ) \nPARSE USING Global.DSVParser (\n  trimwhitespace: false,\n  linenumber: '-1',\n  columndelimittill: '-1',\n  trimquote: true,\n  ignoreemptycolumn: false,\n  parserName: 'DSVParser',\n  quoteset: '\\\"',\n  handler: 'com.webaction.proc.DSVParser_1_0',\n  charset: 'UTF-8',\n  columndelimiter: ':',\n  ignoremultiplerecordbegin: 'true',\n  ignorerowdelimiterinquote: false,\n  separator: ',',\n  header: false,\n  blockascompleterecord: false,\n  rowdelimiter: '\\n',\n  nocolumndelimiter: false,\n  headerlineno: 0 )\nOUTPUT TO STREAM_SRC_FR_KAFKA_HOURLYTOTALS;\n\nCREATE OR REPLACE TYPE SRC_FILECACHE_KAFKA_HOURLYTOTALS_Type (\n topic java.lang.String KEY,\n timerange java.lang.Integer,\n rawdatacount java.lang.Integer);\n\nCREATE OR REPLACE CQ CQ_FR_KAFKA_HOURLYTOTALS_COLMAP\nINSERT INTO STREAM_CQ_FR_KAFKA_HOURLYTOTALS_COLMAP\nSELECT to_string(data[0]) as topic, to_int(data[1]) as timerange, to_int(data[2]) as rawdatacount, to_string(to_string(data[0])+ '-' + to_string(data[1])) as TopicTimeRange FROM STREAM_SRC_FR_KAFKA_HOURLYTOTALS s;\n\nCREATE OR REPLACE EVENTTABLE ET_HOURLYTOTALS_KAFKADATA_FILE USING STREAM (\n  name: 'STREAM_CQ_FR_KAFKA_HOURLYTOTALS_COLMAP' )\nQUERY (\n  keytomap: 'TopicTimeRange',\n  replicas: 'all',\n  persistPolicy: 'true' )\nOF STREAM_CQ_FR_KAFKA_HOURLYTOTALS_COLMAP_Type;\n\nCREATE WINDOW SLIDE_WND_HOURLYTOTALS_KAFKADATA_FILE OVER STREAM_CQ_FR_KAFKA_HOURLYTOTALS_COLMAP\nKEEP WITHIN 1 DAY\nPARTITION BY timerange;\n\nEND APPLICATION APP_KAFKA_DATASOURCES;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE PROPERTYVARIABLE Partitionkey1='@metadata(RecordOffset)';\nCREATE OR REPLACE PROPERTYVARIABLE Partitionkey2='Col1';\nCREATE OR REPLACE PROPERTYVARIABLE OperationTimeout='500000';\nCREATE OR REPLACE PROPERTYVARIABLE ConnectionRetryPolicy='Retries:5,RetryBackOff:1m';\n\nSTOP APPLICATION EH;\nUNDEPLOY APPLICATION EH;\nDROP APPLICATION EH CASCADE;\nCREATE APPLICATION EH @Recovery@;\nCREATE SOURCE s USING FileReader (\n\tdirectory:'Product/IntegrationTests/TestData/',\n    WildCard:'posdata.csv',\n\tpositionByEOF:false\n\t)\nPARSE USING DSVParser (\n\theader:yes\n)OUTPUT TO ss;\n\nCREATE TYPE userType(\nCol1 String,\nCol2 String,\nCol3 String,\nCol4 String,\nCol5 String\n);\n\nCREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;\n\nCREATE CQ cq1\nINSERT INTO userDefinedTypedStream\nSELECT data[0],data[1],data[2], data[3],data[4]\nFROM ss;\n\ncreate Target t1 using AzureEventHubWriter (\n\tEventHubNamespace:'qatestnov9',\n\tEventHubName:'test_2_partitions',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tPartitionkey:'$Partitionkey1',\n\t--ParallelThreads:'2',\n\tConsumerGroup:'reader',\n\tE1P:'true',\n\tOperationTimeout:'$OperationTimeout',\n\tConnectionRetryPolicy:'$ConnectionRetryPolicy'\n)\nformat using AvroFormatter (\n\tschemaFileName:'kafkaAvroTest1.avsc')\ninput from ss;\n\ncreate Target t2 using AzureEventHubWriter (\n\tEventHubNamespace:'qatestnov9',\n\tEventHubName:'test_2_partitions',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\t--ParallelThreads:'2',\n\tPartitionkey:'$Partitionkey2',\n\tConsumerGroup:'reader',\n\tE1P:'true',\n\tOperationTimeout:'500000',\n\tConnectionRetryPolicy:'$ConnectionRetryPolicy'\n)\nformat using AvroFormatter (\n\tschemaFileName:'kafkaAvroTest2.avsc')\ninput from userDefinedTypedStream;\n\nEND APPLICATION EH;\nDEPLOY APPLICATION EH;\nstart application EH;\n\n\nSTOP APPLICATION ER;\nUNDEPLOY APPLICATION ER;\nDROP APPLICATION ER CASCADE;\nCREATE APPLICATION ER;\nCREATE SOURCE ER_S1 USING AzureEventHubReader (\n    EventHubNamespace:'qatestnov9',\n\tEventHubName:'test_2_partitions',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'Default',\n\tstartSeqNo:'0'\t\n\t)\nPARSE USING avroParser (\nschemaFileName:'kafkaAvroTest1.avsc'\n)OUTPUT TO ER_SS1;\nCREATE SOURCE ER_S2 USING AzureEventHubReader (\n    EventHubNamespace:'qatestnov9',\n\tEventHubName:'test_2_partitions',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'Default',\n\tstartSeqNo:'0'\t\n\t)\nPARSE USING avroParser (\nschemaFileName:'kafkaAvroTest2.avsc'\n)OUTPUT TO ER_SS2;\n\ncreate Type CustType \n(writerdata com.fasterxml.jackson.databind.JsonNode\n--TopicName java.lang.String,\n--PartitionID java.lang.String\n);\n\nCreate Stream datastream1 of CustType;\nCreate Stream datastream2 of CustType;\n\nCREATE CQ CustCQ1\nINSERT INTO datastream1\nSELECT AvroToJson(s1.data)\n--metadata.get(\"TopicName\").toString() AS TopicName,\n--metadata.get(\"PartitionID\").toString() AS PartitionID\nFROM ER_SS1 s1;\n\nCREATE CQ CustCQ2\nINSERT INTO datastream2\nSELECT AvroToJson(s2.data)\n--metadata.get(\"TopicName\").toString() AS TopicName,\n--metadata.get(\"PartitionID\").toString() AS PartitionID\nFROM ER_SS2 s2;\n\ncreate Target ER_t1 using FileWriter (\nfilename:'FT1_5L_AVRO_RT',\ndirectory:'FEATURE-DIR/logs/',\nrolloverpolicy:'eventcount:5000000'\t\n)\nformat using dsvFormatter()\ninput from datastream1;\n\ncreate Target ER_t2 using FileWriter (\nfilename:'FT2_5L_AVRO_RT',\ndirectory:'FEATURE-DIR/logs/',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000'\t\n)\nformat using dsvFormatter()\ninput from datastream2;\nend application ER;\ndeploy application ER;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application consoletest.noApp;\nundeploy application consoletest.noApp;\ndrop application consoletest.noApp cascade;\n\ncreate application noApp;\ncreate source CSVSource using FileReader (\n  directory:'Wrong/Dir/Path',\n  WildCard:'posdata.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\n  filename:'Foo',\n        sequence:'00',\n  rolloverpolicy:'eventcount:200'\n)\nformat using DSVFormatter (\n\n)\ninput from TypedCSVStream;\nend application noApp;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION OneAgentCQTester.CSV;\nUNDEPLOY APPLICATION OneAgentCQTester.CSV;\nDROP APPLICATION OneAgentCQTester.CSV cascade;\n\ncreate application CSV;\n\nCREATE FLOW AgentFlow;\n\ncreate source CSVSource using CSVReader\n(\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'StoreNames.csv',\n  columndelimiter:',',\n  positionByEOF:false\n)OUTPUT TO CsvStream;\n\nCREATE TYPE MyType (\nStore_Id String KEY,\nStore_Name String\n);\n\nCREATE STREAM TypedStream of MyType;\n\nCREATE CQ TypeConversionCQ\nINSERT INTO TypedStream\nSELECT data[0], data[1]\nfrom CsvStream;\n\nEND FLOW AgentFlow;\n\nCREATE FLOW ServerFlow;\n\nCREATE WACTIONSTORE StoreInfo CONTEXT OF MyType\nEVENT TYPES ( MyType )\n@PERSIST-TYPE@\n\nCREATE CQ StoreWaction\nINSERT INTO StoreInfo\nSELECT * FROM TypedStream\nLINK SOURCE EVENT;\n\nEND FLOW ServerFlow;\n\nend application CSV;\nDEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;\nSTART CSV;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop APPLICATION @AppName@;\nUndeploy APPLICATION @AppName@;\ndrop APPLICATION @AppName@ cascade;\nCREATE OR REPLACE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW @AgentFlow@1;\nCREATE OR REPLACE SOURCE @SourceName@1 USING Global.MSJet (\n  Tables: 'dbo.compsrc',\n  username: 'qatest',\n  DatabaseName: 'qatest',\n  FetchTransactionMetadata: true,\n  filterTransactionBoundaries: true,\n  compression: false,\n  ConnectionURL: '@ConnectionURL@1',\n  Mode: '@mode@',\n  CommittedTransactions: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',\n  SendBeforeImage: true,\n  password: 'w3b@ct10n' )\nOUTPUT TO @StreamName@;\nEND FLOW @AgentFlow@1;\n\nCREATE FLOW @AgentFlow@2;\nCREATE OR REPLACE SOURCE @SourceName@2 USING Global.MSJet (\n  Tables: 'dbo.compsrc',\n  username: 'qatest',\n  DatabaseName: 'qatest',\n  FetchTransactionMetadata: true,\n  filterTransactionBoundaries: true,\n  compression: false,\n  ConnectionURL: '@ConnectionURL@2',\n  CaptureType: '@captureType@',\n  CommittedTransactions: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',\n  SendBeforeImage: true,\n  password: 'w3b@ct10n' )\nOUTPUT TO @StreamName@;\nEND FLOW @AgentFlow@2;\n\nCREATE TARGET @SysTarget@ USING Global.SysOut (\n  name: 'MS_CDC_SYSOUT' )\nINPUT FROM @StreamName@;\n\nCREATE FLOW @ServerFlow@1;\nCREATE TARGET @TargetName@1 USING Global.DatabaseWriter (\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  ParallelThreads: '',\n  CheckPointTable: 'CHKPOINT',\n  CDDLAction: 'Process',\n  CommitPolicy: 'EventCount:1000,Interval:60',\n  ConnectionURL: '@ConnectionURL@',\n  StatementCacheSize: '50',\n  DatabaseProviderType: 'Default',\n  Username: 'qatest',\n  Tables: 'dbo.compsrc,dbo.comptar',\n  Password: 'w3b@ct10n',\n  PreserveSourceTransactionBoundary: 'false',\n  BatchPolicy: 'EventCount:1,Interval:60' )\nINPUT FROM @StreamName@;\nEND FLOW @ServerFlow@1;\n\nCREATE FLOW @ServerFlow@2;\nCREATE TARGET @TargetName@2 USING Global.DatabaseWriter (\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  ParallelThreads: '',\n  CheckPointTable: 'CHKPOINT',\n  CDDLAction: 'Process',\n  CommitPolicy: 'EventCount:1000,Interval:60',\n  ConnectionURL: '@ConnectionURL@',\n  StatementCacheSize: '50',\n  DatabaseProviderType: 'Default',\n  Username: 'qatest',\n  Tables: 'dbo.compsrc,dbo.comptar',\n  Password: 'w3b@ct10n',\n  PreserveSourceTransactionBoundary: 'false',\n  BatchPolicy: 'EventCount:1,Interval:60' )\nINPUT FROM @StreamName@;\nEND FLOW @ServerFlow@2;\n\nEND APPLICATION @AppName@;\nDEPLOY APPLICATION @AppName@ with @AgentFlow@1 in AGENTS, @AgentFlow@2 in AGENTS, @ServerFlow@1 on any in default, @ServerFlow@2 on any in default;\nSTART APPLICATION @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "UNDEPLOY APPLICATION admin.LongRunningAppOracle;\nDROP APPLICATION admin.LongRunningAppOracle cascade;\n\nCREATE APPLICATION LongRunningAppOracle;\n\n\n --COMMENT::   Modify type attributes as desired.\n --COMMENT::   Type must be created first before creating a source using ranReader\n\nCREATE TYPE RandomData(\n  myName String,\n  streetAddress String,\n  bankName String,\n  bankNumber int KEY,\n  bankAmount double\n);\n\nCREATE source ranDataSource USING ranReader(\n  OutputType:'LongRunningAppOracle.RandomData',\n  TimeInterval:5,\n  NoLimit:true,\n  SampleSize:10000,\n  DataKey:bankName,\n  NumberOfUniqueKeys:500\n) OUTPUT TO CSVDataStream;\n\n\nCREATE STREAM RandomDataStream OF RandomData;\n\nCREATE CQ ParseRandomData\nINSERT INTO RandomDataStream\nSELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4])\nFROM CSVDataStream;\n\n\nCREATE JUMPING WINDOW RandomData10Rows\nOVER RandomDataStream KEEP 10 ROWS\nPARTITION BY bankNumber;\n\n\nCREATE TYPE myData(\n  myName String,\n  myAddress String,\n  myBankName String,\n  myBankNumber int KEY,\n  myBankAmount double\n);\n\nCREATE STREAM myDataStream OF myData;\n\nCREATE CQ GetMyData\nINSERT INTO MyDataStream\nSELECT myName, streetAddress, bankName, bankNumber, bankAmount\nFROM RandomData10Rows WHERE bankNumber > 20000 AND bankNumber < 20300;\n\n\nCREATE WACTIONSTORE MyDataActivity CONTEXT OF MyData\nEVENT TYPES(myData )\nPERSIST EVERY 10 second USING (\nJDBC_DRIVER:'@WASTORE-DRIVER@',\nJDBC_URL:'@WASTORE-URL@',\nJDBC_USER:'@WASTORE-UNAME@',\nJDBC_PASSWORD:'@WASTORE-PASSWORD@',\npu_name:oracle,\nDDL_GENERATION:'drop-and-CREATE-tables'\n);\n\nCreate CQ TrackMyDataActivity\nINSERT INTO MyDataActivity\nSelect * from myDataStream\nLINK SOURCE EVENT;\n\n\nEND APPLICATION LongRunningAppOracle;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application APP_KAFKASOURCE_AGG;\nundeploy application APP_KAFKASOURCE_AGG;\nalter application APP_KAFKASOURCE_AGG;\n\nCREATE OR REPLACE CQ CQ_CALCULATE_HOURLY_TOTAL\nINSERT INTO STREAM_CQ_CALCULATE_HOURLY_TOTAL\nSELECT f.topic as topic, sum(f.rawdatacount) as TotalLast24hour, B.rawdatacount as TotalLast1hour FROM JUMP_WND_1EVT_1MIN h\n       join SLIDE_WND_HOURLYTOTALS_KAFKADATA_FILE f on 1=1\n       join (SELECT rawdatacount, topic,timerange from ET_HOURLYTOTALS_KAFKADATA_FILE,JUMP_WND_1EVT_30SEC where timerange = DHOURS(DNOW())-1) B on B.topic=f.topic\n       Group by f.topic;\n\nalter application APP_KAFKASOURCE_AGG recompile;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop OracleToKudu;\nundeploy application OracleToKudu;\ndrop application OracleToKudu cascade;\n\nCREATE APPLICATION OracleToKudu;\nCreate Source oracSource\n Using OracleReader\n(\n Username:'@LOGMINER-UNAME@',\n Password:'@LOGMINER-PASSWORD@',\n ConnectionURL:'@LOGMINER-URL@',\n Tables:'@SOURCE_TABLES@',\n OnlineCatalog:true,\n FetchSize:1\n) Output To DataStream;\n\nCREATE TARGET WriteintoKudu using KuduWriter (\nkuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',\npkupdatehandlingmode:'@MODE@',\ntables: '@TARGET_TABLES@',\nbatchpolicy: 'EventCount:1,Interval:0')\nINPUT FROM DataStream;\n\nEND APPLICATION OracleToKudu;\ndeploy application OracleToKudu in default;\nstart OracleToKudu;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nalter APPLICATION @APPNAME@;\nCREATE OR REPLACE SOURCE @APPNAME@src USING Global.FileReader \n(\n  adapterName:'FileReader',\n  rolloverstyle:'Default',\n  blocksize:64,\n  networkfilesystem:true,\n  wildcard:'client_xref*',\n  compressiontype:'gzip',\n  includesubdirectories:false,\n  directory:'@APPNAME@',\n  skipbom:false,\n  positionbyeof:false\n)\nPARSE USING Global.DSVParser \n(\n  trimwhitespace:false,\n  linenumber: '-1',\n  columndelimittill: '-1',\n  trimquote: true,\n  ignoreemptycolumn: false,\n  separator: ':',\n  parserName: 'DSVParser',\n  quoteset: '\\\"',\n  handler:'com.webaction.proc.DSVParser_1_0',\n  charset:'UTF-8',\n  ignoremultiplerecordbegin:'true',\n  ignorerowdelimiterinquote:false,\n  columndelimiter:'|',\n  blockascompleterecord:false,\n  rowdelimiter:'\\n',\n  nocolumndelimiter:false,\n  headerlineno:0,\n  header:'true'\n)                  \nOUTPUT TO @APPNAME@STREAM;\nALTER APPLICATION @APPNAME@ RECOMPILE;\ndeploy application @APPNAME@;\nstart @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION AgenCQTester.CSV;\nUNDEPLOY APPLICATION AgenCQTester.CSV;\nDROP APPLICATION AgenCQTester.CSV cascade;\n\ncreate application CSV;\n\nCREATE FLOW AgentFlow;\ncreate source CSVSource using CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'customerdetails-agent.csv',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE MyType (\n    PAN String,\n    FNAME String\n);\n\nCREATE STREAM TypedStream of MyType;\n\nCREATE CQ TypeConversionCQ\nINSERT INTO TypedStream\nSELECT data[0], data[1]\nfrom CsvStream;\n\nEND FLOW AgentFlow;\n\nCREATE FLOW ServerFlow;\nCREATE TARGET myout1 using LogWriter(name: CQSource, filename:'@FEATURE-DIR@/logs/logCQ.txt') input from TypedStream;\nEND FLOW ServerFlow;\n\nend application CSV;\nDEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;\nSTART CSV;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION SourcePosApp;\n\nCREATE SOURCE PosCsvDataSource USING FileReader ( \n  directory: '@TEST-DATA-PATH@', \n  wildcard: 'posdata.csv', \n  positionbyeof: false ) \nPARSE USING DSVParser ( \n  charset: 'UTF-8' )\nOUTPUT TO PosCsvStream;\n\nCREATE TARGET PosSourceDump using FileWriter(\n  directory: '@FEATURE-DIR@/logs',  \n  filename: 'SourcePosAppData',\n  rolloverpolicy: 'EventCount:6000000'\n   )\nFORMAT USING Global.DSVFormatter (\n  members: 'data',\n  charset: 'UTF-8' ) \n input from PosCsvStream;\n\nCREATE Source PosHourlyAveLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'hourlyData.txt',\n  positionByEOF:false )\nPARSE USING DSVParser ( \n    charset: 'UTF-8' )\nOUTPUT TO PosCacheSource1;\n\nCREATE TARGET PosCacheDump1 using FileWriter(\n  directory: '@FEATURE-DIR@/logs',  \n  filename: 'SourcePosCacheData1',\n  rolloverpolicy: 'EventCount:6000000' ) \nFORMAT USING DSVFormatter (\n    members: 'data', \n    charset: 'UTF-8') \n input from PosCacheSource1;\n \nCREATE Source PosNameLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'MerchantNames.csv',\n  positionByEOF:false )\nPARSE USING DSVParser ( \n    charset: 'UTF-8' )\n OUTPUT TO PosCacheSource2;\n\nCREATE TARGET PosCacheDump2 using FileWriter(\n  directory: '@FEATURE-DIR@/logs',  \n  filename: 'SourcePosCacheData2',\n  rolloverpolicy: 'EventCount:6000000' ) \nFORMAT USING DSVFormatter (\n    members: 'data', \n    charset: 'UTF-8')\n input from PosCacheSource2;\n \nCREATE Source PosZipLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'USAddresses.txt',\n  positionByEOF:false ) \nPARSE USING DSVParser ( \n    columndelimiter:'\\t',\n    charset: 'UTF-8' )\nOUTPUT To PosCacheSource3;\n\nCREATE TARGET PosCacheDump3 using FileWriter(\n  directory: '@FEATURE-DIR@/logs',  \n  filename: 'SourcePosCacheData3',\n  rolloverpolicy: 'EventCount:6000000') \nFORMAT USING DSVFormatter (\n    members: 'data', \n    charset: 'UTF-8')\nINPUT from PosCacheSource3;\n\nEND APPLICATION SourcePosApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop application PosAppToFW;\nUNDEPLOY APPLICATION PosAppToFW;\nDROP APPLICATION PosAppToFW CASCADE;\n\nCREATE APPLICATION PosAppToFW RECOVERY 1 MINUTE INTERVAL AUTORESUME MAXRETRIES 2 RETRYINTERVAL 60;\n\nCREATE source CsvPosDataSource USING FileReader (\n  WildCard: '',\n  directory: '',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE OR REPLACE CQ CsvPosAppCq \nINSERT INTO FromCsvPosAppCq \nSELECT TO_STRING(data[1]) as merchantId,\n       TO_DOUBLE(data[7]) as amount,\n       TO_STRING(data[9]) as zip\nFROM CsvStream;\n\n\nCREATE TARGET FileWriterTarget USING Global.FileWriter ( \n \n  flushpolicy: 'EventCount:1000,Interval:30s', \n  directory: '',\n  filename: '' ) \nFORMAT USING Global.DSVFormatter  ( \n  quotecharacter: '\\\"', \n  columndelimiter: ',', \n  usequotes: 'false', \n  rowdelimiter: '\\n', \n  standard: 'none', \n  header: 'false' ) \nINPUT FROM FromCsvPosAppCq;\n\nend application PosAppToFW;\ndeploy application PosAppToFW;\nstart application PosAppToFW;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop @APPNAME@_App;\nundeploy application @APPNAME@_App;\ndrop application @APPNAME@_App cascade;\n\nCREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;\nCREATE SOURCE @APPNAME@_Src USING Global.Ojet(\n  Username:'@SOURCE_USER@',\n  Password:'@SOURCE_PASSWORD@',\n  ConnectionURL:'@SOURCE_URL@',\n  Tables:'@SOURCE_TABLES@',\n  ConnectionRetryPolicy:'@AUTO_CONNECTION_RETRY@',\n  OJetConfig: '{ \"OJET\" : [ \"retriable_errors:ORA-26804\" ] }'\n)\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE TARGET @APPNAME@_SYS USING SysOut (\n  name: '@APPNAME@_SYS' )\nINPUT FROM @APPNAME@_Stream;\n\nEnd APPLICATION @APPNAME@_App;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING ADLSReader ()\nPARSE USING Global.DSVParser ()\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE OR REPLACE CQ @APPNAME@_CQ\nINSERT INTO @APPNAME@_CQOut\nSELECT\n    data[0] as BusinessName,\n    data[1] as MerchantId,\n    data[2] as PosDataCode,\n    data[3] as AccNumber,\n    data[4] as DateTime,\n    data[5] as ExpDate,\n    data[6] as CurrencyCode,\n    data[7] as AuthAmount,\n    data[8] as TerminalId,\n    data[9] as Zip,\n    data[10] as City\nFROM @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_KafkaTarget USING Global.KafkaWriter VERSION @KAFKA_VERSION@(\n  brokerAddress: '',\n  Topic: '',\n  Mode: 'Sync' )\nFORMAT USING DSVFormatter()\nINPUT FROM @APPNAME@_CQOut;\n\nCREATE OR REPLACE TARGET @APPNAME@_BlobTarget USING Global.AzureBlobWriter (\n  containername: '',\n  blobname: '',\n  accountaccesskey: '',\n  accountname: '',\n  foldername: '' )\nFORMAT USING JSONFormatter()\nINPUT FROM @APPNAME@_CQOut;\n\nCREATE OR REPLACE TARGET @APPNAME@_DWHTarget USING Global.BigQueryWriter (\n  Tables: '',\n  BatchPolicy: '',\n  projectId: '',\n  ServiceAccountKey: '' )\nINPUT FROM @APPNAME@_CQOut;\n\nCREATE OR REPLACE TARGET @APPNAME@_NoSqlTarget USING Global.MongoDBWriter (\n  AuthDB: 'admin',\n  ConnectionURL: '',\n  Username: '',\n  collections: '',\n  Password: '' )\nINPUT FROM @APPNAME@_CQOut;\n\nCREATE OR REPLACE TARGET @APPNAME@_FileTarget USING Global.FileWriter (\n  flushpolicy: '',\n  directory: '',\n  rolloverpolicy: '',\n  filename: '' )\nFORMAT USING DSVFormatter()\nINPUT FROM @APPNAME@_CQOut;\n\nCREATE OR REPLACE TARGET @APPNAME@_OLTPTarget USING Global.DatabaseWriter (\n  ConnectionURL: '',\n  Password: '',\n  Username: '',\n  Tables: '' )\nINPUT FROM @APPNAME@_CQOut;\n\nCREATE SOURCE @APPNAME@_KafkaSource USING KafkaReader VERSION @KAFKA_VERSION@ (\n  brokerAddress: '',\n  Topic: '',\n  startOffset: '0' )\nPARSE USING DSVParser ()\nOUTPUT TO @APPNAME@_Stream2;\n\nCREATE OR REPLACE TARGET @APPNAME@_KafkaFileTarget USING Global.FileWriter (\n  flushpolicy: '',\n  directory: '',\n  rolloverpolicy: '',\n  filename: '' )\nFORMAT USING JSONFormatter(\nmembers:'data')\nINPUT FROM @APPNAME@_Stream2;\n\nEND APPLICATION @APPNAME@;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP bq;\nUNDEPLOY APPLICATION bq;\nDROP APPLICATION bq CASCADE;\n\nCREATE APPLICATION bq RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE BQ_source USING OracleReader\n(\n\tUsername:'qatest',\n  Password:'qatest',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\n  OnlineCatalog:true,\n  FetchSize:'1',\n  Tables: 'QATEST.sourceTable'\n)\nOUTPUT TO SS;\n\n\nCREATE or replace TARGET BQ_target USING BigQueryWriter (\n\tserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\n\tprojectId: 'bigquerywritertest',\n    Tables:'QATEST.sourceTable,qatest.% keycolumns(RONUM)',\n    mode:'Appendonly',\n    datalocation: 'US',\n\tnullmarker: 'NULL',\n\tcolumnDelimiter: '|',\n\tBatchPolicy: 'eventCount:100,Interval:10'\t\n) INPUT FROM ss;\n\nCREATE OR REPLACE TARGET bq_SysOut USING Global.SysOut (name: 'wa') INPUT FROM ss;\n\nEND APPLICATION bq;\nDEPLOY APPLICATION bq;\nSTART APPLICATION bq;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create Target @TARGET_NAME@ using AzureBlobWriter(\n\taccountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:50'\n)\nformat using ParquetFormatter (\n\tschemaFileName:'@FILENAME@',\n\tFormatAs:'@FORMATAS@',\n\tmembers:'Table=@metadata(TableName),OpName=@metadata(OperationName)'\n)\ninput from @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION App1;\nUNDEPLOY APPLICATION App1;\nDROP APPLICATION App1 CASCADE;\nCREATE APPLICATION App1;\nCREATE FLOW AgentFlow;\nCREATE OR REPLACE SOURCE App1_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App1_SampleStream;\nEND FLOW AgentFlow;\nCREATE OR REPLACE TARGET App1_NullTarget using NullWriter()\nINPUT FROM App1_SampleStream;\nEND APPLICATION App1;\ndeploy application App1 with AgentFlow on any in AGENTS;\nSTART APPLICATION App1;\n\nSTOP APPLICATION App2;\nUNDEPLOY APPLICATION App2;\nDROP APPLICATION App2 CASCADE;\nCREATE APPLICATION App2;\nCREATE FLOW AgentFlow2;\nCREATE OR REPLACE SOURCE App2_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App2_SampleStream;\nEND FLOW AgentFlow2;\nCREATE OR REPLACE TARGET App2_NullTarget using NullWriter()\nINPUT FROM App2_SampleStream;\nEND APPLICATION App2;\ndeploy application App2 with AgentFlow2 on any in AGENTS;\nSTART APPLICATION App2;\n\nSTOP APPLICATION App3;\nUNDEPLOY APPLICATION App3;\nDROP APPLICATION App3 CASCADE;\nCREATE APPLICATION App3;\nCREATE OR REPLACE SOURCE App3_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App3_SampleStream;\nCREATE OR REPLACE TARGET App3_NullTarget using NullWriter()\nINPUT FROM App3_SampleStream;\nEND APPLICATION App3;\nDEPLOY APPLICATION App3;\nSTART APPLICATION App3;\n\nSTOP APPLICATION App4;\nUNDEPLOY APPLICATION App4;\nDROP APPLICATION App4 CASCADE;\nCREATE APPLICATION App4;\nCREATE OR REPLACE SOURCE App4_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App4_SampleStream;\nCREATE OR REPLACE TARGET App4_NullTarget using NullWriter()\nINPUT FROM App4_SampleStream;\nEND APPLICATION App4;\nDEPLOY APPLICATION App4;\nSTART APPLICATION App4;\n\nSTOP APPLICATION App5;\nUNDEPLOY APPLICATION App5;\nDROP APPLICATION App5 CASCADE;\nCREATE APPLICATION App5;\nCREATE OR REPLACE SOURCE App5_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App5_SampleStream;\nCREATE OR REPLACE TARGET App5_NullTarget using NullWriter()\nINPUT FROM App5_SampleStream;\nEND APPLICATION App5;\nDEPLOY APPLICATION App5;\nSTART APPLICATION App5;\n\nSTOP APPLICATION App6;\nUNDEPLOY APPLICATION App6;\nDROP APPLICATION App6 CASCADE;\nCREATE APPLICATION App6;\nCREATE OR REPLACE SOURCE App6_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App6_SampleStream;\nCREATE OR REPLACE TARGET App6_NullTarget using NullWriter()\nINPUT FROM App6_SampleStream;\nEND APPLICATION App6;\nDEPLOY APPLICATION App6;\nSTART APPLICATION App6;\n\nSTOP APPLICATION App7;\nUNDEPLOY APPLICATION App7;\nDROP APPLICATION App7 CASCADE;\nCREATE APPLICATION App7;\nCREATE OR REPLACE SOURCE App7_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App7_SampleStream;\nCREATE OR REPLACE TARGET App7_NullTarget using NullWriter()\nINPUT FROM App7_SampleStream;\nEND APPLICATION App7;\nDEPLOY APPLICATION App7;\nSTART APPLICATION App7;\n\nSTOP APPLICATION App8;\nUNDEPLOY APPLICATION App8;\nDROP APPLICATION App8 CASCADE;\nCREATE APPLICATION App8;\nCREATE OR REPLACE SOURCE App8_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App8_SampleStream;\nCREATE OR REPLACE TARGET App8_NullTarget using NullWriter()\nINPUT FROM App8_SampleStream;\nEND APPLICATION App8;\nDEPLOY APPLICATION App8;\nSTART APPLICATION App8;\n\n\nSTOP APPLICATION App9;\nUNDEPLOY APPLICATION App9;\nDROP APPLICATION App9 CASCADE;\nCREATE APPLICATION App9;\nCREATE OR REPLACE SOURCE App9_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO App9_SampleStream;\nCREATE OR REPLACE TARGET App9_NullTarget using NullWriter()\nINPUT FROM App9_SampleStream;\nEND APPLICATION App9;\nDEPLOY APPLICATION App9;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application FileWriterTest;\nundeploy application FileWriterTest;\ndrop application FileWriterTest cascade;\ncreate application FileWriterTest recovery 1 second interval;\ncreate source File_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:'@eof@',\n\tcharset:'@charset@'\n)\nparse using DSVParser (\n\theader:'@header@'\n)\nOUTPUT TO CsvStream;\n\ncreate Target FileTarget using FileWriter(\n    rolloverpolicy:'eventcount:100',\n    filename :'@FILE@',\n    directory : '@FOLDER@'\n)\nformat using DSVFormatter (\n)\ninput from CsvStream;\n\nend application FileWriterTest;\ndeploy application FileWriterTest on all in default;\nstart application FileWriterTest;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP bq;\nUNDEPLOY APPLICATION bq;\nDROP APPLICATION bq CASCADE;\n\nCREATE APPLICATION bq RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE S USING OracleReader\n(\n\tUsername: 'qatest',\n\tPassword: 'qatest',\n\tConnectionURL: 'dockerhost:1521:orcl',\n\tTables: 'QATEST.TABLE_TEST_1000001',\n\tFetchSize: '1'\n)\nOUTPUT TO SS;\n\n\nCREATE or replace TARGET T USING BigQueryWriter (\n\tserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\n\tprojectId: 'bigquerywritertest',\n    Tables:'QATEST.TABLE_TEST_1000001,qatest.% keycolumns(RONUM)',\n    mode:'Appendonly',\n    datalocation: 'US',\n\tnullmarker: 'defaultNULL',\n\tcolumnDelimiter: '|',\n\tBatchPolicy: 'eventCount:100,Interval:10'\t\n) INPUT FROM ss;\n\nEND APPLICATION bq;\nDEPLOY APPLICATION bq;\nSTART APPLICATION bq;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSV1Source using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'MerchantNames.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO Csv1Stream;\n\nCreate Type CSV1Type (\n  merchantId String,\n  merchantName String\n);\n\nCreate Stream TypedCSV1Stream of CSV1Type;\n\nCREATE CQ CsvToMerchantNames\nINSERT INTO TypedCSV1Stream\nSELECT data[0],\n       data[1]\nFROM Csv1Stream;\n\ncreate Target t using FileWriter(\n  filename:'Merchant',\n  sequence:'00',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00'\n)\nformat using DSVFormatter (\n\n)\ninput from TypedCSV1Stream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetTS_Merchant_actual.log') input from TypedCSV1Stream;\n\nend application DSV;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@1;\nundeploy application @APPNAME@1;\nstop application @APPNAME@2;\nundeploy application @APPNAME@2;\n\nDROP STREAM @APPNAME@_STREAM;\nDROP APPLICATION @APPNAME@1 CASCADE;\nDROP APPLICATION @APPNAME@2 CASCADE;\n\ndrop propertyset MysqlToMysqlPlatfm_App_KafkaPropset;\ndrop stream  MysqlToMysqlPlatfm_Stream CASCADE;\n\n\n--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\n--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;\n\t\t\t\t\t\nCREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';\ncreate application @APPNAME@1 recovery 5 SECOND INTERVAL;\n\ncreate or replace stream @STREAM@2 of Global.waevent;\n\nCREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using MySQLReader(\n  Username: '@READER-UNAME@',\n  Password: '@READER-PASSWORD@',\n  ConnectionURL: '@CDC-READER-URL@',\n  sendBeforeImage:'true',\n  FilterTransactionBoundaries: 'true',\n  Tables: '$table1'\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_AGENTFLOW1;\n\nCREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using MySQLReader( \n  Username: '@READER-UNAME@',\n  Password: '@READER-PASSWORD@',\n  ConnectionURL: '@CDC-READER-URL@',\n  sendBeforeImage:'true',\n  FilterTransactionBoundaries: 'true',\n  Tables: '@TABLENAME@2'\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_SERVERFLOW1;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n ) \nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;\nCREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x\n WHERE META(x,'TableName').toString() == 'WACTION.MYSQLTOMYSQLPLATFM_SOURCE4';\n\nCREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n ) \nINPUT FROM @STREAM@2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(\n  name: 'CDDL',\n  filename:'WAEventDump.log'\n)INPUT FROM @STREAM@;\n\nEND APPLICATION @APPNAME@1;\n\nCREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';\nCREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;\n\ncreate or replace stream @STREAM@3 of Global.waevent;\n\nCREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using MySQLReader( \n  Username: '@READER-UNAME@',\n  Password: '@READER-PASSWORD@',\n  ConnectionURL: '@CDC-READER-URL@',\n  sendBeforeImage:'true',\n  FilterTransactionBoundaries: 'true',\n  Tables: '$table2'\n)OUTPUT TO @STREAM@;\n\nEND FLOW @APPNAME@_AGENTFLOW2;\n\nCREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using MySQLReader(\n  Username: '@READER-UNAME@',\n  Password: '@READER-PASSWORD@',\n  ConnectionURL: '@CDC-READER-URL@',\n  sendBeforeImage:'true',\n  FilterTransactionBoundaries: 'true',\n  Tables: '@TABLENAME@4',\n  \n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_SERVERFLOW2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n ) \nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;\nCREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y\n WHERE META(y,'TableName').toString() == 'WACTION.MYSQLTOMYSQLPLATFM_SOURCE2';\n\nCREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n ) \nINPUT FROM @STREAM@3;\n\nCREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@;\n\nEND APPLICATION @APPNAME@2;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE TYPE @APPNAME@oracleType (\n companyName java.lang.String,\n merchantId java.lang.String);\n\nCREATE SOURCE @APPNAME@_Orcl USING OracleReader (\n  ConnectionURL: '',\n  Password: '',\n  Tables: '',\n  Username: ''\n  )\nOUTPUT TO @APPNAME@OracleOut;\n\nCREATE OR REPLACE STREAM @APPNAME@TypedStream OF @APPNAME@oracleType;\n\nCREATE OR REPLACE CQ @APPNAME@CQOut\nINSERT INTO @APPNAME@TypedStream\nSELECT\nTO_STRING(data[0]).replaceAll(\"COMPANY \", \"\") as companyName,\nTO_STRING(data[1]) as merchantID\nFROM @APPNAME@OracleOut o;\n\nCREATE OR REPLACE TARGET @APPNAME@_jmstrgt USING JMSWriter (\n  QueueName: '',\n  UserName: '',\n  Password: '',\n  Ctx: '',\n  Provider: ''\n  )\nFORMAT USING DSVFormatter()\nINPUT FROM @APPNAME@TypedStream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP BuiltInTester.test_todate_builtinfunc;\nUNDEPLOY APPLICATION BuiltInTester.test_todate_builtinfunc;\nDROP APPLICATION BuiltInTester.test_todate_builtinfunc CASCADE;\n\nCREATE APPLICATION test_todate_builtinfunc;\n\nCREATE OR REPLACE TYPE date_cq_Type (\n gapdate org.joda.time.DateTime,\n randomdate org.joda.time.DateTime);\n\nCREATE SOURCE date_file USING FileReader (\n  blocksize: 64,\n  positionbyeof: false,\n  rolloverstyle: 'Default',\n  includesubdirectories: false,\n  directory: '@TEST-DATA-PATH@',\n  skipbom: true,\n  wildcard: 'anomalyBound.csv'\n )\n PARSE USING DSVParser  (\n  charset: 'UTF-8',\n  handler: 'com.webaction.proc.DSVParser_1_0',\n  linenumber: '-1',\n  nocolumndelimiter: false,\n  trimwhitespace: false,\n  columndelimiter: ',',\n  columndelimittill: '-1',\n  ignoremultiplerecordbegin: 'true',\n  ignorerowdelimiterinquote: false,\n  parserName: 'DSVParser',\n  separator: ':',\n  blockascompleterecord: false,\n  ignoreemptycolumn: false,\n  rowdelimiter: '\\n',\n  header: true,\n  headerlineno: 0,\n  quoteset: '\\\"',\n  trimquote: true\n )\nOUTPUT TO date_file_stream;\n\nCREATE OR REPLACE WACTIONSTORE datef_ws CONTEXT OF date_cq_Type EVENT TYPES ( date_cq_Type ) USING ( storageProvider: 'elasticsearch' );\n\nCREATE OR REPLACE CQ date_cq INSERT INTO datef_ws SELECT TO_DATEF(\"1983-04-01\", 'yyyy-MM-dd') as gapdate, TO_DATEF(\"2021-02-18\", 'yyyy-MM-dd') as randomdate FROM date_file_stream d;\n\nEND APPLICATION test_todate_builtinfunc;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n--CREATE APPLICATION @APPNAME@;\ncreate application @APPNAME@ Recovery 5 second Interval;\n\n--create or replace flow @APPNAME@_agentflow;\n\nCREATE OR REPLACE SOURCE @APPNAME@_source USING MariaDBReader \n(\nUsername: '@READER-UNAME@',\nPassword: '@READER-PASSWORD@',\nConnectionURL: '@CDC-READER-URL@',\nTables: @WATABLES@,\nsendBeforeImage:'true',\nFilterTransactionBoundaries: 'true'\n) \nOUTPUT TO @APPNAME@_stream ;\n\n--end flow @APPNAME@_@APPNAME@_agentflow;\n\nCREATE OR REPLACE TARGET @APPNAME@_target USING CassandraCosmosDBWriter  (\n  AccountEndpoint: 'cassandracosmostest.cassandra.cosmos.azure.com',\n  AccountKey: 'pqDZvVgbdSCg7VzIzD77dAhPG2odGRZPLhAQA1qnZbAKoIDk6RuQX5r2phbRQFnR1l54qxOcvBXNdz8DeijYIg==',\n  KeySpace: 'test',\n  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',\n  Tables: 'QATEST.Source1,test.target1',\n  adapterName: 'CassandraCosmosDBWriter'\n )\n INPUT FROM @APPNAME@_stream;\n\n END APPLICATION @APPNAME@;\n\ndeploy application @APPNAME@;\n --deploy application @APPNAME@ with agentflow in agents;\n start application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop PatternMatchingTcp.CSV;\nundeploy application PatternMatchingTcp.CSV;\ndrop application PatternMatchingTCp.CSV cascade;\n\ncreate application CSV;\n\ncreate source TCPSource using TCPReader\n(\n  IpAddress:'127.0.0.1',\n  PortNo:'3549'\n)\n\nPARSE USING DSVParser\n(\nheader:'false',\nmetadata:'@TEST-DATA-PATH@/ctest-TCP.csv',endian : false\n\n)\nOUTPUT TO TcpStream;\n\ncreate Target t1 using SysOut(name:Typed1) input from TcpStream;\n\nCREATE CQ ParseUserData\nINSERT INTO UserDataStream\nSELECT  TO_INT(data[0]) as UserId,\n\t    TO_INT(data[1]) as temp1,\n        TO_DOUBLE(data[2]) as temp2,\n\t    TO_STRING(data[3]) as temp3\nFROM TcpStream;\n\n-- scenario 1.1 check pattern using timer within 10 seconds and wait\nCREATE CQ TypeConversionTCPCQ1\nINSERT INTO TypedStream1\nSELECT UserId as typeduserid,\n\tA.temp1 as typedtemp1,\n\tB.temp2 as typedtemp2,\n\tC.temp3 as typedtemp3\nfrom UserDataStream\nMATCH_PATTERN T A (W | B | C)\ndefine T = timer(interval 10 second),\nA = UserDataStream(temp1 >= 20), B= UserDataStream(temp2 < 30.40), C= UserDataStream(temp3 = 'Bret'), W = wait(T)\nPARTITION BY UserId;\n\n-- scenario 1.2 check pattern using timer within 20 seconds\nCREATE CQ TypeConversionTCPCQ2\nINSERT INTO TypedStream2\nSELECT UserId as typeduserid,\n\tA.temp1 as typedtemp1,\n\tB.temp2 as typedtemp2,\n\tC.temp3 as typedtemp3\nfrom UserDataStream\nMATCH_PATTERN T A C\ndefine T = timer(interval 20 second), B= UserDataStream(temp2 < 30.40), C= UserDataStream(temp3 = 'zalak'),\nA = UserDataStream(temp1 >= 20)\nPARTITION BY UserId;\n\n-- scenario 1.3 check pattern using timer within 5 seconds with between values\nCREATE CQ TypeConversionTCPCQ3\nINSERT INTO TypedStream3\nSELECT UserId as typeduserid,\n\t   A.temp1 as typedtemp1\nfrom UserDataStream\nMATCH_PATTERN T A\ndefine T = timer(interval 5 second),\nA = UserDataStream(temp1 between 10 and 40)\nPARTITION BY UserId;\n\n-- scenario 1.4 check pattern using timer which match no events\nCREATE CQ TypeConversionTCPCQ4\nINSERT INTO TypedStream4\nSELECT UserId as typeduserid\nfrom UserDataStream\nMATCH_PATTERN T W\ndefine T = timer(interval 50 second), W = wait(T)\nPARTITION BY UserId;\n\n-- scenario 1.5 check pattern using stop timer\nCREATE CQ TypeConversionTCPCQ5\nINSERT INTO TypedStream5\nSELECT UserId as typeduserid,\n       A.temp1 as typedtemp1,\n       B.temp2 as typedtemp2\nfrom UserDataStream\nMATCH_PATTERN T A C T2 B\ndefine\nT = timer(interval 50 second),\nA = UserDataStream(temp1 between 10 and 40),\nC = stoptimer(T),\nT2 = timer(interval 30 second),\nB = UserDataStream(temp2 >= 20)\nPARTITION BY UserId;\n\nCREATE WACTIONSTORE UserActivityInfoTcp1\nCONTEXT OF TypedStream1_Type\nEVENT TYPES ( TypedStream1_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfoTcp2\nCONTEXT OF TypedStream2_Type\nEVENT TYPES ( TypedStream2_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfoTcp3\nCONTEXT OF TypedStream3_Type\nEVENT TYPES ( TypedStream3_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfoTcp4\nCONTEXT OF TypedStream4_Type\nEVENT TYPES ( TypedStream4_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfoTcp5\nCONTEXT OF TypedStream5_Type\nEVENT TYPES ( TypedStream5_Type )\n@PERSIST-TYPE@\n\ncreate Target t2 using SysOut(name:Typed2) input from TypedStream1;\n\n--get data from UserDataStream and place into wactionStore UserWaction\nCREATE CQ UserWaction1\nINSERT INTO UserActivityInfoTcp1\nSELECT * FROM TypedStream1\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction2\nINSERT INTO UserActivityInfoTcp2\nSELECT * FROM TypedStream2\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction3\nINSERT INTO UserActivityInfoTcp3\nSELECT * FROM TypedStream3\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction4\nINSERT INTO UserActivityInfoTcp4\nSELECT * FROM TypedStream4\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction5\nINSERT INTO UserActivityInfoTcp5\nSELECT * FROM TypedStream5\nLINK SOURCE EVENT;\n\nend application CSV;\ndeploy application csv;\nstart csv;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE CQ @CQ_NAME@\nINSERT INTO @EMB_STREAM@\n@SELECT_QUERY@\nFROM @STREAM@ e;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 50 is a simple Kafka test\n-- Nicholas Keene WebAction, Inc.\n--\n-- S -> CQ -> KSu -> JWc10 -> WS\n--\n\nSTOP Recov50Tester.RecovTest50;\nUNDEPLOY APPLICATION Recov50Tester.RecovTest50;\nDROP APPLICATION Recov50Tester.RecovTest50 CASCADE;\nCREATE APPLICATION RecovTest50 RECOVERY 5 SECOND INTERVAL;\n\nCREATE or REPLACE TYPE KafkaType(\n  value java.lang.Long KEY  \n);\n\nCREATE SOURCE KafkaSource USING NumberSource ( \n  lowValue: '1',\n  highValue: '1003',\n  delayMillis: '10',\n  delayNanos: '0',\n  repeat: 'false'\n ) \nOUTPUT TO NumberSourceOut;\n\nCREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaStream OF KafkaType using KafkaProps;\n\nCREATE OR REPLACE CQ KafkaStreamPopulate \nINSERT INTO KafkaStream\nSELECT data[1]\nFROM NumberSourceOut;\n\nCREATE JUMPING WINDOW SizeTenWindow\nOVER KafkaStream KEEP 10 ROWS;\n\n\nCREATE WACTIONSTORE Wactions CONTEXT of KafkaType\n@PERSIST-TYPE@\n\nCREATE CQ WactionsPopulate\nINSERT INTO Wactions\nSELECT * FROM SizeTenWindow;\n\nEND APPLICATION RecovTest50;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCreate Source @SourceName@\n Using OracleReader\n(\n Username:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@SourceConnectURL@',\n Tables:'@SourceTable@',\n _h_useClassic: false,\n SupportPDB: false,\n Fetchsize:1000\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP cacheCase;\nUNDEPLOY APPLICATION cacheCase;\nDROP APPLICATION cacheCase cascade;\n\nCREATE APPLICATION cacheCase;\n\n\nCREATE TYPE Atm(\n  productID String KEY,\n  stateID String,\n  productWeight int,\n  quantity double,\n  size long,\n  currentDate DateTime);\n\nCREATE CACHE cAcHe1 USING CsvReader(\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'ISdata.csv',\n  header: false,\n  columndelimiter: ',',\n  trimquote: false\n  ) QUERY (keytomap:'PRODUCTID') OF Atm;\n\n\nCREATE WACTIONSTORE WS1 CONTEXT OF Atm\nEVENT TYPES\n(Atm );\n\n\nCREATE CQ cq1\nINSERT INTO ws1\nSELECT * FROM CACHE1;\n\nEND APPLICATION cacheCase;\nDEPLOY APPLICATION cacheCase;\nSTART cacheCase;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP QueryAggTester.ws_one;\nUNDEPLOY APPLICATION QueryAggTester.ws_one;\nDROP APPLICATION QueryAggTester.ws_one cascade;\n\nCREATE APPLICATION ws_one;\n\n\nCREATE source wsSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'banks.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:No,\n  trimquote:false\n) OUTPUT TO QaStream;\n\n\nCREATE TYPE wsData\n(\nbankID Integer KEY,\nbankName String\n);\n\n\nCREATE STREAM wsStream OF wsData;\n\n--Select data from QaStream and insert into wsStream\n\nCREATE CQ csvTowsData\nINSERT INTO wsStream\nSELECT TO_INT(data[0]),data[1] FROM QaStream;\n\n--create jumping window over data in wsStream\n\nCREATE WACTIONSTORE oneWS\nCONTEXT OF wsData\nEVENT TYPES(wsData )\n@PERSIST-TYPE@\n\n--get data from wsStream and place into wactionStore oneWS\nCREATE CQ wsToWaction\nINSERT INTO oneWS\nSELECT * FROM wsStream\nLINK SOURCE EVENT;\n\n\nEND APPLICATION ws_one;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCREATE SOURCE @SourceName@ USING MSSqlReader  ( \nTransactionSupport: false, \n  FetchTransactionMetadata: false, \n  autodisabletablecdc: true,\n  Compression: false, \n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', \n  Password_encrypted: 'false', \n  ConnectionURL: '@ConnectionURL@', \n  Fetchsize: 0, \n  ConnectionPoolSize: 10, \n  StartPosition: 'EOF', \n  DatabaseName: 'qatest', \n  Username: '@UN@', \n  cdcRoleName: 'STRIIM_READER', \n  Password: '@PWD@', \n  Tables: '@sourcetable@', \n  FilterTransactionBoundaries: true, \n  SendBeforeImage: true, \n  ExcludedTables: 'qatest.CHKPOINT',\n  AutoDisableTableCDC: false ) \nOUTPUT TO @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\n\nCREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'))) FROM @SRCINPUTSTREAM@ x; ;\n\nCREATE TARGET @targetName@ USING DatabaseWriter  ( \nConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', \n  ParallelThreads: '', \n  CheckPointTable: 'CHKPOINT', \n  Password_encrypted: 'false', \n  ConnectionURL: '@ConnectionURL@', \n  CommitPolicy: 'EventCount:1,Interval:60', \n  StatementCacheSize: '50', \n  DatabaseProviderType: 'Default', \n  Username: '@UN@', \n  Password: '@PWD@', \n  PreserveSourceTransactionBoundary: 'false', \n  BatchPolicy: 'EventCount:1,Interval:60', \n  Tables: '@TablemappingwithColmap@' ) \nINPUT FROM admin.sqlreader_cq_out;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "UNDEPLOY APPLICATION TcpDsvAgentTester.TcpDsvWithAgent;\nDROP APPLICATION TcpDsvAgentTester.TcpDsvWithAgent cascade;\n\ncreate Application TcpDsvWithAgent;\n\n\ncreate source TcpDsvAgent using TCPReader\n(\n  IpAddress:'127.0.0.1',\n  PortNo:'3549',\n  charset: 'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO TcpDsvAgentStream;\n\n\nCREATE TYPE UserDataType\n(\n  UserId String KEY,\n  UserName String,\n  CompanyName String,\n  UserZip int,\n  CompanyZip int\n);\n\nCREATE STREAM UserDataStream OF UserDataType PARTITION BY UserId;\n\nCREATE CQ ParseUserData\nINSERT INTO UserDataStream\nSELECT  data[0],\n        data[1],\n        data[2],\n        TO_INT(data[3]),\n        TO_INT(data[4])\nFROM TcpDsvAgentStream;\n\n\nCREATE WACTIONSTORE UserActivityInfo\nCONTEXT OF UserDataType\nEVENT TYPES ( UserDataType )\nPERSIST EVERY 6 second USING (\nJDBC_DRIVER:'@WASTORE-DRIVER@',\nJDBC_URL:'@WASTORE-URL@',\nJDBC_USER:'@WASTORE-UNAME@',\nJDBC_PASSWORD:'@WASTORE-PASSWORD@',\nDDL_GENERATION:'drop-and-create-tables',\nLOGGING_LEVEL:'SEVERE',\nCONTEXT_TABLE:'USERTABLE',\nEVENT_TABLE:'USEREVENTS'\n);\n\n\n--get data from UserDataStream and place into wactionStore UserWaction\nCREATE CQ UserWaction\nINSERT INTO UserActivityInfo\nSELECT * FROM UserDataStream\nLINK SOURCE EVENT;\n\n\n\nend Application TcpDsvWithAgent;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');\n\nCREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.XMLNodeEvent PERSIST USING @APPNAME@KafkaPropset;\n\nCREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (\n  ProviderName: '',\n  UserName: '',\n  transactionpolicy: '',\n  Provider: '',\n  Ctx: '',\n  EnableTransaction: '',\n  QueueName: '',\n  Topic:'',\n  Password: '' )\nPARSE USING XMLParserV2 (\n  rootnode:'/JMSXMLIN'\n  )\nOUTPUT TO @APPNAME@PersistStream@RANDOM@;\n\nCREATE CQ @APPNAME@CQ\nINSERT INTO @APPNAME@XMLStream\nSELECT\n  data.element(\"companyName\").attributeValue(\"merchantId\") as merchantId,\n  data.element(\"companyName\").getText() as companyName\nFROM @APPNAME@PersistStream@RANDOM@;\n\nCREATE OR REPLACE TARGET @APPNAME@_filetrgt USING Global.FileWriter (\n  filename: '',\n  flushpolicy: '',\n  directory: '',\n  rolloverpolicy: '' )\nFORMAT USING JSONFormatter ()\nINPUT FROM @APPNAME@XMLStream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Kafka Stream Recovery Test 1\n-- Bert Hashemi and Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> KS -> WS\n\nSTOP KStreamAvroRecov1Tester.KStreamAvroRecovTest1;\nUNDEPLOY APPLICATION KStreamAvroRecov1Tester.KStreamAvroRecovTest1;\nDROP APPLICATION KStreamAvroRecov1Tester.KStreamAvroRecovTest1 CASCADE;\nDROP USER KStreamAvroRecov1Tester;\nDROP NAMESPACE KStreamAvroRecov1Tester CASCADE;\nCREATE USER KStreamAvroRecov1Tester IDENTIFIED BY KStreamAvroRecov1Tester;\n-- GRANT 'Global:create,drop:deploymentgroup:*' TO USER KStreamAvroRecov1Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamAvroRecov1Tester;\nCONNECT KStreamAvroRecov1Tester KStreamAvroRecov1Tester;\n\nCREATE APPLICATION KStreamAvroRecovTest1 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250', dataformat:'avro');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE or REPLACE TYPE KafkaCsvStreamType(\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE Wactions CONTEXT OF KafkaCsvStreamType \nEVENT TYPES ( KafkaCsvStreamType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions\nINSERT INTO Wactions\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream;\n\nEND APPLICATION KStreamAvroRecovTest1;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop ADW;\nundeploy application ADW;\nDROP APPLICATION ADW CASCADE;\nCREATE APPLICATION ADW recovery 5 second interval;\nCreate Source Ojetsrc Using Ojet\n(\n Username:'@ORACLE-USERNAME',\n Password:'@ORACLE-PASSWORD',\n ConnectionURL: '@ORACLE-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'@FETCH-SIZE@'\n) \nOutput To str;\n\n\ncreate target AzureTarget using AzureSQLDWHWriter (\nConnectionURL: '@SQLDW-URL@',\nusername: '@SQLDW-USERNAME@',\npassword: '@SQLDW-PASSWORD@',\nAccountName: '@STORAGEACCOUNT@',\nAccountAccessKey: '@ACCESSKEY@',\nTables: '@TARGET-TABLES@',\nuploadpolicy:'@EVENT-COUNT@',\nMode:'Merge'\n) INPUT FROM str;\n\nEND APPLICATION ADW;\ndeploy application ADW;\nstart application ADW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\n\nCREATE OR REPLACE APPLICATION @AppName@;\n\nCREATE SOURCE @AppName@_MssqlSource USING MSSqlReader\n(\n   Username:'@userName@',\n  Tables: '@tableName@', \n  Password:'@password@',\n DatabaseName:'qatest',\n  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',\n AutoDisableTableCDC:false,\n connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5'\n)\nOUTPUT TO @AppName@_Stream;\n\nCREATE OR REPLACE CQ @AppName@_Cq\nINSERT INTO InputData\nselect userdata, metadata,\nCASE WHEN meta(h,'OperationName').toString() = 'DELETE' THEN data(h) ELSE before(h) END AS before , \nCASE WHEN meta(h,'OperationName').toString() = 'DELETE' THEN NULL ELSE data(h) END AS data,\nCASE WHEN meta(h,'OperationName').toString() = 'UPDATE' THEN data(h) ELSE before(h) END AS before ,\nCASE WHEN meta(h,'OperationName').toString() = 'INSERT' THEN data(h) ELSE before(h) END AS before\nfrom  @AppName@_Stream  h where meta(h,'TableName').tostring() = 'qatest.MetaDataTable';;\n\n\nCREATE OR REPLACE TARGET @AppName@_FileWriterTarget USING Global.FileWriter ( \n  flushpolicy: 'EventCount:10000,Interval:30s', \n  adapterName: 'FileWriter', \n  directory: '@logsDir@',\n  filename: '@FileName@', \n  rolloverpolicy: 'EventCount:10000,Interval:30s' ) \nFORMAT USING Global.JSONFormatter  ( \n  handler: 'com.webaction.proc.JSONFormatter', \n  jsonMemberDelimiter: '\\n', \n  EventsAsArrayOfJsonObjects: 'true', \n  formatterName: 'JSONFormatter', \n  jsonobjectdelimiter: '\\n' ) \nINPUT FROM InputData;\n\nEND APPLICATION @AppName@;\nDeploy application @AppName@;\nStart application @AppName@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCreate Source @SOURCE@\n        Using OracleReader\n(\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: 'qatest.test77',\n  FetchSize:1,\n  QueueSize:2000,\n  CommittedTransactions:true,\n  Compression:false\n)\nOutput To @STREAM@;\n\ncreate Target @TARGET@ using AzureblobWriter(\n    accountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:7'\n)\nformat using DSVFormatter (\n)\ninput from @STREAM@;\n\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\n-----------------------------------\nstop application SourceAgentApp1;\nundeploy application SourceAgentApp1;\n\nstop application SourceAgentApp2;\nundeploy application SourceAgentApp2;\n\nstop application TargetServerApp;\nundeploy application TargetServerApp;\n\ndrop application SourceAgentApp1 cascade;\ndrop application SourceAgentApp2 cascade;\ndrop application TargetServerApp cascade;\n\n\nCREATE APPLICATION SourceAgentApp1;\ncreate flow flow1;\ncreate source CSVSource1 using FileReader (\ndirectory: '@TEST-DATA-PATH@/tmp',\nWildCard:'mybanks*',\npositionByEOF: true,\ncharset:'UTF-8'\n) parse using DSVParser (header:'no')\nOUTPUT TO CsvStream;\nend flow flow1;\n\n--CREATE TARGET T USING Sysout(name:'sysout1') INPUT FROM CsvStream;\n\nEND APPLICATION SourceAgentApp1;\n\nDEPLOY APPLICATION SourceAgentApp1 with flow1 in AGENTS;\n\n\nCREATE APPLICATION SourceAgentApp2;\n\ncreate flow flow2;\ncreate source CSVSource2 using FileReader (\ndirectory: '@TEST-DATA-PATH@/tmp',\nWildCard:'mybanks*',\npositionByEOF: true,\ncharset:'UTF-8'\n) parse using DSVParser (header:'no')\nOUTPUT TO CsvStream;\nend flow flow2;\n\n--CREATE TARGET T USING Sysout(name:'sysout2') INPUT FROM CsvStream;\n\nEND APPLICATION SourceAgentApp2;\n\nDEPLOY APPLICATION SourceAgentApp2 with flow2 in AGENTS;\n\n\n-- One app consuming from stream from 2 sources running in agent\nCREATE APPLICATION TargetServerApp;\ncreate flow flow3;\n\nCREATE TARGET T5 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')\nFORMAT USING JSONFormatter ()\nINPUT FROM CsvStream;\nend flow flow3;\n\nEND APPLICATION TargetServerApp;\ndeploy application TargetServerApp with flow3 in default;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create or replace type @STREAM@details(\nID INT,\nname String,\ncompany String);\n\ncreate or replace stream @STREAM@_TYPED of @STREAM@details PARTITION BY name;\n\nCreate or replace CQ @STREAM@detailsCQ\ninsert into @STREAM@_TYPED\nselect \nto_int(data[0]),data[1],data[2]\nfrom @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@ USING FileWriter  ( \n  filename:'CompressedMerchant.gz',\n  directory:'/logs/',\n  rolloverpolicy:'EventCount:10000,Interval:30s'\n ) Format using DSVFormatter()\nINPUT FROM @STREAM@_TYPED;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application MySQLAllDataTypes;\nundeploy application MySQLAllDataTypes;\ndrop application MySQLAllDataTypes CASCADE;\ncreate application MySQLAllDataTypes;\n\nCREATE OR REPLACE SOURCE MySQLSource USING MySQLReader  (\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Database:'@DATABASE@',\n  Tables: '@SOURCE_TABLES@'\n ) Output To MySQLStream;\n \n--create Target t2 using SysOut(name:Foo2) input from MySQLStream; \nCREATE TARGET RedshiftTarget USING RedshiftWriter\n\t(\n\t  ConnectionURL: '@TARGET-URL@',\n\t  Username: '@TARGET-UNAME@',\n\t  Password: '@TARGET-PASSWORD@',\n\t  bucketname: '@BUCKETNAME@',\n\t  accesskeyId: '@ACCESS-KEY-ID@',\n\t  secretaccesskey: '@SECRET-ACCESS-KEY@',\n\t  Tables: '@TARGET-TABLES@',\n\t  uploadpolicy:'eventcount:1,interval:20s',\n\t  Mode:'incremental'\n\t) INPUT FROM MySQLStream;\n\t\nEND APPLICATION MySQLAllDataTypes;\ndeploy application MySQLAllDataTypes;\nSTART application MySQLAllDataTypes;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @AppName@;\nUndeploy application @AppName@;\nalter application @AppName@;\nCREATE FLOW CP_Agent_flow;\nCREATE OR REPLACE SOURCE CP_Oracle_source USING OracleReader ( \n  ConnectionURL: '@url@', \n  Tables: '@tables@', \n  Username: '@Username@', \n  Password: '@password@' ) \nOUTPUT TO CP_EndToEnd_SF_Adapter_Stream;\nEnd Flow CP_Agent_flow;\nalter application @AppName@ recompile;\nDEPLOY APPLICATION @AppName@ with CP_Agent_flow on any in AGENTS;\nstart application @AppName@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 3\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> JW -> CQ(aggregate) -> WS\n--\n\nSTOP KStreamRecov3Tester.KStreamRecovTest3;\nUNDEPLOY APPLICATION KStreamRecov3Tester.KStreamRecovTest3;\nDROP APPLICATION KStreamRecov3Tester.KStreamRecovTest3 CASCADE;\n\nDROP USER KStreamRecov3Tester;\nDROP NAMESPACE KStreamRecov3Tester CASCADE;\nCREATE USER KStreamRecov3Tester IDENTIFIED BY KStreamRecov3Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov3Tester;\nCONNECT KStreamRecov3Tester KStreamRecov3Tester;\n\nCREATE APPLICATION KStreamRecovTest3 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE TYPE WactionType (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount int,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionType;\n\nCREATE CQ CsvToData\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_INT(TO_DOUBLE(data[7])),\n    data[10]\nFROM KafkaCsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.merchantId),\n    FIRST(p.dateTime),\n    SUM(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nEND APPLICATION KStreamRecovTest3;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Undeploy application bq;\nalter application bq;\nCREATE OR REPLACE SOURCE S USING OracleReader  ( \n  Username: 'qatest',\n  Password: 'qatest',\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.TABLE_TEST_1000100',\n  DictionaryMode: offlineCatalog,\n  FetchSize: '1'\n ) \nOUTPUT TO SS;\nalter application bq recompile;\ndeploy application bq;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n--create flow AgentFlow;\nCREATE SOURCE @APPNAME@_S USING OracleReader\n(\n\tUsername: 'qatest',\n\tPassword: 'qatest',\n\tConnectionURL: 'dockerhost:1521:xe',\n\tTables: 'qatest.test01',\n\tFetchSize: '1',\n\tconnectionRetryPolicy:'timeOut=00,retryInterval=1,maxRetries=3'\n)\nOUTPUT TO @APPNAME@_SS;\n--end flow AgentFlow;\n--create flow serverFlow;\n--create target sys_out using sysout (name:sysoutput) input from ss;\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'Eventcount:1000,Interval:30',\nStandardSQL:true,\noptimizedMerge:true\t\t\n) INPUT FROM @APPNAME@_ss;\n--end flow serverFlow;\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\n--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop httpjsonapp;\nundeploy application httpjsonapp;\ndrop APPLICATION httpjsonapp cascade;\ncreate application httpjsonapp;\ncreate source HTTPSource using HTTPReader (\n        IpAddress:'127.0.0.1',\n        PortNo:'10001'\n) OUTPUT TO HttpDataStream;\n\ncreate Target HttpDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/httpjsondata') input from HttpDataStream;\nend application httpjsonapp;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION tpcc;\nUNDEPLOY APPLICATION tpcc;\nDROP APPLICATION tpcc CASCADE;\n\nCREATE APPLICATION tpcc RECOVERY 5 SECOND Interval;\n\nCREATE SOURCE Ojet_Source USING Ojet\n(\n    Username: '@Username@',\n    Password: '@Password@',\n    ConnectionURL: '@ConnectionURL@',\n    Tables: '@Tables@',\n)\n\nOUTPUT TO SourceStream ;\n\ncreate Target Ojet_FileWriter using FileWriter(\n  filename:'qatar.csv',\n  directory:'',\n  flushpolicy: 'EventCount:10000,Interval:60s',\n  rolloverpolicy: 'EventCount:10000,Interval:60s'\n)\nformat using DSVFormatter (\n\n)\ninput from SourceStream;\n\ncreate Target t2 using SysOut(name:Foo2) input from SourceStream;\n\nEND APPLICATION tpcc;\n\nDEPLOY APPLICATION tpcc;\nSTART APPLICATION tpcc;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application DualGen;\nundeploy application DualGen;\ndrop application DualGen cascade;\nCREATE APPLICATION DualGen;\n\nCREATE OR REPLACE TYPE DualEvent (\n     Dummy DateTime,\n    PhoneNo java.lang.String\n);\n\nCREATE OR REPLACE STREAM DualEvents OF DualEvent;\n\nCREATE OR REPLACE CQ GenDual \nINSERT INTO DualEvents\nSELECT\n    TO_DATEF('28-FEB-22',\"dd-MMM-yy\") as Dummy,\n    maskPhoneNumber('44 844 493 0787', \"(\\\\\\\\d{0,4}\\\\\\\\s)(\\\\\\\\d{0,4}\\\\\\\\s)([0-9 ]+)\", 1, 2) as PhoneNo\nFROM\n   heartbeat(interval 10 second) h;\n\n   CREATE OR REPLACE CQ GenDual2 \nINSERT INTO DualEvents\nSELECT\n   TO_DATEF('12/JAN/32',\"dd/MMM/yy\") as Dummy,\n   maskPhoneNumber('12 345 678 9101', \"(\\\\\\\\d{0,4}\\\\\\\\s)(\\\\\\\\d{0,4}\\\\\\\\s)([0-9 ]+)\", 1, 2) as PhoneNo\nFROM\n   heartbeat(interval 30 second) h;\n\nCREATE  SOURCE Orac_Src USING OracleReader  ( \n  Compression: false,\n  DictionaryMode: 'OnlineCatalog',\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  StartSCN: 'null',\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  DDLCaptureMode: 'All',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  ReaderType: 'LogMiner',\n  FilterTransactionBoundaries: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Username: 'miner',\n  Password: 'miner',\n  Tables: 'QATEST.HEARTBEAT',\n  OutboundServerProcessName: 'WebActionXStream',\n  Password_encrypted: false\n ) \nOUTPUT TO CDC_Events ;\n\nCREATE OR REPLACE CQ cdcdual \nINSERT INTO DualEvents\nSELECT\n   TO_DATE(data[0]) as Dummy,\n   data[1] as PhoneNo\nFROM\n   CDC_Events;\n\n\nCREATE OR REPLACE TARGET DualSys USING SysOut  ( \n  name: 'heartbeat_out'\n ) \nINPUT FROM DualEvents;\n\nCREATE TARGET DSVFormatterOut using FileWriter(\n filename:'HeartBeat_Reader.log',\n flushpolicy:'EventCount:16',\n rolloverpolicy:'EventCount:16,interval:117s')\nFORMAT USING DSVFormatter ()\nINPUT FROM DualEvents;\n\n\nEND APPLICATION DualGen;\ndeploy application DualGen;\nstart application DualGen;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@1;\nundeploy application @APPNAME@1;\nstop application @APPNAME@2;\nundeploy application @APPNAME@2;\n\nDROP STREAM @APPNAME@_STREAM;\nDROP APPLICATION @APPNAME@1 CASCADE;\nDROP APPLICATION @APPNAME@2 CASCADE;\n\ndrop propertyset OrcToSpnPlatfm_App_KafkaPropset;\ndrop stream  OrcToSpnPlatfm_Stream CASCADE;\n\n\n--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\n--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;\n\nCREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';\ncreate application @APPNAME@1 recovery 5 SECOND INTERVAL;\n\ncreate or replace stream @STREAM@2 of Global.waevent;\n\nCREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNEC4TION_URL@',\n  Tables: '$table1',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_AGENTFLOW1;\n\nCREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@TABLENAME@2',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_SERVERFLOW1;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@1 USING SpannerWriter (\n  Tables: 'QATEST.%,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tinstanceId: 'qatest'\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;\nCREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x\n WHERE META(x,'TableName').toString() == 'QATEST.ORCTOSPNPLATFM_SOURCE4';\n\nCREATE OR REPLACE TARGET @TARGET_NAME@2 USING SpannerWriter (\n  Tables: 'QATEST.%,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tinstanceId: 'qatest'\n )\nINPUT FROM @STREAM@2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(\n  name: 'CDDL',\n  filename:'WAEventDump.log'\n)INPUT FROM @STREAM@;\n\nCREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;\n\nEND APPLICATION @APPNAME@1;\n\n\n\nCREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';\nCREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;\n\ncreate or replace stream @STREAM@3 of Global.waevent;\n\nCREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '$table2',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\n\nEND FLOW @APPNAME@_AGENTFLOW2;\n\nCREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@TABLENAME@4',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_SERVERFLOW2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@3 USING SpannerWriter  (\n  Tables: 'QATEST.%,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tinstanceId: 'qatest'\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;\nCREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y\n WHERE META(y,'TableName').toString() == 'QATEST.ORCTOSPNPLATFM_SOURCE2';\n\nCREATE OR REPLACE TARGET @TARGET_NAME@4 USING SpannerWriter  (\n  Tables: 'QATEST.%,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tinstanceId: 'qatest'\n )\nINPUT FROM @STREAM@3;\n\nCREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;\n\nEND APPLICATION @APPNAME@2;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application ConsoleApplication;\ncreate type someType(zip Int);\ndrop application ConsoleApplication;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop application @APPNAME@Apps2;\nundeploy application @APPNAME@Apps2;\ndrop application @APPNAME@Apps2 cascade;\n\n\nstop application @APPNAME@Apps3;\nundeploy application @APPNAME@Apps3;\ndrop application @APPNAME@Apps3 cascade;\n\n\n\nstop application @APPNAME@Apps4;\nundeploy application @APPNAME@Apps4;\ndrop application @APPNAME@Apps4 cascade;\n\n\n\nstop application @APPNAME@Apps1;\nundeploy application @APPNAME@Apps1;\ndrop application @APPNAME@Apps1 cascade;\n\nCREATE OR REPLACE PROPERTYSET @APPNAME@Apps_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9099', kafkaversion:'0.11');\n\nCREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream1 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;\nCREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream2 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;\nCREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream3 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;\nCREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream4 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;\n\n\n--**********************Application 1*******************\n-- with 2 agent flow \n-- <agentflow1>source1->PS1<agentflow1>\n-- <agentflow2>Source2->Inmemory1->cq1->PS2<agentflow2>\n\nCREATE APPLICATION @APPNAME@Apps1 RECOVERY 5 SECOND INTERVAL;\ncreate flow @APPNAME@agentflow;\nCREATE OR REPLACE SOURCE @APPNAME@AppsDBSource1 USING OracleReader  ( \n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECT_URL@',\n  Tables: '@SOURCE_TABLE@1',\n  adapterName: 'OracleReader',\n  Password: '@PASSWORD@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  StartSCN: 'null',\n  ReaderType: 'LogMiner',\n  Username: '@USERNAME@',\n  OutboundServerProcessName: 'WebActionXStream'\n ) \nOUTPUT TO @APPNAME@Apps_PS_Stream1;\nend flow @APPNAME@agentflow;\n\ncreate flow @APPNAME@agentflow2;\nCREATE OR REPLACE SOURCE @APPNAME@AppsDBSource2 USING OracleReader  ( \n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECT_URL@',\n  Tables: '@SOURCE_TABLE@2',\n  adapterName: 'OracleReader',\n  Password: '@PASSWORD@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  StartSCN: 'null',\n  ReaderType: 'LogMiner',\n  Username: '@USERNAME@',\n  OutboundServerProcessName: 'WebActionXStream'\n ) \nOUTPUT TO @APPNAME@Apps_PS_InmemoryStream1;\n\nCREATE CQ @APPNAME@cq_Inmemory1\nINSERT INTO @APPNAME@Apps_PS_Stream2\nSELECT *\nFROM @APPNAME@Apps_PS_InmemoryStream1;\nend flow @APPNAME@agentflow2;\n\nend application @APPNAME@Apps1;\n\n-- ********************Application 2***********************\n-- with 2 agent flow \n-- <agentflow3>source3->inmemory2<agentflow3>inmemory->cq2->ps3\n-- <agentflow4>Source2->Inmemory3<agentflow4><serverflow1>inmemory->cq3->ps4<serverflow1>\n\nCREATE APPLICATION @APPNAME@Apps2 RECOVERY 5 SECOND INTERVAL;\n\ncreate flow @APPNAME@agentflow3;\nCREATE OR REPLACE SOURCE @APPNAME@AppsDBSource3 USING OracleReader  ( \n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECT_URL@',\n  Tables: '@SOURCE_TABLE@3',\n  adapterName: 'OracleReader',\n  Password: '@PASSWORD@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  StartSCN: 'null',\n  ReaderType: 'LogMiner',\n  Username: '@USERNAME@',\n  OutboundServerProcessName: 'WebActionXStream'\n ) \nOUTPUT TO @APPNAME@Apps_PS_InmemoryStream2;\n\nCREATE CQ @APPNAME@cq_Inmemory2\nINSERT INTO @APPNAME@Apps_PS_Stream3\nSELECT *\nFROM @APPNAME@Apps_PS_InmemoryStream2;\n\nend flow @APPNAME@agentflow3;\n\ncreate flow @APPNAME@agentflow4;\nCREATE OR REPLACE SOURCE @APPNAME@AppsDBSource4 USING OracleReader  ( \n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECT_URL@',\n  Tables: '@SOURCE_TABLE@4',\n  adapterName: 'OracleReader',\n  Password: '@PASSWORD@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  StartSCN: 'null',\n  ReaderType: 'LogMiner',\n  Username: '@USERNAME@',\n  OutboundServerProcessName: 'WebActionXStream'\n ) \nOUTPUT TO @APPNAME@Apps_PS_InmemoryStream3;\n\nend flow @APPNAME@agentflow4;\n\ncreate  flow @APPNAME@serverflow;\nCREATE CQ @APPNAME@cq_Inmemory3\nINSERT INTO @APPNAME@Apps_PS_Stream4\nSELECT *\nFROM @APPNAME@Apps_PS_InmemoryStream3;\nend flow @APPNAME@serverflow;\nEND APPLICATION @APPNAME@Apps2;\n\n-- ********************Application 3**************************\n--WITH 3 TARGET FLOW\n-- 1. <targetflow1> ps1->target1 <targetflow1>\n-- 2. <targetflow2>ps2->target2 <targetflow2>\n-- 3. <targetflow3> ps1,ps2->cq4 -> target3 <targetflow3>\n\nCREATE APPLICATION @APPNAME@Apps3 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW @APPNAME@TARGETFLOW1;\nCREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget1 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TGT_USERNAME@',\n  BatchPolicy: 'EventCount:100,Interval:0',\n  CommitPolicy: 'EventCount:100,Interval:0',\n  ConnectionURL: '@CONNECT_URL@',\n  Tables: '@SOURCE_TABLE@1,@TARGET_TABLE@1',\n  Password: '@TGT_PASSWORD@',\n  Password_encrypted: false\n ) \nINPUT FROM @APPNAME@Apps_PS_Stream1;\nEND FLOW @APPNAME@TARGETFLOW1;\n\nCREATE FLOW @APPNAME@TARGETFLOW2;\nCREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget2 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TGT_USERNAME@',\n  BatchPolicy: 'EventCount:100,Interval:0',\n  CommitPolicy: 'EventCount:100,Interval:0',\n  ConnectionURL: '@CONNECT_URL@',\n  Tables: '@SOURCE_TABLE@2,@TARGET_TABLE@2',\n  Password: '@TGT_PASSWORD@',\n  Password_encrypted: false\n ) \nINPUT FROM @APPNAME@Apps_PS_Stream2;\nEND FLOW @APPNAME@TARGETFLOW2;\n\nCREATE OR REPLACE STREAM @APPNAME@Apps_PS_InmemoryStream4 OF Global.waevent;\n\nCREATE CQ @APPNAME@cq_ps1\nINSERT INTO @APPNAME@Apps_PS_InmemoryStream4\nSELECT *\nFROM @APPNAME@Apps_PS_Stream1;\n\nCREATE CQ @APPNAME@cq_ps2\nINSERT INTO @APPNAME@Apps_PS_InmemoryStream4\nSELECT *\nFROM @APPNAME@Apps_PS_Stream2;\n\nCREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget2 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TGT_USERNAME@',\n  BatchPolicy: 'EventCount:100,Interval:0',\n  CommitPolicy: 'EventCount:100,Interval:0',\n  ConnectionURL: '@CONNECT_URL@',\n  Tables: '@SOURCE_TABLE@1,@TARGET_TABLE@5;@SOURCE_TABLE@2,@TARGET_TABLE@5',\n  Password: '@TGT_PASSWORD@',\n  Password_encrypted: false\n ) \nINPUT FROM @APPNAME@Apps_PS_InmemoryStream4;\n\nEND APPLICATION @APPNAME@Apps3;\n\n\n--********************Application 4************************\n--WITH 3 TARGET FLOW\n-- 1. <targetflow1> ps3->target4 <targetflow1>\n-- 2. <targetflow2>ps4->target5 <targetflow2>\n-- 3. <targetflow3> ps3,ps4->cq5 -> target6 <targetflow3>\n\nCREATE APPLICATION @APPNAME@Apps4 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW @APPNAME@TARGETFLOW3;\nCREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget4 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TGT_USERNAME@',\n  BatchPolicy: 'EventCount:100,Interval:0',\n  CommitPolicy: 'EventCount:100,Interval:0',\n  ConnectionURL: '@CONNECT_URL@',\n  Tables: '@SOURCE_TABLE@3,@TARGET_TABLE@3',\n  Password: '@TGT_PASSWORD@',\n  Password_encrypted: false\n ) \nINPUT FROM @APPNAME@Apps_PS_Stream3;\nEND FLOW @APPNAME@TARGETFLOW3;\n\nCREATE FLOW @APPNAME@TARGETFLOW4;\nCREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget5 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TGT_USERNAME@',\n  BatchPolicy: 'EventCount:100,Interval:0',\n  CommitPolicy: 'EventCount:100,Interval:0',\n  ConnectionURL: '@CONNECT_URL@',\n  Tables: '@SOURCE_TABLE@4,@TARGET_TABLE@4',\n  Password: '@TGT_PASSWORD@',\n  Password_encrypted: false\n ) \nINPUT FROM @APPNAME@Apps_PS_Stream4;\nEND FLOW @APPNAME@TARGETFLOW4;\n\nCREATE OR REPLACE STREAM @APPNAME@Apps_PS_InmemoryStream5 OF Global.waevent;\n\nCREATE CQ @APPNAME@cq_ps3\nINSERT INTO @APPNAME@Apps_PS_InmemoryStream5\nSELECT *\nFROM @APPNAME@Apps_PS_Stream3;\n\nCREATE CQ @APPNAME@cq_ps4\nINSERT INTO @APPNAME@Apps_PS_InmemoryStream5\nSELECT *\nFROM @APPNAME@Apps_PS_Stream4;\n\n\nCREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget6 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TGT_USERNAME@',\n  BatchPolicy: 'EventCount:100,Interval:0',\n  CommitPolicy: 'EventCount:100,Interval:0',\n  ConnectionURL: '@CONNECT_URL@',\n  Tables: '@SOURCE_TABLE@3,@TARGET_TABLE@6;@SOURCE_TABLE@4,@TARGET_TABLE@6',\n  Password: '@TGT_PASSWORD@',\n  Password_encrypted: false\n ) \nINPUT FROM @APPNAME@Apps_PS_InmemoryStream5;\n\nEND APPLICATION @APPNAME@Apps4;\n\ndeploy application @APPNAME@Apps4 on ANY in Appsdg with @APPNAME@Targetflow3 in Targetdg1,@APPNAME@Targetflow4 in Targetdg;\nstart application @APPNAME@Apps4;\n\ndeploy application @APPNAME@Apps3 on ANY in Appsdg with @APPNAME@Targetflow2 in Targetdg1,@APPNAME@Targetflow1 in Targetdg;\nstart application @APPNAME@Apps3;\n\ndeploy application @APPNAME@Apps2 in Appsdg with @APPNAME@agentflow3 in agents, @APPNAME@agentflow4 in agents2, @APPNAME@serverflow in default;\nstart application @APPNAME@Apps2;\n\ndeploy application @APPNAME@Apps1 in Appsdg with @APPNAME@agentflow in agents, @APPNAME@agentflow2 in agents2;\nstart application @APPNAME@Apps1;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application OneAgentWithMultiTester.AgentWithMultiReader;\nundeploy application OneAgentWithMultiTester.AgentWithMultiReader;\ndrop application OneAgentWithMultiTester.AgentWithMultiReader cascade;\n\ncreate application AgentWithMultiReader;\n\n\nCREATE FLOW AgentFlow;\n\ncreate source XMLSource using FileReader (\n  Directory:'@TEST-DATA-PATH@',\n  WildCard:'books.xml',\n  positionByEOF:false\n)\nparse using XMLParser (\n  RootNode:'/catalog/book'\n)\nOUTPUT TO XmlStream;\n\ncreate source DSVCSVSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'customerdetails-agent.csv',\n  charset: 'UTF-8',\n  positionByEOF:false\n)\nparse using DSVParser (\n  header:'no'\n)\nOUTPUT TO DSVCsvStream;\n\n-- Read from File\n\ncreate source CSVSource using CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'StoreNames.csv',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO CSVStream;\n\nEND FLOW AgentFlow;\n\n--  Rest of the Stream and CQs are executed in Server flow\n\nCREATE FLOW ServerFlow;\n\nCREATE TARGET myout using LogWriter(name: XMLSource, filename:'@FEATURE-DIR@/logs/logXML.txt') input from XmlStream;\nCREATE TARGET myout1 using LogWriter(name: DSVSource, filename:'@FEATURE-DIR@/logs/logDSV.txt', charset:'UTF-8') input from DSVCsvStream;\nCREATE TARGET myout2 using LogWriter(name: CSVSource, filename:'@FEATURE-DIR@/logs/logCSV.txt') input from CSVStream;\n\n\nEND FLOW ServerFlow;\n\nend application AgentWithMultiReader;\nDEPLOY APPLICATION AgentWithMultiReader with AgentFlow in AGENTS, ServerFlow on any in default;\n\nstart AgentWithMultiReader;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSVSource using FileReader (\n\tdirectory:'@TEST-DATA-PATH@',\n        WildCard:'banks.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'no'\n)\nOUTPUT TO CsvStream;\n\nCREATE TARGET DBDump USING FileWriter(\nfilename:'@FEATURE-DIR@/logs/JsonRaw_RT.log', rolloverpolicy:'EventCount:10000,Interval:30s' )Format using JSONFormatter()\nINPUT FROM CsvStream;\nend application DSV;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n\nCREATE SOURCE @APPNAME@_S USING MSSqlReader\n(\n  Compression: false,\n  cdcRoleName: 'STRIIM_READER',\n  DatabaseName: 'QATEST',\n  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',\n  ConnectionPoolSize: 1,\n  FetchTransactionMetadata: false,\n  StartPosition: 'EOF',\n  Username: 'qatest',\n  SendBeforeImage: true,\n  AutoDisableTableCDC: true,\n  ConnectionURL: 'localhost:1433',\n  Tables: 'qatest.test01',\n  adapterName: 'MSSqlReader',\n  Password: 'w3b@ct10n'\n)\nOUTPUT TO @APPNAME@_SS;\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(id,col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'NULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:0',\nStandardSQL:true\t,\noptimizedMerge:true\t\n) INPUT FROM @APPNAME@_ss;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\n--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @appName@;\nundeploy application @appName@;\ndrop application @appName@ cascade;\n\nCREATE APPLICATION @appName@ USE EXCEPTIONSTORE TTL : '7d' AUTORESUME MAXRETRIES 1000 RETRYINTERVAL 60;\n\nCREATE OR REPLACE SOURCE @appName@_Source USING Global.MysqlReader (\n  ConnectionURL: '@ConnectionURL@',\n  Tables: '@dbTable@',\n  Password: '@Password@',\n  Username: '@Username@',\n  adapterName: 'MysqlReader')\nOUTPUT TO @appName@st1;\n\nCREATE CQ @appName@CQ\nINSERT INTO @appName@st2\nSELECT data[0] as Id,data[1] as Description FROM @appName@st1 s;;\n\ncreate target @appName@_tgt using NullWriter() input from @appName@st2;\n\nEND APPLICATION @appName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING CassandraWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:100,Interval:0',\n  CommitPolicy: 'EventCount:100,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  --Password_encrypted: false\n ) \nINPUT FROM @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop DataGenSampleApp;\nundeploy application DataGenSampleApp;\ndrop application DataGenSampleApp cascade;\n\n\nCREATE APPLICATION DataGenSampleApp;\n\nCREATE Source dataGenSrc Using PostgreSQLReader  ( \n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: true,\n  Username: 'waction',\n  Password_encrypted: true,\n  ConnectionURL: 'jdbc:postgresql://127.0.0.1:5432/webaction?stringtype=unspecified',\n  Tables:'@tableNames@',\n  adapterName: 'PostgreSQLReader',\n  Password: 'xFzvJYZf1b8=',\n  Password_encrypted: 'true'\n ) \nOutput To LCRStream;\n\ncreate Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;\n\nEND APPLICATION DataGenSampleApp;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION HW ;\nundeploy application HW ;\ndrop application HW cascade;\n\nCREATE APPLICATION HW Recovery 5 second interval;\n\nCREATE  SOURCE S USING OracleReader  ( \n  Username: 'miner',\n  Password: '@miner',\n  ConnectionURL: '@conn-url@',\n  Tables: '@src@',\n  FetchSize: 1) \nOUTPUT TO hivestream;\n\nCreate Target T using HiveWriter (\n  ConnectionURL:'@hive-url@',\n  Username:'@uname@', \n            Password:'@pwd@',\n            hadoopurl:'hdfs://dockerhost:9000/',\n\t        Mode:'incremental',\n\t        mergepolicy: 'eventcount:5,interval:1s',\n            Tables:'@tgt-table@',\n            hadoopConfigurationPath:'/Users/saranyad/Documents/hello/'\n )\nINPUT FROM hivestream;\n\n\nEND APPLICATION HW;\ndeploy application HW on all in default;\n\nStart application HW;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "import com.webaction.proc.events.StringArrayEvent;\nimport com.webaction.proc.events.WAEvent;\n\nSTOP FunctionTester.BuiltInFunctionApp;\nUNDEPLOY APPLICATION FunctionTester.BuiltInFunctionApp;\nDROP APPLICATION FunctionTester.BuiltInFunctionApp cascade;\n\nCREATE APPLICATION BuiltInFunctionApp;\n\n\nCREATE source rawSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'csvdata.txt',\n  positionByEOF:false\n)\nPARSE USING DSVParser\n(\n   header:False,\n   columndelimiter:','\n) OUTPUT TO rawStream;\n\n\nCREATE TYPE rawDataType(\n  level String,\n  floatnumber float,\n  doublenumber double,\n  intvalue  int,\n  ourEvent  WAEvent\n);\n\n\nCREATE STREAM testStream OF rawDataType ;\n\n\n-- Create a WAEvent out of the first to items, using makeWAEvent makeWAEvent(data[0], data[1])\nCREATE CQ testCQ\n  INSERT INTO testStream\n  SELECT data[0], ROUND_FLOAT(data[1],4), ROUND_DOUBLE(data[2],0), TO_INT(data[3]), makeWAEvent(data[0], data[1])\nFROM rawStream;\n\nCREATE OR REPLACE Target BuiltFuncTarget using LogWriter(name:BuiltFunc,filename:'@FEATURE-DIR@/logs/BuiltInFuncResults') input from testStream;\n\n\nCREATE TYPE waEventDataType\n(\n  level String,\n  floatnumber float\n);\n\nCREATE STREAM waEventStream OF waEventDataType;\n\n-- Parse the WAEvent created by the CQ testCQ and store the values\nCREATE CQ otherCQ\n  INSERT INTO waEventStream\n  SELECT TO_STRING(VALUE(ourEvent,0)), ROUND_FLOAT(VALUE(ourEvent,1),4)\nFROM testStream;\n\n\nCREATE OR REPLACE Target BuiltFuncWAEventTarget using LogWriter(name:BuiltFunc,filename:'@FEATURE-DIR@/logs/BuiltInFuncResultsWAEvent') input from waEventStream;\n\n\n\nCREATE TYPE stringArrayEventDataType\n(\n  level String,\n  floatnumber float,\n  doublenumber double,\n  intvalue  int,\n  stringArrayEvent StringArrayEvent\n);\n\nCREATE STREAM stringArrayEventStream OF stringArrayEventDataType;\n\n-- Create a StringArrayEvent from the raw stream\n-- We can't use the target below to verify, as it's generating a timestamp field, based on system generated time\nCREATE CQ stringArrayEventCQ\n  INSERT INTO stringArrayEventStream\n  SELECT data[0], ROUND_FLOAT(data[1],4), ROUND_DOUBLE(data[2],0), TO_INT(data[3]), makeStringArrayEvent(data[0], data[2], data[3])\nFROM rawStream;\n\n\nCREATE OR REPLACE Target BuiltFuncSimpleEventTarget using LogWriter(name:BuiltFunc,filename:'@FEATURE-DIR@/logs/BuiltInFuncResultsStringArrayEvent') input from stringArrayEventStream;\n\n\nEND APPLICATION BuiltInFunctionApp;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "undeploy application GCSWriterTest;\nalter application GCSWriterTest;\n\nCREATE OR REPLACE SOURCE OracleSource USING OracleReader  (\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: '@SOURCE_TABLES@',\n  FetchSize: 1\n ) Output To OracleStream;\n\n create or replace Target OracleGCSTarget using GCSWriter(\n    bucketname:'@bucketname@',\n    objectname:'@objectname@',\n    projectId:'@project-id@',\n    uploadPolicy:'@UPLOAD-SIZE@',\n    ServiceAccountKey:'@file-path@'\n)\nformat using DSVFormatter (\nmembers:'data'\n)\ninput from OracleStream;\nend application GCSWriterTest;\nalter application GCSWriterTest recompile;\ndeploy application GCSWriterTest;\nstart application GCSWriterTest;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop @APPNAME@_App;\nundeploy application @APPNAME@_App;\ndrop application @APPNAME@_App cascade;\n\nCREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;\nCREATE SOURCE @APPNAME@_Source USING Global.OracleReader\n\n(\n  FetchSize:'1',\n  Username:'@SOURCE_USER@',\n  Password:'@SOURCE_PASSWORD@',\n  ConnectionURL:'@SOURCE_URL@',\n  Tables:'@SOURCE_TABLES@'\n)\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE TARGET @APPNAME@_SYS USING SysOut (\n  name: '@APPNAME@_SYS' )\nINPUT FROM @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_CDB_Target USING DatabaseWriter\n\n(\n  Username:'@TARGET_CDB_USER@',\n  ConnectionURL:'@TARGET_CDB_URL@',\n  Tables:'@TARGET_CDB_TABLES@',\n  Password:'@TARGET_CDB_PASSWORD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n  CommitPolicy:'EventCount:1,Interval:1'\n)\nINPUT FROM @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_PDB_Target USING DatabaseWriter\n\n(\n  Username:'@TARGET_PDB_USER@',\n  ConnectionURL:'@TARGET_PDB_URL@',\n  Tables:'@TARGET_PDB_TABLES@',\n  Password:'@TARGET_PDB_PASSWORD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n  CommitPolicy:'EventCount:1,Interval:1'\n)\nINPUT FROM @APPNAME@_Stream;\n\nEnd APPLICATION @APPNAME@_App;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\ncreate flow @APPNAME@_agentflow;\n\nCreate Source @SOURCE@\n        Using OracleReader\n(\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: 'qatest.test77',\n  FetchSize:1,\n  QueueSize:2000,\n  CommittedTransactions:true,\n  Compression:false\n)\nOutput To @STREAM@;\n\nend flow @APPNAME@_agentflow;\n\ncreate flow @APPNAME@_serverflow;\n\ncreate Target @TARGET@ using S3Writer(\n    bucketname:'@BUCKET@',\n   objectname:'upgradeData.csv',\n   foldername:'upgradefolder',\n  uploadpolicy:'EventCount : 10000,Interval :1m '\n)\nformat using DSVFormatter (\n)\ninput from @STREAM@;\n\nend flow @APPNAME@_serverflow;\n\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION OtelMonitoringApp;\n\nCREATE OR REPLACE TYPE MonitorBatchEvent_Type (\nitems java.util.ArrayList\n);\nCREATE OR REPLACE STREAM MonitorBatchStream OF MonitorBatchEvent_Type;\n\nCREATE OR REPLACE TYPE MonitorEvent_Type (\nitem com.webaction.runtime.monitor.MonitorEvent\n);\nCREATE OR REPLACE STREAM MonitorEventStream OF MonitorEvent_Type;\n\n\nCREATE OR REPLACE CQ MonitorBatchCQ\nINSERT INTO MonitorBatchStream\nselect filterMonEventsForOtelWriter(ms,'INPUT','MEMORY_USED_PERCENT','DISK_FREE','CPU_PER_CORE_PCT','LAG_END2END', 'STATUS_CHANGE') from global.MonitoringSourceStream ms;\n\nCREATE CQ MonitorEventCQ\nINSERT INTO MonitorEventStream\nSELECT ri FROM MonitorBatchStream m, ITERATOR(m.items) ri;\n\nCREATE OR REPLACE Target OTelWriter1 USING StriimOTelWriter INPUT FROM MonitorEventStream;\n\nEND APPLICATION OtelMonitoringApp;\ndeploy application OtelMonitoringApp;\nstart OtelMonitoringApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application GCSWriterTest;\nundeploy application GCSWriterTest;\ndrop application GCSWriterTest cascade;\ncreate application GCSWriterTest recovery 1 second interval;\ncreate source GCS_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'false'\n)\nOUTPUT TO CsvStream;\n\ncreate target sys using sysout(name:'raw')input from CsvStream;\n\ncreate Target GCSTarget using GCSWriter(\n    bucketname:'@bucketname@',\n    objectname:'@objectname@',\n    foldername:'@foldername@',\n    projectId:'@project-id@',\n    uploadPolicy:'@UPLOAD-SIZE@'    \n)\nformat using AvroFormatter (\n)\ninput from CsvStream;\n\nend application GCSWriterTest;\ndeploy application GCSWriterTest on all in default;\nstart application GCSWriterTest;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "IMPORT cern.colt.list.DoubleArrayList;\n\nIMPORT com.webaction.runtime.LocationInfo;\n\nIMPORT com.webaction.runtime.LocationArrayList;\n\nUNDEPLOY APPLICATION FraudTester.FraudDetectionApp;\nDROP APPLICATION FraudTester.FraudDetectionApp cascade;\n\nCREATE APPLICATION FraudDetectionApp;\n\nCREATE SOURCE CsvDataSource USING CSVReader\n(\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'fraudPosData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE PosData\n(\n  merchantId String,\n  pan String,\n  datetime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCREATE STREAM PosDataStream OF PosData PARTITION BY merchantId;\n\nCREATE CQ CsvToPosData\nINSERT INTO PosDataStream\nSELECT data[1], data[2], TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]), data[9]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW PosData5Minutes\nOVER PosDataStream KEEP WITHIN 5 MINUTE ON datetime\nPARTITION BY merchantId;\n\nCREATE TYPE USAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\nCREATE CACHE ZipLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'USAddresses.txt',\n  header: Yes,\n  columndelimiter: '\t',\n  trimquote:false\n) QUERY (keytomap:'zip') OF USAddressData;\n\nCREATE TYPE MerchantNameData(\n  merchantId\t\tString KEY,\n  companyName\t\tString\n);\n\nCREATE CACHE NameLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'MerchantNames.csv',\n  header: Yes,\n  columndelimiter: ',',\n  trimquote:false\n) QUERY(keytomap:'merchantId') OF MerchantNameData;\n\nCREATE TYPE CustomerDetails(\n  pan String KEY,\n  fname String,\n  lname String,\n  address String,\n  city String,\n  state String,\n  zip String,\n  gender String\n);\n\nCREATE CACHE CustomerLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'customerdetails.csv',\n  header: Yes,\n  columndelimiter: ',',\n  trimquote:false\n) QUERY(keytomap:'pan') OF CustomerDetails;\n\nCREATE TYPE PanWithMerchantData(\nwKey String KEY,\npan String,\ncount int,\ndatetime DateTime,\nlocations com.webaction.runtime.LocationArrayList);\n\nCREATE STREAM PanWithMerchantDataStream of PanWithMerchantData PARTITION BY wKey;\n\nCREATE CQ CsvToPanWithMerchantData\nINSERT into PanWithMerchantDataStream\nSELECT p.pan+p.datetime, p.pan , count(*), p.datetime, locInfoList(z.latVal, z.longVal, z.city, z.zip, n.companyName)\nFROM PosData5Minutes p, ZipLookup z, NameLookup n\nWHERE p.zip = z.zip AND p.merchantId = n.merchantId group by p.pan having count(*) > 1;\n\nCREATE TYPE PanWithMerchantAndCustomerData(\nwKey String KEY,\npan String,\nfname String,\nlname String,\naddress String,\ncity String,\nstate String,\nzip String,\ngender String,\ncount int,\ndatetime DateTime,\nlocations com.webaction.runtime.LocationArrayList);\n\nCREATE STREAM PanWithMerchantAndCustomerDataStream of PanWithMerchantAndCustomerData PARTITION BY wKey;\n\nCREATE CQ PanWithMerchantToMerchantAndCustomerData\nINSERT into PanWithMerchantAndCustomerDataStream SELECT ds.wKey, ds.pan, c.fname, c.lname, c.address, c.city, c.state, c.zip, c.gender, ds.count, ds.datetime, ds.locations\nFROM PanWithMerchantDataStream ds, CustomerLookup c\nWHERE ds.pan = c.pan;\n\nCREATE TYPE AlertRecord(\n  name String,\n  keyVal String,\n  severity String,\n  flag String ,\n  message String\n);\n\nCREATE STREAM AlertStream OF AlertRecord;\n\nCREATE CQ GenerateAlerts\nINSERT INTO AlertStream\nSELECT p.pan, p.wKey,\n       CASE\n         WHEN p.count <= 2  THEN 'info'\n      WHEN p.count > 2 THEN 'warning'\n         ELSE 'error' END,\n       CASE\n         WHEN p.count > 1 THEN 'raise'\n         ELSE 'cancel' END,\n       CASE\n         WHEN p.count <= 2 THEN textFromFields('PAN $2 has been used $10 times in a 5 minute period',p)\n         WHEN p.count > 2 THEN textFromFields('[Abnormal Usage] PAN $2 has been used $10 times in a 5 minute period',p)\n         ELSE ''\n         END\nFROM PanWithMerchantAndCustomerDataStream p;\n\nCREATE TARGET SendAlert USING AlertSender(\n name: unusualActivity\n) INPUT FROM AlertStream;\n\nCREATE WACTIONSTORE FraudActivity CONTEXT OF PanWithMerchantAndCustomerData\nEVENT TYPES ( PanWithMerchantAndCustomerData )\n@PERSIST-TYPE@\n\n\nCreate CQ TrackFraudActivity\nINSERT INTO FraudActivity\nSelect * from PanWithMerchantAndCustomerDataStream\nLINK SOURCE EVENT;\n\n\nEND APPLICATION FraudDetectionApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@ recovery 5 second Interval;\n\nCREATE SOURCE @src1Name@ USING MySQLReader (\n  Username:'@srcusername@',\n  Password:'@srcpassword@',\n  ConnectionURL:'@srcurl@',\n  Tables:'@srcschema@.@srctable@',\n  BidirectionalMarkerTable: '@srcschema@.@srcbidirectionaltable@'\n)\nOUTPUT TO @outstream1name@;\n\nCREATE TARGET @tgt1Name@ USING DatabaseWriter (\n  ConnectionURL:'@tgturl@',\n  Username:'@tgtusername@',\n  Password:'@tgtpassword@',\n  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',\n  CheckPointTable: 'CHKPOINT',\n  BidirectionalMarkerTable: '@tgtschema@.@tgtbidirectionaltable@'\n)\nINPUT FROM @instream1name@;\n\nCREATE SOURCE @src2Name@ USING MSSQLReader (\n  ConnectionURL:'@tgturl@',\n  DatabaseName:'@databasename@',\n  Username:'@tgtusername@',\n  Password:'@tgtpassword@',\n  Tables:'@tgtschema@.@tgttable@',\n  BidirectionalMarkerTable: '@tgtschema@.@tgtbidirectionaltable@',\n  TransactionSupport: true\n)\nOUTPUT TO @outstream2name@;\n\nCREATE TARGET @tgt2Name@ USING DatabaseWriter (\n  Username:'@srcusername@',\n  Password:'@srcpassword@',\n  ConnectionURL:'@srcurl@',\n  Tables: '@tgtschema@.@tgttable@,@srcschema@.@srctable@',\n  CheckPointTable: 'CHKPOINT',\n  BidirectionalMarkerTable: '@srcschema@.@srcbidirectionaltable@'\n)\nINPUT FROM @instream2name@;\n\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "-- The PosApp sample application demonstrates how a credit card\n-- payment processor might use Striim to generate reports on current\n-- transaction activity by merchant and send alerts when transaction\n-- counts for a merchant are higher or lower than average for the time\n-- of day.\n\nstop test.PosApp;\nundeploy application test.PosApp;\ndrop application test.PosApp cascade;\n\nCREATE APPLICATION PosApp;\n\n-- All CREATE statements between here and the END APPLICATION\n-- statement will create objects in the PosApp application.\n\n-- source CsvDataSource\n\nCREATE source CsvDataSource USING FileReader (\n  directory:'Samples/Customer/PosApp/appData',\n  wildcard : '$admin.wildcard',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\n\n-- This is the primary data source for this application.\n-- In a real-world application, it would be real-time data. Here,\n-- the data comes from a comma-delimited file, posdata.csv. The first\n-- two lines of that file are:\n--\n-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY\n-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand\n--\n-- The \"header:Yes\" setting tells Striim that the first line contains\n-- field labels that should not be treated as data.\n--\n-- The \"positionByEOF:false\" setting tells Striim to start reading\n-- from the beginning of the file. (In a real-world application\n-- reading real log files, you would typically use the default \"true\"\n-- setting so that the application would read only new data.)\n--\n-- The OUTPUT TO clause automatically creates the stream\n-- CsvStream using the WAEvent type associated with the CSVReader\n-- adapater. The only field from WAEvent used by this application\n-- is \"data\", an array containing the delimited fields.\n\n\n-- CQ CsvToPosData\n\nCREATE CQ CsvToPosData\nINSERT INTO PosDataStream\nSELECT TO_STRING(data[1]) as merchantId,\n       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,\n       TO_DOUBLE(data[7]) as amount,\n       TO_STRING(data[9]) as zip\nFROM CsvStream;\n\n-- Here, \"data\" refers to the array mentioned above, and the number\n-- in brackets specifies a field from the array, counting from zero.\n-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH\n-- AMOUNT, and data[9] is ZIP.\n--\n-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as\n-- the types to be used in PosDataStream, which is created automatically.\n\n-- The DATETIME field from the source is converted to both a dateTime\n-- value, used as the event timestamp by the application, and (via the\n-- function) an integer hourValue, which is used to look up\n-- historical hourly averages from the HourlyAveLookup cache,\n-- discussed below.\n--\n-- The other six fields are discarded. Thus the first line of data\n-- from posdata.csv has at this point been reduced to five values:\n--\n-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)\n-- 20130312173210 (DateTime)\n-- 17 (hourValue)\n-- 2.20 (amount)\n-- 41363 (zip)\n\n\n-- CQ GenerateMerchantTxRateOnly\n--\n-- The PosData5Minutes window bounds the data so that the query can\n-- use the COUNT, FIRST, and SUM functions and join data from the\n-- HourlyAveLookup cache. (Aggregate functions cannot be used and\n-- joins cannot be performed on unbound real-time data.)\n--\n-- The HourlyAveLookup cache provides historical average sales\n-- amounts for the current hour for each merchant.\n\nCREATE JUMPING WINDOW PosData5Minutes\nOVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;\n\nCREATE TYPE MerchantHourlyAve(\n  merchantId String,\n  hourValue integer,\n  hourlyAve integer\n);\nCREATE CACHE HourlyAveLookup using FileReader (\n  directory: 'Samples/Customer/PosApp/appData',\n  wildcard: 'hourlyData.txt'\n)\nPARSE USING DSVParser (\n  header: Yes,\n  trimquote:false\n) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;\n\nCREATE TYPE MerchantTxRate(\n  merchantId String KEY,\n  zip String,\n  startingTime DateTime,\n  count integer,\n  totalAmount double,\n  hourlyAve integer,\n  upperLimit double,\n  lowerLimit double,\n  category String,\n  status String\n);\nCREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;\n\nCREATE CQ GenerateMerchantTxRateOnly\nINSERT INTO MerchantTxRateOnlyStream\nSELECT p.merchantId,\n       FIRST(p.zip),\n       FIRST(p.dateTime),\n       COUNT(p.merchantId),\n       SUM(p.amount),\n       FIRST(l.hourlyAve/12),\n       FIRST(l.hourlyAve/12 * CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       FIRST(l.hourlyAve/12 / CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       '<NOTSET>',\n       '<NOTSET>'\nFROM PosData5Minutes p, HourlyAveLookup l\nWHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue\nGROUP BY p.merchantId;\n\n-- This query aggregates five minutes' worth of data for each\n-- merchant, calculating the total transaction count and amount, and\n-- calculates the upperLimit and lowerLimit values based on the\n-- historical average transaction count for the current hour of the\n-- day from the HourlyAveLookup cache. The category and status fields\n-- are left unset to be populated by the next query.\n\n\n-- CQ GenerateMerchantTxRateWithStatus\n--\n-- This query sets the count values used by the Dashboard map and the\n-- status values used to trigger alerts.\n\nCREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;\n\nCREATE CQ GenerateMerchantTxRateWithStatus\nINSERT INTO MerchantTxRateWithStatusStream\nSELECT merchantId,\n       zip,\n       startingTime,\n       count,\n       totalAmount,\n       hourlyAve,\n       upperLimit,\n       lowerLimit,\n       CASE\n         WHEN count >10000 THEN 'HOT'\n         WHEN count > 800 THEN 'WARM'\n         WHEN count >200 THEN 'COOL'\n         ELSE 'COLD' END,\n       CASE\n         WHEN count > upperLimit THEN 'TOOHIGH'\n         WHEN count < lowerLimit THEN 'TOOLOW'\n         ELSE 'OK' END\nFROM MerchantTxRateOnlyStream;\n\n\n-- WAction store MerchantActivity\n--\n-- The following group of statements create and populate the MerchantActivity\n-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced\n-- with merchant details from NameLookup cache and with latitude and longitude\n-- values from the USAddressData cache.\n\nCREATE TYPE MerchantActivityContext(\n  MerchantId String KEY,\n  StartTime  DateTime,\n  CompanyName String,\n  Category String,\n  Status String,\n  Count integer,\n  HourlyAve integer,\n  UpperLimit double,\n  LowerLimit double,\n  Zip String,\n  City String,\n  State String,\n  LatVal double,\n  LongVal double\n);\n\nCREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext\nEVENT TYPES ( MerchantTxRate )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\n\nCREATE TYPE MerchantNameData(\n  merchantId String KEY,\n  companyName String\n);\n\nCREATE TYPE USAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\nCREATE CACHE NameLookup using FileReader (\n  directory:'Samples/Customer/PosApp/appData',\n  WildCard:'MerchantNames.csv',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:'yes',\n  trimquote:false\n)\nQUERY(keytomap:'merchantId') OF MerchantNameData;\n\nCREATE CACHE ZipLookup using FileReader (\n  directory: 'Samples/Customer/PosApp/appData',\n  wildcard: 'USAddresses.txt',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header: Yes,\n  columndelimiter: '\\t',\n  trimquote:false\n) QUERY (keytomap:'zip') OF USAddressData;\n\nCREATE CQ GenerateWactionContext\nINSERT INTO MerchantActivity\nSELECT  m.merchantId,\n        m.startingTime,\n        n.companyName,\n        m.category,\n        m.status,\n        m.count,\n        m.hourlyAve,\n        m.upperLimit,\n        m.lowerLimit,\n        m.zip,\n        z.city,\n        z.state,\n        z.latVal,\n        z.longVal\nFROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z\nWHERE m.merchantId = n.merchantId AND m.zip = z.zip\nLINK SOURCE EVENT;\n\n\n-- CQ GenerateAlerts\n--\n-- This CQ sends an alert when a merchant's status value changes to\n-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.\n\n\nCREATE STREAM AlertStream OF Global.AlertEvent;\n\nCREATE CQ GenerateAlerts\nINSERT INTO AlertStream\nSELECT n.CompanyName,\n       m.MerchantId,\n       CASE\n         WHEN m.Status = 'OK' THEN 'info'\n         ELSE 'warning' END,\n       CASE\n         WHEN m.Status = 'OK' THEN 'cancel'\n         ELSE 'raise' END,\n       CASE\n         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)\n         ELSE ''\n         END\nFROM MerchantTxRateWithStatusStream m, NameLookup n\nWHERE m.merchantId = n.merchantId;\n\n\n\n--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM AlertStream;\n\nCREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;\n\n\nEND APPLICATION PosApp;\n\n\nCREATE DASHBOARD USING \"Samples/Customer/PosApp/PosAppDashboard.json\";\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use PosTester;\nalter application PosApp;\n\nCREATE CQ CsvToPosData\nINSERT INTO PosDataStream\nSELECT TO_STRING(data[1]) as merchantId,\n       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,\n       TO_DOUBLE(data[7]) as amount,\n       TO_STRING(data[9]) as zip\nFROM CsvStream;\n\nend application PosApp;\n\nalter application PosApp recompile;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING PostgreSQLReader  (\n ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --startLSN:'758199500005967269518578814280000',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.postgrestopostgres_src',\n  ExcludedTables:'public.postgres_2000target'\n )\nOUTPUT TO @STREAM@ ;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@;\n\nCREATE OR REPLACE TYPE @APPNAME@_Type (\n HEADER java.lang.String,\n DETAILNode com.fasterxml.jackson.databind.JsonNode\n );\n\nCREATE OR REPLACE SOURCE CCBReader USING FileReader (\n  wildcard: '@WILDCARD@',\n  positionbyeof: false,\n  directory: '@TESTDIR@'\n  )\nPARSE USING Global.CobolCopybookParser (\n  copybookDialect: 'Mainframe',\n  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',\n  GroupPolicy: '@GRPPOLICY@',\n  TimeoutPolicy: '1s',\n  dataFileOrganization: 'FixedLength',\n  ProcessCopyBookFileAs: 'MultipleEvents',\n  skipIndent: 0,\n  dataFileFont: 'utf8',\n  CopybookFileFormat: 'USE_STANDARD_COLUMNS',\n  copybookSplit: 'None',\n  copybookFileName: '@CCBFILE@'\n   )\nOUTPUT TO CCBStream;\n\nCREATE OR REPLACE CQ @APPNAME@_CQ\nINSERT INTO @APPNAME@_CQOut\nSELECT data.get('HEADER-RECORD').get('DSD-BATCH').textvalue() as HEADER,\ndata.get('DETAIL-RECORD') as DETAILNode FROM CCBStream c;\n\nCREATE OR REPLACE CQ @APPNAME@_CQ1\nINSERT INTO @APPNAME@_CQOut1\nSELECT HEADER,\nDETAILNode.get('REC-TYPE').textvalue() as DRECTYPE,\nDETAILNode.get('AP-VENDOR').textvalue() as DAPVENDOR,\nDETAILNode.get('FACILITY').textvalue() as DFACILITY,\nDETAILNode.get('INVOICE-NUM').textvalue() as DINVOICENUM,\nDETAILNode.get('DIV').textvalue() as DDIV,\nDETAILNode.get('BILLING-COST').doubleValue() as DBILLINGCOST,\nDETAILNode.get('BILLING-RETAIL').doubleValue() as DBILLINGRETAIL,\nDETAILNode.get('TAX-AMOUNT').doubleValue() as DTAXAMOUNT,\nDETAILNode.get('CASH-DISCOUNT').doubleValue() as DCASHDISCOUNT\nFROM @APPNAME@_CQOut c, iterator(c.DETAILNode) DETAILNode;;\n\nCREATE OR REPLACE TARGET JSONWriter USING FileWriter (\n  filename: '%@metadata(FileName)%',\n  directory: '@DIR@',\n  rolloverpolicy: 'EventCount:10000,Interval:30s',\n  flushpolicy: 'EventCount:1,Interval:30s'\n  )\nFORMAT USING JSONFormatter  ()\nINPUT FROM @APPNAME@_CQOut1;\n\nCREATE OR REPLACE TARGET OracleTarget USING DatabaseWriter (\n  ConnectionURL: '', \n  Password: '', \n  Username: '',\n  Tables: '',  \n  CommitPolicy: 'EventCount:10,Interval:10', \n  BatchPolicy: 'EventCount:10,Interval:10'\n  )\nINPUT FROM @APPNAME@_CQOut1;\n\nCREATE TARGET BigQueryTarget USING BigQueryWriter (\n  Tables: '',\n  projectId:'',\n  BatchPolicy: 'eventCount:1, Interval:1',\n  ServiceAccountKey: '',\n)\nINPUT FROM @APPNAME@_CQOut1;\n\nend application @APPNAME@;\ndeploy application @APPNAME@ on all in default;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader\n(\nFetchSize:'@FETCHSIZE@',\nQueueSize:'@QUEUESIZE@',\nUsername:'@USERNAME@',\nPassword:'@PASSWORD@',\nConnectionURL:'@URL@',\n_h_useClassic: @CLASSICMODE@,\nTables:'@TABLES@',\nCommittedTransactions:'@COMMITEDT@'\nCompression:'@COMPRESSION@'\n)\nOUTPUT TO @STREAM@;\n\nCREATE SOURCE @SOURCE_NAME@2 USING Global.OracleReader\n(\nFetchSize:'@FETCHSIZE@',\nQueueSize:'@QUEUESIZE@',\nUsername:'@USERNAME@',\nPassword:'@PASSWORD@',\nConnectionURL:'@URL@',\n_h_useClassic: @CLASSICMODE@,\nTables:'@TABLES@',\nCommittedTransactions:'@COMMITEDT@'\nCompression:'@COMPRESSION@'\n)\nOUTPUT TO @STREAM@2;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE @SOURCE@ USING MSSQLReader\n (\n   Username: '@LOGMINER-UNAME@',\n   Password: '@LOGMINER-PASSWORD@',\n   ConnectionURL: '@LOGMINER-URL@',\n   DatabaseName:'qatest',\n   Tables: '@SOURCE_TABLE@',\n    Compression:false,\n    AutoDisableTableCDC:false,FetchTransactionMetadata:true,\n    StartPosition:'EOF'\n )\nOUTPUT TO @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET@1 USING SnowflakeWriter (\n  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',\n  optimizedMerge: 'false',\n  password: 'Web@ct10n',\n  username: 'striim_regression2',\n  appendOnly: 'false',\n  uploadPolicy: 'eventcount:10000,interval:5m',\n  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET@2 USING SnowflakeWriter (\n  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',\n  optimizedMerge: 'false',\n  password: 'Web@ct10n',\n  username: 'striim_regression2',\n  appendOnly: 'true',\n  uploadPolicy: 'eventcount:10000,interval:5m',\n  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET@3 USING SnowflakeWriter (\n  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',\n  optimizedMerge: 'true',\n  password: 'Web@ct10n',\n  username: 'striim_regression2',\n  appendOnly: 'false',\n  uploadPolicy: 'eventcount:10000,interval:5m',\n  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',\n )\nINPUT FROM @STREAM@;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application MySQLToOracle;\nundeploy application MySQLToOracle;\ndrop application MySQLToOracle cascade;\n\nCREATE APPLICATION MySQLToOracle recovery 5 second interval;\n\ncreate source Src1ReadFromMySQL USING MySQLReader (\nUsername: 'root',\n  Password: 'w@ct10n',\n  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',\n--Tables: 'waction.mytable1;waction.mytable2;qatest.mysqlmarker',\nTables: 'waction.Parent%;waction.Child%;',\n--Tables: 'waction.mytable%',\nBiDirectionalMarkerTable: 'waction.mysqlmarker',\ncompression:'false',\nsendBeforeImage:True\n) OUTPUT TO App1Stream;\n\n/*\nCREATE TARGET MySQLReaderOutput\nUSING SysOut(name:MySQLReaderOutput)\nINPUT FROM App1Stream;\n*/\n\nCREATE TARGET MySQLReaderOutput using FileWriter(\n  filename:'MySQLReaderOutput.log',\n  flushpolicy: 'EventCount:1, Interval:30s',\n  rolloverpolicy: 'Eventcount:1000, rolloverpolicy:3000s')\nFORMAT USING JsonFormatter ()\n\nINPUT FROM App1Stream;\n\n\nCREATE TARGET WriteToOracle USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'EventCount:10,Interval:5',\nCommitPolicy:'EventCount:15,Interval:10',\nBiDirectionalMarkerTable: 'qatest.mysqlmarker',\n--Tables: 'waction.mytable1,qatest.mytable1;'\n--Tables: 'waction.table_1_1,qatest.table_1_1;waction.table_2_1,qatest.table_2_1;waction.table_1_2,qatest.table_1_2;waction.table_2_2,qatest.table_2_2'\n--Tables: 'waction.mytable%,qatest.%'\nTables: 'waction.Parent%,qatest.%;waction.Child%,qatest.%'\n\n)\nINPUT FROM App1Stream;\n\n\n\n--Orcle to MySQL\n\nCreate Source SrcReadFromOracle\nUsing OracleReader\n(\nUsername:'qatest',\nPassword:'qatest',\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\n--Tables:'qatest.mytable1;qatest.mytable2;qatest.mysqlmarker;qatest.mysqlmarker2',\nTables:'QATEST.PARENT%;QATEST.CHILD%',\n--Tables:'qatest.mytable%',\nBiDirectionalMarkerTable: 'QATEST.MYSQLMARKER',\nFilterTransactionBoundaries: true,\ncompression:'false',\nFetchSize: 1\n\n)\nOutput To App2Stream;\n\n/*\nCREATE TARGET MSSqlReaderOutput\nUSING SysOut(name:MSSqlReaderOutput)\nINPUT FROM App2Stream;\n*/\n\nCREATE TARGET OracleReaderOutput using FileWriter(\n  filename:'OracleReaderOutput.log',\n    flushpolicy: 'EventCount:1, Interval:30s',\n  rolloverpolicy: 'Eventcount:1000, rolloverpolicy:3000s')\nFORMAT USING JsonFormatter ()\n\nINPUT FROM App2Stream;\n\n\n\nCREATE TARGET WriteToMySQL1 USING DatabaseWriter(\nConnectionURL:'jdbc:mysql://localhost:3306/waction',\nUsername:'root',\nPassword:'w@ct10n',\nBatchPolicy:'EventCount:10,Interval:10',\nCommitPolicy: 'EventCount:15,Interval:12',\nBiDirectionalMarkerTable: 'waction.mysqlmarker',\n--Tables: 'qatest.mytable1,waction.mytable1;qatest.mytable2,waction.mytable3;'\nTables: 'QATEST.PARENT_1,waction.Parent_1;QATEST.PARENT_2,waction.Parent_2;QATEST.CHILD_1,waction.Child_1;QATEST.CHILD_2,waction.Child_2;'\n--Tables: 'qatest.mytable%,waction.%'\n)\nINPUT FROM App2Stream;\n\n\n\n\nEND APPLICATION MySQLToOracle;\ndeploy application MySQLToOracle;\nstart application MySQLToOracle;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE STREAM @STREAM@_JSON OF Global.JsonNodeEvent;\n\nCREATE TARGET @TARGET_NAME@ USING SpannerWriter (\n\tTables: 'QATEST.%,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tinstanceId: 'qatest'\n) INPUT FROM @STREAM@_JSON;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\n--drop exceptionstore admin.Oracle12C_To_Oracle12CApp_ExceptionStore;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ use exceptionstore;\n\nCreate Source @SourceName@\n Using OracleReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'qatest.@SourceTable@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: 'qatest.@SourceTable@,qatest.@TargetTable@',\n    CommitPolicy: 'Interval:5'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\ncreate or replace cq @cq@\ninsert into @finalstream@\nselect exceptionType,action,appName,entityType,entityName,className,message,relatedActivity from @APPNAME@_ExceptionStore;\n\nCreate target @targetfile@ using filewriter (\nfilename:'@APPNAME@_file.log',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000,sequence:00'\n)\nformat using jsonFormatter()\ninput from @finalstream@;\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;\n\nstop @APPNAME@_ExpStore;\nundeploy application @APPNAME@_ExpStore;\ndrop application @APPNAME@_ExpStore cascade;\nCREATE APPLICATION @APPNAME@_ExpStore;\n\nCREATE TYPE @APPNAME@_ExpStore_CDCStreams_Type  (\n  evtlist java.util.List  \n );\n\nCREATE STREAM @APPNAME@_ExpStore_CDCStreams OF @APPNAME@_ExpStore_CDCStreams_Type;\n\nCREATE CQ @APPNAME@_ReadFromExpStore \nINSERT INTO @APPNAME@_ExpStore_CDCStreams\nselect to_waevent(s.relatedObjects) as evtlist from admin.@APPNAME@_ExceptionStore [jumping 5 second] s;\n\nCREATE STREAM @APPNAME@_ExpStore_CDCEventStream OF Global.WAEvent;\n\nCREATE CQ @APPNAME@_ExpStore_GetCDCEvent \nINSERT INTO @APPNAME@_ExpStore_CDCEventStream\nSELECT com.webaction.proc.events.WAEvent.makecopy(cdcevent) FROM @APPNAME@_ExpStore_CDCStreams a, iterator(a.evtlist) cdcevent;\n\nCREATE CQ @APPNAME@_ExpStore_JoinDataCQ\nINSERT INTO @APPNAME@_ExpStore_JoinedDataStream\nSELECT  TO_INT(f.data[0]),\n        TO_STRING(f.data[1])\n        from @APPNAME@_ExpStore_CDCEventStream f;\n        \nCREATE OR REPLACE TARGET @APPNAME@_ExpStore_WriteToFileAsJSON USING FileWriter  ( \n  filename: 'expEvent_Oracle',\n  rolloveronddl: 'true',\n  flushpolicy: 'eventcount:1,interval:30',\n  adapterName: 'FileWriter',\n  directory: 'ExpStore_logs',\n  rolloverpolicy: 'eventcount:6,interval:30s'\n ) \nFORMAT USING JSONFormatter  ( \n  handler: 'com.webaction.proc.JSONFormatter',\n  formatterName: 'JSONFormatter',\n  jsonMemberDelimiter: '\\n',\n  jsonobjectdelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true'\n ) \nINPUT FROM @APPNAME@_ExpStore_JoinedDataStream;\n        \nCREATE TARGET @APPNAME@_ExpStore_dbtarget USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:1,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:1',\nTables:'@TargetTable@'\n) INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;\n\nEND APPLICATION @APPNAME@_ExpStore;\n\ndeploy application @APPNAME@_ExpStore;\nstart @APPNAME@_ExpStore;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ recovery 5 second interval;\n\nCREATE  SOURCE @QuerySource@1 USING DatabaseReader  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@1;\ncreate Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;\n CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@1;\n\nCREATE  SOURCE @QuerySource@2 USING DatabaseReader  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@2;\ncreate Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;\n CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@2;\n\nCREATE  SOURCE @QuerySource@3 USING DatabaseReader  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@3;\ncreate Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;\n CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@3;\n\nCREATE  SOURCE @QuerySource@4 USING DatabaseReader  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@4;\ncreate Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;\n CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@4;\n\nCREATE  SOURCE @QuerySource@5 USING DatabaseReader  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@5;\ncreate Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;\n CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@5;\n\nCREATE  SOURCE @QuerySource@6 USING DatabaseReader  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@6;\ncreate Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;\n CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@6;\n\nCREATE  SOURCE @QuerySource@7 USING DatabaseReader  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@7;\ncreate Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;\n CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@7;\n\nCREATE  SOURCE @QuerySource@8 USING DatabaseReader  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@8;\ncreate Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;\n CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@8;\n\nCREATE  SOURCE @QuerySource@9 USING DatabaseReader  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@9;\ncreate Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;\n CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@9;\n\nCREATE  SOURCE @QuerySource@10 USING DatabaseReader  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@10;\ncreate Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;\n CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (\n  ConnectionURL: '@CONN_URL@',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@10;\nend application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION KafkaWPTester.KWApp;\nSTOP APPLICATION KafkaWPTester.KRApp;\nUNDEPLOY APPLICATION KafkaWPTester.KWApp;\nUNDEPLOY APPLICATION KafkaWPTester.KRApp;\nDROP APPLICATION KafkaWPTester.KWApp CASCADE;\nDROP APPLICATION KafkaWPTester.KRApp CASCADE;\n\nCREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;\nCONNECT KafkaWPTester KafkaWPTester;\n\n\nCREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE SOURCE CSVSource USING FileReader (\n\tdirectory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',\n    WildCard:'access_log_half',\n\tpositionByEOF:false,\n        Charset:'UTF-8'\n)\nPARSE USING DSVParser (\n  columndelimiter:' ',\n  ignoreemptycolumn:'Yes',\n  quoteset:'[]~\"',\n  separator:'~'\n)\nOUTPUT TO FileStream;\n\nCREATE TYPE AccessLogType(\nCol1 String,\nCol2 String,\nCol3 String,\nCol4 String,\nCol5 String,\nCol6 String,\nCol7 String,\nCol8 String,\nCol9 String,\nCol10 String\n);\n\nCREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;\n\nCREATE CQ AceeslogCQ\nINSERT INTO TypedAccessLogStream\nSELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]\nFROM FileStream;\n\ncreate Target DsvWriter using KafkaWriter VERSION '2.1.0' ( \n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V2dsvExistTopic',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream;\n\n\ncreate Target JsonWriter using KafkaWriter VERSION '2.1.0' ( \n\tbrokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n\tTopic:'V2jsonExistTopic',\n\tMode:'Sync'\n    )\n \nFORMAT USING JSONFormatter ()\ninput from TypedAccessLogStream;\n\n\ncreate Target AvroWriter using KafkaWriter VERSION '2.1.0' ( \n\tbrokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n\tTopic:'V2avroExistTopic',\n    Mode:'Sync'\n    )\n \nFORMAT USING AvroFormatter (\n\tschemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')\ninput from TypedAccessLogStream;\n\n\nEND APPLICATION KWApp;\n\nDEPLOY APPLICATION KWApp on any in default;\n\n\n-- Kafka Reader Apps\n\nCREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE TYPE AccessLogType2(\nCol1 String,\nCol2 String,\nCol3 String,\nCol4 String,\nCol5 String,\nCol6 String,\nCol7 String,\nCol8 String,\nCol9 String,\nCol10 String\n);\n\nCREATE Stream KafkaDSVReaderStream of AccessLogType2;\nCREATE Stream KafkaJsonReaderStream of AccessLogType2;\nCREATE Stream KafkaAvroReaderStream of AccessLogType2;\n\n\nCREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V2dsvExistTopic',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream1;\n\n\n\n\nCREATE CQ DSVtoKafkaDSVReaderStream\nINSERT INTO KafkaDSVReaderStream\nSELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]\nFROM KafkaReaderStream1;\n\nCREATE TARGET DSVDump USING FileWriter(\nname:testOuput,\nrolloverpolicy:'filesize:500M',\nfilename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V2_RT_DSV')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaDSVReaderStream;\n\nCREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V2jsonExistTopic',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO KafkaReaderStream2;\n\n\n\n\nCREATE CQ DSVtoKafkaJsonReaderStream\nINSERT INTO KafkaJsonReaderStream\nSELECT \ndata.get('Col1').textValue(),\ndata.get('Col2').textValue(),\ndata.get('Col3').textValue(),\ndata.get('Col4').textValue(),\ndata.get('Col5').textValue(),\ndata.get('Col6').textValue(),\ndata.get('Col7').textValue(),\ndata.get('Col8').textValue(),\ndata.get('Col9').textValue(),\ndata.get('Col10').textValue()\nFROM KafkaReaderStream2;\n\nCREATE TARGET JSONDump USING FileWriter(\nname:testOuput,\nrolloverpolicy:'filesize:500M',\nfilename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V2_RT_JSON')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaJsonReaderStream;\n\nCREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V2avroExistTopic',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'\n)\nOUTPUT TO KafkaReaderStream3;\n\n\n\nCREATE Stream KRTypedStream of AccessLogType;\n\nCREATE CQ DSVtoKafkaAvroReaderStream\nINSERT INTO KafkaAvroReaderStream\nSELECT    \ndata.get(\"Col1\").toString(), \ndata.get(\"Col2\").toString(), \ndata.get(\"Col3\").toString(), \ndata.get(\"Col4\").toString(),\ndata.get(\"Col5\").toString(), \ndata.get(\"Col6\").toString(), \ndata.get(\"Col7\").toString(), \ndata.get(\"Col8\").toString(),\ndata.get(\"Col9\").toString(), \ndata.get(\"Col10\").toString()\nFROM KafkaReaderStream3;\n\nCREATE TARGET AVRODump USING FileWriter(\nname:testOuput,\nrolloverpolicy:'filesize:500M',\nfilename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V2_RT_AVRO')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaAvroReaderStream;\n\nEND APPLICATION KRApp;\nDEPLOY APPLICATION KRApp on any in default;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nstop application @appname@Out;\nundeploy application @appname@Out;\ndrop application @appname@Out cascade;\n\ndrop stream @appname@PSStream;\nCREATE OR REPLACE PROPERTYSET @appname@KafkaPropset (zk.address:'localhost:@keeperport@', bootstrap.brokers:'localhost:@brokerport@', partitions:'50');\nCREATE STREAM @appname@PSStream@rand@ OF Global.JSONNodeEvent PERSIST USING @appname@KafkaPropset;\n\nCREATE APPLICATION @appname@ recovery 5 second interval;;\n\nCREATE OR REPLACE SOURCE @cobolsrc@ USING FileReader (\n  wildcard: '',\n  positionbyeof: false,\n  directory: ''\n  )\nPARSE USING Global.CobolCopybookParser (\n  copybookDialect: 'Mainframe',\n  DataHandlingScheme: 'ProcessRecordAsEvent',\n  dataFileOrganization: 'FixedLength',\n  ProcessCopyBookFileAs: 'SingleEvent',\n  skipIndent: 0,\n  dataFileFont: 'utf8',\n  CopybookFileFormat: 'USE_STANDARD_COLUMNS',\n  copybookSplit: 'Level01',\n  copybookFileName: ''\n   )\nOUTPUT TO @appname@PSStream@rand@;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n\nCREATE APPLICATION @appname@Out recovery 5 second interval;\n\nCREATE OR REPLACE TARGET @filetarget@ USING FileWriter (\n  filename: '',\n  directory: '',\n  rolloverpolicy: 'EventCount:10000,Interval:30s',\n  flushpolicy: 'EventCount:1,Interval:30s'\n  )\nFORMAT USING JSONFormatter  (\n  members: 'data',\n  EventsAsArrayOfJsonObjects: 'true'\n )\nINPUT FROM @appname@PSStream@rand@;\n\nEND APPLICATION @appname@Out;\ndeploy application @appname@Out on all in default;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@;\n\nCREATE OR REPLACE TYPE @appname@CQOUT1_Type (\n companyName java.lang.String,\n merchantId java.lang.String,\n dateTime org.joda.time.DateTime,\n hourValue java.lang.String,\n amount java.lang.String,\n zip java.lang.String,\n FileName java.lang.String);\n\nCREATE SOURCE @parquetsrc@ USING FILEReader (\n    wildcard: '',\n    directory: '',\n    positionbyeof: false\n )\nPARSE USING ParquetParser (\n)\nOUTPUT TO @appname@Stream;\n\nCREATE OR REPLACE STREAM @appname@CQOUT1 OF @appname@CQOUT1_Type;\nCREATE OR REPLACE CQ @appname@CQ_PQEvent\nINSERT INTO @appname@CQOUT1\n    Select\n    data.get(\"companyName\").toString(),\n    data.get(\"merchantId\").toString(),\n    TO_DATE(data.get(\"dateTime\").toString()),\n    data.get(\"hourValue\").toString(),\n    data.get(\"amount\").toString(),\n    data.get(\"zip\").toString(),\n    metadata.get(\"FileName\").toString()\n    FROM @appname@Stream p;\n\ncreate Target @s3target@ using S3Writer(\n    bucketname:'',\n    objectname:'',\n    uploadpolicy:'',\n    foldername:''\n)\nformat using JSONFormatter ()\ninput from @appname@CQOUT1;\n\nCREATE OR REPLACE TARGET @dbtarget@ USING DatabaseWriter (\n  Tables: '',\n  ConnectionURL:'',\n  Username:'',\n  Password:'',\n  CommitPolicy: 'EventCount:10,Interval:0',\n  BatchPolicy:'EventCount:10,Interval:0'\n)\nINPUT FROM @appname@CQOUT1;\n\ncreate Target @jsontarget@ using FileWriter(\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:10,Interval:30s',\nrolloverpolicy: 'EventCount:10,Interval:30s' )\nformat using JSONFormatter ()\nINPUT FROM @appname@CQOUT1;\n\ncreate Target @xmltarget@ using FileWriter(\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:10,Interval:30s',\nrolloverpolicy: 'EventCount:10,Interval:30s' )\nformat using XMLFormatter (\n    rootelement:'',\n    elementtuple:'',\n    charset:'UTF-8'\n)\nINPUT FROM @appname@CQOUT1;\n\ncreate Target @dsvtarget@ using FileWriter(\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:10,Interval:30s',\nrolloverpolicy: 'EventCount:10,Interval:30s' )\nformat using DSVFormatter ()\nINPUT FROM @appname@CQOUT1;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@;\n\nCREATE TYPE @APPNAME@type1 (\n companyName java.lang.String,\n merchantId java.lang.String,\n city java.lang.String);\n\nCREATE STREAM @APPNAME@TypedStream OF @APPNAME@type1 PARTITION BY city;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (\n  wildcard: '',\n  positionByEOF: false,\n  directory: ''\n  )\nPARSE USING DSVParser (\nheader:'true'\n)\nOUTPUT TO @APPNAME@Stream;\n\nCREATE OR REPLACE CQ @APPNAME@CQ\nINSERT INTO @APPNAME@TypedStream\nSELECT TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\") as companyName,\nTO_STRING(data[1]) as merchantID,\nTO_STRING(data[10]) as city\nFROM @APPNAME@Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_jmstrgt USING JMSWriter (\n  QueueName: '',\n  UserName: '',\n  Password: '',\n  Ctx: '',\n  Provider: ''\n  )\nFORMAT USING JSONFormatter()\nINPUT FROM @APPNAME@TypedStream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@;\n\nCREATE SOURCE @dsvsrc@ USING FileReader ( \n  directory: '', \n  wildcard: '', \n  positionbyeof: false ) \nPARSE USING DSVParser ()\nOUTPUT TO @appname@OUT;\n\nCREATE TARGET @prqttrgt@ USING FileWriter ( \n  filename: '', \n  directory: '', \n  rolloverpolicy: 'EventCount:10000,Interval:30s' ) \nFORMAT USING ParquetFormatter  ( \n  schemaFileName: '',\n  compressiontype: 'GZIP',\n  members:'data')\nINPUT FROM @appname@OUT;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSVSource using FileReader (\ndirectory:'@TEST-DATA-PATH@',\nWildCard:'smallposdata.csv',\npositionByEOF:false,\ncharset:'UTF-8'\n)\nparse using DSVParser (\nheader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\ndirectory:'@FEATURE-DIR@/logs/',\nfilename:'PosData',\nrolloverpolicy:'EventCount:5000000,Interval:60s'\n)\nformat using DSVFormatter (\n\n)\ninput from TypedCSVStream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizePosDataDefault_actual.log') input from TypedCSVStream;\n\nend application DSV;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP banker.bankApp;\nUNDEPLOY APPLICATION banker.bankApp;\nDROP APPLICATION banker.bankApp cascade;\n\nCREATE APPLICATION bankApp;\n\n\nCREATE source wsSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'banks.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:No,\n  trimquote:false\n) OUTPUT TO QaStream;\n\nCREATE TYPE cacheType( merchantId String, \n\t\t\thourValue int, \n\t\t\thourlyAve int);\n\nCREATE TYPE wsData\n(\nbankID Integer KEY,\nbankName String\n);\n\nCREATE CACHE adhcache using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'banks.csv',\n  header: Yes,\n  columndelimiter: ',',\n  trimquote:false\n) QUERY (keytomap:'bankID') OF wsData;\n\n\nCREATE STREAM wsStream OF wsData;\n\n--Select data from QaStream and insert into wsStream\n\nCREATE CQ csvTowsData\nINSERT INTO wsStream\nSELECT TO_INT(data[0]),data[1] FROM QaStream;\n\n--create jumping window over data in wsStream\n\nCREATE WACTIONSTORE oneWS\nCONTEXT OF wsData\nEVENT TYPES(wsData )\n@PERSIST-TYPE@\n\n--get data from wsStream and place into wactionStore oneWS\nCREATE CQ wsToWaction\nINSERT INTO oneWS\nSELECT * FROM wsStream\nLINK SOURCE EVENT;\n\n\nEND APPLICATION bankApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nCREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()\nPARSE USING Global.DSVParser ()\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()\nFORMAT USING DSVFormatter (\nmembers:'data')\nINPUT FROM @APPNAME@_Stream;\n\nEND APPLICATION @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP virtualTester.VirtualApp;\nUNDEPLOY APPLICATION virtualTester.VirtualApp;\nDROP APPLICATION virtualTester.VirtualApp cascade;\n\nCREATE APPLICATION VirtualApp;\n\n\nCREATE OR REPLACE SOURCE wsSource USING FileReader\n(\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'AdhocQueryData2.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser\n(\n   header:True,\n   columndelimiter:',',\n   trimquote:false\n)OUTPUT TO QaStream;\n\nCREATE OR REPLACE Target TheData using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/QaStream.log') input from QaStream;\n\nCREATE OR REPLACE TYPE USAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\nCREATE OR REPLACE CACHE ZipLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'AdhocQueryData.csv'\n)\nparse using DSVParser\n(\n  header:'yes',\n  columndelimiter: '\t',\n  trimquote:false\n\n)QUERY (keytomap:'zip') OF USAddressData;\n\nCreate OR REPLACE TYPE wsData(\n  CompanyNum String,\n  CompanyName String KEY,\n  CompanyCode int,\n  Zip String\n);\n\n\nCREATE OR REPLACE STREAM wsStream OF wsData;\n\n--Select data from QaStream and insert into wsStream\n\nCREATE OR REPLACE CQ csvTowsData\nINSERT INTO wsStream\nSELECT data[0],\n       data[1],\n       TO_INT(data[3]),\n       data[9]\nFROM QaStream;\n\n\nCREATE OR REPLACE WACTIONSTORE oneWS CONTEXT OF wsData\n  EVENT TYPES (wsData );\n\n\nCREATE OR REPLACE CQ wsToWaction\n  INSERT INTO oneWS\n  SELECT * FROM wsStream\n  LINK SOURCE EVENT;\n\nEND APPLICATION VirtualApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application KinesisTest RECOVERY 1 SECOND INTERVAL;\nCREATE OR REPLACE SOURCE ora_reader USING OracleReader (\n  Username: 'miner',\n  Password: 'miner',\n  ConnectionURL: '192.168.1.113:1521:ORCL',\n  TABLES: 'QATEST.H_REGION;QATEST.H_NATION;QATEST.H_CUSTOMER',\n  FetchSize: '1'\n )\nOUTPUT TO DDLCDCStream;\n\ncreate or replace Target t using KinesisWriter (\n\tregionName:'TARGET_REGION',\n\tstreamName:'TARGET_STREAM',\n\taccesskeyid:'ACCESS_KEY',\n\tsecretaccesskey:'SECRET_KEY'\n)\nformat using JSONFormatter (\n)\ninput from DDLCDCStream;\n\nend application KinesisTest;\ndeploy application KinesisTest in default;\nstart application KinesisTest;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@ Recovery 5 second interval;\n\ncreate stream @APPNAME@_UserdataStream of Global.WAEvent;\n\ncreate type @APPNAME@_Order_type(\nid int,\norder_id int,\nzipcode int,\ncategory String,\ntablename string\n);\n\nCREATE OR REPLACE SOURCE @APPNAME@S1 USING PostgreSQLReader\n(\n  ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: true,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.order_%'\n)\nOUTPUT TO @APPNAME@_OrdersStream;\n\nCREATE OR REPLACE SOURCE @APPNAME@S2 USING PostgreSQLReader\n(\n  ReplicationSlotName: 'test_slot_2',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: true,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.second_order_%'\n)\nOUTPUT TO @APPNAME@_OrdersStream2;\n\nCREATE OR REPLACE SOURCE @APPNAME@S3 USING PostgreSQLReader\n(\n  ReplicationSlotName: 'test_slot_3',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: true,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --startLSN:'758199500005967269518578814280000',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.third_order_%'\n)\nOUTPUT TO @APPNAME@_OrdersStream3;\n\nCREATE OR REPLACE SOURCE @APPNAME@S4 USING PostgreSQLReader\n(\n  ReplicationSlotName: 'test_slot_4',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: true,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --startLSN:'758199500005967269518578814280000',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.fourth_order_%'\n)\nOUTPUT TO @APPNAME@_OrdersStream4;\n\n\nCreate CQ @APPNAME@_CQUser\ninsert into @APPNAME@_UserdataStream\nselect \nputuserdata (data,'Fileowner','FIRST_ORDER') from @APPNAME@_OrdersStream data;\n\n\nCreate CQ @APPNAME@_CQUser2\ninsert into @APPNAME@_UserdataStream\nselect \nputuserdata (data2,'Fileowner','SECOND_ORDER') from @APPNAME@_OrdersStream2 data2;\n\n\nCreate CQ @APPNAME@_CQUser3\ninsert into @APPNAME@_UserdataStream\nselect \nputuserdata (data3,'Fileowner','THIRD_ORDER') from @APPNAME@_OrdersStream3 data3;\n\n\nCreate CQ @APPNAME@_CQUser4\ninsert into @APPNAME@_UserdataStream\nselect \nputuserdata (data4,'Fileowner','FOURTH_ORDER') from @APPNAME@_OrdersStream4 data4;\n\ncreate stream @APPNAME@_OrderTypedStream1 of @APPNAME@_Order_type;\ncreate stream @APPNAME@_OrderTypedStream2 of @APPNAME@_Order_type;\ncreate stream @APPNAME@_OrderTypedStream3 of @APPNAME@_Order_type;\ncreate stream @APPNAME@_OrderTypedStream4 of @APPNAME@_Order_type;\n\nCREATE CQ @APPNAME@_fin_cq\nINSERT INTO @APPNAME@_OrderTypedStream1\nSELECT to_int(data[0]),\nto_int(data[1]),\nto_int(data[2]),\nUSERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),\nMETA(@APPNAME@_UserdataStream,'TableName').toString()\nFROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FIRST_ORDER';\n\nCREATE CQ @APPNAME@_fin_cq2\nINSERT INTO @APPNAME@_OrderTypedStream2\nSELECT to_int(data[0]),\nto_int(data[1]),\nto_int(data[2]),\nUSERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),\nMETA(@APPNAME@_UserdataStream,'TableName').toString()\nFROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'SECOND_ORDER';\n\nCREATE CQ @APPNAME@_fin_cq3\nINSERT INTO @APPNAME@_OrderTypedStream3\nSELECT to_int(data[0]),\nto_int(data[1]),\nto_int(data[2]),\nUSERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),\nMETA(@APPNAME@_UserdataStream,'TableName').toString()\nFROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'THIRD_ORDER';\n\nCREATE CQ @APPNAME@_fin_cq4\nINSERT INTO @APPNAME@_OrderTypedStream4\nSELECT to_int(data[0]),\nto_int(data[1]),\nto_int(data[2]),\nUSERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),\nMETA(@APPNAME@_UserdataStream,'TableName').toString()\nFROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FOURTH_ORDER';\n\n\ncreate Target @APPNAME@T1 using ADLSGen2Writer(\n        filename:'event_data.csv',\n        directory:'%category%/%tablename%',\n       \taccountname:'adlsgen2nntest',\n        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',\n        filesystemname:'qatestadls2',\n        uploadpolicy:'eventcount:8,interval:20s'\n)\nformat using DSVFormatter (\n    header:'true'\n)\ninput from @APPNAME@_OrderTypedStream1; \n\ncreate Target @APPNAME@T2 using ADLSGen2Writer(\n        filename:'event_data.xml',\n        directory:'%category%/%tablename%',\n\t\taccountname:'adlsgen2nntest',\n        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',\n        filesystemname:'qatestadls2',\n   \t\tuploadpolicy:'eventcount:8,interval:20s'\n)\nformat using XMLFormatter (\n  elementtuple: 'Order_id:id:order_id:zipcode:category:text=tablename',\n  formatterName: 'XMLFormatter',\n  rowdelimiter: '\\n',\n  rootelement: 'document'\n)\ninput from @APPNAME@_OrderTypedStream2; \n\ncreate Target @APPNAME@T3 using ADLSGen2Writer(\n        filename:'event_data.avro',\n        directory:'%category%/%tablename%',\n\t\taccountname:'adlsgen2nntest',\n        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',\n        filesystemname:'qatestadls2',\n  \t\tuploadpolicy:'eventcount:8,interval:20s'\n)\nformat using AvroFormatter (\n  formatAs: 'Default',\n  formatterName: 'AvroFormatter',\n  schemaFileName: '@SCHEMA-FILE@'\n)\ninput from @APPNAME@_OrderTypedStream3; \n\n\ncreate Target @APPNAME@T4 using ADLSGen2Writer(\n        filename:'event_data.json',\n        directory:'%category%/%tablename%',\n        accountname:'adlsgen2nntest',\n        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',\n        filesystemname:'qatestadls2',\n  \t\tuploadpolicy:'eventcount:8,interval:20s'\n)\nFORMAT USING JSONFormatter()\nINPUT FROM @APPNAME@_OrderTypedStream4;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop ADW;\nundeploy application ADW;\nDROP APPLICATION ADW CASCADE;\nCREATE APPLICATION ADW recovery 5 second interval;;\n\nCreate Source OracleSource Using OracleReader\n(\n Username:'@ORACLE-USERNAME',\n Password:'@ORACLE-PASSWORD',\n ConnectionURL: '@ORACLE-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'@FETCH-SIZE@'\n) \nOutput To str;\n\n\ncreate target AzureTarget1 using AzureSQLDWHWriter (\n\t\tCoNNectionURL: '@SQLDW-URL@',\n        username: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@',  \n        uploadpolicy:'@EVENT-COUNT@'\n) INPUT FROM str;\n\n\ncreate target AzureTarget2 using AzureSQLDWHWriter (\n\t\tConnectionURL: '@SQLDW-URL@',\n        UserName: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@', \n        uploadpolicy:'@EVENT-COUNT@'\n) INPUT FROM str;\n\n\ncreate target AzureTarget3 using AzureSQLDWHWriter (\n\t\tConnectionURL: '@SQLDW-URL@',\n        UserName: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@', \n        uploadpolicy:'@EVENT-COUNT@'\n) INPUT FROM str;\n\ncreate target AzureTarget4 using AzureSQLDWHWriter (\n\t\tConnectionURL: '@SQLDW-URL@',\n        UserName: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@', \n        uploadpolicy:'@EVENT-COUNT@'\n) INPUT FROM str;\n\nEND APPLICATION ADW;\ndeploy application ADW;\nstart application ADW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop OracleTOFileWriterApp;\nundeploy application OracleTOFileWriterApp;\ndrop application OracleTOFileWriterApp cascade;\n\nCREATE APPLICATION OracleTOFileWriterApp recovery 5 second interval;\n\nCreate Source ReadFromOracle\nUsing OracleReader\n(\nUsername:'qatest',\nPassword:'qatest',\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:xe',\nTables:'QATEST.VARRAY_AS_VARCHAR',\nOnlineCatalog:true,\nCommittedTransactions:true\n)\nOutput To DataStream;\n\n\nCREATE OR REPLACE TARGET FileWriter1 USING FileWriter  (\nfilename: 'OutFile.csv',\n  directory:'@DIR@/%@metadata(TableName)%',\n  rolloverpolicy: 'EventCount:6,Interval:60s'\n)\nformat using DSVFormatter (\nmembers: 'data'\n)\ninput from DataStream;\n\ncreate target tout using sysout(name:'out') input from DataStream;\n\nCREATE OR REPLACE TARGET FileWriter2 USING FileWriter  (\nfilename: 'OutFile.json',\n  directory:'@DIR@/%@metadata(TableName)%',\n  rolloverpolicy: 'EventCount:6,Interval:60s'\n)\nformat using JSONFormatter (\n)\ninput from DataStream;\n\n\nCREATE OR REPLACE TARGET FileWriter3 USING FileWriter  (\nfilename: 'OutFile.xml',\n  directory:'@DIR@/%@metadata(TableName)%',\n  rolloverpolicy: 'EventCount:6,Interval:60s'\n)\nformat using XMLFormatter (\nrootelement : 'data',\nFormatColumnValueAS:'xmlelement')\ninput from DataStream;\n\n\nCREATE OR REPLACE TARGET FileWriter4 USING FileWriter  (\n  filename: 'OutFile.parquet',\n  directory:'@DIR@/%@metadata(TableName)%',\n  rolloverpolicy: 'EventCount:6,Interval:60s' \n  )\nFORMAT USING ParquetFormatter  \n( \nblocksize: '128000000',\n  formatAs: 'Default',\n  schemaFileName: '@DIR@/parquetSchema'\n  )\nINPUT FROM DataStream;\n\nCREATE OR REPLACE TARGET FileWriter5 USING FileWriter  (\n  filename: 'OutFile.avro',\n  directory:'@DIR@/%@metadata(TableName)%',\n  rolloverpolicy: 'EventCount:6,Interval:60s' \n  )\nFORMAT USING AvroFormatter  \n( \nschemaFileName : '@DIR@/varray.avsc'\n  )\nINPUT FROM DataStream;\n\nEND APPLICATION OracleTOFileWriterApp;\ndeploy application OracleTOFileWriterApp in default;\nstart OracleTOFileWriterApp;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop DataGenSampleApp;\nundeploy application DataGenSampleApp;\ndrop application DataGenSampleApp cascade;\n\n\nCREATE APPLICATION DataGenSampleApp;\n\nCreate Source dataGenSrc Using MSSqlReader\n(\n Username:'qatest',\n Password:'w3b@ct10n',\n DatabaseName:'qatest',\n ConnectionURL:'localhost:1433',\n Tables:'@tableNames@', \n ConnectionPoolSize:1,\n StartPosition:'EOF'\n )\n Output To LCRStream;\n\ncreate Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;\n\nEND APPLICATION DataGenSampleApp;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Kafka Stream with KryoParser Kafka Reader without Recovery Test 2 \n-- Bert Hashemi and Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> KS -> WS\n-- S -> K -> CQ -> WS\n\nSTOP KStreamKryoParserTester2WOR.KStreamKryoParserTest2WOR;\nUNDEPLOY APPLICATION KStreamKryoParserTester2WOR.KStreamKryoParserTest2WOR;\nDROP APPLICATION KStreamKryoParserTester2WOR.KStreamKryoParserTest2WOR CASCADE;\nDROP USER KStreamKryoParserTester2WOR;\nDROP NAMESPACE KStreamKryoParserTester2WOR CASCADE;\nCREATE USER KStreamKryoParserTester2WOR IDENTIFIED BY KStreamKryoParserTester2WOR;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamKryoParserTester2WOR;\nCONNECT KStreamKryoParserTester2WOR KStreamKryoParserTester2WOR;\n\n-- CREATE APPLICATION KStreamKryoParserTest2WOR RECOVERY 5 SECOND INTERVAL;\nCREATE APPLICATION KStreamKryoParserTest2WOR;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'1');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE or REPLACE TYPE KafkaCsvStreamType(\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE Wactions CONTEXT OF KafkaCsvStreamType \nEVENT TYPES ( KafkaCsvStreamType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions\nINSERT INTO Wactions\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream;\n\nCREATE or REPLACE TYPE KafkaStreamType(\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\n--CREATE STREAM KafkaStream OF KafkaStreamType;\nCREATE STREAM KafkaStream OF Global.waevent;\n\nCREATE SOURCE KafkaSource USING KafkaReader\n(\n        brokerAddress:'localhost:9092',\n        Topic:'KStreamKryoParserTester2WOR_KafkaCsvStream',\n        PartitionIDList:'0',\n        startOffset:0\n)\nPARSE USING KryoParser ()\nOUTPUT TO KafkaStream;\n\nCREATE WACTIONSTORE KRWactions CONTEXT OF KafkaStreamType\nEVENT TYPES ( KafkaStreamType )\n@PERSIST-TYPE@\n\nCREATE CQ KRInsertWactions\nINSERT INTO KRWactions\nSELECT TO_STRING(data[1]) as merchantId,\n    TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\n    TO_DOUBLE(data[7]) as amount,\n    TO_STRING(data[10]) as city \nFROM KafkaStream;\n\nEND APPLICATION KStreamKryoParserTest2WOR;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@  RECOVERY 1 SECOND INTERVAL;\nCREATE SOURCE @srcName@ USING OracleReader (\n Username: '@srcusername@',\n  Password: '@srcpassword@',\n  ConnectionURL: '@srcurl@',\n  Tables: '@srcschema@.@srctable@'\n)\nOUTPUT TO @outstreamname@;\n\nCREATE  TARGET @tgtName@ USING Global.KinesisWriter ( \n  BatchPolicy: 'Size:900000,Interval:1', \n  streamName: '@streamname@', \n  accesskeyid: '@accesskeyid@', \n  Mode: 'Sync', \n  regionName: '@region@', \n  secretaccesskey: '@secretaccesskey@', \n  adapterName: 'KinesisWriter' ) \nFORMAT USING Global.DSVFormatter  ( \n  quotecharacter: '\\\"', \n  handler: 'com.webaction.proc.DSVFormatter', \n  columndelimiter: ',', \n  formatterName: 'DSVFormatter', \n  nullvalue: 'NULL', \n  usequotes: 'false', \n  rowdelimiter: '\\n', \n  standard: 'none', \n  header: 'false' ) \nINPUT FROM @instreamname@;\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application GCSWriterTest;\nundeploy application GCSWriterTest;\ndrop application GCSWriterTest cascade;\ncreate application GCSWriterTest recovery 1 second interval;\ncreate source GCS_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'false'\n)\nOUTPUT TO CsvStream;\n\ncreate Target GCSTarget using GCSWriter(\n    bucketname:'@bucketname@',\n    objectname:'@objectname@',\n    projectId:'@project-id@',\n    uploadPolicy:'@UPLOAD-SIZE@',\n    ServiceAccountKey:'@file-path@'\n)\nformat using DSVFormatter (\nmembers:'data'\n)\ninput from CsvStream;\n\nend application GCSWriterTest;\ndeploy application GCSWriterTest on all in default;\nstart application GCSWriterTest;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@1;\nundeploy application @APPNAME@1;\nstop application @APPNAME@2;\nundeploy application @APPNAME@2;\n\nDROP STREAM @APPNAME@_STREAM;\nDROP APPLICATION @APPNAME@1 CASCADE;\nDROP APPLICATION @APPNAME@2 CASCADE;\n\n\nCREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',\n\t\t\t\t\tacks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\nCREATE OR REPLACE STREAM @APPNAME@_stream OF Global.waevent persist using @APPNAME@_KafkaPropset;\n\nCREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';\ncreate application @APPNAME@1 recovery 5 SECOND INTERVAL;\n\ncreate or replace stream @APPNAME@_stream2 of Global.waevent;\n\nCREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNEC4TION_URL@',\n  Tables: '$table1',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_AGENTFLOW1;\n\nCREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@TABLENAME@2',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_SERVERFLOW1;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@1 USING SnowflakeWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;\nCREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x\n WHERE META(x,'TableName').toString() == 'QATEST.ORCTOSFPLATFM_SOURCE4';\n\nCREATE OR REPLACE TARGET @TARGET_NAME@2 USING SnowflakeWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n )\nINPUT FROM @STREAM@2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(\n  name: 'CDDL',\n  filename:'WAEventDump.log'\n)INPUT FROM @STREAM@;\n\nCREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo) input from @STREAM@;\n\nEND APPLICATION @APPNAME@1;\n\n\n\nCREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';\nCREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;\n\ncreate or replace stream @APPNAME@_stream3 of Global.waevent;\n\nCREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '$table2',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\n\nEND FLOW @APPNAME@_AGENTFLOW2;\n\nCREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@TABLENAME@4',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_SERVERFLOW2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@3 USING SnowflakeWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;\nCREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y\n WHERE META(y,'TableName').toString() == 'QATEST.ORCTOSFPLATFM_SOURCE2';\n\nCREATE OR REPLACE TARGET @TARGET_NAME@4 USING SnowflakeWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n )\nINPUT FROM @STREAM@3;\n\nCREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@3;\n\nEND APPLICATION @APPNAME@2;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application KinesisTest RECOVERY 1 SECOND INTERVAL;\nCREATE OR REPLACE SOURCE ora_reader USING OracleReader (\n  Username: 'miner',\n  Password: 'miner',\n  ConnectionURL: '192.168.1.113:1521:ORCL',\n  TABLES: 'QATEST.H_REGION;QATEST.H_NATION;QATEST.H_CUSTOMER',\n  FetchSize: '1'\n )\nOUTPUT TO DDLCDCStream;\n\ncreate or replace Target t using KinesisWriter (\n\tregionName:'TARGET_REGION',\n\tstreamName:'TARGET_STREAM',\n\taccesskeyid:'ACCESS_KEY',\n\tsecretaccesskey:'SECRET_KEY'\n)\nformat using JSONFormatter (\n)\ninput from DDLCDCStream;\n\ncreate or replace Target t2 using KinesisWriter (\n  regionName:'TARGET_REGION',\n  streamName:'TARGET_STREAM',\n  accesskeyid:'ACCESS_KEY',\n  secretaccesskey:'SECRET_KEY'\n)\nformat using JSONFormatter (\n)\ninput from DDLCDCStream;\n\nend application KinesisTest;\ndeploy application KinesisTest in default;\nstart application KinesisTest;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application recoveryTestAgent.CSV;\nundeploy application recoveryTestAgent.CSV;\ndrop application recoveryTestAgent.CSV cascade;\n\ncreate application CSV\nRECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW AgentFlow;\ncreate source CSVSource using CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'customerdetails-recovery.csv',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO CsvStream;\nEND FLOW AgentFlow;\n\nCREATE FLOW ServerFlow;\nCREATE TYPE UserDataType\n(\n  UserId String KEY,\n  UserName String\n);\n\nCREATE STREAM UserDataStream OF UserDataType PARTITION BY UserId;\n\nCREATE CQ ParseUserData\nINSERT INTO UserDataStream\nSELECT  data[0],\n        data[1]\nFROM CsvStream;\n\n\nCREATE WACTIONSTORE UserActivityInfo\nCONTEXT OF UserDataType\nEVENT TYPES ( UserDataType )\n@PERSIST-TYPE@\n\n--get data from UserDataStream and place into wactionStore UserWaction\nCREATE CQ UserWaction\nINSERT INTO UserActivityInfo\nSELECT * FROM UserDataStream\nLINK SOURCE EVENT;\nEND FLOW ServerFlow;\n\nEND APPLICATION CSV;\nDEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;\nSTART CSV;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION OneAgentTester.CSV;\nUNDEPLOY APPLICATION OneAgentTester.CSV;\nDROP APPLICATION OneAgentTester.CSV cascade;\n\nDROP USER OneAgentTester;\nDROP NAMESPACE OneAgentTester CASCADE;\nCREATE USER OneAgentTester IDENTIFIED BY OneAgentTester;\nGRANT create,drop ON deploymentgroup Global.* To user OneAgentTester;\nCONNECT OneAgentTester OneAgentTester;\n\nCREATE APPLICATION CSV;\n\nCREATE FLOW AgentFlow;\ncreate source CSVSource using FileReader\n(\ndirectory:'@TEST-DATA-PATH@',\nwildcard:'StoreNames.csv',\npositionByEOF:false\n)\nparse using DSVParser\n(\nheader:'yes',\ncolumndelimiter:','\n)\nOUTPUT TO CsvStream;\nEND FLOW AgentFlow;\n\nCREATE FLOW ServerFlow;\nCREATE TARGET myout using LogWriter(name: OneAgentSource, filename:'@FEATURE-DIR@/logs/log.txt') input from CsvStream;\nEND FLOW ServerFlow;\n\nEND APPLICATION CSV;\nDEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;\nSTART CSV;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application reconnect;\nundeploy application reconnect;\ndrop application reconnect cascade;\nCREATE APPLICATION reconnect recovery 1 second interval;\n\nCREATE  SOURCE OracleSource USING OracleReader  ( \n  Username: '@USERNAME@',\n  Password: '@PASSWORD@',\n  ConnectionURL: '@URL@',\n  Tables: 'qatest.ReconnectTestSource;qatest.ReconnectTestSourceDummy',\n  FetchSize: '@FETCHSIZE@'\n )output to sqlstream;\n --output to sqlstream MAP (table:'qatest.ReconnectTestSource');\n\nCREATE TARGET dbtarget USING DatabaseWriter(\n  ConnectionURL:'@URL@',\n  Username:'@USERNAME@',\n  Password:'@PASSWORD@',\n  ConnectionRetryPolicy: 'retryInterval=10s, maxRetries=4',\n  BatchPolicy:'EventCount:5000,Interval:30',\n  CommitPolicy:'EventCount:30000,Interval:30',\n  Tables: '@TABLES@'\n ) INPUT FROM sqlstream;\n \n create target filetgt using filewriter(\n filename:'recovering_validation',\n rolloverpolicy:'eventcount:1000000,interval:100m',\n flushpolicy:'eventcount:1000000,interval:100m'\n )format using dsvformatter()\n input from sqlstream;\n\n--create Target tSysOut using Sysout(name:OrgData) input from sqlstream;\n end application reconnect;\n deploy application reconnect;\n start application reconnect;\n "}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application app1PS;\nundeploy application app1PS;\ndrop application app1PS cascade;\n\ncreate application app1PS;\n\ncreate target File_TargerPS using FileWriter\n(\ndirectory : '',\nfilename : ''\n)\nformat using DSVFormatter()\ninput from Recoveryss1;\n\nend application app1PS;\n\ndeploy application app1PS;\nstart application app1PS;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@;\n\nCREATE OR REPLACE SOURCE CCBReader USING FileReader (\n  wildcard: '@WILDCARD@',\n  positionbyeof: false,\n  directory: '@TESTDIR@'\n  )\nPARSE USING Global.CobolCopybookParser (\n  copybookDialect: 'Mainframe',\n  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',\n  GroupPolicy: '@GRPPOLICY@',\n  TimeoutPolicy: '1s',\n  dataFileOrganization: 'Text',\n  ProcessCopyBookFileAs: 'MultipleEvents',\n  skipIndent: 0,\n  dataFileFont: 'utf8',\n  CopybookFileFormat: 'USE_STANDARD_COLUMNS',\n  copybookSplit: 'None',\n  copybookFileName: '@CCBFILE@'\n   )\nOUTPUT TO CCBStream;\n\nCREATE OR REPLACE TARGET JSONWriter USING FileWriter (\n  filename: '%@metadata(FileName)%',\n  directory: '@DIR@',\n  rolloverpolicy: 'EventCount:10000,Interval:30s',\n  flushpolicy: 'EventCount:1,Interval:30s'\n  )\nFORMAT USING JSONFormatter  ()\nINPUT FROM CCBStream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@ on all in default;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop ADW;\nundeploy application ADW;\nDROP APPLICATION ADW CASCADE;\nCREATE APPLICATION ADW recovery 5 second interval;;\n\nCreate Source OracleSource1 Using OracleReader\n(\n Username:'@ORACLE-USERNAME',\n Password:'@ORACLE-PASSWORD',\n ConnectionURL: '@ORACLE-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'@FETCH-SIZE@'\n) \nOutput To str;\n\nCreate Source OracleSource2 Using OracleReader\n(\n Username:'@ORACLE-USERNAME',\n Password:'@ORACLE-PASSWORD',\n ConnectionURL: '@ORACLE-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'@FETCH-SIZE@'\n) \nOutput To str;\n\nCreate Source OracleSource3 Using OracleReader\n(\n Username:'@ORACLE-USERNAME',\n Password:'@ORACLE-PASSWORD',\n ConnectionURL: '@ORACLE-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'@FETCH-SIZE@'\n) \nOutput To str;\n\nCreate Source OracleSource4 Using OracleReader\n(\n Username:'@ORACLE-USERNAME',\n Password:'@ORACLE-PASSWORD',\n ConnectionURL: '@ORACLE-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'@FETCH-SIZE@'\n) \nOutput To str;\n\nCreate Source OracleSource5 Using OracleReader\n(\n Username:'@ORACLE-USERNAME',\n Password:'@ORACLE-PASSWORD',\n ConnectionURL: '@ORACLE-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'@FETCH-SIZE@'\n) \nOutput To str;\n\ncreate target AzureTarget using AzureSQLDWHWriter (\n\t\tConnectionURL: '@SQLDW-URL@',\n        username: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@',  \n        uploadpolicy:'@EVENT-COUNT@'\n) INPUT FROM str;\nEND APPLICATION ADW;\ndeploy application ADW;\nstart application ADW;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create or replace Target EH_TARGET using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_01',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_01_cg',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:256000',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter(\n)\ninput from EH_SS;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop APPLICATION @AppName@;\nUndeploy APPLICATION @AppName@;\ndrop APPLICATION @AppName@ cascade;\nCREATE OR REPLACE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW @AgentFlow@1;\nCREATE OR REPLACE SOURCE @SourceName@1 USING Global.MSJet (\n  Tables: 'dbo.compsrc',\n  username: 'qatest',\n  DatabaseName: 'qatest',\n  FetchTransactionMetadata: true,\n  filterTransactionBoundaries: true,\n  compression: false,\n  ConnectionURL: '@ConnectionURL@1',\n  Mode: '@mode@',\n  CommittedTransactions: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',\n  SendBeforeImage: true,\n  password: 'w3b@ct10n' )\nOUTPUT TO @StreamName@;\nEND FLOW @AgentFlow@1;\n\nCREATE FLOW @AgentFlow@2;\nCREATE OR REPLACE SOURCE @SourceName@2 USING Global.MSJet (\n  Tables: 'dbo.compsrc',\n  username: 'qatest',\n  DatabaseName: 'qatest',\n  FetchTransactionMetadata: true,\n  filterTransactionBoundaries: true,\n  compression: false,\n  ConnectionURL: '@ConnectionURL@2',\n  CaptureType: '@captureType@',\n  CommittedTransactions: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',\n  SendBeforeImage: true,\n  password: 'w3b@ct10n' )\nOUTPUT TO @StreamName@;\nEND FLOW @AgentFlow@2;\n\nCREATE TARGET @SysTarget@ USING Global.SysOut (\n  name: 'MS_CDC_SYSOUT' )\nINPUT FROM @StreamName@;\n\nCREATE FLOW @ServerFlow@1;\n\nCREATE TARGET @TargetName@1 USING Global.DatabaseWriter (\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  ParallelThreads: '',\n  CheckPointTable: 'CHKPOINT',\n  CDDLAction: 'Process',\n  CommitPolicy: 'EventCount:1000,Interval:60',\n  ConnectionURL: '@ConnectionURL@',\n  StatementCacheSize: '50',\n  DatabaseProviderType: 'Default',\n  Username: 'qatest',\n  Tables: 'dbo.compsrc,dbo.comptar',\n  Password: 'w3b@ct10n',\n  PreserveSourceTransactionBoundary: 'false',\n  BatchPolicy: 'EventCount:1,Interval:60' )\nINPUT FROM @StreamName@;\n\nEND FLOW @ServerFlow@1;\n\nEND APPLICATION @AppName@;\nDEPLOY APPLICATION @AppName@ with @AgentFlow@1 in AGENTS, @AgentFlow@2 in AGENTS, @ServerFlow@1 on any in default;\nSTART APPLICATION @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCreate Source @SOURCE@\n        Using OracleReader\n(\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: 'qatest.test77',\n  FetchSize:1,\n  QueueSize:2000,\n  CommittedTransactions:true,\n  Compression:false\n)\nOutput To @STREAM@;\n\ncreate Target @TARGET@ using GCSWriter(\n    bucketname:'@bucketname@',\n    objectname:'@objectname@',\n    projectId:'@project-id@',\n    uploadpolicy:'EventCount:7'\n    ServiceAccountKey:'@file-path@'\n)\nformat using DSVFormatter (\nmembers:'data'\n)\ninput from @STREAM@;\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 50 is a simple Kafka test\n-- Nicholas Keene WebAction, Inc.\n--\n-- S -> CQ -> KSu -> JWc10 -> WS\n--\n\nSTOP Recov50Tester.RecovTest50;\nUNDEPLOY APPLICATION Recov50Tester.RecovTest50;\nDROP APPLICATION Recov50Tester.RecovTest50 CASCADE;\nCREATE APPLICATION RecovTest50 RECOVERY 5 SECOND INTERVAL;\n\nCREATE or REPLACE TYPE KafkaType(\n  value java.lang.Long KEY  \n);\n\nCREATE SOURCE KafkaSource USING NumberSource ( \n  lowValue: '1',\n  highValue: '1003',\n  delayMillis: '10',\n  delayNanos: '0',\n  repeat: 'false'\n ) \nOUTPUT TO NumberSourceOut;\n\nCREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaStream OF KafkaType using KafkaProps;\n\nCREATE OR REPLACE CQ KafkaStreamPopulate \nINSERT INTO KafkaStream\nSELECT data[1]\nFROM NumberSourceOut;\n\nCREATE JUMPING WINDOW SizeTenWindow\nOVER KafkaStream KEEP 10 ROWS;\n\n\nCREATE WACTIONSTORE Wactions CONTEXT of KafkaType\n@PERSIST-TYPE@\n\nCREATE CQ WactionsPopulate\nINSERT INTO Wactions\nSELECT * FROM SizeTenWindow;\n\nEND APPLICATION RecovTest50;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop application Postgres_To_Filewriter;\nundeploy application Postgres_To_Filewriter;\ndrop application Postgres_To_Filewriter cascade;\n\nCREATE APPLICATION Postgres_To_Filewriter RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE Postgres_Src USING PostgreSQLReader  ( \n  ReplicationSlotName: '',\n  FilterTransactionBoundaries: 'true',\n  Username: '',\n  Password_encrypted: false,\n  ConnectionURL: '',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --startLSN:'758199500005967269518578814280000',\n  Password: '',\n  Tables: ''\n ) \nOUTPUT TO Change_Data_Stream;\n\n\nCREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM Change_Data_Stream;\n \n CREATE TARGET BinaryDump USING LogWriter(\n  name: 'PostgresCDCData',\n  filename:'PostgresCDCData.log'\n)INPUT FROM Change_Data_Stream;\n \n\nend application Postgres_To_Filewriter;\ndeploy application Postgres_To_Filewriter;\nstart Postgres_To_Filewriter;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE OR REPLACE SOURCE @SOURCE@ USING Ojet  (\n  FilterTransactionBoundaries: true,\n  ConnectionURL: '@OCI-URL@',\n  Tables: '@SOURCE_TABLE@',\n  Password: '@OJET-PASSWORD@',\n  fetchsize: 1,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  Username: '@OJET-UNAME@'\n )\nOUTPUT TO @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET@1 USING SnowflakeWriter (\n  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',\n  optimizedMerge: 'false',\n  password: 'Web@ct10n',\n  username: 'striim_regression2',\n  appendOnly: 'false',\n  uploadPolicy: 'eventcount:10000,interval:5m',\n  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET@2 USING SnowflakeWriter (\n  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',\n  optimizedMerge: 'false',\n  password: 'Web@ct10n',\n  username: 'striim_regression2',\n  appendOnly: 'true',\n  uploadPolicy: 'eventcount:10000,interval:5m',\n  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET@3 USING SnowflakeWriter (\n  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',\n  optimizedMerge: 'true',\n  password: 'Web@ct10n',\n  username: 'striim_regression2',\n  appendOnly: 'false',\n  uploadPolicy: 'eventcount:10000,interval:5m',\n  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',\n )\nINPUT FROM @STREAM@;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP OneAgentTester.CSV;\nUNDEPLOY APPLICATION OneAgentTester.CSV;\nDROP APPLICATION OneAgentTester.CSV CASCADE;\nCONNECT ADMIN abc;\nDROP USER OneAgentTester;\nUSE ADMIN;\nDROP NAMESPACE OneAgentTester CASCADE;\ndrop dg AGENTS;\ndrop dg LocalServer;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader\n(\nFetchSize:'@FETCHSIZE@',\nQueueSize:'@QUEUESIZE@',\nUsername:'@USERNAME@',\nPassword:'@PASSWORD@',\nConnectionURL:'@URL@',\n_h_useClassic: @CLASSICMODE@,\nTables:'@TABLES@',\nCommittedTransactions:'@COMMITEDT@'\nCompression:'@COMPRESSION@'\n)\nOUTPUT TO @STREAM@;\n\nCREATE SOURCE @SOURCE_NAME@2 USING Global.OracleReader\n(\nFetchSize:'@FETCHSIZE@',\nQueueSize:'@QUEUESIZE@',\nUsername:'@USERNAME@',\nPassword:'@PASSWORD@',\nConnectionURL:'@URL@',\n_h_useClassic: @CLASSICMODE@,\nTables:'@TABLES@',\nCommittedTransactions:'@COMMITEDT@'\nCompression:'@COMPRESSION@'\n)\nOUTPUT TO @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION DGLimitServerApp1;\nUNDEPLOY APPLICATION DGLimitServerApp1;\nDROP APPLICATION DGLimitServerApp1 CASCADE;\nCREATE APPLICATION DGLimitServerApp1;\nCREATE FLOW DGLimitServerAgentFlow;\nCREATE OR REPLACE SOURCE DGLimitServerApp1_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO DGLimitServerApp1_SampleStream;\nEND FLOW DGLimitServerAgentFlow;\nCREATE FLOW DGLimitServerServerFlow;\nCREATE OR REPLACE TARGET DGLimitServerApp1_NullTarget using NullWriter()\nINPUT FROM DGLimitServerApp1_SampleStream;\nEND FLOW DGLimitServerServerFlow;\nEND APPLICATION DGLimitServerApp1;\ndeploy application DGLimitServerApp1 on any in ServerDG1 with DGLimitServerAgentFlow on any in Agents, DGLimitServerServerFlow on any in ServerDG1;\nSTART APPLICATION DGLimitServerApp1;\n\nSTOP APPLICATION DGLimitServerApp2;\nUNDEPLOY APPLICATION DGLimitServerApp2;\nDROP APPLICATION DGLimitServerApp2 CASCADE;\nCREATE APPLICATION DGLimitServerApp2;\nCREATE FLOW DGLimitServerAgentFlow2;\nCREATE OR REPLACE SOURCE DGLimitServerApp2_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO DGLimitServerApp2_SampleStream;\nEND FLOW DGLimitServerAgentFlow2;\nCREATE FLOW DGLimitServerServerFlow2;\nCREATE OR REPLACE TARGET DGLimitServerApp2_NullTarget using NullWriter()\nINPUT FROM DGLimitServerApp2_SampleStream;\nEND FLOW DGLimitServerServerFlow2;\nEND APPLICATION DGLimitServerApp2;\ndeploy application DGLimitServerApp2 on any in ServerDG1 with DGLimitServerAgentFlow2 on any in Agents, DGLimitServerServerFlow2 on any in ServerDG1;\nSTART APPLICATION DGLimitServerApp2;\n\nSTOP APPLICATION ExceedApp1;\nUNDEPLOY APPLICATION ExceedApp1;\nDROP APPLICATION ExceedApp1 CASCADE;\nCREATE APPLICATION ExceedApp1;\nCREATE FLOW ExceedAgentFlow;\nCREATE OR REPLACE SOURCE ExceedApp1_FileSource USING FileReader (\ndirectory:'@dirPath@',\nwildcard:'posdata.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO ExceedApp_SampleStream;\nEND FLOW ExceedAgentFlow;\nCREATE FLOW ExceedServerFlow;\nCREATE OR REPLACE TARGET ExceedApp1_NullTarget using NullWriter()\nINPUT FROM ExceedApp_SampleStream;\nEND FLOW ExceedServerFlow;\nEND APPLICATION ExceedApp1;\ndeploy application ExceedApp1 on any in ServerDG1 with ExceedAgentFlow on any in Agents, ExceedServerFlow on any in ServerDG1;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\n-----------------------------------\n\nstop application TargetServerApp3;\nundeploy application TargetServerApp3;\ndrop application TargetServerApp3 cascade;\n\n\n-- another app consuming from app running in agent\n\nCREATE APPLICATION TargetServerApp3;\ncreate flow flow4;\n\nCREATE TARGET T4 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp3_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')\n FORMAT USING DSVFormatter ()\n INPUT FROM CsvStream;\nend flow flow4;\n\nEND APPLICATION TargetServerApp3;\ndeploy application TargetServerApp3 with flow4 in default;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@ RECOVERY 1 SECOND INTERVAL;\nCREATE SOURCE @srcName@ USING OracleReader (\n  Username: '@srcusername@',\n  Password: '@srcpassword@',\n  ConnectionURL: '@srcurl@',\n  Tables: '@srcschema@.@srctable@'\n)\nOUTPUT TO @outstreamname@;\n\nCREATE TARGET @tgtName@ USING Global.SpannerWriter ( \n   Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',\n  InstanceID: '@instanceId@', \n  ServiceAccountKey: '@keyFileName@', \n  CheckpointTable: 'CHKPOINT', \n  ProjectId: '@projectId@', \n  PrivateServiceConnectEndpoint: '@privatelinkname@', \n  BatchPolicy: 'EventCount: 1, Interval: 60s') \nINPUT FROM @instreamname@;\n\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application iteratortester.iteratorapp;\nundeploy application iteratortester.iteratorapp;\ndrop application iteratortester.iteratorapp cascade;\n\nCREATE APPLICATION iteratorapp;\n\ncreate flow sourceFlow;\n\nCREATE SOURCE JSONAccessLogSource USING FileReader(\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'iterator.json',\n  positionByEOF:'false'\n)\nparse using JSONParser (\n) OUTPUT TO jsonSourceStream;\n\nend flow sourceflow;\n\n-- ******ARRAY LIST****** --\ncreate flow processFlow;\n\ncreate type cacheType (bankID string key, bankName string);\nCREATE cache dsvcache USING FileReader (\ndirectory:'@TEST-DATA-PATH@',\nwildcard:'banks.csv',\nblocksize: 10240,\npositionByEOF:false\n)\nPARSE USING DSVParser (\nheader:No,\ntrimquote:false\n) QUERY (keytomap:'bankID') OF cacheType;\n\nCREATE TYPE listType (id integer KEY, bankname string, lst java.util.List);\nCREATE TYPE listStoreType (id integer KEY, bankname string, lst java.util.List, lstoflst java.util.List);\n\nCREATE STREAM listStream of listType partition by bankname;\n\nCREATE JUMPING WINDOW listJWindow\nOVER listStream\nkeep 3 rows;\n\nCREATE WINDOW listWindow\nOVER listStream\nkeep 3 rows;\n\nCREATE WACTIONSTORE listStore CONTEXT OF listStoreType EVENT TYPES (listStoreType ) \n@PERSIST-TYPE@\n\ncreate cq updatelistStream\ninsert into listStream\nselect TO_INT(bankID), bankName, makelist(bankID,bankName) as lst \nfrom dsvcache;\n\ncreate cq updatelistStore \ninsert into listStore\nselect ID, bankName, lst, makelist(lst,lst) from listStream\nLINK SOURCE EVENT;\n\ncreate stream listTargetStream( str String);\n\ncreate cq updateListTarget\ninsert into listTargetStream\nselect itr\nfrom listStream, iterator(listStream.lst) itr order by cast(itr as java.lang.Comparable);\n\n-- CREATE TARGET listout USING SYSOUT(name:\"list\") input from listStream;\n\n-- ******JsonNode****** --\n\n--CREATE TYPE jsonType (id integer KEY,  lst com.fasterxml.jackson.databind.JsonNode);\nCREATE TYPE jsonType (int integer, bankname string, lst com.fasterxml.jackson.databind.JsonNode key);\n\nCREATE STREAM jsonStream of jsonType partition by bankname;\n\nCREATE JUMPING WINDOW jsonJWindow\nOVER jsonStream\nkeep 3 rows\npartition by bankname;\n\nCREATE WINDOW jsonWindow\nOVER jsonStream\nkeep 3 rows;\n\nCREATE WACTIONSTORE jsonStore CONTEXT OF jsonType EVENT TYPES (jsonType ) \n@PERSIST-TYPE@\n\ncreate cq updateJsonStream\ninsert into jsonStream\n--select TO_INT(MATCH(data[0], '.*\\\\\\\\s([0-9]+)')) , makeJSON('[{\"x\":\"a\"},{\"x\":\"b\",\"y\":\"c\"},{\"y\":\"c\"}]') as lst \nselect TO_INT(y.bankID), y.bankName, x.data \nfrom JSONSourceStream x, dsvcache y;\n\ncreate cq updateJsonStore \ninsert into jsonStore\nselect * from jsonStream\nLINK SOURCE EVENT;\n\ncreate stream jsonTargetStream( str String);\n\ncreate cq updateJsonTarget\ninsert into jsonTargetStream\nselect to_string(itr.StringJSON)\nfrom jsonStream, iterator(jsonStream.lst) itr order by to_string(itr.StringJSON);\nCREATE TARGET jsonout USING SYSOUT(name:\"jlist\") input from jsonStream;\n\nend flow processflow; \n\nend application iteratorapp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create or replace type @STREAM@orderBill(\nid int,\nname String,\ncost float,\nTableName string,\noperationName String\n);\n\ncreate or replace stream @STREAM@_TYPED of @STREAM@OrderBill;\n\nCreate or replace CQ @STREAM@orderbillCQ\ninsert into @STREAM@_TYPED\nselect \nto_int(data[0]),data[1],to_float(data[2]),\nmeta(@STREAM@,'TableName'),Meta(@STREAM@,'OperationName') from @STREAM@;\n\n\ncreate or replace Target @TARGET_NAME@ using FileWriter(\n  filename:'CompressedMerchant.gz',\n  directory:'/logs/',\n  rolloverpolicy:'EventCount:10000,Interval:30s'\n)\nformat using ParquetFormatter (\n\tschemaFileName:'@FILENAME@',\n\tFormatAs:'@FORMATAS@'\n)\ninput from @STREAM@_TYPED;\t"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\n-------------------------\n-- The tql checks the auto conversion feature for multiple tables.\n-- It covers all the data types supported by sqlmp (except \"interval\").\n-------------------------\n\nIMPORT static com.webaction.runtime.converters.DateConverter.*;\n\nUNDEPLOY APPLICATION admin.SQLMPReaderApp;\nDROP APPLICATION admin.SQLMPReaderApp cascade;\n\nCREATE APPLICATION SQLMPReaderApp;\ncreate source SQLMPSource using HPNonStopSQLMPReader (\n    portno:2020,\n\tipaddress:'10.10.196.103',\n\tName:'intg',\n\tAuditTrails:'parallel',\n\tAgentPortNo:8012,\n\tAgentIpAddress:'10.10.197.150', \n    Tables:'$DATA06.MAHA.ESA;$DATA06.MAHA.ESB;$DATA06.MAHA.ESC'\n) OUTPUT TO CDCStream,\nESAStream MAP (table:'\\\\RPC4.$DATA06.MAHA.ESA'),\nESBStream MAP (table:'\\\\RPC4.$DATA06.MAHA.ESB');\n\n\nCREATE TYPE ESCStreamType(\nC0 Short,\nC1 Long,\nC2 Long,\nC3 String,\nC4 String,\nC5 Integer,\nC6 Long,\nC7 String,\nC8 String,\nC9 Double,\nC10 Double,\nOPR String,\nTABLENAME String\n);\n\nCREATE STREAM ESCStream OF ESCStreamType;\n\n\nCREATE JUMPING WINDOW SQLMPDataWindow\nOVER ESCStream KEEP 5 ROWS\nPARTITION BY OPR;\n\nCREATE CQ ESCStreamCq\nINSERT INTO ESCStream\nSELECT TO_SHORT(data[0]),\n    TO_LONG(data[1]),\n    TO_LONG(data[2]),\n    data[3],\n    data[4],\n    TO_INT(data[5]),\n       TO_LONG(data[6]),\n    data[7],\n    data[8],\n    TO_DOUBLE(data[9]),\n    TO_DOUBLE(data[10]),\n    META(x,\"OperationName\").toString(),\n    META(x, \"TableName\").toString()\nFROM CDCStream x\nWHERE not(META(x,\"OperationName\").toString() = \"BEGIN\") AND not(META(x,\"OperationName\").toString() = \"COMMIT\") AND not(META(x, \"TableName\").toString() is null) AND META(x, \"TableName\").toString() = \"\\\\\\\\RPC4.$DATA06.MAHA.ESC\";\n\n\nCREATE TYPE SQLMPOperationData(\n    TableName String,\n    OperationName String,\n    Count Integer\n);\n\nCREATE STREAM SQLMPOperationDataStream OF SQLMPOperationData;\n\nCREATE CQ SQLMPOperationCheck\nINSERT INTO SQLMPOperationDataStream\nSELECT x.TABLENAME,\nCASE WHEN x.OPR = 'INSERT' THEN x.OPR\n     WHEN x.OPR = 'DELETE' THEN x.OPR\n     WHEN x.OPR = 'UPDATE' THEN x.OPR\n     ELSE 'UNSUPPORTED OPREATION' END,\nCASE WHEN x.OPR = 'INSERT' THEN COUNT(x.OPR)\n     WHEN x.OPR = 'DELETE' THEN COUNT(x.OPR)\n     WHEN x.OPR = 'UPDATE' THEN COUNT(x.OPR)\n     ELSE 0 END\nFROM SQLMPDataWindow x\nGROUP BY OPR;\n\nCREATE TARGET Log USING LogWriter(\n  name:SQLMPReaderAppESA,\n-- filename:'@FEATURE-DIR@/logs/SQLMPReaderAppESA.log'\n  filename:'mp.log'\n) INPUT FROM ESAStream;\n\n\nCREATE TARGET Log1 USING LogWriter(\n  name:SQLMPReaderAppESB,\n--  filename:'@FEATURE-DIR@/logs/SQLMPReaderAppESB.log'\n  filename:'mp1.log'\n) INPUT FROM ESBStream;\n\n\nCREATE TARGET OperationLog USING LogWriter(\n  name:SQLMPReaderAppESC,\n--  filename:'@FEATURE-DIR@/logs/SQLMPReaderOperationCheck.log'\n  filename:'mp2.log'\n) INPUT FROM SQLMPOperationDataStream;\n\nEND APPLICATION SQLMPReaderApp;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @AppName@;\n\nCREATE SOURCE @AppName@_FileReaderSource USING FileReader ( \n  wildcard: 'posdata.csv', \n  positionByEOF: false, \n  blocksize: 10240, \n  directory: 'Samples/PosApp/appData' ) \nPARSE USING DSVParser ( \n  trimquote: false, \n  header: 'Yes' ) \nOUTPUT TO @AppName@_CsvStream;\n\nCREATE TYPE n1 (\n c1 java.lang.Integer KEY,\n _plan java.lang.String AS \"plan\");\n\nCREATE OR REPLACE TARGET @AppName@_NullWriterTrg USING NullWriter ( \n ) \nINPUT FROM @AppName@_CsvStream;\n\nEND APPLICATION @AppName@;\nDEPLOY APPLICATION @AppName@;\nSTART APPLICATION @AppName@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCreate Source @SourceName@\n Using OracleReader\n(\n Username:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@SourceConnectURL@',\n Tables:'@SourceTable@',\n _h_scnRange: 1000,\n _h_eoffDelay: 10,\n SupportPDB: false,\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop Oracle_IRLogWriter;\nundeploy application Oracle_IRLogWriter;\ndrop application Oracle_IRLogWriter cascade;\nCREATE APPLICATION Oracle_IRLogWriter;\n\nCreate Source s1 Using IncrementalBatchReader (\n FetchSize: 1,\n  Username: 'striim',\n  Password: 'o4l1uMpwIDQ=',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',\n  Tables: 'striim.autotest01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.autotest01=id',\n  startPosition: '%=0'\n )\nOUTPUT TO data_stream1;\n\ncreate source s2 using IncrementalBatchReader (\nFetchSize: 1,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',\n  Tables: 'striim.autotest02',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.autotest02=id',\n  startPosition: '%=0'\n )\nOUTPUT TO data_stream2;\n\ncreate source s3 using IncrementalBatchReader (\nFetchSize: 1,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',\n  Tables: 'striim.autotest03',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.autotest03=id',\n  startPosition: '%=0'\n )\nOUTPUT TO data_stream3;\n\nCreate Type EventType (\nID int,\nNAME string,\nCOMPANY string,\nCOUNTRY string\n);\n\nCREATE STREAM insertData1  of EventType;\nCREATE STREAM deleteData1 of EventType;\nCREATE STREAM joinData1 of EventType;\nCREATE STREAM joinData2 of EventType;\nCREATE STREAM deleteData2 of EventType;\nCREATE STREAM OutStream of EventType;\n\nCREATE CQ cq1 INSERT INTO insertData1  SELECT TO_INT(data[0]),data[1],data[2],data[3] FROM data_stream1;\n\nCREATE CQ cq2 INSERT INTO deleteData1 SELECT TO_INT(data[0]),data[1],data[2],data[3] FROM data_stream2;\n\nCREATE CQ cq3 INSERT INTO joinData1 SELECT TO_INT(data[0]),data[1],data[2],data[3] FROM data_stream3;\n\nCREATE JUMPING WINDOW DataWin1 OVER deleteData1 KEEP 1 ROWS;\n\nCREATE CQ TypedEventKeyCQ6 INSERT INTO deleteData2 SELECT * from DataWin1;\n\nCREATE JUMPING WINDOW DataWin2 OVER joinData1 KEEP 1 ROWS;\n\nCREATE CQ TypedEventKeyCQ5 INSERT INTO joinData2 SELECT * from DataWin2;\n\nCREATE EVENTTABLE ETABLE1 using STREAM ( NAME: 'insertData1 ' )\n--DELETE using STREAM ( NAME: 'deleteData1')\nQUERY (keytomap:\"ID\", persistPolicy: 'true') OF EventType;\n\nCREATE CQ cq4 INSERT INTO OutStream SELECT B.ID,B.NAME,B.COMPANY,B.COUNTRY FROM joinData2 A, ETABLE1 B where A.ID=B.ID;\n\nCREATE TARGET EventTableFW USING FileWriter\n(filename:'BasicOracle_IRLogWriter_RT.log',\n rolloverpolicy: 'EventCount:1000000')\nFORMAT USING DSVFormatter () INPUT FROM OutStream;\n\ncreate target Target_Azure using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'STRIIM',\n        password: 'W3b@ct10n',\n        AccountName: 'striimqatestdonotdelete',\n        accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables:'dbo.autotest01',\n        uploadpolicy:'eventcount:0,interval:0s'\n) INPUT FROM OutStream;\n\nEND APPLICATION Oracle_IRLogWriter;\ndeploy application Oracle_IRLogWriter in default;\nstart Oracle_IRLogWriter;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application app2PS;\nundeploy application app2PS;\ndrop application app2PS cascade;\n\ncreate application app2PS;\n\ncreate target File_TargerPS2 using FileWriter\n(\ndirectory : '',\nfilename : ''\n)\nformat using DSVFormatter()\ninput from KPSRss2;\n\nend application app2PS;\n\ndeploy application app2PS;\nstart application app2PS;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@;\nCREATE OR REPLACE SOURCE @APP_NAME@_src1 USING Global.OracleReader (\n  FetchSize: 1,\n  ConnectionURL: '@ORACLE-URL@',\n  Tables: '@SOURCE-TABLES@',\n  Username: '@ORACLE-USERNAME@',\n  Password: '@ORACLE-PASSWORD@'\n) OUTPUT TO @APP_NAME@_Stream;\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt1 USING SnowflakeWriter\n\n(\n  ConnectionURL:'@SNOWFLAKE-URL@',\n  username:'@SNOWFLAKE-USERNAME@',\n  appendOnly:'false',\n  Tables:'@TARGET-TABLES@',\n  uploadpolicy:'eventcount:3,interval:10s',\n  externalStageType:'local'\n)\nINPUT FROM @APP_NAME@_Stream;\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt2 USING SnowflakeWriter\n\n(\n  ConnectionURL:'@SNOWFLAKE-URL@',\n  username:'@SNOWFLAKE-USERNAME@',\n  appendOnly:'false',\n  Tables:'@TARGET-TABLES@',\n  uploadpolicy:'eventcount:3,interval:10s',\n  externalStageType:'local'\n)\nINPUT FROM @APP_NAME@_Stream;\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt3 USING SnowflakeWriter\n\n(\n  ConnectionURL:'@SNOWFLAKE-URL@',\n  username:'@SNOWFLAKE-USERNAME@',\n  appendOnly:'false',\n  Tables:'@TARGET-TABLES@',\n  uploadpolicy:'eventcount:3,interval:10s',\n  externalStageType:'local'\n)\nINPUT FROM @APP_NAME@_Stream;\n\n\nEND APPLICATION @APP_NAME@;\nDEPLOY APPLICATION @APP_NAME@;\nSTART APPLICATION @APP_NAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION snow2FW;\nUNDEPLOY APPLICATION snow2FW;\nDROP APPLICATION snow2FW CASCADE;\nCREATE OR REPLACE APPLICATION snow2FW;\n\nCREATE OR REPLACE SOURCE snow_fw USING Global.ServiceNowReader (\n  Mode: 'InitialLoad',\n  ServiceNow.ConnectionTimeOut: 60,\n  ServiceNow.MaxConnections: 20,\n  ServiceNow.FetchSize: 10000,\n  ThreadPoolCount: '10',\n  ServiceNow.ConnectionRetries: 3,\n  PollingInterval: '1',\n  ClientSecret: '6Wa-cv`I7x',\n  Password: '^Pre&$EMO%6O.e_{96h+$R?rJd,=[4Vt=K)Szh?6g<J9D3,3zs8R;hpZqh]-3?C&.u-@GvSakPXH1:2eygbBDI>ou-z#GjBw[u8x',\n  ServiceNow.Tables: 'u_empl',\n  UserName: 'snr',\n  ClientID: 'ce4fd5af894a11103d2c5c3a8fe075e1',\n  adapterName: 'ServiceNowReader',\n  ServiceNow.BatchAPI: true,\n  ServiceNow.ConnectionUrl: 'https://dev84954.service-now.com/' )\nOUTPUT TO sn;\n\n\nCREATE TARGET ft USING Global.FileWriter (\n  DataEncryptionKeyPassphrase: '',\n  flushpolicy: 'EventCount:10000,Interval:30s',\n  rolloveronddl: 'true',\n  encryptionpolicy: '',\n  DataEncryptionKeyPassphrase_encrypted: 'true',\n  rolloverpolicy: 'EventCount:10000,Interval:30s',\n  filename: 'dt' )\nFORMAT USING Global.JSONFormatter  (\n  jsonMemberDelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true',\n  jsonobjectdelimiter: '\\n' )\nINPUT FROM sn;\n\nCREATE TARGET pg_target USING Global.DatabaseWriter (\n\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n    Password: 'w@ct10n',\n    Tables: 'u_empl,u_empl ColumnMap(name=u_name,age=u_age,address=u_address,sys_id=sys_id)',\n    ParallelThreads: '',\n    CheckPointTable: 'CHKPOINT',\n    CDDLAction: 'Process',\n    ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n    CommitPolicy: 'EventCount:1000,Interval:60',\n    StatementCacheSize: '50',\n    Username: 'waction',\n    DatabaseProviderType: 'Postgres',\n    BatchPolicy: 'EventCount:1000,Interval:60',\n    PreserveSourceTransactionBoundary: 'false' )\n  INPUT FROM sn;\n\nEND APPLICATION snow2FW;\ndeploy application snow2FW;\nstart snow2FW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use PosTester;\nalter application PosApp;\n\nCREATE CACHE HourlyAveLookup using CSVReader (\n  directory: 'Samples/Customer/PosApp/appData',\n  wildcard: 'hourlyData.txt',\n  header: Yes,\n  columndelimiter: ',',\n  trimquote:false\n) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;\n\nend application PosApp;\n\nalter application PosApp recompile;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src1 USING Global.GCSReader ()\nPARSE USING Global.DSVParser ()\nOUTPUT TO @APPNAME@_Stream1;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer ()\nFORMAT USING DSVFormatter (\nmembers:'data')\nINPUT FROM @APPNAME@_Stream1;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src2 USING Global.GCSReader ()\nPARSE USING Global.JSONParser ()\nOUTPUT TO @APPNAME@_Stream2;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer ()\nFORMAT USING JSONFormatter ()\nINPUT FROM @APPNAME@_Stream2;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src3 USING GCSReader ()\nPARSE USING AvroParser ()\nOUTPUT TO @APPNAME@_Stream3;\n\nCREATE CQ @APPNAME@_CQ3\nINSERT INTO @APPNAME@_CQOut3\nSELECT AvroToJson(data,false) FROM @APPNAME@_Stream3;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer ()\nFORMAT USING JSONFormatter ()\nINPUT FROM @APPNAME@_CQOut3;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src4 USING Global.GCSReader ()\nPARSE USING Global.XMLParser ()\nOUTPUT TO @APPNAME@_Stream4;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt4 USING S3Writer ()\nFORMAT USING JSONFormatter ()\nINPUT FROM @APPNAME@_Stream4;\n\nEND APPLICATION @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace hubspot cascade force;\ncreate namespace hubspot;\nuse hubspot;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@ RECOVERY 30 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;\n\nCREATE FLOW @AppName@_SourceFlow;\n\nCREATE SOURCE @srcName@ USING Global.HubSpotReader ( \n  PollingInterval: '1m', \n  StartPosition: '%=-1', \n  Tables: '@objectname@s__c', \n  ThreadPoolCount: '10', \n  ConnectionPoolSize: '20', \n  ClientSecret_encrypted: 'false', \n  RefreshToken: '', \n  RefreshToken_encrypted: 'false', \n  Mode: 'automated', \n  useConnectionProfile: false,\n  AuthMode: 'PrivateAppToken', \n  PrivateAppToken: '@accesstoken@', \n  ClientSecret: '', \n  ClientId: '', \n  PrivateAppToken_encrypted: 'false', \n  MigrateSchema: true ) \nOUTPUT TO @outstreamname@;\n\nEND FLOW @AppName@_SourceFlow;\n\nCREATE TARGET @tgtName@ USING Global.BigQueryWriter ( \n  projectId: '@projectId@',\n  batchPolicy: 'eventcount:10000,interval:2', \n  streamingUpload: 'true', \n  Mode: 'MERGE', \n  CDDLAction : 'process',\n  CDDLOptions: '{\\\"CreateTable\\\":{\\\"action\\\":\\\"IgnoreIfExists\\\",\\\"options\\\":[{\\\"CreateSchema\\\":{\\\"action\\\":\\\"IgnoreIfExists\\\"}}]}}', \n  ServiceAccountKey: '@keyFileName@', \n  Tables: '%,@tgtschema@.%' ) \nINPUT FROM @instreamname@;\n\nEnd APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application MSSQLTransactionSupportMultiReaderWriter;\nundeploy application MSSQLTransactionSupportMultiReaderWriter;\ndrop application MSSQLTransactionSupportMultiReaderWriter cascade;\n\nCREATE APPLICATION MSSQLTransactionSupportMultiReaderWriter recovery 1 second interval;\n\nCreate Source ReadFromMSSQL4\nUsing MSSqlReader\n(\nUsername:'@READER-NAME@',\nPassword:'@READER-PASSWORD@',\nDatabaseName:'@SRCDB-NAME@',\nConnectionURL:'@CONN-URL@',\nTables:'@WATABLES-SRC@',\nTransactionSupport: 'true',\n--AutoDisableTableCDC:'true',\nFetchTransactionMetadata:'true',\nFilterTransactionBoundaries: false,\nCompression:'false',\nConnectionPoolSize:1\n)\nOutput To MSSQLTransactionSupportMultiReaderWriterStream1;\n\nCreate Source ReadFromMSSQL5\nUsing MSSqlReader\n(\nUsername:'@READER-NAME@',\nPassword:'@READER-PASSWORD@',\nDatabaseName:'@SRCDB-NAME@',\nConnectionURL:'@CONN-URL@',\nTables:'@WATABLES-SRC@',\nTransactionSupport: 'true',\n--AutoDisableTableCDC:'true',\nFetchTransactionMetadata:'true',\nFilterTransactionBoundaries: false,\nCompression:'true',\nConnectionPoolSize:1\n)\nOutput To MSSQLTransactionSupportMultiReaderWriterStream2;\n\nCREATE TARGET WriteToMSSQL4 USING DatabaseWriter(\nConnectionURL:'@TGT-URL@',\nUsername:'@WRITER-UNAME@',\nPassword:'@WRITER-PASSWORD@',\nBatchPolicy:'EventCount:5,Interval:5',\nCommitPolicy:'EventCount:1,Interval:1',\nTables: '@WATABLES-SRC,@@WATABLES-TGT@'\n)\nINPUT FROM MSSQLTransactionSupportMultiReaderWriterStream1;\n\nCREATE TARGET WriteToMSSQL5 USING DatabaseWriter(\nConnectionURL:'@TGT-URL@',\nUsername:'@WRITER-UNAME@',\nPassword:'@WRITER-PASSWORD@',\nBatchPolicy:'EventCount:5,Interval:5',\nCommitPolicy:'EventCount:1,Interval:1',\nTables: '@WATABLES-SRC,@@WATABLES-TGT@'\n)\nINPUT FROM MSSQLTransactionSupportMultiReaderWriterStream2;\n\n/*CREATE TARGET MSSqlReaderOutput4 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportStream; \n\n\n/*CREATE OR REPLACE TARGET MSSQLFileOut4 USING FileWriter  ( \n  rolloverpolicy: 'EventCount:10000,Interval:30s',\n  adapterName: 'FileWriter',\n  directory:'@FEATURE-DIR@/logs/',\n  filename: 'TransactionSupportAutoDisableTableCdcTrue.txt'\n ) \nFORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true',\n  jsonobjectdelimiter: '\\n'\n ) \nINPUT FROM MSSQLTransactionSupportMultiReaderWriterStream;\n*/\nEND APPLICATION MSSQLTransactionSupportMultiReaderWriter;\ndeploy application MSSQLTransactionSupportMultiReaderWriter;\nstart application MSSQLTransactionSupportMultiReaderWriter;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION TCPReader.TCPAPP;\nUNDEPLOY APPLICATION TCPReader.TCPAPP;\nDROP APPLICATION TCPReader.TCPAPP cascade;\n\nCREATE APPLICATION TCPAPP;\n\n\nCREATE SOURCE Tsource USING TCPReader (\n@CHARSET@,\nIpAddress:'@TCPREADERIPADDR@',\nPortNo:'@TCPREADERPORT@'\n)\nPARSE USING @TSOURCEFORMATTERTYPE@ (\n@TSOURCEFORMATTERMEMBERS@\n)\nOUTPUT TO TCPStream;\n\n\nEND APPLICATION TCPAPP;\ndeploy application TCPAPP in default;\nstart application TCPAPP;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop application DEV21349.app1;\nUndeploy application DEV21349.app1;\nDrop application DEV21349.app1 cascade;\nDrop namespace DEV21349 cascade;\nCreate namespace DEV21349;\nUse DEV21349;\n\nCREATE APPLICATION app1;\n\nCREATE CQ cq1 INSERT INTO cq1_outputstream select * from global.exceptionsstream exStream;\n\nCREATE WACTIONSTORE exceptionsWS CONTEXT OF cq1_outputstream_Type @PERSIST-TYPE@;\n\nCREATE CQ cq2 INSERT INTO exceptionsWS SELECT * FROM cq1_outputstream c;\n\nCREATE OR REPLACE CQ readingWS_cq INSERT INTO outputstream2 select * from exceptionsWS;\n\nCREATE OR REPLACE TARGET ExTarget USING Global.FileWriter  (\nDataEncryptionKeyPassphrase: '',\n  filename: 'ExceptionsOutPutFile',\n  flushpolicy: 'EventCount:10000,Interval:30s',\n  rolloveronddl: 'true',\n  encryptionpolicy: '',\n  directory: '@TEST-DATA-PATH@',\n  rolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING Global.JSONFormatter  ( jsonMemberDelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true',\n  jsonobjectdelimiter: '\\n' )\nINPUT FROM outputstream2;\n\nEND APPLICATION app1;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @appname@;\nundeploy application @appname@;\nDROP APPLICATION @appname@ CASCADE;\nCREATE APPLICATION @appname@;\n\nCREATE SOURCE @appname@_src USING databaseReader  (\n  Username: '@@',\n  Password: '@@',\n  ConnectionURL: '@@',\n  Tables: '@@',\n  FetchSize: '100'\n )\nOUTPUT TO @appname@_ss;\n\nCREATE JUMPING WINDOW @appname@_win OVER @appname@_ss KEEP @winsize@ ROWS;\n\nCREATE TYPE @appname@_MapType\n    (   \n       id INTEGER,\n        name STRING,\n        city  STRING\n    );\n    \nCREATE EXTERNAL CACHE @appname@_cach (\n  AdapterName: 'DatabaseReader',\n    ConnectionURL: '@url@',\n    UserName: '@uname@',\n    Password: '@pwd@',\n   Table: '@tablename@',\n  FetchSize: 100,\n  skipinvalid : @valid@,\n  Columns: 'id,name,city',\n  trimquote: false,\n  KeyToMap: '@key@'\n )\n OF @appname@_MapType;\n \nCREATE TYPE @appname@_MapTypenew\n    (   id_t            INTEGER,\n        name_t           STRING,\n        city_t            STRING,\n        id_c            INTEGER,\n        name_c            STRING,\n        city_c            STRING\n    );\n    \nCREATE STREAM @appname@_JoinedData OF @appname@_MapTypenew;\n\nCREATE CQ @appname@_JoinDataCQ\nINSERT INTO @appname@_JoinedData\nSELECT  TO_INT(f.data[0]),\n        TO_STRING(f.data[1]),\n        TO_STRING(f.data[2]),\n        z.id,\n        z.name,\n        z.city\nFROM @appname@_win f, @appname@_cach z\nwhere TO_INT(f.data[0]) = z.id\n@Ex@;\n\nCREATE TARGET @appname@_tgt USING DatabaseWriter\n(\n  ConnectionURL:'@@',\n  Username:'@@',\n  Password:'@@',\n  BatchPolicy:'Eventcount:10000,Interval:1',\n  CommitPolicy:'Interval:1,Eventcount:10000',\n  Tables:'@@'\n) \nINPUT FROM @appname@_JoinedData;\n\nEND APPLICATION @appname@;\ndeploy application @appname@;\nstart @appname@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION BankDataApp;\n\nCREATE TYPE MoreBankData (\n  bankID java.lang.Integer KEY,\n  bankName java.lang.String,\n  bankRouting java.lang.Long,\n  bankAmount java.lang.Double\n);\n\nCREATE STREAM BankDataStream OF MoreBankData;\n\nCREATE TARGET SysOut USING Global.SysOut ( \n  name: 'SysOut'\n) \nINPUT FROM BankDataStream;\n\nEND APPLICATION BankDataApp;\n\nDEPLOY APPLICATION BankDataApp;\nSTART BankDataApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE OR REPLACE APPLICATION @AppName@;\nCREATE SOURCE @srcName@ USING Global.OracleReader ( \n  Username: '@srcusername@',\n  Password: '@srcpassword@',\n  ConnectionURL: '@srcurl@',\n  Tables: '@srcschema@.@srctable@') \nOUTPUT TO @instreamname@;\n\nCREATE OR REPLACE TARGET @tgtName@ USING Global.SnowflakeWriter ( \n  connectionUrl: '@tgturl@', \n  tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@', \n  password: '@tgtpassword@',   \n  username: '@tgtusername@', \n  uploadPolicy: 'eventcount:1,interval:5m', \n  authenticationType: 'Password',\n  externalStageType: 'Local', \n  adapterName: 'SnowflakeWriter' ) \nINPUT FROM @outstreamname@;\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE  EMBEDDINGGENERATOR @EMB_NAME@ USING @MODEL@ (\n'modelProvider': '@MODEL@',\n'modelName': '@MODEL_NAME@',\n'project': '@PROJECT@',\n'location': '@LOCATION@',\n'publisher': '@PUBLISHER@',\n'serviceAccountKey': '@SERVICE_ACCOUNT_KEY@'\n);\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\n\ncreate application @appname@ recovery 5 second interval;\n\nCREATE OR REPLACE SOURCE @cobolsrc@ USING FileReader (\n  wildcard: '',\n  positionbyeof: false,\n  directory: ''\n  )\nPARSE USING Global.CobolCopybookParser (\n  copybookDialect: 'Mainframe',\n  DataHandlingScheme: 'ProcessRecordAsEvent',\n  dataFileOrganization: 'FixedLength',\n  ProcessCopyBookFileAs: 'SingleEvent',\n  skipIndent: 0,\n  dataFileFont: 'utf8',\n  CopybookFileFormat: 'USE_STANDARD_COLUMNS',\n  copybookSplit: 'Level01',\n  copybookFileName: ''\n   )\nOUTPUT TO @appname@Stream;\n\nCREATE OR REPLACE TARGET @filetarget@ USING FileWriter (\n  filename: '',\n  directory: '',\n  rolloverpolicy: 'EventCount:10000,Interval:30s',\n  flushpolicy: 'EventCount:1,Interval:30s'\n  )\nFORMAT USING JSONFormatter  (\n  members: 'data',\n  EventsAsArrayOfJsonObjects: 'true'\n )\nINPUT FROM @appname@Stream;\n\nend application @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSVSource using FileReader (\ndirectory:'@TEST-DATA-PATH@',\nWildCard:'smallposdata.csv',\npositionByEOF:false,\ncharset:'UTF-8'\n)\nparse using DSVParser (\nheader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\ndirectory:'@FEATURE-DIR@/logs',\nfilename:'PosDataFS',\nrolloverpolicy:'filesize:1M,sequence:00'\n)\nformat using DSVFormatter (\n\n)\ninput from TypedCSVStream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizePosData_actual.log') input from TypedCSVStream;\n\nend application DSV;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop IR;\nundeploy application IR;\ndrop application IR cascade;\nCREATE APPLICATION IR recovery 5 second interval;\n\n CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( \n  FetchSize: 100,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.test01;striim.test02;striim.test03',\n  adapterName: 'IncrementalBatchReader',\n CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',\n  PollingInterval: '5sec'\n  )\n  OUTPUT TO data_stream ;\n\n  CREATE OR REPLACE TARGET sys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM data_stream;\n\ncreate target AzureSQLDWHTarget using AzureSQLDWHWriter(\n\t\tConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        accountname: 'striimqatestdonotdelete',\n        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',\n        uploadpolicy:'eventcount:1,interval:10s'\n) INPUT FROM data_stream;\n\n\nCREATE TARGET log USING LOGWRITER\n(\n name:log,\n filename:'LOGFILENAME'\n)\nINPUT FROM data_stream;\n\n\nEND APPLICATION IR;\n\ndeploy application IR;\nstart IR;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\ncreate flow @APPNAME@_agentflow;\n\nCreate Source @SOURCE@\n        Using OracleReader\n(\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: 'qatest.test77',\n  FetchSize:1,\n  QueueSize:2000,\n  CommittedTransactions:true,\n  Compression:false\n)\nOutput To @STREAM@;\n\nend flow @APPNAME@_agentflow;\n\ncreate flow @APPNAME@_serverflow;\n\ncreate Target @TARGET@ using AzureblobWriter(\n    accountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:7'\n)\nformat using DSVFormatter (\n)\ninput from @STREAM@;\n\nend flow @APPNAME@_serverflow;\n\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 36 with two sources, two jumping attribute windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Ja5W/p -> CQ1 -> WS\n-- S2 -> Ja6W/p -> CQ2 -> WS\n--\n\nSTOP NameW03.W03;\nUNDEPLOY APPLICATION NameW03.W03;\nDROP APPLICATION NameW03.W03 CASCADE;\nCREATE APPLICATION W03 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'Canon1000.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  wildcard:'Canon1000.csv',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[2],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[3])\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[2],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[3])\nFROM CsvStream2;\n\nCREATE JUMPING WINDOW JWc51uW03\nOVER DataStream1 KEEP 51 ROWS;\n\nCREATE JUMPING WINDOW JWc101uW03\nOVER DataStream2 KEEP 101 ROWS;\n\nCREATE WACTIONSTORE WactionsW03 CONTEXT OF WactionData\nEVENT TYPES ( CsvData KEY(merchantId) )\n@PERSIST-TYPE@\n\nCREATE CQ JWc51uW03_to_WactionsW03\nINSERT INTO WactionsW03\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM JWc51uW03 p\nGROUP BY p.merchantId;\n\nCREATE CQ JWc101uW03_to_WactionsW03\nINSERT INTO WactionsW03\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM JWc101uW03 p\nGROUP BY p.merchantId;\n\nEND APPLICATION W03;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop Quiesce_CDC;\nundeploy application Quiesce_CDC;\nalter application Quiesce_CDC;\nCREATE or replace FLOW Quiesce_CDC_flow;\nCreate or replace Source Quiesce_CDC_Oraclesrc Using oraclereader(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',\n Tables:'QATEST.QUIESCE_TABLE1',\n _h_fetchexactrowcount: 'true'\n)\nOutput To Quiesce_CDC_OrcStrm;\nEND FLOW Quiesce_CDC_flow;\nalter application Quiesce_CDC recompile;\nDEPLOY APPLICATION Quiesce_CDC;\nstart application Quiesce_CDC;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION @AppName@;\n\nCreate Source @AppName@_source Using OracleReader(\n  Username:'@username@',\n  Password:'@password@',\n  ConnectionURL:'@url@',\n  Tables:'@srctableName@',\n  Fetchsize:1\n)\nOutput To @AppName@_Stream;\nCREATE OR REPLACE TARGET @AppName@_SF_Target USING Global.SnowflakeWriter (\n  streamingUpload: 'false',\n  useConnectionProfile:'true',\n  connectionProfileName: 'admin.@SFCP@',\n  CDDLAction: 'Process',\n  optimizedMerge: 'false',\n  columnDelimiter: '|',\n  tables: '@srctableName@,@trgtableName@',\n  appendOnly: 'false',\n  uploadPolicy: 'eventcount:1,interval:5m',\n  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',\n  externalStageType: 'Local',\n  adapterName: 'SnowflakeWriter',\n  fileFormatOptions: 'null_if = \\\"\\\"' )\nINPUT FROM @AppName@_Stream;\n\nCREATE OR REPLACE TARGET @AppName@_SF_Target2 USING Global.SnowflakeWriter (\n  streamingUpload: 'false',\n  useConnectionProfile:'true',\n  connectionProfileName: 'admin.@SFCP@',\n  CDDLAction: 'Process',\n  optimizedMerge: 'false',\n  columnDelimiter: '|',\n  tables: '@srctableName@,@trgtableName@',\n  appendOnly: 'false',\n  uploadPolicy: 'eventcount:1,interval:5m',\n  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',\n  externalStageType: 'Local',\n  adapterName: 'SnowflakeWriter',\n  fileFormatOptions: 'null_if = \\\"\\\"' )\nINPUT FROM @AppName@_Stream;\n\nEND APPLICATION @AppName@;\nDeploy application @AppName@;\nStart application @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create Application netflow;\ncreate source NetflowSource using UDPReader (\n\tIpAddress:'127.0.0.1',\n\tPortNo:'3546'\n)\nparse using NetflowParser (\n)\nOUTPUT TO NetflowStream;\n\ncreate type NetflowV5_Type (\nvers int,\nsys_uptime long,\nunix_sec long,\nunix_nsec long,\nflow_sequence long,\nengine_type string,\nengine_id string,\n\nsrc_ip string,\ndst_ip string,\nnext_hop  string,\ninput_idx int,\noutput_idx  int,\n\nflow_pkt_cnt long,\nl3_cnt long,\nfirst_sys_uptime long,\nlast_sys_uptime long,\nsrc_port int,\n\ndst_port int,\nunused1 string,\ntcp_flg string,\nprotocol_type string,\ntos string,\n\nsrc_as int,\ndst_as int,\nsrc_mask string,\ndst_mask string,\nunused2 int\n\n);\n\nCREATE STREAM NetflowV5Stream of NetflowV5_Type;\n\nCREATE CQ NetflowCQ\nINSERT INTO NetflowV5Stream\nSELECT META(x,'version'),META(x,'sys_uptime'),META(x,'unix_sec'),META(x,'unix_nsec'),META(x,'flow_sequence'),META(x,'engine_type').toString(),META(x,'engine_id').toString(),\n       VALUE(x,'src_ip'), VALUE(x,'dst_ip'), VALUE(x,'next_hop'), VALUE(x,'input_idx'), VALUE(x,'output_idx'),\n       VALUE(x,'flow_pkt_cnt'), VALUE(x,'l3_cnt'), VALUE(x,'first_sys_uptime'), VALUE(x,'last_sys_uptime'), VALUE(x,'src_port'),\n       VALUE(x,'dst_port'), VALUE(x,'unused1').toString(), VALUE(x,'tcp_flg').toString(), VALUE(x,'protocol_type').toString(), VALUE(x,'tos').toString(),\n       VALUE(x,'src_as'), VALUE(x,'dst_as'), VALUE(x,'src_mask').toString(), VALUE(x,'dst_mask').toString(), VALUE(x,'unused2')\nFROM NetflowStream x;\n\ncreate Target dump1 using LogWriter(name:'NetflowV5',fileName:'@FEATURE-DIR@/logs/netflow.out') input from NetflowV5Stream;\nend Application netflow;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ recovery 5 second interval;\nCREATE  SOURCE @FirstSourceName@ USING DatabaseReader  ( \n  ConnectionURL: '@SourceConnectionURL@',\n  Username: '@UserName@',\n  Password: '@Password@',\n  Tables: '@SourceTable@',\n  ReplicationSlotName: 'null'\n ) OUTPUT TO @SRCFirstINPUTSTREAM@;\n\n CREATE  SOURCE @MiddleSourceName@ USING DatabaseReader  ( \n  ConnectionURL: '@SourceConnectionURL@',\n  Username: '@UserName@',\n  Password: '@Password@',\n  Tables: '@SourceTable@',\n  ReplicationSlotName: 'null'\n )\nOUTPUT TO @SRCMiddleINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo4) input from @SRCMiddleINPUTSTREAM@;\n\nCREATE  TARGET @FirsttargetName@ USING DatabaseWriter  ( \n  ConnectionURL: '@TargetConnectionURL@',\n  Username: '@UserName@',\n  Password: '@Password@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n ) \n INPUT FROM @SRCFirstINPUTSTREAM@;\n\n CREATE  TARGET @MiddletargetName@ USING DatabaseWriter  ( \n  ConnectionURL: '@TargetConnectionURL@',\n  Username: '@UserName@',\n  Password: '@Password@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n )\nINPUT FROM @SRCMiddleINPUTSTREAM@;\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use PosTester;\nDROP CQ CsvToPosData;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;\n\n\n CREATE SOURCE DBSource USING MySQLReader (\n Compression: true,\n  FetchSize: 1,\n  SendBeforeImage: true,\n  ConnectionURL: '@CONNECTION_URL@',\n  DatabaseName: 'testcassandra',\n  Tables: '@SOURCE_TABLE@',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  Username: '@SOURCE_USER@'\n)\nOUTPUT TO Mysql_ChangeDataStream;\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'test.chkpoint',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  IgnorableExceptionCode:'PRIMARY KEY',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n ) \nINPUT FROM Mysql_ChangeDataStream;\ncreate Target t2 using SysOut(name:Foo2) input from Mysql_ChangeDataStream;\n\nEND APPLICATION DBRTOCW;\ndeploy application DBRTOCW in default;\nstart application DBRTOCW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION testApp;\nUNDEPLOY APPLICATION testApp;\nDROP APPLICATION testApp CASCADE;\n-- DROP EXCEPTIONSTORE testApp_exceptionstore;\n\nCREATE APPLICATION testApp WITH ENCRYPTION RECOVERY 10 SECOND INTERVAL USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE SOURCE testApp_Source USING OracleReader  (\n  Username:'qatest',\n  Password:'qatest',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\n  OnlineCatalog:true,\n  FetchSize:'1',\n  Tables: 'QATEST.sourceTable'\n  ) OUTPUT TO testApp_Stream  ;\n\nCREATE OR REPLACE TARGET testApp_Target USING BigQueryWriter  (\n  ColumnDelimiter: '|', \n  NullMarker: 'NULL', \n  projectId:'striimqa-214712',\n  Encoding: 'UTF-8', \n  BatchPolicy: 'eventCount:5,Interval:120',\n  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', \n  AllowQuotedNewLines: 'false', \n  adapterName: 'BigQueryWriter', \n  optimizedMerge: 'true', \n  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', \n  StandardSQL: 'true', \n  QuoteCharacter: '\\\"', \n  Tables: 'QATEST.sourceTable,transOption_test.targettable',\n  Mode: 'MERGE',\n  StandardSQL: 'true',\n  _h_TransportOptions:'connectionTimeout=30s, readTimeout=12s',\n  QuoteCharacter: '\\\"'\n  ) INPUT FROM testApp_Stream;\n\nCREATE OR REPLACE TARGET testApp_SysOut USING Global.SysOut (name: 'wa') INPUT FROM testApp_Stream;\n\nEND APPLICATION testApp;\nDEPLOY APPLICATION testApp;\nSTART testApp;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use PosTester;\nalter application PosApp;\n\nCREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;\n\nend application PosApp;\nalter application PosApp recompile;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION bq;\nUNDEPLOY APPLICATION bq;\nDROP APPLICATION bq CASCADE;\nCREATE APPLICATION bq recovery 1 second interval;\n\nCreate Source s Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL: 'dockerhost:1521:orcl',\n Tables:'qatest.src_multi_target01',\n FetchSize:1\n) \nOutput To ss;\n\nCREATE TARGET t1 USING BigQueryWriter (\n\tserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\n\tprojectId: 'bigquerywritertest',\n\tTables:'qatest.src_multi_target01,qatest.tgt_multi_target01 columnmap(col1=ID,col2=NAME,OperationName=@METADATA(OperationName),TableName=@METADATA(TableName))',\n\tdatalocation: 'US',\n\tnullmarker: 'aaaa',\n\tcolumnDelimiter: '|',\n\tBatchPolicy: 'eventCount:10, Interval:30'\t\n) INPUT FROM ss;\n\nCREATE TARGET t2 USING BigQueryWriter (\n\tserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\n\tprojectId: 'bigquerywritertest',\n\tTables:'qatest.src_multi_target01,qatest.tgt_multi_target02',\n\tdatalocation: 'US',\n\tcolumnDelimiter: '|',\n\tBatchPolicy: 'eventCount:10, Interval:30'\t\n) INPUT FROM ss;\n\nCREATE TARGET t3 USING BigQueryWriter (\n\tserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\n\tprojectId: 'bigquerywritertest',\n\tTables:'qatest.src_multi_target01,qatest.tgt_multi_target03',\n\tdatalocation: 'US',\n\tnullmarker: 'NOTNULL',\n\tcolumnDelimiter: '|',\n\tBatchPolicy: 'eventCount:10, Interval:30'\t\n) INPUT FROM ss;\n\nCREATE or replace TARGET t4 USING BigQueryWriter (\n\tserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\n\tprojectId: 'bigquerywritertest',\n\tTables:'qatest.src_multi_target01,qatest.tgt_multi_target04 columnmap(OperationName=@METADATA(OperationName))',\n\tdatalocation: 'US',\n\tnullmarker: 'NULL',\n\tcolumnDelimiter: '|',\n\tBatchPolicy: 'eventCount:10, Interval:30'\t\n) INPUT FROM ss;\n\nEND APPLICATION bq;\nDEPLOY APPLICATION bq;\nSTART APPLICATION bq;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop application Postgres_To_PostgresApp;\nundeploy application Postgres_To_PostgresApp;\ndrop application Postgres_To_PostgresApp cascade;\n\nCREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;\n\ncreate type employee\n(\nid integer,\nename String,\noperationname String,\nLSN String\n);\nCREATE STREAM Postgres_TypedStream of employee;\n\nCREATE OR REPLACE SOURCE Postgres_Src USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --startLSN:'758199500005967269518578814280000',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.postgres_src'\n ) \nOUTPUT TO Postgres_Change_Data_Stream;\n\ncreate stream UserdataStream1 of Global.WAEvent;\n\nCreate CQ CQUser\ninsert into UserdataStream1\nselect putuserdata (data1,'OperationName',META(data1,'OperationName').toString()) from Postgres_Change_Data_Stream data1;\n\ncreate CQ Cqfilter \ninsert into Postgres_TypedStream\nselect \nto_int(data[0]),\ndata[1],\nMETA(u,'OperationName').toString(),\nMETA(u,'LSN').toString()\nfrom UserdataStream1 u \nwhere USERDATA(u,'OperationName').toString()=='INSERT';\n\nCREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM Postgres_TypedStream;\n\nCREATE TARGET Postgres_FW USING FileWriter (\n  filename:'Postgres_FW.log'\n)\nFORMAT USING DSVFormatter ()\nINPUT FROM Postgres_TypedStream;\n\nCREATE OR REPLACE TARGET Postgres_tgt USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy: 'EventCount:1,Interval:60',\nCommitPolicy: 'EventCount:1,Interval:300',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.postgres_tgt',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM Postgres_TypedStream;\n\nend application Postgres_To_PostgresApp;\ndeploy application Postgres_To_PostgresApp;\nstart Postgres_To_PostgresApp;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n--CREATE APPLICATION @APPNAME@;\ncreate application @APPNAME@ Recovery 5 second Interval;\n\n--create or replace flow @APPNAME@_agentflow;\n\nCREATE OR REPLACE SOURCE @APPNAME@_source USING DatabaseReader  (\n  Username: 'qatest',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.EMP_INIT',\n  adapterName: 'DatabaseReader',\n  FetchSize: 100,\n  Password: 'qatest'\n )\nOUTPUT TO @APPNAME@_stream ;\n\n--end flow @APPNAME@_@APPNAME@_agentflow;\n\nCREATE OR REPLACE TARGET @APPNAME@_target USING CassandraCosmosDBWriter  (\n  AccountEndpoint: 'cassandracosmostest.cassandra.cosmos.azure.com',\n  AccountKey: 'pqDZvVgbdSCg7VzIzD77dAhPG2odGRZPLhAQA1qnZbAKoIDk6RuQX5r2phbRQFnR1l54qxOcvBXNdz8DeijYIg==',\n  KeySpace: 'test',\n  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',\n  Tables: 'QATEST.Source1,test.target1',\n  adapterName: 'CassandraCosmosDBWriter'\n )\n INPUT FROM @APPNAME@_stream;\n\n END APPLICATION @APPNAME@;\n\ndeploy application @APPNAME@;\n --deploy application @APPNAME@ with agentflow in agents;\n start application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE TYPE @appname@CQOUT1_Type (\n companyName java.lang.String,\n merchantId java.lang.String,\n dateTime org.joda.time.DateTime,\n hourValue java.lang.String,\n amount java.lang.String,\n zip java.lang.String,\n FileName java.lang.String);\n\nCREATE SOURCE @parquetsrc@ USING S3Reader (\n    bucketname:'',\n    objectnameprefix:'',\n    foldername:''\n     )\nPARSE USING ParquetParser (\n)\nOUTPUT TO @appname@Stream;\n\nCREATE OR REPLACE STREAM @appname@CQOUT1 OF @appname@CQOUT1_Type;\nCREATE OR REPLACE CQ @appname@CQ_PQEvent\nINSERT INTO @appname@CQOUT1\n    Select\n    data.get(\"companyName\").toString(),\n    data.get(\"merchantId\").toString(),\n    TO_DATE(data.get(\"dateTime\").toString()),\n    data.get(\"hourValue\").toString(),\n    data.get(\"amount\").toString(),\n    data.get(\"zip\").toString(),\n    metadata.get(\"FileName\").toString()\n    FROM @appname@Stream p;\n\ncreate Target @s3target@ using S3Writer(\n    bucketname:'',\n    objectname:'',\n    uploadpolicy:'',\n    foldername:''\n)\nformat using DSVFormatter ()\ninput from @appname@CQOUT1;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on @node@ in default;\nstart application @appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP OuterJoinTester.OJApp;\nUNDEPLOY APPLICATION OuterJoinTester.OJApp;\nDROP APPLICATION OuterJoinTester.OJApp cascade;\n\nCREATE APPLICATION OJApp;\n\nCREATE source wsSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'banks.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:No,\n  trimquote:false\n) OUTPUT TO QaStream;\n\nCREATE TYPE bankData\n(\nbankID Integer KEY,\nbankName String\n);\n\nCREATE TYPE cardData\n(\ncardID Integer KEY,\ncardName String\n);\n\nCREATE STREAM wsStream OF bankData;\n\nCREATE CACHE cache1 USING CsvReader(\ndirectory: '@TEST-DATA-PATH@',\nwildcard: 'banks.csv',\nheader: No,\ncolumndelimiter: ',',\ntrimquote: false\n) QUERY (keytomap:'bankID') OF bankData;\n\n\nCREATE CACHE cache2 USING CsvReader(\ndirectory: '@TEST-DATA-PATH@',\nwildcard: 'bankCards.csv',\nheader: No,\ncolumndelimiter: ',',\ntrimquote: false\n) QUERY (keytomap:'cardID') OF cardData;\n\n\nCREATE WACTIONSTORE oneWS CONTEXT OF bankData\nEVENT TYPES(bankData )\n@PERSIST-TYPE@\n\nCREATE CQ csvTobankData\nINSERT INTO oneWS\nSELECT TO_INT(data[0]), data[1] FROM QaStream;\n\n\n\nEND APPLICATION OJApp;\n\n\n\n\n\n\n\n\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP Mssqltobigquery;\nUNDEPLOY APPLICATION Mssqltobigquery;\nDROP APPLICATION Mssqltobigquery CASCADE;\n\nCREATE APPLICATION Mssqltobigquery RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE Mssqltobigquery_source USING MSSqlReader\n(\nUsername:'qatest',\nPassword:'w3b@ct10n',\nDatabaseName:'qatest',\nConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',\nTables:'qatest.srctb',\nConnectionPoolSize:1,\nCompression:'true'\n)\nOUTPUT TO SS;\n\n\nCREATE or replace TARGET Mssqltobigquery_Target USING BigQueryWriter (\nColumnDelimiter: '|', \n  NullMarker: 'NULL', \n  projectId:'striimqa-214712',\n  Encoding: 'UTF-8', \n  BatchPolicy: 'eventCount:5,Interval:120',\n  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', \n  AllowQuotedNewLines: 'false', \n  adapterName: 'BigQueryWriter', \n  optimizedMerge: 'true', \n  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', \n  StandardSQL: 'true', \n  QuoteCharacter: '\\\"', \n  Tables: 'QATEST.srctb,.oratobqtgt',\n  Mode: 'MERGE',\n  StandardSQL: 'true',\n  QuoteCharacter: '\\\"'\n) INPUT FROM SS;\n\nEND APPLICATION Mssqltobigquery;\nDEPLOY APPLICATION Mssqltobigquery;\nSTART APPLICATION Mssqltobigquery;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@;\nCREATE FLOW @APP_NAME@SrcFlow;\n\nCREATE OR REPLACE SOURCE @APP_NAME@_src USING FileReader (\ndirectory:'@DIRECTORY@',\nWildCard:'posdata5L.csv',\npositionByEOF:false\n)\nparse using DSVParser (\nheader:'no'\n)\nOUTPUT TO @APP_NAME@_Stream;\nEND FLOW @APP_NAME@SrcFlow;\n\nCREATE FLOW @APP_NAME@TgtFlow;\n\nCREATE OR REPLACE TYPE @APP_NAME@_Type  ( BUSINESS_NAME java.lang.String KEY,\nMERCHANT_ID java.lang.String,\nPRIMARY_ACCOUNT_NUMBER java.lang.String\n ) ;\n\nCREATE OR REPLACE STREAM @APP_NAME@_Stream2 OF @APP_NAME@_Type;\nCREATE OR REPLACE CQ @APP_NAME@_CQ\nINSERT INTO @APP_NAME@_Stream2\nSELECT data[0],data[1],data[2]\nFROM @APP_NAME@_Stream;\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.DeltaLakeWriter (\n  personalAccessToken: '',\n  stageLocation: '/',\n  Mode: 'MERGE',\n  Tables: '\"QATEST\".\"%\",DEFAULT.student',\n  adapterName: 'DeltaLakeWriter',\n  personalAccessToken_encrypted: 'false',\n  optimizedMerge: 'false',\n  uploadPolicy: 'eventcount:50000,interval:50s',\n  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )\nINPUT FROM @APP_NAME@_Stream2;\n\nEND FLOW @APP_NAME@TgtFlow;\n\nEND APPLICATION @APP_NAME@;\nDEPLOY APPLICATION @APP_NAME@ WITH @APP_NAME@SrcFlow IN agents,@APP_NAME@TgtFlow IN default;\nSTART APPLICATION @APP_NAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE or replace TARGET @TARGET_NAME@ USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.TABLE_TEST_%,@DATASET@.%',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1000,Interval:10',\nStandardSQL:true\t\n) INPUT FROM @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION oracletokudu;\nUNDEPLOY APPLICATION oracletokudu;\nDROP APPLICATION oracletokudu CASCADE;\nCREATE APPLICATION oracletokudu;\nCreate Type CSVType (\n\tcompanyname String,\n  merchantName String\n);\n\nCreate Stream TypedFileStream of CSVType;\n\ncreate source CSVSource using FileReader (\n\tdirectory:'/Users/jenniffer/Product2/IntegrationTests/TestData',\n\tWildCard:'posdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\t\n)\nOUTPUT TO FileStream;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedFileStream\nSELECT TO_STRING(data[0]),TO_STRING(data[1])\nFROM FileStream;\n\nCREATE TARGET WriteintoKudu using KuduWriter (\nkuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',\npkupdatehandlingmode:'@MODE@',\ntables: '@TARGET_TABLES@',\nbatchpolicy: 'EventCount:10000,Interval:20s')\nINPUT FROM TypedFileStream;\nDEPLOY APPLICATION oracletokudu;\nSTART APPLICATION oracletokudu;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION bq;\nUNDEPLOY APPLICATION bq;\nDROP APPLICATION bq CASCADE;\nCREATE APPLICATION bq;\n\nCREATE SOURCE s USING FileReader\n(\n  directory:'/Users/sujith_syk/MyTasks/Analyse_BQtableScan_on_partitioned_table_merge_query',\n  WildCard:'testdata1.csv',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n\theader:yes\n)OUTPUT TO FileStream;\n\nCREATE TYPE cdctype(\n    STORE_ID  String,\n    NAME  String,\n    CITY  String,\n    STATE  String,\n    ZIP  String,\n    CUSTOMER_ACCOUNT_NUMBER  String,\n    ORDER_ID  String,\n    SKU  String,\n    ORDER_AMOUNT  String,\n    DATETIME  String\n);\n\nCREATE STREAM cdctypestream OF cdctype;\n\nCREATE CQ cdcstreamcq\nINSERT INTO cdctypestream\nSELECT TO_STRING(p.data[0]),\n       TO_STRING(p.data[1]),\n       TO_STRING(p.data[2]),\n       TO_STRING(p.data[3]),\n       TO_STRING(p.data[4]),\n       TO_STRING(p.data[5]),\n       TO_STRING(p.data[6]),\n       TO_STRING(p.data[7]),\n       TO_STRING(p.data[8]),\n       TO_STRING(p.data[9])\nFROM FileStream p;\n\n\nCREATE TARGET t1 USING BigQueryWriter\n(\n  ServiceAccountKey:'/Users/sujith_syk/Documents/striimdev-creds.json',\n  projectId:'striimdev',\n  Tables:'qatest.FILETOBQHEAPTEST1',\n  datalocation:'US',\n  nullmarker:'NOTNULL',\n  columnDelimiter:'|',\n  BatchPolicy:'eventCount:100, Interval:180',\n  Mode: 'MERGE'\n) INPUT FROM cdctypestream;\n\nCREATE TARGET t2 USING BigQueryWriter\n(\n  ServiceAccountKey:'/Users/sujith_syk/Documents/striimdev-creds.json',\n  projectId:'striimdev',\n  Tables:'qatest.FILETOBQHEAPTEST2',\n  datalocation:'US',\n  nullmarker:'NOTNULL',\n  columnDelimiter:'|',\n  BatchPolicy:'eventCount:100, Interval:180',\n  Mode: 'MERGE'\n) INPUT FROM cdctypestream;\n\nCREATE TARGET t3 USING BigQueryWriter\n(\n  ServiceAccountKey:'/Users/sujith_syk/Documents/striimdev-creds.json',\n  projectId:'striimdev',\n  Tables:'qatest.FILETOBQHEAPTEST3',\n  datalocation:'US',\n  nullmarker:'NOTNULL',\n  columnDelimiter:'|',\n  BatchPolicy:'eventCount:100, Interval:180',\n  Mode: 'MERGE'\n) INPUT FROM cdctypestream;\n\nEND APPLICATION bq;\nDEPLOY APPLICATION bq;\nSTART APPLICATION bq;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\ncreate flow @APPNAME@_SourceFlow;\nCREATE OR REPLACE SOURCE @SOURCE@ USING Ojet  (\n  FilterTransactionBoundaries: true,\n  ConnectionURL: '@OCI-URL@',\n  Tables: '@SOURCE_TABLE@',\n  Password: '@OJET-PASSWORD@',\n  fetchsize: 1,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  Username: '@OJET-UNAME@'\n )\nOUTPUT TO @STREAM@;\nend flow @APPNAME@_SourceFlow;\ncreate flow @APPNAME@_TargetFlow;\nCREATE OR REPLACE TARGET @TARGET@1 USING SnowflakeWriter (\n  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',\n  optimizedMerge: 'false',\n  password: 'Web@ct10n',\n  username: 'striim_regression2',\n  appendOnly: 'false',\n  uploadPolicy: 'eventcount:10000,interval:5m',\n  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET@2 USING SnowflakeWriter (\n  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',\n  optimizedMerge: 'false',\n  password: 'Web@ct10n',\n  username: 'striim_regression2',\n  appendOnly: 'true',\n  uploadPolicy: 'eventcount:10000,interval:5m',\n  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET@3 USING SnowflakeWriter (\n  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',\n  optimizedMerge: 'true',\n  password: 'Web@ct10n',\n  username: 'striim_regression2',\n  appendOnly: 'false',\n  uploadPolicy: 'eventcount:10000,interval:5m',\n  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',\n )\nINPUT FROM @STREAM@;\nend flow @APPNAME@_TargetFlow;\nEND APPLICATION @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@;\n\nCREATE SOURCE @parquetsrc@ USING FileReader (\nwildcard: '',\ndirectory: '',\npositionbyeof: false )\nPARSE USING ParquetParser (\n)\nOUTPUT TO @appname@Stream;\n\nCREATE CQ @appname@CQ\nINSERT INTO @appname@CqOut\n    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;\n\nCREATE OR REPLACE TARGET @filetarget@ USING FileWriter (\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:1,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING ParquetFormatter  (\nschemaFileName: 'ParquetFileSchema'\n)\nINPUT FROM @appname@CqOut;\n\ncreate Target @s3target@ using S3Writer(\n    bucketname:'',\n    objectname:'',\n    uploadpolicy:'',\n    foldername:''\n)\nformat using ParquetFormatter (\nschemaFileName: 'ParquetS3Schema'\n)\ninput from @appname@CqOut;\n\ncreate Target @blobtarget@ using AzureBlobWriter(\n\taccountname:'',\n\taccountaccesskey:'',\n\tcontainername:'',\n    blobname:'',\n\tfoldername:'',\n\tuploadpolicy:'EventCount:10,interval:5s'\n)\nformat using ParquetFormatter (\nschemaFileName: 'ParquetAzureSchema'\n)\nINPUT FROM @appname@CqOut;\n\nCREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (\n    bucketname:'',\n    objectname:'',\n    foldername:'',\n    projectId:'',\n    uploadPolicy:''\n)\nformat using ParquetFormatter (\nschemaFileName: 'ParquetGCSSchema'\n)\nINPUT FROM @appname@CqOut;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION orrs;\nUNDEPLOY APPLICATION orrs;\nDROP APPLICATION orrs CASCADE;\nCREATE APPLICATION orrs recovery 5 second interval;\nCreate Source OraSource Using OracleReader \n\t(\n\t Username:'user-name',\t\n\t Password:'password',\n\t ConnectionURL: 'src_url',\n\t Tables:'src_table',\n\t FilterTransactionBoundaries:true,\n\t FetchSize:'fetch-size'\n\t) Output To LCRStream;\n\t\n\tCREATE TARGET RSTarget USING RedshiftWriter\n\t(\n\t  ConnectionURL: 'tgt_url',\n\t  Username: 'tgt_username',\n\t  Password: 'tgt_pwrd',\n\t  bucketname: 'bucket_name',\n\t  --accesskeyId: '@ACCESS-KEY-ID@',\n\t  --secretaccesskey: '@SECRET-ACCESS-KEY@',\n\t  S3IAMRole:'@IAMROLE@',\n\t  Tables: 'tgt_table',\n\t  uploadpolicy:'eventcount:300,interval:1m'\n\t) INPUT FROM LCRStream;\nEND APPLICATION orrs;\nDEPLOY APPLICATION orrs;\nSTART APPLICATION orrs;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE SOURCE @SOURCE_NAME@ USING Global.FileReader (\n  positionbyeof: false )\nPARSE USING Global.DSVParser (\n )\nOUTPUT TO @STREAM@;\n\nCREATE TARGET @SOURCE_NAME@_sysout USING Global.SysOut (\n  name: '@SOURCE_NAME@_sysout' )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP banker.bankApp;\nUNDEPLOY APPLICATION banker.bankApp;\nDROP APPLICATION banker.bankApp cascade;\n\nCREATE APPLICATION bankApp;\n\n\nCREATE source wsSource USING FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard:'banks.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:No,\n  trimquote:false\n) OUTPUT TO QaStream;\n\n\nCREATE TYPE wsData\n(\nbankID Integer KEY,\nbankName String\n);\n\n\nCREATE WACTIONSTORE oneWS\nCONTEXT OF wsData\nEVENT TYPES(wsData )\n@PERSIST-TYPE@\n\nCREATE CQ oneWSCQ\nINSERT INTO oneWS\nSELECT TO_INT(data[0]),data[1] FROM QaStream\nLINK SOURCE EVENT;\n\nEND APPLICATION bankApp;\ndeploy application bankapp;\nstart application bankapp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop IR;\nUndeploy application IR;\ndrop application IR cascade;\nCREATE APPLICATION IR recovery 5 second interval;\ncreate flow AgentFlow;\nCREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( \n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.119/DBS_PORT=1025',\n  Tables: 'striim.upgrade01',\n  CheckColumn:'striim.upgrade01=t1',\n  startPosition:'striim.upgrade01=2018-09-20 06:43:59',\n  ReturnDateTimeAs:'string'\n  )\nOUTPUT TO data_stream1;\nend flow AgentFlow;\n\ncreate flow serverFlow;\ncreate target AzureSQLDWHTarget using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test26,dbo.test26',\n        uploadpolicy:'eventcount:10000,interval:10s'\n) INPUT FROM data_stream1;\n\nend flow serverFlow;\nEND APPLICATION IR;\ndeploy application IR with AgentFlow in Agents, ServerFlow in default;\nstart application IR;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.GGTrailReader (\n  Tables:'@TABLES@',\n  CDDLCapture: false,\n  TrailDirectory: '@TRAIL_FILE_DIR@',\n  TrailFilePattern: '@WILDCARD@',\n  Compression: false,\n  SupportColumnCharset: false,\n  CDDLAction: 'Process',\n  FilterTransactionBoundaries: true,\n  adapterName: 'GGTrailReader',\n  TrailByteOrder: '@ENDIAN@' )\nOUTPUT TO @STREAM@;\n\nCREATE TARGET @SOURCE_NAME@_sysout USING Global.SysOut (\n  name: '@SOURCE_NAME@_SysOut' )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Canon Test W02\n-- Nicholas Keene, WebAction, Inc.\n--\n-- Basic test for an unpartitioned jumping count window\n--\n-- S -> JWc101uW02 -> CQ -> WS\n--\n\n\nUNDEPLOY APPLICATION NameW02.W02;\nDROP APPLICATION NameW02.W02 CASCADE;\nCREATE APPLICATION W02 RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE FLOW DataAcquisitionW02;\n\n\nCREATE SOURCE CsvSourceW02 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'Canon1000.csv',\n  columndelimiter:',',\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStreamW02;\n\n\nEND FLOW DataAcquisitionW02;\n\n\n\nCREATE FLOW DataProcessingW02;\n\nCREATE TYPE DataTypeW02 (\n    eventID String,\n    word String KEY,\n    dateTime DateTime\n);\n\nCREATE STREAM DataStreamW02 OF DataTypeW02;\n\nCREATE CQ CSVStreamW02_to_DataStreamW02\nINSERT INTO DataStreamW02\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[2],'yyyyMMddHHmmss')\nFROM CsvStreamW02;\n\nCREATE JUMPING WINDOW JWc101uW02\nOVER DataStreamW02\nKEEP 101 ROWS;\n\nCREATE WACTIONSTORE WactionStoreW02 CONTEXT OF DataTypeW02\nEVENT TYPES ( DataTypeW02 KEY(word) )\n@PERSIST-TYPE@\n\nCREATE CQ JWc101uW02_to_WactionStoreW02\nINSERT INTO WactionStoreW02\nSELECT\n    FIRST(eventID),\n    FIRST(word),\n    FIRST(dateTime)\nFROM JWc101uW02;\n\nEND FLOW DataProcessingW02;\n\n\n\nEND APPLICATION W02;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP liveStream;\nUNDEPLOY APPLICATION liveStream;\nDROP APPLICATION liveStream CASCADE;\n\nCREATE APPLICATION liveStream;\n\nCREATE TYPE Atm(\n  productID String KEY,\n  stateID String,\n  productWeight int,\n  quantity int,\n  size int);\n\n--SOURCE DESCRIPTION\n---------------------------------------------\n--OutputType: name of the corresponding TYPE\n--noLimit: produce infinite data\n--maxRows: (if noLimit = false) produces specified rows\n--iterations (optional): 1 iteration = populating Type attributes once.\n--iterationDelay (ms)(optional): delay between iterations (0 if none)\n\n-- ** either maxRows or iterations must be 0 **\n\n--StringSet: columns of type String. column values within brackets (seperate values by dash), seperate columns by comma\n--NumberSet: columns of type Int, Double, Long. supply a range between brackets, followed by G (Gaussian) or R (Random) distribution.\n---------------------------------------------\n\nCREATE SOURCE liveSource using StreamReader(\n  OutputType: 'eventLister.Atm',\n  noLimit: 'false',\n  maxRows: 20,\n  iterations: 0,\n  iterationDelay: 100,\n  StringSet: 'productID[001-002-003-004],stateID[AS-CA-WA-NY]',\n  NumberSet: 'productWeight[3-3]R,quantity[20-20]R,size[250-250]R'\n  )OUTPUT TO CsvStream;\n\nCREATE STREAM newStream OF Atm;\n\n\nCREATE CQ newCQ\nINSERT INTO newStream\nSELECT data[0], data[1], TO_INT(data[2]), TO_INT(data[3]), TO_INT(data[4]) FROM\nCsvStream;\n\nCREATE WACTIONSTORE streamActivity CONTEXT OF Atm\nEVENT TYPES ( Atm )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ newCQ2\nINSERT INTO streamActivity\nSELECT * FROM newStream\nlink source event;\n\nEND APPLICATION liveStream;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application RedshiftColmap;\nundeploy application RedshiftColmap;\ndrop application RedshiftColmap CASCADE;\ncreate application RedshiftColmap recovery 1 second interval;\n\nCREATE OR REPLACE SOURCE OracleSource USING OracleReader  (\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: '@SOURCE_TABLES@',\n  FetchSize: 1\n ) Output To LogminerStream;\n \n--create Target t2 using SysOut(name:Foo2) input from LogminerStream; \n \nCREATE TARGET RedshiftTarget USING RedshiftWriter\n\t(\n\t  ConnectionURL: '@TARGET-URL@',\n\t  Username: '@TARGET-UNAME@',\n\t  Password: '@TARGET-PASSWORD@',\n\t  bucketname: '@BUCKETNAME@',\n\t  --accesskeyId: '@ACCESS-KEY-ID@',\n\t  --secretaccesskey: '@SECRET-ACCESS-KEY@',\n\t  S3IAMRole:'@IAMROLE@',\n\t  Tables: '@TARGET-TABLES@',\n\t  uploadpolicy:'eventcount:5,interval:10s',\n\t  Mode:'incremental'\n\t) INPUT FROM LogminerStream;\n\t\nEND APPLICATION RedshiftColmap;\ndeploy application RedshiftColmap;\nSTART application RedshiftColmap;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application TestApp;\ncreate source CSVSource using FileReader (\n\tdirectory:'Samples/AppData',\n\tWildCard:'posdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ TestCQ\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\n\tfilename:'posdata_JSON',\n\trolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'\n)\nformat using JSONFormatter (\n\tmembers:'merchantname,merchantid,dateTime,hourValue,amount,zip'\n)\n\ninput from TypedCSVStream;\nend application TestApp;\n\ndeploy application TestApp;\nstart application TestApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\n-----------------------------------\nstop application SourceAgentApp;\nundeploy application SourceAgentApp;\n\nstop application TargetServerApp1;\nundeploy application TargetServerApp1;\n\nstop application TargetServerApp2;\nundeploy application TargetServerApp2;\n\nstop application TargetServerApp3;\nundeploy application TargetServerApp3;\n\ndrop application SourceAgentApp cascade;\ndrop application TargetServerApp1 cascade;\ndrop application TargetServerApp2 cascade;\ndrop application TargetServerApp3 cascade;\n\n\nCREATE APPLICATION SourceAgentApp;\n\ncreate flow flow1;\ncreate source CSVSource using FileReader (\ndirectory: '@TEST-DATA-PATH@/tmp',\nWildCard:'mybanks*',\npositionByEOF: true,\ncharset:'UTF-8'\n) parse using DSVParser (header:'no')\nOUTPUT TO CsvStream;\nend flow flow1;\n\n--CREATE TARGET T USING Sysout(name:'sysout1') INPUT FROM CsvStream;\n\nEND APPLICATION SourceAgentApp;\n\nDEPLOY APPLICATION SourceAgentApp with flow1 in AGENTS;\n\n-- Fisrt app consuming from app running in agent\nCREATE APPLICATION TargetServerApp1;\ncreate flow flow2;\n\nCREATE TARGET T2 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp1_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')\nFORMAT USING JSONFormatter ()\nINPUT FROM CsvStream;\nend flow flow2;\n\nEND APPLICATION TargetServerApp1;\ndeploy application TargetServerApp1 with flow2 in default;\n\n\n-- another app consuming from app running in agent\n\nCREATE APPLICATION TargetServerApp2;\ncreate flow flow3;\n\nCREATE TARGET T3 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp2_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')\nFORMAT USING JSONFormatter ()\nINPUT FROM CsvStream;\nend flow flow3;\n\nEND APPLICATION TargetServerApp2;\ndeploy application TargetServerApp2 with flow3 in default;\n\n-- another app consuming from app running in agent. This app will be deployed and started after other apps started.\n\nCREATE APPLICATION TargetServerApp3;\ncreate flow flow4;\n\nCREATE TARGET T4 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp3_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')\n FORMAT USING JSONFormatter ()\n INPUT FROM CsvStream;\nend flow flow4;\n\nEND APPLICATION TargetServerApp3;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create source @SOURCE_NAME@ USING MariaDBReader \n(\nUsername: '@READER-UNAME@',\nPassword: '@READER-PASSWORD@',\nConnectionURL: '@CDC-READER-URL@',\nTables: @WATABLES@,\nsendBeforeImage:'true',\nFilterTransactionBoundaries: 'true'\n) \nOUTPUT TO @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION @AppName@;\nCreate connectionProfile @CP@ TYPE Snowflake(ConnectionURL:'@SFurl@',username:'@SFUsername@',Password:'@SFPassword@',authenticationType:'Password');\n\nCreate Source @AppName@_source Using OracleReader(\n  Username:'@username@',\n  Password:'@password@',\n  ConnectionURL:'@url@',\n  Tables:'@tableName@',\n  Fetchsize:1\n)\nOutput To @AppName@_Stream;\nCREATE OR REPLACE TARGET @AppName@_Target USING Global.SnowflakeWriter (\n  streamingUpload: 'false',\n  useConnectionProfile:'true',\n  connectionProfileName: 'admin.@CP@',\n  CDDLAction: 'Process',\n  optimizedMerge: 'false',\n  columnDelimiter: '|',\n  tables: '@tableName@,SANJAYPRATAP.SAMPLESCHEMA.SAMPLE_PK',\n  appendOnly: 'false',\n  uploadPolicy: 'eventcount:1,interval:5m',\n  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',\n  externalStageType: 'Local',\n  adapterName: 'SnowflakeWriter',\n  fileFormatOptions: 'null_if = \\\"\\\"' )\nINPUT FROM @AppName@_Stream;\nEND APPLICATION @AppName@;\nDeploy application @AppName@;\nStart application @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE OR REPLACE SOURCE @SOURCE@ USING MSSQLReader\n (\n   Username: '@LOGMINER-UNAME@',\n   Password: '@LOGMINER-PASSWORD@',\n   ConnectionURL: '@LOGMINER-URL@',\n   DatabaseName:'qatest',\n   Tables: '@SOURCE_TABLE@',\n    Compression:false,\n    AutoDisableTableCDC:false,FetchTransactionMetadata:true,\n    StartPosition:'EOF'\n )\nOUTPUT TO @STREAM@;\n\n\nCREATE OR REPLACE TARGET @TARGET@1 USING Global.DeltaLakeWriter (\n  uploadPolicy: 'eventcount:10,interval:10s',\n  Mode: 'MERGE',\n  hostname: ' ',\n  Tables: ' ',\n  stageLocation: '/',\n  CDDLAction: 'Process',\n  adapterName: 'DeltaLakeWriter',\n  personalAccessToken: ' ',\n  connectionUrl: ' '  )\n\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET@2 USING Global.DeltaLakeWriter (\n  uploadPolicy: 'eventcount:10,interval:10s',\n  Mode: 'MERGE',\n  optimizedMerge: 'true',\n  hostname: ' ',\n  Tables: ' ',\n  stageLocation: '/',\n  CDDLAction: 'Process',\n  adapterName: 'DeltaLakeWriter',\n  personalAccessToken: ' ',\n  connectionUrl: ' '  )\n\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET@3 USING Global.DeltaLakeWriter (\n  uploadPolicy: 'eventcount:10,interval:10s',\n  Mode: 'APPENDONLY',\n  hostname: ' ',\n  Tables: ' ',\n  stageLocation: '/',\n  CDDLAction: 'Process',\n  adapterName: 'DeltaLakeWriter',\n  personalAccessToken: ' ',\n  connectionUrl: ' '  )\n\nINPUT FROM @STREAM@;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION OneAgentEncryptionTester.CSV;\nUNDEPLOY APPLICATION OneAgentEncryptionTester.CSV;\nDROP APPLICATION OneAgentEncryptionTester.CSV cascade;\n\ncreate application CSV WITH ENCRYPTION;\n\nCREATE FLOW AgentFlow;\ncreate source CSVSource using CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'customerdetails-agent.csv',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO CsvStream1;\n\nEND FLOW AgentFlow;\n\nCREATE FLOW ServerFlow;\n\nCREATE TYPE MyTypeCsv(\nPAN String,\nFNAME String KEY,\nLNAME String,\nADDRESS String,\nCITY String,\nSTATE String,\nZIP String,\nGENDER String\n);\n\nCREATE STREAM TypedStreamCsv of MyTypeCsv;\n\nCREATE CQ TypeConversionCQCsv\nINSERT INTO TypedStreamCsv\nSELECT\ndata[0],\ndata[1],\ndata[2],\ndata[3],\ndata[4],\ndata[5],\ndata[6],\ndata[7]\nfrom CsvStream1;\n\nCREATE WACTIONSTORE StoreInfoCsv CONTEXT OF MyTypeCsv\nEVENT TYPES ( MyTypeCsv )\n@PERSIST-TYPE@\n\nCREATE CQ StoreWactionCsv\nINSERT INTO StoreInfoCsv\nSELECT * FROM TypedStreamCsv\nLINK SOURCE EVENT;\n\n\nEND FLOW ServerFlow;\n\nend application CSV;\n\nDEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;\nSTART CSV;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSVSource using FileReader (\n\tdirectory:'@TEST-DATA-PATH@',\n\tWildCard:'smallposdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\n\tfilename:'TargetPosDataXmlFSDefault',\n\tdirectory:'@FEATURE-DIR@/logs/',\n    sequence:'00',\n\trolloverpolicy:'FileSizeRollingPolicy'\n)\nformat using XMLFormatter (\n\trootelement:'document',\n\telementtuple:'MerchantName:zip:text=merchantname'\n)\ninput from TypedCSVStream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlDFS_actual.log') input from TypedCSVStream;\n\nend application DSV;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\ncreate application @appname@;\n\nCREATE SOURCE @parquetsrc@ USING FileReader (\n  directory: '',\n  positionByEOF: false,\n  WildCard: '' )\nPARSE USING ParquetParser (\n )\nOUTPUT TO @appname@Streams;\n\nCREATE OR REPLACE CQ @appname@CQOrder3\nINSERT INTO @appname@Stream3\nSELECT\nPUTUSERDATA(s,'schemaName',s.data.getSchema().getName())\nFROM @appname@Streams s;\n\nCREATE TARGET @filetarget@ USING Global.FileWriter (\n  DataEncryptionKeyPassphrase: '',\n  flushpolicy: 'EventCount:10000,Interval:30s',\n  directory: '',\n  rolloveronddl: 'true',\n  encryptionpolicy: '',\n  filename: 'AvroOutFile',\n  DataEncryptionKeyPassphrase_encrypted: 'true',\n  rolloverpolicy: 'EventCount:390,Interval:30m' )\nFORMAT USING Global.AvroFormatter  (\n  schemaFileName: 'AvroSchema',\n  formatAs: 'default',\n  schemaregistryConfiguration: '' )\nINPUT FROM @appname@Stream3;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application app2;\nundeploy application app2;\nalter application app2;\nCREATE or replace TARGET app2_targetNew USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test01,QATEST.KPS1_Alter'\n) INPUT FROM sourcestream;\nalter application app2 recompile;\ndeploy application app2;\n\nstop application app3;\nundeploy application app3;\nalter application app3;\n\nCREATE or replace TARGET app3_targetNew USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test02,QATEST.KPS2_Alter'\n) INPUT FROM sourcestream;\nalter application app3 recompile;\ndeploy application app3;\n\n\nstop application app4;\nundeploy application app4;\nalter application app4;\nCREATE TARGET app4_targetNew USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'qatest.test03,QATEST.KPS3_Alter'\n) INPUT FROM sourcestream;\nalter application app4 recompile;\ndeploy application app4;\n\n\nstop application app5;\nundeploy application app5;\nalter application app5;\nCREATE or replace TARGET app5_target1_New USING KafkaWriter VERSION '0.11.0'(\nbrokeraddress:'localhost:9092',\ntopic:'snappy_Alter',\nKafkaConfig:'compression.type=snappy'\n) \nFORMAT USING DSVFormatter ()\nINPUT FROM kps_typedStream;\n\nCREATE or replace TARGET app5_target2_New USING KafkaWriter VERSION '0.11.0'(\nbrokeraddress:'localhost:9092',\ntopic:'gzip_Alter',\nKafkaConfig:'compression.type=gzip'\n) \nFORMAT USING DSVFormatter ()\nINPUT FROM sourcestream;\n\nCREATE or replace TARGET app5_target3_New USING KafkaWriter VERSION '0.11.0'(\nbrokeraddress:'localhost:9092',\ntopic:'lz4_Alter',\nKafkaConfig:'compression.type=lz4'\n) \nFORMAT USING DSVFormatter ()\nINPUT FROM sourcestream;\n\nalter application app5 recompile;\ndeploy application app5;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@  RECOVERY 1 SECOND INTERVAL;\nCREATE SOURCE @srcName@ USING OracleReader (\n Username: '@srcusername@',\n  Password: '@srcpassword@',\n  ConnectionURL: '@srcurl@',\n  Tables: '@srcschema@.@srctable@'\n)\nOUTPUT TO @outstreamname@;\n\nCREATE TARGET @tgtName@ USING Global.DeltaLakeWriter (\n  personalAccessToken:'@tgtpassword@',\n  hostname:'@tgthostname@',\n  stageLocation:'/',\n  Mode:'MERGE',\n  AuthenticationType: 'PersonalAccessToken',\n  Tables:'@srcschema@.@srctable@,@tgtschema@.@tgttable@',\n  adapterName:'DeltaLakeWriter',\n  personalAccessToken_encrypted:'false',\n  optimizedMerge:'false',\n  uploadPolicy:'eventcount:1,interval:10s',\n  connectionUrl:'@tgturl@',\n  IgnorableExceptionCode:'TABLE_NOT_FOUND',\n  externalStageType:'DBFSROOT'\n)\nINPUT FROM @instreamname@;\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application admin.SampleApp;\nundeploy application admin.SampleApp;\ndrop application admin.SampleApp cascade;\n\n\nCREATE APPLICATION SampleApp RECOVERY 10 SECOND INTERVAL;\n\nCREATE SOURCE Oracle_Src USING Global.OracleReader (\n  Tables: 'QATEST.TEST01',\n  TransactionBufferDiskLocation: '.striim/LargeBuffer',\n  OutboundServerProcessName: 'WebActionXStream',\n  Password: 'qatest',\n  Compression: false,\n  ReaderType: 'LogMiner',\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  FetchSize: 1,\n  Password_encrypted: 'false',\n  SupportPDB: false,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CDDLAction: 'Process',\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  DictionaryMode: 'OnlineCatalog',\n  QueueSize: 2048,\n  CommittedTransactions: true,\n  XstreamTimeOut: 600,\n  CDDLCapture: false,\n  TransactionBufferType: 'Disk',\n  Username: 'qatest',\n  TransactionBufferSpilloverSize: '100MB',\n  FilterTransactionBoundaries: true,\n  SendBeforeImage: true,\n  DatabaseRole: 'Primary' )\nOUTPUT TO Striim_Buffer;\n\nCREATE TARGET Oracle_tgt USING Global.DatabaseWriter (\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'qatest',\n  ParallelThreads: '',\n  DatabaseProviderType: 'Oracle',\n  CheckPointTable: 'CHKPOINT',\n  Password_encrypted: 'false',\n  CDDLAction: 'Process',\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.TEST01,QATEST.TEST02',\n  CommitPolicy: 'EventCount:1,Interval:10',\n  StatementCacheSize: '50',\n  Username: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:10',\n  PreserveSourceTransactionBoundary: 'false' )\nINPUT FROM Striim_Buffer;\n\nEND APPLICATION SampleApp;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSVSource using FileReader (\ndirectory:'@TEST-DATA-PATH@',\nWildCard:'smallposdata.csv',\npositionByEOF:false,\ncharset:'UTF-8'\n)\nparse using DSVParser (\nheader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\nfilename:'Events',\ndirectory:'@FEATURE-DIR@/logs/',\nrolloverpolicy:'eventcount:200,sequence:00'\n)\nformat using DSVFormatter (\n\n)\ninput from TypedCSVStream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetEventCount_actual.log') input from TypedCSVStream;\n\nend application DSV;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@;\n\n\nCREATE OR REPLACE SOURCE @APPNAME@CDC_Source1 USING OracleReader( \n  StartTimestamp: 'null',\n  SupportPDB: false,\n  TransactionBufferDiskLocation: '.striim/LargeBuffer',\n  FetchSize: 1,\n  DDLCaptureMode: 'All',\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.EMP',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  TransactionBufferType: 'Memory',\n  DictionaryMode: 'OnlineCatalog',\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  StartSCN: 'null',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  TransactionBufferSpilloverSize: '1MB',\n  compression: true,\n  OutboundServerProcessName: 'WebActionXStream'\n ) \nOUTPUT TO @APPNAME@AppStream1;\n\n\nCREATE OR REPLACE TARGET @APPNAME@sap_target USING DatabaseWriter( \n  DatabaseProviderType:'SAPHANA',\n  ConnectionRetryPolicy: 'retryInterval=30,maxRetries=3',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'SYSTEM',\n  Password_encrypted: 'false',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:sap://10.77.21.116:39013/?databaseName=striim&currentSchema=QA',\n  Tables: 'waction.crash_type,QA.CRASH_TYPES',\n  adapterName: 'DatabaseWriter',\n  --IgnorableExceptionCode: '',\n  Password: 'Striim_SAP@123'\n ) \nINPUT FROM @APPNAME@AppStream1;\n\n\ncreate or replace target @APPNAME@sys_tgt using sysout(\nname:Foo2\n)input from @APPNAME@AppStream1;\n\nEND APPLICATION @APPNAME@;\n\ndeploy application @APPNAME@;\nstart application @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop Oracle_IRLogWriter;\nUndeploy application Oracle_IRLogWriter;\ndrop application Oracle_IRLogWriter cascade;\nCREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;\ncreate flow AgentFlow;\nCREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( \n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.119/DBS_PORT=1025',\n  Tables: 'striim.upgrade01',\n  CheckColumn:'striim.upgrade01=t1',\n  startPosition:'striim.upgrade01=2018-09-20 06:43:59',\n  ReturnDateTimeAs:'string'\n  )\nOUTPUT TO data_stream1;\nend flow AgentFlow;\n\ncreate flow serverFlow;\ncreate target AzureSQLDWHTarget using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test26,dbo.test26',\n        uploadpolicy:'eventcount:10000,interval:10s'\n) INPUT FROM data_stream1;\n\nend flow serverFlow;\nEND APPLICATION Oracle_IRLogWriter;\ndeploy application Oracle_IRLogWriter with AgentFlow in Agents, ServerFlow in default;\nstart application Oracle_IRLogWriter;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@;\n\nCREATE OR REPLACE SOURCE CCBReader USING FileReader (\n  wildcard: '@WILDCARD@',\n  positionbyeof: false,\n  directory: '@TESTDIR@'\n  )\nPARSE USING Global.CobolCopybookParser (\n  copybookDialect: 'Mainframe',\n  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',\n  GroupPolicy: '@GRPPOLICY@',\n  TimeoutPolicy: '1s',\n  dataFileOrganization: 'FixedLength',\n  ProcessCopyBookFileAs: 'MultipleEvents',\n  skipIndent: 0,\n  dataFileFont: 'utf8',\n  CopybookFileFormat: 'USE_STANDARD_COLUMNS',\n  copybookSplit: 'None',\n  copybookFileName: '@CCBFILE@'\n   )\nOUTPUT TO CCBStream;\n\nCREATE TARGET MongoTarget USING MongoDBWriter (\n  Username: '',\n  AuthDB: 'admin',\n  ConnectionURL: '',\n  batchpolicy: 'EventCount:10, Interval:30',\n  Password: '',\n  collections: '' )\nINPUT FROM CCBStream;\n\ncreate Target BlobTarget using AzureBlobWriter(\n\taccountname:'',\n\taccountaccesskey:'',\n\tcontainername:'',\n    blobname:'',\n\tfoldername:'',\n\tuploadpolicy:'EventCount:10,Interval:30s'\n)\nformat using JSONFormatter (\n)\nINPUT FROM CCBStream;\n\n\nCREATE OR REPLACE TARGET GCSTarget USING GCSWriter (\n    bucketname:'',\n    objectname:'',\n    projectId:'',\n    uploadPolicy:'EventCount:10,Interval:30s'\n)\nformat using JSONFormatter ()\nINPUT FROM CCBStream;\n\ncreate Target KafkaTarget using KafkaWriter VERSION @KAFKAVERSION@ (\nbrokerAddress:'',\nTopic:'',\nMode: 'Sync',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;linger.ms=30000'\n)\nFORMAT USING JSONFormatter (\nmembers:'data')\ninput from CCBStream;\n\nCREATE TARGET S3Target USING Global.S3Writer (\n  bucketname: '',\n  objectname: '',\n  uploadpolicy: 'EventCount:10,Interval:30s' )\nFORMAT USING Global.JSONFormatter  ()\nINPUT FROM CCBStream;\n\ncreate source KafkaSource using KafkaReader VERSION @KAFKAVERSION@(\nbrokerAddress:'',\n\tTopic:''\n)\nparse using JSONParser ()\noutput to KafkaStream;\n\nCREATE OR REPLACE TARGET JSONWriter USING FileWriter (\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:10,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING JSONFormatter  ()\nINPUT FROM KafkaStream;\n\n\nend application @APPNAME@;\ndeploy application @APPNAME@ on all in default;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP AdhocTester.ws_one;\nUNDEPLOY APPLICATION AdhocTester.ws_one;\nDROP APPLICATION AdhocTester.ws_one cascade;\n\nCREATE APPLICATION ws_one;\n\nCREATE SOURCE wsSource USING CSVReader  ( \n  blocksize: 10240,\n  charset: 'UTF-8',\n  positionByEOF: false,\n  columndelimiter: ',',\n  directory: '@TEST-DATA-PATH@',\n  eofdelay: 100,\n  wildcard: 'sampleByData.csv',\n  expected_column_count: 0,\n  rowdelimiter: '\\n',\n  header: true,\n  adapterName: 'CSVReader',\n  quoteset: '\\\"',\n  trimquote: true,\n  skipbom: true\n ) \nOUTPUT TO QaStream ;\n\nCREATE TYPE sampleS_Type  ( vID java.lang.Integer KEY, \nvInt java.lang.Integer , \nvDouble java.lang.Double , \nvLong java.lang.Long , \nvShort java.lang.Short , \nvFloat java.lang.Float  \n ) ;\n\nCREATE STREAM sampleS OF sampleS_Type;\n\nCREATE CQ csvTowsData \nINSERT INTO sampleS\nSELECT  to_int(data[0]) as vID,\nto_int(data[1]) as vInt,\nto_double(data[1]) as vDouble,\nto_long(data[1]) as vLong,\nto_short(data[1]) as vShort,\nto_float(data[1]) as vFloat\nFROM QaStream\n;\n\nCREATE WACTIONSTORE oneWS  CONTEXT OF sampleS_Type\nEVENT TYPES(sampleS_Type )\n@PERSIST-TYPE@\n\nCREATE CQ wsToWaction \nINSERT INTO oneWS\nSELECT * FROM sampleS s\nLINK SOURCE EVENT;\n\nEND APPLICATION ws_one;\nDEPLOY APPLICATION ws_one on any in default;\nSTART ws_one;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE FLOW @STREAM@_SourceFlow;\n\nCREATE SOURCE @SOURCE_NAME@ USING MySQLReader (\n Username: '@READER-UNAME@',\nPassword: '@READER-PASSWORD@',\nConnectionURL: '@CDC-READER-URL@',\nTables: @WATABLES@,\nsendBeforeImage:'true',\nFilterTransactionBoundaries: 'true'\n) OUTPUT TO @STREAM@;\n\nEND FLOW @STREAM@_SourceFlow;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application MSSQLTransactionSupportFTMFTBTrue;\nundeploy application MSSQLTransactionSupportFTMFTBTrue;\ndrop application MSSQLTransactionSupportFTMFTBTrue cascade;\n\nCREATE APPLICATION MSSQLTransactionSupportFTMFTBTrue recovery 1 second interval;\n\nCreate Source ReadFromMSSQL2\nUsing MSSqlReader\n(\nUsername:'@READER-NAME@',\nPassword:'@READER-PASSWORD@',\nDatabaseName:'@SRCDB-NAME@',\nConnectionURL:'@CONN-URL@',\nTables:'@WATABLES-SRC@',\nTransactionSupport: 'true',\nFetchTransactionMetadata:'true',\nFilterTransactionBoundaries: true,\nCompression:'false',\nConnectionPoolSize:1\n)\nOutput To MSSQLTransactionSupportFTMFTBTrueStream;\n\n\nCREATE TARGET WriteToMSSQL2 USING DatabaseWriter(\nConnectionURL:'@TGT-URL@',\nUsername:'@WRITER-UNAME@',\nPassword:'@WRITER-PASSWORD@',\nBatchPolicy:'EventCount:5,Interval:5',\nCommitPolicy:'EventCount:1,Interval:1',\nTables: '@WATABLES-SRC@,@WATABLES-TGT@'\n)\nINPUT FROM MSSQLTransactionSupportFTMFTBTrueStream;\n\nCREATE TARGET MSSqlReaderOutput2 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportFTMFTBTrueStream; \n\n\nCREATE OR REPLACE TARGET MSSQLFileOut2 USING FileWriter  ( \n  rolloverpolicy: 'EventCount:10000,Interval:30s',\n  adapterName: 'FileWriter',\n  directory:'@FEATURE-DIR@/logs/',\n  filename: 'TransactionSupportFTMFTBTrue.txt'\n ) \nFORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true',\n  jsonobjectdelimiter: '\\n'\n ) \nINPUT FROM MSSQLTransactionSupportFTMFTBTrueStream;\n\nEND APPLICATION MSSQLTransactionSupportFTMFTBTrue;\ndeploy application MSSQLTransactionSupportFTMFTBTrue;\nstart application MSSQLTransactionSupportFTMFTBTrue;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME1@;\nundeploy application @APPNAME1@;\nstop application @APPNAME2@;\nundeploy application @APPNAME2@;\ndrop application @APPNAME1@ cascade;\ndrop application @APPNAME2@ cascade;\n\n\nCREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\nCREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;\nCREATE OR REPLACE SOURCE @SourceName@ Using PostgreSQLReader\n(\n   adapterName: PostgreSQLReader,\n   CDDLAction: Quiesce_Cascade,\n   CDDLCapture: true,\n   CDDLTrackingTable:'striim.ddlcapturetable',\n   ConnectionURL: jdbc:postgresql://localhost:5432/qatest,\n   FilterTransactionBoundaries: true,\n   Password: w@ct10n,\n   ReplicationSlotName:'test_slot',\n   Tables: public.sample,\n   Username: sa,\n)OUTPUT TO @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME1@;\nDEPLOY APPLICATION @APPNAME1@;\nSTART APPLICATION @APPNAME1@;\n\nCREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  \n(\n  DatabaseProviderType:'Default',\n  CheckPointTable:'CHKPOINT',\n  PreserveSourceTransactionBoundary:'false',\n  Username:'qatest',\n  BatchPolicy:'EventCount:1',\n  CommitPolicy:'EventCount:1',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',\n  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',\n  Password:'qatest',\n  CDDLAction:'Process'\n) \nINPUT FROM @SRCINPUTSTREAM@;\n\nEnd APPLICATION @APPNAME2@;\nDEPLOY APPLICATION @APPNAME2@;\nSTART APPLICATION @APPNAME2@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\nDROP EXCEPTIONSTORE @APP_NAME@_EXCEPTIONSTORE;\n\nCREATE APPLICATION @APP_NAME@ WITH ENCRYPTION RECOVERY 2 SECOND INTERVAL USE EXCEPTIONSTORE;\n\nCREATE SOURCE @APP_NAME@_Source USING @SOURCE_ADAPTER@(\n)OUTPUT TO @APP_NAME@DataStream;\n\n\nCREATE TARGET @APP_NAME@_Target USING @TARGET_ADAPTER@( \n) INPUT FROM @APP_NAME@DataStream;\n\n\nCREATE OR REPLACE TARGET @APP_NAME@_SysOut USING Global.SysOut(\n\tname: 'waEvent'\n) INPUT FROM @APP_NAME@DataStream;\n\nEND APPLICATION @APP_NAME@;\n\nDEPLOY APPLICATION @APP_NAME@ in default;\nSTART APPLICATION @APP_NAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\nCREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE Source @SourceName@ USING Global.MSSqlReader (\n  PollingInterval: 5,\n  FetchTransactionMetadata: false,\n  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  TransactionSupport: true,\n  StartPosition: 'NOW',\n  Tables: 'dbo.sourceTable',\n  adapterName: 'MSSqlReader',\n  Password: 'w3b@ct10n',\n  ConnectionPoolSize: 10,\n  Compression: true,\n  cdcRoleName: 'STRIIM_READER',\n  DatabaseName: 'qatest',\n  Username: 'qatest',\n  FetchSize: 0,\n  IntegratedSecurity: false,\n  FilterTransactionBoundaries: true,\n  SendBeforeImage: true,\n  AutoDisableTableCDC: false )\nOUTPUT TO @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetsys@ USING Global.SysOut (\n  name: 'sysout' )\nINPUT FROM @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING Global.DatabaseWriter (\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',\n  BatchPolicy: 'EventCount:10000,Interval:30',\n  CheckPointTable: 'CHKPOINT',\n  CDDLAction: 'Process',\n  CommitPolicy: 'EventCount:10000,Interval:100',\n  StatementCacheSize: '50',\n  Password: 'w3b@ct10n',\n  Username: 'qatest',\n  IgnorableExceptionCode: '547,DUPLICATE_ROW_EXISTS,NO_OP_UPDATE,NO_OP_DELETE,NO_OP_PKUPDATE',\n  DatabaseProviderType: 'SQLServer',\n  PreserveSourceTransactionBoundary: 'false',\n  Tables: 'dbo.sourceTable,dbo.targetTable',\n  VendorConfiguration: 'enableIdentityInsert=true',\n  adapterName: 'DatabaseWriter' )\nINPUT FROM @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCreate Source @SOURCE@\n        Using OracleReader\n(\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: 'qatest.test77',\n  FetchSize:1,\n  QueueSize:2000,\n  CommittedTransactions:true,\n  Compression:false\n)\nOutput To @STREAM@;\n\ncreate Target @TARGET@ using AzureblobWriter(\n    accountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tfoldername:'@FOLDER@',\n\tuploadpolicy:'EventCount:7'\n)\nformat using DSVFormatter (\n)\ninput from @STREAM@;\n\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create Target @TARGET_NAME@ using ADLSGen2Writer(\n          accountname:'adlsgen2nntest',\n        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',\n        filesystemname:'qatestadls2',\n        directory:'%@metadata(TableName)%',\n        filename:'table.csv',\n        uploadpolicy:'filesize:10M'\n)\nFORMAT USING dsvFormatter()\ninput from @STREAM@; "}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 8\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> WS\n--\n\nSTOP Recov8Tester.RecovTest8;\nUNDEPLOY APPLICATION Recov8Tester.RecovTest8;\nDROP APPLICATION Recov8Tester.RecovTest8 CASCADE;\nCREATE APPLICATION RecovTest8 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE WactionType (\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions\nINSERT INTO Wactions\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nEND APPLICATION RecovTest8;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@1;\nSTOP APPLICATION @APP_NAME@2;\n\nUNDEPLOY APPLICATION @APP_NAME@1;\nUNDEPLOY APPLICATION @APP_NAME@2;\n\nDROP APPLICATION @APP_NAME@1 CASCADE;\nDROP APPLICATION @APP_NAME@2 CASCADE;\n\nCREATE APPLICATION @APP_NAME@1;\n\nCREATE OR REPLACE SOURCE @APP_NAME@1_Source USING @SOURCE_ADAPTER@  ( \n  Tables:'@SRC_TABLE@'\n) OUTPUT TO @APP_NAME@_In1;\n\nCREATE OR REPLACE TARGET @APP_NAME@1_Target USING @TARGET_ADAPTER@  ( \n  Tables: '@SRC_TABLE@, @TGT_TABLE@1', \n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', \n  CheckPointTable: '@CHK_TABLE@', \n) INPUT FROM @APP_NAME@_In1;\n\nCREATE TARGET @APP_NAME@_SysOut1\nUSING SysOut(name:@APP_NAME@1Sys)\nINPUT FROM @APP_NAME@_In1;\n\nEND APPLICATION @APP_NAME@1;\n\nDEPLOY APPLICATION @APP_NAME@1;\nSTART APPLICATION @APP_NAME@1;\n\n\nCREATE APPLICATION @APP_NAME@2;\n\nCREATE OR REPLACE SOURCE @APP_NAME@2_Source USING @SOURCE_ADAPTER@  ( \n  Tables:'@SRC_TABLE@'\n) OUTPUT TO @APP_NAME@_In2;\n\nCREATE OR REPLACE TARGET @APP_NAME@2_Target USING @TARGET_ADAPTER@  ( \n  Tables: '@SRC_TABLE@, @TGT_TABLE@2', \n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', \n  CheckPointTable: '@CHK_TABLE@'\n) INPUT FROM @APP_NAME@_In2;\n\nCREATE TARGET @APP_NAME@_SysOut2\nUSING SysOut(name:@APP_NAME@2Sys)\nINPUT FROM @APP_NAME@_In2;\n\nEND APPLICATION @APP_NAME@2;\n\nDEPLOY APPLICATION @APP_NAME@2;\nSTART APPLICATION @APP_NAME@2;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop ROLLUPMON_IL;\nundeploy application ROLLUPMON_IL;\nalter application ROLLUPMON_IL;\nCREATE or replace FLOW ROLLUPMON_IL_flow;\nCreate or replace Source ROLLUPMON_IL_Oraclesrc Using databasereader(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',\n Tables:'QATEST.ROLLUPMON_TABLE1;QATEST.ROLLUPMON_TABLE2;QATEST.ROLLUPMON_TABLE3;QATEST.ROLLUPMON_TABLE4;QATEST.ROLLUPMON_TABLE5',\n _h_fetchexactrowcount: 'true',\nFetchSize:1000\n)\nOutput To ROLLUPMON_IL_OrcStrm;\nEND FLOW ROLLUPMON_IL_flow;\nalter application ROLLUPMON_IL recompile;\ndeploy application ROLLUPMON_IL;\nstart application ROLLUPMON_IL;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application FileWriterTest;\nundeploy application FileWriterTest;\ndrop application FileWriterTest cascade;\ncreate application FileWriterTest recovery 1 second interval;\ncreate source File_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tcharset:'@charset@'\n)\nparse using AvroParser (\n\tschemaFileName:'@fname@',\n\tschemaRegistryURI:'@evty@'\n)\nOUTPUT TO CsvStream;\n/*\ncreate Target FileTarget using FileWriter(\n    rolloverpolicy:'eventcount:100',\n    filename :'@FILE@',\n    directory : '@FOLDER@'\n)\nformat using AvroFormatter (\nschemaFileName:'@fname@'\n)\ninput from CsvStream;\n*/\nend application FileWriterTest;\ndeploy application FileWriterTest on all in default;\nstart application FileWriterTest;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop DataGenSampleApp;\nundeploy application DataGenSampleApp;\ndrop application DataGenSampleApp cascade;\n\n\nCREATE APPLICATION DataGenSampleApp;\n\nCreate Source dataGenSrc Using MSSqlReader\n(\n Username:'qatest',\n Password:'w3b@ct10n',\n DatabaseName:'qatest',\n ConnectionURL:'localhost:1433',\n Tables:'@tableNames@', \n ConnectionPoolSize:1,\n FetchSize:1,\n StartPosition:'EOF'\n )\n Output To LCRStream;\n\ncreate Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;\n\nEND APPLICATION DataGenSampleApp;\n\ndeploy application DataGenSampleApp;\nstart  application DataGenSampleApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application BigqueryBulkLoadMonMetrics_cdc;\nundeploy application BigqueryBulkLoadMonMetrics_cdc;\ndrop application BigqueryBulkLoadMonMetrics_cdc cascade;\n\nCREATE APPLICATION BigqueryBulkLoadMonMetrics_cdc;\n\nCREATE FLOW BigqueryBulkLoadMonMetrics_cdc_SourceFlow;\n\nCREATE SOURCE BigqueryBulkLoadMonMetrics_cdc_DBSource USING oracleReader ( \n  Username: 'qatest', \n  FetchSize: 10000, \n  Password_encrypted: 'false', \n  Password: 'JVaLv3ZpgQDY8R2ZxS38xg==', \n  Tables: 'QATEST.EMPLOYEE', \n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe' ) \nOUTPUT TO BigqueryBulkLoadMonMetrics_cdc_OutputStream;\n\nEND FLOW BigqueryBulkLoadMonMetrics_cdc_SourceFlow;\n\nCREATE OR REPLACE TARGET BigqueryBulkLoadMonMetrics_cdc_BigQueryTarget1 USING BigQueryWriter ( \n  ColumnDelimiter: '|', \n  NullMarker: 'NULL', \n  streamingUpload: 'false', \n  projectId: 'striimqa-214712', \n  Encoding: 'UTF-8', \n  BatchPolicy: 'eventCount:100000, Interval:90', \n   AllowQuotedNewLines: 'false', \n  optimizedMerge: 'false', \n  TransportOptions: 'connectionTimeout=300, readTimeout=120', \n  adapterName: 'BigQueryWriter', \n  Mode: 'MERGE', \n  StandardSQL: 'true', \n  includeInsertId: 'true', \n  QuoteCharacter: '\\\"', \n  ServiceAccountKey: '/Users/jenniffer/Product2/IntegrationTests/TestData/google-gcs.json' ) \nINPUT FROM BigqueryBulkLoadMonMetrics_cdc_OutputStream;\n\nEND APPLICATION BigqueryBulkLoadMonMetrics_cdc;\n\ndeploy application BigqueryBulkLoadMonMetrics_cdc;\nstart application BigqueryBulkLoadMonMetrics_cdc;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION EnvvarTester.envVar;\nUNDEPLOY APPLICATION EnvvarTester.envVar;\nDROP APPLICATION EnvvarTester.envVar CASCADE;\n\nCREATE APPLICATION envVar;\n\n\nCREATE SOURCE AccessLogSource USING FileReader (\ndirectory:'@TEST-DATA-PATH@/envVar',\nwildcard:'$FILENAME',\nblocksize: $BLOCKSIZE,\npositionByEOF:false\n)\nPARSE USING DSVParser (\ncolumndelimiter:' ',\nignoreemptycolumn:'Yes',\nquoteset:'[]~\"',\nseparator:'~'\n)\nOUTPUT TO RawAccessStream;\n\n\nEND APPLICATION envVar;\nDEPLOY APPLICATION envVar on any in default;\nSTART envVar;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@;\nCREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (\n  ConnectionURL: '@CONN_URL@',\n  Tables: '@TABLES@',\n  Username: '@USERNAME@',\n  Password: '@PASSWORD@'\n) OUTPUT TO @APP_NAME@_Stream;\n\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.FabricDataWarehouseWriter (\n  Tables: '@TABLES@',\n  ConnectionURL: '@CONN_URL@',\n  Username: '@USERNAME@',\n  Password: '@PASSWORD@',\n  uploadpolicy: 'eventcount:1',\n  AccountName: '@ACCOUNTNAME@')\nINPUT FROM @APP_NAME@_Stream;\n\nEND APPLICATION @APP_NAME@;\nDEPLOY APPLICATION @APP_NAME@;\nSTART APPLICATION @APP_NAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION @AppName@;\n\nCreate Source @AppName@_source Using OracleReader(\n  Username:'@username@',\n  Password:'@password@',\n  ConnectionURL:'@url@',\n  Tables:'@srctableName@',\n  Fetchsize:1\n)\nOutput To @AppName@_Stream;\n\n\nCREATE OR REPLACE TARGET @AppName@_DB_Target USING Global.DeltaLakeWriter (\nconnectionProfileName: 'admin.@DBCP@',\n   useConnectionProfile: 'true',\n  Tables: '@srctableName@,@trgtableName@',\n  uploadPolicy: 'eventcount:100000,interval:60s'\n)\n\nINPUT FROM @AppName@_Stream;\nEND APPLICATION @AppName@;\nDeploy application @AppName@;\nStart application @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION FileReaderWithBinaryParserApp;\nUNDEPLOY APPLICATION FileReaderWithBinaryParserApp;\nDROP APPLICATION FileReaderWithBinaryParserApp cascade;\n\nCREATE APPLICATION FileReaderWithBinaryParserApp;\n\ncreate source BinarySource using FileReader (\n  directory:'@TEST-DATA-PATH@/binary',\n  wildcard:'10rows.bin',\n  positionByEOF:false\n)\nparse using BinaryParser (\n  metadata:'@TEST-DATA-PATH@/binary/metadata.json',\n  endian:true,\n  StringTerminatedByNull:false\n)\nOUTPUT TO BinaryStream;\n\ncreate Target DSVDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/binaryDetails') input from BinaryStream;\nCREATE TARGET RawOut using SysOut(name: TCPRaw) INPUT FROM BinaryStream;\n\n\nEND APPLICATION FileReaderWithBinaryParserApp;\ndeploy application FileReaderWithBinaryParserApp;\nstart application FileReaderWithBinaryParserApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application dev15823;\nundeploy application dev15823;\ndrop application dev15823 cascade;\nCREATE APPLICATION dev15823 RECOVERY 1 SECOND INTERVAL;\n\nCREATE  SOURCE OracleSource USING OracleReader  (\n  Username: '@USERNAME@',\n  Password: '@PASSWORD@',\n  ConnectionURL: '@URL@',\n  Tables: '@source-table@',\n  FetchSize: 1\n )\nOUTPUT TO LogminerStream;\n\n--Create or replace Target test using SysOut (name:test) input from MySQLTestStream;\n\nCREATE OR REPLACE TARGET WriteCDCMySQL USING DatabaseWriter  (\n  Username: '@USERNAME@',\n  BatchPolicy: 'Eventcount:5,Interval:300',\n  CommitPolicy: 'Eventcount:5,Interval:300',\n  ConnectionURL: '@URL@',\n  Tables: '@TABLES@',\n  Checkpointtable: 'CHKPOINT',\n  Password: '@PASSWORD@'\n )\nINPUT FROM LogminerStream;\n\nEND APPLICATION dev15823;\ndeploy application dev15823;\nstart dev15823;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP AdhocTester.ws_one;\nUNDEPLOY APPLICATION AdhocTester.ws_one;\nDROP APPLICATION AdhocTester.ws_one cascade;\n\nCREATE APPLICATION ws_one;\n\nCREATE source wsSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'posdata.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n) OUTPUT TO QaStream;\n\nCREATE TYPE USAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\nCREATE CACHE ZipLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'AdhocQueryData.csv',\n  header: Yes,\n  columndelimiter: '\t',\n  trimquote:false\n) QUERY (keytomap:'zip') OF USAddressData;\n\nCreate TYPE wsData(\n\tCompanyNum String,\n\tCompanyName String KEY,\n\tCompanyCode int,\n\tZip String\n);\n\n\nCREATE STREAM wsStream OF wsData;\n\n--Select data from QaStream and insert into wsStream\n\nCREATE CQ csvTowsData\nINSERT INTO wsStream\nSELECT  data[0],\n    data[1],\n    TO_INT(data[3]),\n    data[9]\n FROM QaStream;\n\n\n\n\nCREATE WACTIONSTORE oneWS CONTEXT OF wsData\nEVENT TYPES(wsData )\n@PERSIST-TYPE@\n\n\nCREATE CQ wsToWaction\nINSERT INTO oneWS\nSELECT * FROM wsStream\nLINK SOURCE EVENT;\n\nEND APPLICATION ws_one;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet\n(\n  ConnectionURL: '@CONNECTION_URL@',\n  CommittedTransactions: true,\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OJet',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  Username: '@SOURCE_USER@'\n)OUTPUT TO @STREAM@;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OJet\n(\n  ConnectionURL: '@CONNECTION_URL@',\n  CommittedTransactions: true,\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OJet',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  Username: '@SOURCE_USER@'\n)OUTPUT TO @STREAM@2;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "-- Creating a namespace ensures there won't be conflicts with the regular version of\n-- PosApp. The only difference between this version and the regular version is\n-- that the CQ that parses the source stream includes a PAUSE clauses that introduces a\n-- 40-millisecond pause after each event is read, simulating the way the dashboard would\n-- work with real-time data.\nStop PosAppOracle.PosAppOracle;\nundeploy application PosAppOracle.PosAppOracle;\ndrop application PosAppOracle.PosAppOracle cascade;\n\n\n-- The PosApp sample application demonstrates how a credit card\n-- payment processor might use WebAction to generate reports on current\n-- transaction activity by merchant and send alerts when transaction\n-- counts for a merchant are higher or lower than average for the time\n-- of day.\n\nCREATE Application PosAppOracle;\n\n-- All CREATE statements between here and the END APPLICATION\n-- statement will create objects in the PosApp application.\n\n-- source CsvDataSource\n\nCREATE source CsvDataSource USING OracleReader (\nOnlineCatalog:true,\nFetchSize:1000,\nQueueSize:2048,\nCommittedTransactions:false,\nCompression:false,\nUsername:'@LOGMINER-UNAME@',\nPassword:'@LOGMINER-PASSWORD@',\nConnectionURL:'@LOGMINER-URL@',\nTables:'@LOGMINER-SCHEMA@.POSDATA',\nOnlineCatalog:true\n) output to CsvStream;\n\n\n-- This is the primary data source for this application.\n-- In a real-world application, it would be real-time data. Here,\n-- the data comes from a comma-delimited file, posdata.csv. The first\n-- two lines of that file are:\n--\n-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY\n-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand\n--\n-- The \"header:Yes\" setting tells WebAction that the first line contains\n-- field labels that should not be treated as data.\n--\n-- The \"positionByEOF:false\" setting tells WebAction to start reading\n-- from the beginning of the file. (In a real-world application\n-- reading real log files, you would typically use the default \"true\"\n-- setting so that the application would read only new data.)\n--\n-- The OUTPUT TO clause automatically creates the stream\n-- CsvStream using the WAEvent type associated with the CSVReader\n-- adapater. The only field from WAEvent used by this application\n-- is \"data\", an array containing the delimited fields.\n\n\n-- CQ CsvToPosData\n--\n-- A stream's type must be declared before the stream, and a CQ's\n-- output stream must be defined before the CQ. Hence type-stream-CQ\n-- sequences like the following are very common.\n\nCREATE CQ CsvToPosData\nINSERT INTO PosDataStream\nSELECT TO_STRING(data[1]) as merchantId,\n       TO_DATE(data[4]) as dateTime,\n       DHOURS(TO_DATE(data[4])) as hourValue,\n       TO_DOUBLE(data[7]) as amount,\n       To_String(data[9]) as zip\nFROM CsvStream c;\n\n-- Here, \"data\" refers to the array mentioned above, and the number\n-- in brackets specifies a field from the array, counting from zero.\n-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH\n-- AMOUNT, and data[9] is ZIP. These correspond to the merchantId,\n-- dateTime, hourValue, amount, and zip fields in PosDataStream, as\n-- defined by the PosData type.\n--\n-- The DATETIME field from the source is converted to both a DateTime\n-- value, used as the event timestamp by the application, and an int,\n-- which is used to look up historical hourly averages from the\n-- HourlyAveLookup cache, discussed below.\n--\n-- The other six fields are discarded. Thus the first line of data\n-- from posdata.csv has at this point been reduced to five values:\n--\n-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)\n-- 20130312173210 (DateTime)\n\n-- 2.20 (amount)\n-- 41363 (zip)\n\n\n-- CQ GenerateMerchantTxRateOnly\n--\n-- The PosData5Minutes window bounds the data so that the query can\n-- use the COUNT, FIRST, and SUM functions and join data from the\n-- HourlyAveLookup cache. (Aggregate functions cannot be used and\n-- joins cannot be performed on unbound real-time data.)\n--\n-- The HourlyAveLookup cache provides historical average sales\n-- amounts for the current hour for each merchant.\n\nCREATE JUMPING WINDOW PosData5Minutes\nOVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE TYPE MerchantHourlyAve(\n  merchantId String,\n  hourValue int,\n  hourlyAve int\n);\n\nCREATE CACHE HourlyAveLookup using DatabaseReader (\n        ConnectionURL:'@READER-URL@',\n        Username:'@READER-UNAME@',\n        Password:'@READER-PASSWORD@',\n        Table:'@READER-SCHEMA@.HOURLYDATA',\n        FetchSize:12000\n) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;\n\nCREATE CQ GenerateMerchantTxRateOnly\nINSERT INTO MerchantTxRateOnlyStream\nSELECT p.merchantId as merchantId,\n       p.zip as zip,\n       FIRST(p.dateTime) as startingTime,\n       COUNT(p.merchantId) as count,\n       SUM(p.amount) as totalAmount,\n       l.hourlyAve/12 as hourlyAve,\n       l.hourlyAve/12 * CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END as upperLimit,\n       l.hourlyAve/12 / CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END as lowerLimit,\n       '<NOTSET>' as category,\n       '<NOTSET>' as status\nFROM PosData5Minutes p, HourlyAveLookup l\nWHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue\nGROUP BY p.merchantId;\n\n-- This query aggregates five minutes' worth of data for each\n-- merchant, calculating the total transaction count and amount, and\n-- calculates the upperLimit and lowerLimit values based on the\n-- historical average transaction count for the current hour of the\n-- day from the HourlyAveLookup cache. The category and status fields\n-- are left unset to be populated by the next query.\n\n\n-- CQ GenerateMerchantTxRateWithStatus\n--\n-- This query sets the count values used by the Dashboard map and the\n-- status values used to trigger alerts.\n\nCREATE CQ GenerateMerchantTxRateWithStatus\nINSERT INTO MerchantTxRateWithStatusStream\nSELECT merchantId as merchantId,\n       zip as zip,\n       startingTime as startingTime,\n       count as count,\n       totalAmount as totalAmount,\n       hourlyAve as hourlyAve,\n       upperLimit as upperLimit,\n       lowerLimit as lowerLimit,\n       CASE\n         WHEN count >10000 THEN 'HOT'\n         WHEN count > 800 THEN 'WARM'\n         WHEN count >200 THEN 'COOL'\n         ELSE 'COLD' END as category,\n       CASE\n         WHEN count > upperLimit THEN 'TOOHIGH'\n         WHEN count < lowerLimit THEN 'TOOLOW'\n         ELSE 'OK' END as status\nFROM MerchantTxRateOnlyStream;\n\n\n-- WAction store MerchantActivity\n--\n-- The following group of statements create and populate the MerchantActivity\n-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced\n-- with merchant details from NameLookup cache and with latitude and longitude\n-- values from the USAddressData cache.\n\nCREATE TYPE MerchantActivityContext(\n  MerchantId String KEY,\n  StartTime  DateTime,\n  CompanyName String,\n  Category String,\n  Status String,\n  Count int,\n  HourlyAve int,\n  UpperLimit double,\n  LowerLimit double,\n  Zip String,\n  City String,\n  State String,\n  LatVal double,\n  LongVal double\n);\nCREATE TYPE MerchantTxRate(\n  merchantId String KEY,\n  zip String,\n  startingTime DateTime,\n  count int,\n  totalAmount double,\n  hourlyAve int,\n  upperLimit double,\n  lowerLimit double,\n  category String,\n  status String\n);\n\nCREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext\nEVENT TYPES ( MerchantTxRate )\n@PERSIST-TYPE@\n\nCREATE TYPE MerchantNameData(\n  merchantId String KEY,\n  companyName String\n);\n\nCREATE TYPE USAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\nCREATE CACHE NameLookup using DatabaseReader (\n        ConnectionURL:'@READER-URL@',\n        Username:'@READER-UNAME@',\n        Password:'@READER-PASSWORD@',\n        Table:'@READER-SCHEMA@.MERCHANTNAMES',\n        FetchSize:12000\n) QUERY (keytomap:'merchantId') OF MerchantNameData;\n\nCREATE CACHE ZipLookup using DatabaseReader (\n        ConnectionURL:'@READER-URL@',\n        Username:'@READER-UNAME@',\n        Password:'@READER-PASSWORD@',\n        Table:'@READER-SCHEMA@.USADDRESSES',\n        FetchSize:12000\n) QUERY (keytomap:'zip') OF USAddressData;\n\nCREATE CQ GenerateWactionContext\nINSERT INTO MerchantActivity\nSELECT  m.merchantId,\n        m.startingTime,\n        n.companyName,\n        m.category,\n        m.status,\n        m.count,\n        m.hourlyAve,\n        m.upperLimit,\n        m.lowerLimit,\n        m.zip,\n        z.city,\n        z.state,\n        z.latVal,\n        z.longVal\nFROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z\nWHERE m.merchantId = n.merchantId AND m.zip = z.zip\nLINK SOURCE EVENT;\n\n\n-- CQ GenerateAlerts\n--\n-- This CQ sends an alert when a merchant's status value changes to\n-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.\n\n\nCREATE STREAM AlertStream OF Global.AlertEvent;\n\nCREATE CQ GenerateAlerts\nINSERT INTO AlertStream\nSELECT n.CompanyName,\n       m.MerchantId,\n       CASE\n         WHEN m.Status = 'OK' THEN 'info'\n         ELSE 'warning' END,\n       CASE\n         WHEN m.Status = 'OK' THEN 'cancel'\n         ELSE 'raise' END,\n       CASE\n         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)\n         ELSE ''\n         END\nFROM MerchantTxRateWithStatusStream m, NameLookup n\nWHERE m.merchantId = n.merchantId;\n\n\n-- The following statement loads visualization (Dashboard) settings\n-- from a file.\n\n\nCREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;\n\n\nEND APPLICATION PosAppOracle;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nCREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()\nPARSE USING JSONParser ()\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()\nFORMAT USING JSONFormatter ()\nINPUT FROM @APPNAME@_Stream;\n\nEND APPLICATION @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application DS.DSAPP;\nundeploy application DS.DSAPP;\n\nstop application DS.MyPosApp;\nundeploy application DS.MyPosApp;\n\nstop application DS.DsPosApp;\nundeploy application DS.DsPosApp;\n\nstop application DSL.DSLAPP;\nundeploy application DSL.DSLAPP;\n\nunload cache DS.C1;\nunload wactionstore DS.MerchantActivity;\n\ndrop namespace DS cascade;\ndrop user DS cascade;\n\ndrop namespace DSL cascade;\ndrop user DSL cascade;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application FileWriterTest;\nundeploy application FileWriterTest;\ndrop application FileWriterTest cascade;\ncreate application FileWriterTest recovery 1 second interval;\ncreate source File_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'false'\n)\nOUTPUT TO CsvStream;\n\ncreate Target FileTarget using FileWriter(\n    rolloverpolicy:'@UPLOAD-SIZE@',\n    filename :'@FILE@',\n    directory : '@FOLDER@'\n)\nformat using JSONFormatter (\ncharset:'@charset@',\nmembers:'@mem@'\n)\ninput from CsvStream;\n\nend application FileWriterTest;\ndeploy application FileWriterTest on all in default;\nstart application FileWriterTest;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @AppName@;\nUNDEPLOY APPLICATION @AppName@;\nDROP APPLICATION @AppName@ CASCADE;\nCREATE APPLICATION @AppName@ recovery 1 second interval;\nCREATE SOURCE @AppName@_Source USING FileReader (\n\tWildCard: 'posdata100.csv',\n directory: '@dir@',\n\tpositionByEOF:false\n\t)\nPARSE USING DSVParser (\n\theader:yes\n)OUTPUT TO @AppName@_Stream;\n\nCREATE TYPE cdctypestream(\n id int,\n name String\n);\n\ncreate stream @StreamType@ of cdctypestream persist using Global.DefaultKafkaProperties;\nCREATE OR REPLACE CQ CsvToPosData\nINSERT INTO @StreamType@\nSELECT\nTO_INT(TO_STRING(data[0]).replaceAll(\"COMPANY \", \"\")),\ndata[1]\nFROM @AppName@_Stream;\nDeploy application @AppName@;\nStart @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\nCREATE APPLICATION DSV;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE  SOURCE CSVSource USING FileReader (\n  directory: '@TEST-DATA-PATH@',\n  WildCard: 'posdata.csv',\n  positionByEOF: false,\n  charset: 'UTF-8'\n )\n PARSE USING DSVParser (\n  header: 'yes'\n )\nOUTPUT TO CsvStream;\n\nCREATE OR REPLACE TARGET t USING FileWriter (\n  filename: 'TargetDefault',\n  directory:'@FEATURE-DIR@/logs/',\n  sequence:'00',\n  --flushinterval: '0',\n  rolloverpolicy:'EventCount:5000000,Interval:200s'\n )\n format using DSVFormatter (\n\n)\nINPUT FROM TypedCSVStream;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetDefaultPD_actual.log') input from TypedCSVStream;\n\nEND APPLICATION DSV;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCreate Source @SourceName@ Using MSSqlReader\n(\n Username:'qatest',\n Password:'w3b@ct10n',\n DatabaseName:'qatest',\n ConnectionURL:'localhost:1433',\n Tables:'qatest.source1',\n ConnectionPoolSize:1,\n StartPosition:'EOF'\n ) Output To @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter ( \nCheckPointTable: 'CHKPOINT',\nUsername: 'qatest',\nBatchPolicy: 'EventCount:1,Interval:60',\nCommitPolicy: 'EventCount:1,Interval:60',\nConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',\nTables: 'QATEST.SOURCE1,qatest.target1',\nPassword: 'qatest'\n)INPUT FROM @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetsys@ USING SysOut (name: 'ora12_out') INPUT FROM @SRCINPUTSTREAM@;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application ThreeAgentTester.CSV;\nundeploy application ThreeAgentTester.CSV;\ndrop application ThreeAgentTester.CSV cascade;\n\ncreate application CSV;\n\nCREATE FLOW AgentFlowOne;\ncreate source CSVSource using CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'customerdetails.csv',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO CsvStreamOne;\nEND FLOW AgentFlowOne;\n\nCREATE FLOW AgentFlowTwo;\ncreate source CSVSourceOne using CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'customerdetails.csv',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO CsvStreamTwo;\nEND FLOW AgentFlowTwo;\n\nCREATE FLOW AgentFlowThree;\ncreate source CSVSourceTwo using CSVReader (\n  Directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'customerdetails.csv',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO CsvStreamThree;\nEND FLOW AgentFlowThree;\n\nCREATE FLOW ServerFlow;\n\nCREATE TARGET FileDumpOne Using FileWriter (\ndirectory:'@FEATURE-DIR@/logs/',\nfilename:'SourceDumpFromStreamOne_%yyyy-MM-dd_HH_mm_ss_SSS%.csv',\nrolloverpolicy:'eventcount:39'\n\n)\nformat using DSVFormatter (\n)\ninput from CsvStreamOne;\nCREATE TYPE UserDataType\n(\n  UserId String KEY,\n  UserName String\n);\n\nCREATE STREAM UserDataStream OF UserDataType PARTITION BY UserId;\n\nCREATE CQ ParseUserData\nINSERT INTO UserDataStream\nSELECT  data[0],\n        data[1]\nFROM CsvStreamOne;\n\nCREATE TARGET FileDumpTwo Using FileWriter (\ndirectory:'@FEATURE-DIR@/logs/',\nfilename:'CQDumpFromUserDataStream_%yyyy-MM-dd_HH_mm_ss_SSS%.csv',\nrolloverpolicy:'eventcount:39'\n\n)\nformat using DSVFormatter (\n)\ninput from UserDataStream;\n\nCREATE WACTIONSTORE UserActivityInfo\nCONTEXT OF UserDataType\nEVENT TYPES ( UserDataType )\n@PERSIST-TYPE@\n\n--get data from UserDataStream and place into wactionStore UserWaction\nCREATE CQ UserWaction\nINSERT INTO UserActivityInfo\nSELECT * FROM UserDataStream\nLINK SOURCE EVENT;\nEND FLOW ServerFlow;\n\nEND APPLICATION CSV;\nDEPLOY APPLICATION CSV with AgentFlowOne on all in AGENTS, AgentFlowTwo on all in AGENTS,AgentFlowThree on all in AGENTS,ServerFlow on any in default;\nSTART CSV;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop Teradata_LogWriter;\nUndeploy application Teradata_LogWriter;\ndrop application Teradata_LogWriter cascade;\n\nCREATE APPLICATION Teradata_LogWriter recovery 5 second interval;\n\nCREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( \n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.TDSOURCE',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.TEST01=ID;',\n  PollingInterval: '5sec',\n  ReturnDateTimeAs: 'String',\n  startPosition:'striim.test01=0'\n  )\n  OUTPUT TO data_stream;\n\n  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM data_stream;\n\nCREATE TARGET BinaryDump USING LogWriter(\n  name: 'TeraData',\n  filename:'TeraData.log',\n  flushpolicy:'EventCount:100,Interval:30s'\n)INPUT FROM data_stream;\n\nEND APPLICATION Teradata_LogWriter;\n\ndeploy application Teradata_LogWriter in default;\n\nstart application Teradata_LogWriter;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (\n  StartPosition: '@startPosition@',\n  ConnectionURL: '@sourceURL@',\n  Username: '@userName@',\n  Tables: '@tables@',\n  CheckColumn: '@checkColum@',\n  FetchSize: 1,\n  Password: '@password@',\n  DatabaseProviderType: 'Default',\n  ThreadPoolSize: 5,\n  pollingInterval: '2sec',\n  ConnectionPoolSize: 1 )\nOUTPUT TO @STREAM@;\n\n create type @SOURCE_NAME@_AutoType(\n  ID int,\n  name string,\n  country string\n);\n\nCREATE STREAM @STREAM@_stream OF @SOURCE_NAME@_AutoType;\n\nCREATE CQ Lookup\nINSERT INTO @STREAM@_stream\nselect data[0],data[1],data[2] from @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ recovery 1 second interval;\n\nCREATE OR REPLACE SOURCE @SOURCENAME@ Using OJet\n(\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  Username: '@SOURCE_USER@'\n)\n\nOUTPUT TO @STREAM@ ;\n\nCREATE TARGET @TARGETNAME@ using DatabaseWriter\n(\n    ConnectionURL: '@TARGETURL',\n    username: '@TARGETUSERNAME@',\n    Password: '@TARGETPASSWORD@',\n    Tables: '@TARGETTABLE@',\n    BatchPolicy:'EventCount:1,Interval:1',\n    CommitPolicy:'EventCount:1,Interval:1'\n)\nINPUT FROM @STREAM@;\n\nEND APPLICATION @APPNAME@;\n\nDEPLOY APPLICATION @APPNAME@;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "DROP APPLICATION SliderSorter CASCADE;\n\nCREATE APPLICATION SliderSorter;\n\nCREATE OR REPLACE TYPE OrdersDataPARSED_TP (\n    tDateTime       org.joda.time.DateTime KEY,\n    sBusinessName   java.lang.String KEY,\n    sMerchantID     java.lang.String KEY,\n    sOrderId        java.lang.String,\n    sZip            java.lang.String,\n    lTerminalID     java.lang.Long,\n    fPaidAmount     java.lang.Float  \n);\n\nCREATE OR REPLACE STREAM OrdersDataPARSED_ST OF OrdersDataPARSED_TP;\n\nCREATE OR REPLACE TYPE ReturnsDataPARSED_TP (\n    tDateTime org.joda.time.DateTime KEY,\n    sOrderId java.lang.String KEY,\n    fReturnedAmount java.lang.Float\n);\n\nCREATE OR REPLACE STREAM ReturnsDataPARSED_ST OF ReturnsDataPARSED_TP;\n\nCREATE OR REPLACE STREAM OrdersDataSORTED_ST OF OrdersDataPARSED_TP;\nCREATE OR REPLACE STREAM ReturnsDataSORTED_ST OF ReturnsDataPARSED_TP;\nCREATE OR REPLACE STREAM Errors_ST OF Global.WAEvent;\n\nCREATE SORTER MySorter OVER\nOrdersDataPARSED_ST  ON tDateTime OUTPUT TO OrdersDataSORTED_ST,\nReturnsDataPARSED_ST ON tDateTime OUTPUT TO ReturnsDataSORTED_ST\nWITHIN 2 MINUTE\nOUTPUT ERRORS TO Errors_ST;\n\nEND APPLICATION SliderSorter;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION HW ;\nundeploy application HW ;\ndrop application HW cascade;\n\nCREATE APPLICATION HW Recovery 5 second interval;\n\nCREATE  SOURCE S USING OrReader  ( \n  Username: 'miner',\n  Password: '@miner',\n  ConnectionURL: '@conn-url@',\n  Tables: '@src@',\n  FetchSize: 1) \nOUTPUT TO hivestream;\n\nCreate Target T using HiveWriter (\n  ConnectionURL:'@hive-url@',\n  Username:'@uname@', \n            Password:'@pwd@',\n        --hadoopurl:'hdfs://localhost:9000/',\n        hadoopurl:'hdfs://dockerhost:9000/',\n\t        Mode:'incremental',\n\t        mergepolicy: 'eventcount:100,interval:1s',\n            Tables:'@tgt-table@',\n            hadoopConfigurationPath:'/Users/saranyad/Documents/hello/'\n )\nINPUT FROM hivestream;\n\n\nEND APPLICATION HW;\ndeploy application HW on all in default;\n\nStart application HW;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop application Postgres_To_PostgresApp;\nundeploy application Postgres_To_PostgresApp;\ndrop application Postgres_To_PostgresApp cascade;\n\nCREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE Postgres_src USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --startLSN:'758199500005967269518578814280000',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.postgrestopostgres_src',\n  ExcludedTables:'public.postgres_2000target'\n ) \nOUTPUT TO Change_Data_Stream ;\n\n\nCREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM Change_Data_Stream;\n\nCREATE OR REPLACE TARGET Postgres_tgt USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy: 'EventCount:100,Interval:60',\nCommitPolicy: 'EventCount:100,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM Change_Data_Stream;\n\nend application Postgres_To_PostgresApp;\ndeploy application Postgres_To_PostgresApp;\nstart Postgres_To_PostgresApp;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (\n  Username: '@TARGET_USER@',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  (\n  Username: '@TARGET_USER@',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n )\nINPUT FROM @STREAM@2;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\ncreate flow @STREAM@Flow1;\ncreate type @STREAM@type (id string, name string);\n\nCREATE OR REPLACE STREAM @STREAM@2 OF @STREAM@type;\nCREATE OR REPLACE STREAM @STREAM@3 OF @STREAM@type;\nCREATE OR REPLACE STREAM @STREAM@4 OF @STREAM@type;\n\ncreate cq @STREAM@cq1\ninsert into @STREAM@2\nselect \nTO_STRING(data[0]).replaceAll(\"COMPANY \", \"\"),\ndata[1]\nfrom @STREAM@;\n\nend flow @STREAM@Flow1;\n\n\ncreate flow @STREAM@Flow2;\n\ncreate cq @STREAM@cq2\ninsert into @STREAM@3\nselect * \nfrom @STREAM@2 where id is not null ;\n\ncreate cq @STREAM@cq3\ninsert into @STREAM@4\nselect * \nfrom @STREAM@3 where id is not null ;\n\nend flow @STREAM@Flow2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@ USING FileWriter( \n  filename:'CompressedMerchant.gz',\n  directory:'/logs/',\n  rolloverpolicy:'EventCount:10000,Interval:30s'\n   ) \nFORMAT USING dsvFormatter  ()\nINPUT FROM @STREAM@4;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create Target @TARGET_NAME@ using FileWriter(\n  filename:'CompressedMerchant.gz',\n  directory:'/logs/',\n  rolloverpolicy:'EventCount:10000,Interval:30s'\n)\nformat using DSVFormatter (\n\n)\ninput from @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @SOURCENAME1@ USING IncrementalBatchReader  (\n  FetchSize: 10,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: @checkColumn@,\n  startPosition: '@startPosition@',\n  PollingInterval: '20sec'\n  )\n  OUTPUT TO @STREAM@;\n\n  CREATE OR REPLACE SOURCE @SOURCENAME2@ USING IncrementalBatchReader  (\n    FetchSize: 10,\n    Username: 'striim',\n    Password: 'striim',\n    ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n    Tables: 'striim.test01',\n    adapterName: 'IncrementalBatchReader',\n    CheckColumn: @checkColumn1@,\n    startPosition: '@startPosition1@',\n    PollingInterval: '20sec'\n    )\n    OUTPUT TO @STREAM@;\n\n  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;\n\n  CREATE TARGET @targetName@ USING DatabaseWriter(\n    ConnectionURL:'@READER-URL@',\n    Username:'@READER-UNAME@',\n    Password:'@READER-PASSWORD@',\n    BatchPolicy:'Eventcount:1,Interval:1',\n    CommitPolicy:'Eventcount:1,Interval:1',\n    Checkpointtable:'CHKPOINT',\n    Tables:'@WATABLES@,@WATABLES@_target'\n  ) INPUT FROM @STREAM@;\n\n\n  END APPLICATION @APPNAME@;\n\n  DEPLOY APPLICATION @APPNAME@;\n  start application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nCREATE APPLICATION @AppName@;\nCREATE FLOW Il_Agent_flow;\nCREATE OR REPLACE SOURCE FileReader_Src USING FileReader  (\n   WildCard: 'posdata100.csv',\n  directory: '@dir@',\n  positionbyeof: false)\n PARSE USING DSVParser  (\n )\nOUTPUT TO CsvStream ;\nEND FLOW Il_Agent_flow;\n\nCreate Type CSVType (\n  companyid String,\n  merchantId String\n);\n\nCREATE STREAM TypedCSVStream OF CSVType;\n\nCREATE OR REPLACE  CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT\nTO_STRING(data[0]).replaceAll(\"COMPANY \", \"\"),\ndata[1]\nFROM CsvStream;\n\nCREATE OR REPLACE TARGET initialLoadPostgres_Trg USING Global.DatabaseWriter (\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  CheckPointTable: 'CHKPOINT',\n  CDDLAction: 'Process',\n  CommitPolicy: 'EventCount:100,Interval:60',\n  StatementCacheSize: '50',\n  ConnectionURL: '@TrgUrl@',\n  Username: '@TrgUserName@',\n  BatchPolicy: 'EventCount:100,Interval:60',\n  Tables: '@TrgTable@',\n  Password: '@TrgPswd@',\n  adapterName: 'DatabaseWriter' )\nINPUT FROM TypedCSVStream;\nEND APPLICATION @AppName@;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop Teradata_LogWriter;\nUndeploy application Teradata_LogWriter;\ndrop application Teradata_LogWriter cascade;\n\nCREATE APPLICATION Teradata_LogWriter WITH ENCRYPTION recovery 5 second interval;\n\nCREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( \n  FetchSize: 1,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.TDSOURCE',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.TEST01=ID;',\n  PollingInterval: '5sec',\n  ReturnDateTimeAs: 'String',\n  startPosition:'striim.test01=0'\n  )\n  OUTPUT TO data_stream;\n\n  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM data_stream;\n\nCREATE TARGET BinaryDump USING LogWriter(\n  name: 'TeraData',\n  filename:'TeraData.log'\n)INPUT FROM data_stream;\n\nEND APPLICATION Teradata_LogWriter;\n\ndeploy application Teradata_LogWriter in default;\n\nstart application Teradata_LogWriter;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@ recovery 1 second interval;\n\ncreate flow agentflow;\n\ncreate source @APPNAME@_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using CobolCopybookParser (\ncopybookFileName : '@TD@/@PROP1@',\n  dataFileFont: '@PROP2@',\n  copybookSplit: '@PROP3@',\n  dataFileOrganization: '@PROP4@',\n  copybookDialect: '@PROP5@', \n  skipIndent:'@PROP6@',\n  DatahandlingScheme:'@PROP7@'\n  --recordSelector: 'ARDPLKBX-RECORD:ARDPLKBX-RECORD-TYPE=ARDPLKBX-RECORD'\n  --recordSelector: 'OH:MOH-SEG-ID=OH, OH2:OH2-SEG-ID=OH2, OHU:MOH-SEG-ID=OHU, OR1:OR1-SEG-ID=OR1, OR2:OR2-SEG-ID=OR2, OR3:OR3-SEG-ID=OR3, OR4:OR4-SEG-ID=OR4, OHM:OHM-SEG-ID=OHM, OD:OD-SEG-ID=OD, ODU:ODU-SEG-ID=ODU, OD1:OD1-SEG-ID=OD1, ODM:ODM-SEG-ID=ODM, OT:OT-SEG-ID=OT'\n)\nOUTPUT TO @APPNAME@Stream;\n\nend flow agentflow;\n\ncreate flow serverflow;\n\ncreate Target @APPNAME@Target using FileWriter(\n    filename :'@FILE@',\n    directory : '@FOLDER@'\n)\nformat using JsonFormatter (\n)\ninput from @APPNAME@Stream;\n\n\nCREATE TYPE test_typeRe \n(\nnode_new com.fasterxml.jackson.databind.JsonNode,\nnode_name com.fasterxml.jackson.databind.JsonNode,\nnode_addr com.fasterxml.jackson.databind.JsonNode\n);\n\nCreate stream cqAsJSONNodeStreamRe of test_typeRe;\n\nCREATE CQ GetPOAsJsonNodesRe\nINSERT into cqAsJSONNodeStreamRe\nselect \ndata.get('ACCTS-RECORD'),\ndata.get('ACCTS-RECORD').get('NAME'),\ndata.get('ACCTS-RECORD').get('ADDRESS3')\nfrom @APPNAME@Stream js;\n\ncreate type finaldtypeRe\n(ACCOUNT_NO int,\nFIRST_NAME String,\nLAST_NAME String,\nADDRESS1 String,\nADDRESS2 String,\nCITY String,\nSTATE String,\nZIP_CODE int);\n\nCREATE STREAM getdataStreamPS OF finaldtypeRe;\n\nCREATE CQ getdataRe\nINSERT into getdataStreamPS\nselect JSONGetInteger(x.node_new,\"ACCOUNT-NO\"),\nJSONGetString(x.node_name,\"FIRST-NAME\"),\nJSONGetString(x.node_name,\"LAST-NAME\"),\nJSONGetString(x.node_new,\"ADDRESS1\"),\nJSONGetString(x.node_new,\"ADDRESS2\"),\nJSONGetString(x.node_addr,\"CITY\"),\nJSONGetString(x.node_addr,\"STATE\"),\nJSONGetInteger(x.node_addr,\"ZIP-CODE\")\nfrom cqAsJSONNodeStreamRe x;\n\ncreate Target @APPNAME@DBTarget using DatabaseWriter(\n  Username: 'qatest',\n  Password: 'qatest',\n  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',\n  BatchPolicy: 'EventCount:1000,Interval:50',\n  CommitPolicy: 'EventCount:1000,Interval:50',\n  Tables: 'QATEST.@table@'\n)\ninput from getdataStreamPS;\nend flow serverflow;\n\nend application @APPNAME@;\ndeploy application @APPNAME@ with agentflow on any in agents,serverflow in default; \nstart application @APPNAME@;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @Appname@;\nUNDEPLOY APPLICATION @Appname@;\nDROP APPLICATION @Appname@ CASCADE;\nunload OPEN PROCESSOR \"OP_SCM_PATH\";\n\nCREATE APPLICATION @Appname@ RECOVERY 10 SECOND INTERVAL;\n\nCreate flow AgentFlow;\n\nCREATE  SOURCE @Appname@s USING databaseReader  (\nUsername: 'qatest',\nPassword: 'qatest',\nConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',\nTables: 'QATEST.TGT_T%')\nOUTPUT TO @Appname@Stream1;\n\nCREATE SOURCE @Appname@sd USING databaseReader  (\nUsername: 'qatest',\nPassword: 'qatest',\nConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',\nTables: 'QATEST.TGT_T%')\nOUTPUT TO @Appname@Stream2;\n\nLOAD OPEN PROCESSOR \"OP_SCM_PATH\";\n\nCREATE STREAM  @Appname@outStream OF Global.WAEvent;\n\nCREATE OPEN PROCESSOR @Appname@op USING TUPLECONVERTER (\n   magicprop:\"ASIA\"\n)\nINSERT INTO @Appname@outStream\nFROM @Appname@Stream1;\n\nCREATE OR REPLACE ROUTER @Appname@router1 INPUT FROM @Appname@outStream s CASE\nWHEN meta(s,\"TableName\").toString()='QATEST.OPA_INPUTR1' THEN ROUTE TO @Appname@ss1,\nWHEN meta(s,\"TableName\").toString()='QATEST.OPA_INPUTR2' THEN ROUTE TO @Appname@ss2,\nELSE ROUTE TO @Appname@ss_else;\n\n\nCREATE OR REPLACE ROUTER @Appname@router2 INPUT FROM @Appname@Stream2 s CASE\nWHEN meta(s,\"TableName\").toString()='QATEST.OPA_OUTPUTR1' THEN ROUTE TO @Appname@ss1,\nWHEN meta(s,\"TableName\").toString()='QATEST.OPA_OUTPUTR2' THEN ROUTE TO @Appname@ss2,\nELSE ROUTE TO @Appname@ss_else;\n\nend flow AgentFlow;\n\ncreate flow nflow;\n\ncreate Target @Appname@Target_1 using FileWriter\n(\ndirectory: 'testApr22',\nfilename:'%@metadata(TableName)%',\nflushpolicy:'eventCount:1000,Interval:90s',\nRollOverPolicy:'eventCount:1000,Interval:90s'\n)\nFORMAT USING dsvFormatter ()\ninput from @Appname@ss1;\n\ncreate Target @Appname@Target_2 using FileWriter\n(\ndirectory: 'testApr22',\nfilename:'%@metadata(TableName)%',\nflushpolicy:'eventCount:1000,Interval:90s',\nRollOverPolicy:'eventCount:1000,Interval:90s'\n)\nFORMAT USING dsvFormatter ()\ninput from @Appname@ss2;\n\nend flow nflow;\n\nend application @Appname@;\ndeploy application @Appname@ with AgentFlow in agents, nflow in default;\nstart @Appname@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION DBRTOCW;\nUNDEPLOY APPLICATION DBRTOCW;\nDROP APPLICATION DBRTOCW CASCADE;\nCREATE APPLICATION DBRTOCW;\n\ncreate source CSVSource using FileReader (\n\tdirectory:'/Users/jenniffer/Product2/IntegrationTests/TestData/',\n\tWildCard:'posdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\t\n)\nOUTPUT TO FileStream\n(\nid String,\nename String\n)\nselect \ndata[2],\ndata[0];\n\ncreate Target t2 using SysOut(name:OrgData) input from FileStream;\nCREATE OR REPLACE Target DBTarget USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:1000,Interval:60',\n  CommitPolicy: 'EventCount:1000,Interval:60',\n  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',\n  Tables: 'test.employee',\n  Password: 'cassandra',\n  Password_encrypted: false\n )INPUT FROM FileStream;\n\nEND APPLICATION DBRTOCW;\nDEPLOY APPLICATION DBRTOCW;\nSTART APPLICATION DBRTOCW;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCreate Source @SOURCE@\n        Using OracleReader\n(\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: 'qatest.test77',\n  FetchSize:1,\n  QueueSize:2000,\n  CommittedTransactions:true,\n  Compression:false\n)\nOutput To @STREAM@;\n\ncreate Target @TARGET@ using GCSWriter(\n    bucketname:'@bucketname@',\n    objectname:'@objectname@',\n    projectId:'@project-id@',\n    uploadpolicy:'EventCount:7'\n    ServiceAccountKey:'@file-path@'\n)\nformat using DSVFormatter (\nmembers:'data'\n)\ninput from @STREAM@;\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create Application UdpXml;\ncreate source UdpXMLSource using UDPReader (\n\tIpAddress:'127.0.0.1',\n\tPortNo:'3546'\n)\nparse using XMLParser (\n    RootNode:'/catalog/book'\n)\nOUTPUT TO UdpXMLStream;\ncreate Target UdpDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/xmldata') input from UdpXMLStream;\nend Application UdpXml;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@ recovery 5 second Interval;\n\nCREATE source @srcName@ USING Global.OracleReader ( \n Username:'@srcusername@',\n  Password:'@srcpassword@',\n  ConnectionURL:'@srcurl@',\n  Tables:'@srcschema@.@srctable@',\n  sendBeforeImage:'true',\n  FilterTransactionBoundaries:'true'\n  ) \nOUTPUT TO @outstreamname@;\n\n\nCREATE OR REPLACE TARGET @tgtName@ USING Global.SalesforceWriter ( \n  autoAuthTokenRenewal: 'true', \n  sObjects: '@srcschema@.@srctable@,@tgtobject@ COLUMNMAP(num__c=a,Name=b)', \n  useConnectionProfile: 'false', \n  consumerSecret: '@tgtconsumersecret@', \n  JWTKeystorePath: '', \n  BatchPolicy:'EventCount:1,Interval:10s',\n  CommitPolicy:'EventCount:1,Interval:10s', \n  Mode: 'APPENDONLY', \n  consumerKey: '@tgtconsumerkey@', \n  apiEndPoint: '@tgtapiurl@', \n  InMemory: 'true', \n  FieldDelimeter: 'COMMA', \n  ApplicationErrorCountThreshold: '0', \n  useBulkApi: 'true', \n  OAuthAuthorizationFlows: 'PASSWORD', \n  JWTCertificateName: '', \n  hardDelete: 'false', \n  Username: '@tgtusername@', \n  useQuotes: 'false', \n  adapterName: 'SalesforceWriter', \n  Password: '@tgtpassword@', \n  connectionRetryPolicy: 'retryInterval=30, maxRetries=3', \n  securityToken: '@tgtsecuritytoken@', \n  JWTKeystorePassword: '' ) \nINPUT FROM @instreamname@;\n\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (\n  Tables: '',\n  ConnectionURL: '',\n  Password: '',\n  Username: ''\n  )\nOUTPUT TO @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '',\n  region: '')\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSVSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'smallposdata.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\n  filename:'TargetPosDataXmlTI',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00'\n)\nformat using XMLFormatter (\n  rootelement:'document',\n  elementtuple:'MerchantName:merchantid:text=merchantname'\n)\ninput from TypedCSVStream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlTI_actual.log') input from TypedCSVStream;\n\nend application DSV;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 42 with two sources and two WactionStores. A variety of partitioned windows in between\n-- assure that we are testing a complicated recovery scenario.\n--\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Stream -> JWc5 -> WS1\n--   S2 -> Stream -> JWc10 -> WS2\n--\n\nSTOP Recov42Tester.RecovTest42;\nUNDEPLOY APPLICATION Recov42Tester.RecovTest42;\nDROP APPLICATION Recov42Tester.RecovTest42 CASCADE;\nCREATE APPLICATION RecovTest42 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10242,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10242,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');\nCREATE STREAM DataStreamTop OF CsvData using KafkaProps;\n\nCREATE CQ Csv1ToDataStreamTop\nINSERT INTO DataStreamTop\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ Csv2ToDataStreamTop\nINSERT INTO DataStreamTop\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\n\n\n\n\n\n\n\n\n\nCREATE JUMPING WINDOW LeftJWc5\nOVER DataStreamTop KEEP 5 ROWS;\n\nCREATE JUMPING WINDOW RightJWc10\nOVER DataStreamTop KEEP 10 ROWS;\n\n\n\nCREATE WACTIONSTORE WactionsLeft CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE WactionsRight CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ ToWactionsLeft\nINSERT INTO WactionsLeft\nSELECT\n    FIRST(p.merchantId),\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    TO_DOUBLE(FIRST(p.amount))\nFROM LeftJWc5 p;\n\nCREATE CQ ToWactionsRight\nINSERT INTO WactionsRight\nSELECT\n    FIRST(p.merchantId),\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    TO_DOUBLE(FIRST(p.amount))\nFROM RightJWc10 p;\n\nEND APPLICATION RecovTest42;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop OracleToKudu;\nundeploy application OracleToKudu;\ndrop application OracleToKudu cascade;\n\nCREATE APPLICATION OracleToKudu;\nCreate Source oracSource Using OracleReader\n(\n Username:'@LOGMINER-UNAME@',\n Password:'@LOGMINER-PASSWORD@',\n ConnectionURL:'@LOGMINER-URL@',\n Tables:'@SOURCE_TABLES@',\n OnlineCatalog:true,\n FetchSize:1\n) Output To DataStream;\n\nCREATE TARGET WriteintoKudu using KuduWriter (\nkuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',\npkupdatehandlingmode:'@MODE@',\ntables: '@TARGET_TABLES@',\nbatchpolicy: 'EventCount:1,Interval:0')\nINPUT FROM DataStream;\n\nEND APPLICATION OracleToKudu;\ndeploy application OracleToKudu in default;\nstart OracleToKudu;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@;\n\nCREATE SOURCE @parquetsrc@ USING FileReader (\nwildcard: '',\ndirectory: '',\npositionbyeof: false )\nPARSE USING ParquetParser (\n)\nOUTPUT TO @appname@Stream;\n\nCREATE CQ @appname@CQ\nINSERT INTO @appname@CqOut\n    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;\n\nCREATE OR REPLACE TARGET @filetarget@ USING FileWriter (\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:1,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING AvroFormatter  (\nschemaFileName: 'AvroFileSchema'\n)\nINPUT FROM @appname@CqOut;\n\ncreate Target @s3target@ using S3Writer(\n    bucketname:'',\n    objectname:'',\n    uploadpolicy:'',\n    foldername:''\n)\nformat using AvroFormatter (\nschemaFileName: 'AvroS3Schema'\n)\ninput from @appname@CqOut;\n\ncreate Target @blobtarget@ using AzureBlobWriter(\n\taccountname:'',\n\taccountaccesskey:'',\n\tcontainername:'',\n    blobname:'',\n\tfoldername:'',\n\tuploadpolicy:'EventCount:10,interval:5s'\n)\nformat using AvroFormatter (\nschemaFileName: 'AvroAzureSchema'\n)\nINPUT FROM @appname@CqOut;\n\nCREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (\n    bucketname:'',\n    objectname:'',\n    foldername:'',\n    projectId:'',\n    uploadPolicy:''\n)\nformat using AvroFormatter (\nschemaFileName: 'AvroGCSSchema'\n)\nINPUT FROM @appname@CqOut;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@1;\nundeploy application @APPNAME@1;\nstop application @APPNAME@2;\nundeploy application @APPNAME@2;\n\nDROP STREAM @APPNAME@_STREAM;\nDROP APPLICATION @APPNAME@1 CASCADE;\nDROP APPLICATION @APPNAME@2 CASCADE;\n\ndrop propertyset OrcToOrcPlatfm_App_KafkaPropset;\ndrop stream  OrcToOrcPlatfm_Stream CASCADE;\n\n\n--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\n--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;\n\nCREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';\ncreate application @APPNAME@1 recovery 5 SECOND INTERVAL;\n\ncreate or replace stream @STREAM@2 of Global.waevent;\n\nCREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNEC4TION_URL@',\n  Tables: '$table1',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_AGENTFLOW1;\n\nCREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@TABLENAME@2',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_SERVERFLOW1;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;\nCREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x\n WHERE META(x,'TableName').toString() == 'QATEST.ORCTOORCPLATFM_SOURCE4';\n\nCREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n )\nINPUT FROM @STREAM@2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(\n  name: 'CDDL',\n  filename:'WAEventDump.log'\n)INPUT FROM @STREAM@;\n\nCREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;\n\nEND APPLICATION @APPNAME@1;\n\n\n\nCREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';\nCREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;\n\ncreate or replace stream @STREAM@3 of Global.waevent;\n\nCREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '$table2',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\n\nEND FLOW @APPNAME@_AGENTFLOW2;\n\nCREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@TABLENAME@4',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_SERVERFLOW2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;\nCREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y\n WHERE META(y,'TableName').toString() == 'QATEST.ORCTOORCPLATFM_SOURCE2';\n\nCREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n )\nINPUT FROM @STREAM@3;\n\nCREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;\n\nEND APPLICATION @APPNAME@2;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ Recovery 5 second interval;\n--create application @APPNAME@;\n\ncreate type @APPNAME@employee\n(\nid integer,\nname String,\ncompany String\n);\nCREATE STREAM @APPNAME@cosmoscassandra_TypedStream of @APPNAME@employee;\n\nCREATE OR REPLACE SOURCE @APPNAME@OnPrem_Oracle USING OracleReader  (\n  Compression: false,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  TransactionBufferDiskLocation: '.striim/LargeBuffer',\n  FetchSize: 1,\n  DDLCaptureMode: 'All',\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'localhost:1521:xe',\n Tables: 'QATEST.EMP%',\n-- Tables: 'QATEST.OracToCql_alldatatypes',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  TransactionBufferType: 'Memory',\n  DictionaryMode: 'OnlineCatalog',\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  TransactionBufferSpilloverSize: '1MB'\n )\nOUTPUT TO @APPNAME@kperststream ;\n\ncreate stream @APPNAME@UserdataStream1 of Global.WAEvent;\n\nCreate CQ @APPNAME@CQUser\ninsert into @APPNAME@UserdataStream1\nselect putuserdata (data1,'OperationName',META(data1,'OperationName').toString()) from @APPNAME@kperststream data1;\n\nCreate CQ @APPNAME@CQUser_typed\ninsert into @APPNAME@cosmoscassandra_TypedStream\nselect \nto_int(data[0]),\ndata[1],\ndata[2]\nfrom @APPNAME@UserdataStream1 u \nwhere USERDATA(u,'OperationName').toString()=='INSERT' and meta(u,'TableName').toString()=\"QATEST.EMP3\";\n\n\nCREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target1 USING CassandraCosmosDBWriter  ( \n  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --CheckPointTable: '',\n  FlushPolicy: 'EventCount:1000,Interval:60',\n  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',\n  AccountKey:'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',\n  KeySpace: 'test',\n  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',\n  --ConsistencyLevel:'fh',\n  --port:'10505',\n  Tables: 'test.emp_tgt',\n  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',\n  adapterName: 'CassandraCosmosDBWriter'\n ) \nINPUT FROM @APPNAME@cosmoscassandra_TypedStream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use PosTester;\nDROP Source CsvDataSource;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;\n\nCreate Source @SourceName@ Using OracleReader\n\n(\n  Username:'c##qatest',\n  Password:'qatest',\n  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',\n  Tables:'CDB$ROOT.\"C##QATEST\".ojet_src;ORCLPDB.QATEST.ojet_src',\n  _h_useClassic:false,\n  Fetchsize:1,\n  Compression: true,\n  SupportPDB:true,\n  ReplicationSlotName:'null'\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter\n(\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',\n  Username:'c##qatest',\n  Password:'qatest',\n  BatchPolicy:'EventCount:1,Interval:1',\n  Tables:'CDB$ROOT.\"C##QATEST\".ojet_src,CDB$ROOT.\"C##QATEST\".ojet_tgt'\n) INPUT FROM @SRCINPUTSTREAM@;\n\n\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW;\nCREATE OR REPLACE PROPERTYSET KafkaProps(zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.9');\nCREATE STREAM Oracle_ChangeDataStream1 of Global.WAEvent PERSIST USING KafkaProps;\nCREATE OR REPLACE SOURCE DBSource USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  QuiesceMarkeRTable:'QATEST.QUIESCEMARKER',\n  Password_encrypted: false,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//127.0.0.1:1521/xe',\n  Tables: 'QATEST.OracToCql_alldatatypes',\n  adapterName: 'OracleReader',\n  Password: 'miner',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'miner',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream1;\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'test.chkpoint',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:1,Interval:60',\n  CommitPolicy: 'EventCount:1,Interval:60',\n  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',\n  Tables: 'QATEST.OracToCql_alldatatypes,test.oractocq_alldatatypes',\n  Password: 'cassandra',\n  Password_encrypted: false\n )\nINPUT FROM Oracle_ChangeDataStream1;\ncreate Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream1;\nEND APPLICATION DBRTOCW;\ndeploy application DBRTOCW in default;\nstart DBRTOCW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCreate Source @SOURCE@\n        Using OracleReader\n(\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: 'qatest.test77',\n  FetchSize:1,\n  QueueSize:2000,\n  CommittedTransactions:true,\n  Compression:false\n)\nOutput To @STREAM@;\n\nCREATE TARGET @TARGET@ USING RedshiftWriter\n\t(\n\t  ConnectionURL: 'tgt_url',\n\t  Username: 'tgt_username',\n\t  Password: 'tgt_pwrd',\n\t  bucketname: 'bucket_name',\n\t  --accesskeyId: 'access_key',\n\t  --secretaccesskey: 'secret_access',\n\t  Tables: 'QATEST.%,QATEST.%',\n\t   S3IAMRole:'@IAMROLE@',\n\tuploadpolicy:'EventCount:7'\n\t) INPUT FROM @STREAM@;\n\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP rest2.applicationApi;\nUNDEPLOY APPLICATION rest2.applicationApi;\nDROP APPLICATION rest2.applicationApi cascade;\n\nCREATE APPLICATION applicationApi;\n\n-- All CREATE statements between here and the END APPLICATION\n-- statement will create objects in the PosApp application.\n\n-- source CsvDataSource\n\nCREATE source CsvDataSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'posdata.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\n\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE CQ CsvToPosData\nINSERT INTO PosDataStream partition by merchantId\nSELECT TO_STRING(data[1]) as merchantId,\n       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,\n       TO_DOUBLE(data[7]) as amount,\n       TO_STRING(data[9]) as zip\nFROM CsvStream;\n\nCREATE JUMPING WINDOW PosData5Minutes\nOVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;\n\nCREATE TYPE MerchantHourlyAve(\n  merchantId String,\n  hourValue integer,\n  hourlyAve integer\n);\nCREATE CACHE HourlyAveLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'hourlyData.txt'\n)\nPARSE USING DSVParser (\n  header: Yes,\n  trimquote:false,\n  trimwhitespace:true\n) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;\n\nCREATE TYPE MerchantTxRate(\n  merchantId String KEY,\n  zip String,\n  startingTime DateTime,\n  count integer,\n  totalAmount double,\n  hourlyAve integer,\n  upperLimit double,\n  lowerLimit double,\n  category String,\n  status String\n);\nCREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;\n\nCREATE CQ GenerateMerchantTxRateOnly\nINSERT INTO MerchantTxRateOnlyStream\nSELECT p.merchantId,\n       FIRST(p.zip),\n       FIRST(p.dateTime),\n       COUNT(p.merchantId),\n       SUM(p.amount),\n       FIRST(l.hourlyAve/12),\n       FIRST(l.hourlyAve/12 * CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       FIRST(l.hourlyAve/12 / CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       '<NOTSET>',\n       '<NOTSET>'\nFROM PosData5Minutes p, HourlyAveLookup l\nWHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue\nGROUP BY p.merchantId;\n\n\nCREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;\n\nCREATE CQ GenerateMerchantTxRateWithStatus\nINSERT INTO MerchantTxRateWithStatusStream\nSELECT merchantId,\n       zip,\n       startingTime,\n       count,\n       totalAmount,\n       hourlyAve,\n       upperLimit,\n       lowerLimit,\n       CASE\n         WHEN count >10000 THEN 'HOT'\n         WHEN count > 800 THEN 'WARM'\n         WHEN count >200 THEN 'COOL'\n         ELSE 'COLD' END,\n       CASE\n         WHEN count > upperLimit THEN 'TOOHIGH'\n         WHEN count < lowerLimit THEN 'TOOLOW'\n         ELSE 'OK' END\nFROM MerchantTxRateOnlyStream;\n\nCREATE TYPE MerchantActivityContext(\n  MerchantId String KEY,\n  StartTime  DateTime,\n  CompanyName String,\n  Category String,\n  Status String,\n  Count integer,\n  HourlyAve integer,\n  UpperLimit double,\n  LowerLimit double,\n  Zip String,\n  City String,\n  State String,\n  LatVal double,\n  LongVal double\n);\n\nCREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext\nEVENT TYPES ( MerchantTxRate )\n@PERSIST-TYPE@\n\n\nCREATE TYPE MerchantNameData(\n  merchantId String KEY,\n  companyName String\n);\n\nCREATE TYPE USAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\nCREATE CACHE NameLookup using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'MerchantNames.csv',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:'yes',\n  trimquote:false,\n  trimwhitespace:true\n) \nQUERY(keytomap:'merchantId') OF MerchantNameData;\n\nCREATE CACHE ZipLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'USAddresses.txt',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header: Yes,\n  columndelimiter: '\\t',\n  trimquote:false,\n  trimwhitespace:true\n) QUERY (keytomap:'zip') OF USAddressData;\n\n\nCREATE CQ GenerateWactionContext\nINSERT INTO MerchantActivity\nSELECT  m.merchantId,\n        m.startingTime,\n        n.companyName,\n        m.category,\n        m.status,\n        m.count,\n        m.hourlyAve,\n        m.upperLimit,\n        m.lowerLimit,\n        m.zip,\n        z.city,\n        z.state,\n        z.latVal,\n        z.longVal\nFROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z\nWHERE m.merchantId = n.merchantId AND m.zip = z.zip\nLINK SOURCE EVENT;\n\nCREATE STREAM AlertStream OF Global.AlertEvent;\n\nCREATE CQ GenerateAlerts\nINSERT INTO AlertStream\nSELECT n.CompanyName,\n       m.MerchantId,\n       CASE\n         WHEN m.Status = 'OK' THEN 'info'\n         ELSE 'warning' END,\n       CASE\n         WHEN m.Status = 'OK' THEN 'cancel'\n         ELSE 'raise' END,\n       CASE\n         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)\n         ELSE ''\n         END\nFROM MerchantTxRateWithStatusStream m, NameLookup n\nWHERE m.merchantId = n.merchantId;\n\nCREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;\n        \nEND APPLICATION applicationApi;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\n--drop exceptionstore admin.MSSQLServer_To_MSSQLServerApp_ExceptionStore;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ use exceptionstore;\n\n\nCreate Source @SourceName@ Using MSSqlReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'@SourceTable@',\n ConnectionPoolSize:1,\n StartPosition:'EOF'\n ) Output To @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;\n\n\n CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@SourceTable@,@TargetTable@',\n    CommitPolicy: 'Interval:5'\n\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate or replace cq @cq@\ninsert into @finalstream@\nselect exceptionType,action,appName,entityType,entityName,className,message,relatedActivity from @APPNAME@_ExceptionStore;\n\nCreate target @targetfile@ using filewriter (\nfilename:'@APPNAME@_file.log',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000,sequence:00'\n)\nformat using jsonFormatter()\ninput from @finalstream@;\n\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;\n\nstop @APPNAME@_ExpStore;\nundeploy application @APPNAME@_ExpStore;\ndrop application @APPNAME@_ExpStore cascade;\nCREATE APPLICATION @APPNAME@_ExpStore;\n\nCREATE TYPE @APPNAME@_ExpStore_CDCStreams_Type  (\n  evtlist java.util.List\n );\n\nCREATE STREAM @APPNAME@_ExpStore_CDCStreams OF @APPNAME@_ExpStore_CDCStreams_Type;\n\nCREATE CQ @APPNAME@_ReadFromExpStore\nINSERT INTO @APPNAME@_ExpStore_CDCStreams\nselect to_waevent(s.relatedObjects) as evtlist from admin.@APPNAME@_ExceptionStore [jumping 5 second] s;\n\nCREATE STREAM @APPNAME@_ExpStore_CDCEventStream OF Global.WAEvent;\n\nCREATE CQ @APPNAME@_ExpStore_GetCDCEvent\nINSERT INTO @APPNAME@_ExpStore_CDCEventStream\nSELECT com.webaction.proc.events.WAEvent.makecopy(cdcevent) FROM @APPNAME@_ExpStore_CDCStreams a, iterator(a.evtlist) cdcevent;\n\nCREATE CQ @APPNAME@_ExpStore_JoinDataCQ\nINSERT INTO @APPNAME@_ExpStore_JoinedDataStream\nSELECT  TO_INT(f.data[0]),\n        TO_STRING(f.data[1])\n        from @APPNAME@_ExpStore_CDCEventStream f;\n\nCREATE OR REPLACE TARGET @APPNAME@_ExpStore_WriteToFileAsJSON USING FileWriter  (\n  filename: 'expEvent_MSSQL',\n  rolloveronddl: 'true',\n  flushpolicy: 'eventcount:1,interval:30',\n  adapterName: 'FileWriter',\n  directory: 'ExpStore_logs',\n  rolloverpolicy: 'eventcount:6,interval:30s'\n )\nFORMAT USING JSONFormatter  (\n  handler: 'com.webaction.proc.JSONFormatter',\n  formatterName: 'JSONFormatter',\n  jsonMemberDelimiter: '\\n',\n  jsonobjectdelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true'\n )\nINPUT FROM @APPNAME@_ExpStore_JoinedDataStream;\n\nCREATE TARGET @APPNAME@_ExpStore_dbtarget USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:1,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:1',\nTables:'@TargetTable@'\n) INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;\n\nEND APPLICATION @APPNAME@_ExpStore;\n\ndeploy application @APPNAME@_ExpStore;\nstart @APPNAME@_ExpStore;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\nCREATE OR REPLACE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCREATE FLOW @APPNAME@_Agent_flow;\n\nCREATE OR REPLACE SOURCE @SourceName@1 USING Global.MSJet (\n  Tables: 'dbo.compsrc',\n    username: 'qatest',\n    DatabaseName: 'qatest',\n    FetchTransactionMetadata: true,\n    filterTransactionBoundaries: true,\n    compression: true,\n    ConnectionURL: '10.211.55.3:1433',\n    CommittedTransactions: true,\n    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',\n    SendBeforeImage: true,\n    password: 'w3b@ct10n' )\nOUTPUT TO @SRCINPUTSTREAM@;\n\nEND FLOW @APPNAME@_Agent_flow;\n\nCREATE FLOW @APPNAME@_Agent_flow1;\n\nCREATE OR REPLACE SOURCE @SourceName@2 USING Global.MSJet (\n  Tables: 'dbo.compsrc',\n    username: 'qatest',\n    DatabaseName: 'qatest',\n    FetchTransactionMetadata: true,\n    filterTransactionBoundaries: true,\n    compression: false,\n    ConnectionURL: '10.211.55.3:1433',\n    CommittedTransactions: true,\n    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',\n    SendBeforeImage: true,\n    password: 'w3b@ct10n' )\nOUTPUT TO @SRCINPUTSTREAM@;\n\nEND FLOW @APPNAME@_Agent_flow1;\n\nCREATE FLOW @APPNAME@_server_flow;\n\nCREATE OR REPLACE TARGET @targetName@ USING Global.DatabaseWriter\n(\n  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',\n  BatchPolicy:'EventCount:10,Interval:60',\n  CommitPolicy:'EventCount:10,Interval:60',\n  ParallelThreads:'',\n  CheckPointTable:'CHKPOINT',\n  Password_encrypted:'false',\n  Tables:'qatest.MSJEtsrc1,qatest.MSJEtar1;qatest.MSJEtsrc2,qatest.MSJEtar2;',\n  CDDLAction:'Process',\n  Password:'w3b@ct10n',\n  StatementCacheSize:'50',\n  ConnectionURL:'jdbc:sqlserver://10.211.55.3:1433;databaseName=qatest',\n  DatabaseProviderType:'Default',\n  Username:'qatest',\n  PreserveSourceTransactionBoundary:'false',\n  adapterName:'DatabaseWriter'\n)\nINPUT FROM @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetsys@ USING Global.SysOut (\n  name: '@targetsys@' )\nINPUT FROM @SRCINPUTSTREAM@;\n\nEND FLOW @APPNAME@_server_flow;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@ with @APPNAME@_Agent_flow in AGENTS, @APPNAME@_Agent_flow1 in AGENTS ,@APPNAME@_server_flow on any in default;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop application Postgres_To_PostgresApp;\nundeploy application Postgres_To_PostgresApp;\ndrop application Postgres_To_PostgresApp cascade;\n\nCREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW agentflow;\n\nCREATE OR REPLACE SOURCE Postgres_Src1 USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot_1',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.tablename0'\n ) \nOUTPUT TO Change_Data_Stream ;\n\nCREATE OR REPLACE SOURCE Postgres_Src2 USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot_2',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.tablename1'\n ) \nOUTPUT TO Change_Data_Stream ;\n\n\nCREATE OR REPLACE SOURCE Postgres_Src3 USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot_3',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.tablename2'\n ) \nOUTPUT TO Change_Data_Stream ;\n\nCREATE OR REPLACE SOURCE Postgres_Src4 USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot_4',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.tablename3'\n ) \nOUTPUT TO Change_Data_Stream ;\n\nend flow agentflow;\n\ncreate flow serverflow;\n\nCREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM Change_Data_Stream;\n\nCREATE OR REPLACE TARGET PostgreSQL_Tgt1 USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy: 'EventCount:1,Interval:60',\nCommitPolicy: 'EventCount:1,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.tablename0, public.target0;public.tablename1, public.target0;public.tablename2, public.target0;public.tablename3, public.target0;',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM Change_Data_Stream;\n\nCREATE OR REPLACE TARGET PostgreSQL_Tgt2 USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy: 'EventCount:1,Interval:60',\nCommitPolicy: 'EventCount:1,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.tablename0, public.target1;public.tablename1, public.target1;public.tablename2, public.target1;public.tablename3, public.target1;',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM Change_Data_Stream;\n\nCREATE OR REPLACE TARGET PostgreSQL_Tgt3 USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy: 'EventCount:1,Interval:60',\nCommitPolicy: 'EventCount:1,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.tablename0, public.target2;public.tablename1, public.target2;public.tablename2, public.target2;public.tablename3, public.target2;',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM Change_Data_Stream;\n\nCREATE OR REPLACE TARGET PostgreSQL_Tgt4 USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy: 'EventCount:1,Interval:60',\nCommitPolicy: 'EventCount:1,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.tablename0, public.target3;public.tablename1, public.target3;public.tablename2, public.target3;public.tablename3, public.target3;',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM Change_Data_Stream;\n\nend flow serverflow;\nend application Postgres_To_PostgresApp;\ndeploy application Postgres_To_PostgresApp with agentflow on any in agents, serverflow in default;\nstart Postgres_To_PostgresApp;\n\n\n\n\n\n\n\nstop application Postgres_To_PostgresApp2;\nundeploy application Postgres_To_PostgresApp2;\ndrop application Postgres_To_PostgresApp2 cascade;\n\nCREATE APPLICATION Postgres_To_PostgresApp2 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW agentflow2;\n\nCREATE OR REPLACE SOURCE Postgres_Src21 USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot_1',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.tablename0'\n ) \nOUTPUT TO Change_Data_Stream2 ;\n\nCREATE OR REPLACE SOURCE Postgres_Src22 USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot_2',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.tablename1'\n ) \nOUTPUT TO Change_Data_Stream2 ;\n\n\nCREATE OR REPLACE SOURCE Postgres_Src23 USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot_3',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.tablename2'\n ) \nOUTPUT TO Change_Data_Stream2 ;\n\nCREATE OR REPLACE SOURCE Postgres_Src24 USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot_4',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.tablename3'\n ) \nOUTPUT TO Change_Data_Stream2 ;\n\nend flow agentflow2;\n\ncreate flow serverflow2;\n\nCREATE OR REPLACE TARGET Postgres_Sys2 USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM Change_Data_Stream2;\n\nCREATE OR REPLACE TARGET PostgreSQL_Tgt21 USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy: 'EventCount:1,Interval:60',\nCommitPolicy: 'EventCount:1,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.tablename0, public.target0;public.tablename1, public.target0;public.tablename2, public.target0;public.tablename3, public.target0;',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM Change_Data_Stream2;\n\nCREATE OR REPLACE TARGET PostgreSQL_Tgt22 USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy: 'EventCount:1,Interval:60',\nCommitPolicy: 'EventCount:1,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.tablename0, public.target1;public.tablename1, public.target1;public.tablename2, public.target1;public.tablename3, public.target1;',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM Change_Data_Stream2;\n\nCREATE OR REPLACE TARGET PostgreSQL_Tgt23 USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy: 'EventCount:1,Interval:60',\nCommitPolicy: 'EventCount:1,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.tablename0, public.target2;public.tablename1, public.target2;public.tablename2, public.target2;public.tablename3, public.target2;',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM Change_Data_Stream2;\n\nCREATE OR REPLACE TARGET PostgreSQL_Tgt24 USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy: 'EventCount:1,Interval:60',\nCommitPolicy: 'EventCount:1,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.tablename0, public.target3;public.tablename1, public.target3;public.tablename2, public.target3;public.tablename3, public.target3;',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM Change_Data_Stream2;\n\nend flow serverflow2;\n\nend application Postgres_To_PostgresApp2;\ndeploy application Postgres_To_PostgresApp2 with agentflow2 on any in agents, serverflow2 in default;\nstart Postgres_To_PostgresApp2;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create namespace ns1;\ncreate namespace ns2;\n\nuse ns1;\n\ncreate application group group1;\n\nCREATE or REPLACE  APPLICATION testgroupapp1 RECOVERY 15 SECOND INTERVAL;\nend application testgroupapp1;\n\nCREATE or REPLACE  APPLICATION testgroupapp2 RECOVERY 15 SECOND INTERVAL;\nend application testgroupapp2;\n\nuse ns2;\ncreate application group group2;\n\nCREATE or REPLACE  APPLICATION testgroupapp1 RECOVERY 15 SECOND INTERVAL;\nend application testgroupapp1;\n\nCREATE or REPLACE  APPLICATION testgroupapp2 RECOVERY 15 SECOND INTERVAL;\nend application testgroupapp2;\n\nalter application group group1 add ns1.testgroupapp1,testgroupapp1;\n\nuse ns1;\nalter application group group2 add testgroupapp2,ns2.testgroupapp2;\n\nuse admin;\n\ncreate user PosTester identified by PosTester;\n\ndrop role PosTester.enduser; drop role PosTester.useradmin;\ndrop role PosTester.dev; drop user PosTester cascade;\ndrop namespace PosTester;\ncreate user PosTester identified by PosTester;\ndrop role PosTester.enduser; drop role PosTester.useradmin;\ndrop role PosTester.dev; drop user PosTester cascade;\ndrop namespace PosTester;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE KafkaReaderSource USING Global.KafkaReader VERSION '2.1.0' \n( \nAutoMapPartition: true, \nbrokerAddress: 'localhost:9092',       \nKafkaConfigPropertySeparator: ',',\nTopic: 'test011', \nstartOffset: 0,\nKafkaConfigValueSeparator: ':', \nKafkaConfig: 'request.timeout.ms:6001,session.timeout.ms:6000,security.protocol:SASL_SSL,sasl.mechanism:SCRAM-SHA-512,sasl.jaas.config:org.apache.kafka.common.security.scram.ScramLoginModule required username=kafkauser password=\\\"Oppenheimer\\\";,value.deserializer:com.striim.avro.deserializer.LengthDelimitedAvroRecordDeserializer' \n) \nPARSE USING Global.DSVParser () \nOUTPUT TO KafkaDevReadSourceStream;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCreate Source @SourceName@ Using Ojet\n(\nUsername:'@OJET-UNAME@',\nPassword:'@OJET-PASSWORD@',\nConnectionURL:'@OCI-URL@',\nTables:'@SourceTable@',\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\nUsername:'@UN@',\nPassword:'@PWD@',\nBatchPolicy:'EventCount:1,Interval:1',\nTables: '@Tablemapping@'\n)INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop OracleToKudu;\nundeploy application OracleToKudu;\ndrop application OracleToKudu cascade;\n\nCREATE APPLICATION OracleToKudu;\n\nCreate Source oracSource\n Using DatabaseReader\n(\n Username:'oracle_username',\n Password:'oracle_password',\n ConnectionURL:'oracle_connection',\n Tables:'src_tables',\n Query:\"SELECT * FROM qatest.oracle_alldatatypes\",\n OnlineCatalog:true,\n FetchSize:10000,\n QueueSize:2048,\n CommittedTransactions:true,\n Compression:false\n) Output To DataStream;\nCREATE TARGET WriteintoKudu using KuduWriter (\nkuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',\npkuPDaTehAnDliNgmOdE:'DELETEANDINSERT',\ntables: 'tgt_tables',\nbatchpolicy: 'EventCount:10000,Interval:30')\nINPUT FROM DataStream;\n\nEND APPLICATION OracleToKudu;\ndeploy application OracleToKudu in default;\nstart OracleToKudu;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application JMSWriter.JMS;\nundeploy application JMSWriter.JMS;\ndrop application JMSWriter.JMS cascade;\n\ncreate application JMS;\ncreate source JMSCSVSource using FileReader (\n        directory:'@TEST-DATA-PATH@',\n        WildCard:'AdhocQueryData2.csv',\n        positionByEOF:false,\n        charset:'UTF-8'\n)\nparse using DSVParser (\n        header:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target JmsTarget  using JMSWriter (\n\t\tProvider:'@JMSWRITERPROVIDER@',\n\t\tCtx:'@JMSWRITERCONTEXT@',\n\t\tmessagetype: @MESSAGETYPE@,\n\t\tUserName:'@JMSWRITERUSERNAME@',\n\t\tPassword:'@JMSWRITERPASSWORD@',\n\t\t@DESTINATIONTYPE@)\nformat using @JMSTARGETFORMATTERTYPE@ (\n@JMSTARGETFORMATTERMEMBERS@\n)\ninput from TypedCSVStream;\n\n--SECOND TARGET--\n\nCreate Type SecondType (\n  zip String,\n  city String\n);\n\nCreate Stream SecondStream of SecondType;\n\nCREATE CQ SecondCQ\nINSERT INTO SecondStream\nSELECT data[9],data[10]\nFROM CsvStream;\n\ncreate Target SecondTarget using JMSWriter (\n\t\tProvider:'@JMSWRITERPROVIDER@',\n\t\tCtx:'@JMSWRITERCONTEXT@',\n\t\tmessagetype: @MESSAGETYPE@,\n\t\tUserName:'@JMSWRITERUSERNAME@',\n\t\tPassword:'@JMSWRITERPASSWORD@',\n\t\t@DESTINATIONTYPE@)\nformat using @SECONDTARGETFORMATTERTYPE@ (\n@SECONDTARGETFORMATTERMEMBERS@\n)\ninput from SecondStream;\n\nend Application Jms;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@;\n\nCREATE SOURCE @parquetsrc@ USING FileReader (\nwildcard: '',\ndirectory: '',\npositionbyeof: false )\nPARSE USING ParquetParser (\n)\nOUTPUT TO @appname@Stream;\n\nCREATE CQ @appname@CQ\nINSERT INTO @appname@CqOut\n    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;\n\nCREATE OR REPLACE TARGET @filetarget@ USING FileWriter (\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:1,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING ParquetFormatter  (\nschemaFileName: ''\n)\nINPUT FROM @appname@CqOut;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP bq;\nUNDEPLOY APPLICATION bq;\nDROP APPLICATION bq CASCADE;\n\nCREATE APPLICATION bq RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE S USING OracleReader\n(\n\tUsername: 'qatest',\n\tPassword: 'qatest',\n\tConnectionURL: 'dockerhost:1521:orcl',\n\tTables: 'QATEST.TABLE_TEST_1000100',\n\tDictionaryMode: offlineCatalog,\n\tFetchSize: '1'\n)\nOUTPUT TO SS;\n\n\nCREATE or replace TARGET T USING BigQueryWriter (\n\tserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\n\tprojectId: 'bigquerywritertest',\n    Tables:'QATEST.TABLE_TEST_1000100,qatest.% keycolumns(RONUM)',\n    mode:'Appendonly',\n    datalocation: 'US',\n\tnullmarker: 'defaultNULL',\n\tcolumnDelimiter: '|',\n\tBatchPolicy: 'eventCount:100,Interval:10'\t\n) INPUT FROM ss;\n\nEND APPLICATION bq;\nDEPLOY APPLICATION bq;\nSTART APPLICATION bq;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application RedshiftColmap;\nundeploy application RedshiftColmap;\ndrop application RedshiftColmap CASCADE;\ncreate application RedshiftColmap recovery 1 second interval;\nCREATE OR REPLACE SOURCE OracleSource USING OracleReader  (\n  Username: 'miner',\n  Password: 'miner',\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.redshift_emp',\n  FetchSize: 1\n )\nOUTPUT TO LogminerStream;\nCREATE  TARGET RedshiftTarget USING RedshiftWriter  (\n  ConnectionURL: '@URL@',\n  Username: '@USERNAME@',\n  Password: '@PASSWORD@',\n  bucketname: 'striim-s3',\n  --accesskeyId: 'AKIAJFHRZ7VIH2GHON5A',\n  --secretaccesskey: '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',\n  S3IAMRole:'@IAMROLE@',\n  Tables: 'QATEST.redshift_emp,qatest.EMPLOYEE COLUMNMAP(E_DOJ = DOJ, E_NAME = NAME,E_ID = ID)',\n  uploadpolicy: 'eventcount:3,interval:5s',\n  QuoteCharacter: '@QUOTECHARACTER@',\n  Mode: 'incremental',\n  ColumnDelimiter: '|'\n )\nINPUT FROM LogminerStream;\nEND APPLICATION RedshiftColmap;\ndeploy application RedshiftColmap;\nSTART application RedshiftColmap;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\ncreate application @appname@ recovery 1 second interval;\n\nCREATE OR REPLACE SOURCE @parquetsrc@ USING Global.HDFSReader (\n  eofdelay: 100,\n  wildcard: '@File@',\n  rolloverstyle: 'Default',\n  directory: '@DIR@',\n  adapterName: 'HDFSReader',\n  hadoopurl: 'hdfs://dockerhost:9000',\n  hadoopconfigurationpath: '@CONF@',\n  skipbom: true,\n  includesubdirectories: false,\n  positionbyeof: false )\n  PARSE USING ParquetParser (\n   )\nOUTPUT TO @appname@ParquetStreams;\n\nCREATE OR REPLACE CQ @appname@CQOrder3\nINSERT INTO @appname@newStream3\nSELECT\nPUTUSERDATA(s,'schemaName',s.data.getSchema().getName())\nFROM @appname@ParquetStreams s;\n\nCREATE TARGET @avrotarget@ USING Global.FileWriter (\n  DataEncryptionKeyPassphrase: '',\n  flushpolicy: 'EventCount:10000,Interval:30s',\n  directory: '@FOLDER@',\n  rolloveronddl: 'true',\n  encryptionpolicy: '',\n  filename: 'AvroOutFile',\n  DataEncryptionKeyPassphrase_encrypted: 'true',\n  flushpolicy: 'EventCount:10,Interval:30s',\n  rolloverpolicy: 'EventCount:10,Interval:30s' )\nFORMAT USING Global.AvroFormatter  (\n  schemaFileName: 'AvroSchema',\n  formatAs: 'default',\n  schemaregistryConfiguration: '' )\nINPUT FROM @appname@newStream3;\n\nCREATE TARGET @parquettarget@ USING Global.FileWriter (\n  DataEncryptionKeyPassphrase: '',\n  flushpolicy: 'EventCount:10000,Interval:30s',\n  directory: '@FOLDER@',\n  rolloveronddl: 'true',\n  encryptionpolicy: '',\n  filename: 'AvroOutFile',\n  DataEncryptionKeyPassphrase_encrypted: 'true',\n  flushpolicy: 'EventCount:10,Interval:30s',\n  rolloverpolicy: 'EventCount:10,Interval:30s' )\nFORMAT USING Global.ParquetFormatter  (\n  blocksize: '128000000',\n  compressiontype: 'UNCOMPRESSED',\n  formatAs: 'Default',\n  handler: 'com.webaction.proc.ParquetFormatter',\n  formatterName: 'ParquetFormatter' )\nINPUT FROM @appname@newStream3;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop bq;\nUndeploy application bq;\nalter application bq;\n--CREATE CQ cq1\n--INSERT INTO OpsStream\n--SELECT  \n--CASE WHEN (META(a,\"OperationName\").toString() == \"INSERT\")\n--THEN putUserData(a, 'ops_name', 'I') \n--Else putUserData(a, 'ops_name', 'NOT_INSERT') \n--END\n--FROM ss a;\nCREATE OR REPLACE TARGET T USING BigQueryWriter  ( \n  serviceAccountKey: '/Users/saranyad/Product/IntegrationTests/TestData/google-gcs.json',\n  projectId: 'bigquerywritertest',\n  Tables: '@TABLE@',\n  datalocation: 'US',\n  nullmarker: 'null',\n  columnDelimiter: '|',\n  Mode: 'Merge',\n  _h_throwExceptionOnTableNotFound:'false',\n  IgnorableExceptioncode:'TABLE_NOT_FOUND',\n  BatchPolicy: 'eventcount:100,interval:10'\n ) INPUT FROM ss;\nalter application bq recompile;\ndeploy application bq;\nStart bq;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCreate Source @SOURCE@\n        Using OracleReader\n(\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: 'qatest.test77',\n  FetchSize:1,\n  QueueSize:2000,\n  CommittedTransactions:true,\n  Compression:false\n)\nOutput To @STREAM@;\n\ncreate Target @TARGET@ using S3Writer(\n    bucketname:'@BUCKET@',\n   objectname:'upgradeData.csv',\n   foldername:'upgradefolder',\n  uploadpolicy:'EventCount : 10000,Interval :1m '\n)\nformat using DSVFormatter (\n)\ninput from @STREAM@;\n\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n--CREATE APPLICATION @APPNAME@;\ncreate application @APPNAME@ Recovery 5 second Interval;\n\n--create or replace flow @APPNAME@_agentflow;\n\nCREATE OR REPLACE SOURCE @APPNAME@_source USING OracleReader  (\n  Compression: false,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  TransactionBufferDiskLocation: '.striim/LargeBuffer',\n  FetchSize: 1,\n  DDLCaptureMode: 'All',\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'localhost:1521:xe',\n Tables: 'QATEST.Source1',\n-- Tables: 'QATEST.OracToCql_alldatatypes',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  TransactionBufferType: 'Memory',\n  DictionaryMode: 'OnlineCatalog',\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  TransactionBufferSpilloverSize: '1MB'\n )\nOUTPUT TO @APPNAME@_stream ;\n\n--end flow @APPNAME@_@APPNAME@_agentflow;\n\nCREATE OR REPLACE TARGET @APPNAME@_target USING CassandraCosmosDBWriter  (\n  AccountEndpoint: 'cassandracosmostest.cassandra.cosmos.azure.com',\n  AccountKey: 'pqDZvVgbdSCg7VzIzD77dAhPG2odGRZPLhAQA1qnZbAKoIDk6RuQX5r2phbRQFnR1l54qxOcvBXNdz8DeijYIg==',\n  KeySpace: 'test',\n  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',\n  Tables: 'QATEST.Source1,test.target1',\n  adapterName: 'CassandraCosmosDBWriter'\n )\n INPUT FROM @APPNAME@_stream;\n\n END APPLICATION @APPNAME@;\n\ndeploy application @APPNAME@;\n --deploy application @APPNAME@ with agentflow in agents;\n start application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application MSSQLTransactionSupportFTBTrue;\nundeploy application MSSQLTransactionSupportFTBTrue;\ndrop application MSSQLTransactionSupportFTBTrue cascade;\n\nCREATE APPLICATION MSSQLTransactionSupportFTBTrue recovery 1 second interval;\n\nCreate Source ReadFromMSSQL1\nUsing MSSqlReader\n(\nUsername:'@READER-NAME@',\nPassword:'@READER-PASSWORD@',\nDatabaseName:'@SRCDB-NAME@',\nConnectionURL:'@CONN-URL@',\nTables:'@WATABLES-SRC@',\nTransactionSupport: 'true',\nFetchTransactionMetadata:'false',\nFilterTransactionBoundaries: true,\nCompression:'false',\nConnectionPoolSize:1\n)\nOutput To MSSQLTransactionSupportFTBTrueStream;\n\n\nCREATE TARGET WriteToMSSQL1 USING DatabaseWriter(\nConnectionURL:'@TGT-URL@',\nUsername:'@WRITER-UNAME@',\nPassword:'@WRITER-PASSWORD@',\nBatchPolicy:'EventCount:5,Interval:5',\nCommitPolicy:'EventCount:1,Interval:1',\nTables: '@WATABLES-SRC@,@WATABLES-TGT@'\n)\nINPUT FROM MSSQLTransactionSupportFTBTrueStream;\n\nCREATE TARGET MSSqlReaderOutput1 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportFTBTrueStream; \n\n\nCREATE OR REPLACE TARGET MSSQLFileOut1 USING FileWriter  ( \n  rolloverpolicy: 'EventCount:10000,Interval:30s',\n  adapterName: 'FileWriter',\n  directory:'@FEATURE-DIR@/logs/',\n  filename: 'TransactionSupportFTBTrue.txt'\n ) \nFORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true',\n  jsonobjectdelimiter: '\\n'\n ) \nINPUT FROM MSSQLTransactionSupportFTBTrueStream;\n\nEND APPLICATION MSSQLTransactionSupportFTBTrue;\ndeploy application MSSQLTransactionSupportFTBTrue;\nstart application MSSQLTransactionSupportFTBTrue;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP DSLAPP;\nUNDEPLOY APPLICATION DSLAPP;\nDROP APPLICATION DSLAPP CASCADE;\n\nCREATE APPLICATION DSLAPP;\n\n-- CacheWaction WACTIONSTORE is being loaded from DSCache\n\nCREATE OR REPLACE WACTIONSTORE CacheWactionDSL CONTEXT OF DS.T1\nEVENT TYPES ( DS.T1 )\n@PERSIST-TYPE@\n\nCREATE CQ DSLDerby\nINSERT INTO CacheWactionDSL\nselect * from DS.C1\nLINK SOURCE EVENT;\n\nEND APPLICATION DSLAPP;\nDEPLOY APPLICATION DSLAPP;\nSTART APPLICATION DSLAPP;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d';\n\nCREATE OR REPLACE SOURCE @SOURCE@1 USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n )\nOUTPUT TO @STREAM@1;\n\nCREATE OR REPLACE SOURCE @SOURCE@2 USING PostgreSQLReader  (\n    ReplicationSlotName:'test_slot',\n    FilterTransactionBoundaries:'true',\n    Username:'@SOURCE_USER@',\n    Password_encrypted:false,\n    ConnectionURL:'@CONNECTION_URL@',\n    adapterName:'PostgreSQLReader',\n    ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',\n    Password:'@SOURCE_PASS@',\n    Tables:'@SOURCE_TABLE@',\n    ExcludedTables:'public.chkpoint',\n )\nOUTPUT TO @STREAM@1;\n\nCREATE OR REPLACE SOURCE @SOURCE@3 USING MySQLReader (\n    Username: '@READER-UNAME@',\n    Password: '@READER-PASSWORD@',\n    ConnectionURL: '@CDC-READER-URL@',\n    Tables: @WATABLES@,\n    sendBeforeImage:'true',\n    FilterTransactionBoundaries: 'true',\n    ExcludedTables: 'waction.CHKPOINT'\n)\nOUTPUT TO @STREAM@1;\n\n -- CREATE OR REPLACE SOURCE @SOURCE@4 USING MariaDbXpandReader\n -- (\n -- Username: '@READER-UNAME@',\n -- Password: '@READER-PASSWORD@',\n -- ConnectionURL: '@CDC-READER-URL@',\n -- Tables: @WATABLES@,\n -- sendBeforeImage:'true',\n -- FilterTransactionBoundaries: 'true',\n -- ExcludedTables: 'test.CHKPOINT'\n -- )\n -- OUTPUT TO @STREAM@1;\n\nCREATE OR REPLACE TARGET @TARGET@1 USING SnowflakeWriter (\n  ConnectionURL: '@SNOWFLAKE-URL@',\n  username: '@SNOWFLAKE-USERNAME@',\n  password: '@SNOWFLAKE-PASSWORD@',\n  appendOnly:'false',\n  uploadPolicy: 'eventcount:1,interval:5m',\n  Tables: '@TARGET-TABLES@'\n )\nINPUT FROM @STREAM@1;\n\nCREATE OR REPLACE TARGET @TARGET@2 USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'@WATABLES@',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:10',\nStandardSQL:true\n)\nINPUT FROM @STREAM@1;\n\n\n\nEND APPLICATION @APPNAME@;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION eh;\nUNDEPLOY APPLICATION eh;\nDROP APPLICATION eh CASCADE;\nCREATE APPLICATION eh @Recovery@;\ncreate flow AgentFlow;\nCREATE OR REPLACE SOURCE s USING IncrementalBatchReader  ( \n  FetchSize: 1,\n  StartPosition:'QATEST.IBR01=-1;QATEST.IBR02=-1;QATEST.IBR03=0;QATEST.IBR04=0;QATEST.IBR05=1;QATEST.IBR06=2018-09-20 06:43:59;QATEST.%=-1',\n  Username: 'qatest',\n  ConnectionURL: 'jdbc:oracle:thin:@//dockerhost:1521/orcl',\n  Tables: 'QATEST.IBR%',\n  CheckColumn: 'QATEST.IBR01=id;QATEST.IBR03=id;QATEST.IBR05=id;QATEST.%=t1',\n  Password: 'qatest' ) \nOUTPUT TO sourcestream ;\n\n\nCREATE TYPE cdctype(\n  id int,\n  name String  \n);\n\nCREATE STREAM cdctypestream OF cdctype;\n\nCREATE CQ cdcstreamcq\nINSERT INTO cdctypestream\nSELECT TO_INT(p.data[0]), \n       TO_STRING(p.data[1])\nFROM sourcestream p;\n\nend flow AgentFlow;\n\ncreate flow serverFlow;\n\ncreate Target t1_dsv using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_01',\n\tE1P:'true',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',\n\tPartitionKey:'id',\n\tOperationTimeoutMS:'120000',\n    BatchPolicy:'Size:256000,Interval:30s',\n    ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n\t--ParallelThreads:'2'\n)\nformat using DSVFormatter ( \n)\ninput from cdctypestream;\n\ncreate Target t2_json using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_02',\n\tE1P:'true',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',\n\tPartitionKey:'id',\n\tOperationTimeoutMS:'140000',\n\tBatchPolicy:'Size:200000,Interval:1m',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m',\n\tConsumerGroup:'reader')\nformat using JSONFormatter ( \n)\ninput from cdctypestream;\n\ncreate Target t3_avro using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_03',\n\tE1P:'true',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',\n\tPartitionKey:'id',\n\tOperationTimeoutMS:'140000',\n\tBatchPolicy:'Size:256000,Interval:1h',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m',\n\tConsumerGroup:'reader')\nformat using AvroFormatter (\nschemaFileName:'kafkaAvroTest_Agent_ibr_orcl.avsc'\n)input from cdctypestream;\nend flow serverFlow;\n\nEND APPLICATION eh;\n--deploy application eh;\ndeploy application eh with AgentFlow in Agents, ServerFlow in default;\n\nstart application eh;\n\nSTOP APPLICATION ER;\nUNDEPLOY APPLICATION ER;\nDROP APPLICATION ER CASCADE;\nCREATE APPLICATION ER;\nCREATE STREAM ER_SS2 OF Global.JsonNodeEvent;\n\nCREATE SOURCE ER_S1 USING AzureEventHubReader (\n    EventHubNamespace:'qatestnov9',\n\tEventHubName:'test_2_partitions',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'Default',\n\tstartSeqNo:'0'\t\n\t)\nPARSE USING dsvParser (\n)OUTPUT TO ER_SS1;\n\nCREATE SOURCE ER_S2 USING AzureEventHubReader (\n    EventHubNamespace:'qatestnov9',\n\tEventHubName:'test_2_partitions',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'Default',\n\tstartSeqNo:'0'\t\n\t)\nPARSE USING jsonparser (members:'data')\nOUTPUT TO ER_SS2;\n\n\nCREATE SOURCE ER_S3 USING AzureEventHubReader (\n    EventHubNamespace:'qatestnov9',\n\tEventHubName:'test_2_partitions',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'Default',\n\tstartSeqNo:'0'\t\n\t)\nPARSE USING avroParser (\nschemaFileName:'kafkaAvroTest_Agent_ibr_orcl.avsc'\n)OUTPUT TO ER_SS3;\n\n\ncreate Type CustType \n(writerdata com.fasterxml.jackson.databind.JsonNode,\nTopicName java.lang.String,\nPartitionID java.lang.String);\n\nCreate Stream datastream3 of CustType;\n\nCREATE CQ CustCQ3\nINSERT INTO datastream3\nSELECT AvroToJson(s3.data),\nmetadata.get(\"TopicName\").toString() AS TopicName,\nmetadata.get(\"PartitionID\").toString() AS PartitionID\nFROM ER_SS3 s3;\n\n\n\ncreate Target ER_t1 using FileWriter (\nfilename:'FT1_5L_AVRO_RT',\ndirectory:'FEATURE-DIR/logs/',\nrolloverpolicy:'eventcount:5000000'\t\n)\nformat using dsvFormatter()\ninput from ER_SS1;\n\ncreate Target ER_t2 using FileWriter (\nfilename:'FT2_JSON_RT',\ndirectory:'FEATURE-DIR/logs/',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000'\t\n)\nformat using jsonFormatter(members: 'data' )\ninput from ER_SS2;\n\ncreate Target ER_t3 using FileWriter (\nfilename:'FT2_JSON_RT',\ndirectory:'FEATURE-DIR/logs/',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000'\t\n)\nformat using dsvFormatter()\ninput from datastream3;\n\n\nend application ER;\ndeploy application ER;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 40 with two sources and two WactionStores. A variety of partitioned windows in between\n-- assure that we are testing a complicated recovery scenario.\n--\n-- NOTE THIS APP IS INCONSISTENT AND NOT COMPATIBLE WITH THE CURRENT VERSION OF RECOVERY BECAUSE IT HAS COMBINING STREAMS\n--\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> JWc2 -> JWc5 -> WS1\n--   S2 -> JWc2 -> JWc7 -> WS2\n--\n\nSTOP Recov40Tester.RecovTest40;\nUNDEPLOY APPLICATION Recov40Tester.RecovTest40;\nDROP APPLICATION Recov40Tester.RecovTest40 CASCADE;\nCREATE APPLICATION RecovTest40 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStreamTop OF CsvData;\n\nCREATE CQ Csv1ToDataStreamTop\nINSERT INTO DataStreamTop\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ Csv2ToDataStreamTop\nINSERT INTO DataStreamTop\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\n\n\n\n\n\n\n\nCREATE JUMPING WINDOW TopJWc2\nOVER DataStreamTop KEEP 2 ROWS;\n\n\n\n\n\n\n\nCREATE STREAM DataStreamLeft OF CsvData;\nCREATE STREAM DataStreamRight OF CsvData;\n\nCREATE CQ DataToLeft\nINSERT INTO DataStreamLeft\nSELECT\n    FIRST(p.merchantId),\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    TO_DOUBLE(FIRST(p.amount))\nFROM TopJWc2 p;\n\nCREATE CQ DataToRight\nINSERT INTO DataStreamRight\nSELECT\n    FIRST(p.merchantId),\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    TO_DOUBLE(FIRST(p.amount))\nFROM TopJWc2 p;\n\n\n\n\n\n\nCREATE JUMPING WINDOW LeftJWc5\nOVER DataStreamLeft KEEP 5 ROWS;\n\nCREATE JUMPING WINDOW RightJWc10\nOVER DataStreamRight KEEP 10 ROWS;\n\n\n\nCREATE WACTIONSTORE WactionsLeft CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE WactionsRight CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ ToWactionsLeft\nINSERT INTO WactionsLeft\nSELECT\n    FIRST(p.merchantId),\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    TO_DOUBLE(FIRST(p.amount))\nFROM LeftJWc5 p;\n\nCREATE CQ ToWactionsRight\nINSERT INTO WactionsRight\nSELECT\n    FIRST(p.merchantId),\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    TO_DOUBLE(FIRST(p.amount))\nFROM RightJWc10 p;\n\nEND APPLICATION RecovTest40;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ recovery 5 second interval;\n\nCREATE  SOURCE @QuerySource@1 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@1;\ncreate Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;\n CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@1;\n\nCREATE  SOURCE @QuerySource@2 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@2;\ncreate Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;\n CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@2;\n\nCREATE  SOURCE @QuerySource@3 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@3;\ncreate Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;\n CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@3;\n\nCREATE  SOURCE @QuerySource@4 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@4;\ncreate Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;\n CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@4;\n\nCREATE  SOURCE @QuerySource@5 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@5;\ncreate Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;\n CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@5;\n\nCREATE  SOURCE @QuerySource@6 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@6;\ncreate Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;\n CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@6;\n\nCREATE  SOURCE @QuerySource@7 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@7;\ncreate Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;\n CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@7;\n\nCREATE  SOURCE @QuerySource@8 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@8;\ncreate Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;\n CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@8;\n\nCREATE  SOURCE @QuerySource@9 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@9;\ncreate Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;\n CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@9;\n\nCREATE  SOURCE @QuerySource@10 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@10;\ncreate Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;\n CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@10;\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;\nCREATE OR REPLACE SOURCE @AppName@_Source USING FileReader (\ndirectory:'@dataDir@',\nwildcard:'data.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO @AppName@_rawstream;\n\n\nCREATE CQ @BuiltinFunc@CQ\nINSERT INTO @BuiltinFunc@_Stream\nSELECT @BuiltinFunc@(x, 'Last_Date', data[5], 'Country', data[10])\nFROM @AppName@_rawstream x;\n\nCREATE OR REPLACE CQ cq1\nINSERT INTO RemoveUserData_Stream\nSELECT\nremoveUserData(s1, 'Last_Date')\nFROM @BuiltinFunc@_Stream s1;\n\n\nCREATE OR REPLACE TARGET @AppName@_FileTarget USING Global.FileWriter ( \n  flushpolicy: 'EventCount:10000,Interval:30s', \n  adapterName: 'FileWriter', \n  directory: '@logs@',\n  filename: '@BuiltinFunc@_RemoveData', \n  rolloverpolicy: 'EventCount:10000,Interval:30s' ) \nFORMAT USING Global.JSONFormatter  ( \n  handler: 'com.webaction.proc.JSONFormatter', \n  jsonMemberDelimiter: '\\n', \n  EventsAsArrayOfJsonObjects: 'true', \n  formatterName: 'JSONFormatter', \n  jsonobjectdelimiter: '\\n' ) \nINPUT FROM RemoveUserData_Stream;\n\nEnd application @AppName@;\nDeploy application @AppName@; \nStart application @AppName@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:100,Interval:0',\n  CommitPolicy: 'EventCount:100,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  --Password_encrypted: false\n ) \nINPUT FROM @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( \n  Username: '@SRC_USERNAME@',\n  Password: '@SRC_PASSWORD@',\n  ConnectionURL: '@CDC_URL@',\n  Tables: '@Source1Tables@' ) \nOUTPUT TO @APPNAME@cdcStream;\n\nCREATE OR REPLACE EXTERNAL CACHE @APPNAME@1 ( \n  AdapterName:'DatabaseReader',\n  Username: '@SRC_USERNAME@',\n  Password: '@SRC_PASSWORD@',\n  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',\n  FetchSize: 20,\n  DatabaseProviderType: 'Default',\n  Table: '@Source3Tables@',\n  Columns: 'col1,col2,col3,col4,uniquecol',\n  keytomap: 'uniquecol')  \nOF @APPNAME@cachetype;\n\nCREATE OR REPLACE EXTERNAL CACHE @APPNAME@2 ( \n  AdapterName:'DatabaseReader',\n  Username: '@SRC_USERNAME@',\n  Password: '@SRC_PASSWORD@',\n  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',\n  FetchSize: 10,\n  DatabaseProviderType: 'Default',\n  Table: '@Source2Tables@',\n  Columns: 'col1,col2,col3,col4,uniquecol',\n  keytomap: 'uniquecol')  \nOF @APPNAME@cachetype;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop IR;\nundeploy application IR;\ndrop application IR cascade;\nCREATE APPLICATION IR recovery 5 second interval;\n\n CREATE OR REPLACE SOURCE Teradata_source_App2 USING IncrementalBatchReader  ( \n  FetchSize: 100,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.test01;striim.test02;striim.test03',\n  adapterName: 'IncrementalBatchReader',\n CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',\n startPosition: 'striim.test01=1;striim.test02=-1;%=0',\n  PollingInterval: '5sec'\n  )\n  OUTPUT TO data_stream1 ;\n\n  CREATE OR REPLACE TARGET sys2 USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM data_stream1;\n\ncreate target AzureSQLDWHTarget_app2 using AzureSQLDWHWriter(\n\t\tConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        accountname: 'striimqatestdonotdelete',\n        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',\n        uploadpolicy:'eventcount:1,interval:10s'\n) INPUT FROM data_stream1;\n\n\nCREATE TARGET log USING LOGWRITER\n(\n name:log,\n filename:'LOGFILENAME'\n)\nINPUT FROM data_stream1;\n\n\nEND APPLICATION IR;\n\ndeploy application IR;\nstart IR;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\\create Target @TARGET_NAME@ using FileWriter(\n  filename:'CompressedMerchant.gz',\n  directory:'/logs/',\n  rolloverpolicy:'EventCount:10000,Interval:30s'\n)\nformat using DSVFormatter (\n\n)\ninput from @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@ Recovery 5 second interval;\n\ncreate stream @APPNAME@_UserdataStream of Global.WAEvent;\n\ncreate type @APPNAME@_Order_type(\nid int,\norder_id int,\nzipcode int,\ncategory String,\ntablename string\n);\n\nCREATE OR REPLACE SOURCE @APPNAME@Postgres_Src1 USING PostgreSQLReader\n(\n  ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: true,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.order_%'\n)\nOUTPUT TO @APPNAME@_OrdersStream;\n\nCREATE OR REPLACE SOURCE @APPNAME@Postgres_Src2 USING PostgreSQLReader\n(\n  ReplicationSlotName: 'test_slot_2',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: true,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.second_order_%'\n)\nOUTPUT TO @APPNAME@_OrdersStream2;\n\nCREATE OR REPLACE SOURCE @APPNAME@Postgres_Src3 USING PostgreSQLReader\n(\n  ReplicationSlotName: 'test_slot_3',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: true,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --startLSN:'758199500005967269518578814280000',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.third_order_%'\n)\nOUTPUT TO @APPNAME@_OrdersStream3;\n\nCREATE OR REPLACE SOURCE @APPNAME@Postgres_Src4 USING PostgreSQLReader\n(\n  ReplicationSlotName: 'test_slot_4',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: true,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --startLSN:'758199500005967269518578814280000',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.fourth_order_%'\n)\nOUTPUT TO @APPNAME@_OrdersStream4;\n\n\nCreate CQ @APPNAME@_CQUser\ninsert into @APPNAME@_UserdataStream\nselect \nputuserdata (data,'Fileowner','FIRST_ORDER') from @APPNAME@_OrdersStream data;\n\n\nCreate CQ @APPNAME@_CQUser2\ninsert into @APPNAME@_UserdataStream\nselect \nputuserdata (data2,'Fileowner','SECOND_ORDER') from @APPNAME@_OrdersStream2 data2;\n\n\nCreate CQ @APPNAME@_CQUser3\ninsert into @APPNAME@_UserdataStream\nselect \nputuserdata (data3,'Fileowner','THIRD_ORDER') from @APPNAME@_OrdersStream3 data3;\n\n\nCreate CQ @APPNAME@_CQUser4\ninsert into @APPNAME@_UserdataStream\nselect \nputuserdata (data4,'Fileowner','FOURTH_ORDER') from @APPNAME@_OrdersStream4 data4;\n\ncreate stream @APPNAME@_OrderTypedStream1 of @APPNAME@_Order_type;\ncreate stream @APPNAME@_OrderTypedStream2 of @APPNAME@_Order_type;\ncreate stream @APPNAME@_OrderTypedStream3 of @APPNAME@_Order_type;\ncreate stream @APPNAME@_OrderTypedStream4 of @APPNAME@_Order_type;\n\nCREATE CQ @APPNAME@_fin_cq\nINSERT INTO @APPNAME@_OrderTypedStream1\nSELECT to_int(data[0]),\nto_int(data[1]),\nto_int(data[2]),\nUSERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),\nMETA(@APPNAME@_UserdataStream,'TableName').toString()\nFROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FIRST_ORDER';\n\nCREATE CQ @APPNAME@_fin_cq2\nINSERT INTO @APPNAME@_OrderTypedStream2\nSELECT to_int(data[0]),\nto_int(data[1]),\nto_int(data[2]),\nUSERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),\nMETA(@APPNAME@_UserdataStream,'TableName').toString()\nFROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'SECOND_ORDER';\n\nCREATE CQ @APPNAME@_fin_cq3\nINSERT INTO @APPNAME@_OrderTypedStream3\nSELECT to_int(data[0]),\nto_int(data[1]),\nto_int(data[2]),\nUSERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),\nMETA(@APPNAME@_UserdataStream,'TableName').toString()\nFROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'THIRD_ORDER';\n\nCREATE CQ @APPNAME@_fin_cq4\nINSERT INTO @APPNAME@_OrderTypedStream4\nSELECT to_int(data[0]),\nto_int(data[1]),\nto_int(data[2]),\nUSERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),\nMETA(@APPNAME@_UserdataStream,'TableName').toString()\nFROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FOURTH_ORDER';\n\n\ncreate Target @APPNAME@_ADLSGen2_tgt1 using ADLSGen2Writer(\n        filename:'event_data.csv',\n        directory:'',\n       \taccountname:'adlsgen2nntest',\n        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',\n        filesystemname:'qatestadls2',\n        uploadpolicy:'eventcount:8,interval:20s'\n)\nformat using DSVFormatter (\n    header:'true'\n)\ninput from @APPNAME@_OrderTypedStream1; \n\ncreate Target @APPNAME@_ADLSGen2_tgt2 using ADLSGen2Writer(\n        filename:'event_data.xml',\n        directory:'',\n\t\taccountname:'adlsgen2nntest',\n        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',\n        filesystemname:'qatestadls2',\n   \t\tuploadpolicy:'eventcount:8,interval:20s'\n)\nformat using XMLFormatter (\n  elementtuple: 'Order_id:id:order_id:zipcode:category:text=tablename',\n  formatterName: 'XMLFormatter',\n  rowdelimiter: '\\n',\n  rootelement: 'document'\n)\ninput from @APPNAME@_OrderTypedStream2; \n\ncreate Target @APPNAME@_ADLSGen2_tgt3 using ADLSGen2Writer(\n        filename:'event_data.avro',\n        directory:'',\n\t\taccountname:'adlsgen2nntest',\n        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',\n        filesystemname:'qatestadls2',\n  \t\tuploadpolicy:'eventcount:8,interval:20s'\n)\nformat using AvroFormatter (\n  formatAs: 'Default',\n  formatterName: 'AvroFormatter',\n  schemaFileName: '@SCHEMA-FILE@'\n)\ninput from @APPNAME@_OrderTypedStream3; \n\n\ncreate Target @APPNAME@_ADLSGen2_tgt4 using ADLSGen2Writer(\n        filename:'event_data.json',\n        directory:'',\n        accountname:'adlsgen2nntest',\n        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',\n        filesystemname:'qatestadls2',\n  \t\tuploadpolicy:'eventcount:8,interval:20s'\n)\nFORMAT USING JSONFormatter()\nINPUT FROM @APPNAME@_OrderTypedStream4;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @SOURCENAME@ USING IncrementalBatchReader  (\n  FetchSize: 10,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: @checkColumn@,\n  startPosition: '@startPosition@',\n  PollingInterval: '20sec'\n  )\n  OUTPUT TO @STREAM@;\n\n  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;\n\n  CREATE TARGET @targetName1@ USING DatabaseWriter(\n    ConnectionURL:'@READER-URL@',\n    Username:'@READER-UNAME@',\n    Password:'@READER-PASSWORD@',\n    BatchPolicy:'Eventcount:1,Interval:1',\n    CommitPolicy:'Eventcount:1,Interval:1',\n    Checkpointtable:'CHKPOINT',\n    Tables:'@WATABLES@,@WATABLES@_target'\n  ) INPUT FROM @STREAM@;\n\n  create Target @targetsys1@ using SysOut(name:@targetsys1@) input from @STREAM@;\n\n    CREATE TARGET @targetName2@ USING DatabaseWriter(\n      ConnectionURL:'@READER-URL@',\n      Username:'@READER-UNAME@',\n      Password:'@READER-PASSWORD@',\n      BatchPolicy:'Eventcount:1,Interval:1',\n      CommitPolicy:'Eventcount:1,Interval:1',\n      Checkpointtable:'CHKPOINT',\n      Tables:'@WATABLES@,@WATABLES@_target1'\n    ) INPUT FROM @STREAM@;\n\n  END APPLICATION @APPNAME@;\n\n  DEPLOY APPLICATION @APPNAME@;\n  start application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE TARGET @TARGET_NAME@ USING SpannerWriter (\n\tTables: 'QATEST.%,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tinstanceId: 'qatest'\n) INPUT FROM @STREAM@;\n\nCREATE TARGET @TARGET_NAME@2 USING SpannerWriter (\n\tTables: 'QATEST.%,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tinstanceId: 'qatest'\n) INPUT FROM @STREAM@;\n\nCREATE TARGET @TARGET_NAME@3 USING SpannerWriter (\n\tTables: 'QATEST.%,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tinstanceId: 'qatest'\n) INPUT FROM @STREAM@;\n\n\nCREATE TARGET @TARGET_NAME@4 USING SpannerWriter (\n\tTables: 'QATEST.%,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tinstanceId: 'qatest'\n) INPUT FROM @STREAM@;\n\nCREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(\n  name: 'CDDL',\n  filename:'WAEventDump.log'\n)INPUT FROM @STREAM@;;\n\ncreate Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE or replace TARGET @TARGET_NAME@ USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'QATEST.TABLE_TEST_%,@DATASET@.%',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:10000,Interval:600',\nStandardSQL:true\t\n) INPUT FROM @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE  SOURCE @SOURCE_NAME@ USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @STREAM@1;\n\nCREATE  SOURCE @SOURCE_NAME@2 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @STREAM@2;\n\nCREATE  SOURCE @SOURCE_NAME@3 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @STREAM@3;\n\nCREATE  SOURCE @SOURCE_NAME@4 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @STREAM@4;\n\nCREATE  SOURCE @SOURCE_NAME@5 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @STREAM@5;\n\nCREATE  SOURCE @SOURCE_NAME@6 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @STREAM@6;\n\nCREATE  SOURCE @SOURCE_NAME@7 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @STREAM@7;\n\nCREATE  SOURCE @SOURCE_NAME@8 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @STREAM@8;\n\nCREATE  SOURCE @SOURCE_NAME@9 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @STREAM@9;\n\nCREATE  SOURCE @SOURCE_NAME@10 USING DatabaseReader  (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Username: 'qatest',\n  Password: 'qatest',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @STREAM@10;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "alter APPLICATION DBRTOCW;\n\n\nCREATE OR REPLACE SOURCE DBSource USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',\n  Tables: 'QATEST.OracToCql_alldatatypes1',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE SOURCE DBSource2 USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',\n  Tables: 'QATEST.OracToCql_alldatatypes2',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE SOURCE DBSource3 USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//192.168.1.142:1521/orcl',\n  Tables: 'QATEST.OracToCql_alldatatypes3',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',\n  Tables: '@Tables@',\n  IgnorableExceptionCode: 'PRIMARY KEY',\n  Password: 'cassandra',\n  Password_encrypted: false\n )\nINPUT FROM Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget2 USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',\n  Tables: '@Tables@',\n  IgnorableExceptionCode: 'PRIMARY KEY',\n  Password: 'cassandra',\n  Password_encrypted: false\n )\nINPUT FROM Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget3 USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:1,Interval:60',\n  CommitPolicy: 'EventCount:1,Interval:60',\n  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',\n  Tables: '@Tables@',\n  IgnorableExceptionCode: 'PRIMARY KEY',\n  Password: 'cassandra',\n  Password_encrypted: false\n )\nINPUT FROM Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget4 USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:1,Interval:120',\n  CommitPolicy: 'EventCount:1,Interval:120',\n  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',\n  Tables: '@Tables@',\n  IgnorableExceptionCode: 'PRIMARY KEY',\n  Password: 'cassandra',\n  Password_encrypted: false\n )\nINPUT FROM Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget5 USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:1,Interval:120',\n  CommitPolicy: 'EventCount:1,Interval:120',\n  ConnectionURL: 'jdbc:cassandra://18.144.18.199:9042/test',\n  Tables: '@Tables@',\n  IgnorableExceptionCode: 'PRIMARY KEY',\n  ExcludedTables:'QATEST.ORACTOCQL_ALLDATATYPES',\n  Password: '+hbb060plSWQwscvI105cg==',\n  Password_encrypted: true\n )\nINPUT FROM Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET FWTarget USING FileWriter(\n\tname:CassandraOuput,\n\tfilename:'OracToFw.log',\n\tflushpolicy : 'interval:120,eventcount:11',\n\trolloverpolicy : 'interval:300s'\n)\nFORMAT USING DSVFormatter()\nINPUT FROM Oracle_ChangeDataStream;\ncreate  or replace Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\n\nEND APPLICATION DBRTOCW;\nALTER APPLICATION DBRTOCW RECOMPILE;\ndeploy application DBRTOCW in default;\nstart DBRTOCW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet\n(\n  ConnectionURL: '@CONNECTION_URL@',\n  FetchSize:1,\n  CommittedTransactions: true,\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OJet',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  Username: '@SOURCE_USER@'\n)OUTPUT TO @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING SnowflakeWriter (\n  ConnectionURL: '@SNOWFLAKE-URL@',\n  username: '@SNOWFLAKE-USERNAME@',\n  password: '@SNOWFLAKE-PASSWORD@',\n  appendOnly:'false',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  Tables: '@TARGET-TABLES@',\n  uploadpolicy:'eventcount:10,interval:5s'\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@2 USING SnowflakeWriter (\n  ConnectionURL: '@SNOWFLAKE-URL@',\n  username: '@SNOWFLAKE-USERNAME@',\n  password: '@SNOWFLAKE-PASSWORD@',\n  appendOnly:'false',\n  BatchPolicy: 'Interval:10',\n  CommitPolicy: 'Interval:10',\n  Tables: '@TARGET-TABLES@',\n  uploadpolicy:'eventcount:10,interval:5s'\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@3 USING SnowflakeWriter (\n  ConnectionURL: '@SNOWFLAKE-URL@',\n  username: '@SNOWFLAKE-USERNAME@',\n  password: '@SNOWFLAKE-PASSWORD@',\n  appendOnly:'false',\n  BatchPolicy: 'eventCount:100000,Interval:20',\n  CommitPolicy: 'eventCount:100000,Interval:20',\n  Tables: '@TARGET-TABLES@',\n  uploadpolicy:'eventcount:10,interval:5s'\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@4 USING SnowflakeWriter (\n  ConnectionURL: '@SNOWFLAKE-URL@',\n  username: '@SNOWFLAKE-USERNAME@',\n  password: '@SNOWFLAKE-PASSWORD@',\n  appendOnly:'false',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  Tables: '@TARGET-TABLES@',\n  uploadpolicy:'eventcount:10,interval:5s'\n )\nINPUT FROM @STREAM@;\n\nCREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(\n  name: 'CDDL',\n  filename:'WAEventDump.log'\n)\nINPUT FROM @STREAM@;;\n\ncreate Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop OracleToKudu;\nundeploy application OracleToKudu;\ndrop application OracleToKudu cascade;\nCREATE APPLICATION OracleToKudu RECOVERY 1 SECOND INTERVAL;\nCREATE  SOURCE oracSource USING OracleReader  ( \n  Username: 'qatest',\n  Password: 'qatest',\n  ConnectionURL: 'jdbc:oracle:thin:@//192.168.56.102:1521/orcl',\n  Tables: 'QATEST.oracle_alldatatypes',\n  OnlineCatalog: true,\n  FetchSize: 1\n ) \nOUTPUT TO DataStream;\nCREATE  TARGET WriteintoKudu USING KuduWriter  ( \n  kuduclientconfig: 'master.addresses->192.168.56.101:7051;socketreadtimeout->240;operationtimeout->1200',\n  pkupdatehandlingmode: 'DELETEANDINSERT',\n  tables: 'QATEST.ORACLE_ALLDATATYPES,KUDU_ALLDATATYPES',\n  batchpolicy: 'EventCount:1,Interval:0'\n ) \nINPUT FROM DataStream;\nEND APPLICATION OracleToKudu;\ndeploy application OracleToKudu in default;\nstart OracleToKudu;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "alter application oraddl;\nCreate or replace Source Ora Using OracleReader \n(\n Username:'@user-name@',\n Password:'@password@',\n ConnectionURL:'src_url',\n Tables:'QATEST.ORACLEDDL%',\n DictionaryMode:OfflineCatalog,\n DDLCaptureMode : 'All',\n FetchSize:2\n) Output To LogminerStream;\nALTER APPLICATION oraddl RECOMPILE;\ndeploy application oraddl;\nSTART application oraddl;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@ RECOVERY 30 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;\n\nCREATE FLOW @AppName@_SourceFlow;\n\nCREATE SOURCE @srcName@ USING ZendeskReader ( \n  MaxConnections: 20, \n  Password_encrypted: 'false', \n  ThreadPoolCount: '10', \n  ConnectionTimeOut: 60, \n  ConnectionPoolSize: '20', \n  MigrateSchema: 'true', \n  Mode: 'Automated', \n  authflow: 'false', \n  Username: '@srcusername@', \n  Password: '@srcpassword@', \n  ConnectionRetries: '3', \n  ZendeskObjects: 'sessions;settings;targets;tickets', \n  AccessToken: '', \n  ConnectionUrl: '@connectionUrl@' ) \nOUTPUT TO @outstreamname@;\n\nEND FLOW @AppName@_SourceFlow;\n\nCREATE TARGET @tgtName@ USING Global.BigQueryWriter ( \n  Tables: '%,zendeskallresize.%', \n  projectId: '@projectId@', \n  batchPolicy: 'eventcount:10000,interval:60', \n  ServiceAccountKey: '@keyFileName@', \n  streamingUpload: 'true', \n  Mode: 'MERGE', \n  CDDLOptions: '{\\\"CreateTable\\\":{\\\"action\\\":\\\"IgnoreIfExists\\\",\\\"options\\\":[{\\\"CreateSchema\\\":{\\\"action\\\":\\\"IgnoreIfExists\\\"}}]}}' ) \nINPUT FROM @outstreamname@;\n\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop ADW;\nundeploy application ADW;\ndrop application ADW cascade;\n\nCREATE APPLICATION ADW recovery 5 second interval;\ncreate flow agentflow;\nCREATE OR REPLACE SOURCE CSVPoller USING FileReader (\ndirectory:'@DIRECTORY@',\nWildCard:'posdata5L.csv',\npositionByEOF:false\n)\nparse using DSVParser (\nheader:'no'\n)\nOUTPUT TO CsvStream;\nend flow agentflow;\n\ncreate flow targetflow;\nCREATE OR REPLACE TYPE CSVStream_Type  ( BUSINESS_NAME java.lang.String KEY,\nMERCHANT_ID java.lang.String,\nPRIMARY_ACCOUNT_NUMBER java.lang.String\n ) ;\n\nCREATE OR REPLACE STREAM CSVTypeStream OF CSVStream_Type;\nCREATE OR REPLACE CQ CQ1\nINSERT INTO CSVTypeStream\nSELECT data[0],data[1],data[2]\nFROM CsvStream;\n\ncreate target WriteToAzureSQLWH using AzureSQLDWHWriter (\n       ConnectionURL: '@CONNECTION-URL@',\n       username: '@USERNAME@',\n       password: '@PASSWORD@',\n\t   AccountName: '@STORAGE-ACCOUNT@',\n       AccountAccessKey: '@ACCESS-KEY@',\n      Tables: '@TARGET-TABLE@',\n      uploadpolicy:'eventcount:20000,interval:1m'\n) INPUT FROM CSVTypeStream;\nend flow targetflow;\nEND APPLICATION ADW;\ndeploy application ADW with agentflow in agents,targetflow in default;\nstart ADW;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION DsPosApp;\n\nCREATE OR REPLACE CQ CQPosApp\nINSERT INTO PosStrm\nselect * from DS.MerchantActivity\nLINK SOURCE EVENT;\n\n-- WACTIONSTORE PosWaction is being loaded from MerchantActivity from DSWaction.tql\n\nCREATE WACTIONSTORE PosWaction CONTEXT OF DS.MerchantActivityContext\nEVENT TYPES ( DS.MerchantTxRate )\n@PERSIST-TYPE@\n\n\nCREATE CQ CQPosWaction\nINSERT INTO PosWaction\nselect * from DS.PosStrm\nLINK SOURCE EVENT;\n\n\nEND APPLICATION DsPosApp;\nDEPLOY APPLICATION DsPosApp;\nSTART APPLICATION DsPosApp;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\nCREATE  SOURCE @SOURCENAME@ USING OracleReader  (\n  Username: 'qatest',\n  Password: 'qatest',\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  Tables: 'QATEST.emp',\n  --OnlineCatalog: true,\n  FetchSize: @FETCHSIZE@\n )\nOUTPUT TO DataStream;\n\nCREATE TARGET @TARGETNAME@ USING DatabaseWriter(\nConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',\nUsername:'qatest',\nPassWord:'w3b@ct10n',\nTables: 'qatest.emp,dbo.emp',\nCHECKPOINTTABLE : 'qatest.CHKPOINT',\nConnectionRetryPolicy: '@RETRY_INTERVAL@,@MAX_RETRY@',\nBatchPolicy:'EventCount:@BATCH_EVENT@,Interval:@BATCH_INTERVAL@',\nCommitPolicy:'EventCount:@COMMIT_EVENT@,Interval:@COMMIT_INTERVAL@'\n) INPUT FROM DataStream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nstop application @APPNAME@2;\nundeploy application @APPNAME@2;\ndrop application @APPNAME@2 cascade;\n\ncreate application @APPNAME@ recovery 1 second interval;\n\ncreate source @APPNAME@_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using CobolCopybookParser (\ncopybookFileName : '@TD@/@PROP1@',\n  dataFileFont: '@PROP2@',\n  copybookSplit: '@PROP3@',\n  dataFileOrganization: '@PROP4@',\n  copybookDialect: '@PROP5@', \n  skipIndent:'@PROP6@',\n  DatahandlingScheme:'@PROP7@'\n)\nOUTPUT TO @APPNAME@Stream;\n\ncreate Target @APPNAME@Target using FileWriter(\n    filename :'@FILE@',\n    directory : '@FOLDER@'\n)\nformat using JsonFormatter (\n)\ninput from @APPNAME@Stream;\n\n\n\nCREATE TYPE test_typeJo (\n GG String,\nCORP String,\nDIV String,\nYR FLOAT,\nWK INT,\nSOURCEID String,\nJENUM INT,\nACCTPRIME String,\nACCTSUB String,\nAMTTYPE String,\nFAC String,\nDEPT String,\nSECT String,\nREF String,\nAMT FLOAT,\nUNITAMT INT,\nPOSTINGDATE String,\nEFFECTIVEDATE String,\nSOURCEDESC String,\nSEQUENCENUMBER INT,\nSYSTEMN String\n);\n\nCreate stream cqAsJSONNodeStreamJo of test_typeJo;\n\nCREATE CQ cqAsJSONNodeStreamJo\nINSERT into JSONNodeStreamJo\n    select \n    data.get('GG'),\ndata.get('CORP'),\ndata.get('DIV'),\nTO_FLOAT(data.get('YR')),\nTO_INT(data.get('WK')),\ndata.get('SOURCE-ID'),\nTO_INT(data.get('JE-NUM')),\ndata.get('ACCT-PRIME'),\ndata.get('ACCT-SUB'),\ndata.get('AMT-TYPE'),\ndata.get('FAC'),\ndata.get('DEPT'),\ndata.get('SECT'),\ndata.get('REF'),\nTO_FLOAT(data.get('AMT')),\nTO_INT(data.get('UNIT-AMT')),\ndata.get('POSTING-DATE'),\ndata.get('EFFECTIVE-DATE'),\ndata.get('SOURCE-DESC'),\nTO_INT(data.get('SEQUENCE-NUMBER')),\ndata.get('SYSTEM-N')\nfrom @APPNAME@Stream js;\n\ncreate Target @APPNAME@DBTarget using DatabaseWriter(\n  Username: 'qatest',\n  Password: 'qatest',\n  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',\n  BatchPolicy: 'EventCount:100,Interval:5',\n  CommitPolicy: 'EventCount:100,Interval:5',\n  Tables: 'QATEST.@table@'\n)\ninput from JSONNodeStreamJo;\n\nend application @APPNAME@;\n\n-----------------------------------------------\ncreate application @APPNAME@2 recovery 1 second interval;\n\ncreate source @APPNAME@2_SRC Using FileReader(\n\tdirectory:'@DIRECTORY2@',\n\tWildCard:'@FILENAME2@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using CobolCopybookParser (\ncopybookFileName : '@TD@/@PROP12@',\n  dataFileFont: '@PROP22@',\n  copybookSplit: '@PROP32@',\n  dataFileOrganization: '@PROP42@',\n  copybookDialect: '@PROP52@', \n  skipIndent:'@PROP62@',\n  DatahandlingScheme:'@PROP72@'\n )\nOUTPUT TO @APPNAME@2Stream;\n\ncreate Target @APPNAME@2Target using FileWriter(\n    filename :'@FILE2@',\n    directory : '@FOLDER2@'\n)\nformat using JsonFormatter (\n)\ninput from @APPNAME@2Stream;\n\n\nCREATE TYPE test_typeRe \n(\nnode_new com.fasterxml.jackson.databind.JsonNode,\nnode_name com.fasterxml.jackson.databind.JsonNode,\nnode_addr com.fasterxml.jackson.databind.JsonNode\n);\n\nCreate stream cqAsJSONNodeStreamRe of test_typeRe;\n\nCREATE CQ GetPOAsJsonNodesRe\nINSERT into cqAsJSONNodeStreamRe\nselect \ndata.get('ACCTS-RECORD'),\ndata.get('ACCTS-RECORD').get('NAME'),\ndata.get('ACCTS-RECORD').get('ADDRESS3')\nfrom @APPNAME@2Stream js;\n\ncreate type finaldtypeRe\n(ACCOUNT_NO int,\nFIRST_NAME String,\nLAST_NAME String,\nADDRESS1 String,\nADDRESS2 String,\nCITY String,\nSTATE String,\nZIP_CODE int);\n\nCREATE STREAM getdataStreamPS OF finaldtypeRe;\n\nCREATE CQ getdataRe\nINSERT into getdataStreamPS\nselect JSONGetInteger(x.node_new,\"ACCOUNT-NO\"),\nJSONGetString(x.node_name,\"FIRST-NAME\"),\nJSONGetString(x.node_name,\"LAST-NAME\"),\nJSONGetString(x.node_new,\"ADDRESS1\"),\nJSONGetString(x.node_new,\"ADDRESS2\"),\nJSONGetString(x.node_addr,\"CITY\"),\nJSONGetString(x.node_addr,\"STATE\"),\nJSONGetInteger(x.node_addr,\"ZIP-CODE\")\nfrom cqAsJSONNodeStreamRe x;\n\ncreate Target @APPNAME@2DBTarget using DatabaseWriter(\n  Username: 'qatest',\n  Password: 'qatest',\n  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',\n  BatchPolicy: 'EventCount:1000,Interval:50',\n  CommitPolicy: 'EventCount:1000,Interval:50',\n  Tables: 'QATEST.@table2@'\n)\ninput from getdataStreamPS;\n\nend application @APPNAME@2;\ndeploy application @APPNAME@2 on all in default;\ndeploy application @APPNAME@ on all in default;\n\nstart application @APPNAME@2;\nstart application @APPNAME@;\n\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@;\n\nCREATE SOURCE @SourceName@ USING MSSQLReader  ( \nReaderType: 'LogMiner', \n  Password_encrypted: 'false', \n  DatabaseName: 'qatest',\n  SupportPDB: false, \n  QuiesceMarkerTable: 'QUIESCEMARKER', \n  QueueSize: 2048, \n  CommittedTransactions: true, \n  Username: '@UserName@', \n  TransactionBufferType: 'Memory', \n  TransactionBufferDiskLocation: '.striim/LargeBuffer', \n  OutboundServerProcessName: 'WebActionXStream', \n  Password: '@Password@', \n  DDLCaptureMode: 'All', \n  Compression: false, \n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', \n  FetchSize: 1, \n  Tables: '@SourceTables@', \n  DictionaryMode: 'OnlineCatalog', \n  XstreamTimeOut: 600, \n  TransactionBufferSpilloverSize: '1MB', \n  StartTimestamp: 'null', \n  FilterTransactionBoundaries: true, \n  StartSCN: 'null', \n  ConnectionURL: '@ConnectionURL@', \n  SendBeforeImage: true ) \nOUTPUT TO @AppStream@  ;\n\nCREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream SELECT data[2], to_string(to_date(data[2]), \"dd-MMM-yy hh.mm.ss\") FROM @AppStream@ o ;\n\nCREATE  TARGET @targetsys@ USING Global.SysOut  ( \nname: 'ora1_sys' ) \nINPUT FROM admin.ZDT_cq_stream;\n\ncreate Target @TargetFile@ using FileWriter(\n  filename:'toStringOut.log',\n  directory:'@FilePath@',\n  rolloverpolicy:'eventcount:1000'\n)\nformat using DSVFormatter (\n\n)\ninput from admin.ZDT_cq_stream;\n\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 41 with two sources, two jumping count windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jc5W/p -> CQ1 -> WS\n--   S2 -> Jc6W/p -> CQ2 -> WS\n--\n\nSTOP Recov41Tester.RecovTest41;\nUNDEPLOY APPLICATION Recov41Tester.RecovTest41;\nDROP APPLICATION Recov41Tester.RecovTest41 CASCADE;\nCREATE APPLICATION RecovTest41 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP 5 ROWS\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP 6 ROWS\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nEND APPLICATION RecovTest41;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Canon Test W10\n-- Nicholas Keene, WebAction, Inc.\n--\n-- Basic test for an unpartitioned sliding count window\n--\n-- S -> SWc5u -> CQ -> WS\n--\n\n\nUNDEPLOY APPLICATION NameW10.W10;\nDROP APPLICATION NameW10.W10 CASCADE;\nCREATE APPLICATION W10 RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE FLOW DataAcquisitionW10;\n\nCREATE SOURCE CsvSourceW10 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'Canon1000.csv',\n  columndelimiter:',',\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStreamW10;\n\nEND FLOW DataAcquisitionW10;\n\n\nCREATE FLOW DataProcessingW10;\n\nCREATE TYPE DataTypeW10 (\n    eventID String,\n    word String KEY,\n    dateTime DateTime\n);\n\nCREATE STREAM DataStreamW10 OF DataTypeW10;\n\nCREATE CQ CSVStreamW10_to_DataStreamW10\nINSERT INTO DataStreamW10\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[2],'yyyyMMddHHmmss')\nFROM CsvStreamW10;\n\nCREATE WINDOW SWc5uW10\nOVER DataStreamW10\nKEEP 5 ROWS;\n\nCREATE WACTIONSTORE WactionStoreW10 CONTEXT OF DataTypeW10\nEVENT TYPES ( DataTypeW10 KEY(word) )\n@PERSIST-TYPE@\n\nCREATE CQ SWc5uW10_to_WactionStoreW10\nINSERT INTO WactionStoreW10\nSELECT\n    FIRST(eventID),\n    FIRST(word),\n    FIRST(dateTime)\nFROM SWc5uW10;\n\nEND FLOW DataProcessingW10;\n\n\n\nEND APPLICATION W10;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "-- The PosApp sample application demonstrates how a credit card\n-- payment processor might use Striim to generate reports on current\n-- transaction activity by merchant and send alerts when transaction\n-- counts for a merchant are higher or lower than average for the time\n-- of day.\nSTOP APPLICATION PosAppKafka.PosAppKafka;\nUNDEPLOY APPLICATION PosAppKafka.PosAppKafka;\nDROP APPLICATION PosAppKafka.PosAppKafka CASCADE;\n\nCREATE APPLICATION PosAppKafka;\n\n-- All CREATE statements between here and the END APPLICATION\n-- statement will create objects in the PosApp application.\n\n-- source CsvDataSource\n\nCREATE source CsvDataSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'posdata.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\n\n-- This is the primary data source for this application.\n-- In a real-world application, it would be real-time data. Here,\n-- the data comes from a comma-delimited file, posdata.csv. The first\n-- two lines of that file are:\n--\n-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY\n-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand\n--\n-- The \"header:Yes\" setting tells Striim that the first line contains\n-- field labels that should not be treated as data.\n--\n-- The \"positionByEOF:false\" setting tells Striim to start reading\n-- from the beginning of the file. (In a real-world application\n-- reading real log files, you would typically use the default \"true\"\n-- setting so that the application would read only new data.)\n--\n-- The OUTPUT TO clause automatically creates the stream\n-- CsvStream using the WAEvent type associated with the CSVReader\n-- adapater. The only field from WAEvent used by this application\n-- is \"data\", an array containing the delimited fields.\n\n\n-- CQ CsvToPosData\n\ncreate type posdatatype(\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip string\n);\n\nCREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092');\n\ncreate stream PosDataStream of posdatatype persist using KafkaProps;\n\nCREATE CQ CsvToPosData\nINSERT INTO PosDataStream\nSELECT TO_STRING(data[1]),\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       TO_STRING(data[9])\nFROM CsvStream;\n\n-- Here, \"data\" refers to the array mentioned above, and the number\n-- in brackets specifies a field from the array, counting from zero.\n-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH\n-- AMOUNT, and data[9] is ZIP.\n--\n-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as\n-- the types to be used in PosDataStream, which is created automatically.\n\n-- The DATETIME field from the source is converted to both a dateTime\n-- value, used as the event timestamp by the application, and (via the\n-- function) an integer hourValue, which is used to look up\n-- historical hourly averages from the HourlyAveLookup cache,\n-- discussed below.\n--\n-- The other six fields are discarded. Thus the first line of data\n-- from posdata.csv has at this point been reduced to five values:\n--\n-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)\n-- 20130312173210 (DateTime)\n-- 17 (hourValue)\n-- 2.20 (amount)\n-- 41363 (zip)\n\n\n-- CQ GenerateMerchantTxRateOnly\n--\n-- The PosData5Minutes window bounds the data so that the query can\n-- use the COUNT, FIRST, and SUM functions and join data from the\n-- HourlyAveLookup cache. (Aggregate functions cannot be used and\n-- joins cannot be performed on unbound real-time data.)\n--\n-- The HourlyAveLookup cache provides historical average sales\n-- amounts for the current hour for each merchant.\n\nCREATE JUMPING WINDOW PosData5Minutes\nOVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;\n\nCREATE TYPE MerchantHourlyAve(\n  merchantId String,\n  hourValue integer,\n  hourlyAve integer\n);\nCREATE CACHE HourlyAveLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'hourlyData.txt'\n)\nPARSE USING DSVParser (\n  header: Yes,\n  trimquote:false\n) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;\n\nCREATE TYPE MerchantTxRate(\n  merchantId String KEY,\n  zip String,\n  startingTime DateTime,\n  count integer,\n  totalAmount double,\n  hourlyAve integer,\n  upperLimit double,\n  lowerLimit double,\n  category String,\n  status String\n);\nCREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;\n\nCREATE CQ GenerateMerchantTxRateOnly\nINSERT INTO MerchantTxRateOnlyStream\nSELECT p.merchantId,\n       FIRST(p.zip),\n       FIRST(p.dateTime),\n       COUNT(p.merchantId),\n       SUM(p.amount),\n       FIRST(l.hourlyAve/12),\n       FIRST(l.hourlyAve/12 * CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       FIRST(l.hourlyAve/12 / CASE\n         WHEN l.hourlyAve/12 >10000 THEN 1.15\n         WHEN l.hourlyAve/12 > 800 THEN 1.2\n         WHEN l.hourlyAve/12 >200 THEN 1.25\n         ELSE 1.5 END),\n       '<NOTSET>',\n       '<NOTSET>'\nFROM PosData5Minutes p, HourlyAveLookup l\nWHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue\nGROUP BY p.merchantId;\n\n-- This query aggregates five minutes' worth of data for each\n-- merchant, calculating the total transaction count and amount, and\n-- calculates the upperLimit and lowerLimit values based on the\n-- historical average transaction count for the current hour of the\n-- day from the HourlyAveLookup cache. The category and status fields\n-- are left unset to be populated by the next query.\n\n\n-- CQ GenerateMerchantTxRateWithStatus\n--\n-- This query sets the count values used by the Dashboard map and the\n-- status values used to trigger alerts.\n\nCREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;\n\nCREATE CQ GenerateMerchantTxRateWithStatus\nINSERT INTO MerchantTxRateWithStatusStream\nSELECT merchantId,\n       zip,\n       startingTime,\n       count,\n       totalAmount,\n       hourlyAve,\n       upperLimit,\n       lowerLimit,\n       CASE\n         WHEN count >10000 THEN 'HOT'\n         WHEN count > 800 THEN 'WARM'\n         WHEN count >200 THEN 'COOL'\n         ELSE 'COLD' END,\n       CASE\n         WHEN count > upperLimit THEN 'TOOHIGH'\n         WHEN count < lowerLimit THEN 'TOOLOW'\n         ELSE 'OK' END\nFROM MerchantTxRateOnlyStream;\n\n\n-- WAction store MerchantActivity\n--\n-- The following group of statements create and populate the MerchantActivity\n-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced\n-- with merchant details from NameLookup cache and with latitude and longitude\n-- values from the USAddressData cache.\n\nCREATE TYPE MerchantActivityContext(\n  MerchantId String KEY,\n  StartTime  DateTime,\n  CompanyName String,\n  Category String,\n  Status String,\n  Count integer,\n  HourlyAve integer,\n  UpperLimit double,\n  LowerLimit double,\n  Zip String,\n  City String,\n  State String,\n  LatVal double,\n  LongVal double\n);\n\nCREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext\nEVENT TYPES ( MerchantTxRate )\n@PERSIST-TYPE@\n\nCREATE TYPE MerchantNameData(\n  merchantId String KEY,\n  companyName String\n);\n\nCREATE TYPE USAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\nCREATE CACHE NameLookup using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'MerchantNames.csv',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:'yes',\n  trimquote:false\n)\nQUERY(keytomap:'merchantId') OF MerchantNameData;\n\nCREATE CACHE ZipLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'USAddresses.txt',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header: Yes,\n  columndelimiter: '\\t',\n  trimquote:false\n) QUERY (keytomap:'zip') OF USAddressData;\n\n\nCREATE CQ GenerateWactionContext\nINSERT INTO MerchantActivity\nSELECT  m.merchantId,\n        m.startingTime,\n        n.companyName,\n        m.category,\n        m.status,\n        m.count,\n        m.hourlyAve,\n        m.upperLimit,\n        m.lowerLimit,\n        m.zip,\n        z.city,\n        z.state,\n        z.latVal,\n        z.longVal\nFROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z\nWHERE m.merchantId = n.merchantId AND m.zip = z.zip\nLINK SOURCE EVENT;\n\n-- CQ GenerateAlerts\n--\n-- This CQ sends an alert when a merchant's status value changes to\n-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.\n\n\nCREATE STREAM AlertStream OF Global.AlertEvent;\n\nCREATE CQ GenerateAlerts\nINSERT INTO AlertStream\nSELECT n.CompanyName,\n       m.MerchantId,\n       CASE\n         WHEN m.Status = 'OK' THEN 'info'\n         ELSE 'warning' END,\n       CASE\n         WHEN m.Status = 'OK' THEN 'cancel'\n         ELSE 'raise' END,\n       CASE\n         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)\n         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)\n         ELSE ''\n         END\nFROM MerchantTxRateWithStatusStream m, NameLookup n\nWHERE m.merchantId = n.merchantId;\n\n\n\n--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM AlertStream;\n\nCREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;\n\nEND APPLICATION PosAppKafka;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop ADW;\nundeploy application ADW;\nDROP APPLICATION ADW CASCADE;\nCREATE APPLICATION ADW recovery 5 second interval;;\n\nCreate Source MSSqlsource Using MSSqlReader\n(\n Username:'@SQL-USERNAME',\n Password:'@SQL-PASSWORD',\n DatabaseName:'@DATABASE-NAME@',\n ConnectionURL: '@SQLSERVER-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'1'\n) \nOutput To str;\n\ncreate target MssqlAzureTarget using AzureSQLDWHWriter (\n\t\tConnectionURL: '@SQLDW-URL@',\n        username: '@SQLDW-USERNAME@',\n        password: '@SQLDW-PASSWORD@',\n        AccountName: '@STORAGEACCOUNT@',\n        AccountAccessKey: '@ACCESSKEY@',\n        Tables: '@TARGET-TABLES@',\n        uploadpolicy:'@EVENT-COUNT@'\n) INPUT FROM str;\n\nEND APPLICATION ADW;\ndeploy application ADW;\nstart application ADW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade; \n\ncreate application @APPNAME@;\n\ncreate or replace type @APPNAME@emp_type(\nSno integer,\nEmpname string,\nDoj string,\nCountry string,\nCompanyName string\n);\n\nCREATE OR REPLACE SOURCE @APPNAME@File_SOURCE1 using Filereader(\n\tdirectory:'@DIRECTORY@',\n  wildcard:'File_empdata.csv',\n  positionByEOF:false\n)parse using dsvParser(\n    header:'yes'\n)\nOUTPUT TO @APPNAME@File_Stream1,\nOUTPUT TO @APPNAME@File_Stream1_automap MAP(filename:'File_empdata.csv');\n\nCREATE OR REPLACE SOURCE @APPNAME@Init_Source1 USING DatabaseReader  ( \n  Username: '@SRC-USER@',\n  Password_encrypted: false,\n  ConnectionURL: '@SRC-URL@',\n  Tables: 'QATEST.EMP_INIT',\n  adapterName: 'DatabaseReader',\n  FetchSize: 100,\n  Password: '@SRC-PASS@'\n ) \nOUTPUT TO \t@APPNAME@InitialLoad_Stream1;\n\nCREATE OR REPLACE SOURCE @APPNAME@CDC_Source1 USING OracleReader  ( \n  StartTimestamp: 'null',\n  SupportPDB: false,\n  TransactionBufferDiskLocation: '.striim/LargeBuffer',\n  FetchSize: 1,\n  DDLCaptureMode: 'All',\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@SRC-URL@',\n  Tables: 'QATEST.EMP_CDC',\n  adapterName: 'OracleReader',\n  Password: '@SRC-USER@',\n  TransactionBufferType: 'Memory',\n  DictionaryMode: 'OnlineCatalog',\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  StartSCN: 'null',\n  ReaderType: 'LogMiner',\n  Username: '@SRC-USER@',\n  TransactionBufferSpilloverSize: '1MB',\n  compression: true,\n  OutboundServerProcessName: 'WebActionXStream'\n ) \nOUTPUT TO @APPNAME@CDC_Stream1 ;\n\ncreate or replace stream @APPNAME@FileSource_cdc_init_TypedStream of @APPNAME@emp_type;\ncreate or replace cq @APPNAME@file_typed_streamcq \ninsert into @APPNAME@FileSource_cdc_init_TypedStream \nSELECT to_int(data[0]),\ndata[1],\ndata[2],\ndata[3],\ndata[4]\nfrom @APPNAME@File_Stream1;\n\ncreate or replace cq @APPNAME@cdc_typed_streamcq \ninsert into @APPNAME@FileSource_cdc_init_TypedStream \nSELECT to_int(data[0]),\ndata[1],\ndata[2],\ndata[3],\ndata[4]\nfrom @APPNAME@CDC_Stream1;\n\ncreate or replace cq @APPNAME@init_typed_streamcq \ninsert into @APPNAME@FileSource_cdc_init_TypedStream \nSELECT to_int(data[0]),\ndata[1],\ndata[2],\ndata[3],\ndata[4]\nfrom @APPNAME@InitialLoad_Stream1;\n\nCREATE OR REPLACE TARGET @APPNAME@sap_target1 USING DatabaseWriter  ( \n  DatabaseProviderType:'SAPHANA',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TGT-USER@',\n  Password_encrypted: 'false',\n  BatchPolicy: 'EventCount:100,Interval:0',\n  CommitPolicy: 'EventCount:100,Interval:0',\n  ConnectionURL: '@TGT-URL@',\n  Tables: 'QA.FILE_EMP',\n  adapterName: 'DatabaseWriter',\n  IgnorableExceptionCode: '301',\n  Password: '@TGT-PASS@'\n ) \nINPUT FROM @APPNAME@File_Stream1_automap;\n\nCREATE OR REPLACE TARGET @APPNAME@sap_target2 USING DatabaseWriter  ( \n  DatabaseProviderType:'SAPHANA',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TGT-USER@',\n  Password_encrypted: 'false',\n  BatchPolicy: 'EventCount:100,Interval:0',\n  CommitPolicy: 'EventCount:100,Interval:0',\n  ConnectionURL: '@TGT-URL@',\n  Tables: 'QATEST.EMP,QA.CDC_EMP',\n  adapterName: 'DatabaseWriter',\n  --IgnorableExceptionCode: '301',\n  Password: '@TGT-PASS@'\n ) \nINPUT FROM @APPNAME@InitialLoad_Stream1;\n\nCREATE OR REPLACE TARGET @APPNAME@sap_target3 USING DatabaseWriter  ( \n  DatabaseProviderType:'SAPHANA',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TGT-USER@',\n  Password_encrypted: 'flase',\n  BatchPolicy: 'EventCount:100,Interval:0',\n  CommitPolicy: 'EventCount:100,Interval:0',\n  ConnectionURL: '@TGT-URL@',\n  Tables: 'QATEST.EMP_INIT,QA.INITIALLOAD_EMP',\n  adapterName: 'DatabaseWriter',\n  --IgnorableExceptionCode: '301',\n  Password: '@TGT-PASS@'\n ) \nINPUT FROM @APPNAME@CDC_Stream1;\n\n\nCREATE OR REPLACE TARGET @APPNAME@sap_target4 USING DatabaseWriter  ( \n  DatabaseProviderType:'SAPHANA',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TGT-USER@',\n  Password_encrypted: 'false',\n  BatchPolicy: 'EventCount:100,Interval:0',\n  CommitPolicy: 'EventCount:100,Interval:0',\n  ConnectionURL: '@TGT-URL@',\n  Tables: 'QA.FILE_INIT_CDC_EMP',\n  adapterName: 'DatabaseWriter',\n  IgnorableExceptionCode: '301',\n  Password: '@TGT-PASS@'\n ) \nINPUT FROM @APPNAME@FileSource_cdc_init_TypedStream;\n\ncreate or replace target @APPNAME@sys_file_tgt using sysout(\nname:'foo_file'\n)input from @APPNAME@FileSource_Stream1;\n\ncreate or replace target @APPNAME@sys_cdc_tgt using sysout(\nname:'foo_cdc'\n)input from @APPNAME@CDC_Stream1;\n\ncreate or replace target @APPNAME@sys_init_tgt using sysout(\nname:'foo_init'\n)input from @APPNAME@InitialLoad_Stream1;\n\nEnd Application @APPNAME@;\n\n\ndeploy application @APPNAME@;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop Oracle_IRLogWriter;\nundeploy application Oracle_IRLogWriter;\ndrop application Oracle_IRLogWriter cascade;\n\nCREATE APPLICATION Oracle_IRLogWriter;\n\nCREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( \n  FetchSize: 100,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.autotest01',\n  adapterName: 'IncrementalBatchReader',\n CheckColumn: 'striim.autotest01=id',\n startPosition: 'striim.autotest01=2',\n  PollingInterval: '5sec'\n  )\n  OUTPUT TO data_stream;\n\n  create type AutoType(\n  ID string,\n  name string,\n  company string,\n  country string\n);\n\nCREATE STREAM CDCdata_stream OF AutoType;\n\nCREATE CQ Lookup\nINSERT INTO CDCdata_stream\nselect data[0],data[1],data[2],data[3] from data_stream;\n\nCREATE  TARGET AzureSQLDWHTarget USING AzureSQLDWHWriter  ( \n  ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n  username: 'striim',\n  password: 'W3b@ct10n',\n   accountname: 'striimqatestdonotdelete',\n   Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n   Tables: 'STRIIM.AUTOTEST01',\n  uploadpolicy: 'eventcount:1,interval:10s'\n ) \nINPUT FROM CDCdata_stream;\n\nCREATE OR REPLACE TARGET sys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM CDCdata_stream;\n\n\nEND APPLICATION Oracle_IRLogWriter;\ndeploy application Oracle_IRLogWriter;\nstart Oracle_IRLogWriter;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application CDCTester.CDCTest;\nundeploy application CDCTester.CDCTest;\ndrop application CDCTester.CDCTest cascade;\n\ncreate application CDCTest;\n\nCreate Source Rac11g Using OracleReader\n(\n Username:'@LOGMINER-UNAME@',\n Password:'@LOGMINER-PASSWORD@',\n ConnectionURL:'@LOGMINER-URL@',\n Tables:'QATEST.SAMPLETEST2',\n FetchSize:1,\n QueueSize:2048\n)\nOutput To LCRStream;\n\n\nend application CDCTest;\ndeploy application CDCTest;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": " Stop Oracle_Agent;\nUndeploy application Oracle_Agent;\ndrop application Oracle_Agent cascade;\n\nCREATE APPLICATION Oracle_Agent;\n\nCREATE FLOW test_SourceFlow;\n\nCREATE  SOURCE Oracle_Source USING OracleReader  ( \n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//192.168.124.25:1521/orcl',\n  Tables: 'QATEST.%',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  StartSCN: 'null',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  OutboundServerProcessName: 'WebActionXStream'\n ) \nOUTPUT TO Oracle_ChangeDataStream ;\n\nEND FLOW test_SourceFlow;\n\nCREATE OR REPLACE TARGET TeraSys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM Oracle_ChangeDataStream;\n\nEND APPLICATION Oracle_Agent;\nDEPLOY APPLICATION Oracle_Agent ON ANY IN default WITH test_SourceFlow ON ANY IN AGENTS;\nstart Oracle_Agent;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop application ConsoleApplication cascade;\ncreate application ConsoleApplication;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet\n(\n  ConnectionURL: '@CONNECTION_URL@',\n  CommittedTransactions: true,\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OJet',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  Username: '@SOURCE_USER@'\n)OUTPUT TO @STREAM@ MAP (table: 'QATEST.SMFTEST6')\nSELECT NUM_COL,CHAR_COL,VARCHAR2_COL,LONG_COL,DATE_COL,TIMESTAMP_COL where TO_INT(NUM_COL) > 1;\n\nCREATE TYPE LogType(\nNum_col String key,\nChar_col String,\nVarchar2_col String,\nlong_col String,\nTable String,\nOperation String\n);\n\nCREATE WINDOW CDCWindow\nOVER @STREAM@\nKEEP 1 ROWS;\n\nCREATE WACTIONSTORE CDCWS CONTEXT of LogType\nEVENT TYPES ( LogType )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ ToWactionStore\nINSERT INTO CDCWS\nSELECT * FROM CDCWindow\nLINK SOURCE EVENT;\n\nCREATE TARGET @SOURCE_NAME@_SYS USING SysOut (\n  name: '@SOURCE_NAME@_SYS' )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application FileWriterTest;\nundeploy application FileWriterTest;\ndrop application FileWriterTest cascade;\ncreate application FileWriterTest recovery 1 second interval;\ncreate source File_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'false'\n)\nOUTPUT TO CsvStream;\n\ncreate Target FileTarget using FileWriter(\n    rolloverpolicy:'@UPLOAD-SIZE@',\n    filename :'@FILE@',\n    directory : '@FOLDER@'\n)\nformat using DSVFormatter (\ncharset:'@charset@',\nmembers:'@mem@',\nheader:'@head@'\n\n)\ninput from CsvStream;\n\nend application FileWriterTest;\ndeploy application FileWriterTest on all in default;\nstart application FileWriterTest;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW ;\n\n \n\nCREATE OR REPLACE SOURCE DBSource USING DatabaseReader  ( \n  Username: '@SOURCE_USER@',\n  Password_encrypted: false,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  Query: \"SELECT * FROM waction.MysqlToCql_alldatatypes\",\n  adapterName: 'DatabaseReader',\n  FetchSize: 100,\n  Password: '@SOURCE_PASS@'\n ) \nOUTPUT TO Oracle_ChangeDataStream;\n\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:100,Interval:0',\n  CommitPolicy: 'EventCount:100,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n ) \nINPUT FROM Oracle_ChangeDataStream;\n\ncreate Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\nEND APPLICATION DBRTOCW;\ndeploy application DBRTOCW in default;\nstart DBRTOCW;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE TYPE PosData(\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\nCREATE STREAM PosDataStream OF PosData PARTITION BY merchantId;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatc (\n  StartPosition: '@startPosition@',\n  ConnectionURL: '@sourceURL@',\n  Username: '@userName@',\n  Tables: '@tables@',\n  CheckColumn: '@checkColum@',\n  FetchSize: 1,\n  ReturnDateTimeAs: 'JODA',\n  Password: '@password@',\n  Password_encrypted: 'false',\n  DatabaseProviderType: 'Default',\n  ThreadPoolSize: 5,\n  pollingInterval: '2sec',\n  ConnectionPoolSize: 1 )\nOUTPUT TO @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ recovery 5 second interval;\nCREATE OR REPLACE SOURCE @SourceName@ Using OJet\n(\n  Username: '@Username@',\n  Password: '@Password@',\n  ConnectionURL: '@ConnectionURL@',\n  connectionRetryPolicy: @ConnectionRetryPolicy@,\n  Tables: '@SourceTables@',\n  ConnectionPoolSize: 1,\n  StartPosition: 'EOF',\n  Password_encrypted: 'false',\n  CommittedTransactions: true,\n  adapterName: 'OJet',\n)OUTPUT TO @SRCINPUTSTREAM@;\ncreate Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop IR;\nundeploy application IR;\ndrop application IR cascade;\nCREATE APPLICATION IR recovery 5 second interval;\n\n CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( \n  FetchSize: 100,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.test01;striim.test02;striim.test03',\n  adapterName: 'IncrementalBatchReader',\n CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',\n startPosition: 'striim.test01=1;striim.test02=-1;%=0',\n  PollingInterval: '5sec'\n  )\n  OUTPUT TO data_stream ;\n\n  CREATE OR REPLACE TARGET sys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM data_stream;\n\ncreate target AzureSQLDWHTarget1 using AzureSQLDWHWriter(\n\t\tConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'STRIIM.AUTO_LENGTHY_TABLE_NAME_FROM_INCREMENTAL_READER_TO_AZURE_STRUCTURED_QUERY_LANGUAGE_DATABASE_WAREHOUSE_WITH_128_CHARACTER_MAXIMUM,DBO.AUTOTEST_LENGTHY1 COLUMNMAP(Field1=AUTO_LENGTHY_COLUMNNAME_FROM_INCREMENTAL_READER_TO_AZURE_STRUCTURED_QUERY_LANGUAGE_DATABASE_WAREHOUSE_WITH_128_CHARACTER_MAXIMUM)',\n        uploadpolicy:'eventcount:1,interval:10s'\n) INPUT FROM data_stream;\n\ncreate target AzureSQLDWHTarget2 using AzureSQLDWHWriter(\n\t\tConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'STRIIM.AUTO_LENGTHY_TABLE_NAME_FROM_INCREMENTAL_READER_TO_AZURE_STRUCTURED_QUERY_LANGUAGE_DATABASE_WAREHOUSE_WITH_128_CHARACTER_MAXIMUM,DBO.AUTOTEST_LENGTHY2 COLUMNMAP(Field1=AUTO_LENGTHY_COLUMNNAME_FROM_INCREMENTAL_READER_TO_AZURE_STRUCTURED_QUERY_LANGUAGE_DATABASE_WAREHOUSE_WITH_128_CHARACTER_MAXIMUM,Field2=Field1026,Field3=Field1027,Field4=Field1028,Field5=Field1029,Field6=Field1030,Field7=Field1031,Field8=Field1032,Field9=Field1033,Field10=Field1034,Field11=Field1035,Field12=Field1036,Field13=Field1037,Field14=Field1038,Field15=Field1039,Field16=Field1040,Field17=Field1041,Field18=Field1042,Field19=Field1043,Field20=Field1044,Field21=Field1045,Field22=Field1046,Field23=Field1047,Field24=Field1048,Field25=Field1049,Field26=Field1050,Field27=Field1051,Field28=Field1052,Field29=Field1053,Field30=Field1054,Field31=Field1055,Field32=Field1056,Field33=Field1057,Field34=Field1058,Field35=Field1059,Field36=Field1060,Field37=Field1061,Field38=Field1062,Field39=Field1063,Field40=Field1064,Field41=Field1065,Field42=Field1066,Field43=Field1067,Field44=Field1068,Field45=Field1069,Field46=Field1070,Field47=Field1071,Field48=Field1072,Field49=Field1073,Field50=Field1074,Field51=Field1075,Field52=Field1076,Field53=Field1077,Field54=Field1078,Field55=Field1079,Field56=Field1080,Field57=Field1081,Field58=Field1082,Field59=Field1083,Field60=Field1084,Field61=Field1085,Field62=Field1086,Field63=Field1087,Field64=Field1088,Field65=Field1089,Field66=Field1090,Field67=Field1091,Field68=Field1092,Field69=Field1093,Field70=Field1094,Field71=Field1095,Field72=Field1096,Field73=Field1097,Field74=Field1098,Field75=Field1099,Field76=Field1100,Field77=Field1101,Field78=Field1102,Field79=Field1103,Field80=Field1104,Field81=Field1105,Field82=Field1106,Field83=Field1107,Field84=Field1108,Field85=Field1109,Field86=Field1110,Field87=Field1111,Field88=Field1112,Field89=Field1113,Field90=Field1114,Field91=Field1115,Field92=Field1116,Field93=Field1117,Field94=Field1118,Field95=Field1119,Field96=Field1120,Field97=Field1121,Field98=Field1122,Field99=Field1123,Field100=Field1124,Field101=Field1125,Field102=Field1126,Field103=Field1127,Field104=Field1128,Field105=Field1129,Field106=Field1130,Field107=Field1131,Field108=Field1132,Field109=Field1133,Field110=Field1134,Field111=Field1135,Field112=Field1136,Field113=Field1137,Field114=Field1138,Field115=Field1139,Field116=Field1140,Field117=Field1141,Field118=Field1142,Field119=Field1143,Field120=Field1144,Field121=Field1145,Field122=Field1146,Field123=Field1147,Field124=Field1148,Field125=Field1149,Field126=Field1150,Field127=Field1151,Field128=Field1152,Field129=Field1153,Field130=Field1154,Field131=Field1155,Field132=Field1156,Field133=Field1157,Field134=Field1158,Field135=Field1159,Field136=Field1160,Field137=Field1161,Field138=Field1162,Field139=Field1163,Field140=Field1164,Field141=Field1165,Field142=Field1166,Field143=Field1167,Field144=Field1168,Field145=Field1169,Field146=Field1170,Field147=Field1171,Field148=Field1172,Field149=Field1173,Field150=Field1174,Field151=Field1175,Field152=Field1176,Field153=Field1177,Field154=Field1178,Field155=Field1179,Field156=Field1180,Field157=Field1181,Field158=Field1182,Field159=Field1183,Field160=Field1184,Field161=Field1185,Field162=Field1186,Field163=Field1187,Field164=Field1188,Field165=Field1189,Field166=Field1190,Field167=Field1191,Field168=Field1192,Field169=Field1193,Field170=Field1194,Field171=Field1195,Field172=Field1196,Field173=Field1197,Field174=Field1198,Field175=Field1199,Field176=Field1200,Field177=Field1201,Field178=Field1202,Field179=Field1203,Field180=Field1204,Field181=Field1205,Field182=Field1206,Field183=Field1207,Field184=Field1208,Field185=Field1209,Field186=Field1210,Field187=Field1211,Field188=Field1212,Field189=Field1213,Field190=Field1214,Field191=Field1215,Field192=Field1216,Field193=Field1217,Field194=Field1218,Field195=Field1219,Field196=Field1220,Field197=Field1221,Field198=Field1222,Field199=Field1223,Field200=Field1224,Field201=Field1225,Field202=Field1226,Field203=Field1227,Field204=Field1228,Field205=Field1229,Field206=Field1230,Field207=Field1231,Field208=Field1232,Field209=Field1233,Field210=Field1234,Field211=Field1235,Field212=Field1236,Field213=Field1237,Field214=Field1238,Field215=Field1239,Field216=Field1240,Field217=Field1241,Field218=Field1242,Field219=Field1243,Field220=Field1244,Field221=Field1245,Field222=Field1246,Field223=Field1247,Field224=Field1248,Field225=Field1249,Field226=Field1250,Field227=Field1251,Field228=Field1252,Field229=Field1253,Field230=Field1254,Field231=Field1255,Field232=Field1256,Field233=Field1257,Field234=Field1258,Field235=Field1259,Field236=Field1260,Field237=Field1261,Field238=Field1262,Field239=Field1263,Field240=Field1264,Field241=Field1265,Field242=Field1266,Field243=Field1267,Field244=Field1268,Field245=Field1269,Field246=Field1270,Field247=Field1271,Field248=Field1272,Field249=Field1273,Field250=Field1274,Field251=Field1275,Field252=Field1276,Field253=Field1277,Field254=Field1278,Field255=Field1279,Field256=Field1280,Field257=Field1281,Field258=Field1282,Field259=Field1283,Field260=Field1284,Field261=Field1285,Field262=Field1286,Field263=Field1287,Field264=Field1288,Field265=Field1289,Field266=Field1290,Field267=Field1291,Field268=Field1292,Field269=Field1293,Field270=Field1294,Field271=Field1295,Field272=Field1296,Field273=Field1297,Field274=Field1298,Field275=Field1299,Field276=Field1300,Field277=Field1301,Field278=Field1302,Field279=Field1303,Field280=Field1304,Field281=Field1305,Field282=Field1306,Field283=Field1307,Field284=Field1308,Field285=Field1309,Field286=Field1310,Field287=Field1311,Field288=Field1312,Field289=Field1313,Field290=Field1314,Field291=Field1315,Field292=Field1316,Field293=Field1317,Field294=Field1318,Field295=Field1319,Field296=Field1320,Field297=Field1321,Field298=Field1322,Field299=Field1323,Field300=Field1324,Field301=Field1325,Field302=Field1326,Field303=Field1327,Field304=Field1328,Field305=Field1329,Field306=Field1330,Field307=Field1331,Field308=Field1332,Field309=Field1333,Field310=Field1334,Field311=Field1335,Field312=Field1336,Field313=Field1337,Field314=Field1338,Field315=Field1339,Field316=Field1340,Field317=Field1341,Field318=Field1342,Field319=Field1343,Field320=Field1344,Field321=Field1345,Field322=Field1346,Field323=Field1347,Field324=Field1348,Field325=Field1349,Field326=Field1350,Field327=Field1351,Field328=Field1352,Field329=Field1353,Field330=Field1354,Field331=Field1355,Field332=Field1356,Field333=Field1357,Field334=Field1358,Field335=Field1359,Field336=Field1360,Field337=Field1361,Field338=Field1362,Field339=Field1363,Field340=Field1364,Field341=Field1365,Field342=Field1366,Field343=Field1367,Field344=Field1368,Field345=Field1369,Field346=Field1370,Field347=Field1371,Field348=Field1372,Field349=Field1373,Field350=Field1374,Field351=Field1375,Field352=Field1376,Field353=Field1377,Field354=Field1378,Field355=Field1379,Field356=Field1380,Field357=Field1381,Field358=Field1382,Field359=Field1383,Field360=Field1384,Field361=Field1385,Field362=Field1386,Field363=Field1387,Field364=Field1388,Field365=Field1389,Field366=Field1390,Field367=Field1391,Field368=Field1392,Field369=Field1393,Field370=Field1394,Field371=Field1395,Field372=Field1396,Field373=Field1397,Field374=Field1398,Field375=Field1399,Field376=Field1400,Field377=Field1401,Field378=Field1402,Field379=Field1403,Field380=Field1404,Field381=Field1405,Field382=Field1406,Field383=Field1407,Field384=Field1408,Field385=Field1409,Field386=Field1410,Field387=Field1411,Field388=Field1412,Field389=Field1413,Field390=Field1414,Field391=Field1415,Field392=Field1416,Field393=Field1417,Field394=Field1418,Field395=Field1419,Field396=Field1420,Field397=Field1421,Field398=Field1422,Field399=Field1423,Field400=Field1424,Field401=Field1425,Field402=Field1426,Field403=Field1427,Field404=Field1428,Field405=Field1429,Field406=Field1430,Field407=Field1431,Field408=Field1432,Field409=Field1433,Field410=Field1434,Field411=Field1435,Field412=Field1436,Field413=Field1437,Field414=Field1438,Field415=Field1439,Field416=Field1440,Field417=Field1441,Field418=Field1442,Field419=Field1443,Field420=Field1444,Field421=Field1445,Field422=Field1446,Field423=Field1447,Field424=Field1448,Field425=Field1449,Field426=Field1450,Field427=Field1451,Field428=Field1452,Field429=Field1453,Field430=Field1454,Field431=Field1455,Field432=Field1456,Field433=Field1457,Field434=Field1458,Field435=Field1459,Field436=Field1460,Field437=Field1461,Field438=Field1462,Field439=Field1463,Field440=Field1464,Field441=Field1465,Field442=Field1466,Field443=Field1467,Field444=Field1468,Field445=Field1469,Field446=Field1470,Field447=Field1471,Field448=Field1472,Field449=Field1473,Field450=Field1474,Field451=Field1475,Field452=Field1476,Field453=Field1477,Field454=Field1478,Field455=Field1479,Field456=Field1480,Field457=Field1481,Field458=Field1482,Field459=Field1483,Field460=Field1484,Field461=Field1485,Field462=Field1486,Field463=Field1487,Field464=Field1488,Field465=Field1489,Field466=Field1490,Field467=Field1491,Field468=Field1492,Field469=Field1493,Field470=Field1494,Field471=Field1495,Field472=Field1496,Field473=Field1497,Field474=Field1498,Field475=Field1499,Field476=Field1500,Field477=Field1501,Field478=Field1502,Field479=Field1503,Field480=Field1504,Field481=Field1505,Field482=Field1506,Field483=Field1507,Field484=Field1508,Field485=Field1509,Field486=Field1510,Field487=Field1511,Field488=Field1512,Field489=Field1513,Field490=Field1514,Field491=Field1515,Field492=Field1516,Field493=Field1517,Field494=Field1518,Field495=Field1519,Field496=Field1520,Field497=Field1521,Field498=Field1522,Field499=Field1523,Field500=Field1524,Field501=Field1525,Field502=Field1526,Field503=Field1527,Field504=Field1528,Field505=Field1529,Field506=Field1530,Field507=Field1531,Field508=Field1532,Field509=Field1533,Field510=Field1534,Field511=Field1535,Field512=Field1536,Field513=Field1537,Field514=Field1538,Field515=Field1539,Field516=Field1540,Field517=Field1541,Field518=Field1542,Field519=Field1543,Field520=Field1544,Field521=Field1545,Field522=Field1546,Field523=Field1547,Field524=Field1548,Field525=Field1549,Field526=Field1550,Field527=Field1551,Field528=Field1552,Field529=Field1553,Field530=Field1554,Field531=Field1555,Field532=Field1556,Field533=Field1557,Field534=Field1558,Field535=Field1559,Field536=Field1560,Field537=Field1561,Field538=Field1562,Field539=Field1563,Field540=Field1564,Field541=Field1565,Field542=Field1566,Field543=Field1567,Field544=Field1568,Field545=Field1569,Field546=Field1570,Field547=Field1571,Field548=Field1572,Field549=Field1573,Field550=Field1574,Field551=Field1575,Field552=Field1576,Field553=Field1577,Field554=Field1578,Field555=Field1579,Field556=Field1580,Field557=Field1581,Field558=Field1582,Field559=Field1583,Field560=Field1584,Field561=Field1585,Field562=Field1586,Field563=Field1587,Field564=Field1588,Field565=Field1589,Field566=Field1590,Field567=Field1591,Field568=Field1592,Field569=Field1593,Field570=Field1594,Field571=Field1595,Field572=Field1596,Field573=Field1597,Field574=Field1598,Field575=Field1599,Field576=Field1600,Field577=Field1601,Field578=Field1602,Field579=Field1603,Field580=Field1604,Field581=Field1605,Field582=Field1606,Field583=Field1607,Field584=Field1608,Field585=Field1609,Field586=Field1610,Field587=Field1611,Field588=Field1612,Field589=Field1613,Field590=Field1614,Field591=Field1615,Field592=Field1616,Field593=Field1617,Field594=Field1618,Field595=Field1619,Field596=Field1620,Field597=Field1621,Field598=Field1622,Field599=Field1623,Field600=Field1624,Field601=Field1625,Field602=Field1626,Field603=Field1627,Field604=Field1628,Field605=Field1629,Field606=Field1630,Field607=Field1631,Field608=Field1632,Field609=Field1633,Field610=Field1634,Field611=Field1635,Field612=Field1636,Field613=Field1637,Field614=Field1638,Field615=Field1639,Field616=Field1640,Field617=Field1641,Field618=Field1642,Field619=Field1643,Field620=Field1644,Field621=Field1645,Field622=Field1646,Field623=Field1647,Field624=Field1648,Field625=Field1649,Field626=Field1650,Field627=Field1651,Field628=Field1652,Field629=Field1653,Field630=Field1654,Field631=Field1655,Field632=Field1656,Field633=Field1657,Field634=Field1658,Field635=Field1659,Field636=Field1660,Field637=Field1661,Field638=Field1662,Field639=Field1663,Field640=Field1664,Field641=Field1665,Field642=Field1666,Field643=Field1667,Field644=Field1668,Field645=Field1669,Field646=Field1670,Field647=Field1671,Field648=Field1672,Field649=Field1673,Field650=Field1674,Field651=Field1675,Field652=Field1676,Field653=Field1677,Field654=Field1678,Field655=Field1679,Field656=Field1680,Field657=Field1681,Field658=Field1682,Field659=Field1683,Field660=Field1684,Field661=Field1685,Field662=Field1686,Field663=Field1687,Field664=Field1688,Field665=Field1689,Field666=Field1690,Field667=Field1691,Field668=Field1692,Field669=Field1693,Field670=Field1694,Field671=Field1695,Field672=Field1696,Field673=Field1697,Field674=Field1698,Field675=Field1699,Field676=Field1700,Field677=Field1701,Field678=Field1702,Field679=Field1703,Field680=Field1704,Field681=Field1705,Field682=Field1706,Field683=Field1707,Field684=Field1708,Field685=Field1709,Field686=Field1710,Field687=Field1711,Field688=Field1712,Field689=Field1713,Field690=Field1714,Field691=Field1715,Field692=Field1716,Field693=Field1717,Field694=Field1718,Field695=Field1719,Field696=Field1720,Field697=Field1721,Field698=Field1722,Field699=Field1723,Field700=Field1724,Field701=Field1725,Field702=Field1726,Field703=Field1727,Field704=Field1728,Field705=Field1729,Field706=Field1730,Field707=Field1731,Field708=Field1732,Field709=Field1733,Field710=Field1734,Field711=Field1735,Field712=Field1736,Field713=Field1737,Field714=Field1738,Field715=Field1739,Field716=Field1740,Field717=Field1741,Field718=Field1742,Field719=Field1743,Field720=Field1744,Field721=Field1745,Field722=Field1746,Field723=Field1747,Field724=Field1748,Field725=Field1749,Field726=Field1750,Field727=Field1751,Field728=Field1752,Field729=Field1753,Field730=Field1754,Field731=Field1755,Field732=Field1756,Field733=Field1757,Field734=Field1758,Field735=Field1759,Field736=Field1760,Field737=Field1761,Field738=Field1762,Field739=Field1763,Field740=Field1764,Field741=Field1765,Field742=Field1766,Field743=Field1767,Field744=Field1768,Field745=Field1769,Field746=Field1770,Field747=Field1771,Field748=Field1772,Field749=Field1773,Field750=Field1774,Field751=Field1775,Field752=Field1776,Field753=Field1777,Field754=Field1778,Field755=Field1779,Field756=Field1780,Field757=Field1781,Field758=Field1782,Field759=Field1783,Field760=Field1784,Field761=Field1785,Field762=Field1786,Field763=Field1787,Field764=Field1788,Field765=Field1789,Field766=Field1790,Field767=Field1791,Field768=Field1792,Field769=Field1793,Field770=Field1794,Field771=Field1795,Field772=Field1796,Field773=Field1797,Field774=Field1798,Field775=Field1799,Field776=Field1800,Field777=Field1801,Field778=Field1802,Field779=Field1803,Field780=Field1804,Field781=Field1805,Field782=Field1806,Field783=Field1807,Field784=Field1808,Field785=Field1809,Field786=Field1810,Field787=Field1811,Field788=Field1812,Field789=Field1813,Field790=Field1814,Field791=Field1815,Field792=Field1816,Field793=Field1817,Field794=Field1818,Field795=Field1819,Field796=Field1820,Field797=Field1821,Field798=Field1822,Field799=Field1823,Field800=Field1824,Field801=Field1825,Field802=Field1826,Field803=Field1827,Field804=Field1828,Field805=Field1829,Field806=Field1830,Field807=Field1831,Field808=Field1832,Field809=Field1833,Field810=Field1834,Field811=Field1835,Field812=Field1836,Field813=Field1837,Field814=Field1838,Field815=Field1839,Field816=Field1840,Field817=Field1841,Field818=Field1842,Field819=Field1843,Field820=Field1844,Field821=Field1845,Field822=Field1846,Field823=Field1847,Field824=Field1848,Field825=Field1849,Field826=Field1850,Field827=Field1851,Field828=Field1852,Field829=Field1853,Field830=Field1854,Field831=Field1855,Field832=Field1856,Field833=Field1857,Field834=Field1858,Field835=Field1859,Field836=Field1860,Field837=Field1861,Field838=Field1862,Field839=Field1863,Field840=Field1864,Field841=Field1865,Field842=Field1866,Field843=Field1867,Field844=Field1868,Field845=Field1869,Field846=Field1870,Field847=Field1871,Field848=Field1872,Field849=Field1873,Field850=Field1874,Field851=Field1875,Field852=Field1876,Field853=Field1877,Field854=Field1878,Field855=Field1879,Field856=Field1880,Field857=Field1881,Field858=Field1882,Field859=Field1883,Field860=Field1884,Field861=Field1885,Field862=Field1886,Field863=Field1887,Field864=Field1888,Field865=Field1889,Field866=Field1890,Field867=Field1891,Field868=Field1892,Field869=Field1893,Field870=Field1894,Field871=Field1895,Field872=Field1896,Field873=Field1897,Field874=Field1898,Field875=Field1899,Field876=Field1900,Field877=Field1901,Field878=Field1902,Field879=Field1903,Field880=Field1904,Field881=Field1905,Field882=Field1906,Field883=Field1907,Field884=Field1908,Field885=Field1909,Field886=Field1910,Field887=Field1911,Field888=Field1912,Field889=Field1913,Field890=Field1914,Field891=Field1915,Field892=Field1916,Field893=Field1917,Field894=Field1918,Field895=Field1919,Field896=Field1920,Field897=Field1921,Field898=Field1922,Field899=Field1923,Field900=Field1924,Field901=Field1925,Field902=Field1926,Field903=Field1927,Field904=Field1928,Field905=Field1929,Field906=Field1930,Field907=Field1931,Field908=Field1932,Field909=Field1933,Field910=Field1934,Field911=Field1935,Field912=Field1936,Field913=Field1937,Field914=Field1938,Field915=Field1939,Field916=Field1940,Field917=Field1941,Field918=Field1942,Field919=Field1943,Field920=Field1944,Field921=Field1945,Field922=Field1946,Field923=Field1947,Field924=Field1948,Field925=Field1949,Field926=Field1950,Field927=Field1951,Field928=Field1952,Field929=Field1953,Field930=Field1954,Field931=Field1955,Field932=Field1956,Field933=Field1957,Field934=Field1958,Field935=Field1959,Field936=Field1960,Field937=Field1961,Field938=Field1962,Field939=Field1963,Field940=Field1964,Field941=Field1965,Field942=Field1966,Field943=Field1967,Field944=Field1968,Field945=Field1969,Field946=Field1970,Field947=Field1971,Field948=Field1972,Field949=Field1973,Field950=Field1974,Field951=Field1975,Field952=Field1976,Field953=Field1977,Field954=Field1978,Field955=Field1979,Field956=Field1980,Field957=Field1981,Field958=Field1982,Field959=Field1983,Field960=Field1984,Field961=Field1985,Field962=Field1986,Field963=Field1987,Field964=Field1988,Field965=Field1989,Field966=Field1990,Field967=Field1991,Field968=Field1992,Field969=Field1993,Field970=Field1994,Field971=Field1995,Field972=Field1996,Field973=Field1997,Field974=Field1998,Field975=Field1999,Field976=Field2000,Field977=Field2001,Field978=Field2002,Field979=Field2003,Field980=Field2004,Field981=Field2005,Field982=Field2006,Field983=Field2007,Field984=Field2008,Field985=Field2009,Field986=Field2010,Field987=Field2011,Field988=Field2012,Field989=Field2013,Field990=Field2014,Field991=Field2015,Field992=Field2016,Field993=Field2017,Field994=Field2018,Field995=Field2019,Field996=Field2020,Field997=Field2021,Field998=Field2022,Field999=Field2023,Field1000=Field2024,Field1001=Field2025,Field1002=Field2026,Field1003=Field2027,Field1004=Field2028,Field1005=Field2029,Field1006=Field2030,Field1007=Field2031,Field1008=Field2032,Field1009=Field2033,Field1010=Field2034,Field1011=Field2035,Field1012=Field2036,Field1013=Field2037,Field1014=Field2038,Field1015=Field2039,Field1016=Field2040,Field1017=Field2041,Field1018=Field2042,Field1019=Field2043,Field1020=Field2044,Field1021=Field2045,Field1022=Field2046)',\n        uploadpolicy:'eventcount:1,interval:10s'\n) INPUT FROM data_stream;\n\n\nCREATE TARGET log USING LOGWRITER\n(\n name:log,\n filename:'LOGFILENAME'\n)\nINPUT FROM data_stream;\n\n\nEND APPLICATION IR;\ndeploy application IR;\nstart application IR;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop MySQLToHBase;\nundeploy application MySQLToHBase;\ndrop application MySQLToHBase cascade;\n\nCREATE APPLICATION MySQLToHBase RECOVERY 5 SECOND INTERVAL;;\n\nCREATE SOURCE MySQLCDCIn USING MySQLReader (\nUsername:'root',\nPassword:'root',\nConnectionURL:'mysql://192.168.123.14:3306',\nDatabase:'qatest',\nTables:'qatest.BUSINESS'\n)\nOUTPUT TO DataStream;\n\nCREATE TARGET MySQLCDCOut\nUSING SysOut(name:MySQLCDC)\nINPUT FROM DataStream;\n\nCREATE OR REPLACE TARGET Target2 using HBaseWriter(\n  HBaseConfigurationPath:\"/Users/ravipathak/soft/hbase-1.1.5/conf/hbase-site.xml\",\n  Tables: \"qatest.BUSINESS,PKTEST.data\",\n  PKUpdateHandlingMode: \"DELETEANDINSERT\",\n  BatchPolicy: \"eventCount:1\")\nINPUT FROM DataStream;\n\nEND APPLICATION MySQLToHBase;\n\ndeploy application MySQLToHBase;\nstart MySQLToHBase;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION  @AppName@ RECOVERY 5 SECOND INTERVAL;\nCREATE OR REPLACE SOURCE  @AppName@_Source USING FileReader (\ndirectory:'@dataDir@',\nwildcard:'JsonNodeEvent.json',\npositionByEOF:false\n)\nPARSE USING Global.JSONParser (\n )  OUTPUT TO  @AppName@_rawstream;\n\nCREATE CQ @BuiltinFunc@CQ\nINSERT INTO  @BuiltinFunc@_Stream\nSELECT @BuiltinFunc@(x, 'Sno', data.get(\"_id\"), 'Name', data.get(\"firstname\"))\nFROM @AppName@_rawstream x;\n\nCREATE OR REPLACE CQ cq1\nINSERT INTO RemoveUserData_Stream\nSELECT\nremoveUserData(s1, 'Sno')\nFROM @BuiltinFunc@_Stream s1;\n\nCREATE OR REPLACE TARGET  @AppName@_FileTarget USING Global.FileWriter (\n  flushpolicy: 'EventCount:10000,Interval:30s',\n  adapterName: 'FileWriter',\n directory: '@logs@',\n  filename: '@BuiltinFunc@_JsonEventRemoveData',\n  rolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING Global.JSONFormatter  (\n  handler: 'com.webaction.proc.JSONFormatter',\n  jsonMemberDelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true',\n  formatterName: 'JSONFormatter',\n  jsonobjectdelimiter: '\\n' )\nINPUT FROM RemoveUserData_Stream;\n\nEnd application  @AppName@;\nDeploy application  @AppName@;\nStart application  @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION LongRunningQueryTester.LongRunningApp;\nUNDEPLOY APPLICATION LongRunningQueryTester.LongRunningApp;\nDROP APPLICATION LongRunningQueryTester.LongRunningApp cascade;\n\nCREATE APPLICATION LongRunningApp;\n\n\n --COMMENT::   Modify type attributes as desired.\n --COMMENT::   Type must be created first before creating a source using ranReader\n\nCREATE TYPE RandomData(\n  myName String,\n  streetAddress String,\n  bankName String,\n  bankNumber int KEY,\n  bankAmount double\n);\n\nCREATE source ranDataSource USING ranReader(\n  OutputType:'LongRunningQueryTester.RandomData',\n  TimeInterval:5,\n  NoLimit:true,\n  SampleSize:10000,\n  DataKey:bankName,\n  NumberOfUniqueKeys:500\n) OUTPUT TO CSVDataStream;\n\n\nCREATE STREAM RandomDataStream OF RandomData;\n\nCREATE CQ ParseRandomData\nINSERT INTO RandomDataStream\nSELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4])\nFROM CSVDataStream;\n\n\nCREATE JUMPING WINDOW RandomData10Rows\nOVER RandomDataStream KEEP 10 ROWS\nPARTITION BY bankNumber;\n\n\nCREATE TYPE myData(\n  myName String,\n  myAddress String,\n  myBankName String,\n  myBankNumber int KEY,\n  myBankAmount double\n);\n\nCREATE STREAM myDataStream OF myData;\n\nCREATE CQ GetMyData\nINSERT INTO MyDataStream\nSELECT myName, streetAddress, bankName, bankNumber, bankAmount\nFROM RandomData10Rows WHERE bankNumber > 20000 AND bankNumber < 20300;\n\n\nCREATE WACTIONSTORE MyDataActivity CONTEXT OF MyData\nEVENT TYPES(myData )\nPERSIST EVERY 60 second USING (\nJDBC_DRIVER:'@WASTORE-DRIVER@',\nJDBC_URL:'@WASTORE-URL@',\nJDBC_USER:'@WASTORE-UNAME@',\nJDBC_PASSWORD:'@WASTORE-PASSWORD@',\nDDL_GENERATION:'drop-and-CREATE-tables'\n);\n\n\nCreate CQ TrackMyDataActivity\nINSERT INTO MyDataActivity\nSelect * from myDataStream\nLINK SOURCE EVENT;\n\n\nEND APPLICATION LongRunningApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE PROPERTYVARIABLE RetryPolicy='timeOut=10,retryInterval=10,maxRetries=30';\nCREATE OR REPLACE PROPERTYVARIABLE KafkaBrokerAddress='localhost:9099';\n--CREATE OR REPLACE PROPERTYVARIABLE KafkaConfig='batch.size=1048576;linger.ms=30000';\n\nSTOP @WRITERAPPNAME@;\nUNDEPLOY APPLICATION @WRITERAPPNAME@;\nDROP APPLICATION @WRITERAPPNAME@ CASCADE;\n\nCREATE APPLICATION @WRITERAPPNAME@ Recovery 5 second interval;\ncreate flow @WRITERAPPNAME@AgentFlow;\nCREATE SOURCE @WRITERAPPNAME@S USING OracleReader\n(\n\tUsername: 'qatest',\n\tPassword: 'qatest',\n\tConnectionURL: 'dockerhost:1521:xe',\n\tTables: 'qatest.oracle_kw_quiesce_simple_test%',\n\tFetchSize: '1',\n\tconnectionRetryPolicy:'$RetryPolicy',\n\tDictionaryMode: onlineCatalog\n)\nOUTPUT TO @WRITERAPPNAME@SS;\nend flow @WRITERAPPNAME@AgentFlow;\ncreate flow @WRITERAPPNAME@serverFlow;\n--create target sys_out using sysout (name:sysoutput) input from ss;\ncreate stream @WRITERAPPNAME@out_cq_select_SS_1 of global.waevent;\n\nCREATE OR REPLACE CQ @WRITERAPPNAME@cq_select_SS1 \nINSERT INTO @WRITERAPPNAME@out_cq_select_SS_1\nselect *\nfrom @WRITERAPPNAME@ss o\nwhere TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_QUIESCE_SIMPLE_TEST';\n\n\ncreate Target @WRITERAPPNAME@TARGET1 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@WRITERAPPNAME@_dsv_sync_CQ',\n--ParallelThreads:'2',\nPartitionKey:'@metadata(COMMIT_TIMESTAMP)',\nMode:'sync',\nKafkaConfig: 'batch.size=1048576;linger.ms=300000;')\nFORMAT USING dsvFormatter ()\ninput from @WRITERAPPNAME@ss;\n\ncreate Target @WRITERAPPNAME@TARGET2 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@WRITERAPPNAME@_json_sync_CQ',\nParallelThreads:'2',\nPartitionKey:'@metadata(COMMIT_TIMESTAMP)',\nMode:'sync',\nKafkaConfig: 'batch.size=1048576;linger.ms=300000;')\nFORMAT USING jsonFormatter ()\ninput from @WRITERAPPNAME@out_cq_select_SS_1;\n\ncreate Target @WRITERAPPNAME@TARGET3 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@WRITERAPPNAME@_avro_sync_CQ',\n--ParallelThreads:'2',\nPartitionKey:'@metadata(COMMIT_TIMESTAMP)',\nMode:'sync',\nKafkaConfig: 'batch.size=1048576;linger.ms=300000;')\nFORMAT USING avroFormatter (\nschemaFileName:'KafkaAvroTestQuiesce_sync_CQ.avsc')\ninput from @WRITERAPPNAME@out_cq_select_SS_1;\n\nend flow @WRITERAPPNAME@serverFlow;\nend application @WRITERAPPNAME@;\n@DEPLOYAPP@;\nstart @WRITERAPPNAME@;\n\n\nstop application @READERAPPNAME@;\nundeploy application @READERAPPNAME@;\ndrop application @READERAPPNAME@ cascade;\nCREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE SOURCE @READERAPPNAME@_DSV_sync_CQ USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@WRITERAPPNAME@_dsv_sync_CQ',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO @READERAPPNAME@KafkaReaderStream1;\n\nCREATE TARGET kafkaDumpDSV USING FileWriter(\nname:kafkaOuputDSV,\nrolloverpolicy:'filesize:500M',\nfilename:'@READERAPPNAME@_dsv_sync_CQ')\nFORMAT USING DSVFormatter()\nINPUT FROM @READERAPPNAME@KafkaReaderStream1;\n\nCREATE SOURCE @READERAPPNAME@_JSON_sync_CQ USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@WRITERAPPNAME@_json_sync_CQ',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO @READERAPPNAME@KafkaReaderStream2;\n\nCREATE SOURCE @READERAPPNAME@_AVRO_sync_CQ USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@WRITERAPPNAME@_avro_sync_CQ',\n        startOffset:0          \n)\nPARSE USING AvroParser (\nschemaFileName:'KafkaAvroTestQuiesce_sync_CQ.avsc')\nOUTPUT TO @READERAPPNAME@KafkaReaderStream3;\n\n\nend application @READERAPPNAME@;\ndeploy application @READERAPPNAME@;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @WRITERAPPNAME@ RECOVERY 1 SECOND INTERVAL;\n\ncreate flow @APPNAME@_agentflow;\n\nCREATE SOURCE @SOURCE@ USING Ojet\n(\nFetchSize:1,\nUsername:'@SOURCE_USER@',\nPassword:'@pass@',\n--Password:'$SRC_PASSWORD',\nConnectionURL:'@CONNECTION_URL@',\nTables:'@SOURCE_TABLE@',\n)\nOUTPUT TO @STREAM1@;\n\n\nend flow @APPNAME@_agentflow;\n\ncreate flow @APPNAME@_serverflow;\n\nCREATE OR REPLACE TYPE @TYPE@( \ndatae java.util.HashMap , \nTABLE_NAME java.lang.String , \nOPS_NAME java.lang.String , \nDB_TIMESTAMP java.lang.String  ,\nCOMMITSCN java.lang.String ,\nSCN java.lang.String ,\nREC_INS_TIME java.lang.String );\n\nCREATE CQ @CQ1@\nINSERT INTO @STREAM2@\nSELECT  \nCASE WHEN (META(c,\"OperationName\").toString() == \"DELETE\")\nTHEN putUserData(c, 'isDelete', 'true') \nELSE\nputUserData(c,'isDelete', 'false')\nEND\nFROM @STREAM1@ c;\n\nCREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;\n\nCREATE OR REPLACE CQ @CQ2@ \nINSERT INTO @STREAM3@\nSELECT \ndata(e),\nMETA(e,\"TableName\").toString() as TABLE_NAME,\nMETA(e, \"OperationName\").toString() as OPS_NAME,\nMETA(e, \"TimeStamp\").toString() as DB_TIMESTAMP,\nMETA(e,\"COMMITSCN\").toString() as COMMITSCN ,\nMETA(e,\"SCN\").toString() as  SCN ,\nDNOW().toString() as REC_INS_TIME\nFROM @STREAM1@ e;\n\ncreate Target @TARGET1@ using KafkaWriter VERSION @kafakaversion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC1',\nParallelThreads:'',\nPartitionKey:'@metadata(TableName)',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING dsvFormatter ()\ninput from @STREAM1@;\n\n\ncreate Target @TARGET2@ using KafkaWriter VERSION @kafakaversion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC2',\nParallelThreads:'2',\nPartitionKey:'TABLE_NAME',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING jsonFormatter ()\ninput from @STREAM3@;\n\ncreate Target @TARGET3@ using KafkaWriter VERSION @kafakaversion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC3',\nParallelThreads:'',\nPartitionKey:'@userdata(isDelete)',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING avroFormatter (\nschemaFileName:'KafkaAvroTest.avsc')\ninput from @STREAM2@;\n\nend application @WRITERAPPNAME@;\n\nCREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafakaversion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC1',\n        startOffset:0       \n)\nPARSE USING DSVParser ()\nOUTPUT TO KafkaReaderStream1;\n\nCREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafakaversion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC2',\n        startOffset:0          \n)\nPARSE USING JSONParser ()\nOUTPUT TO KafkaReaderStream2;\n\nCREATE TARGET kafkaDumpJSON USING FileWriter(\nfilename:'@READERAPPNAME@_RT_JSON')\nFORMAT USING JSONFormatter()\nINPUT FROM KafkaReaderStream2;\n\nCREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafakaversion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC3',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'KafkaAvroTest.avsc'\n)\nOUTPUT TO KafkaReaderStream3;\n\nCREATE TARGET kafkaDumpAVRO USING FileWriter(\nfilename:'@READERAPPNAME@_RT_AVRO')\nFORMAT USING AvroFormatter(\n    schemaFileName:'@avro_schema_file@'\n)\nINPUT FROM KafkaReaderStream3;\n\nend flow @APPNAME@_serverflow;\nend application @READERAPPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\ncreate flow @APPNAME@_agentflow;\n\nCreate Source @SOURCE@\n        Using OracleReader\n(\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: 'qatest.test77',\n  FetchSize:1,\n  QueueSize:2000,\n  CommittedTransactions:true,\n  Compression:false\n)\nOutput To @STREAM@;\n\nend flow @APPNAME@_agentflow;\n\ncreate flow @APPNAME@_serverflow;\n\ncreate Target @TARGET@ using GCSWriter(\n    bucketname:'@bucketname@',\n    objectname:'@objectname@',\n    projectId:'@project-id@',\n    uploadpolicy:'EventCount:7'\n    ServiceAccountKey:'@file-path@'\n)\nformat using DSVFormatter (\nmembers:'data'\n)\ninput from @STREAM@;\n\nend flow @APPNAME@_serverflow;\n\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 7 with Jumping window and partitioned on two node cluster with one agent\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> KafkaStream -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS\n--\n\nSTOP APPLICATION KStreamN2S2CR7Tester.KStreamN2S2CRTest7;\nUNDEPLOY APPLICATION KStreamN2S2CR7Tester.KStreamN2S2CRTest7;\nDROP APPLICATION KStreamN2S2CR7Tester.KStreamN2S2CRTest7 CASCADE;\nDROP USER KStreamN2S2CR7Tester;\nDROP NAMESPACE KStreamN2S2CR7Tester CASCADE;\nCREATE USER KStreamN2S2CR7Tester IDENTIFIED BY KStreamN2S2CR7Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR7Tester;\nCONNECT KStreamN2S2CR6Tester KStreamN2S2CR7Tester;\n\nCREATE APPLICATION KStreamN2S2CRTest7 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionKStreamN2S2CRTest7;\n\nCREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');\nCREATE STREAM KafkaCsvStream of CsvDataTypeKStreamN2S2CRTest7 using KafkaProps;\n\nCREATE SOURCE CsvSourceKStreamN2S2CRTest7 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\n\nCREATE TYPE CsvDataTypeKStreamN2S2CRTest7 (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE CQ TransferToKafka\nINSERT INTO KafkaCsvStream\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream;\n\nEND FLOW DataAcquisitionKStreamN2S2CRTest7;\n\nCREATE FLOW DataProcessingKStreamN2S2CRTest7;\n\nCREATE STREAM DataStream OF CsvDataTypeKStreamN2S2CRTest7 PARTITION BY merchantId;\n\nCREATE CQ CsvToDataKStreamN2S2CRTest7\nINSERT INTO DataStream\nSELECT\n    *\nFROM KafkaCsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE WactionsKStreamN2S2CRTest7 CONTEXT OF CsvDataTypeKStreamN2S2CRTest7\nEVENT TYPES ( CsvDataTypeKStreamN2S2CRTest7 )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWactionKStreamN2S2CRTest7\nINSERT INTO WactionsKStreamN2S2CRTest7\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND FLOW DataProcessingKStreamN2S2CRTest7;\n\nEND APPLICATION KStreamN2S2CRTest7;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION KafkaWPTester.KWApp;\nSTOP APPLICATION KafkaWPTester.KRApp;\nUNDEPLOY APPLICATION KafkaWPTester.KWApp;\nUNDEPLOY APPLICATION KafkaWPTester.KRApp;\nDROP APPLICATION KafkaWPTester.KWApp CASCADE;\nDROP APPLICATION KafkaWPTester.KRApp CASCADE;\n\nCREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;\nCONNECT KafkaWPTester KafkaWPTester;\n\n\nCREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE SOURCE CSVSource USING FileReader (\n\tdirectory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',\n    WildCard:'access_log_half',\n\tpositionByEOF:false,\n        Charset:'UTF-8'\n)\nPARSE USING DSVParser (\n  columndelimiter:' ',\n  ignoreemptycolumn:'Yes',\n  quoteset:'[]~\"',\n  separator:'~'\n)\nOUTPUT TO FileStream;\n\nCREATE TYPE AccessLogType(\nCol1 String,\nCol2 String,\nCol3 String,\nCol4 String,\nCol5 String,\nCol6 String,\nCol7 String,\nCol8 String,\nCol9 String,\nCol10 String\n);\n\nCREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;\n\nCREATE CQ AceeslogCQ\nINSERT INTO TypedAccessLogStream\nSELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]\nFROM FileStream;\n\ncreate Target DsvWriter using KafkaWriter VERSION '0.10.0' ( \n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V10dsvExistTopic',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream;\n\n\ncreate Target JsonWriter using KafkaWriter VERSION '0.10.0' ( \n\tbrokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n\tTopic:'V10jsonExistTopic',\n\tMode:'Sync'\n    )\n \nFORMAT USING JSONFormatter ()\ninput from TypedAccessLogStream;\n\n\ncreate Target AvroWriter using KafkaWriter VERSION '0.10.0' ( \n\tbrokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n\tTopic:'V10avroExistTopic',\n    Mode:'Sync'\n    )\n \nFORMAT USING AvroFormatter (\n\tschemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')\ninput from TypedAccessLogStream;\n\n\nEND APPLICATION KWApp;\n\nDEPLOY APPLICATION KWApp on any in default;\n\n\n-- Kafka Reader Apps\n\nCREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE TYPE AccessLogType2(\nCol1 String,\nCol2 String,\nCol3 String,\nCol4 String,\nCol5 String,\nCol6 String,\nCol7 String,\nCol8 String,\nCol9 String,\nCol10 String\n);\n\nCREATE Stream KafkaDSVReaderStream of AccessLogType2;\nCREATE Stream KafkaJsonReaderStream of AccessLogType2;\nCREATE Stream KafkaAvroReaderStream of AccessLogType2;\n\n\nCREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V10dsvExistTopic',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream1;\n\n\n\n\nCREATE CQ DSVtoKafkaDSVReaderStream\nINSERT INTO KafkaDSVReaderStream\nSELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]\nFROM KafkaReaderStream1;\n\nCREATE TARGET DSVDump USING FileWriter(\nname:testOuput,\nrolloverpolicy:'filesize:500M',\nfilename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V10_RT_DSV')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaDSVReaderStream;\n\nCREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V10jsonExistTopic',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO KafkaReaderStream2;\n\n\n\n\nCREATE CQ DSVtoKafkaJsonReaderStream\nINSERT INTO KafkaJsonReaderStream\nSELECT \ndata.get('Col1').textValue(),\ndata.get('Col2').textValue(),\ndata.get('Col3').textValue(),\ndata.get('Col4').textValue(),\ndata.get('Col5').textValue(),\ndata.get('Col6').textValue(),\ndata.get('Col7').textValue(),\ndata.get('Col8').textValue(),\ndata.get('Col9').textValue(),\ndata.get('Col10').textValue()\nFROM KafkaReaderStream2;\n\nCREATE TARGET JSONDump USING FileWriter(\nname:testOuput,\nrolloverpolicy:'filesize:500M',\nfilename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V10_RT_JSON')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaJsonReaderStream;\n\nCREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '0.10.0' (\n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V10avroExistTopic',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'\n)\nOUTPUT TO KafkaReaderStream3;\n\n\n\nCREATE Stream KRTypedStream of AccessLogType;\n\nCREATE CQ DSVtoKafkaAvroReaderStream\nINSERT INTO KafkaAvroReaderStream\nSELECT    \ndata.get(\"Col1\").toString(), \ndata.get(\"Col2\").toString(), \ndata.get(\"Col3\").toString(), \ndata.get(\"Col4\").toString(),\ndata.get(\"Col5\").toString(), \ndata.get(\"Col6\").toString(), \ndata.get(\"Col7\").toString(), \ndata.get(\"Col8\").toString(),\ndata.get(\"Col9\").toString(), \ndata.get(\"Col10\").toString()\nFROM KafkaReaderStream3;\n\nCREATE TARGET AVRODump USING FileWriter(\nname:testOuput,\nrolloverpolicy:'filesize:500M',\nfilename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V10_RT_AVRO')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaAvroReaderStream;\n\nEND APPLICATION KRApp;\nDEPLOY APPLICATION KRApp on any in default;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create or replace type @STREAM@details(\nC_CUSTKEY int,\nC_MKTSEGMENT String,\nC_NATIONKEY int,\nC_NAME String,\nC_ADDRESS String,\nC_PHONE String,\nC_ACCTBAL int,\nC_COMMENT String\n);\n\ncreate or replace stream @STREAM@_TYPED of @STREAM@details;\n\nCreate or replace CQ @STREAM@detailsCQ\ninsert into @STREAM@_TYPED\nselect \nto_int(data[0]),data[1],to_int(data[2]),data[3],data[4],data[5],to_int(data[6]),data[7]\nfrom @STREAM@;\n\nCREATE WINDOW @STREAM@_DBRWindow\nOVER @STREAM@_TYPED\nKEEP 1000 ROWS;\n\ncreate or replace stream @STREAM@_TYPED2 of @STREAM@details;\n\nCreate or replace CQ @STREAM@detailsCQ2\ninsert into @STREAM@_TYPED2\nselect \nto_int(C_CUSTKEY),C_MKTSEGMENT,to_int(C_NATIONKEY),C_NAME,C_ADDRESS,C_PHONE,to_int(C_ACCTBAL),C_COMMENT\nfrom @STREAM@_DBRWindow;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@ USING FileWriter  ( \n  filename:'CompressedMerchant.gz',\n  directory:'/logs/',\n  rolloverpolicy:'EventCount:10000,Interval:30s'\n ) Format using DSVFormatter()\nINPUT FROM @STREAM@_DBRWindow;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 32 with two sources, two sliding attribute windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Sa5W/p -> CQ1 -> WS\n-- S2 -> Sa6W/p -> CQ2 -> WS\n--\n\nSTOP Recov32Tester.RecovTest32;\nUNDEPLOY APPLICATION Recov32Tester.RecovTest32;\nDROP APPLICATION Recov32Tester.RecovTest32 CASCADE;\nCREATE APPLICATION RecovTest32 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1\nGROUP BY merchantId;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2\nGROUP BY merchantId;\n\nEND APPLICATION RecovTest32;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop APPLICATION @APPNAME@_ExceptionStore;;\nundeploy APPLICATION @APPNAME@_ExceptionStore;\ndrop APPLICATION @APPNAME@_ExceptionStore cascade;\n\nCREATE APPLICATION @APPNAME@_ExceptionStore;\n\nCREATE TYPE @APPNAME@2_CDCStreams_Type  (\n exceptionType java.lang.String,\n  action java.lang.String,\n  appName java.lang.String,\n  entityType java.lang.String,\n  entityName java.lang.String,\n  className java.lang.String,\n  message java.lang.String,\n  relatedEntity java.lang.String,\n  exceptionCode java.lang.String\n );\n\nCREATE STREAM @APPNAME@2_CDCStreams OF @APPNAME@2_CDCStreams_Type;\n\nCREATE CQ @APPNAME@2_ReadFromExpStore\nINSERT INTO @APPNAME@2_CDCStreams\nselect s.exceptionType,s.action,s.appName,s.entityType,s.entityName,s.className,s.message,s.relatedEntity,s.exceptionCode from @APPNAME@_App_ExceptionStore [jumping @WINDOWINTERVAL@ seconds] s;\n\nCREATE OR REPLACE TARGET @APPNAME@2_WriteToFileAsJSON USING FileWriter  (\n  filename: 'expEvent.log',\n  rolloveronddl: 'true',\n  flushpolicy: 'eventcount:5',\n  adapterName: 'FileWriter',\n  directory: 'ExpStore_logs',\n  rolloverpolicy: 'eventcount:5'\n )\nFORMAT USING JSONFormatter  (\n  handler: 'com.webaction.proc.JSONFormatter',\n  formatterName: 'JSONFormatter',\n  jsonMemberDelimiter: '\\n',\n  jsonobjectdelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true'\n )\nINPUT FROM @APPNAME@2_CDCStreams;\n\nEND APPLICATION @APPNAME@_ExceptionStore;\n\ndeploy application @APPNAME@_ExceptionStore;\nstart @APPNAME@_ExceptionStore;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 26 with two sources, two jumping attribute windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Ja5W -> CQ1 -> WS\n-- S2 -> Ja6W -> CQ2 -> WS\n--\n\nSTOP Recov26Tester.RecovTest26;\nUNDEPLOY APPLICATION Recov26Tester.RecovTest26;\nDROP APPLICATION Recov26Tester.RecovTest26 CASCADE;\nCREATE APPLICATION RecovTest26 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream6Minutes p;\n\nEND APPLICATION RecovTest26;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\ncreate flow @APPNAME@_agentflow;\n\nCreate Source @SOURCE@\n        Using OracleReader\n(\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: 'qatest.test77',\n  FetchSize:1,\n  QueueSize:2000,\n  CommittedTransactions:true,\n  Compression:false\n)\nOutput To @STREAM@;\n\nend flow @APPNAME@_agentflow;\n\ncreate flow @APPNAME@_serverflow;\n\ncreate Target @TARGET@ using googlepubsubwriter(\n    ServiceAccountKey:'@SAS-KEY@',\nProjectId:'@PROJECTID@',\ntopic:'@topic@',\nBatchPolicy:'@BATCHPOLICY@'\n)\nformat using DSVFormatter (\n)\ninput from @STREAM@;\n\n\nend flow @APPNAME@_serverflow;\n\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP LngIS2noder.LongRunningISStreamApp;\nUNDEPLOY APPLICATION LngIS2noder.LongRunningISStreamApp;\nDROP APPLICATION LngIS2noder.LongRunningISStreamApp CASCADE;\n\nCREATE APPLICATION LongRunningISStreamApp;\n\nCREATE FLOW ISFLOW1;\n----------------------------------------------------\nCREATE source implicitSOurce USING StreamReader\n(\n   OutputType: 'LngIS2noder.Atm',\n   noLimit: 'true',\n   maxRows: 1000,\n   iterationDelay: 50,\n   StringSet: 'productID[001-002-003-004],stateID[AS-CA-WA-NY],currentDate[2014-2015]',\n   NumberSet: 'productWeight[1-10]R,quantity[50.5-160.1]G,size[763872-4778823]L'\n) OUTPUT TO CsvStream;\n\n\nCREATE TYPE Atm(\n  productID String KEY,\n  stateID String,\n  productWeight int,\n  quantity double,\n  size long,\n  currentDate org.joda.time.DateTime);\n\nEND FLOW ISFLOW1;\n----------------------------------------------------\n\nCREATE FLOW ISFLOW2;\n\nCREATE CACHE cache1 USING CsvReader(\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'ISdata.csv',\n  header: false,\n  columndelimiter: ',',\n  trimquote: false\n  ) QUERY (keytomap:'productID') OF Atm;\n\n\nCREATE STREAM newStream OF Atm;\n\n\nCREATE CQ newCQ\nINSERT INTO newStream\nSELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]), TO_DATE(data[5]) FROM\nCsvStream;\n\nCREATE WINDOW win1\nOVER newStream\nKEEP 50 rows;\n\n\nCREATE CQ newCQ2\nINSERT INTO newStream2\nSELECT productID as A , stateID AS B, productWeight AS C, quantity AS D, size AS E, currentDate AS F FROM\nnewStream;\n\n\nCREATE CQ newCQ3\nINSERT INTO newStream3 PARTITION BY A\nSELECT A,B,C,D,E,F FROM newStream2 order by C,D\nlink source event;\n\nCREATE CQ newCQ4\nINSERT INTO newStream4\nSELECT count(productID),currentDate FROM newStream ORDER BY currentDate\nlink source event;\n\nCREATE CQ newCQ5\nINSERT INTO newStream5\nSELECT x.*, y.* from cache1 x, newStream y WHERE x.productweight > 6 ORDER BY x.currentDate;\n\n\nCREATE WACTIONSTORE WS1 CONTEXT OF Atm\nEVENT TYPES(Atm );\n\nCREATE CQ newCQ6\nINSERT INTO WS1\nSELECT * FROM newStream WHERE productID = '001';\n\nCREATE CQ newCQ7\nINSERT INTO newStream6\nSELECT aa.productID FROM WS1 [push] aa, cache1 bb;\n\n\nCREATE CQ newCQ8\nINSERT INTO newStream7\nSELECT Sum(X.size) FROM (Select size from win1 where productweight > 5) X;\n\n\nEND FLOW ISFLOW2;\n----------------------------------------------------\n\nEND APPLICATION LongRunningISStreamApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n--create flow AgentFlow;\nCREATE SOURCE @APPNAME@_S USING OracleReader\n(\n\tUsername: 'qatest',\n\tPassword: 'qatest',\n\tConnectionURL: 'dockerhost:1521:xe',\n\tTables: 'qatest.test01',\n\tFetchSize: '1',\n\tconnectionRetryPolicy:'timeOut=00,retryInterval=1,maxRetries=3'\n)\nOUTPUT TO @APPNAME@_SS;\n--end flow AgentFlow;\n--create flow serverFlow;\n--create target sys_out using sysout (name:sysoutput) input from ss;\n\nCREATE or replace @APPNAME@_TARGET T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'Eventcount:1000,Interval:30',\nStandardSQL:true,\noptimizedMerge:true\t\t\n) INPUT FROM @APPNAME@_ss;\n--end flow serverFlow;\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\n--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 3\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> JW -> CQ(aggregate) -> WS\n--\n\nSTOP Recov3Tester.RecovTest3;\nUNDEPLOY APPLICATION Recov3Tester.RecovTest3;\nDROP APPLICATION Recov3Tester.RecovTest3 CASCADE;\nCREATE APPLICATION RecovTest3 RECOVERY 1 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE WactionType (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount int,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionType;\n\nCREATE CQ CsvToData\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_INT(TO_DOUBLE(data[7])),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.merchantId),\n    FIRST(p.dateTime),\n    SUM(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nEND APPLICATION RecovTest3;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE TARGET @TARGET_NAME@ USING SpannerWriter (\n  instanceId: 'qatest',\n  BatchPolicy: 'EventCount:10000,Interval:60',\n  Tables: 'QATEST.ORACLETOSPANNER_SOURCETABLE%,oracletospannerdb.OracleToSpannertarget',\n  adapterName: 'SpannerWriter',\n  ServiceAccountKey: '/Users/jenniffer/Product2/IntegrationTests/TestData/google-gcs.json'\n) INPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop DataGenSampleApp;\nundeploy application DataGenSampleApp;\ndrop application DataGenSampleApp cascade;\n\n\nCREATE APPLICATION DataGenSampleApp;\n\ncreate Source waSrcName\n        Using HPNonStopSQLMXReader\n  (AgentPortNo:'7012',\n   AgentIPaddress:'$NSKADDR',\n   PortNo:'5013',\n   -- ipaddress: '$WAADDR',\n   Name:'S00',\n   ReturnDateTimeAs: 'String',\n   Tables:'sncat.snsch.DBWriterTest1')\nOutput To waAppName_Stream;\n\ncreate Target dataGenTgt using SysOut(name:dataGenTgt) input from waAppName_Stream;\n\nEND APPLICATION DataGenSampleApp;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop PatternMatching1.CSV;\nundeploy application PatternMatching1.CSV;\ndrop application PatternMatching1.CSV cascade;\n\ncreate application CSV;\n\ncreate source CSVSource using CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'posdata.csv',\n  columndelimiter:',',\n  positionByEOF:false\n)\nOUTPUT TO CsvStream;\n\nCREATE CQ ParseUserData\nINSERT INTO UserDataStream\nSELECT  TO_STRING(data[0]) as temp1,\n        TO_STRING(data[1]) as temp2,\n        TO_DOUBLE(data[2]) as temp3\nFROM CsvStream;\n\n-- scenario 1.1 check pattern using PosApp data with alteration and quantifier(0 or 1)\nCREATE CQ TypeConversionPosAppCQ1\nINSERT INTO TypedStream1\nSELECT A.temp1 as typeduserid,\n       B.temp2 as typedtemp1,\n       C.temp3 as typedtemp2\nfrom UserDataStream\nMATCH_PATTERN A | B ? C\ndefine A = UserDataStream(temp1 = 'COMPANY 4'),\n       B = UserDataStream(temp2 = 'OFp6pKTMg26n1iiFY00M9uSqh9ZfMxMBRf1'),\n       C = UserDataStream(temp3 = 520236368216865619l)\nPARTITION BY temp1;\n\n-- scenario 1.2 check pattern using PosApp data with permutation and quantifier(0 or 1)\nCREATE CQ TypeConversionPosAppCQ2\nINSERT INTO TypedStream2\nSELECT A.temp1 as typeduserid,\n       B.temp2 as typedtemp1,\n       C.temp3 as typedtemp2\nfrom UserDataStream\nMATCH_PATTERN A & B ? C\ndefine A = UserDataStream(temp1 = 'COMPANY 100'),\n       B = UserDataStream(temp2 = 'IYuqAbAQ07NS3lZO74VGPldfAUAGKwzR2k3'),\n       C = UserDataStream(temp3 = 6388500771470313223l)\nPARTITION BY temp3;\n\nCREATE WACTIONSTORE UserActivityInfoPosApp1\nCONTEXT OF TypedStream1_Type\nEVENT TYPES ( TypedStream1_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfoPosApp2\nCONTEXT OF TypedStream2_Type\nEVENT TYPES ( TypedStream2_Type )\n@PERSIST-TYPE@\n\ncreate Target t2 using SysOut(name:AgentTyped) input from TypedStream1;\ncreate Target t3 using SysOut(name:AgentTyped1) input from TypedStream2;\n\n--get data from UserDataStream and place into wactionStore UserWaction\nCREATE CQ UserWaction1\nINSERT INTO UserActivityInfoPosApp1\nSELECT * FROM TypedStream1\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction2\nINSERT INTO UserActivityInfoPosApp2\nSELECT * FROM TypedStream2\nLINK SOURCE EVENT;\n\nend application CSV;\ndeploy application csv;\nstart csv;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE MongoDB_Source USING Global.MongoDBReader(\ncollections: '@table@',\n  ConnectionURL: '@connectionUrl@',\n  mode: 'Incremental',\n  Username: '@userName@',\n  Password: '@Password@' )\nOutput To mongoDBRaw_Stream;\n\nCREATE CQ JSON2StriimType\nINSERT INTO jsonNodeEventStream\nSELECT data.get(\"_id\").toString() AS ID,\n  data.get(\"firstname\").toString() AS FirstName,\n  data.get(\"lastname\").toString() AS LastName,\n  data.get(\"age\").toString() AS Age\nFROM mongoDBRaw_Stream;\n\nCREATE TARGET MongoJson_FileDSV_Target USING Global.FileWriter ( \n  flushpolicy: 'EventCount:10000,Interval:30s',  \n  directory: '@logs@',\n  filename: '@BuiltinFunc@_Data', \n  rolloverpolicy: 'EventCount:10000,Interval:30s' ) \nFORMAT USING Global.DSVFormatter  ( \n   ) \nINPUT FROM jsonNodeEventStream;\n\nEnd Application @AppName@;\nDeploy application @AppName@;\nStart Application @AppName@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application app1PS;\nundeploy application app1PS;\ndrop application app1PS cascade;\n\ncreate application app1PS;\n\ncreate target File_TargerPS using FileWriter\n(\ndirectory : '',\nfilename : ''\n)\nformat using DSVFormatter()\ninput from KPSRss1;\n\nend application app1PS;\n\ndeploy application app1PS;\nstart application app1PS;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@ @APP_PROPERTY@ USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE STREAM @APP_NAME@_DataStream OF Global.WAEvent;\n\nCreate Source @APP_NAME@_Source Using @SOURCE_ADAPTER@ (\n\n) OUTPUT TO @APP_NAME@_DataStream;\n\nCREATE TARGET @APP_NAME@_Target USING @TARGET_ADAPTER@ ( \n\n) INPUT FROM @APP_NAME@_DataStream;\n\nCREATE OR REPLACE TARGET @APP_NAME@_SysOut USING Global.SysOut ( \n\tname: '@APP_NAME@_SysOutWA' \n) INPUT FROM @APP_NAME@_DataStream;\n\nEND APPLICATION @APP_NAME@;\n\nDEPLOY APPLICATION @APP_NAME@ IN DEFAULT;\nSTART APPLICATION @APP_NAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop OracleReaderToDBWriter;\nUndeploy application OracleReaderToDBWriter;\nalter application OracleReaderToDBWriter;\nCREATE OR REPLACE SOURCE Oraclesrc USING OracleReader  ( \n  Username: 'qatest',\n  Password: 'qatest',\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',\n  Tables: 'QATEST.Orcalesrc',\n  FetchSize: '3'\n ) \nOUTPUT TO OrcStrm;\nalter application OracleReaderToDBWriter recompile;\nDEPLOY APPLICATION OracleReaderToDBWriter with Hz_Agent_flow on any in AGENTS;\nstart application OracleReaderToDBWriter;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 1 second interval;\nCREATE TYPE @APPNAME@_Test_1_type\n( id java.lang.Integer KEY , \nstatus java.lang.String  \n );\n\nCREATE OR REPLACE CACHE @APPNAME@_Test_1 USING DatabaseReader ( \n  --ConnectionURL: 'jdbc:sqlserver://@MSSQLIP@:@MSSQLPORT@',\n  ConnectionURL: 'jdbc:sqlserver://localhost:1433',\n  DatabaseProviderType: 'Default',\n  FetchSize: 1,\n  Password:'@Password@',\n  Query: 'Select id,status from [QATEST].[QATEST].[Test_1]',\n  Username: '@Username@'\n ) \nQUERY ( \n  keytomap: 'id',\n  skipinvalid: 'false'\n ) \n OF @APPNAME@_Test_1_type;\n\nCREATE TYPE @APPNAME@_Test_3_type\n( id java.lang.Integer KEY , \nstatus java.lang.String  \n );\n\nCREATE TYPE @APPNAME@_Test_2_type\n( id java.lang.Integer KEY , \nstatus java.lang.String  \n );\n\nCREATE OR REPLACE CACHE @APPNAME@_Test_3 USING DatabaseReader ( \n  --ConnectionURL: 'jdbc:sqlserver://@MSSQLIP@:@MSSQLPORT@',\n    ConnectionURL: 'jdbc:sqlserver://localhost:1433',\n  DatabaseProviderType: 'Default',\n  FetchSize: 1,\n  Password:'@Password@',\n  Query: 'Select id,status from [QATEST].[QATEST].[Test_3]',\n  Username: '@Username@'\n ) \nQUERY ( \n  keytomap: 'id',\n  skipinvalid: 'false'\n ) \n OF @APPNAME@_Test_3_type;\n \n CREATE OR REPLACE CACHE @APPNAME@_Test_2 USING DatabaseReader ( \n  --ConnectionURL: 'jdbc:sqlserver://@MSSQLIP@:@MSSQLPORT@',\n  ConnectionURL: 'jdbc:sqlserver://localhost:1433',\n  DatabaseProviderType: 'Default',\n  FetchSize: 1,\n  Password:'@Password@',\n  Query: 'Select id,status from [QATEST].[QATEST].[Test_2]',\n  Username: '@Username@'\n ) \nQUERY ( \n  keytomap: 'id',\n  skipinvalid: 'false',\n  refreshinterval: '5 SECOND'\n ) \n OF @APPNAME@_Test_2_type;\n \n\nCREATE OR REPLACE SOURCE @APPNAME@_s1 USING MSSqlReader  ( \n  Compression: true,\n  cdcRoleName: 'STRIIM_READER',\n  DatabaseName: 'QATEST',\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ConnectionPoolSize: 1,\n  FetchTransactionMetadata: false,\n  StartPosition: 'EOF',\n  Username: 'qatest',\n  SendBeforeImage: true,\n  AutoDisableTableCDC: true,\n  ConnectionURL: 'localhost:1433',\n  Tables: 'qatest.Test_Master',\n  adapterName: 'MSSqlReader',\n  Password: 'w3b@ct10n'\n) \nOUTPUT TO @APPNAME@_ss1;\n\nCREATE OR REPLACE SOURCE @APPNAME@_s2 USING MSSqlReader  ( \n  Compression: true,\n  cdcRoleName: 'STRIIM_READER',\n  DatabaseName: 'QATEST',\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ConnectionPoolSize: 1,\n  FetchTransactionMetadata: false,\n  StartPosition: 'EOF',\n  Username: 'qatest',\n  SendBeforeImage: true,\n  AutoDisableTableCDC: true,\n  ConnectionURL: 'localhost:1433',\n  Tables: 'qates%.Test_%',\n  adapterName: 'MSSqlReader',\n  Password: 'w3b@ct10n'\n) \nOUTPUT TO @APPNAME@_ss2;\n\nCREATE TYPE @APPNAME@_ExtractedFields_Type  ( \nid java.lang.Long , \nname java.lang.String , \ncity java.lang.String , \nTest_1_id java.lang.Long , \nTest_3_id java.lang.Long ,\nTest_2_id java.lang.Long \n );\n\nCREATE OR REPLACE STREAM @APPNAME@_ExtractedFields OF @APPNAME@_ExtractedFields_Type;\n\n\nCREATE OR REPLACE CQ @APPNAME@_ExtractedFields_cq\nINSERT INTO @APPNAME@_ExtractedFields\nSELECT \n   b.data[0] as id,\n   b.data[1] as name,\n   b.data[2] as city,\n   b.data[3] as Test_1_ID,\n   b.data[4] as Test_3_ID,\n   b.data[5] as Test_2_ID\nFROM\n@APPNAME@_ss1 b LEFT OUTER JOIN @APPNAME@_Test_2 s ON TO_INT(b.data[0]) = s.id\n LEFT OUTER JOIN \n@APPNAME@_Test_1 p ON TO_INT(b.data[3]) = p.id\n LEFT OUTER JOIN\n@APPNAME@_Test_3 r ON TO_INT(b.data[4]) = r.id\nWHERE (TO_STRING(META(b, \"OperationName\")) = \"UPDATE\" or TO_STRING(META(b, \"OperationName\")) = \"INSERT\")\nlimit 1;\n\nCREATE  TARGET @APPNAME@_t1_dsv USING AzureEventHubWriter  ( \n  EventHubNamespace: 'EventHubWriterTest',\n  EventHubName: 'test_01',\n  E1P: 'true',\n  SASPolicyName: 'RootManageSharedAccessKey',\n  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',\n  PartitionKey: 'id',\n  ConsumerGroup: 'test_01_cg',\n  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n  --ParallelThreads: '2'\n ) \nFORMAT USING DSVFormatter  (  ) \nINPUT FROM @APPNAME@_ExtractedFields;\n\nCREATE  TARGET @APPNAME@_t2_json USING AzureEventHubWriter  ( \n  EventHubNamespace: 'EventHubWriterTest',\n  EventHubName: 'test_02',\n  E1P: 'true',\n  SASPolicyName: 'RootManageSharedAccessKey',\n  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',\n  PartitionKey: 'id',\n  ConsumerGroup: 'test_02_cg',\n  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n  --ParallelThreads: '2'\n ) \nFORMAT USING JSONFormatter  (  ) \nINPUT FROM @APPNAME@_ExtractedFields;\nCREATE  TARGET @APPNAME@_t3_avro USING AzureEventHubWriter  ( \n  EventHubNamespace: 'EventHubWriterTest',\n  EventHubName: 'test_03',\n  E1P: 'true',\n  SASPolicyName: 'RootManageSharedAccessKey',\n  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',\n  PartitionKey: 'id',\n  ConsumerGroup: 'test_03_cg',\n  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n  --ParallelThreads: '2'\n ) \nFORMAT USING AvroFormatter  (   schemaFileName: 'kafkaAvroTest_multipleReader.avsc'\n ) \nINPUT FROM @APPNAME@_ExtractedFields;\n\nCREATE  TARGET @APPNAME@_t1_dsv_rawstream USING AzureEventHubWriter  ( \n  EventHubNamespace: 'EventHubWriterTest',\n  EventHubName: 'test_04',\n  E1P: 'true',\n  SASPolicyName: 'RootManageSharedAccessKey',\n  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',\n  PartitionKey: '@metadata(TableName)',\n  ConsumerGroup: 'test_04_cg',\n  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n  --ParallelThreads: '2'\n ) \nFORMAT USING DSVFormatter  (  ) \nINPUT FROM @APPNAME@_ss2;\n\nCREATE  TARGET @APPNAME@_t2_json_rawstream USING AzureEventHubWriter  ( \n  EventHubNamespace: 'EventHubWriterTest',\n  EventHubName: 'test_05',\n  E1P: 'true',\n  SASPolicyName: 'RootManageSharedAccessKey',\n  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',\n  PartitionKey: '@metadata(TableName)',\n  ConsumerGroup: 'test_05_cg',\n  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n  --ParallelThreads: '2'\n ) \nFORMAT USING JSONFormatter  (  ) \nINPUT FROM @APPNAME@_ss2;\n\nCREATE  TARGET @APPNAME@_t3_avro_rawstream USING AzureEventHubWriter  ( \n  EventHubNamespace: 'EventHubWriterTest',\n  EventHubName: 'test_06',\n  E1P: 'true',\n  SASPolicyName: 'RootManageSharedAccessKey',\n  SASKey: 'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',\n  PartitionKey: '@metadata(TableName)',\n  ConsumerGroup: 'test_06_cg',\n  ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n  --ParallelThreads: '2'\n ) \nFORMAT USING AvroFormatter  (   schemaFileName: 'kafkaAvroTest_multipleReader.avsc'\n ) \nINPUT FROM @APPNAME@_ss2;\n\nEND APPLICATION @APPNAME@;\nDeploy application @APPNAME@;\nstart @APPNAME@;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n--create flow AgentFlow;\nCREATE SOURCE @APPNAME@_S USING OracleReader\n(\n\tUsername: 'qatest',\n\tPassword: 'qatest',\n\tConnectionURL: 'dockerhost:1521:xe',\n\tTables: 'qatest.test01',\n\tFetchSize: '1',\n\tconnectionRetryPolicy:'timeOut=00,retryInterval=1,maxRetries=3'\n)\nOUTPUT TO @APPNAME@_SS;\n--end flow AgentFlow;\n--create flow serverFlow;\n--create target sys_out using sysout (name:sysoutput) input from ss;\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(id,col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'Eventcount:1000,Interval:30',\nStandardSQL:true,\noptimizedMerge:true\t\t\n) INPUT FROM @APPNAME@_ss;\n--end flow serverFlow;\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\n--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @appName@;\nundeploy application @appName@;\ndrop application @appName@ cascade;\nCREATE APPLICATION @appName@ RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE OR REPLACE SOURCE @appName@_Source USING OracleReader\n(\n  FilterTransactionBoundaries:true,\n  ConnectionURL:'@ConnectionURL@',\n  Tables:'@OrcTable@',\n  Password:'@Password@',\n  fetchsize:'1',\n  connectionRetryPolicy:'timeOut=30, retryInterval=30, maxRetries=3',\n  Username:'@Username@'\n)\nOUTPUT TO @appName@_Stream;\n\nCREATE OR REPLACE TARGET @appName@_Target1 USING Global.BigQueryWriter (\n  ColumnDelimiter: '|',\n  NullMarker: 'NULL',\n  ConnectionRetryPolicy: 'retryInterval=30,\\n maxRetries=3',\n  streamingUpload: 'false',\n  Mode: 'Merge',\n  projectId: '@ProjectId@',\n  Encoding: 'UTF-8',\n  TransportOptions: 'connectionTimeout=300,\\n readTimeout=120',\n  Tables: '@OrcTable@,@BqTable@',\n  AllowQuotedNewlines: 'false',\n  CDDLAction: 'Process',\n  adapterName: 'BigQueryWriter',\n  serviceAccountKey: '@GCS-AuthPath@',\n  optimizedMerge: 'true',\n  StandardSQL: 'true',\n  includeInsertId: 'true',\n  QuoteCharacter: '\\\"',\n  BatchPolicy: 'eventCount:100,Interval:10' )\nINPUT FROM @appName@_Stream;\n\nEnd application @appName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @appName@_AzureEventHubWriter USING AzureEventHubWriter (\n  SASKey: '',\n  EventHubName: '',\n  EventHubNamespace: '',\n  SASPolicyName: '',\n  BatchPolicy: 'Size:1000000,Interval:10s' )\nFORMAT USING JSONFormatter  (\n )\nINPUT FROM @appName@_MCQOut1;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CONNECT ADMIN abc;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\nCREATE SOURCE @APPNAME@_S USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.test01',\n  connectionRetryPolicy:'retryInterval=1,maxRetries=3'\n ) \nOUTPUT TO @APPNAME@_SS;\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'public.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1000,Interval:30',\nStandardSQL:true,\noptimizedMerge:true\t\n) INPUT FROM @APPNAME@_ss;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\n--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 1\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> WS\n--\n\n\nUNDEPLOY APPLICATION NameT00.T00;\nDROP APPLICATION NameT00.T00 CASCADE;\nCREATE APPLICATION T00 RECOVERY 5 SECOND INTERVAL;\n\n\n\n\nCREATE FLOW DataAcquisitionT00;\n\n\nCREATE SOURCE CsvSourceT00 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'Canon1000.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO OutputStreamT00;\n\n\nEND FLOW DataAcquisitionT00;\n\n\n\n\nCREATE FLOW DataProcessingT00;\n\n\nCreate Target OutputTargetT00\nUsing Sysout (name: 'OutputTargetT00')\nInput From OutputStreamT00;\n\n\nEND FLOW DataProcessingT00;\n\n\n\nEND APPLICATION T00;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader\n(\nFetchSize:'@FETCHSIZE@',\nQueueSize:'@QUEUESIZE@',\nUsername:'@USERNAME@',\nPassword:'@PASSWORD@',\nConnectionURL:'@URL@',\n_h_useClassic: @CLASSICMODE@,\nTables:'@TABLES@',\nCommittedTransactions:'@COMMITEDT@'\nCompression:'@COMPRESSION@'\n)\nOUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)\nSELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[2]) = 'Null' ;\n\nCREATE TYPE LogType(\nNum_col String key,\nChar_col String,\nVarchar2_col String,\nlong_col String,\nTable String,\nOperation String\n);\n\nCREATE WINDOW CDCWindow\nOVER @STREAM@\nKEEP 1 ROWS;\n\nCREATE WACTIONSTORE CDCWS CONTEXT of LogType\nEVENT TYPES ( LogType )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ ToWactionStore\nINSERT INTO CDCWS\nSELECT * FROM CDCWindow\nLINK SOURCE EVENT;\n\nCREATE TARGET @SOURCE_NAME@_SYS USING SysOut (\n  name: '@SOURCE_NAME@_SYS' )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet\n(\n  ConnectionURL: '@CONNECTION_URL@',\n  CommittedTransactions: true,\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OJet',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  Username: '@SOURCE_USER@'\n)OUTPUT TO @STREAM@;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OJet\n(\n  ConnectionURL: '@CONNECTION_URL@',\n  CommittedTransactions: true,\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OJet',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  Username: '@SOURCE_USER@'\n)OUTPUT TO @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop ORAToBigquery;\nundeploy application ORAToBigquery;\ndrop application ORAToBigquery cascade;\nCREATE APPLICATION ORAToBigquery RECOVERY 5 SECOND INTERVAL;\nCREATE OR REPLACE SOURCE Rac11g USING OracleReader  ( \n  SupportPDB: false,\n  SendBeforeImage: true,\n  ReaderType: 'LogMiner',\n  CommittedTransactions: false,\n  FetchSize: 1,\n  Password: 'manager',\n  DDLTracking: 'true',\n  StartTimestamp: 'null',\n  OutboundServerProcessName: 'WebActionXStream',\n  OnlineCatalog: true,\n  ConnectionURL: '192.168.33.10:1521/XE',\n  SkipOpenTransactions: false,\n  Compression: false,\n  QueueSize: 40000,\n  RedoLogfiles: 'null',\n  Tables: 'SYSTEM.POSTABLE',\n  Username: 'ravi',\n  FilterTransactionBoundaries: true,\n  adapterName: 'OracleReader',\n  XstreamTimeOut: 600,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=60, maxRetries=3'\n ) \nOUTPUT TO DataStream;\nCREATE OR REPLACE TARGET Target1 USING SysOut ( \n  name: \"dstream\"\n ) \nINPUT FROM DataStream;\nCREATE OR REPLACE TARGET Target2 using BigqueryWriter(\n  BQServiceAccountConfigurationPath:\"/Users/ravipathak/Downloads/big-querytest-1963ae421e90.json\",\n  projectId:\"big-querytest\",\n  Tables: \"SYSTEM.GGAUTHORIZATIONS,testing1.ggauthorisation\",\n  parallelismCount: 2,\n  BatchPolicy: \"eventCount:100000,Interval:0\")\nINPUT FROM DataStream;\nEND APPLICATION ORAToBigquery;\ndeploy application ORAToBigquery;\nstart ORAToBigquery;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use PosTester;\nDROP CACHE HourlyAveLookup;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet\n(\n  ConnectionURL: '@CONNECTION_URL@',\n  CommittedTransactions: true,\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OJet',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  Username: '@SOURCE_USER@'\n)OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)\nSELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[0]) != '2';\n\nCREATE TYPE LogType(\nNum_col String key,\nChar_col String,\nVarchar2_col String,\nlong_col String,\nTable String,\nOperation String\n);\n\nCREATE WINDOW CDCWindow\nOVER @STREAM@\nKEEP 1 ROWS;\n\nCREATE WACTIONSTORE CDCWS CONTEXT of LogType\nEVENT TYPES ( LogType )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ ToWactionStore\nINSERT INTO CDCWS\nSELECT * FROM CDCWindow\nLINK SOURCE EVENT;\n\nCREATE TARGET @SOURCE_NAME@_SYS USING SysOut (\n  name: '@SOURCE_NAME@_SYS' )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@;\nCREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.DatabaseReader (\n  FetchSize: 1,\n  ConnectionURL: '@ORACLE-URL@',\n  Tables: '@SOURCE-TABLES@',\n  Username: '@ORACLE-USERNAME@',\n  Password: '@ORACLE-PASSWORD@'\n) OUTPUT TO @APP_NAME@_Stream;\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt USING SnowflakeWriter\n\n(\n  ConnectionURL:'@SNOWFLAKE-URL@',\n  username:'@SNOWFLAKE-USERNAME@',\n  appendOnly:'false',\n  Tables:'@TARGET-TABLES@',\n  uploadpolicy:'eventcount:3,interval:10s',\n  externalStageType:'local'\n)\nINPUT FROM @APP_NAME@_Stream;\n\n\nEND APPLICATION @APP_NAME@;\nDEPLOY APPLICATION @APP_NAME@;\nSTART APPLICATION @APP_NAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING YugabyteReader  (\n ReplicationSlotName: 'slotname',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.postgrestopostgres_src',\n  ExcludedTables:'public.postgres_2000target'\n )\nOUTPUT TO @STREAM@ ;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace stripe cascade force;\ncreate namespace stripe;\nuse stripe;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@ RECOVERY 30 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;\n\nCREATE FLOW @AppName@_SourceFlow;\n\nCREATE SOURCE @srcName@ USING StripeReader ( \n PollingInterval: '5m', \n  AccountId: '', \n  ApiKey: '@srcurl@', \n  StartPosition: '%=-1', \n  ThreadPoolCount: '10', \n  ConnectionPoolSize: '20', \n  RefreshToken: '', \n  useConnectionProfile: false,\n  Mode: 'Automated', \n  IncrementalLoadMarker: 'Created', \n  ApiKey_encrypted: 'false', \n  ConnectedAccount: 'false', \n  ClientSecret: '', \n  ClientId: '', \n  Tables: 'Charges', \n  AuthMode: 'ApiKey', \n  MigrateSchema: true ) \n  OUTPUT TO @outstreamname@;\n\nEND FLOW @AppName@_SourceFlow;\n\nCREATE TARGET Tgtstripebigquerysanity USING Global.BigQueryWriter ( \n  projectId: '@projectId@',\n  batchPolicy: 'eventcount:10000,interval:2', \n  streamingUpload: 'true', \n  Mode: 'MERGE', \n  CDDLOptions: '{\\\"CreateTable\\\":{\\\"action\\\":\\\"IgnoreIfExists\\\",\\\"options\\\":[{\\\"CreateSchema\\\":{\\\"action\\\":\\\"IgnoreIfExists\\\"}}]}}', \n  ServiceAccountKey: 'UploadedFiles/google-gcs-test.json', \n  Tables: '%,rishi.%' ) \nINPUT FROM @outstreamname@;\n\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application MySQLToSQLServer;\nundeploy application MySQLToSQLServer;\ndrop application MySQLToSQLServer cascade;\n\nCREATE APPLICATION MySQLToSQLServer recovery 1 second interval;\n\ncreate source Src1ReadFromMySQL USING MySQLReader (\nUsername: 'root',\n  Password: 'w@ct10n',\n  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',\n--Tables: 'waction.mytable1;waction.mytable2;qatest.mysqlmarker',\nTables: 'waction.Parent%;waction.Child%;',\n--Tables: 'waction.mytable%',\nBiDirectionalMarkerTable: 'waction.mysqlmarker',\ncompression: 'True',\nsendBeforeImage:True\n) OUTPUT TO App1Stream;\n\n\nCREATE  TARGET Src1ReadFromMySQL_Out USING FileWriter  (\n  filename: 'Src1ReadFromMySQL_Out.log',\nflushpolicy: 'eventcount:1',\nrolloverpolicy: 'eventcount:10000'\n )\nFORMAT USING JSONFORMATTER  (\n )\nINPUT FROM App1Stream;\n\n\nCREATE TARGET WriteToMSSQL1 USING DatabaseWriter(\nConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',\nUsername:'qatest',\nPassword:'w3b@ct10n',\nBatchPolicy:'EventCount:8,Interval:5',\nCommitPolicy:'EventCount:10,Interval:5',\nBiDirectionalMarkerTable: 'qatest.mysqlmarker',\n--Tables: 'waction.mytable1,qatest.mytable1;'\nTables: 'waction.Parent%,qatest.%;waction.Child%,qatest.%;'\n--Tables: 'waction.mytable%,qatest.%'\n)\nINPUT FROM App1Stream;\n\n\n--MSSQl tp MySQL\n\nCreate Source SrcReadFromMSSQL\nUsing MSSqlReader\n(\nUsername:'qatest',\nPassword:'w3b@ct10n',\nDatabaseName:'qatest',\nConnectionURL:'localhost:1433',\n--Tables:'qatest.mytable1;qatest.mytable2;qatest.mysqlmarker;qatest.mysqlmarker2',\nTables:'qatest.Parent%;qatest.Child%',\n--Tables:'qatest.mytable%',\nBiDirectionalMarkerTable: 'qatest.mysqlmarker',\nTransactionSupport: 'true',\nFilterTransactionBoundaries: true,\nCompression: 'True',\nConnectionPoolSize:1\n)\nOutput To App2Stream;\n\n\nCREATE  TARGET SrcReadFromMSSQL_Out USING FileWriter  (\n  filename: 'SrcReadFromMSSQL_Out.log',\nflushpolicy: 'eventcount:1',\nrolloverpolicy: 'eventcount:10000'\n )\nFORMAT USING JSONFORMATTER  (\n )\nINPUT FROM App2Stream;\n\n\nCREATE TARGET WriteToMySQL USING DatabaseWriter(\nConnectionURL:'jdbc:mysql://localhost:3306/waction',\nUsername:'root',\nPassword:'w@ct10n',\nBatchPolicy:'EventCount:10,Interval:10',\nCommitPolicy: 'EventCount:15,Interval:12',\nBiDirectionalMarkerTable: 'waction.mysqlmarker',\n--Tables: 'qatest.mytable1,waction.mytable1;qatest.mytable2,waction.mytable3;'\nTables: 'qatest.Parent_1,waction.Parent_1;qatest.Parent_2,waction.Parent_2;qatest.Child_1,waction.Child_1;qatest.Child_2,waction.Child_2;'\n--Tables: 'qatest.mytable%,waction.%'\n)\nINPUT FROM App2Stream;\n\n/*\nCREATE OR REPLACE TARGET MSSQLDDLFileOut USING FileWriter  (\n  rolloverpolicy: 'EventCount:10000,Interval:30s',\n  adapterName: 'FileWriter',\n  filename: 'MSSQL.txt'\n )\nFORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true',\n  jsonobjectdelimiter: '\\n'\n )\nINPUT FROM App2Stream;\n\n*/\n\nEND APPLICATION MySQLToSQLServer;\ndeploy application MySQLToSQLServer;\nstart application MySQLToSQLServer;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION SourceFraudApp;\n\nCREATE SOURCE FraudCsvDataSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:no,\n  wildcard:'fraudPosData.csv',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO FraudCsvStream;\n\nCREATE TARGET FraudSourceDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceFraudAppData') input from FraudCsvStream;\n\nCREATE SOURCE FraudZipLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'USAddresses.txt',\n  header: no,\n  columndelimiter: '\t',\n  positionByEOF:false\n) OUTPUT TO FraudCacheSource1;\n\nCREATE TARGET FraudCacheDump1 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceFraudCacheData1') input from FraudCacheSource1;\n\nCREATE SOURCE FraudNameLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'MerchantNames.csv',\n  header: no,\n  columndelimiter: ',',\n  positionByEOF:false\n) OUTPUT TO FraudCacheSource2;\n\nCREATE TARGET FraudCacheDump2 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceFraudCacheData2') input from FraudCacheSource2;\n\nCREATE SOURCE FraudCustomerLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'customerdetails.csv',\n  header: no,\n  columndelimiter: ',',\n  positionByEOF:false\n) OUTPUT TO FraudCacheSource3;\n\nCREATE TARGET FraudCacheDump3 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceFraudCacheData3') input from FraudCacheSource3;\n\n\nEND APPLICATION SourceFraudApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP TQLwithinTqlApp;\nUNDEPLOY APPLICATION TQLwithinTqlApp;\nDROP APPLICATION TQLwithinTqlApp CASCADE;\n\nCREATE APPLICATION TQLwithinTqlApp;\n\nCREATE source CsvDataSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n\n  wildcard:'banks.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\n\nEND APPLICATION TQLwithinTqlApp;\nDEPLOY APPLICATION TQLwithinTqlApp;\nSTART TQLwithinTqlApp;\n\n@@FEATURE-DIR@/tql/TQLTobeCalled.tql"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ (\n  positionbyeof: false\n)\nPARSE USING DSVParser ()\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()\nFORMAT USING ParquetFormatter (\n  schemaFileName: 'parquetSchema'\n)\nINPUT FROM @APPNAME@_Stream;\n\nEND APPLICATION @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@ Recovery 5 second interval;\n\ncreate stream @APPNAME@_UserdataStream of Global.WAEvent;\n\ncreate type @APPNAME@_Order_type(\nid int,\norder_id int,\nzipcode int,\ncategory String,\ntablename string\n);\n\nCREATE OR REPLACE SOURCE @APPNAME@Postgres_Src1 USING PostgreSQLReader\n(\n  ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: true,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.order_%'\n)\nOUTPUT TO @APPNAME@_OrdersStream;\n\nCREATE OR REPLACE SOURCE @APPNAME@Postgres_Src2 USING PostgreSQLReader\n(\n  ReplicationSlotName: 'test_slot_2',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: true,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.second_order_%'\n)\nOUTPUT TO @APPNAME@_OrdersStream2;\n\nCREATE OR REPLACE SOURCE @APPNAME@Postgres_Src3 USING PostgreSQLReader\n(\n  ReplicationSlotName: 'test_slot_3',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: true,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --startLSN:'758199500005967269518578814280000',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.third_order_%'\n)\nOUTPUT TO @APPNAME@_OrdersStream3;\n\nCREATE OR REPLACE SOURCE @APPNAME@Postgres_Src4 USING PostgreSQLReader\n(\n  ReplicationSlotName: 'test_slot_4',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: true,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --startLSN:'758199500005967269518578814280000',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.fourth_order_%'\n)\nOUTPUT TO @APPNAME@_OrdersStream4;\n\n\nCreate CQ @APPNAME@_CQUser\ninsert into @APPNAME@_UserdataStream\nselect \nputuserdata (data,'Fileowner','FIRST_ORDER') from @APPNAME@_OrdersStream data;\n\n\nCreate CQ @APPNAME@_CQUser2\ninsert into @APPNAME@_UserdataStream\nselect \nputuserdata (data2,'Fileowner','SECOND_ORDER') from @APPNAME@_OrdersStream2 data2;\n\n\nCreate CQ @APPNAME@_CQUser3\ninsert into @APPNAME@_UserdataStream\nselect \nputuserdata (data3,'Fileowner','THIRD_ORDER') from @APPNAME@_OrdersStream3 data3;\n\n\nCreate CQ @APPNAME@_CQUser4\ninsert into @APPNAME@_UserdataStream\nselect \nputuserdata (data4,'Fileowner','FOURTH_ORDER') from @APPNAME@_OrdersStream4 data4;\n\ncreate stream @APPNAME@_OrderTypedStream1 of @APPNAME@_Order_type;\ncreate stream @APPNAME@_OrderTypedStream2 of @APPNAME@_Order_type;\ncreate stream @APPNAME@_OrderTypedStream3 of @APPNAME@_Order_type;\ncreate stream @APPNAME@_OrderTypedStream4 of @APPNAME@_Order_type;\n\nCREATE CQ @APPNAME@_fin_cq\nINSERT INTO @APPNAME@_OrderTypedStream1\nSELECT to_int(data[0]),\nto_int(data[1]),\nto_int(data[2]),\nUSERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),\nMETA(@APPNAME@_UserdataStream,'TableName').toString()\nFROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FIRST_ORDER';\n\nCREATE CQ @APPNAME@_fin_cq2\nINSERT INTO @APPNAME@_OrderTypedStream2\nSELECT to_int(data[0]),\nto_int(data[1]),\nto_int(data[2]),\nUSERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),\nMETA(@APPNAME@_UserdataStream,'TableName').toString()\nFROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'SECOND_ORDER';\n\nCREATE CQ @APPNAME@_fin_cq3\nINSERT INTO @APPNAME@_OrderTypedStream3\nSELECT to_int(data[0]),\nto_int(data[1]),\nto_int(data[2]),\nUSERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),\nMETA(@APPNAME@_UserdataStream,'TableName').toString()\nFROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'THIRD_ORDER';\n\nCREATE CQ @APPNAME@_fin_cq4\nINSERT INTO @APPNAME@_OrderTypedStream4\nSELECT to_int(data[0]),\nto_int(data[1]),\nto_int(data[2]),\nUSERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),\nMETA(@APPNAME@_UserdataStream,'TableName').toString()\nFROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FOURTH_ORDER';\n\n\ncreate Target @APPNAME@_ADLSGen1_tgt1 using ADLSGen1Writer(\n        filename:'event_data.csv',\n        directory:'%category%/%tablename%',\n        datalakestorename:'striimdlstest.azuredatalakestore.net',\n        clientid:'94195e09-651c-431e-8556-59343c99cc05',\n        authtokenendpoint:'https://login.microsoftonline.com/71bfeed5-1905-43da-a4a4-49d8490731da/oauth2/token',\n        clientkey:'Vt7Reaamli1DXpqa3kY1+VTzQuEQrvchs5PJ3VNVmfM=',\n  rolloverpolicy:'eventcount:8,interval:20s'\n)\nformat using DSVFormatter (\n    header:'true'\n)\ninput from @APPNAME@_OrderTypedStream1; \n\ncreate Target @APPNAME@_ADLSGen1_tgt2 using ADLSGen1Writer(\n        filename:'event_data.xml',\n        directory:'%category%/%tablename%',\n        datalakestorename:'striimdlstest.azuredatalakestore.net',\n        clientid:'94195e09-651c-431e-8556-59343c99cc05',\n        authtokenendpoint:'https://login.microsoftonline.com/71bfeed5-1905-43da-a4a4-49d8490731da/oauth2/token',\n        clientkey:'Vt7Reaamli1DXpqa3kY1+VTzQuEQrvchs5PJ3VNVmfM=',\n\trolloverpolicy:'eventcount:8,interval:20s'\n)\nformat using XMLFormatter (\n  elementtuple: 'Order_id:id:order_id:zipcode:category:text=tablename',\n  formatterName: 'XMLFormatter',\n  rowdelimiter: '\\n',\n  rootelement: 'document'\n)\ninput from @APPNAME@_OrderTypedStream2; \n\ncreate Target @APPNAME@_ADLSGen1_tgt3 using ADLSGen1Writer(\n        filename:'event_data.avro',\n        directory:'%category%/%tablename%',\n        datalakestorename:'striimdlstest.azuredatalakestore.net',\n        clientid:'94195e09-651c-431e-8556-59343c99cc05',\n        authtokenendpoint:'https://login.microsoftonline.com/71bfeed5-1905-43da-a4a4-49d8490731da/oauth2/token',\n        clientkey:'Vt7Reaamli1DXpqa3kY1+VTzQuEQrvchs5PJ3VNVmfM=',\n  rolloverpolicy:'eventcount:8,interval:20s'\n)\nformat using AvroFormatter (\n  formatAs: 'Default',\n  formatterName: 'AvroFormatter',\n  schemaFileName: '@SCHEMA-FILE@'\n)\ninput from @APPNAME@_OrderTypedStream3; \n\n\ncreate Target @APPNAME@_ADLSGen1_tgt4 using ADLSGen1Writer(\n        filename:'event_data.json',\n        directory:'%category%/%tablename%',\n        datalakestorename:'striimdlstest.azuredatalakestore.net',\n        clientid:'94195e09-651c-431e-8556-59343c99cc05',\n        authtokenendpoint:'https://login.microsoftonline.com/71bfeed5-1905-43da-a4a4-49d8490731da/oauth2/token',\n        clientkey:'Vt7Reaamli1DXpqa3kY1+VTzQuEQrvchs5PJ3VNVmfM=',\n  rolloverpolicy:'eventcount:8,interval:20s'\n)\nFORMAT USING JSONFormatter()\nINPUT FROM @APPNAME@_OrderTypedStream4;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION BQ;\nUNDEPLOY APPLICATION BQ;\nDROP APPLICATION BQ CASCADE;\nCREATE APPLICATION BQ RECOVERY 1 SECOND INTERVAL;\nCREATE SOURCE s USING FileReader (\n\tdirectory:'Product/IntegrationTests/TestData/',\n    WildCard:'posdata5L.csv',\n\tpositionByEOF:false\n\t)\nPARSE USING DSVParser (\n\theader:yes\n)OUTPUT TO ss;\n\nCREATE TYPE userType(\nCol1 String,\nCol2 String,\nCol3 String,\nCol4 String,\nCol5 String\n);\n\nCREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;\n\nCREATE CQ cq1\nINSERT INTO userDefinedTypedStream\nSELECT data[0],data[1],data[2], data[3],data[4]\nFROM ss;\n\nCREATE or replace TARGET TABLE1 USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'@TABLE1@.TABLE1',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:10000,Interval:10',\nStandardSQL:true\t\n) INPUT FROM userDefinedTypedStream;\n\nCREATE or replace TARGET TABLE2 USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'@TABLE2@.TABLE2',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:10000,Interval:10',\nStandardSQL:true\t\n) INPUT FROM userDefinedTypedStream;\n\nCREATE or replace TARGET TABLE3 USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'@TABLE3@.TABLE3',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:10000,Interval:10',\nStandardSQL:true\t\n) INPUT FROM userDefinedTypedStream;\n\nCREATE or replace TARGET TABLE4 USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'@TABLE4@.TABLE4',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:10000,Interval:10',\nStandardSQL:true\t\n) INPUT FROM userDefinedTypedStream;\n\nCREATE or replace TARGET TABLE5 USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'@TABLE5@.TABLE5',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:10000,Interval:10',\nStandardSQL:true\t\n) INPUT FROM userDefinedTypedStream;\n\nCREATE or replace TARGET TABLE6 USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'@TABLE6@.TABLE6',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:10000,Interval:10',\nStandardSQL:true\t\n) INPUT FROM userDefinedTypedStream;\n\nCREATE or replace TARGET TABLE7 USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'@TABLE7@.TABLE7',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:10000,Interval:10',\nStandardSQL:true\t\n) INPUT FROM userDefinedTypedStream;\n\nCREATE or replace TARGET TABLE8 USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'@TABLE8@.TABLE8',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:10000,Interval:10',\nStandardSQL:true\t\n) INPUT FROM userDefinedTypedStream;\n\nCREATE or replace TARGET TABLE9 USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'@TABLE9@.TABLE9',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:10000,Interval:10',\nStandardSQL:true\t\n) INPUT FROM userDefinedTypedStream;\n\nCREATE or replace TARGET TABLE10 USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'@TABLE10@.TABLE10',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:10000,Interval:10',\nStandardSQL:true\t\n) INPUT FROM userDefinedTypedStream;\n\nEND APPLICATION BQ;\nDEPLOY APPLICATION BQ;\nstart application BQ;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@;\nCREATE  SOURCE @SourceName@ USING MSSqlReader  ( \n  Username: '@UserName@',\n  Password: '@Password@',\n  DatabaseName: 'qatest',\n  ConnectionURL: '@SourceConnectionURL@',\n  Tables: 'qatest.@SourceTable@',\n  ConnectionPoolSize: 1,\n  StartPosition: 'EOF',\n  ReplicationSlotName: 'null'\n ) \nOUTPUT TO @SRCINPUTSTREAM@;\ncreate Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( \n  ConnectionURL: '@TargetConnectionURL@',\n  Username: '@UserName@',\n  Password: '@Password@',\n  BatchPolicy: 'EventCount:1,Interval:1',\n  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n ) \nINPUT FROM @SRCINPUTSTREAM@;\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@ RECOVERY 1 SECOND INTERVAL;\nCREATE SOURCE @srcName@ USING OracleReader (\n Username: '@srcusername@',\n  Password: '@srcpassword@',\n  ConnectionURL: '@srcurl@',\n  Tables: '@srcschema@.@srctable@'\n)\nOUTPUT TO @outstreamname@;\n\nCREATE TARGET @tgtName@ USING DatabaseWriter (\n  CheckPointTable: 'CHKPOINT', \n  ReplicationSlotName:'test_slot',\n  Username:'@tgtusername@',\n  Password:'@tgtpassword@',\n  ConnectionURL:'@tgturl@',\n  adapterName:'PostgreSQLReader',\n  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',\n  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@'\n)\nINPUT FROM @instreamname@;\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use RetailTester;\nalter application RetailApp;\n\nCREATE FLOW RetailSourceFlow;\n\n-- RetailDataSource is the primary data source for this application.\n--\n-- ParseOrderData discards the fields not needed by this application and puts the\n-- data into the appropriate Java types.\n--\n-- ParseOrderData outputs to RetailOrders stream, the start point of the\n-- RetailProductFlow and RetailStoreFlow flows.\n\nCREATE SOURCE RetailDataSource USING CSVReader (\n  directory:'Samples/Customer/RetailApp/appData',\n  header:Yes,\n  wildcard:'retaildata2M.csv',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO Orders;\n\n-- A stream's type must be declared before the stream, and a CQ's\n-- output stream must be defined before the CQ. Hence type-stream-CQ\n-- sequences like the following are very common.\n\n-- output for ParseOrderData\nCREATE TYPE OrderType(\n  storeId      String,\n  orderId      String,\n  sku          String,\n  orderAmount  double,\n  dateTime     DateTime,\n  hourValue    int,\n  state        String,\n  city         String,\n  zip          String\n);\nCREATE STREAM RetailOrders Of OrderType;\n\nCREATE CQ ParseOrderData\nINSERT INTO RetailOrders\nSELECT  data[0],\n        data[6],\n        data[7],\n        TO_DOUBLE(SRIGHT(data[8],1)),\n        TO_DATE(data[9],'yyyyMMddHHmmss'),\n        DHOURS(TO_DATE(data[9],'yyyyMMddHHmmss')),\n        data[3],\n        data[2],\n        data[4]\nFROM Orders;\n\nEND FLOW RetailSourceFlow;\n\nend application RetailApp;\n\nalter application RetailApp recompile;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 2\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS\n--\n\nSTOP Recov2Tester.RecovTest2;\nUNDEPLOY APPLICATION Recov2Tester.RecovTest2;\nDROP APPLICATION Recov2Tester.RecovTest2 CASCADE;\nCREATE APPLICATION RecovTest2 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE WactionType (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionType;\n\nCREATE CQ CsvToWaction\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND APPLICATION RecovTest2;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @appName@;\nUNDEPLOY application @appName@;\nDROP application @appName@ cascade;\n\nCREATE APPLICATION @appName@ @recovery@;\n\nCREATE SOURCE @appName@_MongoDBReader USING MongoDBReader\n(\n  QuiesceOnILCompletion:'true',\n  collections:'',\n  userName:'',\n  password:'',\n  connectionUrl:'',\n  mode:''\n)\nOUTPUT TO @appName@_MOut;\n\nCREATE CQ @appName@_MCQ1\nINSERT INTO @appName@_MCQOut1\nSELECT\ndata.get('_id').textValue() as _id,\ndata.get('index').toString() as index,\ndata.get('isActive').toString() as isActive,\ndata.get('age').toString() as age,\ndata.get('balance').toString() as balance,\ndata.get('name').textValue() as name,\ndata.get('gender').textValue() as gender,\ndata.get('company').textValue() as company,\ndata.get('email').textValue() as email,\ndata.get('phone').textValue() as phone,\ndata.get('registered').textValue() as registered,\ndata.get('latitude').toString() as latitude,\ndata.get('longitude').toString() as longitude\nFROM @appName@_MOut m;;\n\nCREATE TARGET @appName@_AzureEventHubWriter USING AzureEventHubWriter\n(\n  SASKey:'',\n  EventHubName:'',\n  EventHubNamespace:'',\n  SASPolicyName:'',\n  BatchPolicy:'Size:1000000,Interval:10s',\n  ConsumerGroup:'default'\n)\nFORMAT USING JSONFormatter  (\n )\nINPUT FROM @appName@_MCQOut1;\n\nEND APPLICATION @appName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 27 with two sources, two jumping time windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jt1W -> CQ1 -> WS\n--   S2 -> Jt2W -> CQ2 -> WS\n--\n\nSTOP Recov27Tester.RecovTest27;\nUNDEPLOY APPLICATION Recov27Tester.RecovTest27;\nDROP APPLICATION Recov27Tester.RecovTest27 CASCADE;\nCREATE APPLICATION RecovTest27 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream2;\n\nCREATE JUMPING WINDOW DataStream1Second\nOVER DataStream1 KEEP WITHIN 1 SECOND;\n\nCREATE JUMPING WINDOW DataStream2Second\nOVER DataStream2 KEEP WITHIN 2 SECOND;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data1ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream1Second p;\n\nCREATE CQ Data2ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream2Second p;\n\nEND APPLICATION RecovTest27;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 33 with two sources, two sliding time windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> St1W/p -> CQ1 -> WS\n-- S2 -> St2W/p -> CQ2 -> WS\n--\n\nSTOP Recov33Tester.RecovTest33;\nUNDEPLOY APPLICATION Recov33Tester.RecovTest33;\nDROP APPLICATION Recov33Tester.RecovTest33 CASCADE;\nCREATE APPLICATION RecovTest33 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP WITHIN 1 SECOND\nPARTITION BY merchantId;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP WITHIN 2 SECOND\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1\nGROUP BY merchantId;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2\nGROUP BY merchantId;\n\nEND APPLICATION RecovTest33;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create Target @TARGET_NAME@ using FileWriter(\n  filename:'CompressedMerchant.gz',\n  directory:'/logs/',\n  rolloverpolicy:'EventCount:10000,Interval:30s'\n)\nformat using DSVFormatter (\n\n)\ninput from @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\nDROP EXCEPTIONSTORE @APP_NAME@_EXCEPTIONSTORE;\n\nCREATE APPLICATION @APP_NAME@ @APP_PROPERTY@ USE EXCEPTIONSTORE;\n\nCREATE SOURCE @APP_NAME@_Source Using @SOURCE_ADAPTER@ (\n\n) OUTPUT TO @APP_NAME@DataStream;\n\n\nCREATE TARGET @TARGETNAME@ USING @TARGET_ADAPTER@ (\n\n) INPUT FROM @APP_NAME@DataStream;\n\nEND APPLICATION @APP_NAME@;\n\nDEPLOY APPLICATION @APP_NAME@ IN DEFAULT;\nSTART APPLICATION @APP_NAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:100,Interval:0',\n  CommitPolicy: 'EventCount:100,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  --Password_encrypted: false\n ) \nINPUT FROM @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop ROLLUPMON_CDC;\nundeploy application ROLLUPMON_CDC;\nalter application ROLLUPMON_CDC;\nCREATE or replace FLOW ROLLUPMON_CDC_flow;\nCreate or replace Source ROLLUPMON_CDC_Oraclesrc Using oraclereader(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',\n Tables:'QATEST.ROLLUPMON_TABLE1;QATEST.ROLLUPMON_TABLE2;QATEST.ROLLUPMON_TABLE3;QATEST.ROLLUPMON_TABLE4;QATEST.ROLLUPMON_TABLE5',\n Fetchsize:1000,\n connectionRetryPolicy:'maxRetries=4',\n _h_fetchexactrowcount: 'true'\n)\nOutput To ROLLUPMON_CDC_OrcStrm;\nEND FLOW ROLLUPMON_CDC_flow;\nalter application ROLLUPMON_CDC recompile;\nDEPLOY APPLICATION ROLLUPMON_CDC;\nstart application ROLLUPMON_CDC;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 6 with Jumping window and partitioned on two node cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> KafkaStream -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS\n--\n\nSTOP APPLICATION KStreamN2S2CR6Tester.KStreamN2S2CRTest6;\nUNDEPLOY APPLICATION KStreamN2S2CR6Tester.KStreamN2S2CRTest6;\nDROP APPLICATION KStreamN2S2CR6Tester.KStreamN2S2CRTest6 CASCADE;\nDROP USER KStreamN2S2CR6Tester;\nDROP NAMESPACE KStreamN2S2CR6Tester CASCADE;\nCREATE USER KStreamN2S2CR6Tester IDENTIFIED BY KStreamN2S2CR6Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR6Tester;\nCONNECT KStreamN2S2CR6Tester KStreamN2S2CR6Tester;\n\nCREATE APPLICATION KStreamN2S2CRTest6 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionKStreamN2S2CRTest6;\n\nCREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');\nCREATE STREAM KafkaCsvStream using KafkaProps;\n\nCREATE SOURCE CsvSourceKStreamN2S2CRTest6 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nEND FLOW DataAcquisitionKStreamN2S2CRTest6;\n\nCREATE FLOW DataProcessingKStreamN2S2CRTest6;\n\nCREATE TYPE CsvDataTypeKStreamN2S2CRTest6 (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream OF CsvDataTypeKStreamN2S2CRTest6 PARTITION BY merchantId;\n\nCREATE CQ CsvToDataKStreamN2S2CRTest6\nINSERT INTO DataStream\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE WactionsKStreamN2S2CRTest6 CONTEXT OF CsvDataTypeKStreamN2S2CRTest6\nEVENT TYPES ( CsvDataTypeKStreamN2S2CRTest6 )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWactionKStreamN2S2CRTest6\nINSERT INTO WactionsKStreamN2S2CRTest6\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND FLOW DataProcessingKStreamN2S2CRTest6;\n\nEND APPLICATION KStreamN2S2CRTest6;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP BuiltInTester.AnomalyBoundTest;\nUNDEPLOY APPLICATION BuiltInTester.AnomalyBoundTest;\nDROP APPLICATION BuiltInTester.AnomalyBoundTest CASCADE;\n\nCREATE APPLICATION AnomalyBoundTest;\n\nCREATE OR REPLACE TYPE s1_Type  ( mString java.lang.String , \nmDouble java.lang.Double , \nmInt java.lang.Integer , \nmShort java.lang.Short , \nmLong java.lang.Long , \nmFloat java.lang.Float  \n );\n\nCREATE OR REPLACE STREAM s1 OF s1_Type;\n\nCREATE  WINDOW anomalyWin OVER s1 KEEP 4 ROWS;\n\nCREATE OR REPLACE TYPE S3_Type  ( val java.lang.Integer , \nupperInt java.lang.Double , \nlowerInt java.lang.Double , \nupperDouble java.lang.Double , \nlowerDouble java.lang.Double , \nupperFloat java.lang.Double , \nlowerFloat java.lang.Double , \nupperLong java.lang.Double , \nlowerLong java.lang.Double , \nupperShort java.lang.Double , \nlowerShort java.lang.Double  \n ) ;\n\nCREATE WACTIONSTORE anomalyWS  CONTEXT OF S3_Type\nEVENT TYPES(S3_Type )\n@PERSIST-TYPE@\n\n\nCREATE OR REPLACE STREAM S3 OF S3_Type;\n\nCREATE OR REPLACE CQ wsCQ \nINSERT INTO anomalyWS\nSELECT * FROM S3 s;\n;\n\nCREATE OR REPLACE CQ anomalyCQ \nINSERT INTO S3\nSELECT last(mInt) as val, \nround_double(distributionUpperBound(mInt), 2) as upperInt,  round_double(distributionLowerBound(mInt), 2) as lowerInt, round_double(distributionUpperBound(mDouble), 2) as upperDouble,  round_double(distributionLowerBound(mDouble), 2) as lowerDouble, round_double(distributionUpperBound(mFloat), 2) as upperFloat, round_double(distributionLowerBound(mFloat), 2) as lowerFloat, round_double(distributionUpperBound(mLong), 2) as upperLong, round_double(distributionLowerBound(mLong), 2) as lowerLong, round_double(distributionUpperBound(mShort), 2) as upperShort, round_double(distributionLowerBound(mShort), 2) as lowerShort \nFROM anomalyWin a\n;\n\nCREATE OR REPLACE TYPE hpstream3_Type  ( hpTime org.joda.time.DateTime , \nmDouble java.lang.Double , \nmInt java.lang.Integer , \nmShort java.lang.Short , \nmLong java.lang.Long  \n );\n\nCREATE OR REPLACE SOURCE housePowerCSV USING FileReader  ( \n  blocksize: 64,\n  positionbyeof: false,\n  rolloverstyle: 'Default',\n  includesubdirectories: false,\n  adapterName: 'FileReader',\n  directory: '@TEST-DATA-PATH@',\n  skipbom: true,\n  wildcard: 'anomalyBound.csv'\n ) \n PARSE USING DSVParser  ( \n  charset: 'UTF-8',\n  handler: 'com.webaction.proc.DSVParser_1_0',\n  linenumber: '-1',\n  nocolumndelimiter: false,\n  trimwhitespace: false,\n  columndelimiter: ',',\n  columndelimittill: '-1',\n  ignoremultiplerecordbegin: 'true',\n  ignorerowdelimiterinquote: false,\n  parserName: 'DSVParser',\n  separator: ':',\n  blockascompleterecord: false,\n  ignoreemptycolumn: false,\n  rowdelimiter: '\\n',\n  header: true,\n  headerlineno: 0,\n  quoteset: '\\\"',\n  trimquote: true\n ) \nOUTPUT TO housePowerStream ;\n\nCREATE OR REPLACE CQ HPparseCQ \nINSERT INTO s1\nSELECT to_string(data[0]) as mString, to_double(data[0]) as mDouble,  to_int(data[0]) as mInt, to_short(data[0]) as mShort, to_long(data[0]) as mLong, to_float(data[0]) as mFloat \nFROM housePowerStream s;\n\nCREATE OR REPLACE TYPE hpstream_Type  ( hpTime org.joda.time.DateTime , \nmDouble java.lang.Double , \nmInt java.lang.Integer , \nmShort java.lang.Short , \nmString java.lang.String  \n );\n\nCREATE OR REPLACE TYPE hqType  ( hpTime org.joda.time.DateTime , \nm1 java.lang.Double , \nm2 java.lang.Double , \nm3 java.lang.Double , \nm4 java.lang.Double  \n );\n\nCREATE OR REPLACE TYPE s2_Type  ( val java.lang.Integer , \nupper java.lang.Double , \nlower java.lang.Double , \nisAnomaly java.lang.Boolean  \n );\n\nCREATE OR REPLACE TYPE hpStream2_Type  ( hpTime org.joda.time.DateTime , \nm1 java.lang.Double , \nm2 java.lang.Double , \nm3 java.lang.Double , \nm4 java.lang.Double  \n );\n\nEND APPLICATION AnomalyBoundTest;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\n--drop exceptionstore admin.MySQL_To_MySQLApp_ExceptionStore;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ use exceptionstore;\n\ncreate source @SourceName@ using MySQLReader\n  (ConnectionURL: '@SourceConnectionURL@',\n   Username:'@UserName@',\n   Password:'@Password@',\n   Tables: '@SourceTableName@'\n)\noutput to @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\n   ConnectionURL:'@TargetConnectionURL@',\n   Username:'@UserName@',\n   Password:'@Password@',\n   BatchPolicy:'EventCount:1,Interval:0',\n   Tables: '@SourceTableName@,@TargetTableName@',\n   CommitPolicy: 'Interval:5'\n ) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate or replace cq @cq@\ninsert into @finalstream@\nselect exceptionType,action,appName,entityType,entityName,className,message,relatedActivity from @APPNAME@_ExceptionStore;\n\nCreate target @targetfile@ using filewriter (\nfilename:'@APPNAME@_file.log',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000,sequence:00'\n)\nformat using jsonFormatter()\ninput from @finalstream@;\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;\n\nstop @APPNAME@_ExpStore;\nundeploy application @APPNAME@_ExpStore;\ndrop application @APPNAME@_ExpStore cascade;\nCREATE APPLICATION @APPNAME@_ExpStore;\n\nCREATE TYPE @APPNAME@_ExpStore_CDCStreams_Type  (\n  evtlist java.util.List  \n );\n\nCREATE STREAM @APPNAME@_ExpStore_CDCStreams OF @APPNAME@_ExpStore_CDCStreams_Type;\n\nCREATE CQ @APPNAME@_ReadFromExpStore \nINSERT INTO @APPNAME@_ExpStore_CDCStreams\nselect to_waevent(s.relatedObjects) as evtlist from admin.@APPNAME@_ExceptionStore [jumping 5 second] s;\n\nCREATE STREAM @APPNAME@_ExpStore_CDCEventStream OF Global.WAEvent;\n\nCREATE CQ @APPNAME@_ExpStore_GetCDCEvent \nINSERT INTO @APPNAME@_ExpStore_CDCEventStream\nSELECT com.webaction.proc.events.WAEvent.makecopy(cdcevent) FROM @APPNAME@_ExpStore_CDCStreams a, iterator(a.evtlist) cdcevent;\n\nCREATE CQ @APPNAME@_ExpStore_JoinDataCQ\nINSERT INTO @APPNAME@_ExpStore_JoinedDataStream\nSELECT  TO_INT(f.data[0]),\n        TO_STRING(f.data[1])\n        from @APPNAME@_ExpStore_CDCEventStream f;\n        \nCREATE OR REPLACE TARGET @APPNAME@_ExpStore_WriteToFileAsJSON USING FileWriter  ( \n  filename: '@APPNAME@_file',\n  rolloveronddl: 'true',\n  flushpolicy: 'eventcount:1,interval:30',\n  adapterName: 'FileWriter',\n  directory: 'ExpStore_logs',\n  rolloverpolicy: 'eventcount:6,interval:30s'\n ) \nFORMAT USING JSONFormatter  ( \n  handler: 'com.webaction.proc.JSONFormatter',\n  formatterName: 'JSONFormatter',\n  jsonMemberDelimiter: '\\n',\n  jsonobjectdelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true'\n ) \nINPUT FROM @APPNAME@_ExpStore_JoinedDataStream;\n        \nCREATE TARGET @APPNAME@_ExpStore_dbtarget USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:1,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:1',\nTables:'@TargetTable@'\n) INPUT FROM @APPNAME@_ExpStore_JoinedDataStream;\n\nEND APPLICATION @APPNAME@_ExpStore;\n\ndeploy application @APPNAME@_ExpStore;\nstart @APPNAME@_ExpStore;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nCREATE APPLICATION ValidateFiles;\n\nCREATE OR REPLACE TYPE AggregatorOutput_Type  ( SUMsthingSUMtthing java.lang.Long );\n\nCREATE OR REPLACE STREAM AggregatorOutput OF AggregatorOutput_Type;\n\nCREATE OR REPLACE TARGET DiffFile USING FileWriter  (\n  filename: 'Difference',\n  rolloveronddl: 'true',\n  flushpolicy: 'eventcount:10000,interval:30',\n  adapterName: 'FileWriter',\n  directory: '@FEATURE-DIR@/logs',\n  rolloverpolicy: 'eventcount:10000,interval:30s'\n )\nFORMAT USING DSVFormatter  (   nullvalue: 'NULL',\n  standard: 'none',\n  handler: 'com.webaction.proc.DSVFormatter',\n  formatterName: 'DSVFormatter',\n  usequotes: 'false',\n  rowdelimiter: '\\n',\n  quotecharacter: '\\\"',\n  header: 'false',\n  columndelimiter: ','\n )\nINPUT FROM AggregatorOutput;\n\nCREATE OR REPLACE TYPE ewfew_Type  ( thing java.lang.Integer );\n\nCREATE OR REPLACE TYPE newTargetST_Type  ( thing java.lang.Integer );\n\nCREATE OR REPLACE STREAM newTargetST OF newTargetST_Type;\n\nCREATE OR REPLACE JUMPING WINDOW AllTargetSums OVER newTargetST KEEP WITHIN 15 SECOND;\n\nCREATE OR REPLACE SOURCE TargetSums USING FileReader  (\n  blocksize: 64,\n  positionbyeof: false,\n  rolloverstyle: 'Default',\n  includesubdirectories: false,\n  adapterName: 'FileReader',\n  directory: '@FEATURE-DIR@/logs',\n  skipbom: true,\n  wildcard: 'TargetResults.00'\n )\n PARSE USING DSVParser  (\n  charset: 'UTF-8',\n  handler: 'com.webaction.proc.DSVParser_1_0',\n  linenumber: '-1',\n  nocolumndelimiter: true,\n  trimwhitespace: false,\n  columndelimiter: ',',\n  columndelimittill: '-1',\n  ignoremultiplerecordbegin: 'true',\n  ignorerowdelimiterinquote: false,\n  parserName: 'DSVParser',\n  separator: ':',\n  blockascompleterecord: false,\n  ignoreemptycolumn: false,\n  rowdelimiter: '\\n',\n  header: false,\n  headerlineno: 0,\n  quoteset: '\\\"',\n  trimquote: true\n )\nOUTPUT TO compareTstream ;\n\nCREATE OR REPLACE CQ GetDemTargetValues\nINSERT INTO newTargetST\nSELECT TO_LONG(data[0]) as thing\nFROM compareTstream;\n\nCREATE OR REPLACE SOURCE SourceSums USING FileReader  (\n  blocksize: 64,\n  positionbyeof: false,\n  rolloverstyle: 'Default',\n  includesubdirectories: false,\n  adapterName: 'FileReader',\n  directory: '@FEATURE-DIR@/logs',\n  skipbom: true,\n  wildcard: 'SourceResults.00'\n )\n PARSE USING DSVParser  (\n  charset: 'UTF-8',\n  handler: 'com.webaction.proc.DSVParser_1_0',\n  linenumber: '-1',\n  nocolumndelimiter: true,\n  trimwhitespace: false,\n  columndelimiter: ',',\n  columndelimittill: '-1',\n  ignoremultiplerecordbegin: 'true',\n  ignorerowdelimiterinquote: false,\n  parserName: 'DSVParser',\n  separator: ':',\n  blockascompleterecord: false,\n  ignoreemptycolumn: false,\n  rowdelimiter: '\\n',\n  header: false,\n  headerlineno: 0,\n  quoteset: '\\\"',\n  trimquote: true\n )\nOUTPUT TO compareStream ;\n\nCREATE OR REPLACE TYPE newST_Type  ( thing java.lang.Integer );\n\nCREATE OR REPLACE STREAM newST OF newST_Type;\n\nCREATE OR REPLACE JUMPING WINDOW AllSourceSums OVER newST KEEP WITHIN 15 SECOND;\n\nCREATE OR REPLACE CQ Aggregator\nINSERT INTO AggregatorOutput\nSELECT SUM(s.thing) - SUM(t.thing)\nFROM AllSourceSums s, AllTargetSums t;\n\nCREATE TARGET output1 USING SysOut(name : SrcItem) input FROM compareStream;\nCREATE TARGET output2 USING SysOut(name : TrgItem) input FROM compareTstream;\nCREATE TARGET output3 USING SysOut(name : AggregatorItem) input FROM AggregatorOutput;\n\nCREATE OR REPLACE CQ GetdemValues\nINSERT INTO newST\nSELECT TO_LONG(data[0]) as thing\nFROM compareStream;\n\nEND APPLICATION ValidateFiles;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "INPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application JuniperLog;\ncreate source JuniperLogSource using FileReader (\n\tdirectory:'@TEST-DATA-PATH@',\n\tWildCard:'juniper-NSM*',\n\tcharset:'UTF-8',\n\tpositionByEOF:false\n) PARSE USING JuniperNSMLogParser (\n\ttrimwhitespace: yes\n)\nOUTPUT TO JuniperLogStream;\ncreate Target JuniperDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/junipernsm_log') input from JuniperLogStream;\nend application JuniperLog;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE PROPERTYVARIABLE RetryPolicy='timeOut=00,retryInterval=1,maxRetries=3';\nCREATE OR REPLACE PROPERTYVARIABLE KafkaBrokerAddress='localhost:9099';\n\nSTOP @WRITERAPPNAME@;\nUNDEPLOY APPLICATION @WRITERAPPNAME@;\nDROP APPLICATION @WRITERAPPNAME@ CASCADE;\n\nCREATE APPLICATION @WRITERAPPNAME@ @Recovery@;\ncreate flow AgentFlow;\nCREATE SOURCE S USING OracleReader\n(\n\tUsername: 'qatest',\n\tPassword: 'qatest',\n\tConnectionURL: 'dockerhost:1521:xe',\n\tTables: 'qatest.oracle_kw_test%',\n\tFetchSize: '1',\n\tconnectionRetryPolicy:'$RetryPolicy'\n)\nOUTPUT TO SS;\nend flow AgentFlow;\ncreate flow serverFlow;\n--create target sys_out using sysout (name:sysoutput) input from ss;\ncreate stream out_cq_select_SS_1 of global.waevent;\ncreate stream out_cq_select_SS_2 of global.waevent;\ncreate stream out_cq_select_SS_3 of global.waevent;\ncreate stream out_cq_select_SS_4 of global.waevent;\ncreate stream out_cq_select_SS_5 of global.waevent;\ncreate stream out_cq_select_SS_6 of global.waevent;\n\nCREATE OR REPLACE CQ cq_select_SS1 \nINSERT INTO out_cq_select_SS_1\nselect *\nfrom ss o\nwhere TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST1';\n\nCREATE OR REPLACE CQ cq_select_SS2 \nINSERT INTO out_cq_select_SS_2\nselect *\nfrom ss o\nwhere TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST2';\n\nCREATE OR REPLACE CQ cq_select_SS3 \nINSERT INTO out_cq_select_SS_3\nselect *\nfrom ss o\nwhere TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST3';\n\nCREATE OR REPLACE CQ cq_select_SS4 \nINSERT INTO out_cq_select_SS_4\nselect *\nfrom ss o\nwhere TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST4';\n\nCREATE OR REPLACE CQ cq_select_SS5 \nINSERT INTO out_cq_select_SS_5\nselect *\nfrom ss o\nwhere TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST5';\n\nCREATE OR REPLACE CQ cq_select_SS6 \nINSERT INTO out_cq_select_SS_6\nselect *\nfrom ss o\nwhere TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST6';\n\ncreate Target TARGET1 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_dsv_sync_CQ',\n--ParallelThreads:'2',\nPartitionKey:'@metadata(TableName)',\nMode:'sync',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING dsvFormatter ()\ninput from out_cq_select_SS_1;\n\ncreate Target TARGET2 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_json_sync_CQ',\n--ParallelThreads:'2',\nPartitionKey:'@metadata(TableName)',\nMode:'sync',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING jsonFormatter ()\ninput from out_cq_select_SS_2;\n\ncreate Target TARGET3 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_avro_sync_CQ',\n--ParallelThreads:'2',\nPartitionKey:'@metadata(TableName)',\nMode:'sync',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING avroFormatter (\nschemaFileName:'KafkaAvroTest_sync_CQ.avsc')\ninput from out_cq_select_SS_3;\n\ncreate Target TARGET4 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_dsv_Async_CQ',\n--ParallelThreads:'2',\nPartitionKey:'@metadata(ROWID)',\nMode:'Async',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING dsvFormatter ()\ninput from out_cq_select_SS_4;\n\ncreate Target TARGET5 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_json_Async_CQ',\n--ParallelThreads:'2',\nPartitionKey:'@metadata(ROWID)',\nMode:'Async',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING jsonFormatter ()\ninput from out_cq_select_SS_5;\n\ncreate Target TARGET6 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_avro_Async_CQ',\n--ParallelThreads:'2',\nPartitionKey:'@metadata(ROWID)',\nMode:'Async',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING avroFormatter (\nschemaFileName:'KafkaAvroTest_Async_CQ.avsc')\ninput from out_cq_select_SS_6;\n\ncreate Target TARGET7 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_dsv_sync',\n--ParallelThreads:'2',\nPartitionKey:'@metadata(TableName)',\nMode:'sync',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING dsvFormatter ()\ninput from ss;\n\ncreate Target TARGET8 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_json_sync',\n--ParallelThreads:'2',\nPartitionKey:'@metadata(TableName)',\nMode:'sync',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING jsonFormatter ()\ninput from ss;\n\ncreate Target TARGET9 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_avro_sync',\n--ParallelThreads:'2',\nPartitionKey:'@metadata(TableName)',\nMode:'sync',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING avroFormatter (\nschemaFileName:'KafkaAvroTest_sync.avsc')\ninput from ss;\n\ncreate Target TARGET10 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_dsv_Async',\n--ParallelThreads:'2',\nPartitionKey:'@metadata(TableName)',\nMode:'Async',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING dsvFormatter ()\ninput from ss;\n\ncreate Target TARGET11 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_json_Async',\n--ParallelThreads:'2',\nPartitionKey:'@metadata(TableName)',\nMode:'Async',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING jsonFormatter ()\ninput from ss;\n\ncreate Target TARGET12 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'@SOURCE@_avro_Async',\n--ParallelThreads:'2',\nPartitionKey:'@metadata(TableName)',\nMode:'Async',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING avroFormatter (\nschemaFileName:'KafkaAvroTest_Async.avsc')\ninput from ss;\n\nend flow serverFlow;\nend application @WRITERAPPNAME@;\ndeploy application @WRITERAPPNAME@;\nstart @WRITERAPPNAME@;\n\n\n\nstop application @READERAPPNAME@;\nundeploy application @READERAPPNAME@;\ndrop application @READERAPPNAME@ cascade;\nCREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE SOURCE @SOURCE@_DSV_sync_CQ USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_dsv_sync_CQ',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream1;\n\nCREATE TARGET kafkaDumpDSV USING FileWriter(\nname:kafkaOuputDSV,\nrolloverpolicy:'filesize:500M',\nfilename:'@READERAPPNAME@_@SOURCE@_dsv_sync_CQ')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaReaderStream1;\n\nCREATE SOURCE @SOURCE@_JSON_sync_CQ USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_json_sync_CQ',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO KafkaReaderStream2;\n\nCREATE SOURCE @SOURCE@_AVRO_sync_CQ USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_avro_sync_CQ',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'KafkaAvroTest_sync_CQ.avsc'\n)\nOUTPUT TO KafkaReaderStream3;\n\nCREATE SOURCE @SOURCE@_DSV_Async_CQ USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_dsv_Async_CQ',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream4;\n\nCREATE SOURCE @SOURCE@_JSON_Async_CQ USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_json_Async_CQ',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO KafkaReaderStream5;\n\nCREATE SOURCE @SOURCE@_AVRO_Async_CQ USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_avro_Async_CQ',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'KafkaAvroTest_Async_CQ.avsc'\n)\nOUTPUT TO KafkaReaderStream6;\n\nCREATE SOURCE @SOURCE@_DSV_sync USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_dsv_sync',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream7;\n\nCREATE TARGET kafkaDumpDSV_rawstream USING FileWriter(\nname:kafkaOuputDSV_rawstream,\nrolloverpolicy:'filesize:500M',\nfilename:'@READERAPPNAME@_@SOURCE@_dsv_sync')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaReaderStream7;\n\nCREATE SOURCE @SOURCE@_JSON_sync USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_json_sync',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO KafkaReaderStream8;\n\nCREATE SOURCE @SOURCE@_AVRO_sync USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_avro_sync',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'KafkaAvroTest_sync.avsc'\n)\nOUTPUT TO KafkaReaderStream9;\n\nCREATE SOURCE @SOURCE@_DSV_Async USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_dsv_Async',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream10;\n\nCREATE SOURCE @SOURCE@_JSON_Async USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_json_Async',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO KafkaReaderStream11;\n\nCREATE SOURCE @SOURCE@_AVRO_Async USING KafkaReader VERSION '2.1.0' (\n        brokerAddress:'$KafkaBrokerAddress',\n        Topic:'@SOURCE@_avro_Async',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'KafkaAvroTest_Async.avsc'\n)\nOUTPUT TO KafkaReaderStream12;\n\nend application @READERAPPNAME@;\ndeploy application @READERAPPNAME@;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION testApp;\nUNDEPLOY APPLICATION testApp;\nDROP APPLICATION testApp CASCADE;\n-- DROP EXCEPTIONSTORE testApp_exceptionstore;\n\nCREATE APPLICATION testApp WITH ENCRYPTION RECOVERY 10 SECOND INTERVAL USE EXCEPTIONSTORE;\n\nCREATE OR REPLACE SOURCE testApp_Source USING OracleReader  (\n  Username:'qatest',\n  Password:'qatest',\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\n  OnlineCatalog:true,\n  FetchSize:'1',\n  Tables: 'QATEST.srctb'\n  ) OUTPUT TO testApp_Stream  ;\n\nCREATE OR REPLACE TARGET testApp_Target USING BigQueryWriter  (\n  ColumnDelimiter: '|', \n  NullMarker: 'NULL', \n  projectId:'striimqa-214712',\n  Encoding: 'UTF-8', \n  BatchPolicy: 'eventCount:5,Interval:120',\n  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', \n  AllowQuotedNewLines: 'false', \n  adapterName: 'BigQueryWriter', \n  optimizedMerge: 'true', \n  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', \n  StandardSQL: 'true', \n  QuoteCharacter: '\\\"', \n  Tables: 'QATEST.srctb,.oratobqtgt',\n  Mode: 'MERGE',\n  StandardSQL: 'true',\n  _h_TransportOptions:'connectionTimeout=30s, readTimeout=12s',\n  QuoteCharacter: '\\\"'\n  ) INPUT FROM testApp_Stream;\n\nCREATE OR REPLACE TARGET testApp_SysOut USING Global.SysOut (name: 'wa') INPUT FROM testApp_Stream;\n\nEND APPLICATION testApp;\nDEPLOY APPLICATION testApp;\nSTART testApp;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @appName@;\nundeploy application @appName@;\ndrop application @appName@ cascade;\n\nCREATE OR REPLACE APPLICATION @appName@;\n\n-x-\n\nCREATE SOURCE @appName@_Source USING Global.MysqlReader (\n  Username: '@UserName@',\n  ConnectionURL: '@ConnectionURL@',\n  Password: '@Password@',\n  Tables: '@SourceTable@' )\nOUTPUT TO @appName@_st1 ;\n\n-x-\n\nCREATE TYPE @appName@_cache_output_Type (\n product_id java.lang.Integer,\n product_name java.lang.String,\n discount java.lang.Integer);\n\nCREATE TYPE @appName@_cache_type (\n p_id java.lang.Integer KEY,\n category java.lang.String,\n quantity java.lang.Integer,\n discount java.lang.Integer);\n\nCREATE OR REPLACE STREAM @appName@_cache_output_st OF @appName@_cache_output_Type;\n\nCREATE OR REPLACE CACHE @appName@_cache_comp USING DatabaseReader (\n  Query: 'select * from @LookUpTableForCache@',\n  username: '@UserName@',\n  FetchSize: 1,\n  ConnectionURL: '@ConnectionURL@',\n  password: '@Password@' )\nQUERY (\n  keytomap: 'p_id'\n  )\nOF  @appName@_cache_type;\n\nCREATE OR REPLACE CQ @appName@_cache_output_cq\nINSERT INTO @appName@_cache_output_st\nSELECT\n      p.data[0] as product_id,\n      p.data[1] as product_name,\n      pcache.discount as discount\n   FROM\n   @appName@_st1 p INNER JOIN @appName@_cache_comp pcache ON TO_INT(p.data[0]) = pcache.p_id;\n\nCREATE OR REPLACE TARGET @appName@_cacheTarget USING Global.DatabaseWriter (\n  CommitPolicy: 'EventCount:10,Interval:10',\n  BatchPolicy: 'EventCount:10,Interval:10',\n  Username: '@UserName@',\n  ConnectionURL: '@ConnectionURL@',\n  Password: '@Password@',\n  Tables: '@cache_target_table@' )\nINPUT FROM @appName@_cache_output_st;\n\n-x-\n\nCREATE TYPE @appName@_external_cache_type (\n p_id java.lang.Integer KEY,\n category java.lang.String,\n quantity java.lang.Integer,\n discount java.lang.Integer);\n\nCREATE EXTERNAL CACHE @appName@_external_cache_comp (\n  Columns: 'p_id,category,quantity,discount',\n  KeyToMap: 'p_id',\n  AdapterName: 'DatabaseReader',\n  Table: '@LookUpTableForExternalCache@',\n  Username: '@UserName@',\n  ConnectionURL: '@ConnectionURL@',\n  Password: '@Password@' )\nOF @appName@_external_cache_type;\n\nCREATE CQ @appName@_external_cache_output_cq\nINSERT INTO @appName@_external_cache_output_st\nSELECT p.data[0] as product_id,p.data[1] as product_name,ecp.quantity as quantity\n  FROM @appName@_st1  p inner join @appName@_external_cache_comp ecp\n  on TO_INT(p.data[0]) = ecp.p_id;\n\nCREATE OR REPLACE TARGET @appName@_externalCacheTarget USING Global.DatabaseWriter (\n  CommitPolicy: 'EventCount:10,Interval:10',\n  BatchPolicy: 'EventCount:10,Interval:10',\n  Username: '@UserName@',\n  ConnectionURL: '@ConnectionURL@',\n  Password: '@Password@',\n  Tables: '@externalCache_target_table@' )\nINPUT FROM @appName@_external_cache_output_st;\n\n-x-\n\nEND APPLICATION @appName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;\nCREATE OR REPLACE SOURCE @AppName@_Source USING FileReader (\ndirectory:'@dataDir@',\nwildcard:'data.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO @AppName@_rawstream;\n\nCREATE CQ @BuiltinFunc@CQ\nINSERT INTO @BuiltinFunc@Stream\nSELECT updateUserData(x, 'Last_Date', data[5], 'Country', data[10])\nFROM @AppName@_rawstream x;\n\nCREATE OR REPLACE CQ cq1\nINSERT INTO clearUserData_Stream\nSELECT\nclearUserData(s1)\nFROM @BuiltinFunc@Stream s1;\n\nCREATE OR REPLACE TARGET @AppName@_FileTarget USING Global.FileWriter ( \n  flushpolicy: 'EventCount:10000,Interval:30s', \n  adapterName: 'FileWriter', \n  directory: '@logs@',\n  filename: '@BuiltinFunc@_ClearData', \n  rolloverpolicy: 'EventCount:10000,Interval:30s' ) \nFORMAT USING Global.JSONFormatter  ( \n  handler: 'com.webaction.proc.JSONFormatter', \n  jsonMemberDelimiter: '\\n', \n  EventsAsArrayOfJsonObjects: 'true', \n  formatterName: 'JSONFormatter', \n  jsonobjectdelimiter: '\\n' ) \nINPUT FROM clearUserData_Stream;\n\nEnd application @AppName@;\nDeploy application @AppName@; \nStart application @AppName@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ recovery 5 second interval;\nCREATE  SOURCE @TableSourceName@ USING DatabaseReader  ( \n  ConnectionURL: '@SourceConnectionURL@',\n  Username: '@UserName@',\n  Password: '@Password@',\n  Tables: '@SourceTable@',\n  ReplicationSlotName: 'null'\n ) OUTPUT TO @SRCTableINPUTSTREAM@;\n\n CREATE  SOURCE @QuerySourceName@ USING DatabaseReader  ( \n  ConnectionURL: '@SourceConnectionURL@',\n  Username: '@UserName@',\n  Password: '@Password@',\n  Query: '@SourceQuery@',\n  ReplicationSlotName: 'null'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@;\n\nCREATE  TARGET @TabletargetName@ USING DatabaseWriter  ( \n  ConnectionURL: '@TargetConnectionURL@',\n  Username: '@UserName@',\n  Password: '@Password@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n ) \n INPUT FROM @SRCTableINPUTSTREAM@;\n\n CREATE  TARGET @QuerytargetName@ USING DatabaseWriter  ( \n  ConnectionURL: '@TargetConnectionURL@',\n  Username: '@UserName@',\n  Password: '@Password@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,qatest.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@;\nend application @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application AzureApp;\nundeploy application AzureApp;\ndrop application AzureApp cascade;\n\ncreate application AzureApp\nRECOVERY 10 second interval;\ncreate source CSVSource using FileReader (\n\tdirectory:'@DIR@',\n\tWildCard:'@WILDCARD@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  curr String,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       data[6],\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target BlobT using AzureBlobWriter(\n\taccountname:'@ACCNAME@',\n\taccountaccesskey:'@ACCKEY@',\n\tcontainername:'@CONT@',\n        blobname:'@BLOB@',\n\tuploadpolicy:'EventCount:100,interval:5s'\n)\nformat using XMLFormatter (\nrootelement:'document',\nelementtuple:'MerchantName:merchantId:text=merchantname'\n)\ninput from TypedCSVStream;\nend application AzureApp;\ndeploy application AzureApp in default;\nstart application AzureApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@ recovery 5 second Interval;\n\nCREATE SOURCE @srcName@ USING SalesForceReader ( \n  customObjects: false, \n  autoAuthTokenRenewal: 'true',\n  pollingInterval: '1 min', \n  sObjects: '@srcobject@', \n  useConnectionProfile: 'false',\n  consumerSecret: '@srcconsumersecret@',\n  consumerKey: '@srcconsumerkey@', \n  Username: '@srcusername@',\n  Password: '@srcpassword@',\n  mode: 'Automated',\n  connectionRetryPolicy: 'retryInterval=30, maxRetries=3', \n  securityToken: '@srcsecuritytoken@',\n  apiEndPoint: '@srcapiurl@',\n  MigrateSchema: true, \n  threadPoolSize: 5\n)\n\nOUTPUT TO @outstreamname@;\n\nCREATE TARGET @tgtName@ USING Global.DeltaLakeWriter (\n  personalAccessToken:'@tgtpassword@',\n  hostname:'@tgthostname@',\n  stageLocation:'/',\n  Mode:'MERGE',\n  AuthenticationType: 'PersonalAccessToken',\n  Tables:'@srcschema@,@tgtschema@.@tgttable@ COLUMNMAP()',\n  adapterName:'DeltaLakeWriter',\n  personalAccessToken_encrypted:'false',\n  optimizedMerge:'false',\n  uploadPolicy:'eventcount:1,interval:10s',\n  connectionUrl:'@tgturl@',\n  IgnorableExceptionCode:'TABLE_NOT_FOUND',\n  externalStageType:'DBFSROOT'\n)\nINPUT FROM @instreamname@;\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION EH;\nUNDEPLOY APPLICATION EH;\nDROP APPLICATION EH CASCADE;\nCREATE APPLICATION EH @Recovery@;\nCREATE SOURCE s USING FileReader (\n\tdirectory:'Product/IntegrationTests/TestData/',\n    WildCard:'posdata.csv',\n\tpositionByEOF:false\n\t)\nPARSE USING DSVParser (\n\theader:yes\n)OUTPUT TO ss;\n\nCREATE TYPE userType(\nCol1 String,\nCol2 String,\nCol3 String,\nCol4 String,\nCol5 String\n);\n\nCREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;\n\nCREATE CQ cq1\nINSERT INTO userDefinedTypedStream\nSELECT data[0],data[1],data[2], data[3],data[4]\nFROM ss;\n\ncreate Target t1 using AzureEventHubWriter (\n\tEventHubNamespace:'qatestnov9',\n\tEventHubName:'test_2_partitions',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tPartitionkey:'@metadata(RecordOffset)',\n\tParallelThreads:'2',\n\tConsumerGroup:'reader',\n\tE1P:'true',\n\tOperationTimeout:'500000',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using dsvFormatter()\ninput from ss;\n\ncreate Target t2 using AzureEventHubWriter (\n\tEventHubNamespace:'qatestnov9',\n\tEventHubName:'test_2_partitions',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tPartitionkey:'Col1',\n\tParallelThreads:'2',\n\tConsumerGroup:'reader',\n\tE1P:'true',\n\tOperationTimeout:'500000',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using dsvFormatter()\ninput from userDefinedTypedStream;\n\nEND APPLICATION EH;\nDEPLOY APPLICATION EH;\nstart application EH;\n\nSTOP APPLICATION ER;\nUNDEPLOY APPLICATION ER;\nDROP APPLICATION ER CASCADE;\nCREATE APPLICATION ER;\nCREATE SOURCE ER_S1 USING AzureEventHubReader (\n    EventHubNamespace:'qatestnov9',\n\tEventHubName:'test_2_partitions',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'Default',\n\tstartSeqNo:0\t\n\t)\nPARSE USING DSVParser (\n)OUTPUT TO ER_SS1;\nCREATE SOURCE ER_S2 USING AzureEventHubReader (\n    EventHubNamespace:'qatestnov9',\n\tEventHubName:'test_2_partitions',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'Default',\n\tstartSeqNo:0\t\n\t)\nPARSE USING DSVParser (\n)OUTPUT TO ER_SS2;\n\nCREATE TYPE CustType1(\nCol1 String,\nCol2 String,\nCol3 String,\nCol4 String,\nCol5 String,\nCol6 String,\nTopicName String,\nPartitionID String\n);\n\nCREATE TYPE CustType2(\nCol1 String,\nCol2 String,\nCol3 String,\nCol4 String,\nCol5 String,\nTopicName String,\nPartitionID String\n);\n\nCREATE Stream DSVReaderStream1 of CustType1;\nCREATE Stream DSVReaderStream2 of CustType2;\n\nCREATE CQ CustType_CQ1\nINSERT INTO DSVReaderStream1\nSELECT data[5],data[6],data[7],data[8],data[9],data[10],\nmetadata.get(\"TopicName\").toString() AS TopicName,\nmetadata.get(\"PartitionID\").toString() AS PartitionID\nFROM ER_SS1;\n\nCREATE CQ CustType_CQ2\nINSERT INTO DSVReaderStream2\nSELECT data[0],data[1],data[2],data[3],data[4],\nmetadata.get(\"TopicName\").toString() AS TopicName,\nmetadata.get(\"PartitionID\").toString() AS PartitionID\nFROM ER_SS2;\n\ncreate Target ER_t1 using FileWriter (\nfilename:'FT1_5L_DSV_RT',\ndirectory:'FEATURE-DIR/logs/',\nrolloverpolicy:'eventcount:5000000'\t\n)\nformat using dsvFormatter()\ninput from DSVReaderStream1;\n\ncreate Target ER_t2 using FileWriter (\nfilename:'FT2_5L_DSV_RT',\ndirectory:'FEATURE-DIR/logs/',\nflushpolicy:'eventcount:1',\nrolloverpolicy:'eventcount:5000000'\t\n)\nformat using dsvFormatter()\ninput from DSVReaderStream2;\nend application ER;\ndeploy application ER;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @AppName@;\n\nCREATE OR REPLACE SOURCE Filereader_Src USING FileReader  (\n   WildCard: 'posdata100.csv',\n  directory: '@SrcDir@',\n  positionbyeof: false)\n PARSE USING DSVParser  (\n )\nOUTPUT TO CsvStream ;\n\nCREATE OR REPLACE SOURCE initialLoad_Src USING Global.DatabaseReader (\n  QuiesceOnILCompletion: false,\n  Tables: '@SrcTableName@',\n  adapterName: 'DatabaseReader',\n  Password: '@Password@',\n  Username: '@UserName@',\n  ConnectionURL: '@Srcurl@',\n   FetchSize: 10000)\nOUTPUT TO ILStream;\n\nCreate Type CSVType (\n  companyid String,\n  merchantId String\n);\n\nCREATE STREAM CommonTypedStream OF CSVType;\n\n\nCREATE OR REPLACE  CQ CsvToPosData\nINSERT INTO CommonTypedStream\nSELECT\nTO_STRING(data[0]).replaceAll(\"COMPANY \", \"\"),\ndata[1]\nFROM CsvStream;\n\nCREATE CQ cq1\nINSERT INTO CommonTypedStream\nSELECT data[0],data[1]\nFROM ILStream;\n\n\nCREATE OR REPLACE TARGET Postgres_Trg USING Global.DatabaseWriter (\n  ConnectionURL: '@trgUrl@',\n  Username: '@trgUsrName@',\n  Tables: '@trgTable@',\n  Password: '@trgPswd@',\n  CommitPolicy: 'EventCount:10000,Interval:60',\n  adapterName: 'DatabaseWriter' )\nINPUT FROM CommonTypedStream;\n\nCREATE TARGET filewriter_tgt USING Global.FileWriter (\n directory:'@trgDir@',\n  filename: '@fileName@',\n  flushpolicy: 'EventCount:10000,Interval:30s',\n  rolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING Global.JSONFormatter  (\n  jsonMemberDelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true',\n  jsonobjectdelimiter: '\\n' )\nINPUT FROM CommonTypedStream;\n\nCREATE OR REPLACE TARGET BigQuery_Target USING Global.BigQueryWriter (\n  streamingUpload: 'false',\n  projectId: '@projectID@',\n  Tables: '@BQTableName@',\n  optimizedMerge: 'false',\n  ServiceAccountKey: '@ServiceAccountKey@',\n  BatchPolicy: 'EventCount:1000000,Interval:90',\n  Mode: 'APPENDONLY' )\nINPUT from CommonTypedStream;\n\nEND APPLICATION @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop Postgres_SQLDBWHEventTableApp;\nundeploy application Postgres_SQLDBWHEventTableApp;\ndrop application Postgres_SQLDBWHEventTableApp cascade;\nCREATE APPLICATION Postgres_SQLDBWHEventTableApp;\n\nCREATE OR REPLACE SOURCE Postgres_Src1 USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --startLSN:'758199500005967269518578814280000',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.postgres_src1',\n  ExcludedTables: ''\n ) \nOUTPUT TO data_stream1;\n\nCREATE OR REPLACE SOURCE Postgres_Src2 USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --startLSN:'758199500005967269518578814280000',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.postgres_src2',\n  ExcludedTables: ''\n ) \nOUTPUT TO data_stream2;\n\nCREATE OR REPLACE SOURCE Postgres_Src3 USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --startLSN:'758199500005967269518578814280000',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.postgres_src2',\n  ExcludedTables: ''\n ) \nOUTPUT TO data_stream3;\n\nCreate Type EventType (\nID int,\nNAME string,\nCOMPANY string\n);\n\nCREATE STREAM insertData1  of EventType;\nCREATE STREAM deleteData1 of EventType;\nCREATE STREAM joinData1 of EventType;\nCREATE STREAM joinData2 of EventType;\nCREATE STREAM deleteData2 of EventType;\nCREATE STREAM OutStream of EventType;\n\nCREATE CQ cq1 INSERT INTO insertData1  SELECT TO_INT(data[0]),data[1],data[2] FROM data_stream1;\n\nCREATE CQ cq2 INSERT INTO deleteData1 SELECT TO_INT(data[0]),data[1],data[2] FROM data_stream2;\n\nCREATE CQ cq3 INSERT INTO joinData1 SELECT TO_INT(data[0]),data[1],data[2] FROM data_stream3;\n\nCREATE JUMPING WINDOW DataWin1 OVER deleteData1 KEEP 1 ROWS;\n\nCREATE CQ TypedEventKeyCQ6 INSERT INTO deleteData2 SELECT * from DataWin1;\n\nCREATE JUMPING WINDOW DataWin2 OVER joinData1 KEEP 1 ROWS;\n\nCREATE CQ TypedEventKeyCQ5 INSERT INTO joinData2 SELECT * from DataWin2;\n\nCREATE EVENTTABLE ETABLE1 using STREAM ( NAME: 'insertData1 ' )\n--DELETE using STREAM ( NAME: 'deleteData1')\nQUERY (keytomap:\"ID\", persistPolicy: 'true') OF EventType;\n\nCREATE CQ cq4 INSERT INTO OutStream SELECT B.ID,B.NAME,B.COMPANY FROM joinData2 A, ETABLE1 B where A.ID=B.ID;\n\nCREATE TARGET EventTableFW USING FileWriter\n(filename:'BasicPostgres_SQLDBWHEventTableApp_RT.log',\n rolloverpolicy: 'EventCount:1000000')\nFORMAT USING DSVFormatter () INPUT FROM OutStream;\n\ncreate target Target_Azure using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'STRIIM',\n        password: 'W3b@ct10n',\n        AccountName: 'striimqatestdonotdelete',\n        accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables:'dbo.autotest01',\n        uploadpolicy:'eventcount:0,interval:0s'\n) INPUT FROM OutStream;\n\nEND APPLICATION Postgres_SQLDBWHEventTableApp;\ndeploy application Postgres_SQLDBWHEventTableApp in default;\nstart Postgres_SQLDBWHEventTableApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop Oracle_Oracle_IRLogWriterLogWriter;\nUndeploy application Oracle_Oracle_IRLogWriterLogWriter;\ndrop application Oracle_Oracle_IRLogWriterLogWriter cascade;\n\nCREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;\n\nCREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( \n \n  FetchSize: 5000,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n CheckColumn: 'striim.test01=t1',\n startPosition: '%=-1',\n  PollingInterval: '20sec'\n  )\n  OUTPUT TO data_stream;\ncreate target AzureSQLDWHTarget using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',\n\t\tuploadpolicy:'eventcount:0,interval:0s'\n) INPUT FROM data_stream;\n  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM data_stream;\n\nEND APPLICATION Oracle_IRLogWriter;\ndeploy application Oracle_IRLogWriter;\nstart Oracle_IRLogWriter;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCreate Source @SourceName@\n Using OracleReader\n(\n Username:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@SourceConnectURL@',\n Tables:'@SourceTable@',\n _h_scnRange: 1000,\n _h_eoffDelay: 10,\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop PatternMatching.CSV;\nundeploy application PatternMatching.CSV;\ndrop application PatternMatching.CSV cascade;\n\ncreate application CSV RECOVERY 5 SECOND INTERVAL;\n\ncreate source CSVSource using CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'ctest.csv',\n  columndelimiter:',',\n  positionByEOF:false\n)\nOUTPUT TO CsvStream;\n\nCREATE CQ ParseUserData\nINSERT INTO UserDataStream\nSELECT  TO_INT(data[0]) as UserId,\n\tTO_INT(data[1]) as temp1,\n        TO_DOUBLE(data[2]) as temp2,\n\tTO_STRING(data[3]) as temp3\nFROM CsvStream;\n\n-- scenario 1.1 check pattern alterations\nCREATE CQ TypeConversionCsvCQ1\nINSERT INTO TypedStream1\nSELECT UserId as typeduserid,\n\tA.temp1 as typedtemp1,\n\tB.temp2 as typedtemp2,\n\tC.temp3 as typedtemp3\nfrom UserDataStream\nMATCH_PATTERN A | B  | C\ndefine A = UserDataStream(temp1 = 20), B= UserDataStream(temp2 = 30.40), C= UserDataStream(temp3 = 'Bret')\nPARTITION BY UserId;\n\n-- scenario 1.2 check pattern permutation with partition by\nCREATE CQ TypeConversionCsvCQ2\nINSERT INTO TypedStream2\nSELECT UserId as typeduserid,\n\tA.temp1 as typedtemp1,\n\tB.temp2 as typedtemp2,\n\tC.temp3 as typedtemp3\nfrom UserDataStream\nMATCH_PATTERN A & B & C\ndefine A = UserDataStream(temp1 = 10), B= UserDataStream(temp2 = 20.30), C= UserDataStream(temp3 = 'zalak')\nPARTITION BY UserId;\n\n-- scenario 1.3 check pattern quantifire with partition by\nCREATE CQ TypeConversionCsvCQ3\nINSERT INTO TypedStream3\nSELECT UserId as typeduserid,\n\tA.temp1 as typedtemp1,\n\tB.temp2 as typedtemp2,\n\tC.temp3 as typedtemp3\nfrom UserDataStream\nMATCH_PATTERN A ? B ? C\ndefine A = UserDataStream(temp1 = 10), B= UserDataStream(temp2 = 20.30), C= UserDataStream(temp3 = 'zalak')\nPARTITION BY UserId;\n\n-- scenario 1.4 check pattern quantifire with grouping and partition by\nCREATE CQ TypeConversionCsvCQ4\nINSERT INTO TypedStream4\nSELECT UserId as typeduserid,\n\tA.temp1 as typedtemp1,\n\tB.temp2 as typedtemp2,\n\tC.temp3 as typedtemp3\nfrom UserDataStream\nMATCH_PATTERN A ( B ? C)\ndefine A = UserDataStream(temp1 = 10), B= UserDataStream(temp2 = 20.30), C= UserDataStream(temp3 = 'zalak')\nPARTITION BY UserId;\n\n-- scenario 1.5 check pattern overlapping and alteration with grouping and partition by\nCREATE CQ TypeConversionCsvCQ5\nINSERT INTO TypedStream5\nSELECT UserId as typeduserid,\n\tA.temp1 as typedtemp1,\n\tB.temp2 as typedtemp2,\n\tC.temp3 as typedtemp3\nfrom UserDataStream\nMATCH_PATTERN (A # B) | C\ndefine A = UserDataStream(temp1 = 30), B= UserDataStream(temp2 = 20.30), C= UserDataStream(temp3 = 'Bret')\nPARTITION BY UserId;\n\n-- scenario 1.6 check pattern quantifire with grouping and two partition by\nCREATE CQ TypeConversionCsvCQ6\nINSERT INTO TypedStream6\nSELECT UserId as typeduserid,\n\tA.temp1 as typedtemp1,\n\tB.temp2 as typedtemp2,\n\tC.temp3 as typedtemp3\nfrom UserDataStream\nMATCH_PATTERN A ? B ? C\ndefine A = UserDataStream(temp1 = 10), B= UserDataStream(temp2 = 30.40), C= UserDataStream(temp3 = 'Bret')\nPARTITION BY temp1,temp2;\n\n-- scenario 1.7 check pattern quantifire(0 or 1),<=,>= with partition by\nCREATE CQ TypeConversionCsvCQ7\nINSERT INTO TypedStream7\nSELECT UserId as typeduserid,\n\tA.temp1 as typedtemp1,\n\tB.temp2 as typedtemp2,\n\tC.temp3 as typedtemp3\nfrom UserDataStream\nMATCH_PATTERN A * B * C\ndefine A = UserDataStream(temp1 <= 10), B= UserDataStream(temp2 >= 30.40), C= UserDataStream(temp3 = 'zalak')\nPARTITION BY UserId,temp3;\n\n-- scenario 1.8 check pattern alteration and permutation using between values with partition by\nCREATE CQ TypeConversionCsvCQ8\nINSERT INTO TypedStream8\nSELECT UserId as typeduserid,\n\tA.temp1 as typedtemp1,\n\tB.temp2 as typedtemp2,\n\tC.temp3 as typedtemp3\nfrom UserDataStream\nMATCH_PATTERN A | B & C\ndefine A = UserDataStream(temp1 between 10 and 40), B= UserDataStream(temp2 between 10.40 and 30.50), C= UserDataStream(temp3 != 'zalak')\nPARTITION BY temp3;\n\n-- scenario 1.9 check pattern or using <,>,!= values with partition by\nCREATE CQ TypeConversionCsvCQ9\nINSERT INTO TypedStream9\nSELECT UserId as typeduserid,\n\tA.temp1 as typedtemp1,\n\tB.temp2 as typedtemp2,\n\tC.temp3 as typedtemp3\nfrom UserDataStream\nMATCH_PATTERN A  B  C\ndefine A = UserDataStream(temp1 > 20), B= UserDataStream(temp2 < 60), C= UserDataStream(temp3 != 'prajkta')\nPARTITION BY UserId;\n\n-- scenario 1.10 check pattern {m,n} using != values with partition by\nCREATE CQ TypeConversionCsvCQ10\nINSERT INTO TypedStream10\nSELECT UserId as typeduserid,\n\tA.temp1 as typedtemp1,\n\tB.temp2 as typedtemp2,\n\tC.temp3 as typedtemp3\nfrom UserDataStream\nMATCH_PATTERN A{0,2} | B{1,3} & C\ndefine A = UserDataStream(temp1 != 20), B= UserDataStream(temp2 != 40.10), C= UserDataStream(temp3 != 'bert')\nPARTITION BY temp1;\n\n\nCREATE WACTIONSTORE UserActivityInfo1\nCONTEXT OF TypedStream1_Type\nEVENT TYPES ( TypedStream1_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfo2\nCONTEXT OF TypedStream2_Type\nEVENT TYPES ( TypedStream2_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfo3\nCONTEXT OF TypedStream3_Type\nEVENT TYPES ( TypedStream3_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfo4\nCONTEXT OF TypedStream4_Type\nEVENT TYPES ( TypedStream4_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfo5\nCONTEXT OF TypedStream5_Type\nEVENT TYPES ( TypedStream5_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfo6\nCONTEXT OF TypedStream6_Type\nEVENT TYPES ( TypedStream6_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfo7\nCONTEXT OF TypedStream7_Type\nEVENT TYPES ( TypedStream7_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfo8\nCONTEXT OF TypedStream8_Type\nEVENT TYPES ( TypedStream8_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfo9\nCONTEXT OF TypedStream9_Type\nEVENT TYPES ( TypedStream9_Type )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE UserActivityInfo10\nCONTEXT OF TypedStream10_Type\nEVENT TYPES ( TypedStream10_Type )\n@PERSIST-TYPE@\n\ncreate Target t2 using SysOut(name:AgentTyped) input from TypedStream1;\ncreate Target t3 using SysOut(name:AgentTyped1) input from TypedStream2;\n\n--get data from UserDataStream and place into wactionStore UserWaction\nCREATE CQ UserWaction1\nINSERT INTO UserActivityInfo1\nSELECT * FROM TypedStream1\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction2\nINSERT INTO UserActivityInfo2\nSELECT * FROM TypedStream2\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction3\nINSERT INTO UserActivityInfo3\nSELECT * FROM TypedStream3\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction4\nINSERT INTO UserActivityInfo4\nSELECT * FROM TypedStream4\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction5\nINSERT INTO UserActivityInfo5\nSELECT * FROM TypedStream5\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction6\nINSERT INTO UserActivityInfo6\nSELECT * FROM TypedStream6\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction7\nINSERT INTO UserActivityInfo7\nSELECT * FROM TypedStream7\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction8\nINSERT INTO UserActivityInfo8\nSELECT * FROM TypedStream8\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction9\nINSERT INTO UserActivityInfo9\nSELECT * FROM TypedStream9\nLINK SOURCE EVENT;\n\nCREATE CQ UserWaction10\nINSERT INTO UserActivityInfo10\nSELECT * FROM TypedStream10\nLINK SOURCE EVENT;\n\nend application CSV;\ndeploy application csv;\nstart csv;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\ncreate application @appname@ recovery 1 second interval;\n\nCREATE SOURCE @parquetsrc@ USING FileReader (\n  directory: '',\n  positionByEOF: false,\n  WildCard: '' )\nPARSE USING ParquetParser (\n )\nOUTPUT TO @appname@Streams;\n\nCREATE OR REPLACE TARGET @kafkatarget@ USING Global.KafkaWriter VERSION @KAFKAVERSION@(\n     brokerAddress: '',\n     Topic: '',\n     KafkaConfigValueSeparator: '=',\n     MessageKey: '',\n     MessageHeader: '',\n     KafkaConfigPropertySeparator: ';',\n     Mode: 'Sync',\n     KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000' )\nformat using AvroFormatter (\nformatAs: 'Default',\n  schemaregistryurl: 'http://localhost:8081/',\n  SchemaRegistrySubjectName: '',\n  formatterName: 'AvroFormatter',\n  schemaregistryConfiguration: ''\n)\nINPUT FROM @appname@Streams;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "DROP APPLICATION @APP_NAME@1 FORCE;\nDROP APPLICATION @APP_NAME@2 FORCE;\nDROP APPLICATION @APP_NAME@3 FORCE;\n\n\nCREATE APPLICATION @APP_NAME@1 WITH ENCRYPTION USE EXCEPTIONSTORE TTL : '7d';\nCREATE OR REPLACE PROPERTYSET @APP_NAME@1_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',\nacks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\nCREATE STREAM @APP_NAME@_In1 OF Global.waevent persist using @APP_NAME@1_KafkaPropset;\n\nCREATE OR REPLACE SOURCE @APP_NAME@1_Source USING @SOURCE_ADAPTER@  ( \n  Tables:'@SRC_TABLE@'\n) OUTPUT TO @APP_NAME@_In1;\n\nCREATE OR REPLACE TARGET @APP_NAME@1_Target USING @TARGET_ADAPTER@  ( \n  Tables: '@SRC_TABLE@, @TGT_TABLE@1', \n  CheckPointTable: '@CHK_TABLE@', \n) INPUT FROM @APP_NAME@_In1;\n\nCREATE TARGET @APP_NAME@1_SysOut\nUSING SysOut(name:@APP_NAME@1Sys)\nINPUT FROM @APP_NAME@_In1;\n\nCREATE TARGET @APP_NAME@1_FileWriter USING filewriter\n(filename:'@APP_NAME@_FW1.log'\n)\nformat using dsvFormatter()\nINPUT FROM @APP_NAME@_In1;\n\nEND APPLICATION @APP_NAME@1;\n\nDEPLOY APPLICATION @APP_NAME@1;\nSTART APPLICATION @APP_NAME@1;\n\nCREATE APPLICATION @APP_NAME@2;\n\nCREATE OR REPLACE SOURCE @APP_NAME@2_Source USING @SOURCE_ADAPTER@  ( \n  Tables:'@SRC_TABLE@'\n) OUTPUT TO @APP_NAME@_In2;\n\nCREATE OR REPLACE TARGET @APP_NAME@2_Target USING @TARGET_ADAPTER@  ( \n  Tables: '@SRC_TABLE@, @TGT_TABLE@2', \n  CheckPointTable: '@CHK_TABLE@'\n) INPUT FROM @APP_NAME@_In2;\n\nCREATE TARGET @APP_NAME@2_SysOut\nUSING SysOut(name:@APP_NAME@2Sys)\nINPUT FROM @APP_NAME@_In2;\n\nCREATE TARGET @APP_NAME@2_FileWriter USING filewriter\n(filename:'@APP_NAME@_FW2.log'\n)\nformat using dsvFormatter()\nINPUT FROM @APP_NAME@_In2;\n\nEND APPLICATION @APP_NAME@2;\n\nDEPLOY APPLICATION @APP_NAME@2;\nSTART APPLICATION @APP_NAME@2;\n\nCREATE APPLICATION @APP_NAME@3;\nCREATE FLOW @APP_NAME@_SOURCEFLOW;\nCREATE OR REPLACE SOURCE @APP_NAME@3_Source USING @SOURCE_ADAPTER@  ( \n  Tables:'@SRC_TABLE@'\n) OUTPUT TO @APP_NAME@_In3;\nEND FLOW @APP_NAME@_SOURCEFLOW;\n\nCREATE FLOW @APP_NAME@_TARGETFLOW;\nCREATE OR REPLACE TARGET @APP_NAME@3_Target USING @TARGET_ADAPTER@  ( \n  Tables: '@SRC_TABLE@, @TGT_TABLE@3', \n  CheckPointTable: '@CHK_TABLE@', \n) INPUT FROM @APP_NAME@_In3;\n\nCREATE TARGET @APP_NAME@3_SysOut\nUSING SysOut(name:@APP_NAME@3Sys)\nINPUT FROM @APP_NAME@_In3;\n\nCREATE TARGET @APP_NAME@3_FileWriter USING filewriter\n(filename:'@APP_NAME@_FW3.log'\n)\nformat using dsvFormatter()\nINPUT FROM @APP_NAME@_In3;\nEND FLOW @APP_NAME@_TARGETFLOW;\n\nEND APPLICATION @APP_NAME@3;\n\nDEPLOY APPLICATION @APP_NAME@3;\nSTART APPLICATION @APP_NAME@3;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 4 on two node cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS\n-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS\n--\n\nSTOP APPLICATION KStreamN2S2CR4Tester.KStreamN2S2CRTest4;\nUNDEPLOY APPLICATION KStreamN2S2CR4Tester.KStreamN2S2CRTest4;\nDROP APPLICATION KStreamN2S2CR4Tester.KStreamN2S2CRTest4 CASCADE;\n\nDROP USER KStreamN2S2CR4Tester;\nDROP NAMESPACE KStreamN2S2CR4Tester CASCADE;\nCREATE USER KStreamN2S2CR4Tester IDENTIFIED BY KStreamN2S2CR4Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR4Tester;\nCONNECT KStreamN2S2CR4Tester KStreamN2S2CR4Tester;\n\nCREATE APPLICATION KStreamN2S2CRTest4 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionKStreamN2S2CRTest4;\n\nCREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');\nCREATE STREAM KafkaCsvStream using KafkaProps;\n\nCREATE SOURCE CsvSourceKStreamN2S2CRTest4 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nEND FLOW DataAcquisitionKStreamN2S2CRTest4;\n\nCREATE FLOW DataProcessingKStreamN2S2CRTest4;\n\nCREATE TYPE CsvDataKStreamN2S2CRTest4 (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE TYPE WactionTypeKStreamN2S2CRTest4 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\n\nCREATE STREAM DataStream OF CsvDataKStreamN2S2CRTest4;\n\nCREATE CQ CsvToDataKStreamN2S2CRTest4\nINSERT INTO DataStream\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsKStreamN2S2CRTest4 CONTEXT OF WactionTypeKStreamN2S2CRTest4\nEVENT TYPES ( CsvDataKStreamN2S2CRTest4 )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO WactionsKStreamN2S2CRTest4\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nCREATE CQ Data6ToWaction\nINSERT INTO WactionsKStreamN2S2CRTest4\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream6Minutes p;\n\nEND FLOW DataProcessingKStreamN2S2CRTest4;\n\nEND APPLICATION KStreamN2S2CRTest4;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@;\n\ncreate flow agentflow;\nCREATE OR REPLACE SOURCE @APPNAME@_Src USING SpannerBatchReader  (\n  DatabaseProviderType: 'Default',\n  pollingInterval: '5ms',\n  FetchSize: 1,\n  ReturnDateTimeAs: 'JODA',\n  ConnectionURL: 'jdbc:cloudspanner:/projects/bigquerywritertest/instances/testspanner/databases/spannertestdb?credentials=/Users/jenniffer/Downloads/abc.json',\n  Tables: 'Recovery_Timestam%',\n  --_h_mode:'InitialLoad',\n--  VendorConfiguration:'_h_SpannerReadStaleness=MAX_STALENESS 20s',\n  adapterName: 'SpannerBatchReader',\n    StartPosition: '%=0',\n  CheckColumn: '%=id'\n )\nOUTPUT TO @APPNAME@_Output_Stream;\nend flow agentflow;\n\nCREATE TARGET @APPNAME@_tgt USING SpannerWriter (\n\tTables: 'spannersource,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tBatchPolicy: 'EventCount: 1; Interval: 1s',\n\tinstanceId: 'qatest'\n) INPUT FROM @APPNAME@_Output_Stream;\n\ncreate Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_Output_Stream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@ with agentflow in agents;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop application Postgres_To_PostgresApp;\nundeploy application Postgres_To_PostgresApp;\ndrop application Postgres_To_PostgresApp cascade;\n\nCREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE Postgres_src USING PostgreSQLReader  ( \n ReplicationSlotName: 'test_slot',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  PostgresConfig: '@PGConfig@',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --startLSN:'758199500005967269518578814280000',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.postgrestopostgres_src',\n  ExcludedTables:'public.postgres_2000target'\n ) \nOUTPUT TO Change_Data_Stream ;\n\n\nCREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM Change_Data_Stream;\n\nCREATE OR REPLACE TARGET Postgres_tgt USING DatabaseWriter ( \nDatabaseProviderType: 'Default',\nCheckPointTable: 'CHKPOINT',\nPreserveSourceTransactionBoundary: 'false',\nUsername: 'waction',\nPassword_encrypted: 'false',\nBatchPolicy: 'EventCount:1,Interval:60',\nCommitPolicy: 'EventCount:1,Interval:60',\nConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\nTables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',\nadapterName: 'DatabaseWriter',\nPassword: 'w@ct10n'\n)INPUT FROM Change_Data_Stream;\n\nend application Postgres_To_PostgresApp;\ndeploy application Postgres_To_PostgresApp;\nstart Postgres_To_PostgresApp;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APP_NAME@;\nUNDEPLOY APPLICATION @APP_NAME@;\nDROP APPLICATION @APP_NAME@ CASCADE;\n\nCREATE APPLICATION @APP_NAME@;\nCREATE FLOW @APP_NAME@_SrcFlow;\n\nCREATE OR REPLACE SOURCE @APP_NAME@_src USING FileReader (\ndirectory:'',\nWildCard:''\n)\nparse using DSVParser (\nheader:'no'\n)\nOUTPUT TO @APP_NAME@_Stream;\nEND FLOW @APP_NAME@_SrcFlow;\n\nCREATE FLOW @APP_NAME@_TgtFlow;\n\nCREATE OR REPLACE TYPE @APP_NAME@_Type  ( BUSINESS_NAME java.lang.String KEY,\nMERCHANT_ID java.lang.String,\nPRIMARY_ACCOUNT_NUMBER java.lang.String\n ) ;\n\nCREATE OR REPLACE STREAM @APP_NAME@_Stream2 OF @APP_NAME@_Type;\nCREATE OR REPLACE CQ @APP_NAME@_CQ\nINSERT INTO @APP_NAME@_Stream2\nSELECT data[0],data[1],data[2]\nFROM @APP_NAME@_Stream;\n\nCREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.FabricDataWarehouseWriter (\n  Tables: '',\n  ConnectionURL: '@CONN_URL@',\n  Username: '@USERNAME@',\n  Password: '@PASSWORD@',\n  uploadpolicy: 'eventcount:1',\n  AccountName: '@ACCOUNTNAME@')\nINPUT FROM @APP_NAME@_Stream2;\n\nEND FLOW @APP_NAME@_TgtFlow;\n\nEND APPLICATION @APP_NAME@;\nDEPLOY APPLICATION @APP_NAME@ WITH @APP_NAME@_SrcFlow IN agents,@APP_NAME@_TgtFlow IN default;\nSTART APPLICATION @APP_NAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application KinesisTest;\nCREATE OR REPLACE SOURCE ora_reader USING OracleReader (\n  Username: 'miner',\n  Password: 'miner',\n  ConnectionURL: '192.168.1.113:1521:ORCL',\n  TABLES: 'QATEST.H_REGION;QATEST.H_NATION;QATEST.H_CUSTOMER',\n  FetchSize: '1'\n )\nOUTPUT TO DDLCDCStream;\n\ncreate or replace Target t using KinesisWriter (\n\tregionName:'TARGET_REGION',\n\tstreamName:'TARGET_STREAM',\n\taccesskeyid:'ACCESS_KEY',\n\tsecretaccesskey:'SECRET_KEY'\n)\nformat using JSONFormatter (\n)\ninput from DDLCDCStream;\n\nend application KinesisTest;\ndeploy application KinesisTest in default;\nstart application KinesisTest;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (\n  Username: '@TARGET_USER@',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  (\n  Username: '@TARGET_USER@',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Canon Test W60\n-- Nicholas Keene, WebAction, Inc.\n--\n-- Basic test for a partitioned sliding attribute window\n--\n-- S -> SWa5p -> CQ -> WS\n--\n\n\nUNDEPLOY APPLICATION NameW60.W60;\nDROP APPLICATION NameW60.W60 CASCADE;\nCREATE APPLICATION W60 RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE FLOW DataAcquisitionW60;\n\nCREATE SOURCE CsvSourceW60 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'Canon1000.csv',\n  columndelimiter:',',\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStreamW60;\n\nEND FLOW DataAcquisitionW60;\n\n\n\n\nCREATE FLOW DataProcessingW60;\n\nCREATE TYPE DataTypeW60 (\n    eventID String,\n    word String KEY,\n    dateTime DateTime\n);\n\nCREATE STREAM DataStreamW60 OF DataTypeW60;\n\nCREATE CQ CSVStreamW60_to_DataStreamW60\nINSERT INTO DataStreamW60\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[2],'yyyyMMddHHmmss')\nFROM CsvStreamW60;\n\nCREATE WINDOW SWa5pW60\nOVER DataStreamW60\nKEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY word;\n\nCREATE WACTIONSTORE WactionStoreW60 CONTEXT OF DataTypeW60\nEVENT TYPES ( DataTypeW60 KEY(word) )\n@PERSIST-TYPE@\n\nCREATE CQ SWa5pW60_to_WactionStoreW60\nINSERT INTO WactionStoreW60\nSELECT\n    FIRST(eventID),\n    FIRST(word),\n    FIRST(dateTime)\nFROM SWa5pW60\nGROUP BY word;\n\nEND FLOW DataProcessingW60;\n\n\n\nEND APPLICATION W60;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 25 with two sources, two jumping count windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jc5W -> CQ1 -> WS\n--   S2 -> Jc6W -> CQ2 -> WS\n--\n\nSTOP Recov25Tester.RecovTest25;\nUNDEPLOY APPLICATION Recov25Tester.RecovTest25;\nDROP APPLICATION Recov25Tester.RecovTest25 CASCADE;\nCREATE APPLICATION RecovTest25 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP 5 ROWS;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP 6 ROWS;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream6Minutes p;\n\nEND APPLICATION RecovTest25;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.OJet (\n  PrimaryDatabasePassword: '@PRIMARY_PASSWORD@',\n  ConnectionURL: '@DOWNSTREAM_URL@',\n  PrimaryDatabaseUsername: '@PRIMARY_USER@',\n  Password: '@DOWNSTREAM_PASSWORD@',\n  DownstreamCaptureMode: 'REAL_TIME',\n  DownstreamCapture: true,\n  PrimaryDatabaseConnectionURL: '@PRIMARY_URL@',\n  Tables: '@SOURCE_TABLES@',\n  CDDLCapture: true,\n  Username: '@DOWNSTREAM_USER@' )\nOUTPUT TO @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_SYS@ USING Global.SysOut (\n  name: 'Out' )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter (\n  ConnectionURL: '@TARGET_URL@',\n  Username: '@TARGET_USER@',\n  Password: '@TARGET_PASSWORD@',\n  CheckPointTable: 'CHKPOINT',\n  CommitPolicy: 'EventCount:1',\n  Tables: '@TARGET_TABLES@',\n  BatchPolicy: 'EventCount:1' )\nINPUT FROM @STREAM@;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 31 with two sources, two sliding count windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Sc5W/p -> CQ1 -> WS\n-- S2 -> Sc6W/p -> CQ2 -> WS\n--\n\nSTOP Recov31Tester.RecovTest31;\nUNDEPLOY APPLICATION Recov31Tester.RecovTest31;\nDROP APPLICATION Recov31Tester.RecovTest31 CASCADE;\nCREATE APPLICATION RecovTest31 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP 5 ROWS\nPARTITION BY merchantId;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP 6 ROWS\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1\nGROUP BY merchantId;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2\nGROUP BY merchantId;\n\nEND APPLICATION RecovTest31;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ recovery 5 second interval;\n--create application @APPNAME@;\n\n--create flow agentflow;\nCREATE OR REPLACE SOURCE @APPNAME@_Src USING SpannerBatchReader  (\n  DatabaseProviderType: 'Default',\n  pollingInterval: '5ms',\n  FetchSize: 1,\n  ReturnDateTimeAs: 'JODA',\n  ConnectionURL: 'jdbc:cloudspanner:/projects/bigquerywritertest/instances/testspanner/databases/spannertestdb?credentials=/Users/jenniffer/Downloads/abc.json',\n  Tables: 'Recovery_Timestam%',\n  --_h_mode:'InitialLoad',\n--  VendorConfiguration:'_h_SpannerReadStaleness=MAX_STALENESS 20s',\n  adapterName: 'SpannerBatchReader',\n    StartPosition: '%=0',\n  CheckColumn: '%=id'\n )\nOUTPUT TO @APPNAME@_Output_Stream;\n--end flow agentflow;\n\nCREATE TARGET @APPNAME@_tgt USING SpannerWriter (\n\tTables: 'spannersource,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tBatchPolicy: 'EventCount: 1; Interval: 1s',\n\tinstanceId: 'qatest'\n) INPUT FROM @APPNAME@_Output_Stream;\n\ncreate Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_Output_Stream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@;\n--deploy application @APPNAME@ with agentflow in agents;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader\n(\nFetchSize:'@FETCHSIZE@',\nQueueSize:'@QUEUESIZE@',\nUsername:'@USERNAME@',\nPassword:'@PASSWORD@',\nConnectionURL:'@URL@',\n_h_useClassic: @CLASSICMODE@,\nTables:'@TABLES@',\nCommittedTransactions:'@COMMITEDT@'\nCompression:'@COMPRESSION@'\n)\nOUTPUT TO @STREAM@;\n\ncreate TYPE CountTYPE(numcol INT);\n\nCREATE JUMPING WINDOW nEvents OVER @STREAM@ KEEP 10 ROWS;\n\nCREATE STREAM TypedCountStream of CountTYPE;\n\nCREATE CQ CountCQ INSERT INTO TypedCountStream SELECT TO_INT(data[0]) FROM nEvents;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 4 on four node all server cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS\n-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS\n--\n\nSTOP APPLICATION N4S4CR4Tester.N4S4CRTest4;\nUNDEPLOY APPLICATION N4S4CR4Tester.N4S4CRTest4;\nDROP APPLICATION N4S4CR4Tester.N4S4CRTest4 CASCADE;\nCREATE APPLICATION N4S4CRTest4 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN4S4CRTest4;\n\nCREATE SOURCE CsvSourceN4S4CRTest4 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN4S4CRTest4;\n\nCREATE FLOW DataProcessingN4S4CRTest4;\n\nCREATE TYPE CsvDataN4S4CRTest4 (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE TYPE WactionDataN4S4CRTest4 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\n\nCREATE STREAM DataStream OF CsvDataN4S4CRTest4;\n\nCREATE CQ CsvToDataN4S4CRTest4\nINSERT INTO DataStream\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsN4S4CRTest4 CONTEXT OF WactionDataN4S4CRTest4\nEVENT TYPES ( CsvDataN4S4CRTest4 )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO WactionsN4S4CRTest4\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nCREATE CQ Data6ToWaction\nINSERT INTO WactionsN4S4CRTest4\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream6Minutes p;\n\nEND FLOW DataProcessingN4S4CRTest4;\n\nEND APPLICATION N4S4CRTest4;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (\n  Username: '@TARGET_USER@',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  IgnorableExceptionCode: '1,TABLE_NOT_FOUND'\n )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION DatabaseWriterTester.DatabaseReaderApp;\nUNDEPLOY APPLICATION DatabaseWriterTester.DatabaseReaderApp;\nDROP APPLICATION DatabaseWriterTester.DatabaseReaderApp CASCADE;\nSTOP APPLICATION DatabaseWriterTester.DatabaseWriterApp;\nUNDEPLOY APPLICATION DatabaseWriterTester.DatabaseWriterApp;\nDROP APPLICATION DatabaseWriterTester.DatabaseWriterApp CASCADE;\n\nCREATE APPLICATION DatabaseWriterApp;\n\nCREATE SOURCE CSVSource USING FileReader (\ndirectory:'@TEST-DATA-PATH@',\nWildCard:'banks.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\nheader:'yes'\n)\nOUTPUT TO Orders;\n\nCREATE TYPE OrdersData(\n  id String Key,\n  name String);\n\nCREATE STREAM OrdersDataStream OF OrdersData;\n\nCREATE CQ CsvToOrdersData\nINSERT INTO OrdersDataStream (id, name)\nSELECT data[0], data[1]\nFROM Orders;\n\nCREATE TARGET WriteJpaOracle USING DatabaseWriter(\nDriverName:'oracle.jdbc.OracleDriver',\nConnectionURL:'@READER-URL@',\n        Username:'@READER-UNAME@',\n        Password:'@READER-PASSWORD@',\nTables:'QATEST.DB_WRITER_TEST',\neventType:'DatabaseWriterTester.OrdersData'\n) INPUT FROM OrdersDataStream;\n\nEND APPLICATION DatabaseWriterApp;\nDEPLOY APPLICATION DatabaseWriterApp;\nSTART APPLICATION DatabaseWriterApp;\n\nCREATE APPLICATION DatabaseReaderApp;\n\ncreate source DatabaseReaderSource USING DatabaseReader\n(\nConnectionURL:'@READER-URL@',\n        Username:'@READER-UNAME@',\n        Password:'@READER-PASSWORD@',\nTables:'QATEST.DB_WRITER_TEST',\n   FetchSize: 1\n\n)\nOUTPUT TO DatabaseReaderTestStream;\n\ncreate Target t using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/Databasewritertarget_RT') input from DatabaseReaderTestStream;\n\n\nEND APPLICATION DatabaseReaderApp;\nDEPLOY APPLICATION DatabaseReaderApp;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()\nPARSE USING ParquetParser (\n  retryWait: '1m'\n)\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE CQ @APPNAME@_CQ\nINSERT INTO @APPNAME@_CQOut\nSELECT putUserData(x, 'folderName','Parquet') FROM @APPNAME@_Stream x;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()\nFORMAT USING ParquetFormatter (\n  schemaFileName: 'parquetSchema',\n  members:'data'\n)\nINPUT FROM @APPNAME@_CQOut;\n\nEND APPLICATION @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;\n\nCreate Source @SourceName@ Using OracleReader\n\n(\n  Username:'c##qatest',\n  Password:'qatest',\n  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',\n  Tables:'CDB$ROOT.\"C##QATEST\".ojet_src;ORCLPDB.QATEST.ojet_src',\n  _h_useClassic:false,\n  Fetchsize:1,\n  Compression: true,\n  SupportPDB:true,\n  ReplicationSlotName:'null'\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'))) FROM @SRCINPUTSTREAM@ x; ;\n\n\nCREATE TARGET @targetName@ USING DatabaseWriter\n(\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',\n  Username:'c##qatest',\n  Password:'qatest',\n  BatchPolicy:'EventCount:1,Interval:1',\n  Tables:'CDB$ROOT.\"C##QATEST\".ojet_src,CDB$ROOT.\"C##QATEST\".ojet_tgt'\n) INPUT FROM admin.sqlreader_cq_out;\n\n\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop Oracle_IRLogWriter;\nUndeploy application Oracle_IRLogWriter;\ndrop application Oracle_IRLogWriter cascade;\n\nCREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;\n\nCREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( \n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.TDSOURCE',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: 'striim.TEST01=ID;',\n  PollingInterval: '5sec',\n  ReturnDateTimeAs: 'String',\n  startPosition:'striim.test01=0'\n  )\n  OUTPUT TO data_stream;\n\n  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM data_stream;\n\nCREATE TARGET BinaryDump USING LogWriter(\n  name: 'TeraData',\n  filename:'TeraData.log',\n  flushpolicy:'EventCount:100,Interval:30s'\n)INPUT FROM data_stream;\n\nEND APPLICATION Oracle_IRLogWriter;\n\ndeploy application Oracle_IRLogWriter in default;\n\nstart application Oracle_IRLogWriter;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 1\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> WS\n--\n\n\nUNDEPLOY APPLICATION NameM00.M00;\nDROP APPLICATION NameM00.M00 CASCADE;\nCREATE APPLICATION M00 RECOVERY 5 SECOND INTERVAL;\n\n\n\n\nCREATE FLOW DataAcquisitionM00;\n\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'Canon1000.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\n\nEND FLOW DataAcquisitionM00;\n\n\n\n\nCREATE FLOW DataProcessingM00;\n\n\nCREATE TYPE WactionTypeM00 (\n    eventID String,\n    word String KEY,\n    dateTime DateTime\n);\n\nCREATE WACTIONSTORE WactionsM00 CONTEXT OF WactionTypeM00\nEVENT TYPES ( WactionTypeM00 KEY(word) )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsM00\nINSERT INTO WactionsM00\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[2],'yyyyMMddHHmmss')\nFROM CsvStream;\n\n\nEND FLOW DataProcessingM00;\n\n\n\nEND APPLICATION M00;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION OJETTOBIGQUERY;\nUNDEPLOY APPLICATION OJETTOBIGQUERY;\nDROP APPLICATION OJETTOBIGQUERY CASCADE;\n\n--create application \nCREATE APPLICATION OJETTOBIGQUERY RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE OR REPLACE SOURCE ojetSrc USING Ojet (\n ConnectionURL: '192.168.123.12:1521/ORCL',\n Tables: 'QATEST.OJETTOBIGQALLDATATYPE',\n Username: 'qatest',\n Password: 'qatest',\n FetchSize:1\n) OUTPUT TO CDCStream;\n\nCREATE OR REPLACE TARGET bqtables using BigqueryWriter(\n serviceAccountKey:\"/Users/karthikmurugan/Downloads/bqtest-540227c31980.json\",\n projectId:\"bqtest-158706\",\n datalocation: 'US',\n Tables: \"QATEST.OJETTOBIGQALLDATATYPE,QATEST.OJETTOBIGQALLDATATYPE\",\n BatchPolicy: \"eventCount:1,Interval:90\")\nINPUT FROM CDCStream;\n\n\nCREATE OR REPLACE TARGET T1 using SysOut(name :Foo2out) INPUT FROM CDCStream;\n\nEND APPLICATION OJETTOBIGQUERY;\n\nDEPLOY APPLICATION OJETTOBIGQUERY;\nSTART APPLICATION OJETTOBIGQUERY;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW ;\n\n \n\nCREATE OR REPLACE SOURCE DBSource USING DatabaseReader  ( \n  Username: '@SOURCE_USER@',\n  Password_encrypted: false,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  Query: \"SELECT * FROM qatest.MssqlToCql_alldatatypes\",\n  adapterName: 'DatabaseReader',\n  FetchSize: 100,\n  Password: '@SOURCE_PASS@'\n ) \nOUTPUT TO Oracle_ChangeDataStream;\n\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1000,Interval:0',\n  CommitPolicy: 'EventCount:1000,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n ) \nINPUT FROM Oracle_ChangeDataStream;\n\ncreate Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\nEND APPLICATION DBRTOCW;\ndeploy application DBRTOCW in default;\nstart DBRTOCW;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');\n\nCREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.waevent PERSIST USING @APPNAME@KafkaPropset;\n\nCREATE TYPE @APPNAME@posDataType (\n merchantId java.lang.String,\n companyName java.lang.String\n );\n\nCREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (\n  ProviderName: '',\n  UserName: '',\n  transactionpolicy: '',\n  Provider: '',\n  Ctx: '',\n  EnableTransaction: '',\n  QueueName: '',\n  Topic:'',\n  Password: '' )\nPARSE USING XMLParser (\n  rootnode:'/JMSXMLIN'\n  )\nOUTPUT TO @APPNAME@PersistStream@RANDOM@;\n\nCREATE STREAM @APPNAME@TypedStream OF @APPNAME@posDataType;\n\nCREATE CQ @APPNAME@CQ\nINSERT INTO @APPNAME@TypedStream\nSELECT TO_STRING(data[0]) as merchantId,\n  TO_STRING(data[1]) as companyName\nFROM @APPNAME@PersistStream@RANDOM@;\n\nCREATE OR REPLACE TARGET @APPNAME@_filetrgt USING Global.FileWriter (\n  filename: '',\n  flushpolicy: '',\n  directory: '',\n  rolloverpolicy: '' )\nFORMAT USING JSONFormatter  ()\nINPUT FROM @APPNAME@TypedStream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use RetailTester;\nDROP FLOW RetailSourceFlow cascade;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--This TQL only works with HL7v2 2.3 Messages, specifically generated from SimHospital Docker Simulator\n\nCREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE STREAM @APPNAME@_AlertStream OF Global.AlertEvent;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING Global.TCPReader ()\nPARSE USING Global.HL7v2Parser ()\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE OR REPLACE CQ @APPNAME@_ADT_CQ\nINSERT INTO @APPNAME@_ADT_CQOut\nSELECT \ndata.element(\"MSH\").element(\"MSH.6\").element(\"HD.1\").getText() as Facility,\nTO_DATEF(data.element(\"MSH\").element(\"MSH.7\").element(\"TS.1\").getText(),'yyyyMMddHHmmss') as MsgTime,\ndata.element(\"PID\").element(\"PID.3\").element(\"CX.1\").getText() as PatientID, \ndata.element(\"PID\").element(\"PID.5\").element(\"XPN.1\").getText() as FirstName, \ndata.element(\"PID\").element(\"PID.5\").element(\"XPN.2\").getText() as LastName, \nTO_DATEF(SLEFT(data.element(\"PID\").element(\"PID.7\").element(\"TS.1\").getText(),8),'yyyyMMdd') as DOB, \ndata.element(\"PID\").element(\"PID.8\").getText() as Gender, \ndata.element(\"PID\").element(\"PID.11\").element(\"XAD.5\").getText() as ZipCode,\ndata.element(\"PV1\").element(\"PV1.2\").getText() as PatientClass,  \ndata.element(\"EVN\").element(\"EVN.1\").getText() as EventCode,\nTO_DATEF(data.element(\"EVN\").element(\"EVN.2\").element(\"TS.1\").getText(),'yyyyMMddHHmmss') as EventTime \nFROM @APPNAME@_Stream where data.getName() like \"ADT%\";\n\nCREATE OR REPLACE CQ @APPNAME@_ORU_CQ\nINSERT INTO @APPNAME@_ORU_CQOut\nSELECT \ndata.element(\"MSH\").element(\"MSH.6\").element(\"HD.1\").getText() as Facility,\nTO_DATEF(data.element(\"MSH\").element(\"MSH.7\").element(\"TS.1\").getText(),'yyyyMMddHHmmss') as MsgTime,\ndata.element(\"ORU_R01.RESPONSE\").element(\"ORU_R01.PATIENT\").element(\"PID\").element(\"PID.3\").element(\"CX.1\").getText() as PatientID,\ndata.element(\"ORU_R01.RESPONSE\").element(\"ORU_R01.ORDER_OBSERVATION\").element(\"OBR\").element(\"OBR.4\").element(\"CE.1\").getText() as OrderIdentifier,\ndata.element(\"ORU_R01.RESPONSE\").element(\"ORU_R01.ORDER_OBSERVATION\").element(\"OBR\").element(\"OBR.4\").element(\"CE.2\").getText() as OrderText \nFROM @APPNAME@_Stream where data.getName() like \"ORU%\";\n\nCREATE TARGET @APPNAME@_FileTarget USING FileWriter ()\nFORMAT USING Global.XMLFormatter  (\n  rootelement: 'document' )\nINPUT FROM @APPNAME@_Stream;\n\nCREATE TARGET @APPNAME@_OLTPTarget USING DatabaseWriter ()\nINPUT FROM @APPNAME@_ADT_CQOut;\n\nCREATE OR REPLACE TARGET @APPNAME@_NoSqlTarget USING MongoDBWriter ()\nINPUT FROM @APPNAME@_ADT_CQOut;\n\n--CREATE OR REPLACE TARGET @APPNAME@_KafkaTarget USING KafkaWriter VERSION @KAFKA_VERSION@()\n--FORMAT USING JSONFormatter (\n--schemaFileName: 'avroSchema'\n--)\n--INPUT FROM @APPNAME@_ORU_CQOut;\n\nCREATE TARGET @APPNAME@_DWHTarget USING BigQueryWriter ()\nINPUT FROM @APPNAME@_ORU_CQOut;\n\nCREATE TARGET @APPNAME@_ADLSTarget USING AdlsGen2Writer ()\nFORMAT USING Global.JSONFormatter  ()\nINPUT FROM @APPNAME@_ORU_CQOut;\n\nCREATE OR REPLACE CQ @APPNAME@_ADT_FilterCQ\nINSERT INTO @APPNAME@_AlertCQ\nSELECT \nPatientID as PatientID,\nPatientClass as PatientClass\nFROM @APPNAME@_ADT_CQOut p where  p.PatientClass='E';;\n\nCREATE OR REPLACE CQ @APPNAME@_AlertEventCQ\nINSERT INTO @APPNAME@_AlertStream\nSELECT \n'Patient Found' as name,\n'Emergency' as keyVal,\n'info' as severity,\n'raise' as flag,\n'Flagged patient `' + f.PatientID +'` is an Emergency Patient. NEED IMMEDIATE ATTENTION!!!' as message\nFROM @APPNAME@_AlertCQ f;\n\nCREATE OR REPLACE TARGET @APPNAME@_SlackTarget USING SlackAlertAdapter ()\nINPUT FROM @APPNAME@_AlertStream;\n\nEND APPLICATION @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application APP_KAFKASOURCE_AGG;\nundeploy application APP_KAFKASOURCE_AGG;\ndrop application APP_KAFKASOURCE_AGG cascade;\n\nCREATE APPLICATION APP_KAFKASOURCE_AGG;\n\nCREATE OR REPLACE TYPE STREAM_CQ_CALCULATELAG_Type (\n topic java.lang.String KEY,\n TotalTopicLag java.lang.Integer,\n lastdatatime org.joda.time.DateTime,\n status java.lang.String,\n is_green java.lang.Integer);\n\nCREATE OR REPLACE TYPE STREAM_CQ_JOIN_KAFKA_SOURCES1_Type (\n Company java.lang.String KEY,\n TotalLast24hour java.lang.Integer,\n TotalLast1hour java.lang.Integer,\n TotalTopicLag java.lang.Integer,\n lastdatatime org.joda.time.DateTime,\n status java.lang.String,\n is_green java.lang.Integer,\n latitude java.lang.String,\n longitude java.lang.String,\n topic java.lang.String,\n city_name java.lang.String,\n city_id java.lang.Integer);\n\nCREATE OR REPLACE TYPE STREAM_CQ_CALCULATE_HOURLYTOTALS_Type (\n topic java.lang.String KEY,\n TotalLast24hour java.lang.Integer,\n TotalLast1hour java.lang.Integer);\n\nCREATE OR REPLACE CQ CQ_GET_LASTHOUR\nINSERT INTO STREAM_CQ_GET_LASTHOUR\nSELECT rawdatacount, topic,timerange from  ET_HOURLYTOTALS_KAFKADATA_FILE,JUMP_WND_1EVT_30SEC where timerange = DHOURS(DNOW())-1;\n\nCREATE OR REPLACE CQ CQ_CALCULATE_HOURLY_TOTAL\nINSERT INTO STREAM_CQ_CALCULATE_HOURLY_TOTAL\nSELECT f.topic as topic, sum(f.rawdatacount) as TotalLast24hour, B.rawdatacount as TotalLast1hour FROM JUMP_WND_1EVT_1MIN h\n   join SLIDE_WND_HOURLYTOTALS_KAFKADATA_FILE f on 1=1\n   join STREAM_CQ_GET_LASTHOUR B on B.topic=f.topic\n   Group by f.topic;\n\nCREATE OR REPLACE EVENTTABLE ET_KAFKA_HOURLY_TOTAL USING STREAM (\n  name: 'STREAM_CQ_CALCULATE_HOURLY_TOTAL' )\nQUERY (\n  keytomap: 'topic',\n  persistPolicy: 'true' )\nOF STREAM_CQ_CALCULATE_HOURLY_TOTAL_Type;\n\nEND APPLICATION APP_KAFKASOURCE_AGG;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n\t\tBatchPolicy: 'EventCount:1',\n  \t\tCommitPolicy: 'EventCount:1',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',\n\t\tuploadpolicy:'eventcount:10,interval:30s'\n) INPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@2 using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n  \t\tBatchPolicy: 'Interval:10',\n  \t\tCommitPolicy: 'Interval:10',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',\n\t\tuploadpolicy:'eventcount:10,interval:30s'\n) INPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@3 using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n\t\tBatchPolicy: 'eventCount:100000,Interval:20',\n\t\tCommitPolicy: 'eventCount:100000,Interval:20',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',\n\t\tuploadpolicy:'eventcount:10,interval:30s'\n) INPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@4 using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n  \t\tBatchPolicy: 'EventCount:1',\n\t\tCommitPolicy: 'EventCount:1',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',\n\t\tuploadpolicy:'eventcount:10,interval:30s'\n) INPUT FROM @STREAM@;\n\nCREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(\n  name: 'CDDL',\n  filename:'WAEventDump.log'\n)INPUT FROM @STREAM@;;\n\ncreate Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nSTOP Sliding1Tester.Sliding1;\nUNDEPLOY APPLICATION Sliding1Tester.Sliding1;\nDROP APPLICATION Sliding1Tester.Sliding1 CASCADE;\nCREATE APPLICATION Sliding1;\n\ncreate source CsvSource1 using FileReader\n(\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'WindowsTest.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n)\n parse using DSVParser\n(\n\theader:'yes',\n\tcolumndelimiter:','\n)\nOUTPUT TO CsvStream1;\n\nCREATE TYPE CsvData (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\nCREATE TYPE CsvData1 (\n  zip double\n);\n\nCREATE TYPE WactionData1 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData2 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData3 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData4 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData5 (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\nCREATE TYPE WactionData6 (\n  zip double\n);\nCREATE TYPE WactionData7 (\n  zip double\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY companyName;\n\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY city;\n\nCREATE STREAM DataStream3 OF CsvData;\nCREATE STREAM DataStream4 OF CsvData;\nCREATE STREAM DataStream5 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData3\nINSERT INTO DataStream3\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData4\nINSERT INTO DataStream4\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData5\nINSERT INTO DataStream5\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData6\nINSERT INTO DataStream6\nSELECT\n    TO_DOUBLE(data[8])\nFROM CsvStream1;\n\nCREATE CQ CsvToData7\nINSERT INTO DataStream7\nSELECT\n    TO_DOUBLE(data[8])\nFROM CsvStream1;\n\n-- Count based sliding window\nCREATE WINDOW DataStreamCount\nOVER DataStream1 KEEP 5 ROWS\nPARTITION BY companyName;\n\n-- Time based jumping window\nCREATE WINDOW DataStreamTime OVER DataStream2 KEEP\nwithin 350 second\nPARTITION BY companyName,city;\n\n-- Attribute based sliding window\nCREATE WINDOW DataStreamAtrribute\nOVER DataStream3 KEEP\nrange 180 second\nON dateTime;\n\n-- Count + time based sliding window\nCREATE WINDOW DataStreamCountTime\nOVER DataStream4 KEEP\n5 rows\nwithin 155 second;\n\n-- Attribute + time based sliding window\nCREATE WINDOW DataStreamAttributeTime\nOVER DataStream5 KEEP\nrange 50 second\nON dateTime\nwithin 400 second;\n\nCREATE WACTIONSTORE Wactions1 CONTEXT OF WactionData1\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions2 CONTEXT OF WactionData2\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions3 CONTEXT OF WactionData3\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions4 CONTEXT OF WactionData4\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions5 CONTEXT OF WactionData5\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions6 CONTEXT OF WactionData6\nEVENT TYPES ( CsvData1 )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions7 CONTEXT OF WactionData7\nEVENT TYPES ( CsvData1 )\n@PERSIST-TYPE@\n\nCREATE CQ Data1ToWaction\nINSERT INTO Wactions1\nSELECT\n    p.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamCount p\ngroup by companyName;\n\nCREATE CQ Data2ToWaction\nINSERT INTO Wactions2\nSELECT\n    p.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamTime p\ngroup by companyName,city;\n\nCREATE CQ Data3ToWaction\nINSERT INTO Wactions3\nSELECT\n    p.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamAtrribute p;\n\nCREATE CQ Data4ToWaction\nINSERT INTO Wactions4\nSELECT\n    p.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamCountTime p;\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions5\nSELECT\n    p.companyName,\n    p.dateTime,\n    p.amount,\n    p.city\nFROM DataStreamAttributeTime p;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions6\nSELECT\n    count(*)\nFROM DataStreamCount p;\n\nCREATE CQ Data7ToWaction\nINSERT INTO Wactions7\nSELECT\n    count(*)\nFROM DataStreamTime p;\n\nEND APPLICATION Sliding1;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION @appname@ @recovery@ AUTORESUME MAXRETRIES 2 RETRYINTERVAL 60;\n\nCREATE OR REPLACE SOURCE @appname@src USING Global.FileReader (\n  adapterName: 'FileReader',\n  rolloverstyle: 'Default',\n  blocksize: 64,\n  networkfilesystem: true,\n  wildcard: @wildcard@,\n  compressiontype: 'gzip',\n  includesubdirectories: false,\n  directory: @inp_directory@,\n  skipbom: false,\n  positionbyeof: false )\nPARSE USING Global.DSVParser (\n  trimwhitespace: false,\n  linenumber: '-1',\n  columndelimittill: '-1',\n  trimquote: true,\n  ignoreemptycolumn: false,\n  separator: ':',\n  parserName: 'DSVParser',\n  quoteset: '\\\"',\n  handler: 'com.webaction.proc.DSVParser_1_0',\n  charset: 'UTF-8',\n  ignoremultiplerecordbegin: 'true',\n  ignorerowdelimiterinquote: false,\n  columndelimiter: '|',\n  blockascompleterecord: false,\n  rowdelimiter: '\\n',\n  nocolumndelimiter: false,\n  headerlineno: 0,\n  header: true )                  \nOUTPUT TO @appname@STREAM;\n                                                                                                         \nCREATE OR REPLACE CQ @appname@CQ\nINSERT INTO @appname@CQ_Out\nSELECT replaceStringRegex(f,'^$','NULL')\nFROM @appname@STREAM f;\n\nCREATE OR REPLACE CQ @appname@CQ1\nINSERT INTO @appname@CQ_Out1\nSELECT\nCASE when TO_STRING(data[0])!='NULL' THEN TO_STRING(data[0]) ELSE data[0] END AS CLIENT_ID,\nCASE when TO_STRING(data[1])!='NULL' THEN TO_STRING(data[1]) ELSE data[1] END AS ACCOUNT_NAME,\nCASE when TO_STRING(data[2])!='NULL' THEN TO_DATE(data[2], 'MM/dd/yyyy') ELSE data[2] END AS EFFECTIVE_DATE,\nCASE when TO_STRING(data[3])!='NULL' THEN TO_DATE(data[3], 'MM/dd/yyyy') ELSE data[3] END AS EXPIRATION_DATE,\nCASE when TO_STRING(data[4])!='NULL' THEN TO_STRING(data[4]) ELSE data[4] END AS XREF,\nCASE when TO_STRING(data[5])!='NULL' THEN TO_STRING(data[5]) ELSE data[5] END AS XREF_TYPE,\nCASE when TO_STRING(data[6])!='NULL' THEN TO_STRING(data[6]) ELSE data[6] END AS XREF_DESCRIPTION,\nCASE when TO_STRING(data[7])!='NULL' THEN TO_LONG(data[7]) ELSE data[7] END AS AUD_REC_ID,\nCASE when TO_STRING(data[8])!='NULL' THEN TO_LONG(data[8]) ELSE data[8] END AS X_CLIENT_ID,\nCASE when TO_STRING(data[9])!='NULL' THEN TO_LONG(data[9]) ELSE data[9] END AS X_XREF_ID,\nCASE when TO_STRING(data[10])!='NULL' THEN TO_DATE(data[10], 'MM/dd/yyyy HH:mm:ss') ELSE data[10] END AS AUDIT_DATE,\nDNOW() AS UDP_DB2_INSERT_TIMESTAMP,\nDNOW() AS UDP_DB2_UPDATE_TIMESTAMP\nFROM @appname@CQ_Out f;\n\nCREATE OR REPLACE TARGET @appname@tgt USING Global.FileWriter (\n  rolloverpolicy: 'EventCount:150,Interval:120s',\n  rolloveronddl: 'true',\n  encryptionpolicy: '',\n  flushpolicy: 'EventCount:150',\n  filename: @tar_filename@,\n  directory: @tar_directory@ )\nFORMAT USING Global.JSONFormatter  (\n   members:'CLIENT_ID,ACCOUNT_NAME,EXPIRATION_DATE,XREF,XREF_TYPE,XREF_DESCRIPTION,AUD_REC_ID,X_CLIENT_ID,X_XREF_ID'\n\n)\nINPUT FROM @appname@CQ_Out1;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application MSSQLTransactionSupportCompression;\nundeploy application MSSQLTransactionSupportCompression;\ndrop application MSSQLTransactionSupportCompression cascade;\n\nCREATE APPLICATION MSSQLTransactionSupportCompression recovery 1 second interval;\n\nCreate Source ReadFromMSSQL7\nUsing MSSqlReader\n(\nUsername:'@READER-NAME@',\nPassword:'@READER-PASSWORD@',\nDatabaseName:'@SRCDB-NAME@',\nConnectionURL:'@CONN-URL@',\nTables:'@WATABLES-SRC@',\nTransactionSupport: 'true',\nFetchTransactionMetadata:'true',\nFilterTransactionBoundaries: true,\nCompression:'true',\nConnectionPoolSize:1\n)\nOutput To MSSQLTransactionSupportCompressionStream;\n\n\nCREATE TARGET WriteToMySQL7 USING DatabaseWriter(\nConnectionURL:'@TGT-URL@',\nUsername:'@WRITER-UNAME@',\nPassword:'@WRITER-PASSWORD@',\nBatchPolicy:'EventCount:5,Interval:5',\nCommitPolicy:'EventCount:1,Interval:1',\nTables: '@WATABLES-SRC@,@WATABLES-TGT@'\n)\nINPUT FROM MSSQLTransactionSupportCompressionStream;\n\nCREATE TARGET MSSqlReaderOutput7 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportCompressionStream; \n\n\nCREATE OR REPLACE TARGET MSSQLFileOut7 USING FileWriter  ( \n  rolloverpolicy: 'EventCount:10000,Interval:30s',\n  adapterName: 'FileWriter',\n  directory:'@FEATURE-DIR@/logs/',\n  filename: 'TransactionSupportMSSQLToMySQLCompressionOn.txt'\n ) \nFORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true',\n  jsonobjectdelimiter: '\\n'\n ) \nINPUT FROM MSSQLTransactionSupportCompressionStream;\n\nEND APPLICATION MSSQLTransactionSupportCompression;\ndeploy application MSSQLTransactionSupportCompression;\nstart application MSSQLTransactionSupportCompression;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop tpcc;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\n\nCreate Source @SourceName@\n Using OracleReader\n(\n Username:'@READER-UNAME@',\n Password:'@READER-PASSWORD@',\n ConnectionURL:'@CDC-READER-URL@',\n Tables: '@WATABLES-SRC@',\n FetchSize:1,\n QueueSize:25000,\n CommittedTransactions:true,\n Compression:true,\n CaptureDDL: true,\n SendBeforeImage:true\n) Output To @SRCINPUTSTREAM@;\n\n\ncreate Target @targetsys@ using SysOut(name:OrgData) input from DataStream;\n\nCREATE TARGET @targetName@ USING databasewriter(\n  Username: '@WRITER-UNAME@',\n  Password: '@WRITER-PASSWORD@',\n  ConnectionURL:'@WRITER-URL@',\n  BatchPolicy:'EventCount:1,Interval:1',\n  CommitPolicy:'EventCount:1,Interval:1',\n  Tables: '@WATABLES-SRC@,@WATABLES-TGT@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\ncreate application @appname@;\n\nCREATE SOURCE @parquetsrc@ USING FileReader (\n  directory: '',\n  positionByEOF: false,\n  WildCard: '' )\nPARSE USING ParquetParser (\n )\nOUTPUT TO @appname@Streams;\n\nCREATE STREAM @appname@Stream3 OF Global.ParquetEvent;\n\nCREATE STREAM @appname@Stream4 OF Global.ParquetEvent;\n\nCREATE OR REPLACE CQ @appname@CQOrder3\nINSERT INTO @appname@Stream3\nSELECT\nPUTUSERDATA(s,'schemaName',s.data.getSchema().getName())\nFROM @appname@Streams s;\n\nCREATE OR REPLACE CQ @appname@CQOrder4\nINSERT INTO @appname@Stream4\nSELECT\nPUTUSERDATA(s2, 'customFilename', Userdata(s2, 'schemaName').toString().concat(\".test\"))\nFROM @appname@Stream3 s2;\n\nCREATE OR REPLACE TARGET @filetarget@ USING Global.FileWriter (\n  flushpolicy: 'EventCount:10000,Interval:30s',\n  directory: '',\n  filename: '',\n  rolloverpolicy: 'eventcount:10' )\nFORMAT USING ParquetFormatter  (\nschemafilename:''\n)\nINPUT FROM @appname@Stream4;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @AppName@;\n\nCREATE OR REPLACE SOURCE CP_Oracle_source USING OracleReader (\n  ConnectionURL: '',\n  Tables: '',\n  Username: '',\n  Password: '',\n  Fetchsize: 1 )\nOUTPUT TO CP_EndToEnd_DB_Adapter_Stream;\n\nCREATE OR REPLACE TARGET CP_DB_Target USING Global.DeltaLakeWriter (\n  connectionProfileName: '',\n  useConnectionProfile: 'true',\n  externalStageConnectionProfileName: '',\n  Tables: 'QATEST.Test_CP,qa_reg_CDDL_1702803133916.OrcToDLAltAppendtarget1',\n  uploadPolicy: 'eventcount:100000,interval:60s'\n)\nINPUT FROM CP_EndToEnd_DB_Adapter_Stream;\nEND APPLICATION @AppName@;\nDeploy application @AppName@;\nStart application @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION KafkaWPTester.KWApp;\nSTOP APPLICATION KafkaWPTester.KRApp;\nUNDEPLOY APPLICATION KafkaWPTester.KWApp;\nUNDEPLOY APPLICATION KafkaWPTester.KRApp;\nDROP APPLICATION KafkaWPTester.KWApp CASCADE;\nDROP APPLICATION KafkaWPTester.KRApp CASCADE;\n\nCREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;\nCONNECT KafkaWPTester KafkaWPTester;\n\n\nCREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE SOURCE CSVSource USING FileReader (\n\tdirectory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',\n    WildCard:'access_log_half',\n\tpositionByEOF:false,\n        Charset:'UTF-8'\n)\nPARSE USING DSVParser (\n  columndelimiter:' ',\n  ignoreemptycolumn:'Yes',\n  quoteset:'[]~\"',\n  separator:'~'\n)\nOUTPUT TO FileStream;\n\nCREATE TYPE AccessLogType(\nCol1 String,\nCol2 String,\nCol3 String,\nCol4 String,\nCol5 String,\nCol6 String,\nCol7 String,\nCol8 String,\nCol9 String,\nCol10 String\n);\n\nCREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;\n\nCREATE CQ AceeslogCQ\nINSERT INTO TypedAccessLogStream\nSELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]\nFROM FileStream;\n\ncreate Target DsvWriter using KafkaWriter VERSION '0.8.0' ( \n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V8dsvExistTopic',\n\t    Mode:'Sync'\n        )\n\nFORMAT USING DSVFormatter ()\ninput from TypedAccessLogStream;\n\n\ncreate Target JsonWriter using KafkaWriter VERSION '0.8.0' ( \n\tbrokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n\tTopic:'V8jsonExistTopic',\n\tMode:'Sync'\n    )\n \nFORMAT USING JSONFormatter ()\ninput from TypedAccessLogStream;\n\n\ncreate Target AvroWriter using KafkaWriter VERSION '0.8.0' ( \n\tbrokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n\tTopic:'V8avroExistTopic',\n    Mode:'Sync'\n    )\n \nFORMAT USING AvroFormatter (\n\tschemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')\ninput from TypedAccessLogStream;\n\n\nEND APPLICATION KWApp;\n\nDEPLOY APPLICATION KWApp on any in default;\n\n\n-- Kafka Reader Apps\n\nCREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE TYPE AccessLogType2(\nCol1 String,\nCol2 String,\nCol3 String,\nCol4 String,\nCol5 String,\nCol6 String,\nCol7 String,\nCol8 String,\nCol9 String,\nCol10 String\n);\n\nCREATE Stream KafkaDSVReaderStream of AccessLogType2;\nCREATE Stream KafkaJsonReaderStream of AccessLogType2;\nCREATE Stream KafkaAvroReaderStream of AccessLogType2;\n\n\nCREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '0.8.0' (\n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V8dsvExistTopic',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream1;\n\n\n\n\nCREATE CQ DSVtoKafkaDSVReaderStream\nINSERT INTO KafkaDSVReaderStream\nSELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]\nFROM KafkaReaderStream1;\n\nCREATE TARGET DSVDump USING FileWriter(\nname:testOuput,\nrolloverpolicy:'filesize:500M',\nfilename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8_RT_DSV')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaDSVReaderStream;\n\nCREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '0.8.0' (\n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V8jsonExistTopic',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO KafkaReaderStream2;\n\n\n\n\nCREATE CQ DSVtoKafkaJsonReaderStream\nINSERT INTO KafkaJsonReaderStream\nSELECT \ndata.get('Col1').textValue(),\ndata.get('Col2').textValue(),\ndata.get('Col3').textValue(),\ndata.get('Col4').textValue(),\ndata.get('Col5').textValue(),\ndata.get('Col6').textValue(),\ndata.get('Col7').textValue(),\ndata.get('Col8').textValue(),\ndata.get('Col9').textValue(),\ndata.get('Col10').textValue()\nFROM KafkaReaderStream2;\n\nCREATE TARGET JSONDump USING FileWriter(\nname:testOuput,\nrolloverpolicy:'filesize:500M',\nfilename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8_RT_JSON')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaJsonReaderStream;\n\nCREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '0.8.0' (\n        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',\n        Topic:'V8avroExistTopic',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'\n)\nOUTPUT TO KafkaReaderStream3;\n\n\n\nCREATE Stream KRTypedStream of AccessLogType;\n\nCREATE CQ DSVtoKafkaAvroReaderStream\nINSERT INTO KafkaAvroReaderStream\nSELECT    \ndata.get(\"Col1\").toString(), \ndata.get(\"Col2\").toString(), \ndata.get(\"Col3\").toString(), \ndata.get(\"Col4\").toString(),\ndata.get(\"Col5\").toString(), \ndata.get(\"Col6\").toString(), \ndata.get(\"Col7\").toString(), \ndata.get(\"Col8\").toString(),\ndata.get(\"Col9\").toString(), \ndata.get(\"Col10\").toString()\nFROM KafkaReaderStream3;\n\nCREATE TARGET AVRODump USING FileWriter(\nname:testOuput,\nrolloverpolicy:'filesize:500M',\nfilename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8_RT_AVRO')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaAvroReaderStream;\n\nEND APPLICATION KRApp;\nDEPLOY APPLICATION KRApp on any in default;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW ;\n\n create flow myagentflow;\n\nCREATE OR REPLACE SOURCE DBSource USING DatabaseReader  ( \n  Username: '@SOURCE_USER@',\n  Password_encrypted: false,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  Query: \"SELECT * FROM QATEST.OracToCql_alldatatypes\",\n  adapterName: 'DatabaseReader',\n  FetchSize: 100,\n  Password: '@SOURCE_PASS@'\n ) \nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE SOURCE DBSource2 USING DatabaseReader  ( \n  Username: '@SOURCE_USER@',\n  Password_encrypted: false,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  Query: \"SELECT * FROM QATEST.OracToCql_alldatatypes2\",\n  adapterName: 'DatabaseReader',\n  FetchSize: 100,\n  Password: '@SOURCE_PASS@'\n ) \nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE SOURCE DBSource3 USING DatabaseReader  ( \n  Username: '@SOURCE_USER@',\n  Password_encrypted: false,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  Query: \"SELECT * FROM QATEST.OracToCql_alldatatypes3\",\n  adapterName: 'DatabaseReader',\n  FetchSize: 100,\n  Password: '@SOURCE_PASS@'\n ) \nOUTPUT TO Oracle_ChangeDataStream;\n\nend flow myagentflow;\n\ncreate flow myserverflow;\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:100,Interval:0',\n  CommitPolicy: 'EventCount:100,Interval:0',\n  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',\n  Tables: '',\n  Password: 'cassandra',\n  Password_encrypted: false\n ) \nINPUT FROM Oracle_ChangeDataStream;\n\ncreate Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\nend flow myserverflow;\n\nEND APPLICATION DBRTOCW;\n\ndeploy application DBRTOCW on ALL in default with myagentflow on all in Agents, myserverflow on all in  default;\n\nstart DBRTOCW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nSTOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n\nCREATE SOURCE @APPNAME@_S USING OracleReader\n(\n\tUsername: 'qatest',\n\tPassword: 'qatest',\n\tConnectionURL: 'dockerhost:1521:xe',\n\tTables: 'qatest.test01',\n\tFetchSize: '1'\n)\nOUTPUT TO @APPNAME@_SS;\n\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(id,name) COLUMNMAP(id=id,col1=col11,col2=col12)',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1,Interval:10',\nStandardSQL:true\t\t\n) INPUT FROM @APPNAME@_ss;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop OracleReaderToDBWriter;\nundeploy application OracleReaderToDBWriter;\ndrop application OracleReaderToDBWriter cascade;\n\nCREATE APPLICATION  OracleReaderToDBWriter RECOVERY 1 MINUTE INTERVAL AUTORESUME MAXRETRIES 2 RETRYINTERVAL 60;\n\nCREATE FLOW Hz_Agent_flow;\n\nCreate Source Oraclesrc\n Using OracleReader\n(\n Username:'',\n Password:'',\n ConnectionURL:'',\n Tables:'',\n Fetchsize:1\n)\nOutput To OrcStrm;\n\nEND FLOW Hz_Agent_flow;\n\n\nCREATE TARGET OracleTrg USING DatabaseWriter(\nConnectionURL:'',\n  Username:'',\n  Password:'',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: ''\n) \nINPUT FROM OrcStrm;\n\nEND APPLICATION OracleReaderToDBWriter;\nDEPLOY APPLICATION OracleReaderToDBWriter with Hz_Agent_flow on any in AGENTS;\nstart application OracleReaderToDBWriter;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create Target @TARGET_NAME@ using FileWriter(\n  filename:'CompressedMerchant.gz',\n  directory:'/logs/',\n  rolloverpolicy:'EventCount:10000,Interval:30s'\n)\nformat using DSVFormatter (\n\n)\ninput from @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION EH;\nUNDEPLOY APPLICATION EH;\nDROP APPLICATION EH CASCADE;\nCREATE APPLICATION EH recovery 5 second interval;\ncreate flow AgentFlow;\nCREATE SOURCE EH_SOURCE USING MySQLReader (\n  Compression: true,\n  FetchSize: 1,\n  SendBeforeImage: true,\n  ConnectionURL: '@CONNECTION_URL@',\n  DatabaseName: 'waction',\n  Tables: 'waction.Test01',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  Username: '@SOURCE_USER@'\n)\nOUTPUT TO EH_SS;\nend flow AgentFlow;\ncreate flow serverFlow;\ncreate Target EH_TARGET using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_01',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n\tConsumerGroup:'test_01_cg',\n\tE1P:'true',\n\tOperationTimeoutMS:'200000',\n\tBatchPolicy:'Size:1000',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n)\nformat using jsonFormatter()\ninput from EH_SS;\nend flow serverFlow;\n\nEND APPLICATION EH;\n\n--DEPLOY APPLICATION EH;\ndeploy application eh with AgentFlow in Agents, ServerFlow in default;\n\nstart application EH;\n\nSTOP APPLICATION ER;\nUNDEPLOY APPLICATION ER;\nDROP APPLICATION ER CASCADE;\nCREATE APPLICATION ER;\nCREATE STREAM ER_SS1 OF Global.JsonNodeEvent;\n\nCREATE SOURCE ER_S1 USING AzureEventHubReader (\n    EventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_01',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',\n    ConsumerGroup: 'test_01_cg',\n\tstartSeqNo:'0'\n\t)\nPARSE USING jsonparser ()\nOUTPUT TO ER_SS1;\n\n\ncreate Target ER_t1 using FileWriter (\nfilename:'MYSQL_EH_ER_JSON_RT',\ndirectory:'FEATURE-DIR/logs/',\nrolloverpolicy:'eventcount:5000000'\n)\nformat using jsonFormatter (members:'data')\ninput from ER_SS1;\nend application ER;\ndeploy application ER;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop application DEV20814.ValidateFile;\nUndeploy application DEV20814.ValidateFile;\nDrop application DEV20814.ValidateFile cascade;\n\nCREATE APPLICATION ValidateFile;\n\nCREATE STREAM AlertFileStream OF Global.AlertEvent;\n\nCREATE SUBSCRIPTION FileAlert USING WebAlertAdapter( ) INPUT FROM AlertFileStream;\n\nCREATE STREAM jsonFileStream OF Global.JsonNodeEvent;\n\nCREATE  SOURCE readFromFile USING FileReader  (\n  blocksize: 64,\n  rolloverstyle: 'Default',\n  includesubdirectories: false,\n  directory:'@TEST-DATA-PATH@',\n  skipbom: true,\n  wildcard: 'Postgres*'\n )\n PARSE USING JSONParser  (\n )\nOUTPUT TO jsonFileStream ;\n\nCREATE OPEN PROCESSOR alertFileOP USING AlertGenerator\n(\n   messagePrefix:'File: ',\n   severity:'info',\n   flag:'notify'\n)\nINSERT INTO AlertFileStream\nFROM jsonFileStream;\n\nEND APPLICATION ValidateFile;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION TQLwithinTqlTester.TQLwithinTqlApp;\nUNDEPLOY APPLICATION TQLwithinTqlTester.TQLwithinTqlApp;\nDROP APPLICATION TQLwithinTqlTester.TQLwithinTqlApp CASCADE;\n\nCREATE APPLICATION TQLwithinTqlApp;\n\n@@FEATURE-DIR@/tql/TQLwithinTQL5.tql;\n\nEND APPLICATION TQLwithinTqlApp;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION orrs;\nUNDEPLOY APPLICATION orrs;\nDROP APPLICATION orrs CASCADE;\nCREATE APPLICATION orrs;\nCreate Source oraSource1 Using DatabaseReader\n(\n Username:'src_username',\n Password:'src_password',\n ConnectionURL: 'src_url',\n Tables:'QATEST.ORACLETOREDSHIFTIL1;QATEST.ORACLETOREDSHIFTIL2',\n FilterTransactionBoundaries:true,\n FetchSize:1000\n) Output To LCRStream1;\n\nCreate Source oraSource2 Using DatabaseReader\n(\n Username:'src_username',\n Password:'src_password',\n ConnectionURL: 'src_url',\n Tables:'QATEST.ORACLETOREDSHIFTIL3;QATEST.ORACLETOREDSHIFTIL4',\n FilterTransactionBoundaries:true,\n FetchSize:1000\n) Output To LCRStream2;\n\nCREATE TARGET RSTarget USING RedshiftWriter\n\t(\n\t  ConnectionURL: 'tgt_url',\n\t  Username: 'tgt_username',\n\t  Password: 'tgt_pwrd',\n\t  bucketname: 'bucket_name',\n\t  --accesskeyId: 'access_key',\n\t  --secretaccesskey: 'secret_access',\n\t  Tables: 'QATEST.%,QATEST.%',\n\t   S3IAMRole:'@IAMROLE@',\n\t  uploadpolicy:'eventcount:1000,interval:1m'\n\t) INPUT FROM LCRStream1;\n\nCREATE TARGET RSTarget2 USING RedshiftWriter\n\t(\n\t  ConnectionURL: 'tgt_url',\n\t  Username: 'tgt_username',\n\t  Password: 'tgt_pwrd',\n\t  bucketname: 'bucket_name',\n\t  --accesskeyId: 'access_key',\n\t  --secretaccesskey: 'secret_access',\n\t  Tables: 'QATEST.%,QATEST.%',\n\t   S3IAMRole:'@IAMROLE@',\n\t  Tables: 'QATEST.%,QATEST.%',\n\t  uploadpolicy:'eventcount:1000,interval:1m'\n\t) INPUT FROM LCRStream2;\n\nEND APPLICATION orrs;\nDEPLOY APPLICATION orrs;\nSTART APPLICATION orrs;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP TQLwithinTqlApp;\nUNDEPLOY APPLICATION TQLwithinTqlApp;\nDROP APPLICATION TQLwithinTqlApp CASCADE;\n\nCREATE APPLICATION TQLwithinTqlApp;\n\nCREATE source CsvDataSource USING FileReader (\n  directory:'@TEST-DATA-PATH@',\n\n  wildcard:'banks.csv',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header:Yes,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\n@@FEATURE-DIR@/tql/TQLTobeCalled.tql\n\n\nEND APPLICATION TQLwithinTqlApp;\nDEPLOY APPLICATION TQLwithinTqlApp;\nSTART TQLwithinTqlApp;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 28 with two sources, two jumping time-count windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jc5a9W  -> CQ1 -> WS\n--   S2 -> Jc6a11W -> CQ2 -> WS\n--\n\nSTOP Recov28Tester.RecovTest28;\nUNDEPLOY APPLICATION Recov28Tester.RecovTest28;\nDROP APPLICATION Recov28Tester.RecovTest28 CASCADE;\nCREATE APPLICATION RecovTest28 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream6Minutes p;\n\nEND APPLICATION RecovTest28;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop OracleToKudu;\nundeploy application OracleToKudu;\ndrop application OracleToKudu cascade;\n\nCREATE APPLICATION OracleToKudu;\nCreate Source oracSource\n Using OracleReader\n(\n Username:'@LOGMINER-UNAME@',\n Password:'@LOGMINER-PASSWORD@',\n ConnectionURL:'@LOGMINER-URL@',\n Tables:'@SOURCE_TABLES@',\n OnlineCatalog:true,\n FetchSize:1\n) Output To DataStream;\n\nCREATE TARGET WriteintoKudu using KuduWriter (\nkuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',\npkupdatehandlingmode:'@MODE@',\ntables: '@TARGET_TABLES@',\nbatchpolicy: 'EventCount:1,Interval:0')\nINPUT FROM DataStream;\n\nEND APPLICATION OracleToKudu;\ndeploy application OracleToKudu in default;\nstart OracleToKudu;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n\nCREATE SOURCE @APPNAME@_S USING OracleReader\n(\n\tUsername: 'qatest',\n\tPassword: 'qatest',\n\tConnectionURL: 'dockerhost:1521:xe',\n\tTables: 'qatest.e1ptest%',\n\tFetchSize: '1'\n)\nOUTPUT TO @APPNAME@_SS;\n\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'qatest.e1ptest%,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12)',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'Interval:2',\nStandardSQL:true,\noptimizedMerge:true\t\t\n) INPUT FROM @APPNAME@_ss;\n\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 1\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> WS\n--\n\nSTOP Recov1Tester.RecovTest1;\nUNDEPLOY APPLICATION Recov1Tester.RecovTest1;\nDROP APPLICATION Recov1Tester.RecovTest1 CASCADE;\nCREATE APPLICATION RecovTest1 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE WactionType (\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions\nINSERT INTO Wactions\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nEND APPLICATION RecovTest1;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE SOURCE @SOURCE_NAME@ USING Global.incREmEnTALBatchrEADer (\n  StartPosition: '@startPosition@',\n  ConnectionURL: '@sourceURL@',\n  Username: '@userName@',\n  Tables: '@tables@',\n  CheckColumn: '@checkColum@',\n  FetchSize: 1,\n  ReturnDateTimeAs: 'JODA',\n  Password: '@password@',\n  Password_encrypted: 'false',\n  DatabaseProviderType: 'Default',\n  ThreadPoolSize: 5,\n  pollingInterval: '2sec',\n  ConnectionPoolSize: 1 )\nOUTPUT TO @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application GCSWriterTest;\nundeploy application GCSWriterTest;\ndrop application GCSWriterTest cascade;\ncreate application GCSWriterTest recovery 1 second interval;\ncreate source GCS_SRC Using FileReader(\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'@FILENAME@',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'false'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\ncreate Target GCSTarget using GCSWriter(\n    bucketname:'@bucketname@',\n    objectname:'@objectname@',\n    projectId:'@project-id@',\n    uploadPolicy:'@UPLOAD-SIZE@',\n    ServiceAccountKey:'@file-path@'\n)\nformat using DSVFormatter (\n--members:'data'\n)\ninput from TypedCSVStream;\n\nend application GCSWriterTest;\ndeploy application GCSWriterTest on all in default;\nstart application GCSWriterTest;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 5 with Jumping window and partitioned on four node all server cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS\n--\n\nSTOP APPLICATION N4S4CR5Tester.N4S4CRTest5;\nUNDEPLOY APPLICATION N4S4CR5Tester.N4S4CRTest5;\nDROP APPLICATION N4S4CR5Tester.N4S4CRTest5 CASCADE;\nCREATE APPLICATION N4S4CRTest5 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN4S4CRTest5;\n\nCREATE SOURCE CsvSourceN4S4CRTest5 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN4S4CRTest5;\n\nCREATE FLOW DataProcessingN4S4CRTest5;\n\nCREATE TYPE CsvDataN4S4CRTest5 (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream OF CsvDataN4S4CRTest5 PARTITION BY merchantId;\n\nCREATE CQ CsvToDataN4S4CRTest5\nINSERT INTO DataStream\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE WactionsN4S4CRTest5 CONTEXT OF CsvDataN4S4CRTest5\nEVENT TYPES ( CsvDataN4S4CRTest5 )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWactionN4S4CRTest5\nINSERT INTO WactionsN4S4CRTest5\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND FLOW DataProcessingN4S4CRTest5;\n\nEND APPLICATION N4S4CRTest5;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 24 with two sources, two sliding time-count windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Sc5a9W  -> CQ1 -> WS\n-- S2 -> Sc6a11W -> CQ2 -> WS\n--\n\nSTOP Recov24Tester.RecovTest24;\nUNDEPLOY APPLICATION Recov24Tester.RecovTest24;\nDROP APPLICATION Recov24Tester.RecovTest24 CASCADE;\nCREATE APPLICATION RecovTest24 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2;\n\nEND APPLICATION RecovTest24;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@ RECOVERY 5 second interval;\nCREATE SOURCE @APPNAME@_src USING OracleReader ()\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE TARGET @APPNAME@_trgt USING AzureBlobWriter()\nformat using DSVFormatter ()\nINPUT FROM @APPNAME@_Stream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSVSource using FileReader (\ndirectory:'@TEST-DATA-PATH@',\nWildCard:'smallretaildata2M.csv',\npositionByEOF:false,\ncharset:'UTF-8'\n)\nparse using DSVParser (\nheader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\nstoreId String,\nnameId String,\ncity String,\nstate String\n\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],\ndata[1],\ndata[2],\ndata[3]\n\nFROM CsvStream;\n\ncreate Target t using FileWriter(\nfilename:'EventNCDefault',\ndirectory:'@FEATURE-DIR@/logs/',\nsequence:'00',\n--filelimit: '5',\nrolloverpolicy:'eventcount:-100'\n)\nformat using DSVFormatter (\n\n)\ninput from TypedCSVStream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetNegativeECDefault_actual.log') input from TypedCSVStream;\nend application DSV;\nDEPLOY APPLICATION DSV on any in default;\nSTART DSV;\ndeploy application DSV;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\ncreate flow @APPNAME@_agentflow;\n\nCREATE OR REPLACE SOURCE @SOURCE@ USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '85d7qFnwTW8=',\n  Password_encrypted: 'true',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO @STREAM@;\n\nend flow @APPNAME@_agentflow;\n\ncreate flow @APPNAME@_serverflow;\n\nCREATE OR REPLACE TARGET @TARGET@ USING @TARGET_ADAPTER@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n )\nINPUT FROM @STREAM@;\n\nend flow @APPNAME@_serverflow;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 5 with Jumping window and partitioned on two node cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS\n--\n\nSTOP APPLICATION KStreamN2S2CR5Tester.KStreamN2S2CRTest5;\nUNDEPLOY APPLICATION KStreamN2S2CR5Tester.KStreamN2S2CRTest5;\nDROP APPLICATION KStreamN2S2CR5Tester.KStreamN2S2CRTest5 CASCADE;\n\nDROP USER KStreamN2S2CR5Tester;\nDROP NAMESPACE KStreamN2S2CR5Tester CASCADE;\nCREATE USER KStreamN2S2CR5Tester IDENTIFIED BY KStreamN2S2CR5Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR5Tester;\nCONNECT KStreamN2S2CR5Tester KStreamN2S2CR5Tester;\n\nCREATE APPLICATION KStreamN2S2CRTest5 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionKStreamN2S2CRTest5;\n\nCREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');\nCREATE STREAM KafkaCsvStream using KafkaProps;\n\nCREATE SOURCE CsvSourceKStreamN2S2CRTest5 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nEND FLOW DataAcquisitionKStreamN2S2CRTest5;\n\nCREATE FLOW DataProcessingKStreamN2S2CRTest5;\n\nCREATE TYPE CsvDataTypeKStreamN2S2CRTest5 (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream OF CsvDataTypeKStreamN2S2CRTest5 PARTITION BY merchantId;\n\nCREATE CQ CsvToDataKStreamN2S2CRTest5\nINSERT INTO DataStream\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE WactionsKStreamN2S2CRTest5 CONTEXT OF CsvDataTypeKStreamN2S2CRTest5\nEVENT TYPES ( CsvDataTypeKStreamN2S2CRTest5 )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWactionKStreamN2S2CRTest5\nINSERT INTO WactionsKStreamN2S2CRTest5\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND FLOW DataProcessingKStreamN2S2CRTest5;\n\nEND APPLICATION KStreamN2S2CRTest5;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop tpcc;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\n\nCreate Source @SourceName@\n Using Ojet\n(\n Username:'@READER-UNAME@',\n Password:'@READER-PASSWORD@',\n ConnectionURL:'@CDC-READER-URL@',\n Tables: '@WATABLES-SRC@',\n FetchSize:1,\n QueueSize:25000,\n CommittedTransactions:true,\n Compression:true,\n CaptureDDL: true,\n SendBeforeImage:true\n) Output To @SRCINPUTSTREAM@;\n\n\ncreate Target @targetsys@ using SysOut(name:OrgData) input from DataStream;\n\nCREATE TARGET @targetName@ USING databasewriter(\n  Username: '@WRITER-UNAME@',\n  Password: '@WRITER-PASSWORD@',\n  ConnectionURL:'@WRITER-URL@',\n  BatchPolicy:'EventCount:1,Interval:1',\n  CommitPolicy:'EventCount:1,Interval:1',\n  Tables: '@WATABLES-SRC@,@WATABLES-TGT@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application ManyToManyADLSGen2;\nundeploy application ManyToManyADLSGen2;\ndrop application ManyToManyADLSGen2 cascade;\n\ncreate application ManyToManyADLSGen2 Recovery 5 second interval;\n\n\ncreate type csv_type_gen2(\nid String,\nname String,\nseq String\n);\n\ncreate type Order_type_gen2(\nid String,\nName String,\nCompany String\n);\n\n\n\ncreate source CSVSource_multi_gen2 using FileReader (\n        directory:'./Samples/AppData/',\n        WildCard:'Canon1000_All.csv',\n        positionByEOF:false,\n        charset:'UTF-8'\n)\nparse using DSVParser (\n        header:'no'\n)\nOUTPUT TO CsvStream_user_gen2;\n\ncreate source CSVSource2_gen2 using FileReader (\n        directory:'./Samples/AppData/',\n        WildCard:'portfolio.csv',\n        positionByEOF:false,\n        charset:'UTF-8'\n)\nparse using DSVParser (\n        header:'no'\n)\nOUTPUT TO CsvStream_gen2;\n\ncreate source CSVSource3_gen2 using FileReader (\n        directory:'./Samples/AppData/',\n        WildCard:'DataCenterData.csv',\n        positionByEOF:false,\n        charset:'UTF-8'\n)\nparse using DSVParser (\n        header:'no'\n)\nOUTPUT TO CsvStream_gen2;\n\n\nCREATE SOURCE OraSource1_gen2 USING OracleReader\n(\n Username:'miner',\n Password:'miner',\n ConnectionURL:'localhost:1521:xe',\n Tables:'QATEST.CUSTOMER1,QATEST.CUSTOMER2,QATEST.CUSTOMER3',\n OnlineCatalog:true,\n FetchSize:10000,\n QueueSize:2148,\n CommittedTransactions:false,\n Compression:true\n)\nOUTPUT TO OrdersStream_gen2;\n\nCREATE SOURCE OraSource2_gen2 USING OracleReader\n(\n Username:'miner',\n Password:'miner',\n ConnectionURL:'localhost:1521:xe',\n Tables:'QATEST.CUSTOMER4,QATEST.CUSTOMER5',\n OnlineCatalog:true,\n FetchSize:10000,\n QueueSize:2148,\n CommittedTransactions:false,\n Compression:true\n)\nOUTPUT TO OrdersStream_gen2;\n\n\ncreate Target ADLSGen2_tgt1 using ADLSGen2Writer(\n        accountname:'adlsgen2nntest',\n        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',\n        filesystemname:'qatest',\n        directory:'%@metadata(FileName)%',\n        filename:'event_data.csv',\n        uploadpolicy:'eventcount:5'\n)\nFORMAT USING JSONFormatter()\ninput from CsvStream_gen2; \n\ncreate Target ADLSGen2_tgt2 using ADLSGen2Writer(\n          accountname:'adlsgen2nntest',\n        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',\n        filesystemname:'qatestadls2',\n        directory:'%@metadata(TableName)%',\n        filename:'table.csv',\n        uploadpolicy:'filesize:10M',\n        compressiontype: 'true'\n)\nformat using DSVFormatter (\n)\ninput from OrdersStream_gen2; \n\ncreate stream UserdataStream_gen2 of Global.WAEvent;\n\nCreate CQ CQUser_gen2\ninsert into UserdataStream_gen2\nselect \nputuserdata (data1,'Fileowner',data[1]) from CsvStream_user_gen2 data1;\n\ncreate stream CSVTypedStream1_gen2 of csv_type_gen2;\ncreate stream CSVTypedStream2_gen2 of csv_type_gen2;\ncreate stream CSVTypedStream3_gen2 of csv_type_gen2;\n\nCREATE CQ cq1_gen2\nINSERT INTO CSVTypedStream1_gen2\nSELECT data[0],\ndata[1],\ndata[2]\nFROM UserdataStream_gen2\nWHERE USERDATA(UserdataStream_gen2,'Fileowner').toString() == 'Lorem';\n\nCREATE CQ cq2\nINSERT INTO CSVTypedStream2_gen2\nSELECT data[0],\ndata[1],\ndata[2]\nFROM UserdataStream_gen2\nWHERE USERDATA(UserdataStream_gen2,'Fileowner').toString() == 'doloremque';\n\nCREATE CQ cq3\nINSERT INTO CSVTypedStream3_gen2\nSELECT data[0],\ndata[1],\ndata[2]\nFROM UserdataStream_gen2\nWHERE USERDATA(UserdataStream_gen2,'Fileowner').toString() == 'accusantium';\n\ncreate Target ADLSGen2_tgt3 using ADLSGen2Writer(\n        accountname:'adlsgen2nntest',\n        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',\n        filesystemname:'qatestadls2',\n        directory:'1up_%name%',\n        filename:'many_event_data.csv',\n        uploadpolicy:'eventcount:74'\n)\nformat using XMLFormatter (\n  elementtuple: 'Eventname:name:id:seq:text=name',\n  formatterName: 'XMLFormatter',\n  rowdelimiter: '\\n',\n  rootelement: 'document'\n)\ninput from CSVTypedStream1_gen2; \n\ncreate Target ADLSGen2_tgt4 using ADLSGen2Writer(\n         accountname:'adlsgen2nntest',\n        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',\n        filesystemname:'qatestadls2',\n        directory:'2up_%name%',\n        filename:'many_event_data.csv',\n        uploadpolicy:'interval:10s'\n)\nformat using XMLFormatter (\n  elementtuple: 'Eventname:name:id:seq:text=name',\n  formatterName: 'XMLFormatter',\n  rowdelimiter: '\\n',\n  rootelement: 'document'\n)\ninput from CSVTypedStream2_gen2; \n\ncreate Target ADLSGen2_tgt5 using ADLSGen2Writer(\n        accountname:'adlsgen2nntest',\n        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',\n        filesystemname:'qatestadls2',\n        directory:'3up_%name%',\n        filename:'many_event_data.csv',\n        uploadpolicy:'eventcount:10,intervals:30s'\n)\nformat using XMLFormatter (\n  elementtuple: 'Eventname:name:id:seq:text=name',\n  formatterName: 'XMLFormatter',\n  rowdelimiter: '\\n',\n  rootelement: 'document'\n)\ninput from CSVTypedStream3_gen2; \n\n\ncreate stream OrderTypedStream1_gen2 of Order_type_gen2;\ncreate stream OrderTypedStream2_gen2 of Order_type_gen2;\ncreate stream OrderTypedStream3_gen2 of Order_type_gen2;\n\nCREATE CQ cq1_gen2_db\nINSERT INTO OrderTypedStream1_gen2\nSELECT data[0],\ndata[1],\ndata[2]\nFROM OrdersStream_gen2\nWHERE META(OrdersStream_gen2,'TableName').toString() == 'QATEST.CUSTOMER1';\n\nCREATE CQ cq2_gen2_db\nINSERT INTO OrderTypedStream2_gen2\nSELECT data[0],\ndata[1],\ndata[2]\nFROM OrdersStream_gen2\nWHERE META(OrdersStream_gen2,'TableName').toString() == 'QATEST.CUSTOMER2';\n\nCREATE CQ cq3_gen2_db\nINSERT INTO OrderTypedStream3_gen2\nSELECT data[0],\ndata[1],\ndata[2]\nFROM OrdersStream_gen2\nWHERE META(OrdersStream_gen2,'TableName').toString() == 'QATEST.CUSTOMER3';\n\n\ncreate Target ADLSGen2_tgt6 using ADLSGen2Writer(\n        accountname:'adlsgen2nntest',\n        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',\n        filesystemname:'qatestadls2',\n        directory:'Customer1',\n        filename:'many_event_data.csv',\n        uploadpolicy:'eventcount:10000'\n)\nformat using AvroFormatter (\n  formatAs: 'Default',\n  handler: 'com.webaction.proc.AvroFormatter',\n  formatterName: 'AvroFormatter',\n  schemaFileName: '@SCHEMA@1.avsc'\n)\ninput from OrderTypedStream1_gen2; \n\ncreate Target ADLSGen2_tgt7 using ADLSGen2Writer(\n       accountname:'adlsgen2nntest',\n        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',\n        filesystemname:'qatestadls2',\n        directory:'Customer2',\n        filename:'many_event_data.csv',\n        uploadpolicy:'eventcount:10000'\n)\nformat using AvroFormatter (\n  formatAs: 'Default',\n  handler: 'com.webaction.proc.AvroFormatter',\n  formatterName: 'AvroFormatter',\n  schemaFileName: '@SCHEMA@2.avsc'\n)\ninput from OrderTypedStream2_gen2; \n\ncreate Target ADLSGen2_tgt8 using ADLSGen2Writer(\n        accountname:'adlsgen2nntest',\n        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',\n        filesystemname:'qatestadls2',\n        directory:'Customer3',\n        filename:'many_event_data.csv',\n        uploadpolicy:'eventcount:10000'\n)\nformat using AvroFormatter (\n  formatAs: 'Default',\n  handler: 'com.webaction.proc.AvroFormatter',\n  formatterName: 'AvroFormatter',\n  schemaFileName: '@SCHEMA@3.avsc'\n)\ninput from OrderTypedStream3_gen2; \n\nend application ManyToManyADLSGen2;\n\ndeploy application ManyToManyADLSGen2;\nstart application ManyToManyADLSGen2;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop bankApp;\nundeploy application bankApp;\n\nalter application bankApp;\nCREATE OR REPLACE TYPE wsData  ( bankID java.lang.Integer KEY, \nnameOfBank java.lang.String\n);\n\nCREATE OR REPLACE WACTIONSTORE oneWS  CONTEXT OF wsData\nEVENT TYPES(wsData )\n@PERSIST-TYPE@;\n\n\n\nalter application bankApp recompile;\ndeploy application bankApp;\nstart application bankApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE PROPERTYVARIABLE Mode='sync';\nCREATE OR REPLACE PROPERTYVARIABLE BatchPolicy='Size:900000,Interval:1';\ncreate application KinesisTest;\ncreate source CSVSource using FileReader (\n\tdirectory:'/home/dz/src/product/Samples/AppData',\n\tWildCard:'posdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  companyName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0], data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate or replace Target t using KinesisWriter (\n\tregionName:'TARGET_REGION',\n\tstreamName:'TARGET_STREAM',\n\tBatchPolicy: '$BatchPolicy',\n    Mode: '$Mode'\t\n)\nformat using AvroFormatter (\n\tschemaFileName:'/Users/shikhar_nahar/Product/testwaevent.avsc'\n)\ninput from TypedCSVStream;\nend application KinesisTest;\ndeploy application KinesisTest in default;\nstart application KinesisTest;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\n\nCreate Source @SourceName1@\n Using OracleReader\n(\n Username:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@SourceConnectURL@',\n Tables:'@SourceTable@',\n _h_useClassic: false,\n Fetchsize:1000\n)\nOutput To @SRCINPUTSTREAM1@;\nCreate Source @SourceName2@\n Using OracleReader\n(\n Username:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@SourceConnectURL@',\n Tables:'@SourceTable@',\n _h_useClassic: false,\n Fetchsize:1000\n)\nOutput To @SRCINPUTSTREAM2@;\nCreate Source @SourceName3@\n Using OracleReader\n(\n Username:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@SourceConnectURL@',\n Tables:'@SourceTable@',\n _h_useClassic: false,\n Fetchsize:1000\n)\nOutput To @SRCINPUTSTREAM3@;\nCreate Source @SourceName4@\n Using OracleReader\n(\n Username:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@SourceConnectURL@',\n Tables:'@SourceTable@',\n _h_useClassic: false,\n Fetchsize:1000\n)\nOutput To @SRCINPUTSTREAM4@;\nCreate Source @SourceName5@\n Using OracleReader\n(\n Username:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@SourceConnectURL@',\n Tables:'@SourceTable@',\n _h_useClassic: false,\n Fetchsize:1000\n)\nOutput To @SRCINPUTSTREAM5@;\nCreate Source @SourceName6@\n Using OracleReader\n(\n Username:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@SourceConnectURL@',\n Tables:'@SourceTable@',\n _h_useClassic: false,\n Fetchsize:1000\n)\nOutput To @SRCINPUTSTREAM6@;\nCreate Source @SourceName7@\n Using OracleReader\n(\n Username:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@SourceConnectURL@',\n Tables:'@SourceTable@',\n _h_useClassic: false,\n Fetchsize:1000\n)\nOutput To @SRCINPUTSTREAM7@;\nCreate Source @SourceName8@\n Using OracleReader\n(\n Username:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@SourceConnectURL@',\n Tables:'@SourceTable@',\n _h_useClassic: false,\n Fetchsize:1000\n)\nOutput To @SRCINPUTSTREAM8@;\nCreate Source @SourceName9@\n Using OracleReader\n(\n Username:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@SourceConnectURL@',\n Tables:'@SourceTable@',\n _h_useClassic: false,\n Fetchsize:1000\n)\nOutput To @SRCINPUTSTREAM9@;\nCreate Source @SourceName10@\n Using OracleReader\n(\n Username:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@SourceConnectURL@',\n Tables:'@SourceTable@',\n _h_useClassic: false,\n Fetchsize:1000\n)\nOutput To @SRCINPUTSTREAM10@;\n\nCREATE TARGET @targetName1@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM1@;\nCREATE TARGET @targetName2@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM2@;\nCREATE TARGET @targetName3@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM3@;\nCREATE TARGET @targetName4@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM4@;\nCREATE TARGET @targetName5@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM5@;\nCREATE TARGET @targetName6@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM6@;\nCREATE TARGET @targetName7@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM7@;\nCREATE TARGET @targetName8@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM8@;\nCREATE TARGET @targetName9@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM9@;\nCREATE TARGET @targetName10@ USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',\n  Username:'@UN@',\n  Password:'@PWD@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: '@Tablemapping@'\n) INPUT FROM @SRCINPUTSTREAM10@;\n\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@\n Using OracleReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'@SourceTable@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\ncreate stream @stream1@ of Global.WAEvent;\n\nCREATE TYPE @type@(\n  id String,\n  name String  \n);\n\nCREATE STREAM @typeStream@ OF @type@;\n\nCREATE CQ @CQName1@\nINSERT INTO @typeStream@\nSELECT TO_STRING(p.data[0]), \n       TO_STRING(p.data[1])\nFROM @SRCINPUTSTREAM@ p;\n\ncreate Target @targetsys1@ using SysOut(name:TypeOut) input from @typeStream@;\n\ncreate cq @CQName2@ \ninsert into @stream1@\nselect convertTypedeventToWAevent(c, 'admin.@type@')\nfrom @typeStream@ c;\n\ncreate stream @CQOUTPUTSTREAM@ of Global.WAEvent;\n\nCREATE OR REPLACE CQ @CQName@ INSERT INTO @CQOUTPUTSTREAM@ select @FUNCTION@ from @stream1@  s ;\n\ncreate Target @targetsys2@ using SysOut(name:EventOut) input from @CQOUTPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( \n  ConnectionURL: '@TargetConnectionURL@',\n  Username: '@UserName@',\n  Password: '@Password@',\n  BatchPolicy: 'EventCount:1,Interval:1',\n  Tables: '@TargetTableMapping@'\n ) \nINPUT FROM @CQOUTPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@ USE EXCEPTIONSTORE TTL : '7d' ;\n\nCREATE SOURCE @APPNAME@_DBSource USING DatabaseReader (\n  Tables: '\"qatest\".\"dbo\".\"emp\"',\n  Username: 'qatest',\n  Password: '5wZ8jZNAU1dzU0bPbxhATA==',\n  FetchSize: 10000,\n  Password_encrypted: 'false',\n  QuiesceOnILCompletion: 'true',\n  DatabaseProviderType: 'SQLSERVER',\n  ConnectionURL: 'jdbc:sqlserver://localhost:1433;DatabaseName=qatest' )\nOUTPUT TO @APPNAME@_OutputStream;\n\nCREATE OR REPLACE TARGET @APPNAME@_Target USING Global.BigQueryWriter (\n  ColumnDelimiter: '|',\n  NullMarker: 'NULL',\n  streamingUpload: 'false',\n  projectId: 'striimqa-214712',\n  Encoding: 'UTF-8',\n  batchPolicy: 'eventcount:10,interval:60',\n  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30',\n  AllowQuotedNewLines: 'false',\n  CDDLAction: 'Process',\n  optimizedMerge: 'false',\n  TransportOptions: 'connectionTimeout=300, readTimeout=120',\n  adapterName: 'BigQueryWriter',\n  Mode: 'APPENDONLY',\n  ServiceAccountKey: 'Platform/UploadedFiles/google-gcs.json',\n  StandardSQL: 'true',\n  includeInsertId: 'true',\n  QuoteCharacter: '\\\"',\n  Tables: '\"qatest\".\"dbo\".\"%\",DEV_30875.%' )\nINPUT FROM @APPNAME@_OutputStream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Create Source @SOURCE_NAME@ Using MSJet\n(\n Username:'@READER-UNAME@',\n Password:'@READER-PASSWORD@',\n DatabaseName:'@DB_NAME@',\n ConnectionURL:'@CONN_URL@',\n Tables:@WATABLES@,\n compression:'@COMP@'\n)\nOUTPUT TO @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  --Password_encrypted: false\n )\nINPUT FROM @STREAM@_stream;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create or replace Target Quiesce_orcl_kwTARGET1 using KafkaWriter VERSION '2.1.0' (\nbrokerAddress:'$KafkaBrokerAddress',\nTopic:'Quiesce_orcl_kw_dsv_sync_CQ',\nParallelThreads:'2',\nPartitionKey:'@metadata(COMMIT_TIMESTAMP)',\nMode:'sync',\nKafkaConfig: 'batch.size=1048576;linger.ms=300000;')\nFORMAT USING dsvFormatter ()\ninput from Quiesce_orcl_kwss;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@;\n\nCREATE SOURCE @parquetsrc@ USING S3Reader (\n    bucketname:'',\n    objectnameprefix:'',\n     )\nPARSE USING ParquetParser (\n)\nOUTPUT TO @appname@Stream;\n\nCREATE CQ @appname@CQ\nINSERT INTO @appname@CqOut\n    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;\n\nCREATE OR REPLACE TARGET @filetarget@ USING FileWriter (\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:1,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING ParquetFormatter  (\nschemaFileName: 'ParquetFileSchema'\n)\nINPUT FROM @appname@CqOut;\n\ncreate Target @s3target@ using S3Writer(\n    bucketname:'',\n    objectname:'',\n    uploadpolicy:'',\n    foldername:''\n)\nformat using ParquetFormatter (\nschemaFileName: 'ParquetS3Schema'\n)\ninput from @appname@CqOut;\n\ncreate Target @blobtarget@ using AzureBlobWriter(\n\taccountname:'',\n\taccountaccesskey:'',\n\tcontainername:'',\n    blobname:'',\n\tfoldername:'',\n\tuploadpolicy:'EventCount:10,interval:5s'\n)\nformat using ParquetFormatter (\nschemaFileName: 'ParquetAzureSchema'\n)\nINPUT FROM @appname@CqOut;\n\nCREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (\n    bucketname:'',\n    objectname:'',\n    foldername:'',\n    projectId:'',\n    uploadPolicy:''\n)\nformat using ParquetFormatter (\nschemaFileName: 'ParquetGCSSchema'\n)\nINPUT FROM @appname@CqOut;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application XML;\ncreate source CSVSource using FileReader (\n\tdirectory:'Samples/AppData',\n\tWildCard:'posdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\n\tfilename:'posdata_XML',\n\trolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'\n)\nformat using XMLFormatter (\n\trootelement:'document',\n\telementtuple:'MerchantName:merchantid:text=merchantname'\n)\ninput from TypedCSVStream;\nend application XML;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create source @SOURCE_NAME@\nUSING MariaDbXpandReader\n(\n Username:'@UN@',\n Password:'@PWD@',\n ConnectionURL:'@ConnectionURL@',\n Tables:'@Tables@',\n FetchSize:10000,\n QueueSize:2048\n)Output To @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Canon Master Test, Unpartitioned\n-- Nicholas Keene, WebAction, Inc.\n--\n\n\nCREATE APPLICATION MasterUnpartitioned\nRECOVERY 5 SECOND INTERVAL\n;\n\nCREATE OR REPLACE TYPE N100k_JUc100_NoPersist_WS_Type  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY  \n );\n\nCREATE OR REPLACE WACTIONSTORE N100k_JUc100_NoPersist_WS  CONTEXT OF N100k_JUc100_NoPersist_WS_Type\n PERSIST NONE USING ( \n ) ;\n\nCREATE OR REPLACE TYPE R10_SPc10_WS_Type  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY  \n );\n\nCREATE OR REPLACE SOURCE N50k USING NumberSource ( \n  lowValue: '1',\n  highValue: '500000',\n  delayMillis: '0',\n  delayNanos: '0',\n  repeat: 'false'\n ) \nOUTPUT TO NumberStream2_Out;\n\nCREATE OR REPLACE SOURCE N100k USING NumberSource ( \n  lowValue: '1',\n  highValue: '1000000',\n  delayMillis: '0',\n  delayNanos: '0',\n  repeat: 'false'\n ) \nOUTPUT TO N100k_Out;\n\nCREATE OR REPLACE TYPE ComplexNumberType  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY , \nvalueAsDateTime org.joda.time.DateTime , \nvalueMod10 java.lang.Long , \nvalueMod7 java.lang.Long , \nvalueMod13 java.lang.Long  \n );\n\nCREATE OR REPLACE TYPE R10_JPc5_x7_WS_Type  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY  \n );\n\nCREATE OR REPLACE TYPE N100k_JUc100_SUc10_WS  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY  \n );\n\nCREATE OR REPLACE TYPE N100K_N50K_TwoInputs_Type  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY  \n );\n\nCREATE OR REPLACE WACTIONSTORE N100K_N50K_TwoInputs  CONTEXT OF N100K_N50K_TwoInputs_Type\n@PERSIST-TYPE@\n\nCREATE OR REPLACE TYPE N100K_TwoInputs_WS_Type  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY  \n );\n\nCREATE OR REPLACE WACTIONSTORE N100K_TwoInputs_WS  CONTEXT OF N100K_TwoInputs_WS_Type\n@PERSIST-TYPE@\n\nCREATE OR REPLACE TYPE N100k_JUc100_M_JUc200_JUc15_WS_Type  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY  \n );\n\nCREATE OR REPLACE WACTIONSTORE N100k_JUc100_M_JUc200_JUc15_WS  CONTEXT OF N100k_JUc100_M_JUc200_JUc15_WS_Type\n@PERSIST-TYPE@\n\nCREATE OR REPLACE TYPE N100k_JUc100_Dupe_Type  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY  \n );\n\nCREATE OR REPLACE WACTIONSTORE N100k_JUc100_Dupe  CONTEXT OF N100k_JUc100_Dupe_Type\n@PERSIST-TYPE@\n\nCREATE OR REPLACE TYPE N100k_JUc100_SUc10_WS_Type  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY , \ncount java.lang.Long  \n );\n\nCREATE OR REPLACE WACTIONSTORE N100k_JUc100_SUc10_WS  CONTEXT OF N100k_JUc100_SUc10_WS_Type\n@PERSIST-TYPE@\n\nCREATE OR REPLACE TYPE N100k_JUc100_SUc10na_Type  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY  \n );\n\nCREATE OR REPLACE TYPE ComplexNumberType2  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY , \nvalueAsTimeStamp org.joda.time.DateTime , \nvalueMod10 java.lang.Long , \nvalueMod7 java.lang.Long , \nvalueMod13 java.lang.Long  \n );\n\nCREATE OR REPLACE TYPE Type1  ( f java.lang.String KEY  \n );\n\nCREATE OR REPLACE TYPE N50k_JUc100_WS_Type  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY  \n );\n\nCREATE OR REPLACE WACTIONSTORE N50k_JUc100_WS  CONTEXT OF N50k_JUc100_WS_Type\n@PERSIST-TYPE@\n\nCREATE OR REPLACE TYPE Type2  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY , \nvalueAsDateTime org.joda.time.DateTime , \nvalueMod10 java.lang.Long , \nvalueMod3 java.lang.Long , \nvalueMod7 java.lang.Long  \n );\n\nCREATE OR REPLACE TYPE N100k_JUc100_JUc10_WS_Type  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY  \n );\n\nCREATE OR REPLACE WACTIONSTORE N100k_JUc100_JUc10_WS  CONTEXT OF N100k_JUc100_JUc10_WS_Type\n@PERSIST-TYPE@\n\nCREATE OR REPLACE TYPE OneNumberType  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY  \n );\n\nCREATE OR REPLACE STREAM N100k_Stream OF OneNumberType;\n\nCREATE OR REPLACE CQ N100k_Convert \nINSERT INTO N100k_Stream\nSELECT TO_DATE(data[0]), data[1]\n \t \nFROM N100k_Out;\n\nCREATE OR REPLACE JUMPING WINDOW N100k_JUc100 OVER N100k_Stream KEEP 100 ROWS;\n\nCREATE OR REPLACE CQ N100k_JUc100_Pull_Dupe \nINSERT INTO N100k_JUc100_Dupe\nSELECT FIRST(w.timestamp), FIRST(w.value)\n \t \nFROM N100k_JUc100 w;\n\nCREATE OR REPLACE CQ N100k_JUc100_Pull3 \nINSERT INTO N100k_JUc100_NoPersist_WS\nSELECT FIRST(w.timestamp), FIRST(w.value)\n \t \nFROM N100k_JUc100 w;\n\nCREATE OR REPLACE CQ N100k_Merge_JUc100_Pull \nINSERT INTO N100K_TwoInputs_WS\nSELECT FIRST(w.timestamp), FIRST(w.value)\n \t \nFROM N100k_JUc100 w;\n\nCREATE OR REPLACE JUMPING WINDOW N100k_JUc200 OVER N100k_Stream KEEP 200 ROWS;\n\nCREATE OR REPLACE CQ N100K_JUc200_TwoInputs_Pull \nINSERT INTO N100K_N50K_TwoInputs\nSELECT FIRST(w.timestamp), FIRST(w.value)\n \t \nFROM N100k_JUc200 w;\n\nCREATE OR REPLACE STREAM N50k_Stream OF OneNumberType;\n\nCREATE OR REPLACE JUMPING WINDOW N50k_JUc100 OVER N50k_Stream KEEP 100 ROWS;\n\nCREATE OR REPLACE CQ N50K_JUc100_TwoInputs_Pull \nINSERT INTO N100K_N50K_TwoInputs\nSELECT FIRST(w.timestamp), FIRST(w.value)\n \t \nFROM N50k_JUc100 w;\n\nCREATE OR REPLACE CQ N50k_JUc100_Pull \nINSERT INTO N50k_JUc100_WS\nSELECT FIRST(w.timestamp), FIRST(w.value)\n \t \nFROM N50k_JUc100 w;\n\nCREATE OR REPLACE CQ N50k_Convert \nINSERT INTO N50k_Stream\nSELECT TO_DATE(data[0]), data[1]\n \t \nFROM NumberStream2_Out;\n\nCREATE OR REPLACE STREAM N100k_JUc100_Out OF OneNumberType;\n\nCREATE OR REPLACE JUMPING WINDOW N100k_JUc100_JUc10 OVER N100k_JUc100_Out KEEP 10 ROWS;\n\nCREATE OR REPLACE CQ N100k_JUc100_JUc10_Pull \nINSERT INTO N100k_JUc100_JUc10_WS\nSELECT FIRST(w.timestamp), FIRST(w.value)\n \t \nFROM N100k_JUc100_JUc10 w;\n\nCREATE OR REPLACE CQ N100K_TwoInputs_CQ2 \nINSERT INTO N100K_TwoInputs_WS\nSELECT FIRST(w.timestamp), FIRST(w.value)\n \t \nFROM N100k_JUc100_JUc10 w;\n\nCREATE OR REPLACE CQ N100k_JUc100_Pull2 \nINSERT INTO N100k_JUc100_Out\nSELECT FIRST(w.timestamp), FIRST(w.value)\n \t \nFROM N100k_JUc100 w;\n\nCREATE OR REPLACE WINDOW N100k_JUc100_SUc10 OVER N100k_JUc100_Out KEEP 10 ROWS;\n\nCREATE OR REPLACE CQ N100k_JUc100_SUc10_Pull \nINSERT INTO N100k_JUc100_SUc10_WS\nSELECT FIRST(w.timestamp), FIRST(w.value), COUNT(w)\n \t \nFROM N100k_JUc100_SUc10 w;\n\nCREATE OR REPLACE STREAM N100k_JUc100_M_JUc200 OF OneNumberType;\n\nCREATE OR REPLACE JUMPING WINDOW N100k_JUc100_M_JUc200_JUc15 OVER N100k_JUc100_M_JUc200 KEEP 15 ROWS;\n\nCREATE OR REPLACE CQ N100k_JUc100_M_JUc200_JUc15_Pull \nINSERT INTO N100k_JUc100_M_JUc200_JUc15_WS\nSELECT FIRST(w.timestamp), FIRST(w.value)\n \t \nFROM N100k_JUc100_M_JUc200_JUc15 w;\n\nCREATE OR REPLACE CQ N100k_Merge_JUc200 \nINSERT INTO N100k_JUc100_M_JUc200\nSELECT FIRST(w.timestamp), FIRST(w.value)\n \t \nFROM N100k_JUc200 w;\n\nCREATE OR REPLACE CQ N100k_Merge_JUc100 \nINSERT INTO N100k_JUc100_M_JUc200\nSELECT FIRST(w.timestamp), FIRST(w.value)\n \t \nFROM N100k_JUc100 w;\n\nCREATE OR REPLACE STREAM N50K_x100_SUc5_Stream OF OneNumberType;\n\nCREATE OR REPLACE WINDOW N50K_x100_SUc5_SUc10 OVER N50K_x100_SUc5_Stream KEEP 10 ROWS;\n\nCREATE OR REPLACE STREAM N50K_x100_Stream OF OneNumberType;\n\nCREATE OR REPLACE WINDOW N50K_x100_SUc5 OVER N50K_x100_Stream KEEP 5 ROWS;\n\nCREATE OR REPLACE CQ N50K_x100_SUc5_Stream_Pull2 \nINSERT INTO N50K_x100_SUc5_Stream\nSELECT FIRST(w.timestamp), FIRST(w.value)\n \t \nFROM N50K_x100_SUc5 w;\n\nCREATE OR REPLACE CQ N50K_x100_Pop \nINSERT INTO N50K_x100_Stream\nSELECT FIRST(w.timestamp), FIRST(w.value)\n \t \nFROM N50k_Stream w\nWHERE (w.value % 100) == 0;\n\nCREATE OR REPLACE TYPE R10_JPc10_WS_Type  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY  \n );\n\nCREATE OR REPLACE TYPE N100k_JUc100_WS_Type  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY  \n );\n\nCREATE OR REPLACE WACTIONSTORE N100k_JUc100_WS  CONTEXT OF N100k_JUc100_WS_Type\n@PERSIST-TYPE@\n\nCREATE OR REPLACE CQ N100k_JUc100_Pull \nINSERT INTO N100k_JUc100_WS\nSELECT FIRST(w.timestamp), FIRST(w.value)\n \t \nFROM N100k_JUc100 w;\n\nCREATE OR REPLACE TYPE ComplexNumberType3  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY , \nvalueAsDateTime org.joda.time.DateTime , \nvalueMod10 java.lang.Long , \nvalueMod7 java.lang.Long , \nvalueMod3 java.lang.Long  \n );\n\nCREATE OR REPLACE TYPE N50K_x100_SUc5_WS_Type  ( timestamp org.joda.time.DateTime , \nvalue java.lang.Long KEY  \n );\n\nCREATE OR REPLACE WACTIONSTORE N50K_x100_SUc5_WS  CONTEXT OF N50K_x100_SUc5_WS_Type\n@PERSIST-TYPE@\n\nCREATE OR REPLACE CQ N50K_x100_SUc5_Pull \nINSERT INTO N50K_x100_SUc5_WS\nSELECT FIRST(w.timestamp), FIRST(w.value)\n \t \nFROM N50K_x100_SUc5 w;\n\n\n\n\n\nEND APPLICATION MasterUnpartitioned;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION @AppName@;\n\nCREATE OR REPLACE SOURCE CP_Oracle_source USING OracleReader (\n  ConnectionURL: '',\n  Tables: '',\n  Username: '',\n  Password: '',\n  Fetchsize: 1 )\nOUTPUT TO CP_EndToEnd_SF_Adapter_Stream;\n\nCREATE OR REPLACE TARGET CP_SF_Target USING Global.SnowflakeWriter (\n  connectionProfileName: '',\n  streamingUpload: 'false',\n  StreamingConfiguration: 'MaxParallelRequests=5, MaxRequestSizeInMB=5, MaxRecordsPerRequest=10000',\n  useConnectionProfile: 'true',\n  externalStageConnectionProfileName: '',\n  uploadPolicy: 'eventcount:10000,interval:5m',\n  Tables: 'QATEST.Test_CP,SANJAYPRATAP.SAMPLESCHEMA.SAMPLE_PK')\nINPUT FROM CP_EndToEnd_SF_Adapter_Stream;\n\nEND APPLICATION @AppName@;\nDeploy application @AppName@;\nStart application @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE @parquetsrc@ USING FileReader (\nwildcard: '',\ndirectory: '',\npositionbyeof: false )\nPARSE USING ParquetParser (\n)\nOUTPUT TO @appname@Stream;\n\nCREATE CQ @appname@CQ\nINSERT INTO @appname@CqOut\n    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;\n\nCREATE OR REPLACE TARGET @filetarget@ USING FileWriter (\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:1,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING ParquetFormatter  (\nschemaFileName: ''\n)\nINPUT FROM @appname@CqOut;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n--create flow AgentFlow;\nCREATE SOURCE @APPNAME@_S USING MySqlReader\n(\n  Compression: false,\n  FetchSize: 1,\n  SendBeforeImage: true,\n  ConnectionURL: 'mysql://localhost:3306',\n  DatabaseName: 'waction',\n  Tables: 'waction.test01',\n  Password: 'w@ct10n',\n  Password_encrypted: 'false',\n  Username: 'root',\n  connectionRetryPolicy:'retryInterval=1,maxRetries=3'\n)\nOUTPUT TO @APPNAME@_SS;\n--end flow AgentFlow;\n--create flow serverFlow;\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'waction.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1000,Interval:30',\nStandardSQL:true,\noptimizedMerge:true\t\n) INPUT FROM @APPNAME@_ss;\n\n--end flow serverFlow;\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\n--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 1 on two node cluster with Kafka Stream\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> WS\n--\n\nSTOP APPLICATION KStreamN2S2CR1Tester.KStreamN2S2CRTest1;\nUNDEPLOY APPLICATION KStreamN2S2CR1Tester.KStreamN2S2CRTest1;\nDROP APPLICATION KStreamN2S2CR1Tester.KStreamN2S2CRTest1 CASCADE;\n\nDROP USER KStreamN2S2CR1Tester;\nDROP NAMESPACE KStreamN2S2CR1Tester CASCADE;\nCREATE USER KStreamN2S2CR1Tester IDENTIFIED BY KStreamN2S2CR1Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR1Tester;\nCONNECT KStreamN2S2CR1Tester KStreamN2S2CR1Tester;\n\nCREATE APPLICATION KStreamN2S2CRTest1 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionKStreamN2S2CRTest1;\n\nCREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');\nCREATE STREAM KafkaCsvStream using KafkaProps;\n\nCREATE SOURCE CsvSourceKStreamN2S2CRTest1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nEND FLOW DataAcquisitionKStreamN2S2CRTest1;\n\nCREATE FLOW DataProcessingKStreamN2S2CRTest1;\n\nCREATE TYPE WactionTypeKStreamN2S2CRTest1 (\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE WactionsKStreamN2S2CRTest1 CONTEXT OF WactionTypeKStreamN2S2CRTest1\nEVENT TYPES ( WactionTypeKStreamN2S2CRTest1 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsKStreamN2S2CRTest1\nINSERT INTO WactionsKStreamN2S2CRTest1\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream;\n\nEND FLOW DataProcessingKStreamN2S2CRTest1;\n\nEND APPLICATION KStreamN2S2CRTest1;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP NamedQTester.NamedQueryApp;\nUNDEPLOY APPLICATION NamedQTester.NamedQueryApp;\nDROP APPLICATION NamedQTester.NamedQueryApp cascade;\n\nCREATE APPLICATION NamedQueryApp;\n\nCREATE TYPE cacheType( EventID String, \n\t\t\tWord String, \n\t\t\tdatetime Datetime);\n\nCREATE CACHE adhcache using CSVReader (\n  directory: '@TEST-DATA-PATH@/',\n  wildcard: 'Canon1000.csv',\n  header: Yes,\n  columndelimiter: ',',\n  trimquote:false\n) QUERY (keytomap:'EventID') OF cacheType;\n\n\n\nEND APPLICATION NamedQueryApp;\nDEPLOY APPLICATION NamedQueryApp;\nSTART APPLICATION NamedQueryApp;\n\nDROP NAMEDQUERY NamedQTester.nqone;\nDROP NAMEDQUERY NamedQTester.nqtwo;\nDROP NAMEDQUERY NamedQTester.nqthree;\nCREATE NAMEDQUERY nqone select * from adhcache;\nCREATE NAMEDQUERY nqtwo select * from adhcache;\nCREATE NAMEDQUERY nqthree select * from adhcache;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@1;\nundeploy application @APPNAME@1;\nstop application @APPNAME@2;\nundeploy application @APPNAME@2;\n\nDROP STREAM @APPNAME@_STREAM;\nDROP APPLICATION @APPNAME@1 CASCADE;\nDROP APPLICATION @APPNAME@2 CASCADE;\n\ndrop propertyset MysqlToMysqlPlatfm_App_KafkaPropset;\ndrop stream  MysqlToMysqlPlatfm_Stream CASCADE;\n\n\n--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\n--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;\n\t\t\t\t\t\nCREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';\ncreate application @APPNAME@1 recovery 5 SECOND INTERVAL;\n\ncreate or replace stream @STREAM@2 of Global.waevent;\n\nCREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using MySQLReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNEC4TION_URL@',\n  Tables: '$table1',\n  adapterName: 'MySQLReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  StartSCN: 'null',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_AGENTFLOW1;\n\nCREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using MySQLReader( \n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@TABLENAME@2',\n  adapterName: 'MySQLReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  StartSCN: 'null',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_SERVERFLOW1;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n ) \nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;\nCREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x\n WHERE META(x,'TableName').toString() == 'WACTION.MYSQLTOMYSQLPLATFM_SOURCE4';\n\nCREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n ) \nINPUT FROM @STREAM@2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(\n  name: 'CDDL',\n  filename:'WAEventDump.log'\n)INPUT FROM @STREAM@;\n\nCREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;\n\nEND APPLICATION @APPNAME@1;\n\n\n\nCREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';\nCREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;\n\ncreate or replace stream @STREAM@3 of Global.waevent;\n\nCREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using MySQLReader( \n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '$table2',\n  adapterName: 'MySQLReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  StartSCN: 'null',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\n\nEND FLOW @APPNAME@_AGENTFLOW2;\n\nCREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using MySQLReader(\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'WACTION.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@TABLENAME@4',\n  adapterName: 'MySQLReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OfflineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  StartSCN: 'null',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream',\n  _h_ReturnDateTimeAs:'ZonedDateTime',\n  _h_useClassic: true\n)OUTPUT TO @STREAM@;\nEND FLOW @APPNAME@_SERVERFLOW2;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n ) \nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;\nCREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y\n WHERE META(y,'TableName').toString() == 'WACTION.MYSQLTOMYSQLPLATFM_SOURCE2';\n\nCREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1',\n  CommitPolicy: 'EventCount:1',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  Password_encrypted: false\n ) \nINPUT FROM @STREAM@3;\n\nCREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;\n\nEND APPLICATION @APPNAME@2;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application HTTPtest;\ncreate source DSVCSVSource using FileReader (\n\tdirectory:'@TEST-DATA-PATH@',\n\tWildCard:'customerdetails.csv',\n\tcharset: 'UTF-8',\n\tpositionByEOF:false\n)\nparse using DSVParser (\n\theader:'no'\n)\nOUTPUT TO DSVCsvStream;\ncreate Target DSVDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/customerdetails') input from DSVCsvStream;\nend application HTTPtest;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop Quiesce_IL;\nundeploy application Quiesce_IL;\ndrop application Quiesce_IL cascade;\nCREATE APPLICATION Quiesce_IL USE EXCEPTIONSTORE TTL : '7d' ;\nCREATE FLOW Quiesce_IL_flow;\nCreate Source Quiesce_IL_Oraclesrc Using databasereader(\n Username:'@USERNAME@',\n Password:'@PASSWORD@',\n ConnectionURL:'@CONNECTION_URL@',\n Tables:'QATEST.QUIESCE_TABLE1;QATEST.QUIESCE_TABLE2',\n QuiesceOnILCompletion: 'true',\n _h_fetchexactrowcount: 'true'\n)\nOutput To Quiesce_IL_OrcStrm;\nEND FLOW Quiesce_IL_flow;\n\nCREATE TARGET Quiesce_IL_BigQueryTrg USING BigQueryWriter (\n  serviceAccountKey: '@SERVICEACCOUNTKEY@',\n  projectId:'@PROJECTID@',\n  Tables:'QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE1;QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE2'\n)\n\nINPUT FROM Quiesce_IL_OrcStrm;\nEND APPLICATION Quiesce_IL;\nDEPLOY APPLICATION Quiesce_IL;\nstart application Quiesce_IL;\n\nstop Quiesce_CDC;\nundeploy application Quiesce_CDC;\ndrop application Quiesce_CDC cascade;\nCREATE APPLICATION Quiesce_CDC recovery 1 second interval USE EXCEPTIONSTORE TTL : '7d' ;\nCREATE stream Quiesce_CDC_OrcStrm of global.waevent persist using Global.DefaultKafkaProperties;\nCREATE FLOW Quiesce_CDC_flow;\nCreate Source Quiesce_CDC_Oraclesrc Using oraclereader(\n Username:'@USERNAME@',\n Password:'@PASSWORD@',\n ConnectionURL:'@CONNECTION_URL@',\n Tables:'QATEST.QUIESCE_TABLE1;QATEST.QUIESCE_TABLE2',\n _h_fetchexactrowcount: 'true'\n)\nOutput To Quiesce_CDC_OrcStrm;\nEND FLOW Quiesce_CDC_flow;\nEND APPLICATION Quiesce_CDC;\nDEPLOY APPLICATION Quiesce_CDC;\nstart application Quiesce_CDC;\n\nstop Quiesce_CDC_BQ_TARGET;\nundeploy application Quiesce_CDC_BQ_TARGET;\ndrop application Quiesce_CDC_BQ_TARGET cascade;\nCREATE APPLICATION Quiesce_CDC_BQ_TARGET recovery 1 second interval USE EXCEPTIONSTORE TTL : '7d' ;\nCREATE TARGET Quiesce_CDC_BigQueryTrg USING BigQueryWriter (\n  serviceAccountKey: '@SERVICEACCOUNTKEY@',\n  projectId:'@PROJECTID@',\n  BatchPolicy:'Interval:10',\n  _h_maxParallelStreamingRequests: '10',\n  Tables:'QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE1;QATEST.QUIESCE_TABLE1,@TARGET@.QUIESCE_TABLE2'\n)\nINPUT FROM Quiesce_CDC_OrcStrm;\nEND APPLICATION Quiesce_CDC_BQ_TARGET;\nDEPLOY APPLICATION Quiesce_CDC_BQ_TARGET;\nstart application Quiesce_CDC_BQ_TARGET;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop application @APPNAME@Apps2;\nundeploy application @APPNAME@Apps2;\ndrop application @APPNAME@Apps2 cascade;\n\n\nstop application @APPNAME@Apps3;\nundeploy application @APPNAME@Apps3;\ndrop application @APPNAME@Apps3 cascade;\n\n\n\nstop application @APPNAME@Apps4;\nundeploy application @APPNAME@Apps4;\ndrop application @APPNAME@Apps4 cascade;\n\n\n\nstop application @APPNAME@Apps1;\nundeploy application @APPNAME@Apps1;\ndrop application @APPNAME@Apps1 cascade;\n\nCREATE OR REPLACE PROPERTYSET @APPNAME@Apps_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9099', kafkaversion:'0.11');\n\nCREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream1 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;\nCREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream3 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;\nCREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream4 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;\n\n\n--**********************Application 1*******************\n-- with 2 source flow \n-- <sourceflow1>source1->PS1<sourceflow1>\n-- <sourceflow2>Source2->Inmemory1<sourceflow2>\n-- Inmemomry1->cq1->PS1\n\nCREATE APPLICATION @APPNAME@Apps1 RECOVERY 5 SECOND INTERVAL;\ncreate flow @APPNAME@serverflow;\nCREATE OR REPLACE SOURCE @APPNAME@AppsDBSource1 USING OracleReader  ( \n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECT_URL@',\n  Tables: '@SOURCE_TABLE@1',\n  adapterName: 'OracleReader',\n  Password: '@PASSWORD@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  StartSCN: 'null',\n  ReaderType: 'LogMiner',\n  Username: '@USERNAME@',\n  OutboundServerProcessName: 'WebActionXStream'\n ) \nOUTPUT TO @APPNAME@Apps_PS_Stream1;\nend flow @APPNAME@serverflow;\n\ncreate flow @APPNAME@serverflow2;\nCREATE OR REPLACE STREAM @APPNAME@Apps_PS_Stream2 OF Global.waevent persist using  @APPNAME@Apps_KafkaPropset;\nCREATE OR REPLACE SOURCE @APPNAME@AppsDBSource2 USING OracleReader  ( \n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECT_URL@',\n  Tables: '@SOURCE_TABLE@2;@SOURCE_TABLE@3',\n  adapterName: 'OracleReader',\n  Password: '@PASSWORD@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  StartSCN: 'null',\n  ReaderType: 'LogMiner',\n  Username: '@USERNAME@',\n  OutboundServerProcessName: 'WebActionXStream'\n ) \nOUTPUT TO @APPNAME@Apps_PS_InmemoryStream1;\nend flow @APPNAME@serverflow2;\n\nCREATE CQ @APPNAME@cq_Inmemory1\nINSERT INTO @APPNAME@Apps_PS_Stream2\nSELECT *\nFROM @APPNAME@Apps_PS_InmemoryStream1;\n\nend application @APPNAME@Apps1;\n\n-- ********************Application 2***********************\n-- PS1->Target1\n\nCREATE APPLICATION @APPNAME@Apps2 RECOVERY 5 SECOND INTERVAL;\nCREATE OR REPLACE TARGET @APPNAME@Apps2DBTarget1 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TGT_USERNAME@',\n  BatchPolicy: 'EventCount:5,Interval:0',\n  CommitPolicy: 'EventCount:5,Interval:0',\n  ConnectionURL: '@CONNECT_URL@',\n  Tables: '@SOURCE_TABLE@1,@TARGET_TABLE@1',\n  Password: '@TGT_PASSWORD@',\n  Password_encrypted: false\n ) \nINPUT FROM @APPNAME@Apps_PS_Stream1;\nEND APPLICATION @APPNAME@Apps2;\n\n-- ********************Application 3**************************\n--WITH 3 TARGET FLOW\n-- 1. <targetflow1> cq->ps3->target2 <targetflow1>\n-- 2. cq->ps4 <targetflow2>ps4->target3 <targetflow2>\n-- 3. <targetflow3> ps2 -> target4 <targetflow3>\n\nCREATE APPLICATION @APPNAME@Apps3 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW @APPNAME@TARGETFLOW1;\nCREATE CQ @APPNAME@cq_ps1\nINSERT INTO @APPNAME@Apps_PS_Stream3\nSELECT *\nFROM @APPNAME@Apps_PS_Stream2\nWHERE META(@APPNAME@Apps_PS_Stream2,'TableName').toString() == '@SOURCE_TABLE@2';\nCREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget2 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TGT_USERNAME@',\n  BatchPolicy: 'EventCount:100,Interval:0',\n  CommitPolicy: 'EventCount:100,Interval:0',\n  ConnectionURL: '@CONNECT_URL@',\n  Tables: '@SOURCE_TABLE@2,@TARGET_TABLE@2',\n  Password: '@TGT_PASSWORD@',\n  Password_encrypted: false\n ) \nINPUT FROM @APPNAME@Apps_PS_Stream3;\nEND FLOW @APPNAME@TARGETFLOW1;\n\nCREATE CQ @APPNAME@cq_ps2\nINSERT INTO @APPNAME@Apps_PS_Stream4\nSELECT *\nFROM @APPNAME@Apps_PS_Stream2\nWHERE META(@APPNAME@Apps_PS_Stream2,'TableName').toString() == '@SOURCE_TABLE@3';\n\nCREATE FLOW @APPNAME@TARGETFLOW2;\nCREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget3 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TGT_USERNAME@',\n  BatchPolicy: 'EventCount:1000,Interval:0',\n  CommitPolicy: 'EventCount:1000,Interval:0',\n  ConnectionURL: '@CONNECT_URL@',\n  Tables: '@SOURCE_TABLE@3,@TARGET_TABLE@3',\n  Password: '@TGT_PASSWORD@',\n  Password_encrypted: false\n ) \nINPUT FROM @APPNAME@Apps_PS_Stream4;\nEND FLOW @APPNAME@TARGETFLOW2;\n\nCREATE FLOW @APPNAME@TARGETFLOW3;\nCREATE OR REPLACE TARGET @APPNAME@Apps3DBTarget4 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TGT_USERNAME@',\n  BatchPolicy: 'EventCount:50,Interval:0',\n  CommitPolicy: 'EventCount:50,Interval:0',\n  ConnectionURL: '@CONNECT_URL@',\n  Tables: '@SOURCE_TABLE@2,@TARGET_TABLE@4;@SOURCE_TABLE@3,@TARGET_TABLE@4',\n  Password: '@TGT_PASSWORD@',\n  Password_encrypted: false\n ) \nINPUT FROM @APPNAME@Apps_PS_Stream2;\nEND FLOW @APPNAME@TARGETFLOW3;\n\nEND APPLICATION @APPNAME@Apps3;\n\n\n--********************Application 4************************\n-- PS2->cq->Inmemory2 <targetflow4> Inmemory2->target5<targetflow4>\n-- PS3->Target6\n\nCREATE APPLICATION @APPNAME@Apps4 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE STREAM @APPNAME@Apps_PS_InmemoryStream2 OF Global.waevent;\n\nCREATE CQ @APPNAME@cq_Ps_Inmemory1\nINSERT INTO @APPNAME@Apps_PS_InmemoryStream2\nSELECT *\nFROM @APPNAME@Apps_PS_Stream2\nWHERE META(@APPNAME@Apps_PS_Stream2,'TableName').toString() == '@SOURCE_TABLE@3';\n\ncreate flow @APPNAME@targetflow4;\nCREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget5 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TGT_USERNAME@',\n  BatchPolicy: 'EventCount:10,Interval:0',\n  CommitPolicy: 'EventCount:10,Interval:0',\n  ConnectionURL: '@CONNECT_URL@',\n  Tables: '@SOURCE_TABLE@3,@TARGET_TABLE@5',\n  Password: '@TGT_PASSWORD@',\n  Password_encrypted: false\n ) \nINPUT FROM @APPNAME@Apps_PS_InmemoryStream2;\nend flow @APPNAME@targetflow4;\n\nCREATE OR REPLACE TARGET @APPNAME@Apps4DBTarget6 USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TGT_USERNAME@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: '@CONNECT_URL@',\n  Tables: '@SOURCE_TABLE@2,@TARGET_TABLE@6',\n  Password: '@TGT_PASSWORD@',\n  Password_encrypted: false\n ) \nINPUT FROM @APPNAME@Apps_PS_Stream3;\n\nEND APPLICATION @APPNAME@Apps4;\n\ndeploy application @APPNAME@Apps4 on ANY in Appsdg with @APPNAME@Targetflow4 in Targetdg;\nstart application @APPNAME@Apps4;\n\ndeploy application @APPNAME@Apps3 on ANY in Appsdg with @APPNAME@Targetflow3 in Targetdg,@APPNAME@Targetflow2 in Targetdg,@APPNAME@Targetflow1 in Targetdg;\nstart application @APPNAME@Apps3;\n\ndeploy application @APPNAME@Apps2 on ANY in default;\nstart application @APPNAME@Apps2;\n\ndeploy application @APPNAME@Apps1 in Appsdg with @APPNAME@serverflow in sourcedg, @APPNAME@serverflow2 in sourcedg2;\nstart application @APPNAME@Apps1;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCREATE OR REPLACE SOURCE @SOURCENAME@ USING IncrementalBatchReader  (\n  FetchSize: 1000,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: @checkColumn@,\n  startPosition: @startPosition@,\n  PollingInterval: '20sec'\n  )\n  OUTPUT TO @STREAM@;\n\n  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;\n\n  CREATE TARGET @targetName@ USING DatabaseWriter(\n    ConnectionURL:'@READER-URL@',\n    Username:'@READER-UNAME@',\n    Password:'@READER-PASSWORD@',\n    BatchPolicy:'Eventcount:1000,Interval:1000',\n    CommitPolicy:'Eventcount:1000,Interval:1000',\n    Checkpointtable:'CHKPOINT',\n    Tables:'@WATABLES@,@WATABLES@_target'\n  ) INPUT FROM @STREAM@;\n\n  END APPLICATION @APPNAME@;\n\n  DEPLOY APPLICATION @APPNAME@;\n  start application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 20 with two sources going to one wactionstore\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> CQ1 -> WS\n-- S2 -> CQ2 -> WS\n--\n\nSTOP Recov20Tester.RecovTest20;\nUNDEPLOY APPLICATION Recov20Tester.RecovTest20;\nDROP APPLICATION Recov20Tester.RecovTest20 CASCADE;\nCREATE APPLICATION RecovTest20 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE WactionType (\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions1\nINSERT INTO Wactions\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream1;\n\nCREATE CQ InsertWactions2\nINSERT INTO Wactions\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream2;\n\nEND APPLICATION RecovTest20;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 34 with two sources, two sliding time-count windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Sc5a9W/p  -> CQ1 -> WS\n-- S2 -> Sc6a11W/p -> CQ2 -> WS\n--\n\nSTOP Recov34Tester.RecovTest34;\nUNDEPLOY APPLICATION Recov34Tester.RecovTest34;\nDROP APPLICATION Recov34Tester.RecovTest34 CASCADE;\nCREATE APPLICATION RecovTest34 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE\nPARTITION BY merchantId;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1\nGROUP BY merchantId;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2\nGROUP BY merchantId;\n\nEND APPLICATION RecovTest34;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;\n\nCreate Source @SourceName@ Using Ojet\n\n(\n  Username:'c##qatest',\n  Password:'qatest',\n  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',\n  Tables:'CDB$ROOT.\"C##QATEST\".ojet_src;ORCLPDB.QATEST.ojet_src',\n  _h_useClassic:false,\n  Fetchsize:1,\n  Compression: true,\n  SupportPDB:true,\n  ReplicationSlotName:'null'\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'))) FROM @SRCINPUTSTREAM@ x; ;\n\n\nCREATE TARGET @targetName@ USING DatabaseWriter\n(\n  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',\n  Username:'c##qatest',\n  Password:'qatest',\n  BatchPolicy:'EventCount:1,Interval:1',\n  Tables:'CDB$ROOT.\"C##QATEST\".ojet_src,CDB$ROOT.\"C##QATEST\".ojet_tgt'\n) INPUT FROM admin.sqlreader_cq_out;\n\n\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION eh;\nUNDEPLOY APPLICATION eh;\nDROP APPLICATION eh CASCADE;\nCREATE APPLICATION eh @Recovery@;\n\nCreate Source s1_orcl_w Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL: 'localhost:1521:orcl',\n Tables:'QATEST.TEST_%',\n FetchSize:1\n) \nOutput To sourcestream;\n\nCreate Source s2_orcl Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL: 'localhost:1521:orcl',\n Tables:'QATEST.TEST_01',\n FetchSize:1\n) \nOutput To sourcestream;\n\nCreate Source s3_orcl Using OracleReader\n(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL: 'localhost:1521:orcl',\n Tables:'QATEST1.TEST_01',\n FetchSize:1\n) \nOutput To sourcestream;\n\n\nCREATE CQ OperationType\nINSERT INTO OpsStream\nSELECT  \nCASE WHEN (META(c,\"OperationName\").toString() == \"DELETE\")\nTHEN putUserData(c, 'isDelete', 'true') \nELSE\nputUserData(c,'isDelete', 'false')\nEND\nFROM sourcestream c;\n\n\n\nCREATE OR REPLACE SOURCE s4_orcl_ibr USING IncrementalBatchReader  ( \n  FetchSize: 1,\n  StartPosition: '%=0',\n  Username: 'qatest',\n  ConnectionURL: 'jdbc:oracle:thin:@//dockerhost:1521/orcl',\n  Tables: 'QATEST.TEST_%',\n  CheckColumn: '%=id',\n  Password: 'qatest' ) \nOUTPUT TO sourcestream ;\n\nCREATE SOURCE s5_fr USING FileReader (\n\tdirectory:'/Users/saranyad/Product/IntegrationTests/TestData/',\n    WildCard:'banks.csv',\n\tpositionByEOF:false,\n        Charset:'UTF-8'\n)\nPARSE USING DSVParser (\n  columndelimiter:',',\n  ignoreemptycolumn:'Yes',\n  quoteset:'[]~\"',\n  separator:'~'\n)\nOUTPUT TO FileStream;\n\n\nCREATE TYPE cdctype(\n  id int,\n  name String  \n);\n\nCREATE STREAM cdctypestream OF cdctype;\n\nCREATE CQ cdcstreamcq\nINSERT INTO cdctypestream\nSELECT TO_INT(p.data[0]), \n       TO_STRING(p.data[1])\nFROM FileStream p;\n\n\ncreate cq cqtowaevent \ninsert into sourcestream\nselect convertTypedeventToWAevent(c, 'admin.cdctype')\nfrom cdctypestream c;\n\n\ncreate Target t1_dsv using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_01',\n\tE1P:'true',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',\n\tPartitionKey:'@metadata(TableName)',\n\tConsumerGroup:'reader',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n\t--ParallelThreads:'2'\n)\nformat using DSVFormatter ( \n)\ninput from sourcestream;\n\n\n\n\n\ncreate Target t2_json using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_02',\n\tE1P:'true',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',\n\tPartitionKey:'@userdata(isDelete)',\n\tConsumerGroup:'reader',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:30s'\n\t--ParallelThreads:'2'\n)\nformat using JSONFormatter ( \n)\ninput from OpsStream;\n\n\ncreate Target t3_avro using AzureEventHubWriter (\n\tEventHubNamespace:'EventHubWriterTest',\n\tEventHubName:'test_03',\n\tE1P:'true',\n\tSASPolicyName:'RootManageSharedAccessKey',\n\tSASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',\n\tPartitionKey:'@metadata(TableName)',\n\tConsumerGroup:'reader',\n\tConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'\n\t--ParallelThreads:'2'\n)\nformat using AvroFormatter (schemaFileName:'kafkaAvroTest_multipleReader.avsc') \ninput from sourcestream;\n\n\nEND APPLICATION eh;\n\nDEPLOY APPLICATION eh on any in default;\n\nstart application eh;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "undeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\nCreate Source @APPNAME@_s Using DatabaseReader\n(\n Username:'@UNAME@',\n Password:'@PASSWORD@',\n ConnectionURL:'@URL@',\n --Query: \"SELECT * FROM QATEST.AUTHORIZATIONS\",\n Tables:'QATEST.HDFS_IL_%',\n FetchSize:1,\n QuiesceOnILCompletion: true\n)\nOutput To @APPNAME@_ss;\n\n\ncreate Target @APPNAME@_t using HDFSWriter(\n\thadoopurl:'hdfs://localhost:9000/',\n\tdirectory: '%@METADATA(TableName)%',\n\tfilename:'%@METADATA(TableName)%',\n    rolloverpolicy: 'filesize:500M',\n    flushpolicy:'@FLUSHPOLICY@',\n    hadoopConfigurationPath:'@CONF@'\n\t)\nformat using DSVFormatter (\n)input from @APPNAME@_ss;\n\n--create Target t2 using SysOut(name:Foo2) input from DataStream;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 1 on Four node all server cluster \n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> WS\n--\n\nSTOP N4S4CR1Tester.N4S4CRTest1;\nUNDEPLOY APPLICATION N4S4CR1Tester.N4S4CRTest1;\nDROP APPLICATION N4S4CR1Tester.N4S4CRTest1 CASCADE;\nCREATE APPLICATION N4S4CRTest1 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN4S4CRTest1;\n\nCREATE SOURCE CsvSourceN4S4CRTest1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN4S4CRTest1;\n\nCREATE FLOW DataProcessingN4S4CRTest1;\n\nCREATE TYPE WactionTypeN4S4CRTest1 (\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE WACTIONSTORE WactionsN4S4CRTest1 CONTEXT OF WactionTypeN4S4CRTest1\nEVENT TYPES ( WactionTypeN4S4CRTest1 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsN4S4CRTest1\nINSERT INTO WactionsN4S4CRTest1\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nEND FLOW DataProcessingN4S4CRTest1;\n\nEND APPLICATION N4S4CRTest1;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Create Target @TARGET_NAME@ using HiveWriter\n(\n  ConnectionURL:'jdbc:hive2://dockerhost:10000/default',\n  Username:'cloudera',\n  Password:'cloudera',\n  hadoopurl:'hdfs://dockerhost:9000',\n  Mode:'initialload',\n  mergepolicy:'eventcount:5,interval:1s',\n  Tables:'QATEST.HIVE_EMP,default.hive_emp KEYCOLUMNS(id)',\n  hadoopConfigurationPath:'/home/ubuntu/Product/IntegrationTests/TestData/hdfsconf/'\n)\nINPUT FROM @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 5 with Jumping window and partitioned\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS\n--\n\nSTOP Recov5Tester.RecovTest5;\nUNDEPLOY APPLICATION Recov5Tester.RecovTest5;\nDROP APPLICATION Recov5Tester.RecovTest5 CASCADE;\nCREATE APPLICATION RecovTest5 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream OF CsvData PARTITION BY merchantId;\n\nCREATE CQ CsvToData\nINSERT INTO DataStream\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND APPLICATION RecovTest5;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Kafka Stream Recovery Test 10 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same key\n-- Bert Hashemi and Nicholas Keene WebAction, Inc.\n--\n-- S1 -> KS -> CQ -> CW(p) -> CQ -> WS\n--\n\nSTOP KStreamRecov10Tester.KStreamRecovTest10;\nUNDEPLOY APPLICATION KStreamRecov10Tester.KStreamRecovTest10;\nDROP APPLICATION KStreamRecov10Tester.KStreamRecovTest10 CASCADE;\nDROP USER KStreamRecov10Tester;\nDROP NAMESPACE KStreamRecov10Tester CASCADE;\nCREATE USER KStreamRecov10Tester IDENTIFIED BY KStreamRecov10Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov10Tester;\nCONNECT KStreamRecov10Tester KStreamRecov10Tester;\n\nCREATE APPLICATION KStreamRecovTest10 RECOVERY 1 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTest10Data.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE TYPE CsvData (\n  partKey String KEY,\n  serialNumber int\n);\n\nCREATE STREAM DataStream OF CsvData PARTITION BY partKey;\n\nCREATE CQ CsvToData\nINSERT INTO DataStream\nSELECT\n    data[0],\n    TO_INT(data[1])\nFROM KafkaCsvStream;\n\nCREATE JUMPING WINDOW DataStreamTwoItems\nOVER DataStream KEEP 2 ROWS\nPARTITION BY partKey;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction\nINSERT INTO Wactions\nSELECT\n    first(partKey),\n    to_int(first(serialNumber))\nFROM DataStreamTwoItems\nGROUP BY partKey;\n\nEND APPLICATION KStreamRecovTest10;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (\n  Tables: '',\n  ConnectionURL: '',\n  Password: '',\n  Username: ''\n  )\nOUTPUT TO @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING ParquetFormatter (\nschemaFileName: '@SCHEMAFILE@',\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING JSONFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (\n  bucketname: '',\n  uploadpolicy: '',\n  UploadConfiguration: '',\n  objectname: '' )\nFORMAT USING DSVFormatter (\nmembers:'data'\n)\nINPUT FROM @APPNAME@stream;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @appname@;\nundeploy application @appname@;\ndrop application @appname@ cascade;\n\nCREATE APPLICATION @appname@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE @parquetsrc@ USING FileReader (\nwildcard: '',\ndirectory: '',\npositionbyeof: false )\nPARSE USING ParquetParser (\n)\nOUTPUT TO @appname@Stream;\n\nCREATE CQ @appname@CQ\nINSERT INTO @appname@CqOut\n    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;\n\nCREATE OR REPLACE TARGET @filetarget@ USING FileWriter (\nfilename: '',\ndirectory: '',\nflushpolicy: 'EventCount:1,Interval:30s',\nrolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING ParquetFormatter  (\nschemaFileName: ''\n)\nINPUT FROM @appname@CqOut;\n\nEND APPLICATION @appname@;\ndeploy application @appname@ on all in default;\nstart application @appname@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 38 with two sources, two jumping time-count windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jc5a3W/p -> CQ1 -> WS\n--   S2 -> Jc6a4W/p -> CQ2 -> WS\n--\n\nSTOP KStreamRecov38Tester.KStreamRecovTest38;\nUNDEPLOY APPLICATION KStreamRecov38Tester.KStreamRecovTest38;\nDROP APPLICATION KStreamRecov38Tester.KStreamRecovTest38 CASCADE;\n\nDROP USER KStreamRecov38Tester;\nDROP NAMESPACE KStreamRecov38Tester CASCADE;\nCREATE USER KStreamRecov38Tester IDENTIFIED BY KStreamRecov38Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov38Tester;\nCONNECT KStreamRecov38Tester KStreamRecov38Tester;\n\nCREATE APPLICATION KStreamRecovTest38 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream1 OF Global.waevent using KafkaPropset;\nCREATE STREAM KafkaCsvStream2 OF Global.waevent using KafkaPropset;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM KafkaCsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Rows3Seconds\nOVER DataStream1 KEEP 5 ROWS WITHIN 3 SECOND\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Rows4Seconds\nOVER DataStream2 KEEP 6 ROWS WITHIN 4 SECOND\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataStream5Rows3Seconds\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Rows3Seconds p\nGROUP BY p.merchantId;\n\nCREATE CQ DataStream6Rows4Seconds\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Rows4Seconds p\nGROUP BY p.merchantId;\n\nEND APPLICATION KStreamRecovTest38;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @SOURCENAME@ USING IncrementalBatchReader  (\n  FetchSize: 100,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: @checkColumn@,\n  startPosition: @startPosition@,\n  PollingInterval: '120sec'\n  )\n  OUTPUT TO @STREAM@;\n\n  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;\n\n  CREATE TARGET @targetName@ USING DatabaseWriter(\n    ConnectionURL:'@READER-URL@',\n    Username:'@READER-UNAME@',\n    Password:'@READER-PASSWORD@',\n    BatchPolicy:'Eventcount:1,Interval:1',\n    CommitPolicy:'Eventcount:1,Interval:1',\n    Checkpointtable:'CHKPOINT',\n    Tables:'@WATABLES@,@WATABLES@_target'\n  ) INPUT FROM @STREAM@;\n\n  END APPLICATION @APPNAME@;\n\n  DEPLOY APPLICATION @APPNAME@;\n  start application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE FLOW @STREAM@_SourceFlow;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@ USING PostgreSQLReader  ( \n ReplicationSlotName: 'slotname',\n  FilterTransactionBoundaries: 'true',\n  Username: 'waction',\n  Password_encrypted: false,\n  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',\n  adapterName: 'PostgreSQLReader',\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --startLSN:'758199500005967269518578814280000',\n  Password: 'xFzvJYZf1b8=',\n  Tables: 'public.postgrestopostgres_src',\n  ExcludedTables:'public.postgres_2000target'\n ) \nOUTPUT TO @STREAM@ ;\n\nEND FLOW @STREAM@_SourceFlow;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop application @externalCacheName@;\nundeploy application @externalCacheName@;\ndrop application @externalCacheName@ cascade;\ndrop EXCEPTIONSTORE @externalCacheName@_Exceptionstore;\n\ndrop application @ApplicationName@ cascade;\n\nCREATE APPLICATION @ApplicationName@ RECOVERY 5 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;\n\nCREATE OR REPLACE SOURCE @SourceName@ USING Global.DatabaseReader\n(\n  Username: '@Username@',\n  Password: '@Password@',\n  ConnectionURL: '@ConnectionURL@',\n  Tables: '@SourceTableName@',\n  FetchSize: 1\n)\nOutput To @SourceName@_st;\n\nCREATE TYPE @TYPEName@_type (\n id Integer,\n email STRING,\n address STRING\n );\n\nCREATE EXTERNAL CACHE @externalCacheName@ (\n  keytomap: 'id',\n  DatabaseProviderType: 'Default',\n  Table: '@LookUpTableName@',\n  AdapterName: 'DatabaseReader',\n  ConnectionURL: '@ConnectionURL@',\n  FetchSize: 1,\n  Password_encrypted: 'false',\n  Columns: 'id,email,address',\n  Password: '@Password@',\n  connectionRetryPolicy: 'timeOut=5, retryInterval=5, maxRetries=5',\n  Username: '@Username@' )\nOF @TYPEName@_type;\n\n\nCREATE CQ @CQName@\nINSERT INTO @TargetName@_st\nSELECT t1.data[0] as ID,t1.data[1] as Name,t1.data[2] as Deptarment,t2.email as Email,t2.address as address\nFROM @SourceName@_st t1 left outer join @externalCacheName@ t2\non TO_INT(t1.data[0]) = t2.id;\n\n\nCREATE OR REPLACE TARGET @TargetName@ USING Global.DatabaseWriter (\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  BatchPolicy: 'EventCount:1,Interval:10',\n  CheckPointTable: 'CHKPOINT',\n  ConnectionURL: '@ConnectionURL@',\n  Password_encrypted: 'false',\n  CDDLAction: 'Process',\n  Password: '@Password@',\n  CommitPolicy: 'EventCount:1,Interval:30',\n  StatementCacheSize: '50',\n  DatabaseProviderType: 'Default',\n  PreserveSourceTransactionBoundary: 'false',\n  Tables: '@TargetTableName@',\n  Username: '@Username@',\n  adapterName: 'DatabaseWriter' )\nINPUT FROM @TargetName@_st;\n\nEND APPLICATION @ApplicationName@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @AppName@;\n\nCREATE OR REPLACE SOURCE initialLoad_Src USING Global.DatabaseReader (\n  QuiesceOnILCompletion: false,\n  Tables: '@SrcTableName@',\n  adapterName: 'DatabaseReader',\n  Password: '@Password@',\n  Username: '@UserName@',\n  ConnectionURL: '@Srcurl@',\n   FetchSize: 10000)\nOUTPUT TO CommonRawStream;\n\nCreate Source OrcReader_Src Using OracleReader(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@Srcurl@',\n Tables:'@SrcTableName@',\n Fetchsize:100)\nOutput To CommonRawStream;\n\nCREATE OR REPLACE TARGET Postgres_Trg USING Global.DatabaseWriter (\n  ConnectionURL: '@trgUrl@',\n  Username: '@trgUsrName@',\n  Tables: '@SrcTableName@,@trgTable@',\n  Password: '@trgPswd@',\n  CommitPolicy: 'EventCount:10000,Interval:60',\n  adapterName: 'DatabaseWriter' )\nINPUT FROM CommonRawStream;\n\nCREATE TARGET filewriter_tgt USING Global.FileWriter (\n directory:'@trgDir@',\n  filename: '@fileName@',\n  flushpolicy: 'EventCount:10000,Interval:30s',\n  rolloverpolicy: 'EventCount:10000,Interval:30s' )\nFORMAT USING Global.JSONFormatter  (\n  jsonMemberDelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true',\n  jsonobjectdelimiter: '\\n' )\nINPUT FROM CommonRawStream;\n\nCREATE OR REPLACE TARGET BigQuery_Target USING Global.BigQueryWriter (\n  streamingUpload: 'false',\n  projectId: '@projectID@',\n  Tables: '@SrcTableName@,@BQTableName@',\n  optimizedMerge: 'false',\n  ServiceAccountKey: '@ServiceAccountKey@',\n  BatchPolicy: 'EventCount:1000000,Interval:90',\n  Mode: 'APPENDONLY' )\nINPUT from CommonRawStream;\n\nEND APPLICATION @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Alter application app1;\n\nCREATE OR REPLACE SOURCE s USING oracleReader  ( \n  Username:'qatest',\n  Password:'qatest',\n  ConnectionURL:'localhost:1521/xe',\n  Tables:'QATEST.test%',\n  FetchSize:2\n ) \nOUTPUT TO rawstream;\nAlter application app1 recompile;\n\n\n\nAlter application app2;\nCREATE or replace TARGET app2_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Interval:60',\nCommitPolicy:'Interval:60',\nTables:'qatest.test01,QATEST.KPS1'\n) INPUT FROM rawstream;\nAlter application app2 recompile;\n\n\n\n\nAlter application app3;\n\nCREATE or replace TARGET app3_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Interval:60',\nCommitPolicy:'Interval:60',\nTables:'qatest.test02,QATEST.KPS2'\n) INPUT FROM rawstream;\n\nAlter application app3 recompile;\n\n\n\nAlter application app4;\n\nCREATE or replace TARGET app4_target USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Interval:60',\nCommitPolicy:'Interval:60',\nTables:'qatest.test03,QATEST.KPS3'\n) INPUT FROM rawstream;\n\nAlter application app4 recompile;\n\n\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@;\n\nCREATE SOURCE @SourceName@ USING MySQLReader  ( \nReaderType: 'LogMiner', \n  Password_encrypted: 'false', \n  DatabaseName: 'qatest',\n  SupportPDB: false, \n  QuiesceMarkerTable: 'QUIESCEMARKER', \n  QueueSize: 2048, \n  CommittedTransactions: true, \n  Username: '@UserName@', \n  TransactionBufferType: 'Memory', \n  TransactionBufferDiskLocation: '.striim/LargeBuffer', \n  OutboundServerProcessName: 'WebActionXStream', \n  Password: '@Password@', \n  DDLCaptureMode: 'All', \n  Compression: false, \n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', \n  FetchSize: 1, \n  Tables: '@SourceTables@', \n  DictionaryMode: 'OnlineCatalog', \n  XstreamTimeOut: 600, \n  TransactionBufferSpilloverSize: '1MB', \n  FilterTransactionBoundaries: true, \n  StartSCN: 'null', \n  ConnectionURL: '@ConnectionURL@', \n  SendBeforeImage: true ) \nOUTPUT TO @AppStream@  ;\n\nCREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream SELECT data[2], to_string(to_date(data[2]), \"dd-MMM-yy hh.mm.ss\") FROM @AppStream@ o ;\n\nCREATE  TARGET @targetsys@ USING Global.SysOut  ( \nname: 'ora1_sys' ) \nINPUT FROM admin.ZDT_cq_stream;\n\ncreate Target @TargetFile@ using FileWriter(\n  filename:'toStringOut.log',\n  directory:'@FilePath@',\n  rolloverpolicy:'eventcount:1000'\n)\nformat using DSVFormatter (\n\n)\ninput from admin.ZDT_cq_stream;\n\nEND APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\n\ncreate application @APPNAME@;\n\nCREATE OR REPLACE SOURCE CCBReader USING FileReader (\n  wildcard: '@WILDCARD@',\n  positionbyeof: false,\n  directory: '@TESTDIR@'\n  )\nPARSE USING Global.CobolCopybookParser (\n  copybookDialect: 'Mainframe',\n  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',\n  GroupPolicy: '@GRPPOLICY@',\n  TimeoutPolicy: '1s',\n  dataFileOrganization: 'FixedLength',\n  ProcessCopyBookFileAs: 'MultipleEvents',\n  skipIndent: 0,\n  dataFileFont: 'utf8',\n  CopybookFileFormat: 'USE_STANDARD_COLUMNS',\n  copybookSplit: 'None',\n  copybookFileName: '@CCBFILE@'\n   )\nOUTPUT TO CCBStream;\n\nCREATE OR REPLACE TARGET JSONWriter USING FileWriter (\n  filename: '%@metadata(FileName)%',\n  directory: '@DIR@',\n  rolloverpolicy: 'EventCount:10000,Interval:30s',\n  flushpolicy: 'EventCount:1,Interval:30s'\n  )\nFORMAT USING JSONFormatter  ()\nINPUT FROM CCBStream;\n\nend application @APPNAME@;\ndeploy application @APPNAME@ on all in default;\nstart application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP AdhocTester.ws_one;\nUNDEPLOY APPLICATION AdhocTester.ws_one;\nDROP APPLICATION AdhocTester.ws_one cascade;\n\nCREATE APPLICATION ws_one;\n\n\nCREATE SOURCE wsSource USING CSVReader\n(\ndirectory:'@TEST-DATA-PATH@',\nheader: Yes,\nwildcard:'AdhocQueryData2.csv',\ncolumndelimiter:',',\nblocksize: 10240,\npositionByEOF:false\n) OUTPUT TO QaStream;\n\nCREATE TYPE USAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\nCREATE CACHE ZipLookup using CSVReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'AdhocQueryData.csv',\n  header: Yes,\n  columndelimiter: '\t',\n  trimquote:false\n) QUERY (keytomap:'zip') OF USAddressData;\n\nCreate TYPE wsData(\n\tCompanyNum String,\n\tCompanyName String KEY,\n\tCompanyCode int,\n\tZip String\n);\n\n\nCREATE STREAM wsStream OF wsData;\n\n--Select data from QaStream and insert into wsStream\n\nCREATE CQ csvTowsData\nINSERT INTO wsStream\nSELECT  data[0],\n    data[1],\n    TO_INT(data[3]),\n    data[9]\n FROM QaStream;\n\n\n\n\nCREATE WACTIONSTORE oneWS CONTEXT OF wsData\nEVENT TYPES(wsData )\n@PERSIST-TYPE@\n\n\nCREATE CQ wsToWaction\nINSERT INTO oneWS\nSELECT * FROM wsStream\nLINK SOURCE EVENT;\n\nEND APPLICATION ws_one;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP cacheRefresher.cacheApp;\nUNDEPLOY APPLICATION cacheRefresher.cacheApp;\nDROP APPLICATION cacheRefresher.cacheApp cascade;\n\nCREATE APPLICATION cacheApp;\n\n\nCREATE TYPE bankData\n(\nbankID Integer KEY,\nbankName String\n);\n\nCREATE CACHE cache1 USING CsvReader(\ndirectory: '@TEST-DATA-PATH@',\nwildcard: 'banks_cache.csv',\nheader: No,\ncolumndelimiter: ',',\ntrimquote: false,\npositionbyEOF: false\n) QUERY (keytomap:'bankID', refreshinterval:'20 second') OF bankData;\n\n\n\nEND APPLICATION cacheApp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW;\n\n\nCREATE OR REPLACE SOURCE DBSource USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream;\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n )\nINPUT FROM Oracle_ChangeDataStream;\n\n\ncreate Target DBRTOCW_t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\nEND APPLICATION DBRTOCW;\n\ndeploy application DBRTOCW on ANY in default;\n\nstart application DBRTOCW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--To be used with NonCdcAdapterCommons.startExceptionStoreApp()\nCREATE APPLICATION @APPNAME@;\n\nCREATE CQ @APPNAME@CQ INSERT INTO @APPNAME@CQOut\nselect x.exceptionType,\nx.action,\nx.appName,\nx.entityType,\nx.entityName,\nx.className,\nx.message,\nx.relatedActivity,\nx.relatedObjects,\nx.relatedEntity,\nx.exceptionCode\nfrom @EXCEPTIONAPPNAME@_ExceptionStore x;\n\nCREATE TARGET @APPNAME@Trgt USING FileWriter ()\nFORMAT USING JSONFormatter()\nINPUT FROM @APPNAME@CQOut;\n\nEND APPLICATION @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING MariaDbXpandReader\n(\nUsername: '@READER-UNAME@',\nPassword: '@READER-PASSWORD@',\nConnectionURL: '@CDC-READER-URL@',\nTables: @WATABLES@,\nsendBeforeImage:'true',\nFilterTransactionBoundaries: 'true'\n) \nOUTPUT TO @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop IR;\nundeploy application IR;\ndrop application IR cascade;\nCREATE APPLICATION IR;\n\n CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( \n  FetchSize: 100,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.test01;striim.test02;striim.test03',\n  adapterName: 'IncrementalBatchReader',\n CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',\n startPosition: 'striim.test01=1;striim.test02=-1;%=0',\n  PollingInterval: '5sec'\n  )\n  OUTPUT TO data_stream ;\n\n  CREATE OR REPLACE TARGET sys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM data_stream;\n\ncreate target AzureSQLDWHTarget using AzureSQLDWHWriter(\n\t\tConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        accountname: 'striimqatestdonotdelete',\n        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',\n        uploadpolicy:'eventcount:1,interval:10s'\n) INPUT FROM data_stream;\n\n\nCREATE TARGET log USING LOGWRITER\n(\n name:log,\n filename:'LOGFILENAME'\n)\nINPUT FROM data_stream;\n\n\nEND APPLICATION IR;\n\ndeploy application IR;\nstart IR;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  (\n  Username: '@SRC_USERNAME@',\n  Password: '@SRC_PASSWORD@',\n  ConnectionURL: '@CDC_URL@',\n  DatabaseName:'@DatabaseName@',\n  Tables: '@Source1Tables@' )\nOUTPUT TO @APPNAME@cdcStream;\n\nCREATE OR REPLACE EXTERNAL CACHE @APPNAME@1 (\n  AdapterName:'DatabaseReader',\n  Username: '@SRC_USERNAME@',\n  Password: '@SRC_PASSWORD@',\n  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',\n  FetchSize: 20,\n  DatabaseProviderType: 'Default',\n  Table: '@Source3Tables@',\n  Columns: 'col1,col2,col3,col4,uniquecol',\n  keytomap: 'uniquecol')\nOF @APPNAME@cachetype;\n\nCREATE OR REPLACE EXTERNAL CACHE @APPNAME@2 (\n  AdapterName:'DatabaseReader',\n  Username: '@SRC_USERNAME@',\n  Password: '@SRC_PASSWORD@',\n  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',\n  FetchSize: 10,\n  DatabaseProviderType: 'Default',\n  Table: '@Source2Tables@',\n  Columns: 'col1,col2,col3,col4,uniquecol',\n  keytomap: 'uniquecol')\nOF @APPNAME@cachetype;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader\n(\nFetchSize:'@FETCHSIZE@',\nQueueSize:'@QUEUESIZE@',\nUsername:'@USERNAME@',\nPassword:'@PASSWORD@',\nConnectionURL:'@URL@',\n_h_useClassic: @CLASSICMODE@,\nTables:'@TABLES@',\nCommittedTransactions:'@COMMITEDT@'\nCompression:'@COMPRESSION@'\n)\nOUTPUT TO @STREAM@ (a String, b String ,c String,g String)\nSELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]) where TO_String(data[0]) > '1' ;\n\nCREATE TYPE LogType(\nNum_col String key,\nChar_col String,\nVarchar2_col String,\nlong_col String\n);\n\nCREATE WINDOW CDCWindow\nOVER @STREAM@\nKEEP 1 ROWS;\n\nCREATE WACTIONSTORE CDCWS CONTEXT of LogType\nEVENT TYPES ( LogType )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ ToWactionStore\nINSERT INTO CDCWS\nSELECT * FROM CDCWindow\nLINK SOURCE EVENT;\n\nCREATE TARGET @SOURCE_NAME@_SYS USING SysOut (\n  name: '@SOURCE_NAME@_SYS' )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (\n  Username: '@TARGET_USER@',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  _h_BindEmptyStringasNull : 'true'\n )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@;\n\nCREATE SOURCE @APPNAME@_Source USING MSSqlReader\n(\n  Compression: false,\n  cdcRoleName: 'STRIIM_READER',\n  DatabaseName: 'QATEST',\n  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',\n  ConnectionPoolSize: 1,\n  FetchTransactionMetadata: false,\n  StartPosition: 'EOF',\n  Username: 'qatest',\n  SendBeforeImage: true,\n  AutoDisableTableCDC: true,\n  ConnectionURL: 'localhost:1433',\n  Tables: 'qatest.test01',\n  adapterName: 'MSSqlReader',\n  Password: 'w3b@ct10n'\n)\nOUTPUT TO @APPNAME@_stream;\n\nCREATE TARGET @APPNAME@_Target USING SpannerWriter (\n\tTables: 'QATEST.%,testdb.test',\n\tServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',\n\tBatchPolicy: 'EventCount: 1; Interval: 1s',\n\tinstanceId: 'qatest'\n) INPUT FROM @APPNAME@_stream;\n\ncreate Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_stream;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@;\nstart application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  --Password_encrypted: false\n )\nINPUT FROM @STREAM@;\n\n\nCREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@'\n  --Password_encrypted: false\n )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@ @Recovery@;\n--create flow AgentFlow;\nCREATE SOURCE @APPNAME@_S USING MySqlReader\n(\n  Compression: false,\n  FetchSize: 1,\n  SendBeforeImage: true,\n  ConnectionURL: 'mysql://localhost:3306',\n  DatabaseName: 'waction',\n  Tables: 'waction.test01',\n  Password: 'w@ct10n',\n  Password_encrypted: 'false',\n  Username: 'root',\n  connectionRetryPolicy:'retryInterval=1,maxRetries=3'\n)\nOUTPUT TO @APPNAME@_SS;\n--end flow AgentFlow;\n--create flow serverFlow;\n--create target sys_out using sysout (name:sysoutput) input from ss;\n\nCREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (\nAllowQuotedNewlines:False,\nConnectionRetryPolicy:'retryInterval=30,maxRetries=3',\nserviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',\nEncoding:'UTF-8',\nprojectId: 'bigquerywritertest',\nTables:'waction.test01,@DATASET@.% KEYCOLUMNS(col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',\nMode:'merge',\ndatalocation: 'US',\nnullmarker: 'defaultNULL',\ncolumnDelimiter: '|',\nBatchPolicy: 'eventCount:1000,Interval:30',\nStandardSQL:true,\noptimizedMerge:true\t\n) INPUT FROM @APPNAME@_ss;\n\n--end flow serverFlow;\nEND APPLICATION @APPNAME@;\nDEPLOY APPLICATION @APPNAME@;\n--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;\nSTART APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP AES;\nUNDEPLOY APPLICATION AES;\nDROP APPLICATION AES CASCADE;\n\nCREATE APPLICATION AES;\n\n\nCREATE TYPE Atm(\n  productID String KEY,\n  stateID String,\n  productWeight int,\n  quantity double,\n  size long,\n  currentDate DateTime);\n\nCREATE source implicitSOurce USING FileReader (\n      directory:'@TEST-DATA-PATH@',\n      columndelimiter: ',',\n      wildcard:'ISdata.csv',\n      blocksize: 10240,\n      positionByEOF:false\n)\nPARSE USING DSVParser (\n      header:False,\n      trimquote:false\n) OUTPUT TO CsvStream; \n\nCREATE TYPE wsType(\n  quantity double KEY,\n  currentDate DateTime\n  );\n\n\nCREATE STREAM newStream OF Atm;\n\nCREATE CQ newCQ\nINSERT INTO newStream\nSELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]), TO_DATE(data[5]) FROM\nCsvStream;\n\n\nCREATE WINDOW win1\nOVER newStream\nkeep within 3 minute;\n\nCREATE STREAM newStream2 of wsType;\n\n\n\nCREATE WACTIONSTORE WS1 CONTEXT OF wsType\nEVENT TYPES (wsType );\n\n\nCreate cq newCQ2\ninsert into ws1 (quantity,currentDate)\nselect quantity, currentDate from newStream;\n\n\nEND APPLICATION AES;\ndeploy APPLICATION AES;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSVSource using FileReader (\n\tdirectory:'@TEST-DATA-PATH@',\n\tWildCard:'posdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\n\tfilename:'JsonTargetAdd',\n\tdirectory:'@FEATURE-DIR@/logs/',\n\tsequence:'00',\n\trolloverpolicy:'FileSizeRollingPolicy,filesize:33 + 44M'\n)\nformat using JSONFormatter (\n\tmembers:'merchantname,merchantid,dateTime,hourValue,amount,zip'\n)\n\ninput from TypedCSVStream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizeAdd_actual.log') input from TypedCSVStream;\nend application DSV;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop OracleToKudu;\nundeploy application OracleToKudu;\ndrop application OracleToKudu cascade;\n\n\nstop OracleToKudu_ExpStore;\nundeploy application OracleToKudu_ExpStore;\ndrop application OracleToKudu_ExpStore cascade;\n\n--drop exceptionstore admin.OracleToKudu_ExceptionStore;\nCREATE APPLICATION OracleToKudu use exceptionstore;\nCreate Source oracSource Using OracleReader\n(\n Username:'@LOGMINER-UNAME@',\n Password:'@LOGMINER-PASSWORD@',\n ConnectionURL:'@LOGMINER-URL@',\n Tables:'@SOURCE_TABLES@',\n OnlineCatalog:true,\n FetchSize:1\n) Output To DataStream;\n\nCREATE TARGET WriteintoKudu using KuduWriter (\nkuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',\npkupdatehandlingmode:'@MODE@',\ntables: '@TARGET_TABLES@',\nbatchpolicy: 'EventCount:1,Interval:0')\nINPUT FROM DataStream;\n\nEND APPLICATION OracleToKudu;\ndeploy application OracleToKudu in default;\nstart OracleToKudu;\n\n\n\nCREATE APPLICATION OracleToKudu_ExpStore;\n\nCREATE TYPE OracleToKudu_ExpStore_CDCStreams_Type  (\n exceptionType java.lang.String,\n  action java.lang.String,\n  appName java.lang.String,\n  entityType java.lang.String,\n  entityName java.lang.String,\n  className java.lang.String,\n  message java.lang.String \n );\n\nCREATE STREAM OracleToKudu_ExpStore_CDCStreams OF OracleToKudu_ExpStore_CDCStreams_Type;\n\nCREATE CQ OracleToKudu_ReadFromExpStore\nINSERT INTO OracleToKudu_ExpStore_CDCStreams\nselect s.exceptionType,s.action,s.appName,s.entityType,s.entityName,s.className,s.message from admin.OracleToKudu_ExceptionStore [jumping 10 second] s;\n        \nCREATE OR REPLACE TARGET OracleToKudu_ExpStore_WriteToFileAsJSON USING FileWriter  ( \n  filename: 'expEvent_Kudu.log',\n  rolloveronddl: 'true',\n  flushpolicy: 'eventcount:2,interval:30',\n  adapterName: 'FileWriter',\n  directory: 'ExpStore_logs',\n  rolloverpolicy: 'eventcount:2,interval:30s'\n ) \nFORMAT USING JSONFormatter  ( \n  handler: 'com.webaction.proc.JSONFormatter',\n  formatterName: 'JSONFormatter',\n  jsonMemberDelimiter: '\\n',\n  jsonobjectdelimiter: '\\n',\n  EventsAsArrayOfJsonObjects: 'true'\n ) \nINPUT FROM OracleToKudu_ExpStore_CDCStreams;\n        \nEND APPLICATION OracleToKudu_ExpStore;\n\ndeploy application OracleToKudu_ExpStore;\nstart OracleToKudu_ExpStore;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@\n Using OracleReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'qatest.@SourceTable@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(\nConnectionURL:'@TargetConnectionURL@',\n  Username:'@UserName@',\n  Password:'@Password@',\n  BatchPolicy:'EventCount:1,Interval:1',\n Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'\n) INPUT FROM @SRCINPUTSTREAM@;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@;\nstart @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Kafka Stream Recovery Test 11\n-- Bert Hashemi and Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS1\n-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS2\n--\n\nSTOP Recov11Tester.KStreamRecovTest11;\nUNDEPLOY APPLICATION Recov11Tester.KStreamRecovTest11;\nDROP APPLICATION Recov11Tester.KStreamRecovTest11 CASCADE;\nDROP USER KStreamRecov11Tester;\nDROP NAMESPACE KStreamRecov11Tester CASCADE;\nCREATE USER KStreamRecov11Tester IDENTIFIED BY KStreamRecov11Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov11Tester;\nCONNECT KStreamRecov11Tester KStreamRecov11Tester;\n\nCREATE APPLICATION KStreamRecovTest11 RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE TYPE WactionType (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionType;\n\nCREATE CQ CsvToWaction\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE Wactions1 CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE Wactions2 CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions1\nINSERT INTO Wactions1\nSELECT\n    *\nFROM DataStream5Minutes;\n\nCREATE CQ InsertWactions2\nINSERT INTO Wactions2\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND APPLICATION KStreamRecovTest11;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ recovery 5 second interval;\n\nCREATE  SOURCE @QuerySource@1 USING DatabaseReader  (\n  ReplicationSlotName:'@slotname1@',\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@1;\ncreate Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;\n CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,public.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@1;\n\nCREATE  SOURCE @QuerySource@2 USING DatabaseReader  (\n  ReplicationSlotName:'@slotname2@',\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@2;\ncreate Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;\n CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,public.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@2;\n\nCREATE  SOURCE @QuerySource@3 USING DatabaseReader  (\n  ReplicationSlotName:'@slotname3@',\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@3;\ncreate Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;\n CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,public.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@3;\n\nCREATE  SOURCE @QuerySource@4 USING DatabaseReader  (\n  ReplicationSlotName:'@slotname4@',\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@4;\ncreate Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;\n CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,public.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@4;\n\nCREATE  SOURCE @QuerySource@5 USING DatabaseReader  (\n  ReplicationSlotName:'@slotname5@',\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@5;\ncreate Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;\n CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,public.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@5;\n\nCREATE  SOURCE @QuerySource@6 USING DatabaseReader  (\n  ReplicationSlotName:'@slotname6@',\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@6;\ncreate Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;\n CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,public.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@6;\n\nCREATE  SOURCE @QuerySource@7 USING DatabaseReader  (\n  ReplicationSlotName:'@slotname7@',\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@7;\ncreate Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;\n CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,public.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@7;\n\nCREATE  SOURCE @QuerySource@8 USING DatabaseReader  (\n  ReplicationSlotName:'@slotname8@',\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@8;\ncreate Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;\n CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,public.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@8;\n\nCREATE  SOURCE @QuerySource@9 USING DatabaseReader  (\n  ReplicationSlotName:'@slotname9@',\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@9;\ncreate Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;\n CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,public.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@9;\n\nCREATE  SOURCE @QuerySource@10 USING DatabaseReader  (\n  ReplicationSlotName:'@slotname10@',\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  Query: '@SourceQuery@'\n )\nOUTPUT TO @SRCQueryINPUTSTREAM@10;\ncreate Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;\n CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (\n  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',\n  Username:'waction',\n  Password:'w@ct10n',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  Tables: 'QUERY,public.@TargetTable@'\n )\nINPUT FROM @SRCQueryINPUTSTREAM@10;\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION orrs;\nUNDEPLOY APPLICATION orrs;\nDROP APPLICATION orrs CASCADE;\nCREATE APPLICATION orrs;\nCreate Type CSVType (\n  merchantName String,\n  companyname String\n);\n\nCreate Stream TypedFileStream of CSVType;\n\ncreate source CSVSource using FileReader (\n\tdirectory:'@DIRECTORY@',\n\tWildCard:'posdata5L.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\t\n)\nOUTPUT TO FileStream;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedFileStream\nSELECT TO_STRING(data[1]),TO_STRING(data[0])\nFROM FileStream;\n\nCREATE TARGET RSTarget USING RedshiftWriter\n(\n  ConnectionURL: '@TARGET_URL@',\n  Username: '@USERNAME@',\n  Password: '@PASSWORD@',\n  bucketname: '@BUCKET-NAME@',\n  --accesskeyId: '@ACCESS-KEY-ID@',\n  --secretaccesskey: '@SECRET-ACCESS-KEY@',\n   S3IAMRole:'@IAMROLE@',\n  Tables: '@TABLES@',\n  uploadpolicy: 'eventcount:10000, interval:1m'\n) INPUT FROM TypedFileStream;\nDEPLOY APPLICATION orrs;\nSTART APPLICATION orrs;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 4\n-- Nicholas Keene, WebAction, Inc.\n--\n-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS\n-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS\n--\n\nSTOP Recov4Tester.RecovTest4;\nUNDEPLOY APPLICATION Recov4Tester.RecovTest4;\nDROP APPLICATION Recov4Tester.RecovTest4 CASCADE;\nCREATE APPLICATION RecovTest4 RECOVERY 50 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE CsvData (\n  companyName String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstCity String\n);\n\nCREATE STREAM DataStream OF CsvData;\n\nCREATE CQ CsvToData\nINSERT INTO DataStream\nSELECT\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.city)\nFROM DataStream6Minutes p;\n\nEND APPLICATION RecovTest4;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop Oracle_LogWriter;\nUndeploy application Oracle_LogWriter;\ndrop application Oracle_LogWriter cascade;\n\nCREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;\n\nCREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  (\n\n  FetchSize: 1000,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n CheckColumn: 'striim.test01=t1',\n startPosition: '%=-1',\n  PollingInterval: '2sec',\n  ConnectionPoolSize: 5,\n  ThreadPoolSize: 5\n  )\n  OUTPUT TO data_stream;\ncreate target AzureSQLDWHTarget using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',\n\t\tuploadpolicy:'eventcount:1000,interval:300s'\n) INPUT FROM data_stream;\n  CREATE OR REPLACE TARGET TeraSys USING SysOut  (\n  name: 'ora12_out'\n ) INPUT FROM data_stream;\n\nEND APPLICATION Oracle_IRLogWriter;\ndeploy application Oracle_IRLogWriter;\nstart Oracle_IRLogWriter;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create or replace Target t using KinesisWriter (\n\tregionName:'TARGET_REGION',\n\tstreamName:'TARGET_STREAM',\n\taccesskeyid:'ACCESS_KEY',\n\tsecretaccesskey:'SECRET_KEY'\n)\nformat using JSONFormatter (\n)\ninput from DDLCDCStream;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\ncreate flow @APPNAME@_agentflow;\n\nCreate Source @SOURCE@\n        Using OracleReader\n(\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: 'qatest.test77',\n  FetchSize:1,\n  QueueSize:2000,\n  CommittedTransactions:true,\n  Compression:false\n)\nOutput To @STREAM@;\n\nend flow @APPNAME@_agentflow;\n\ncreate flow @APPNAME@_serverflow;\n\ncreate Target @TARGET@ using S3Writer(\n    bucketname:'@BUCKET@',\n   objectname:'upgradeData.csv',\n   foldername:'upgradefolder',\n  uploadpolicy:'EventCount : 10000,Interval :1m '\n)\nformat using DSVFormatter (\n)\ninput from @STREAM@;\n\nend flow @APPNAME@_serverflow;\n\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate or replace propertyvariable ms_pass='@SOURCE_PASS@';\nCREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\n\nCREATE OR REPLACE SOURCE @SOURCE@ USING MSSQLReader  ( \n  FilterTransactionBoundaries: true,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  Password: '$ms_pass',\n  Password_encrypted: 'true',\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  Username: '@SOURCE_USER@',\n  DatabaseName: 'qatest',\n  ConnectionPoolSize: 1,\n  StartPosition: 'EOF',\n  ReplicationSlotName: 'null'\n ) \nOUTPUT TO @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET@ USING DatabaseWriter  ( \n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: '@TARGET_USER@',\n  BatchPolicy: 'EventCount:1,Interval:0',\n  CommitPolicy: 'EventCount:1,Interval:0',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n  Password_encrypted: false\n ) \nINPUT FROM @STREAM@;\n\n\nEND APPLICATION @APPNAME@;\n\ndeploy application @APPNAME@ on ANY in default;\n\nstart application @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Canon Test W70\n-- Nicholas Keene, WebAction, Inc.\n--\n-- Basic test for a partitioned jumping count window\n--\n-- S -> JWc5p -> CQ -> WS\n--\n\n\nUNDEPLOY APPLICATION NameW70.W70;\nDROP APPLICATION NameW70.W70 CASCADE;\nCREATE APPLICATION W70 RECOVERY 5 SECOND INTERVAL;\n\n\n\n\nCREATE FLOW DataAcquisitionW70;\n\n\nCREATE SOURCE CsvSourceW70 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'Canon1000.csv',\n  columndelimiter:',',\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStreamW70;\n\n\nEND FLOW DataAcquisitionW70;\n\n\n\n\nCREATE FLOW DataProcessingW70;\n\nCREATE TYPE DataTypeW70 (\n    eventID String,\n    word String KEY,\n    dateTime DateTime\n);\n\nCREATE STREAM DataStreamW70 OF DataTypeW70;\n\nCREATE CQ CSVStreamW70_to_DataStreamW70\nINSERT INTO DataStreamW70\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[2],'yyyyMMddHHmmss')\nFROM CsvStreamW70;\n\nCREATE JUMPING WINDOW JWc5pW70\nOVER DataStreamW70\nKEEP 5 ROWS\nPARTITION BY word;\n\nCREATE WACTIONSTORE WactionStoreW70 CONTEXT OF DataTypeW70\nEVENT TYPES ( DataTypeW70 KEY(word) )\n@PERSIST-TYPE@\n\nCREATE CQ JWc5pW70_to_WactionStoreW70\nINSERT INTO WactionStoreW70\nSELECT\n    FIRST(eventID),\n    FIRST(word),\n    FIRST(dateTime)\nFROM JWc5pW70\nGROUP BY word;\n\nEND FLOW DataProcessingW70;\n\n\n\nEND APPLICATION W70;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 35 with two sources, two jumping count windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jc5W/p -> CQ1 -> WS\n--   S2 -> Jc6W/p -> CQ2 -> WS\n--\n\nSTOP Recov35Tester.RecovTest35;\nUNDEPLOY APPLICATION Recov35Tester.RecovTest35;\nDROP APPLICATION Recov35Tester.RecovTest35 CASCADE;\nCREATE APPLICATION RecovTest35 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP 5 ROWS\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP 6 ROWS\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nEND APPLICATION RecovTest35;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\ncreate application @APPNAME@ recovery 5 second interval;\nCREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9099', kafkaversion:'0.11');\n\nCREATE FLOW @APPNAME@_AgentFlow;\n\nCREATE STREAM @APPNAME@_PS_Stream1 OF Global.waevent persist using  @APPNAME@_KafkaPropset;\nCREATE STREAM @APPNAME@_PS_Stream2 OF Global.waevent persist using  @APPNAME@_KafkaPropset;\nCREATE STREAM @APPNAME@_PS_Stream3 OF Global.waevent persist using  @APPNAME@_KafkaPropset;\nCREATE STREAM @APPNAME@_PS_Stream4 OF Global.waevent persist using  @APPNAME@_KafkaPropset;\n\n\ncreate source @APPNAME@_source using FileReader (\ndirectory:'/Users/jenniffer/Product2/IntegrationTests/TestData/OGG/Recovery',\nWildCard:'lg*.gz',\npositionByEOF:false,\ncompressiontype:'gzip',\nrecoveryInterval: 5\n) parse using GGTrailParser (\nmetadata:'@META-FILE@'\n)\nOUTPUT TO @APPNAME@_Stream;\n\nEND FLOW @APPNAME@_AgentFlow;\n\nCREATE CQ @APPNAME@_CQ1\nINSERT INTO @APPNAME@_PS_Stream1\nSELECT *\nFROM @APPNAME@_Stream sm\nWHERE META(sm, 'TableName').toString() = 'MINER.CUSTOMER';\n\nCREATE CQ @APPNAME@_CQ2\nINSERT INTO @APPNAME@_PS_Stream2\nSELECT *\nFROM @APPNAME@_Stream sm\nWHERE META(sm, 'TableName').toString() = 'MINER.CUSTOMER';\n\nCREATE CQ @APPNAME@_CQ3\nINSERT INTO @APPNAME@_PS_Stream3\nSELECT *\nFROM @APPNAME@_Stream sm\nWHERE META(sm, 'TableName').toString() = 'MINER.CUSTOMER';\n\nCREATE CQ @APPNAME@_CQ4\nINSERT INTO @APPNAME@_PS_Stream4\nSELECT *\nFROM @APPNAME@_Stream sm\nWHERE META(sm, 'TableName').toString() = 'MINER.CUSTOMER';\n\n\nCREATE FLOW @APPNAME@_ServerFlow;\n\nCREATE TARGET @APPNAME@_target1 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'MINER.CUSTOMER,QATEST.CUSTOMER_TARGET_AG4'\n) INPUT FROM @APPNAME@_PS_Stream1;\n\nCREATE TARGET @APPNAME@_target2 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'MINER.CUSTOMER,QATEST.CUSTOMER_TARGET_AG4'\n) INPUT FROM @APPNAME@_PS_Stream2;\n\nCREATE TARGET @APPNAME@_target3 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'MINER.CUSTOMER,QATEST.CUSTOMER_TARGET_AG4'\n) INPUT FROM @APPNAME@_PS_Stream3;\n\nCREATE TARGET @APPNAME@_target4 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:10000,Interval:1',\nCommitPolicy:'Interval:1,Eventcount:10000',\nTables:'MINER.CUSTOMER,QATEST.CUSTOMER_TARGET_AG4'\n) INPUT FROM @APPNAME@_PS_Stream4;\n\nEND FLOW @APPNAME@_ServerFlow;\n\nend application @APPNAME@;\n\ndeploy application @APPNAME@ with @APPNAME@_AgentFlow in AGENTS, @APPNAME@_ServerFlow on any in default;\n\nstart application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 21 with two sources, two sliding count windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Sc5W -> CQ1 -> WS\n-- S2 -> Sc6W -> CQ2 -> WS\n--\n\nSTOP Recov21Tester.RecovTest21;\nUNDEPLOY APPLICATION Recov21Tester.RecovTest21;\nDROP APPLICATION Recov21Tester.RecovTest21 CASCADE;\nCREATE APPLICATION RecovTest21 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP 5 ROWS;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP 6 ROWS;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2;\n\nEND APPLICATION RecovTest21;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "undeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@;\nCreate Source @APPNAME@_s Using DatabaseReader\n(\n Username:'@UNAME@',\n Password:'@PASSWORD@',\n ConnectionURL:'@SRCURL@',\n Tables:'QATEST.HIVE_IL_%',\n FetchSize:1,\n QuiesceOnILCompletion: true\n)\nOutput To @APPNAME@_ss;\n\n\ncreate Target @APPNAME@_t using HiveWriter(\n            ConnectionURL:'@TGTURL@',\n            Username:'@TGTUNAME@', \n            Password:'@TGTPASSWORD@',\n            hadoopurl:'hdfs://dockerhost:9000/',\n\t        Mode:'initialLoad',\n\t        mergepolicy: '@MERGEPOLICY@',\n            Tables:'QATEST.HIVE_IL_01,default.hive_il_01;QATEST.HIVE_IL_02,default.hive_il_02',\n            hadoopConfigurationPath:'@CONF@'\n\t) input from @APPNAME@_ss;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop ROLLUPMON_CDC;\nundeploy application ROLLUPMON_CDC;\nalter application ROLLUPMON_CDC;\nCREATE or replace FLOW ROLLUPMON_CDC_flow;\nCreate or replace Source ROLLUPMON_CDC_Oraclesrc Using oraclereader(\n Username:'qatest',\n Password:'qatest',\n ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',\n Tables:'QATEST.ROLLUPMON_TABLE1;QATEST.ROLLUPMON_TABLE2;QATEST.ROLLUPMON_TABLE3;QATEST.ROLLUPMON_TABLE4;QATEST.ROLLUPMON_TABLE5',\n Fetchsize:1000,\n connectionRetryPolicy:'maxRetries=4',\n TransactionBufferSpilloverSize:'200MB',\n _h_fetchexactrowcount: 'true'\n)\nOutput To ROLLUPMON_CDC_OrcStrm;\nEND FLOW ROLLUPMON_CDC_flow;\nalter application ROLLUPMON_CDC recompile;\nDEPLOY APPLICATION ROLLUPMON_CDC;\nstart application ROLLUPMON_CDC;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.OJet (\n  PrimaryDatabasePassword: '@PRIMARY_PASSWORD@',\n  ConnectionURL: '@DOWNSTREAM_URL@',\n  PrimaryDatabaseUsername: '@PRIMARY_USER@',\n  Password: '@DOWNSTREAM_PASSWORD@',\n  DownstreamCaptureMode: 'REAL_TIME',\n  DownstreamCapture: true,\n  PrimaryDatabaseConnectionURL: '@PRIMARY_URL@',\n  Tables: '@SOURCE_TABLES@',\n  Username: '@DOWNSTREAM_USER@' )\nOUTPUT TO @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_SYS@ USING Global.SysOut (\n  name: 'Out' )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter (\n  ConnectionURL: '@TARGET_URL@',\n  Username: '@TARGET_USER@',\n  Password: '@TARGET_PASSWORD@',\n  CheckPointTable: 'CHKPOINT',\n  CommitPolicy: 'EventCount:1',\n  Tables: '@TARGET_TABLES@',\n  BatchPolicy: 'EventCount:1' )\nINPUT FROM @STREAM@;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()\nPARSE USING HL7v2Parser (\n  EnableMessageValidation: false,\n  MLLPDelimited: false\n  )\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()\nFORMAT USING XMLFormatter (\n    rootelement:\"document\",\n    charset: \"UTF-8\"\n    )\nINPUT FROM @APPNAME@_Stream;\n\nEND APPLICATION @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @APPNAME@;\nUNDEPLOY APPLICATION @APPNAME@;\nDROP APPLICATION @APPNAME@ CASCADE;\n\nCREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.OJet (\n  PrimaryDatabasePassword: '@PRIMARY_PASSWORD@',\n  ConnectionURL: '@DOWNSTREAM_URL@',\n  PrimaryDatabaseUsername: '@PRIMARY_USER@',\n  Password: '@DOWNSTREAM_PASSWORD@',\n  DownstreamCaptureMode: 'REAL_TIME',\n  DownstreamCapture: true,\n  PrimaryDatabaseConnectionURL: '@PRIMARY_URL@',\n  Tables: '@SOURCE_TABLES@',\n  CDDLCapture: true,\n  CDDLAction: 'Process',\n  Username: '@DOWNSTREAM_USER@' )\nOUTPUT TO @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_SYS@ USING Global.SysOut (\n  name: 'Out' )\nINPUT FROM @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter (\n  ConnectionURL: '@TARGET_URL@',\n  Username: '@TARGET_USER@',\n  Password: '@TARGET_PASSWORD@',\n  CheckPointTable: 'CHKPOINT',\n  CommitPolicy: 'EventCount:1',\n  Tables: '@TARGET_TABLES@',\n  BatchPolicy: 'EventCount:1' )\nINPUT FROM @STREAM@;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION persistencetester.ranGenAppES;\nUNDEPLOY APPLICATION persistencetester.ranGenAppES;\nDROP APPLICATION persistencetester.ranGenAppES cascade;\n\nCREATE APPLICATION ranGenAppES;\n\nCREATE TYPE Atm(\n  productID String KEY,\n  stateID String,\n  productWeight int,\n  quantity int,\n  size int,\n  myDate datetime\n);\n\nCREATE  SOURCE liveSource USING StreamReader (\n  OutputType: 'Atm',\n  noLimit: 'false',\n  maxRows: 20,\n  iterations: 0,\n  iterationDelay: 2000,\n  StringSet: 'productID[1001-1002-1003-1004],stateID[BofA-WellsFargo-Chase-NYBank]',\n  NumberSet: 'productWeight[3-3]R,quantity[0-200]R,size[250-1250]R'\n )\nOUTPUT TO CsvStream;\n\nCREATE STREAM ranDataStream OF Atm;\n\nCREATE CQ ranToRanData\nINSERT INTO ranDataStream\nSELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4]), TO_DATE(data[5])\nFROM CsvStream;\n\nCREATE WACTIONSTORE RanGenDataStore\nCONTEXT OF Atm\nEVENT TYPES ( Atm )\n@PERSIST-TYPE@\n\nCREATE CQ TrackRanGenDataActivity\nINSERT INTO RanGenDataStore\nSelect * from ranDataStream\nLINK SOURCE EVENT;\n\n\nEND APPLICATION ranGenAppES;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION TestAPP\nCREATE TYPE TestType(\n    id java.lang.Long KEY,\n    name java.lang.String\n);\n\nCREATE STREAM TestStream OF TestType;\n\n\n\n\nCREATE CQ TestCQ\nINSERT INTO TestStream\nSELECT\n    h.value,\n    TO_STRING(DNOW())\nFROM heartbeat(interval 1 second) h;\n\nEND APPLICATION TestAPP;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application RollOverTester.DSV;\nundeploy application RollOverTester.DSV;\ndrop application RollOverTester.DSV cascade;\n\ncreate application DSV;\ncreate source CSVSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'smallposdata.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\n  filename:'TargetPosDataXmlEC',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'eventcount:2000,sequence:00'\n)\nformat using XMLFormatter (\n  rootelement:'document',\n  elementtuple:'MerchantName:zip:text=merchantname'\n)\ninput from TypedCSVStream;\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlEC_actual.log') input from TypedCSVStream;\n\nend application DSV;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE APPLICATION @AppName@ recovery 5 second Interval;\n\nCREATE SOURCE @srcName@ USING Global.S3Reader ( \n secretaccesskey: '@secretaccesskey@', \n  foldername: '@sourcefoldername@', \n  blocksize: 64, \n  bucketname: '@bucketname@', \n  objectnameprefix: '@sourcefilename@', \n  accesskeyid: '@accesskeyid@' ) \nPARSE USING Global.DSVParser ( \n  trimwhitespace: false, \n  commentcharacter: '', \n  linenumber: '-1', \n  columndelimiter: ',', \n  trimquote: true, \n  columndelimittill: '-1', \n  ignoreemptycolumn: false, \n  separator: ':', \n  quoteset: '\\\"', \n  charset: 'UTF-8', \n  ignoremultiplerecordbegin: 'true', \n  ignorerowdelimiterinquote: false, \n  header: false, \n  blockascompleterecord: false, \n  rowdelimiter: '\\n', \n  nocolumndelimiter: false, \n  headerlineno: 0 ) \nOUTPUT TO @outstreamname@;\n\nCREATE OR REPLACE TARGET @tgtName@ USING Global.S3Writer ( \n  UploadConfigValueSeparator: '=', \n  UploadConfigPropertySeparator: ',', \n  objectname: '@targetfilename@', \n  secretaccesskey: '@secretaccesskey@', \n  ParallelThreads: '', \n  rolloveronddl: 'true', \n  bucketname: '@bucketname@', \n  foldername: '@targetfoldername@', \n  uploadpolicy: 'eventcount:5,interval:60s', \n  accesskeyid: '@accesskeyid@' ) \nFORMAT USING Global.DSVFormatter  ( \n  quotecharacter: '\\\"', \n  columndelimiter: ',', \n  members: 'data', \n  nullvalue: 'NULL', \n  usequotes: 'false', \n  rowdelimiter: '\\n', \n  standard: 'none', \n  header: 'false' ) \nINPUT FROM @instreamname@;\nEnd APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\ncreate application @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCreate Source @SOURCE@\n        Using OracleReader\n(\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: 'qatest.test77',\n  FetchSize:1,\n  QueueSize:2000,\n  CommittedTransactions:true,\n  Compression:false\n)\nOutput To @STREAM@;\n\ncreate Target @TARGET@ using googlepubsubwriter(\n    ServiceAccountKey:'@SAS-KEY@',\nProjectId:'@PROJECTID@',\ntopic:'@topic@',\nBatchPolicy:'@BATCHPOLICY@'\n)\nformat using DSVFormatter (\n)\ninput from @STREAM@;\n\nend application @APPNAME@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP UpdatableCacher.UpdatableCache;\nUNDEPLOY APPLICATION UpdatableCacher.UpdatableCache;\nDROP APPLICATION UpdatableCacher.UpdatableCache CASCADE;\nCREATE APPLICATION UpdatableCacher.UpdatableCache;\n\nCREATE TYPE MerchantHourlyAve(\n  merchantId String KEY,\n  hourlyAve Integer,\n  theDate DateTime,\n  price Double\n);\n\n\nCREATE source CsvDataSource USING FileReader (\n      directory:'@TEST-DATA-PATH@',\n      columndelimiter: ',',\n      wildcard:'ucData.csv',\n      blocksize: 10240,\n      positionByEOF:false\n)\nPARSE USING DSVParser (\n      header:No,\n      trimquote:false\n) OUTPUT TO CsvStream;\n\n\nCREATE STREAM S1 OF MerchantHourlyAve;\n\nCREATE CQ cq1\n\tinsert into S1\n\t\tSELECT data[0],\n\t\t\t\tTO_INT(data[1]),\n\t\t\t\tTO_DATE(data[2]),\n\t\t\t\tTO_DOUBLE(data[3])\n\t\tFROM CsvStream;\n\n\nCREATE EVENTTABLE ET1 using STREAM (\n  NAME: 'S1'\n) QUERY (keytomap:'price', persistPolicy: 'true' ) OF MerchantHourlyAve;\n\n\nCREATE EVENTTABLE ET2 using STREAM (\n  NAME: 'S1'\n) QUERY (keytomap:'merchantId' ) OF MerchantHourlyAve;\n--) QUERY (keytomap:'merchantId', uniquekey:'price' ) OF MerchantHourlyAve;\n\n\n\nEND APPLICATION UpdatableCache;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP DSAPP;\nUNDEPLOY APPLICATION DSAPP;\nDROP APPLICATION DSAPP CASCADE;\n\nCREATE APPLICATION DSAPP;\n\n-- CacheWaction WACTIONSTORE is being loaded from DSCache\n\nCREATE OR REPLACE WACTIONSTORE CacheWaction CONTEXT OF T1\nEVENT TYPES ( T1 )\n@PERSIST-TYPE@\n\nCREATE CQ CQ2Derby\nINSERT INTO CacheWaction\nselect * from C1\nLINK SOURCE EVENT;\n\nEND APPLICATION DSAPP;\nDEPLOY APPLICATION DSAPP;\nSTART APPLICATION DSAPP;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCreate Source @SourceName@\n Using OracleReader\n(\n Username:'@UserName@',\n Password:'@Password@',\n ConnectionURL:'@SourceConnectionURL@',\n Tables:'@SourceTable@',\n Fetchsize:1\n)\nOutput To @SRCINPUTSTREAM@;\n\nCREATE OR REPLACE CQ @CQName@ INSERT INTO @CQOUTPUTSTREAM@ select @FUNCTION@ from @SRCINPUTSTREAM@ s ;\n\ncreate Target @targetsys@ using SysOut(name:Foo2) input from @CQOUTPUTSTREAM@;\n\nCREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( \n  ConnectionURL: '@TargetConnectionURL@',\n  Username: '@UserName@',\n  Password: '@Password@',\n  BatchPolicy: 'EventCount:1,Interval:1',\n  Tables: '@TargetTableMapping@'\n ) \nINPUT FROM @CQOUTPUTSTREAM@;\n\nEND APPLICATION @APPNAME@;\ndeploy application @APPNAME@ in default;\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Multi Node Test 1 with uniqe objects to this App\n-- Nicholas Keene, Bert Hashemi WebAction, Inc.\n--\n-- App with Single Source, CQ and Waction Store to be deployed on two or more node\n-- Full app on node1 and one Source on node2 so source reads from node2 only.\n--\n-- S -> CQ -> WS\n--\n\n\nCREATE APPLICATION RecovTestMN01\nRECOVERY 5 SECOND INTERVAL;\n\n\n\nCREATE FLOW DataAcquisition;\n\nCREATE SOURCE CsvSourceMN01 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStreamMN01;\n\nEND FLOW DataAcquisition;\n\n\n\nCREATE FLOW DataProcessing;\n\nCREATE TYPE WactionTypeMN01 (\n  merchantId String,\n  dateTime DateTime,\n  amount double,\n  city String,\n  serverName String KEY\n);\n\nCREATE STREAM DataStreamMN01 OF WactionTypeMN01\nPARTITION BY serverName;\n\nCREATE CQ CsvToDataMN01\nINSERT INTO DataStreamMN01\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10],\n    data[11]\nFROM CsvStreamMN01;\n\nCREATE WINDOW DataWindowMN01\nOVER DataStreamMN01 KEEP WITHIN 30 SECOND ON dateTime\nPARTITION BY serverName;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionTypeMN01\nEVENT TYPES ( WactionTypeMN01 )\nPERSIST EVERY 1 second USING (\nJDBC_DRIVER:'@WASTORE-DRIVER@',  JDBC_URL:'@WASTORE-URL@;CREATE=true',\nJDBC_USER:'@WASTORE-UNAME@', JDBC_PASSWORD:'@WASTORE-PASSWORD@', pu_name:@WASTORE-TYPE@,\nDDL_GENERATION:'create-or-extend-tables',  LOGGING_LEVEL:'SEVERE' );\n\nCREATE CQ InsertWactions\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataWindowMN01;\n\nEND FLOW DataProcessing;\n\n\n\nEND APPLICATION RecovTestMN01;\n\n\nDEPLOY APPLICATION RecovTestMN01 WITH DataAcquisition ON ALL IN agents, DataProcessing ON ALL IN servers;\n\nSTART RecovTestMN01;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION IR recovery 5 second interval;\n\nCREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( \n \n  FetchSize: 5000,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n CheckColumn: 'striim.test01=t1',\n startPosition: '%=-1',\n  PollingInterval: '20sec'\n  )\n  OUTPUT TO data_stream;\ncreate target AzureSQLDWHTarget using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',\n\t\tuploadpolicy:'eventcount:10000,interval:300s'\n) INPUT FROM data_stream;\n  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM data_stream;\n\nEND APPLICATION IR;\ndeploy application IR;\nstart IR;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@ @RECOVERY@;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src1 USING Global.GCSReader (\n  _h_GCSQueryCoolingTime: '1',\n  ServiceAccountKey: '',\n  ProjectId: '',\n  DownloadPolicy: '',\n  BucketName: '',\n  compressiontype: '',\n  PollingInterval: ,\n  FolderName: '',\n  UseStreaming: false,\n  StartTimestamp: '',\n  ObjectFilter: '',\n  ObjectDetectionMode: '',\n  IncludeSubfolders: false )\nPARSE USING Global.DSVParser ()\nOUTPUT TO @APPNAME@_Stream1;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt1 USING S3Writer (\n  objectname: '',\n  foldername: '',\n  bucketname: '',\n  uploadpolicy: '' )\nFORMAT USING DSVFormatter (\nmembers:'data')\nINPUT FROM @APPNAME@_Stream1;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src2 USING Global.GCSReader (\n  _h_GCSQueryCoolingTime: '1',\n  ServiceAccountKey: '',\n  ProjectId: '',\n  DownloadPolicy: '',\n  BucketName: '',\n  compressiontype: '',\n  PollingInterval: ,\n  FolderName: '',\n  UseStreaming: false,\n  StartTimestamp: '',\n  ObjectFilter: '',\n  ObjectDetectionMode: '',\n  IncludeSubfolders: false )\nPARSE USING Global.JSONParser ()\nOUTPUT TO @APPNAME@_Stream2;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt2 USING S3Writer (\n  objectname: '',\n  foldername: '',\n  bucketname: '',\n  uploadpolicy: '' )\nFORMAT USING JSONFormatter ()\nINPUT FROM @APPNAME@_Stream2;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src3 USING GCSReader (\n  _h_GCSQueryCoolingTime: '1',\n  ServiceAccountKey: '',\n  ProjectId: '',\n  DownloadPolicy: '',\n  BucketName: '',\n  compressiontype: '',\n  PollingInterval: ,\n  FolderName: '',\n  UseStreaming: false,\n  StartTimestamp: '',\n  ObjectFilter: '',\n  ObjectDetectionMode: '',\n  IncludeSubfolders: false )\nPARSE USING AvroParser ()\nOUTPUT TO @APPNAME@_Stream3;\n\nCREATE CQ @APPNAME@_CQ3\nINSERT INTO @APPNAME@_CQOut3\nSELECT AvroToJson(data,false) FROM @APPNAME@_Stream3;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt3 USING S3Writer (\n  objectname: '',\n  foldername: '',\n  bucketname: '',\n  uploadpolicy: '' )\nFORMAT USING JSONFormatter ()\nINPUT FROM @APPNAME@_CQOut3;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src4 USING Global.GCSReader (\n  _h_GCSQueryCoolingTime: '1',\n  ServiceAccountKey: '',\n  ProjectId: '',\n  DownloadPolicy: '',\n  BucketName: '',\n  compressiontype: '',\n  PollingInterval: ,\n  FolderName: '',\n  UseStreaming: false,\n  StartTimestamp: '',\n  ObjectFilter: '',\n  ObjectDetectionMode: '',\n  IncludeSubfolders: false )\nPARSE USING Global.XMLParser (\n  rootnode: ''\n)\nOUTPUT TO @APPNAME@_Stream4;\n\nCREATE OR REPLACE TARGET @APPNAME@_trgt4 USING S3Writer (\n  objectname: '',\n  foldername: '',\n  bucketname: '',\n  uploadpolicy: '' )\nFORMAT USING JSONFormatter ()\nINPUT FROM @APPNAME@_Stream4;\n\nEND APPLICATION @APPNAME@;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP Istreamer.ISAPP;\nUNDEPLOY APPLICATION Istreamer.ISAPP;\nDROP APPLICATION Istreamer.ISAPP CASCADE;\n\nCREATE APPLICATION ISAPP;\n\n\nCREATE source implicitSource USING FileReader (\n      directory:'@TEST-DATA-PATH@',\n      columndelimiter: ',',\n      wildcard:'ISdata.csv',\n      blocksize: 10240,\n      positionByEOF:false\n)\nPARSE USING DSVParser (\n      header:False,\n      trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE Atm(\n  productID String KEY,\n  stateID String,\n  productWeight int,\n  quantity double,\n  size long,\n  currentDate DateTime);\n\nCREATE CACHE cache1 USING CsvReader(\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'ISdata.csv',\n  header: false,\n  columndelimiter: ',',\n  trimquote: false\n  ) QUERY (keytomap:'productID') OF Atm;\n\n\nCREATE STREAM newStream OF Atm;\n\n\nCREATE CQ newCQ\nINSERT INTO newStream\nSELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]), TO_DATE(data[5]) FROM\nCsvStream;\n\nCREATE WINDOW win1\nOVER newStream\nKEEP 50 rows;\n\n\nCREATE CQ newCQ2\nINSERT INTO newStream2\nSELECT productID as A , stateID AS B, productWeight AS C, quantity AS D, size AS E, currentDate AS F FROM\nnewStream;\n\n\nCREATE CQ newCQ3\nINSERT INTO newStream3 PARTITION BY A\nSELECT A,B,C,D,E,F FROM newStream2\nlink source event;\n\nCREATE CQ newCQ4\nINSERT INTO newStream4\nSELECT count(productID),currentDate FROM newStream ORDER BY currentDate\nlink source event;\n\nCREATE CQ newCQ5\nINSERT INTO newStream5\nSELECT x.*, y.* from cache1 x, newStream y WHERE x.productweight > 6 ORDER BY x.currentDate, x.productID;\n\n\nCREATE WACTIONSTORE WS1 CONTEXT OF Atm\nEVENT TYPES(Atm );\n\nCREATE CQ newCQ6\nINSERT INTO WS1\nSELECT * FROM newStream WHERE productID = '001';\n\nCREATE CQ newCQ7\nINSERT INTO newStream6\nSELECT aa.productID FROM WS1 [push] aa, cache1 bb;\n\nCREATE CQ newCQ8\nINSERT INTO newStream7\nSELECT Sum(X.size) FROM (Select size from win1 where productweight > 5) X;\n\nCREATE CQ newCQ9\nINSERT INTO newStream8\nSELECT count(productID) FROM WS1 [push] ORDER BY productID;\n\n\nEND APPLICATION ISAPP;\ndeploy APPLICATION ISAPP;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;\nCREATE OR REPLACE SOURCE @AppName@_Source USING FileReader (\ndirectory:'@dataDir@',\nwildcard:'data.csv',\npositionByEOF:false\n)\nPARSE USING DSVParser (\n) OUTPUT TO @AppName@_rawstream;\n\nCREATE OR REPLACE STREAM @BuiltinFunc@_Stream OF Global.WAEVent;\nCREATE OR REPLACE STREAM CombineStream OF Global.WAEVent;\n\nCREATE OR REPLACE CQ cq1\nINSERT INTO @BuiltinFunc@_Stream\nSELECT\n@BuiltinFunc@(s1, 'city',data[5])\nFROM @AppName@_rawstream s1;\n\nCREATE OR REPLACE CQ cq2\nINSERT INTO CombineStream\nSelect *\nFROM @BuiltinFunc@_Stream s4;\n\nCREATE OR REPLACE CQ cq3\nINSERT INTO CombineStream\nselect *\nFROM @AppName@_rawstream s5;\n\nCREATE OR REPLACE TARGET @AppName@_FileTarget USING Global.FileWriter ( \n  flushpolicy: 'EventCount:10000,Interval:30s', \n  adapterName: 'FileWriter', \n  directory: '@logs@',\n  filename: '@BuiltinFunc@_Data', \n  rolloverpolicy: 'EventCount:10000,Interval:30s' ) \nFORMAT USING Global.JSONFormatter  ( \n  handler: 'com.webaction.proc.JSONFormatter', \n  jsonMemberDelimiter: '\\n', \n  EventsAsArrayOfJsonObjects: 'true', \n  formatterName: 'JSONFormatter', \n  jsonobjectdelimiter: '\\n' ) \nINPUT FROM CombineStream;\n\nEnd application @AppName@;\nDeploy application @AppName@; \nStart application @AppName@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 2 on two node cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS\n--\n\nSTOP APPLICATION KStreamN2S2CR2Tester.KStreamN2S2CRTest2;\nUNDEPLOY APPLICATION KStreamN2S2CR2Tester.KStreamN2S2CRTest2;\nDROP APPLICATION KStreamN2S2CR2Tester.KStreamN2S2CRTest2 CASCADE;\n\nDROP USER KStreamN2S2CR2Tester;\nDROP NAMESPACE KStreamN2S2CR2Tester CASCADE;\nCREATE USER KStreamN2S2CR2Tester IDENTIFIED BY KStreamN2S2CR2Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR2Tester;\nCONNECT KStreamN2S2CR6Tester KStreamN2S2CR2Tester;\n\nCREATE APPLICATION KStreamN2S2CRTest2 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionKStreamN2S2CRTest2;\n\nCREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');\nCREATE STREAM KafkaCsvStream using KafkaProps;\n\nCREATE SOURCE CsvSourceKStreamN2S2CRTest2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nEND FLOW DataAcquisitionKStreamN2S2CRTest2;\n\nCREATE FLOW DataProcessingKStreamN2S2CRTest2;\n\nCREATE TYPE WactionTypeKStreamN2S2CRTest2 (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionTypeKStreamN2S2CRTest2;\n\nCREATE CQ CsvToWaction\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsKStreamN2S2CRTest2 CONTEXT OF WactionTypeKStreamN2S2CRTest2\nEVENT TYPES ( WactionTypeKStreamN2S2CRTest2 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsKStreamN2S2CRTest2\nINSERT INTO WactionsKStreamN2S2CRTest2\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND FLOW DataProcessingKStreamN2S2CRTest2;\n\nEND APPLICATION KStreamN2S2CRTest2;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@;\n\nCREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');\n\nCREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.AvroEvent PERSIST USING @APPNAME@KafkaPropset;\n\nCREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING Global.JMSReader (\n  ProviderName: '',\n  Provider: '',\n  Ctx: '',\n  QueueName: '',\n  Topic:'',\n  UserName: '',\n  Password: '',\n  EnableTransaction: '',\n  transactionpolicy: ''\n  )\nPARSE USING Global.ParquetParser ()\nOUTPUT TO @APPNAME@PersistStream@RANDOM@;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create Target @TARGET_NAME@ using ADLSGen2Writer(\n          accountname:'adlsgen2nntest',\n        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',\n        filesystemname:'qatestadls2',\n        directory:'%@metadata(TableName)%',\n        filename:'table.csv',\n        uploadpolicy:'filesize:10M'\n)\nformat using ParquetFormatter (\n\tschemaFileName:'@FILENAME@',\n\tFormatAs:'@FORMATAS@',\n\tmembers:'Table=@metadata(TableName),OpName=@metadata(OperationName)'\n)\ninput from @STREAM@;\t"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Kafka Stream Recovery Test 1 with FileWriter as Target\n-- Amudha, Striim, Inc.\n--\n-- S -> CQ -> KS -> WS\n\nSTOP KStreamRecov1Tester.KStreamRecovTest1wfwr;\nUNDEPLOY APPLICATION KStreamRecov1Tester.KStreamRecovTest1wfwr;\nDROP APPLICATION KStreamRecov1Tester.KStreamRecovTest1wfwr CASCADE;\nDROP USER KStreamRecov1Tester;\nDROP NAMESPACE KStreamRecov1Tester CASCADE;\nCREATE USER KStreamRecov1Tester IDENTIFIED BY KStreamRecov1Tester;\n-- GRANT 'Global:create,drop:deploymentgroup:*' TO USER KStreamRecov1Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov1Tester;\nCONNECT KStreamRecov1Tester KStreamRecov1Tester;\n\nCREATE APPLICATION KStreamRecovTest1wfwr RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');\nCREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nCREATE or REPLACE TYPE CsvStreamType(\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE STREAM TypedStream OF CsvStreamType; \n\nCREATE CQ InsertEvents\nINSERT INTO TypedStream\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM KafkaCsvStream;\n\nCREATE TARGET FileWrt USING FILEWRITER (\n\tdirectory:'@FEATURE-DIR@/logs/',\n\tFILENAME:'FileKafkaStream.log',\n\tflushpolicy:'eventcount:1'\n--\trolloverpolicy:'EventCount:10000,Interval:30s'\n)\nFORMAT USING DSVFormatter(\n\n) \nINPUT FROM TypedStream;\n\n\nEND APPLICATION KStreamRecovTest1wfwr;\nDEPLOY APPLICATION KStreamRecovTest1wfwr;\nSTART APPLICATION KStreamRecovTest1wfwr;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 37 with two sources, two jumping time windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n--   S1 -> Jt1W/p -> CQ1 -> WS\n--   S2 -> Jt2W/p -> CQ2 -> WS\n--\n\nSTOP Recov37Tester.RecovTest37;\nUNDEPLOY APPLICATION Recov37Tester.RecovTest37;\nDROP APPLICATION Recov37Tester.RecovTest37 CASCADE;\nCREATE APPLICATION RecovTest37 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP WITHIN 1 SECOND\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP WITHIN 2 SECOND\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nEND APPLICATION RecovTest37;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Canon Test W72\n-- Nicholas Keene, WebAction, Inc.\n--\n-- Basic test for sliding count window\n--\n-- S -> SWc5p -> CQ1(aggregate) -> WS\n-- S -> SWc2p -> CQ1(aggregate) -> WS\n--\n\n\nUNDEPLOY APPLICATION NameW72.W72;\nDROP APPLICATION NameW72.W72 CASCADE;\nCREATE APPLICATION W72 RECOVERY 5 SECOND INTERVAL;\n\n\n\n\nCREATE FLOW DataAcquisitionW72;\n\n\nCREATE SOURCE CsvSourceW72 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'Canon1000.csv',\n  columndelimiter:',',\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStreamW72;\n\n\nEND FLOW DataAcquisitionW72;\n\n\n\n\nCREATE FLOW DataProcessingW72;\n\nCREATE TYPE DataTypeW72 (\n    eventID String,\n    word String KEY,\n    dateTime DateTime\n);\n\nCREATE STREAM DataStreamW72 OF DataTypeW72\nPARTITION BY word;\n\nCREATE CQ CSVStreamW72_to_DataStreamW72\nINSERT INTO DataStreamW72\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[2],'yyyyMMddHHmmss')\nFROM CsvStreamW72;\n\nCREATE JUMPING WINDOW JWc5pW72\nOVER DataStreamW72\nKEEP 5 ROWS\nPARTITION BY word;\n\nCREATE JUMPING WINDOW JWc2pW72\nOVER DataStreamW72\nKEEP 2 ROWS\nPARTITION BY word;\n\nCREATE WACTIONSTORE WactionStoreW72 CONTEXT OF DataTypeW72\nEVENT TYPES ( DataTypeW72 KEY(word) )\n@PERSIST-TYPE@\n\nCREATE CQ JWc5pW72_to_WactionStoreW72\nINSERT INTO WactionStoreW72\nSELECT\n    FIRST(eventID),\n    FIRST(word),\n    FIRST(dateTime)\nFROM JWc5pW72 p;\n\nCREATE CQ JWc2pW72_to_WactionStoreW72\nINSERT INTO WactionStoreW72\nSELECT\n    FIRST(eventID),\n    FIRST(word),\n    FIRST(dateTime)\nFROM JWc2pW72;\n\nEND FLOW DataProcessingW72;\n\n\n\nEND APPLICATION W72;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE TARGET  @TARGET_NAME@ USING CassandraCosmosDBWriter  (\n  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  --CheckPointTable: '',\n  FlushPolicy: 'EventCount:1,Interval:60',\n  AccountEndpoint: 'cassandracosmostest.cassandra.cosmos.azure.com',\n  AccountKey: 'pqDZvVgbdSCg7VzIzD77dAhPG2odGRZPLhAQA1qnZbAKoIDk6RuQX5r2phbRQFnR1l54qxOcvBXNdz8DeijYIg==',\n  KeySpace: 'test',\n  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',\n  --ConsistencyLevel:'fh',\n  --port:'10505',\n  Tables: 'QATEST.Source1,test.target1',\n  --Tables: 'QATEST.OracToCql_alldatatypes,test.tgt_data',\n  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',\n  adapterName: 'CassandraCosmosDBWriter'\n )\n INPUT FROM @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 23 with two sources, two sliding time windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> St1W -> CQ1 -> WS\n-- S2 -> St2W -> CQ2 -> WS\n--\n\nSTOP Recov23Tester.RecovTest23;\nUNDEPLOY APPLICATION Recov23Tester.RecovTest23;\nDROP APPLICATION Recov23Tester.RecovTest23 CASCADE;\nCREATE APPLICATION RecovTest23 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP WITHIN 1 SECOND;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP WITHIN 2 SECOND;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2;\n\nEND APPLICATION RecovTest23;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION DBRTOCW;\nUNDEPLOY APPLICATION DBRTOCW;\nDROP APPLICATION DBRTOCW CASCADE;\nCREATE APPLICATION DBRTOCW;\n\nCREATE TYPE employee(\nid String,\nename String\n);\n\nCREATE STREAM Oracle_ChangeDataStream of employee;\n\ncreate source CSVSource using FileReader (\n\tdirectory:'/Users/jenniffer/Product2/IntegrationTests/TestData/',\n\tWildCard:'posdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\n)\nOUTPUT TO FileStream;\n\nCREATE CQ CQfilter\nINSERT INTO Oracle_ChangeDataStream\nselect data[0],data[1] from FileStream;\n\n\ncreate Target t2 using SysOut(name:OrgData) input from Oracle_ChangeDataStream;\nCREATE OR REPLACE Target DBTarget USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:1000,Interval:60',\n  CommitPolicy: 'EventCount:1000,Interval:60',\n  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',\n  Tables: 'test.employee',\n  Password: 'cassandra',\n  Password_encrypted: false\n )INPUT FROM Oracle_ChangeDataStream;\n\nEND APPLICATION DBRTOCW;\nDEPLOY APPLICATION DBRTOCW;\nSTART APPLICATION DBRTOCW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop IR;\nundeploy application IR;\ndrop application IR cascade;\n\nCREATE APPLICATION IR;\n\nCREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( \n  FetchSize: 100,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',\n  Tables: 'striim.autotest01',\n  adapterName: 'IncrementalBatchReader',\n CheckColumn: 'striim.autotest01=id',\n startPosition: 'striim.autotest01=2',\n  PollingInterval: '5sec'\n  )\n  OUTPUT TO data_stream;\n\n  create type AutoType(\n  ID int,\n  col1 int,\n  col2 string,\n  t1 DateTime,\n  t2 DateTime\n);\n\nCREATE STREAM CDCdata_stream OF AutoType;\n\nCREATE CQ Lookup\nINSERT INTO CDCdata_stream\nselect data[0],data[1],data[2],data[3],data[4] from data_stream;\n\nCREATE  TARGET AzureSQLDWHTarget1 USING AzureSQLDWHWriter  ( \n  ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n  username: 'striim',\n  password: 'W3b@ct10n',\n  storageaccount: 'striimqatestdonotdelete',\n  accesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n  Tables: 'STRIIM.AUTOTEST01',\n  uploadpolicy: 'eventcount:1,interval:10s'\n ) \nINPUT FROM CDCdata_stream;\n\nCREATE OR REPLACE TARGET sys USING SysOut  ( \n  name: 'ora12_out'\n ) INPUT FROM CDCdata_stream;\n\n\nEND APPLICATION IR;\ndeploy application IR;\nstart IR;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE OR REPLACE SOURCE @APPNAME@_src USING Global.GCSReader (\n  _h_GCSQueryCoolingTime: '1',\n  ServiceAccountKey: '',\n  ProjectId: '',\n  DownloadPolicy: '',\n  BucketName: '',\n  FolderName: '',\n  ObjectFilter: '',\n  ObjectDetectionMode: '',\n  IncludeSubfolders: false )\nPARSE USING Global.DSVParser ()\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE OR REPLACE CQ @APPNAME@_CQ\nINSERT INTO @APPNAME@_CQOut\nSELECT\n    data[0] as BusinessName,\n    data[1] as MerchantId,\n    data[2] as PosDataCode,\n    data[3] as AccNumber,\n    data[4] as DateTime,\n    data[5] as ExpDate,\n    data[6] as CurrencyCode,\n    data[7] as AuthAmount,\n    data[8] as TerminalId,\n    data[9] as Zip,\n    data[10] as City\nFROM @APPNAME@_Stream;\n\nCREATE OR REPLACE TARGET @APPNAME@_KafkaTarget USING Global.KafkaWriter VERSION @KAFKA_VERSION@(\n  brokerAddress: '',\n  Topic: '',\n  Mode: 'Sync' )\nFORMAT USING DSVFormatter()\nINPUT FROM @APPNAME@_CQOut;\n\nCREATE OR REPLACE TARGET @APPNAME@_HDFSTarget USING Global.HDFSWriter (\n  flushpolicy: '',\n  rolloverpolicy: '',\n  directory: '',\n  hadoopurl: '',\n  hadoopconfigurationpath: '',\n  filename: '')\nFORMAT USING DSVFormatter()\nINPUT FROM @APPNAME@_CQOut;\n\nCREATE OR REPLACE TARGET @APPNAME@_BlobTarget USING Global.AzureBlobWriter (\n  containername: '',\n  blobname: '',\n  accountaccesskey: '',\n  accountname: '',\n  foldername: '' )\nFORMAT USING JSONFormatter()\nINPUT FROM @APPNAME@_CQOut;\n\nCREATE OR REPLACE TARGET @APPNAME@_DWHTarget USING Global.BigQueryWriter (\n  Tables: '',\n  BatchPolicy: '',\n  projectId: '',\n  ServiceAccountKey: '' )\nINPUT FROM @APPNAME@_CQOut;\n\nCREATE OR REPLACE TARGET @APPNAME@_NoSqlTarget USING Global.MongoDBWriter (\n  AuthDB: 'admin',\n  ConnectionURL: '',\n  Username: '',\n  collections: '',\n  Password: '' )\nINPUT FROM @APPNAME@_CQOut;\n\nCREATE OR REPLACE TARGET @APPNAME@_FileTarget USING Global.FileWriter (\n  flushpolicy: '',\n  directory: '',\n  rolloverpolicy: '',\n  filename: '' )\nFORMAT USING DSVFormatter()\nINPUT FROM @APPNAME@_CQOut;\n\nCREATE OR REPLACE TARGET @APPNAME@_OLTPTarget USING Global.DatabaseWriter (\n  ConnectionURL: '',\n  Password: '',\n  Username: '',\n  Tables: '' )\nINPUT FROM @APPNAME@_CQOut;\n\nCREATE SOURCE @APPNAME@_KafkaSource USING KafkaReader VERSION @KAFKA_VERSION@ (\n  brokerAddress: '',\n  Topic: '',\n  startOffset: '0' )\nPARSE USING DSVParser ()\nOUTPUT TO @APPNAME@_Stream2;\n\nCREATE OR REPLACE TARGET @APPNAME@_KafkaFileTarget USING Global.FileWriter (\n  flushpolicy: '',\n  directory: '',\n  rolloverpolicy: '',\n  filename: '' )\nFORMAT USING JSONFormatter(\nmembers:'data')\nINPUT FROM @APPNAME@_Stream2;\n\nEND APPLICATION @APPNAME@;\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--This is a dummy file.\n--Framework doesn't support to create an empty resource directory.\n--Purpose of creating resource dir here is to store test logs in specific test related directory.\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader\n(\nFetchSize:'@FETCHSIZE@',\nQueueSize:'@QUEUESIZE@',\nUsername:'@USERNAME@',\nPassword:'@PASSWORD@',\nConnectionURL:'@URL@',\n_h_useClassic: @CLASSICMODE@,\nTables:'@TABLES@',\nCommittedTransactions:'@COMMITEDT@'\nCompression:'@COMPRESSION@'\n)\nOUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)\nSELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[0]) = '1' ;\n\nCREATE TYPE LogType(\nNum_col String key,\nChar_col String,\nVarchar2_col String,\nlong_col String,\nTable String,\nOperation String\n);\n\nCREATE WINDOW CDCWindow\nOVER @STREAM@\nKEEP 1 ROWS;\n\nCREATE WACTIONSTORE CDCWS CONTEXT of LogType\nEVENT TYPES ( LogType )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ ToWactionStore\nINSERT INTO CDCWS\nSELECT * FROM CDCWindow\nLINK SOURCE EVENT;\n\nCREATE TARGET @SOURCE_NAME@_SYS USING SysOut (\n  name: '@SOURCE_NAME@_SYS' )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 6 with sliding window and partitioned feature\n-- Nicholas Keene, Bert Hashemi WebAction, Inc.\n--\n-- S -> CQ -> SW(partitioned) -> CQ(no aggregate) -> WS\n--\n\nSTOP Recov6Tester.RecovTest6;\nUNDEPLOY APPLICATION Recov6Tester.RecovTest6;\nDROP APPLICATION Recov6Tester.RecovTest6 CASCADE;\nCREATE APPLICATION RecovTest6 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream OF CsvData PARTITION BY merchantId;\n\nCREATE CQ CsvToData\nINSERT INTO DataStream\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream;\n\nCREATE WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND APPLICATION RecovTest6;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 2 on four node all server cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS\n--\n\nSTOP APPLICATION N4S4CR2Tester.N4S4CRTest2;\nUNDEPLOY APPLICATION N4S4CR2Tester.N4S4CRTest2;\nDROP APPLICATION N4S4CR2Tester.N4S4CRTest2 CASCADE;\nCREATE APPLICATION N4S4CRTest2 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN4S4CRTest2;\n\nCREATE SOURCE CsvSourceN4S4CRTest2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN4S4CRTest2;\n\nCREATE FLOW DataProcessingN4S4CRTest2;\n\nCREATE TYPE WactionTypeN4S4CRTest2 (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionTypeN4S4CRTest2;\n\nCREATE CQ CsvToWaction\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsN4S4CRTest2 CONTEXT OF WactionTypeN4S4CRTest2\nEVENT TYPES ( WactionTypeN4S4CRTest2 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsN4S4CRTest2\nINSERT INTO WactionsN4S4CRTest2\nSELECT\n    *\nFROM DataStream5Minutes;\n\nEND FLOW DataProcessingN4S4CRTest2;\n\nEND APPLICATION N4S4CRTest2;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop DBRTOCW;\nundeploy application DBRTOCW;\ndrop application DBRTOCW cascade;\nCREATE APPLICATION DBRTOCW ;\n\nCREATE STREAM Oracle_ChangeDataStream of Global.WAEvent;\n\nCREATE OR REPLACE SOURCE DBSource USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: false,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: 'jdbc:oracle:thin:@//192.168.56.101:1521/orcl',\n  Tables: 'QATEST.OracToCql_alldatatypes',\n  adapterName: 'OracleReader',\n  Password: 'qatest',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: 'qatest',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO Oracle_ChangeDataStream1;\n\n\nCREATE CQ CQfilter\nINSERT INTO Oracle_ChangeDataStream\nselect putuserdata (data1,'IntToInt', data[0]) from Oracle_ChangeDataStream1 data1\nwhere (META(data1, 'OperationName').toString() =='INSERT' or META(data1, 'OperationName').toString() =='UPDATE');\n\nCREATE STREAM Oracle_DataStream of Global.WAEvent;\n\nCREATE CQ CQfilter1\nINSERT INTO Oracle_DataStream\nselect * from Oracle_ChangeDataStream c where to_int(USERDATA(c, 'IntToInt'))<4;\n\nCREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:1,Interval:60',\n  CommitPolicy: 'EventCount:1,Interval:60',\n  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',\n  Tables: 'QATEST.OracToCql_alldatatypes,test.oractocq_alldatatypes columnmap(IntToInt=IntToInt)',\n  Password: 'cassandra',\n  Password_encrypted: false\n )\nINPUT FROM Oracle_DataStream;\n\ncreate Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;\n\nEND APPLICATION DBRTOCW;\ndeploy application DBRTOCW in default;\nstart DBRTOCW;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 13 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same compound key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> CW(p#p) -> CQ -> WS\n--\n\nSTOP Recov13Tester.RecovTest13;\nUNDEPLOY APPLICATION Recov13Tester.RecovTest13;\nDROP APPLICATION Recov13Tester.RecovTest13 CASCADE;\nCREATE APPLICATION RecovTest13 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTest10Data.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE CsvData (\n  partKey String KEY,\n  serialNumber int,\n  partKey2 String KEY\n);\n\nCREATE TYPE WactionData (\n  partKey String KEY,\n  serialNumber int\n);\n\nCREATE STREAM DataStream OF CsvData PARTITION BY partKey, partKey2;\n\nCREATE CQ CsvToData\nINSERT INTO DataStream\nSELECT\n    data[0],\n    TO_INT(data[1]),\n    data[0]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStreamTwoItems\nOVER DataStream KEEP 2 ROWS\nPARTITION BY partKey, partKey2;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction\nINSERT INTO Wactions\nSELECT\n    first(partKey),\n    to_int(first(serialNumber))\nFROM DataStreamTwoItems\nGROUP BY partKey, partKey2;\n\nEND APPLICATION RecovTest13;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader\n(\nFetchSize:'@FETCHSIZE@',\nQueueSize:'@QUEUESIZE@',\nUsername:'@USERNAME@',\nPassword:'@PASSWORD@',\nConnectionURL:'@URL@',\n_h_useClassic: @CLASSICMODE@,\nTables:'@TABLES@',\nCommittedTransactions:'@COMMITEDT@'\nCompression:'@COMPRESSION@'\n)\nOUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)\nSELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]);\n\nCREATE TYPE LogType(\nNum_col String key,\nChar_col String,\nVarchar2_col String,\nlong_col String,\nTable String,\nOperation String\n);\n\nCREATE WINDOW CDCWindow\nOVER @STREAM@\nKEEP 1 ROWS;\n\nCREATE WACTIONSTORE CDCWS CONTEXT of LogType\nEVENT TYPES ( LogType )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ ToWactionStore\nINSERT INTO CDCWS\nSELECT * FROM CDCWindow\nLINK SOURCE EVENT;\n\nCREATE TARGET @SOURCE_NAME@_SYS USING SysOut (\n  name: '@SOURCE_NAME@_SYS' )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE FLOW ServerFlow;\n\n\nCREATE TYPE LogType(\nNum_col String key,\nChar_col String,\nVarchar2_col String,\nFloat_col Double,\nBFLoat_col Double,\nbdouble_col Double,\nlong_col String,\nTable String,\nOperation String\n);\n\nCREATE STREAM CDCFilteredStream OF LogType;\n\nCREATE CQ ToFilteredStream\nINSERT INTO CDCFilteredStream\nSELECT data[0],\ndata[1],\ndata[2],\nto_double(data[3]),\nto_double(data[4]),\nto_double(data[5]),\ndata[6],\nMETA(a, \"TableName\"),\nMETA(a, \"OperationName\")\nfrom @STREAM@ a;\n\n\n\nCREATE WINDOW CDCWindow\nOVER CDCFilteredStream\nKEEP 3 ROWS;\n\nCREATE WACTIONSTORE CDCWS CONTEXT of LogType\nEVENT TYPES ( LogType )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ ToWactionStore\nINSERT INTO CDCWS\nSELECT * FROM CDCWindow\nLINK SOURCE EVENT;\n\nEND FLOW ServerFlow;\n\n\nCREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (\n  Username: '@TARGET_USER@',\n  ConnectionURL: '@TARGET_URL@',\n  Tables: '@TARGET_TABLE@',\n  Password: '@TARGET_PASS@',\n )\nINPUT FROM @STREAM@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP noParser;\nUNDEPLOY APPLICATION noParser;\nDROP APPLICATION noParser CASCADE;\n\nCREATE APPLICATION noParser;\n\nCREATE TYPE Atm(\nproductID String KEY,\nstateID String,\nproductWeight int,\nquantity double,\nsize long,\ncurrentDate DateTime);\n\n\nCREATE CACHE cache1 USING FileReader(\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'ISdata.csv',\n  header: false,\n  columndelimiter: ',',\n  trimquote: false\n  ) QUERY (keytomap:'productID') OF Atm;\n\nEND APPLICATION noParser;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop application appname;\nundeploy application appname;\ndrop application appname cascade;\n\ncreate application appname use exceptionstore;\n\n  \n\n\nCREATE SOURCE OracleSource USING OracleReader  ( \nUsername: 'qatest', \n  Tables: 'QATEST.TEST01', \n  FetchSize: 100, \n  Password_encrypted: 'false', \n  ConnectionURL: 'localhost:1521:xe', \n  DictionaryMode: 'OnlineCatalog', \n  queuesize: 25000, \n  Password: 'qatest'\n  ) \nOUTPUT TO SS_oracle;\n\nCREATE TYPE CDCtestnMapType\n    (   id INTEGER,\n        name STRING,\n        cost DOUBLE\n    );\n\n\nCREATE EXTERNAL CACHE c_ext_oracle (\n  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n    DatabaseProviderType: 'Default',\n  FetchSize: 1,\n  Password: 'qatest',\n  Columns: 'id,name,cost',\n  Table: 'QATEST.SRCCACHE12',\n  trimquote: false,\n  Username: 'qatest',\n  keytomap: 'id',\n  AdapterName:'DatabaseReader',\n  connectionRetryPolicy: 'timeOut=5, retryInterval=5, maxRetries=10'\n )\nOF CDCtestnMapType;\n \n\nCREATE EXTERNAL CACHE c_oracle  (\n ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',\n  DatabaseProviderType: 'Default',\n  FetchSize: 1,\n  Password: 'qatest',\n  Columns: 'id,name,cost',\n  Table: 'QATEST.SRCCACHE12',\n  trimquote: false,\n  Username: 'qatest',\n  keytomap: 'id',\n  AdapterName:'DatabaseReader',\n  connectionRetryPolicy: 'timeOut=5, retryInterval=5, maxRetries=10' \n )\n OF  CDCtestnMapType;\n \n CREATE TYPE CDCtestnMapTypenew\n    (   id_t INTEGER,\n        name_t STRING,\n        cost_t DOUBLE,\n        id_c INTEGER,\n        name_c STRING,\n        cost_c DOUBLE\n    );\n    \ncreate stream JoinedDataStreamOracle of CDCtestnMapTypenew;\ncreate stream JoinedDataStreamOraclenull of CDCtestnMapTypenew;\n\n    \nCREATE CQ JoinDataCQOracle1\nINSERT INTO JoinedDataStreamOracle\nSELECT  TO_INT(f.data[0]),\n        TO_STRING(f.data[1]),\n        TO_DOUBLE(f.data[2]),\n        z.id,\n        z.name,\n        z.cost\nFROM SS_oracle f inner join c_ext_oracle z\non TO_INT(f.data[0]) = z.id;\n\nCREATE CQ JoinDataCQOracle2 \nINSERT INTO JoinedDataStreamOraclenull\nSELECT  TO_INT(s.data[0]),\n        TO_STRING(s.data[1]),\n        TO_DOUBLE(s.data[2]),\n        z.id,\n        z.name,\n        z.cost\nFROM c_oracle f left outer join c_ext_oracle z\non z.id=f.id RIGHT OUTER JOIN SS_oracle s\non  TO_INT(s.data[0])=z.id\nWHERE (TO_STRING(META(s, \"OperationName\")) = \"INSERT\");\n\n\n\nCREATE TARGET Target_DBWriter USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:1000,Interval:60',\nCommitPolicy:'Eventcount:10,Interval:60',\nTables:'QATEST.test03'\n) INPUT FROM JoinedDataStreamOracle;\n\nCREATE TARGET Target_DBWriter2 USING DatabaseWriter(\nConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',\nUsername:'qatest',\nPassword:'qatest',\nBatchPolicy:'Eventcount:1000,Interval:60',\nCommitPolicy:'Eventcount:10,Interval:60',\nTables:'QATEST.test04'\n) INPUT FROM JoinedDataStreamOraclenull;\n\nend application appname;\ndeploy application appname;\nstart appname;\n  "}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION @AppName@;\n\nCreate Source @AppName@_source Using OracleReader(\n  Username:'@username@',\n  Password:'@password@',\n  ConnectionURL:'@url@',\n  Tables:'@srctableName@',\n  Fetchsize:1\n)\nOutput To @AppName@_Stream;\nCREATE OR REPLACE TARGET @AppName@_SF_Target USING Global.SnowflakeWriter (\n  streamingUpload: 'false',\n  useConnectionProfile:'true',\n  connectionProfileName: 'admin.@SFCP@',\n  CDDLAction: 'Process',\n  optimizedMerge: 'false',\n  columnDelimiter: '|',\n  tables: '@srctableName@,@trgtableName@',\n  appendOnly: 'false',\n  uploadPolicy: 'eventcount:1,interval:5m',\n  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',\n  externalStageType: 'Local',\n  adapterName: 'SnowflakeWriter',\n  fileFormatOptions: 'null_if = \\\"\\\"' )\nINPUT FROM @AppName@_Stream;\n\n\nCREATE OR REPLACE TARGET @AppName@_DB_Target USING Global.DeltaLakeWriter (\nconnectionProfileName: 'admin.@DBCP@',\nuseConnectionProfile:'true',\n  Tables: '@srctableName@,@DBtrgtableName@',\n  uploadPolicy: 'eventcount:100000,interval:60s'\n)\n\nINPUT FROM @AppName@_Stream;\nEND APPLICATION @AppName@;\nDeploy application @AppName@;\nStart application @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "Stop @APPNAME@_App;\nundeploy application @APPNAME@_App;\ndrop application @APPNAME@_App cascade;\n\nCREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;\nCREATE SOURCE @APPNAME@_Src USING Global.OracleReader(\n  FetchSize:'1',\n  Username:'@SOURCE_USER@',\n  Password:'@SOURCE_PASSWORD@',\n  ConnectionURL:'@SOURCE_URL@',\n  Tables:'@SOURCE_TABLES@',\n  ConnectionRetryPolicy:'@AUTO_CONNECTION_RETRY@'\n)\nOUTPUT TO @APPNAME@_Stream;\n\nCREATE TARGET @APPNAME@_SYS USING SysOut (\n  name: '@APPNAME@_SYS' )\nINPUT FROM @APPNAME@_Stream;\n\nEnd APPLICATION @APPNAME@_App;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "-- stop application Metadata_Application;\n-- undeploy application Metadata_Application;\ndrop application Metadata_Application force;\nCREATE APPLICATION Metadata_Application;\n\nCREATE OR REPLACE TYPE app1_AccessSource_Type (\n timestamp org.joda.time.DateTime,\n src_ip java.lang.String,\n dest_ip java.lang.String,\n http_code java.lang.String);\n\n\nCREATE OR REPLACE SOURCE app1_AccessSource USING ContinuousGenerator ( \n  OutputType: 'admin.TYPE.app1_AccessSource_Type', \n  Throughput: 'Unrestricted' ) \nOUTPUT TO app1_OutputStream;\n\n\nCREATE OR REPLACE CQ app1_Query \nINSERT INTO app1_AccessStream \nSELECT DNOW() as Time,\n       TO_STRING(data[1]) as Source_IP,\n       TO_STRING(data[2]) as Destination_IP,\n       TO_STRING(data[3]) as HTTP_CODE\nFROM app1_OutputStream;\n\nCREATE OR REPLACE CQ app1_Query \nINSERT INTO app1_AccessStream \nSELECT DNOW() as Time,\n       TO_STRING(data[1]) as Source_IP,\n       TO_STRING(data[2]) as Destination_IP,\n       TO_STRING(data[3]) as HTTP_CODE\nFROM app1_OutputStream;\n\nCREATE OR REPLACE CQ app1_Query \nINSERT INTO app1_AccessStream \nSELECT DNOW() as Time,\n       TO_STRING(data[1]) as Source_IP,\n       TO_STRING(data[2]) as Destination_IP,\n       TO_STRING(data[3]) as HTTP_CODE\nFROM app1_OutputStream;\n\nCREATE OR REPLACE CQ app1_Query \nINSERT INTO app1_AccessStream \nSELECT DNOW() as Time,\n       TO_STRING(data[1]) as Source_IP,\n       TO_STRING(data[2]) as Destination_IP,\n       TO_STRING(data[3]) as HTTP_CODE\nFROM app1_OutputStream;\n\nCREATE OR REPLACE CQ app1_Query \nINSERT INTO app1_AccessStream \nSELECT DNOW() as Time,\n       TO_STRING(data[1]) as Source_IP,\n       TO_STRING(data[2]) as Destination_IP,\n       TO_STRING(data[3]) as HTTP_CODE\nFROM app1_OutputStream;\n\nCREATE OR REPLACE CQ app1_Query \nINSERT INTO app1_AccessStream \nSELECT DNOW() as Time,\n       TO_STRING(data[1]) as Source_IP,\n       TO_STRING(data[2]) as Destination_IP,\n       TO_STRING(data[3]) as HTTP_CODE\nFROM app1_OutputStream;\n\nCREATE OR REPLACE CQ app1_Query \nINSERT INTO app1_AccessStream \nSELECT DNOW() as Time,\n       TO_STRING(data[1]) as Source_IP,\n       TO_STRING(data[2]) as Destination_IP,\n       TO_STRING(data[3]) as HTTP_CODE\nFROM app1_OutputStream;\n\nCREATE OR REPLACE CQ app1_Query \nINSERT INTO app1_AccessStream \nSELECT DNOW() as Time,\n       TO_STRING(data[1]) as Source_IP,\n       TO_STRING(data[2]) as Destination_IP,\n       TO_STRING(data[3]) as HTTP_CODE\nFROM app1_OutputStream;\n\nCREATE OR REPLACE CQ app1_Query \nINSERT INTO app1_AccessStream \nSELECT DNOW() as Time,\n       TO_STRING(data[1]) as Source_IP,\n       TO_STRING(data[2]) as Destination_IP,\n       TO_STRING(data[3]) as HTTP_CODE\nFROM app1_OutputStream;\n\nCREATE OR REPLACE CQ app1_Query \nINSERT INTO app1_AccessStream \nSELECT DNOW() as Time,\n       TO_STRING(data[1]) as Source_IP,\n       TO_STRING(data[2]) as Destination_IP,\n       TO_STRING(data[3]) as HTTP_CODE\nFROM app1_OutputStream;\n\nEND APPLICATION Metadata_Application;\n\n\n\n-- stop application app2;\n-- undeploy application Metadata_Application2;\ndrop application Metadata_Application2 force;\n\nCREATE APPLICATION Metadata_Application2;\n\nCREATE OR REPLACE CQ app2_Query \nINSERT INTO app2_AccessStream \nSELECT *\nFROM app1_AccessStream;\n\nEND APPLICATION Metadata_Application2;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "drop namespace test cascade force;\ncreate namespace test;\nuse test;\nstop @AppName@;\nundeploy application @AppName@;\ndrop application @AppName@ cascade;\nCREATE OR REPLACE APPLICATION @AppName@;\nCREATE SOURCE  @srcName@  USING Global.OJet ( \n  Username:'@srcusername@',\n  Password:'@srcpassword@', \n  Tables:'@srcschema@.@srctable@',\n  ConnectionURL:'@srcurl@',\n  Tables:'@srcschema@.@srctable@',\n  sendBeforeImage:'true',\n  FilterTransactionBoundaries:'true'\n) \nOUTPUT TO @outstreamname@;\n\nCREATE TARGET @tgtName@ USING DatabaseWriter (\n  CheckPointTable: 'CHKPOINT', \n  ReplicationSlotName:'test_slot',\n  Username:'@tgtusername@',\n  Password:'@tgtpassword@',\n  ConnectionURL:'@tgturl@',\n  adapterName:'PostgreSQLReader',\n  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',\n  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@'\n)\nINPUT FROM @outstreamname@;\n\nEnd APPLICATION @AppName@;\ndeploy application @AppName@;\nstart @AppName@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create application CSVToXML;\ncreate source CSVSource using FileReader (\n\tdirectory:'Samples/AppData',\n\tWildCard:'posdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate Target t using FileWriter(\n\tfilename:'posdata_XML',\n\trolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'\n)\nformat using XMLFormatter (\n\trootelement:'document',\n\telementtuple:'MerchantName:merchantid:text=merchantname'\n)\ninput from TypedCSVStream;\nend application CSVToXML;\n\ndeploy application CSVToXML;\nstart application CSVToXML;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE APPLICATION @APPNAME@ recovery 5 second interval;\n\nCREATE OR REPLACE SOURCE @SOURCENAME@ USING IncrementalBatchReader  (\n  FetchSize: 1000,\n  Username: 'striim',\n  Password: 'striim',\n  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',\n  Tables: 'striim.test01',\n  adapterName: 'IncrementalBatchReader',\n  CheckColumn: @checkColumn@,\n  startPosition: @startPosition@,\n  PollingInterval: '20sec'\n  )\n  OUTPUT TO @STREAM@;\n\n  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;\n\n  CREATE TARGET @targetName@ USING DatabaseWriter(\n    ConnectionURL:'@READER-URL@',\n    Username:'@READER-UNAME@',\n    Password:'@READER-PASSWORD@',\n    BatchPolicy:'Eventcount:1000,Interval:1000',\n    CommitPolicy:'Eventcount:1000,Interval:1000',\n    Checkpointtable:'CHKPOINT',\n    Tables:'@WATABLES@,@WATABLES@_target'\n  ) INPUT FROM @STREAM@;\n\n  END APPLICATION @APPNAME@;\n\n  DEPLOY APPLICATION @APPNAME@;\n  start application @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use global;\n\n-- undeploy application T10;\ndrop application T10 cascade;\n\ncreate application T10\nRECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW AgentFlow;\ncreate source T10Source using CSVReader (\n  directory:'Samples/AppData',\n  header:Yes,\n  wildcard:'customerdetails-recovery.csv',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO T10Stream;\nEND FLOW AgentFlow;\n\nCREATE FLOW ServerFlow;\ncreate Target t using CSVWriter(fileName:AgentOut) input from T10Stream;\nEND FLOW ServerFlow;\n\nend application T10;\n\nDEPLOY APPLICATION T10 with AgentFlow in AGENTS, ServerFlow in SERVERS;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application GCSWriterTest;\nundeploy application GCSWriterTest;\ndrop application GCSWriterTest cascade;\ncreate application GCSWriterTest recovery 1 second interval;\nCreate Source OracleSource Using OracleReader\n(\n Username:'@ORACLE-USERNAME',\n Password:'@ORACLE-PASSWORD',\n ConnectionURL: '@ORACLE-IP@',\n Tables: '@SOURCE-TABLES@',\n FetchSize:'@FETCH-SIZE@'\n) \nOutput To OracleStream;\n\ncreate Target OracleGCSTarget using GCSWriter(\n    bucketname:'@bucketname@',\n    objectname:'@objectname@',\n    projectId:'@project-id@',\n    uploadPolicy:'@UPLOAD-SIZE@',\n    ServiceAccountKey:'@file-path@'\n)\nformat using DSVFormatter (\nmembers:'data'\n)\ninput from OracleStream;\n\nend application GCSWriterTest;\ndeploy application GCSWriterTest on all in default;\nstart application GCSWriterTest;\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE PROPERTYVARIABLE Mode='sync';\nCREATE OR REPLACE PROPERTYVARIABLE BatchPolicy='Size:900000,Interval:1';\n\ncreate application KinesisTest;\ncreate source CSVSource using FileReader (\n\tdirectory:'/home/dz/src/product/Samples/AppData',\n\tWildCard:'posdata.csv',\n\tpositionByEOF:false,\n\tcharset:'UTF-8'\n)\nparse using DSVParser (\n\theader:'yes'\n)\nOUTPUT TO CsvStream;\n\nCreate Type CSVType (\n  companyName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVStream\nSELECT data[0], data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvStream;\n\ncreate or replace Target t using KinesisWriter (\n\tregionName:'TARGET_REGION',\n\tstreamName:'TARGET_STREAM',\n\tBatchPolicy: '$BatchPolicy',\n    Mode: '$Mode'\t\n)\nformat using JSONFormatter (\n)\ninput from TypedCSVStream;\nend application KinesisTest;\ndeploy application KinesisTest in default;\nstart application KinesisTest;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP application JSONFormatterTester.DSV;\nundeploy application JSONFormatterTester.DSV;\ndrop application JSONFormatterTester.DSV cascade;\n\ncreate application DSV;\n\ncreate source CSVSmallPosDataSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'smallposdata.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO CsvSmallPosDataStream;\n\ncreate source CSVPosDataSource using FileReader (\n  directory:'@TEST-DATA-PATH@',\n  WildCard:'posdata.csv',\n  positionByEOF:false,\n  charset:'UTF-8'\n)\nparse using DSVParser (\n  header:'yes'\n)\nOUTPUT TO CsvPosDataStream;\n\nCreate Type CSVType (\n  merchantName String,\n  merchantId String,\n  dateTime DateTime,\n  hourValue int,\n  amount double,\n  zip String\n);\n\nCreate Stream TypedCSVPosDataStream of CSVType;\nCreate Stream TypedCSVSmallPosDataStream of CSVType;\n\nCREATE CQ CsvToPosData\nINSERT INTO TypedCSVPosDataStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvPosDataStream;\n\nCREATE CQ CsvToSmallPosData\nINSERT INTO TypedCSVSmallPosDataStream\nSELECT data[0],data[1],\n       TO_DATEF(data[4],'yyyyMMddHHmmss'),\n       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),\n       TO_DOUBLE(data[7]),\n       data[9]\nFROM CsvSmallPosDataStream;\n\n\n\n/**\n* 3.4.5.b FileWriter JsonFileSize 333MB\n**/\ncreate Target JSONFormatterFileSize using FileWriter(\n  filename:'JsonTargetFS',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'FileSizeRollingPolicy,filesize:333M,sequence:00'\n)\nformat using JSONFormatter (\n  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'\n)\ninput from TypedCSVPosDataStream;\n\n/**\n* 3.5.1.b FileWriter Json EventCount 2000\n**/\ncreate Target JSONFormatterEventCount using FileWriter(\n  filename:'JsonTargetEC',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'eventcount:2000,sequence:00'\n)\nformat using JSONFormatter (\n  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'\n)\ninput from TypedCSVSmallPosDataStream;\n\n\n/**\n*  FileWriter Json events as array of json objects :true.\n**/\n\ncreate Target JSONFormatterEventCountT using FileWriter(\n  filename:'JsonFormatterPropT',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'eventcount:500000,sequence:00'\n)\nformat using JSONFormatter (\n  members:'merchantname,merchantid,zip',\n  jsonObjectDelimiter:'|',\n  jsonMemberDelimiter:'~',\n  EventsAsArrayOfJsonObjects : true\n)\ninput from TypedCSVSmallPosDataStream;\n\n/**\n*  FileWriter Json events as array of json objects :false.\n**/\n\ncreate Target JSONFormatterEventCountF using FileWriter(\n  filename:'JsonFormatterPropF',\n  directory:'@FEATURE-DIR@/logs/',\n  rolloverpolicy:'eventcount:500000,sequence:00'\n)\nformat using JSONFormatter (\n  members:'merchantname,merchantid,zip',\n  jsonObjectDelimiter:'|',\n  jsonMemberDelimiter:'~',\n  EventsAsArrayOfJsonObjects : false\n)\ninput from TypedCSVSmallPosDataStream;\n\n\ncreate Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizeBig_actual.log') input from TypedCSVPosDataStream;\n\nend application DSV;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use admin;\ndrop application ns1.testgroupapp1 cascade;\ndrop application ns1.testgroupapp2 cascade;\ndrop application ns2.testgroupapp1 cascade;\ndrop application ns2.testgroupapp2 cascade;\ndrop namespace ns1 CASCADE;\ndrop namespace ns2 CASCADE;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 12 with two sources, two jumping attribute windows, one wactionstore with recovery, and another wactionstore without -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Ja5W/p -> CQ1 -> WS\n-- S2 -> Ja6W/p -> CQ2 -> WS\n-- S2 -> Ja6W/p -> CQ2 -> WS2 (no persists)\n--\n\nSTOP Recov12Tester.RecovTest12;\nUNDEPLOY APPLICATION Recov12Tester.RecovTest12;\nDROP APPLICATION Recov12Tester.RecovTest12 CASCADE;\nCREATE APPLICATION RecovTest12 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE WACTIONSTORE WactionsNoPersist CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n\t\tPERSIST NONE USING ( ) ;\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWactionNoPersist\nINSERT INTO WactionsNoPersist\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nEND APPLICATION RecovTest12;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP twoNodeCacheRefresh.AppTest;\nUNDEPLOY APPLICATION twoNodeCacheRefresh.AppTest;\nDROP APPLICATION twoNodeCacheRefresh.AppTest cascade;\n\nCREATE APPLICATION AppTest;\n\nCREATE FLOW THESOURCEFLOW;\n----------------------------------------------------\n\nCREATE TYPE GenType\n(\n  GenID Integer KEY,\n  GenCode double,\n  GenState String,\n  GenDate DateTime,\n  GenLong Long\n);\n\n\nCREATE SOURCE GenSource USING StreamReader(\n\tOutputType: 'twoNodeCacheRefresh.GenType',\n\tnoLimit: 'false',\n\tmaxRows: 100,\n\titerationDelay: 5,\n\tStringSet: 'GenState[CA-FL]',\n\tNumberSet: 'GenID[0-0]Linc,GenCode[99-99]Linc,GenLong[4999999-4999999]G'\n)OUTPUT TO rawGenStream;\n\n\nEND FLOW THESOURCEFLOW;\n----------------------------------------------------\n\nCREATE FLOW MAINPROCESSFLOW;\n\nCREATE TYPE theGenType\n(\n  theGenID Integer KEY,\n  theGenCode double,\n  theGenState String,\n  theGenDate DateTime,\n  theGenLong Long\n);\n\nCREATE STREAM GenStream OF theGenType;\n\nCREATE CQ GenCQ1\nINSERT INTO GenStream\nSELECT TO_INT(data[0]),\n\t   TO_DOUBLE(data[1]),\n\t   data[2],\n\t   TO_DATE(data[3]),\n\t   TO_LONG(data[4])\nFROM rawGenStream;\n\n\nCREATE TYPE USAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\nCREATE CACHE ZipLookup using FileReader (\n  directory: '@TEST-DATA-PATH@',\n  wildcard: 'addy.txt',\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  header: Yes,\n  columndelimiter: '\\t',\n  trimquote:false\n) QUERY (keytomap:'zip', refreshinterval:'20 second', skipinvalid:'true') OF USAddressData;\n\n\nCREATE TYPE MergedData\n(\n  country String,\n  zip String,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String,\n  theGenID Integer,\n  theGenCode double,\n  theGenState String,\n  theGenDate DateTime,\n  theGenLong Long\n);\n\nCREATE STREAM SanDiegoDataStream OF MergedData;\nCREATE STREAM MiamiDataStream OF MergedData;\n\n-- join the data\nCREATE CQ SanDiegoCQData\nINSERT INTO SanDiegoDataStream\nSELECT z.country, z.zip, z.city, z.state, z.stateCode,\n       z.fullCity, z.someNum, z.pad, z.latVal,\n       z.longVal, z.empty, z.empty2, gs.theGenID,\n       gs.theGenCode, gs.theGenState, gs.theGenDate, gs.theGenLong\n       FROM GenStream gs, ZipLookup z where gs.theGenState = 'CA' AND z.zip LIKE '921%';\n\nCREATE CQ MiamiCQData\nINSERT INTO MiamiDataStream\nSELECT z.country, z.zip, z.city, z.state, z.stateCode,\n       z.fullCity, z.someNum, z.pad, z.latVal,\n       z.longVal, z.empty, z.empty2, gs.theGenID,\n       gs.theGenCode, gs.theGenState, gs.theGenDate, gs.theGenLong\n       FROM GenStream gs, ZipLookup z where gs.theGenState = 'FL' AND z.zip LIKE '331%';\n\n\nCREATE TYPE MiamiData\n(\n    zip String KEY,\n    fullCity String,\n    theGenCode double,\n    theGenState String\n);\n\nCREATE WACTIONSTORE MiamiStore\nCONTEXT OF MiamiData\nEVENT TYPES(MergedData )\nPERSIST NONE USING ( );\n\n\nCREATE CQ MiamiStoreCQ\nINSERT INTO MiamiStore\nSELECT zip, fullCity, theGenCode, theGenState\nFROM MiamiDataStream\nLINK SOURCE EVENT;\n\n\nCREATE TYPE SanDiegoData\n(\n    zip String KEY,\n    fullCity String,\n    theGenCode double,\n    theGenState String\n);\n\nCREATE WACTIONSTORE SanDiegoStore\nCONTEXT OF SanDiegoData\nEVENT TYPES(MergedData )\nPERSIST NONE USING ( );\n\n\nCREATE CQ SanDiegoStoreCQ\nINSERT INTO SanDiegoStore\nSELECT zip, fullCity, theGenCode, theGenState\nFROM SanDiegoDataStream\nLINK SOURCE EVENT;\n\n\nEND FLOW MAINPROCESSFLOW;\n----------------------------------------------------\n\nEND APPLICATION AppTest;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION oraddl;\nUNDEPLOY APPLICATION oraddl;\nDROP APPLICATION oraddl CASCADE;\nCREATE APPLICATION oraddl recovery 5 second interval;\n \nCreate Source Ora Using OracleReader \n(\n Username:'@user-name@',\n Password:'@password@',\n ConnectionURL:'src_url',\n Tables:'QATEST.ORACLEDDL%',\n DictionaryMode:OfflineCatalog,\n DDLCaptureMode : 'All',\n FetchSize:1\n) Output To LogminerStream;\n\nCreate Target tgt using DatabaseWriter \n(\n Username:'@username@',\n Password:'@password@',\n ConnectionURL:'TGT_URL',\n BatchPolicy:'EventCount:1,Interval:1',\n CommitPolicy:'EventCount:1,Interval:1',\n IgnorableExceptionCode: '1,2290,942',\n Tables :'QATEST.ORACLEDDL%,QATEST2.%'\n) input from LogminerStream;\n\nend application oraddl;\ndeploy application oraddl;\nstart application oraddl;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 7\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> WS\n--\n\nSTOP Recov7Tester.RecovTest7;\nUNDEPLOY APPLICATION Recov7Tester.RecovTest7;\nDROP APPLICATION Recov7Tester.RecovTest7 CASCADE;\nCREATE APPLICATION RecovTest7 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nCREATE TYPE WactionType (\n  merchantId String KEY,\n  dateTime DateTime,\n  amount double,\n  city String\n);\n\nCREATE STREAM wacStream OF WactionType;\n\nCREATE CQ InsertWacStream\nINSERT INTO wacStream\nSELECT\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7]),\n    data[10]\nFROM CsvStream;\n\nCREATE WINDOW waWindow\nOVER wacStream KEEP WITHIN 1 SECOND ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionType\nEVENT TYPES ( WactionType )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactions\nINSERT INTO Wactions\nSELECT *\nFROM waWindow\nLINK SOURCE EVENT;\n\nEND APPLICATION RecovTest7;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "create target @TARGET_NAME@ using AzureSQLDWHWriter(\nConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',\n        username: 'striim',\n        password: 'W3b@ct10n',\n        Accountname: 'striimqatestdonotdelete',\n        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',\n        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',\n\t\tuploadpolicy:'eventcount:10,interval:30s'\n) INPUT FROM @STREAM@;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 3 on four node all server cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> JW -> CQ(aggregate) -> WS\n--\n\nSTOP APPLICATION N4S4CR3Tester.N4S4CRTest3;\nUNDEPLOY APPLICATION N4S4CR3Tester.N4S4CRTest3;\nDROP APPLICATION N4S4CR3Tester.N4S4CRTest3 CASCADE;\nCREATE APPLICATION N4S4CRTest3 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionN4S4CRTest3;\n\nCREATE SOURCE CsvSourceN4S4CRTest3 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream;\n\nEND FLOW DataAcquisitionN4S4CRTest3;\n\nCREATE FLOW DataProcessingN4S4CRTest3;\n\nCREATE TYPE WactionTypeN4S4CRTest3 (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount int,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionTypeN4S4CRTest3;\n\nCREATE CQ CsvToDataN4S4CRTest3\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_INT(TO_DOUBLE(data[7])),\n    data[10]\nFROM CsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsN4S4CRTest3 CONTEXT OF WactionTypeN4S4CRTest3\nEVENT TYPES ( WactionTypeN4S4CRTest3 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsN4S4CRTest3\nINSERT INTO WactionsN4S4CRTest3\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.merchantId),\n    FIRST(p.dateTime),\n    SUM(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nEND FLOW DataProcessingN4S4CRTest3;\n\nEND APPLICATION N4S4CRTest3;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "\nCREATE OR REPLACE PROPERTYSET LDAP1 ( PROVIDER_URL:\"ldap://10.77.12.210:389\", SECURITY_AUTHENTICATION:simple, SECURITY_PRINCIPAL: \"praveen,dc=qa,dc=webaction,dc=com\" , SECURITY_CREDENTIALS:InvalidPwd, USER_BASE_DN:\"dc=qa,dc=webaction,dc=com\", User_userId:cn );\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "use PosTester;\nDROP WINDOW PosData5Minutes;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @appname@routerApp;\nUNDEPLOY APPLICATION @appname@routerApp;\nDROP APPLICATION @appname@routerApp CASCADE;\n\nCREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',\n  acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');\n\nCREATE APPLICATION @appname@routerApp;\n\nCREATE  SOURCE @appname@OraSource USING OracleReader  (\nUsername: 'qatest',\nPassword: 'qatest',\nConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',\nTables: 'QATEST.TGT_T%',\n FetchSize:'100'\n)\nOUTPUT TO @appname@MasterStream1;\n\n-- CREATE STREAM @appname@ss1 OF Global.waevent persist using Global.DefaultKafkaProperties;\n-- CREATE STREAM @appname@ss2 OF Global.waevent persist using Global.DefaultKafkaProperties;\n-- CREATE STREAM @appname@ss3 OF Global.waevent persist using Global.DefaultKafkaProperties;\n\nCREATE STREAM @appname@ss1 OF Global.waevent PERSIST USING KafkaPropset;\nCREATE STREAM @appname@ss2 OF Global.waevent PERSIST USING KafkaPropset;\nCREATE STREAM @appname@ss3 OF Global.waevent PERSIST USING KafkaPropset;\n\nCREATE OR REPLACE ROUTER @appname@tablerouter1 INPUT FROM @appname@MasterStream1 s CASE\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T1' THEN ROUTE TO @appname@ss1,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T2' THEN ROUTE TO @appname@ss2,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T3' THEN ROUTE TO @appname@ss3,\nWHEN meta(s,\"TableName\").toString()='QATEST.TGT_T4' THEN ROUTE TO @appname@ss4,\nELSE ROUTE TO @appname@ss_else;\n\ncreate Target @appname@FileTarget_1 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n)\nFORMAT USING dsvFormatter ()\ninput from @appname@ss1;\n\ncreate Target @appname@FileTarget_2 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n)\nFORMAT USING dsvFormatter ()\ninput from @appname@ss2;\n\ncreate Target @appname@FileTarget_3 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n\n)\nFORMAT USING dsvFormatter ()\ninput from @appname@ss3;\n\ncreate Target @appname@FileTarget_4 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n\n)\nFORMAT USING dsvFormatter ()\ninput from @appname@ss4;\n\n\ncreate Target @appname@FileTarget_5 using FileWriter\n(\ndirectory: 'testSep17',\nfilename:'%@metadata(TableName)%'\n\n)\nFORMAT USING dsvFormatter ()\ninput from @appname@ss_else;\n\n\nend application @appname@routerApp;\ndeploy application @appname@routerApp;\nstart @appname@routerApp;"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 22 with two sources, two sliding attribute windows, and one wactionstore -- all with no partitioning\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Sa5W -> CQ1 -> WS\n-- S2 -> Sa6W -> CQ2 -> WS\n--\n\nSTOP Recov22Tester.RecovTest22;\nUNDEPLOY APPLICATION Recov22Tester.RecovTest22;\nDROP APPLICATION Recov22Tester.RecovTest22 CASCADE;\nCREATE APPLICATION RecovTest22 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE STREAM DataStream1 OF CsvData;\nCREATE STREAM DataStream2 OF CsvData;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\nCREATE WINDOW DataStream5Minutes1\nOVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WINDOW DataStream5Minutes2\nOVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF CsvData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ DataToWaction1\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes1;\n\nCREATE CQ DataToWaction2\nINSERT INTO Wactions\nSELECT\n    *\nFROM DataStream5Minutes2;\n\nEND APPLICATION RecovTest22;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop APPLICATION @AppName@;\nUndeploy APPLICATION @AppName@;\ndrop APPLICATION @AppName@ cascade;\nCREATE OR REPLACE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW @AgentFlow@;\n\nCREATE OR REPLACE SOURCE @SourceName@ USING Global.MSJet (\n  Tables: 'dbo.compsrc',\n  username: 'qatest',\n  DatabaseName: 'qatest',\n  FetchTransactionMetadata: true,\n  filterTransactionBoundaries: true,\n  compression: false,\n  ConnectionURL: '@ConnectionURL@',\n  Mode: '@mode@',\n  CommittedTransactions: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',\n  SendBeforeImage: true,\n  password: 'w3b@ct10n' )\nOUTPUT TO @StreamName@;\nEND FLOW @AgentFlow@;\n\nCREATE TARGET @SysTarget@ USING Global.SysOut (\n  name: 'MS_CDC_SYSOUT' )\nINPUT FROM @StreamName@;\n\nCREATE FLOW @ServerFlow@;\n\nCREATE TARGET @TargetName@ USING Global.DatabaseWriter (\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  ParallelThreads: '',\n  CheckPointTable: 'CHKPOINT',\n  CDDLAction: 'Process',\n  CommitPolicy: 'EventCount:1000,Interval:60',\n  ConnectionURL: '@ConnectionURL@',\n  StatementCacheSize: '50',\n  DatabaseProviderType: 'Default',\n  Username: 'qatest',\n  Tables: 'dbo.compsrc,dbo.comptar',\n  Password: 'w3b@ct10n',\n  PreserveSourceTransactionBoundary: 'false',\n  BatchPolicy: 'EventCount:1,Interval:60' )\nINPUT FROM @StreamName@;\n\nEND FLOW @ServerFlow@;\n\nEND APPLICATION @AppName@;\nDEPLOY APPLICATION @AppName@ with @AgentFlow@ in AGENTS ,@ServerFlow@ on any in default;\nSTART APPLICATION @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Recovery Test 36 with two sources, two jumping attribute windows, and one wactionstore -- all partitioned on the same key\n-- Nicholas Keene WebAction, Inc.\n--\n-- S1 -> Ja5W/p -> CQ1 -> WS\n-- S2 -> Ja6W/p -> CQ2 -> WS\n--\n\nSTOP Recov36Tester.RecovTest36;\nUNDEPLOY APPLICATION Recov36Tester.RecovTest36;\nDROP APPLICATION Recov36Tester.RecovTest36 CASCADE;\nCREATE APPLICATION RecovTest36 RECOVERY 5 SECOND INTERVAL;\n\nCREATE SOURCE CsvSource1 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream1;\n\nCREATE SOURCE CsvSource2 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestDataLong.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO CsvStream2;\n\nCREATE TYPE CsvData (\n  merchantId String KEY,\n  companyName String,\n  dateTime DateTime,\n  amount double\n);\n\nCREATE TYPE WactionData (\n  firstCompanyName String KEY,\n  dateTime DateTime,\n  totalCompanies int,\n  firstMerchantId String\n);\n\nCREATE STREAM DataStream1 OF CsvData\nPARTITION BY merchantId;\nCREATE STREAM DataStream2 OF CsvData\nPARTITION BY merchantId;\n\nCREATE CQ CsvToData1\nINSERT INTO DataStream1\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream1;\n\nCREATE CQ CsvToData2\nINSERT INTO DataStream2\nSELECT\n    data[1],\n    data[0],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_DOUBLE(data[7])\nFROM CsvStream2;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE JUMPING WINDOW DataStream6Minutes\nOVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime\nPARTITION BY merchantId;\n\nCREATE WACTIONSTORE Wactions CONTEXT OF WactionData\nEVENT TYPES ( CsvData )\n@PERSIST-TYPE@\n\nCREATE CQ Data5ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream5Minutes p\nGROUP BY p.merchantId;\n\nCREATE CQ Data6ToWaction\nINSERT INTO Wactions\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.dateTime),\n    COUNT(p.amount),\n    FIRST(p.merchantId)\nFROM DataStream6Minutes p\nGROUP BY p.merchantId;\n\nEND APPLICATION RecovTest36;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\nCREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;\n\n create flow myagentflow;\n\nCREATE OR REPLACE SOURCE @APPNAME@DB_emp1 USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO @APPNAME@Oracle_ChangeDataStream;\n\nCREATE OR REPLACE SOURCE @APPNAME@DB_emp2 USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO @APPNAME@Oracle_ChangeDataStream;\n\nCREATE OR REPLACE SOURCE @APPNAME@DB_emp3 USING OracleReader  (\n  Compression: true,\n  StartTimestamp: 'null',\n  SupportPDB: false,\n  FetchSize: 1,\n  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',\n  CommittedTransactions: true,\n  QueueSize: 2048,\n  FilterTransactionBoundaries: true,\n  Password_encrypted: true,\n  SendBeforeImage: true,\n  XstreamTimeOut: 600,\n  ConnectionURL: '@CONNECTION_URL@',\n  Tables: '@SOURCE_TABLE@',\n  adapterName: 'OracleReader',\n  Password: '@SOURCE_PASS@',\n  Password_encrypted: 'false',\n  DictionaryMode: 'OnlineCatalog',\n  FilterTransactionState: true,\n  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',\n  ReaderType: 'LogMiner',\n  Username: '@SOURCE_USER@',\n  OutboundServerProcessName: 'WebActionXStream'\n )\nOUTPUT TO @APPNAME@Oracle_ChangeDataStream;\n\nend flow myagentflow;\n\ncreate flow myserverflow;\n\nCREATE OR REPLACE TARGET @APPNAME@DB_etarget USING DatabaseWriter  (\n  DatabaseProviderType: 'Default',\n  CheckPointTable: 'CHKPOINT',\n  PreserveSourceTransactionBoundary: 'false',\n  Username: 'cassandra',\n  BatchPolicy: 'EventCount:100,Interval:10',\n  CommitPolicy: 'EventCount:100,Interval:10',\n  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',\n  Tables: '',\n  Password: 'cassandra',\n  Password_encrypted: false\n )\nINPUT FROM @APPNAME@Oracle_ChangeDataStream;\n\ncreate Target t2 using SysOut(name:Foo2) input from @APPNAME@Oracle_ChangeDataStream;\n\nend flow myserverflow;\n\nEND APPLICATION @APPNAME@;\n\ndeploy application @APPNAME@ on ALL in default with myagentflow on all in Agents, myserverflow on all in  default;\n\nstart @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "CREATE OR REPLACE APPLICATION @AppFeature@;\n\nCREATE OR REPLACE SOURCE initialLoad_Src USING Global.DatabaseReader (\n  FetchSize: 100,\n  QuiesceOnILCompletion: false,\n  Tables: '@SrcTable@',\n  adapterName: 'DatabaseReader',\n  Password: '@SrcPswd@',\n  Username: '@SrcUserName@',\n  ConnectionURL: '@SrcUrl@'\n   )\nOUTPUT TO @AppName@_Stream;\n\nCREATE OR REPLACE TARGET initialLoadPostgres_Trg USING Global.DatabaseWriter (\n  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',\n  CheckPointTable: 'CHKPOINT',\n  CDDLAction: 'Process',\n  CommitPolicy: 'EventCount:100,Interval:60',\n  StatementCacheSize: '50',\n  ConnectionURL: '@TrgUrl@',\n  Username: '@TrgUserName@',\n  BatchPolicy: 'EventCount:100,Interval:60',\n  Tables: '@SrcTable@,@TrgTable@',\n  Password: '@TrgPswd@',\n  adapterName: 'DatabaseWriter' )\nINPUT FROM @AppName@_Stream;\n\nEND APPLICATION @AppName@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "--\n-- Crash Recovery Test 3 on two node cluster\n-- Bert Hashemi, WebAction, Inc.\n--\n-- S -> CQ -> JW -> CQ(aggregate) -> WS\n--\n\nSTOP APPLICATION KStreamN2S2CR3Tester.KStreamN2S2CRTest3;\nUNDEPLOY APPLICATION KStreamN2S2CR3Tester.KStreamN2S2CRTest3;\nDROP APPLICATION KStreamN2S2CR3Tester.KStreamN2S2CRTest3 CASCADE;\n\nDROP USER KStreamN2S2CR3Tester;\nDROP NAMESPACE KStreamN2S2CR3Tester CASCADE;\nCREATE USER KStreamN2S2CR3Tester IDENTIFIED BY KStreamN2S2CR3Tester;\nGRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR3Tester;\nCONNECT KStreamN2S2CR3Tester KStreamN2S2CR3Tester;\n\nCREATE APPLICATION KStreamN2S2CRTest3 RECOVERY 5 SECOND INTERVAL;\n\nCREATE FLOW DataAcquisitionKStreamN2S2CRTest3;\n\nCREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');\nCREATE STREAM KafkaCsvStream using KafkaProps;\n\nCREATE SOURCE CsvSourceKStreamN2S2CRTest3 USING CSVReader (\n  directory:'@TEST-DATA-PATH@',\n  header:Yes,\n  wildcard:'RecovTestData.csv',\n  columndelimiter:',',\n  blocksize: 10240,\n  positionByEOF:false,\n  trimquote:false\n) OUTPUT TO KafkaCsvStream;\n\nEND FLOW DataAcquisitionKStreamN2S2CRTest3;\n\nCREATE FLOW DataProcessingKStreamN2S2CRTest3;\n\nCREATE TYPE WactionTypeKStreamN2S2CRTest3 (\n  companyName String,\n  merchantId String KEY,\n  dateTime DateTime,\n  amount int,\n  city String\n);\n\nCREATE STREAM DataStream OF WactionTypeKStreamN2S2CRTest3;\n\nCREATE CQ CsvToDataKStreamN2S2CRTest3\nINSERT INTO DataStream\nSELECT\n    data[0],\n    data[1],\n    TO_DATEF(data[4],'yyyyMMddHHmmss'),\n    TO_INT(TO_DOUBLE(data[7])),\n    data[10]\nFROM KafkaCsvStream;\n\nCREATE JUMPING WINDOW DataStream5Minutes\nOVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;\n\nCREATE WACTIONSTORE WactionsKStreamN2S2CRTest3 CONTEXT OF WactionTypeKStreamN2S2CRTest3\nEVENT TYPES ( WactionTypeKStreamN2S2CRTest3 )\n@PERSIST-TYPE@\n\nCREATE CQ InsertWactionsKStreamN2S2CRTest3\nINSERT INTO WactionsKStreamN2S2CRTest3\nSELECT\n    FIRST(p.companyName),\n    FIRST(p.merchantId),\n    FIRST(p.dateTime),\n    SUM(p.amount),\n    FIRST(p.city)\nFROM DataStream5Minutes p;\n\nEND FLOW DataProcessingKStreamN2S2CRTest3;\n\nEND APPLICATION KStreamN2S2CRTest3;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop @APPNAME@;\nundeploy application @APPNAME@;\ndrop application @APPNAME@ cascade;\n\nCREATE OR REPLACE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;\n\nCREATE OR REPLACE SOURCE @SOURCE@ USING SalesForceReader (\n  autoAuthTokenRenewal: 'true',\n  Username: '@userName@',\n  securityToken: '@securityToken@',\n  sObjects: '@SourceObj@',\n  pollingInterval: '1 min',\n  Password_encrypted: 'false',\n  securityToken_encrypted: 'false',\n  customObjects: 'False',\n  consumerKey: '@consumerKey@',\n  startTimestamp: '',\n  apiEndPoint: 'https://ap2.salesforce.com',\n  mode: 'Incremental',\n  consumerSecret: '@consumerSecert@',\n  consumerSecret_encrypted: 'false',\n  Password: '@Password@' )\nOUTPUT TO @STREAM@;\n\nCREATE OR REPLACE TARGET @TARGET@ USING SnowflakeWriter (\n  connectionUrl: '@tgtConnectionUrl@',\n  optimizedMerge: 'true',\n  password: '@TgtPassword@',\n  username: '@TgtUserName@',\n  appendOnly: 'false',\n  uploadPolicy: 'eventcount:10,interval:1m',\n  tables: '@SourceObj@,@TargetTableName@ columnmap(ID=ID,checkbool__c=checkbool__c,dt__c=dt__c,percnt__c=percnt__c,phn__c=phn__c,txtlong__c=txtlong__c,url1__c=url1__c)'\n )\nINPUT FROM @STREAM@;\n\nEND APPLICATION @APPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop APPLICATION MultiLogApp;\nundeploy APPLICATION MultiLogApp;\ndrop APPLICATION MultiLogApp cascade;\n\nCREATE APPLICATION MultiLogApp;\n\n-- This sample application shows how Striim could be used monitor and correlate logs \n-- from web and application server logs from the same web application. See the discussion \n-- in the \"Sample Applications\" section of the Striim documentation for additional \n-- discussion.\n\n\nCREATE FLOW MonitorLogs;\n\n-- MonitorLogs sets up the two log sources used by this application. In a real-world\n--implementation, each source could be reading many logs from many servers.\n\n-- The web server logs are in Apache NCSA extended/ combined log format plus response time:\n-- \"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-agent}i\\\" %D\"\n-- See apache.org for more information.\n\nCREATE SOURCE AccessLogSource USING FileReader (\n  directory:'Samples/Customer/MultiLogApp/appData',\n  wildcard:'access_log',\n  blocksize: 10240,\n  positionByEOF:false\n)\nPARSE USING DSVParser (\n  columndelimiter:' ',\n  ignoreemptycolumn:'Yes',\n  quoteset:'[]~\"',\n  separator:'~',\n  trimwhitespace:true\n)\nOUTPUT TO RawAccessStream;\n\nCREATE TYPE AccessLogEntry (\n    srcIp String KEY,\n    userId String,\n    sessionId String,\n    accessTime DateTime,\n    request String,\n    code integer,\n    size integer,\n    referrer String,\n    userAgent String,\n    responseTime integer\n);\nCREATE STREAM AccessStream OF AccessLogEntry;\n\nCREATE CQ ParseAccessLog \nINSERT INTO AccessStream\nSELECT data[0], data[2], MATCH(data[4], \".*jsessionId=(.*) \"),\n       TO_DATE(data[3], \"dd/MMM/yyyy:HH:mm:ss.SSS Z\"), data[4], TO_INT(data[5]), TO_INT(data[6]),\n       data[7], data[8], TO_INT(data[9])\nFROM RawAccessStream;\n\n-- The application server logs are in Apache's Log4J format.\n\nCREATE SOURCE Log4JSource USING FileReader (\n  directory:'Samples/Customer/MultiLogApp/appData',\n  wildcard:'log4jLog.xml',\n  positionByEOF:false\n) \nPARSE USING XMLParser(\n  rootnode:'/log4j:event',\n  columnlist:'log4j:event/@timestamp,log4j:event/@level,log4j:event/log4j:message,log4j:event/log4j:throwable,log4j:event/log4j:locationInfo/@class,log4j:event/log4j:locationInfo/@method,log4j:event/log4j:locationInfo/@file,log4j:event/log4j:locationInfo/@line'\n)\nOUTPUT TO RawXMLStream;\n\nCREATE TYPE Log4JEntry (\n  logTime DateTime,\n  level String,\n  message String,\n  api String,\n  sessionId String,\n  userId String,\n  sobject String,\n  xception String,\n  className String,\n  method String,\n  fileName String,\n  lineNum String\n);\nCREATE STREAM Log4JStream OF Log4JEntry;\n\nCREATE CQ ParseLog4J\nINSERT INTO Log4JStream\nSELECT TO_DATE(TO_LONG(data[0])), data[1], data[2], \n       MATCH(data[2], '\\\\\\\\[api=([a-zA-Z0-9]*)\\\\\\\\]'),\n       MATCH(data[2], '\\\\\\\\[session=([a-zA-Z0-9\\\\-]*)\\\\\\\\]'),\n       MATCH(data[2], '\\\\\\\\[user=([a-zA-Z0-9\\\\-]*)\\\\\\\\]'),\n       MATCH(data[2], '\\\\\\\\[sobject=([a-zA-Z0-9]*)\\\\\\\\]'),\n       data[3], data[4], data[5], data[6], data[7]\nFROM RawXMLStream;\n\nEND FLOW MonitorLogs;\n\n\nCREATE FLOW ErrorsAndWarnings;\n\n-- ErrorsAndWarnings creates a sliding window (Log4JErrorWarningActivity) containing \n-- the 300 most recent errors and warnings in the application server log. The \n-- ZeroContentCheck and LargeRTCheck flows join events from this window with access log \n-- events.\n\n-- The type Log4JEntry was already defined by the MonitorLogs flow.\nCREATE STREAM Log4ErrorWarningStream OF Log4JEntry;\n\nCREATE CQ GetLog4JErrorWarning\nINSERT INTO Log4ErrorWarningStream\nSELECT l FROM Log4JStream l\nWHERE l.level = 'ERROR' OR l.level = 'WARN';\n\nCREATE WINDOW Log4JErrorWarningActivity \nOVER Log4ErrorWarningStream KEEP 300 ROWS;\n\nEND FLOW ErrorsAndWarnings;\n\n\n-- HackerCheck sends an alert when an access log srcIp value is on a blacklist.\n\nCREATE FLOW HackerCheck;\n\nCREATE TYPE IPEntry (\n    ip String\n);\n\n/* CREATE CACHE BlackListLookup using CSVReader (\n  directory: 'Samples/Customer/MultiLogApp/appData',\n  wildcard: 'multiLogBlackList.txt',\n  header: No,\n  columndelimiter: ','\n) QUERY (keytomap:'ip') OF IPEntry; */\n\nCREATE CACHE BlackListLookup using FileReader (\n  directory: 'Samples/Customer/MultiLogApp/appData',\n  wildcard: 'multiLogBlackList.txt'\n)\nPARSE USING DSVParser ( )\nQUERY (keytomap:'ip') OF IPEntry;\n\n\nCREATE STREAM HackerStream OF AccessLogEntry;\n\nCREATE CQ FindHackers\nINSERT INTO HackerStream\nSELECT ale \nFROM AccessStream ale, BlackListLookup bll\nWHERE ale.srcIp = bll.ip;\n\nCREATE TYPE UnusualContext (\n    typeOfActivity String,\n    accessTime DateTime,\n    accessSessionId String,\n    srcIp String KEY,\n    userId String,\n    country String,\n    city String,\n    lat double,\n    lon double\n);\nCREATE TYPE MergedEntry (\n    accessTime DateTime,\n    accessSessionId String,\n    srcIp String KEY,\n    userId String,\n    request String,\n    code integer,\n    size integer,\n    referrer String,\n    userAgent String,\n    responseTime integer,\n    logTime DateTime,\n    logSessionId String,\n    level String,\n    message String,\n    api String,\n    sobject String,\n    xception String,\n    className String,\n    method String,\n    fileName String,\n    lineNum String\n);\nCREATE WACTIONSTORE UnusualActivity \nCONTEXT OF UnusualContext \nEVENT TYPES (MergedEntry, AccessLogEntry)\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE CQ GenerateHackerContext\nINSERT INTO UnusualActivity\nSELECT 'HackAttempt', accessTime, sessionId, srcIp, userId,\n       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)\nFROM HackerStream\nLINK SOURCE EVENT;\n\nCREATE STREAM HackingAlertStream OF Global.AlertEvent;\n\nCREATE CQ SendHackingAlerts \nINSERT INTO HackingAlertStream \nSELECT 'HackingAlert', ''+accessTime, 'warning', 'raise', \n        'Possible Hacking Attempt from ' + srcIp + ' in ' + IP_COUNTRY(srcIp)\nFROM HackerStream; \n\nCREATE SUBSCRIPTION HackingAlertSub USING WebAlertAdapter( ) INPUT FROM HackingAlertStream;\n\nEND FLOW HackerCheck;\n\n\n-- LargeRTCheck sends an alert when an access log responseTime value exceeds 2000 \n-- microseconds.\n\nCREATE FLOW LargeRTCheck;\n\nCREATE STREAM LargeRTStream of AccessLogEntry;\n\nCREATE CQ FindLargeRT\nINSERT INTO LargeRTStream\nSELECT ale\nFROM AccessStream ale\nWHERE ale.responseTime > 2000;\n\nCREATE WINDOW LargeRTActivity \nOVER LargeRTStream KEEP 100 ROWS;\n\nCREATE STREAM LargeRTAPIStream OF MergedEntry;\n\nCREATE CQ MergeLargeRTAPI\nINSERT INTO LargeRTAPIStream\nSELECT lrt.accessTime, lrt.sessionId, lrt.srcIp, lrt.userId, lrt.request, \n       lrt.code, lrt.size, lrt.referrer, lrt.userAgent, lrt.responseTime,\n       log4j.logTime, log4j.sessionId, log4j.level, log4j.message, log4j.api, log4j.sobject, log4j.xception, \n       log4j.className, log4j.method, log4j.fileName, log4j.lineNum\nFROM LargeRTActivity lrt, Log4JErrorWarningActivity log4j\nWHERE lrt.sessionId = log4j.sessionId\n      AND lrt.accessTime = log4j.logTime;   \n\nCREATE CQ GenerateLargeRTContext\nINSERT INTO UnusualActivity\nSELECT 'LargeResponseTime', accessTime, accessSessionId, srcIp, userId,\n       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)\nFROM LargeRTAPIStream\nLINK SOURCE EVENT;\n\nCREATE STREAM LargeRTAlertStream OF Global.AlertEvent;\n\nCREATE CQ SendLargeRTAlerts \nINSERT INTO LargeRTAlertStream \nSELECT 'LargeRTAlert', ''+accessTime, 'warning', 'raise', \n        'Long response time for call from ' + userId + ' api ' + api + ' message ' + message\nFROM LargeRTAPIStream; \n\nCREATE SUBSCRIPTION LargeRTAlertSub USING WebAlertAdapter( ) INPUT FROM LargeRTAlertStream;\n\nEND FLOW LargeRTCheck;\n\n\n-- ProxyCheck sends an alert when an access log srcIP value is on a list of suspicious \n-- proxies.\n\nCREATE FLOW ProxyCheck;\n\n/* CREATE CACHE ProxyLookup using CSVReader (\n  directory: 'Samples/Customer/MultiLogApp/appData',\n  wildcard: 'multiLogProxies.txt',\n  header: No,\n  columndelimiter: ','\n) QUERY (keytomap:'ip') OF IPEntry; */\n\nCREATE CACHE ProxyLookup using FileReader (\n  directory: 'Samples/Customer/MultiLogApp/appData',\n  wildcard: 'multiLogProxies.txt'\n)\nPARSE USING DSVParser ( )\nQUERY (keytomap:'ip') OF IPEntry; \n\n\nCREATE STREAM ProxyStream OF AccessLogEntry;\n\nCREATE CQ FindProxies\nINSERT INTO ProxyStream\nSELECT ale \nFROM AccessStream ale, ProxyLookup pl\nWHERE ale.srcIp = pl.ip;\n\nCREATE CQ GenerateProxyContext\nINSERT INTO UnusualActivity\nSELECT 'ProxyAccess', accessTime, sessionId, srcIp, userId,\n       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)\nFROM ProxyStream\nLINK SOURCE EVENT;\n\n\nCREATE STREAM ProxyAlertStream OF Global.AlertEvent;\n\nCREATE CQ SendProxyAlerts \nINSERT INTO ProxyAlertStream \nSELECT 'ProxyAlert', ''+accessTime, 'warning', 'raise', \n        'Possible use of Proxy from ' + srcIp + ' in ' + IP_COUNTRY(srcIp) + ' for user ' + userId \nFROM ProxyStream; \n\nCREATE SUBSCRIPTION ProxyAlertSub USING WebAlertAdapter( ) INPUT FROM ProxyAlertStream;\n\nEND FLOW ProxyCheck;\n\n\n-- ZeroContentCheck sends an alert when an access log entry's code value is 200 (that is,\n-- the HTTP request succeeded) but the size value is 0 (the return had no content).\n\nCREATE FLOW ZeroContentCheck;\n\nCREATE STREAM ZeroContentStream of AccessLogEntry;\n\nCREATE CQ FindZeroContent\nINSERT INTO ZeroContentStream\nSELECT ale\nFROM AccessStream ale\nWHERE ale.code = 200 AND ale.size = 0;\n\nCREATE WINDOW ZeroContentActivity \nOVER ZeroContentStream KEEP 100 ROWS;\n\nCREATE STREAM ZeroContentAPIStream OF MergedEntry;\n\nCREATE CQ MergeZeroContentAPI\nINSERT INTO ZeroContentAPIStream\nSELECT zcs.accessTime, zcs.sessionId, zcs.srcIp, zcs.userId, zcs.request, \n       zcs.code, zcs.size, zcs.referrer, zcs.userAgent, zcs.responseTime,\n       log4j.logTime, log4j.sessionId, log4j.level, log4j.message, log4j.api, log4j.sobject, log4j.xception, \n       log4j.className, log4j.method, log4j.fileName, log4j.lineNum\nFROM ZeroContentActivity zcs, Log4JErrorWarningActivity log4j\nWHERE zcs.sessionId = log4j.sessionId\n      AND zcs.accessTime = log4j.logTime;   \n\nCREATE CQ GenerateZeroContentContext\nINSERT INTO UnusualActivity\nSELECT 'ZeroContent', accessTime, accessSessionId, srcIp, userId,\n       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)\nFROM ZeroContentAPIStream\nLINK SOURCE EVENT;\n\n\nCREATE TYPE ZeroContentEventListType (\n    srcIp String KEY,\n    code integer,\n    size integer,\n    level String,\n    message String,\n    xception String);\n    \nCREATE WACTIONSTORE ZeroContentEventList\nCONTEXT OF ZeroContentEventListType \nEVENT TYPES (ZeroContentEventListType )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\n\nCREATE CQ GenerateZeroContentEventList\nINSERT INTO ZeroContentEventList\nSELECT srcIp, code, size, level, message, xception\nFROM ZeroContentAPIStream;\n\n\nCREATE STREAM ZeroContentAlertStream OF Global.AlertEvent;\n\nCREATE CQ SendZeroContentAlerts \nINSERT INTO ZeroContentAlertStream \nSELECT 'ZeroContentAlert', ''+accessTime, 'warning', 'raise', \n        'Zero content returned in call from ' + userId + ' api ' + api + ' message ' + message\nFROM ZeroContentAPIStream; \n\nCREATE SUBSCRIPTION ZeroContentAlertSub USING WebAlertAdapter( ) INPUT FROM ZeroContentAlertStream;\n\nEND FLOW ZeroContentCheck;\n\n\n-- ErrorHandling is functionally identical to ErrorFlow.SaasMonitorApp. It sends an alert \n-- when an error message appears in the application server log.\n\nCREATE FLOW ErrorHandling;\n\nCREATE STREAM ErrorStream OF Log4JEntry;\n\nCREATE CQ GetErrors \nINSERT INTO ErrorStream \nSELECT log4j \nFROM Log4ErrorWarningStream log4j WHERE log4j.level = 'ERROR';\n\nCREATE STREAM ErrorAlertStream OF Global.AlertEvent;\n\nCREATE CQ SendErrorAlerts \nINSERT INTO ErrorAlertStream \nSELECT 'ErrorAlert', ''+logTime, 'error', 'raise', 'Error in log ' + message \nFROM ErrorStream;\n\nCREATE SUBSCRIPTION ErrorAlertSub USING WebAlertAdapter( ) INPUT FROM ErrorAlertStream;\n\nEND FLOW ErrorHandling;\n\n\n-- WarningHandling is a minor variation on WarningFlow.SaasMonitorApp. It sends an alert \n-- once an hour with the count of warnings for each api call for which there has been at \n-- least one alert.\n\nCREATE FLOW WarningHandling;\n\nCREATE STREAM WarningStream OF Log4JEntry;\n\nCREATE CQ GetWarnings \nINSERT INTO WarningStream \nSELECT log4j \nFROM Log4ErrorWarningStream log4j WHERE log4j.level = 'WARN';\n\nCREATE JUMPING WINDOW WarningWindow \nOVER WarningStream KEEP WITHIN 60 MINUTE ON logTime;\n\nCREATE STREAM WarningAlertStream OF Global.AlertEvent;\n\nCREATE CQ SendWarningAlerts \nINSERT INTO WarningAlertStream \nSELECT 'WarningAlert', ''+logTime, 'warning', 'raise', \n        COUNT(logTime) + ' Warnings in log for api ' + api \nFROM WarningWindow \nGROUP BY api \nHAVING count(logTime) > 1;\n\nCREATE SUBSCRIPTION WarningAlertSub USING WebAlertAdapter( ) INPUT FROM WarningAlertStream;\n\nEND FLOW WarningHandling;\n\n\n-- InfoFlow is functionally similar to InfoFlow.SaasMonitorApp. Its output is used by \n-- ApiFlow, CompanyApiFlow, and UserApiFlow.\n\nCREATE FLOW InfoFlow;\n\nCREATE TYPE UserInfo (\n  userId String, \n  userName String, \n  company String,  \n  userZip String,  \n  companyZip String\n);\n\n/* CREATE CACHE MLogUserLookup using CSVReader (\n  directory: 'Samples/Customer/MultiLogApp/appData',\n  wildcard: 'multiLogUser.csv',\n  header: No,\n  columndelimiter: ','\n) QUERY (keytomap:'userId') OF UserInfo; */\n\nCREATE CACHE MLogUserLookup using FileReader (\n  directory: 'Samples/Customer/MultiLogApp/appData',\n  wildcard: 'multiLogUser.csv'\n)\nPARSE USING DSVParser ( )\nQUERY (keytomap:'userId') OF UserInfo;\n\nCREATE STREAM InfoStream OF Log4JEntry;\n\nCREATE CQ GetInfo \nINSERT INTO InfoStream \nSELECT log4j \nFROM Log4JStream log4j WHERE log4j.level = 'INFO';\n\nCREATE TYPE ApiCall (\n  userId String, \n  api String, \n  sobject String, \n  logTime DateTime, \n  userName String, \n  company String,  \n  userZip String,  \n  companyZip String\n);\nCREATE STREAM ApiEnrichedStream OF ApiCall;\n\nCREATE CQ GetUserDetails \nINSERT INTO ApiEnrichedStream \nSELECT a.userId, a.api, a.sobject, a.logTime, u.userName, u.company, u.userZip, u.companyZip \nFROM InfoStream a, MLogUserLookup u \nWHERE a.userId = u.userId;\n\nEND FLOW InfoFlow;\n\n\n-- ApiFlow populates the dashboard's Detail - ApiActivity page and the pie chart on the \n-- Overview page.\n\nCREATE FLOW ApiFlow;\n\nCREATE TYPE ApiUsage (\n  api String key, \n  sobject String, \n  count integer, \n  logTime DateTime\n);\n\nCREATE TYPE ApiContext (\n  api String key, \n  count integer, \n  logTime DateTime\n);\n\nCREATE WACTIONSTORE ApiActivity \nCONTEXT OF ApiContext \nEVENT TYPES (ApiUsage )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE JUMPING WINDOW ApiWindow \nOVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime \nPARTITION BY api;\n\nCREATE STREAM ApiUsageStream OF ApiUsage;\n\nCREATE CQ GetApiUsage \nINSERT INTO ApiUsageStream \nSELECT a.api, a.sobject, \n       COUNT(a.userId), FIRST(a.logTime) \nFROM ApiWindow a \nGROUP BY a.api, a.sobject HAVING FIRST(a.logTime) IS NOT NULL;\n\nCREATE JUMPING WINDOW ApiSummaryWindow \nOVER ApiUsageStream KEEP WITHIN 1 HOUR ON logTime \nPARTITION BY api;\n\nCREATE CQ GetApiSummaryUsage \nINSERT INTO ApiActivity \nSELECT a.api,  \n       sum(a.count), first(a.logTime)\nFROM ApiSummaryWindow a \nGROUP BY a.api\nLINK SOURCE EVENT;\n\nEND FLOW ApiFlow;\n\n\n-- CompanyApiFlow populates the dashboard's Detail - CompanyApiActivity page and the bar \n-- chart on the Overview page. It also sends an alert when an API call is used by a \n-- company more than 1500 times during the flow's one-hour jumping window.\n\nCREATE FLOW CompanyApiFlow;\n\nCREATE TYPE CompanyApiUsage (\n  company String key, \n  companyZip String, \n  companyLat double, \n  companyLong double, \n  api String, \n  count integer, \n  unusual integer,\n  Category String,\n  topObject String,\n  logTime DateTime\n);\n\nCREATE TYPE CompanyApiContext (\n  company String key, \n  companyZip String, \n  companyLat double, \n  companyLong double, \n  count integer, \n  unusual integer,\n  Category String,\n  topObject String,\n  logTime DateTime\n);\n\nCREATE WACTIONSTORE CompanyApiActivity \nCONTEXT OF CompanyApiContext \nEVENT TYPES (CompanyApiUsage )\nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE JUMPING WINDOW CompanyApiWindow \nOVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime \nPARTITION BY company;\n\nCREATE STREAM CompanyApiUsageStream OF CompanyApiUsage;\n\nCREATE TYPE MLogUSAddressData(\n  country String,\n  zip String KEY,\n  city String,\n  state String,\n  stateCode String,\n  fullCity String,\n  someNum String,\n  pad String,\n  latVal double,\n  longVal double,\n  empty String,\n  empty2 String\n);\n\n/*CREATE CACHE MLogZipLookup using CSVReader (\n  directory: 'Samples/Customer/MultiLogApp/appData',\n  wildcard: 'USAddresses.txt',\n  header: Yes,\n  columndelimiter: ','\n) QUERY (keytomap:'zip') OF MLogUSAddressData; */\n\nCREATE CACHE MLogZipLookup using FileReader (\n  directory: 'Samples/Customer/MultiLogApp/appData',\n  wildcard: 'USAddresses.txt'\n)\nPARSE USING DSVParser (\n  header: Yes\n)\nQUERY (keytomap:'zip') OF MLogUSAddressData;\n\n\nCREATE CQ GetCompanyApiUsage \nINSERT INTO CompanyApiUsageStream \nSELECT a.company, a.companyZip, z.latVal, z.longVal, \n       a.api, COUNT(a.sobject), \n       CASE WHEN COUNT(a.sobject) > 1500 THEN 1\n            ELSE 0 END,\n       CASE WHEN COUNT(a.sobject) > 1500 THEN 'UNUSUAL'\n            ELSE 'OK' END,\n       MAXOCCURS(a.sobject),\n       FIRST(a.logTime) \nFROM CompanyApiWindow a, MLogZipLookup z \nWHERE a.companyZip = z.zip \nGROUP BY a.company, a.api HAVING FIRST(a.logTime) IS NOT NULL;\n\nCREATE JUMPING WINDOW CompanyWindow \nOVER CompanyApiUsageStream KEEP WITHIN 1 HOUR ON logTime \nPARTITION BY company;\n\nCREATE CQ GetCompanyUsage \nINSERT INTO CompanyApiActivity \nSELECT a.company, a.companyZip, a.companyLat, a.companyLong, \n       SUM(a.count), SUM(a.unusual), \n       CASE WHEN SUM(a.unusual) > 0 THEN 'UNUSUAL'\n            ELSE 'OK' END,\n       MAXOCCURS(a.topObject),\n       FIRST(a.logTime) \nFROM CompanyWindow a \nGROUP BY a.company\nLINK SOURCE EVENT;\n\nCREATE STREAM CompanyAlertStream OF Global.AlertEvent;\n\nCREATE CQ SendCompanyApiAlerts \nINSERT INTO CompanyAlertStream \nSELECT 'CompanyAPIAlert', ''+logTime, 'warning', 'raise', \n       'Company ' + company + ' has used api ' + api + ' ' + count + ' times for ' + topObject \nFROM CompanyApiUsageStream \nWHERE unusual = 1;\n\nCREATE SUBSCRIPTION CompanyAlertSub USING WebAlertAdapter( ) INPUT FROM CompanyAlertStream;\n\nEND FLOW CompanyApiFlow;\n\n\n-- UserApiFlow populates the dashboard's Detail - UserApiActivity page and the US map on \n-- the Overview page. It also sends an alert when an API call is used by a user more than \n-- 125 times during the flow's one-hour window.\n\nCREATE FLOW UserApiFlow;\n\nCREATE TYPE UserApiUsage (\n  userId String key, \n  userName String, \n  userZip String, \n  userLat double, \n  userLong double, \n  company String, \n  api String, \n  count integer, \n  unusual integer,\n  Category String,\n  topObject String,\n  logTime DateTime\n);\n\nCREATE TYPE UserApiContext (\n  userId String key, \n  userName String, \n  userZip String, \n  userLat double, \n  userLong double, \n  company String, \n  count integer, \n  unusual integer,\n  Category String,\n  topObject String,\n  logTime DateTime\n);\n\nCREATE WACTIONSTORE UserApiActivity\nCONTEXT OF UserApiContext \nEVENT TYPES (  UserApiUsage ) \nPERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );\n\nCREATE JUMPING WINDOW UserApiWindow \nOVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime\nPARTITION BY userId;\n\nCREATE STREAM UserApiUsageStream OF UserApiUsage;\n\nCREATE CQ GetUserApiUsage \nINSERT INTO UserApiUsageStream \nSELECT a.userId, a.userName, a.userZip, z.latVal, z.longVal, a.company,\n       a.api, COUNT(a.sobject), \n       CASE WHEN COUNT(a.sobject) > 125 THEN 1\n            ELSE 0 END,\n       CASE WHEN COUNT(a.sobject) > 125 THEN 'UNUSUAL'\n            ELSE 'OK' END,\n       MAXOCCURS(a.sobject),\n       FIRST(a.logTime) \nFROM UserApiWindow a, MLogZipLookup z \nWHERE a.userZip = z.zip \nGROUP BY a.userId, a.api HAVING FIRST(a.logTime) IS NOT NULL;\n\nCREATE JUMPING WINDOW UserWindow \nOVER UserApiUsageStream KEEP WITHIN 1 HOUR ON logTime \nPARTITION BY userId;\n\nCREATE CQ GetUserUsage \nINSERT INTO UserApiActivity \nSELECT a.userId, a.userName, a.userZip, a.userLat, a.userLong, \n       a.company, SUM(a.count), SUM(a.unusual), \n       CASE WHEN SUM(a.unusual) > 0 THEN 'UNUSUAL'\n            ELSE 'OK' END,\n       MAXOCCURS(a.topObject),\n       FIRST(a.logTime) \nFROM UserWindow a \nGROUP BY a.userId\nLINK SOURCE EVENT;\n\nCREATE STREAM UserAlertStream OF Global.AlertEvent;\n\nCREATE CQ SendUserApiAlerts \nINSERT INTO UserAlertStream \nSELECT 'UserAPIAlert', ''+logTime, 'warning', 'raise', \n       'User ' + userName + ' has used api ' + api + ' ' + count + ' times for ' + topObject \nFROM UserApiUsageStream \nWHERE unusual = 1;\n\nCREATE SUBSCRIPTION UserAlertSub USING WebAlertAdapter( ) INPUT FROM UserAlertStream;\n\nEND FLOW UserApiFlow;\n\nEND APPLICATION MultiLogApp;\n\nDeploy application MultiLogApp;\nStart application MultiLogApp;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "-- detectFraudsterApp1.tql, detectFraudsterApp1_vis_settings.json, apiCallLogs.txt are needed to run this example.\n-- gamelogs simulates api call logs for an online game. xferCur represents virtual currency transfer.\n-- If this method is called by same user more than once in 1000 records its anomaly and hence waction.\n\nIMPORT static com.webaction.runtime.converters.DateConverter.*;\n\nUNDEPLOY APPLICATION admin.detectFraudsterApp1;\nDROP APPLICATION admin.detectFraudsterApp1 CASCADE;\nCREATE APPLICATION detectFraudsterApp1;\n\n\n/* READ DATA FROM A CSV FILE. USE CSV-READER  */\nCREATE source gameCSVDataSource USING CSVReader (\n  directory:'@TEST-DATA-PATH@/persistence',\n  header:No,\n  wildcard:'apiCallLogs.txt',\n  columndelimiter:',',\n  positionByEOF:false\n) OUTPUT TO PCsvStream;\n\ncreate jumping window apiwindow over PCsvStream keep 1000 rows;\n\nCREATE TYPE apiCallData(\n  date DateTime,\n  apiName String,\n  amount int,\n  username String key\n);\n\nCREATE STREAM apiCallStream OF apiCallData;\n\n/* FROM INPUT DATA READ ABOVE, CREATE EVENTS OF TYPE apiCallData\n * TO_INT(data[10]) > 100000 --> 3 WACTIONS\n * TO_INT(data[10]) > 50000 --> 8 WACTIONS\n * TO_INT(data[10]) > 25000 --> 11 WACTIONS\n * TO_INT(data[10]) > 20000 --> 14 WACTIONS\n * */\n\nCREATE CQ gameCQ\nINSERT INTO apiCallStream\nSELECT\n    TO_DATE(TO_LONG(data[3])),\n    data[10],\n    TO_INT(data[11]),\n    data[12]\nFROM apiwindow\nwhere cast (data[10] as String)  =  'xferCur' and  TO_INT(data[11]) > 50000;\n\ncreate TYPE apiCallContext(\n  date DateTime,\n  apiName String,\n  amount int,\n  username String key\n);\n\n/* PERSISTING TO DERBY  */\nCREATE WACTIONSTORE fraudWactionsDerby\nCONTEXT OF apiCallContext\nEVENT TYPES (apiCallData )\nPERSIST EVERY 2 second USING\n(\nJDBC_DRIVER:'@WASTORE-DRIVER@',\nJDBC_URL:'@WASTORE-URL@;CREATE=true',\nJDBC_USER:'@WASTORE-UNAME@',\nJDBC_PASSWORD:'@WASTORE-PASSWORD@',\nDDL_GENERATION:'drop-and-create-tables',\nLOGGING_LEVEL:'SEVERE'\n);\n\n\nCREATE CQ populateFraudWactionsDerby\nINSERT INTO fraudWactionsDerby\nSELECT date, apiName, amount, username\nFROM apiCallStream\nLINK SOURCE EVENT;\n\n\n/* PERSISTING TO MYSQL  */\n/*\nCREATE WACTIONSTORE fraudWactionsMySQL\nCONTEXT OF apiCallContext\nEVENT TYPES (apiCallData )\nPERSIST EVERY 3 second USING\n(\nJDBC_DRIVER:'com.mysql.jdbc.Driver',\nJDBC_URL:'jdbc:mysql://127.0.0.1:3306/test',\nJDBC_USER:root,\nJDBC_PASSWORD:'root',\nDDL_GENERATION:'drop-and-create-tables',\nLOGGING_LEVEL:'SEVERE'\n);\n\nCREATE CQ populateFraudWactionsMySQL\nINSERT INTO fraudWactionsMySQL\nSELECT date, apiName, amount, username\nFROM apiCallStream\nLINK SOURCE EVENT;\n*/\n/* PERSISTING TO MONGODB  */\n/*\nCREATE WACTIONSTORE fraudWactionsMongo\nCONTEXT OF apiCallContext\nEVENT TYPES (apiCallData )\nPERSIST EVERY 5 second USING\n(\nNOSQL_PROPERTY:'localhost:27017',\nTARGET_DATABASE:'org.eclipse.persistence.nosql.adapters.mongo.MongoPlatform',\nDDL_GENERATION:'drop-and-create-tables',\nDB_NAME:db\n);\n\n\nCREATE CQ populateFraudWactionsMongo\nINSERT INTO fraudWactionsMongo\nSELECT date, apiName, amount, username\nFROM apiCallStream\nLINK SOURCE EVENT;\n*/\n\n\n\n--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM PCsvStream;\n--CREATE TARGET output2 USING SysOut(name : selectedinput) input FROM apiCallStream;\n\n\nCREATE VISUALIZATION detectFraudsterApp1 \"@TEST-DATA-PATH@/json/detectFraudsterApp1_vis_settings.json\";\n\nEND APPLICATION detectFraudsterApp1;\n\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "stop application iteratortester.iteratorapp;\nundeploy application iteratortester.iteratorapp;\ndrop application iteratortester.iteratorapp cascade;\n\nCREATE APPLICATION iteratorapp;\n\ncreate flow sourceFlow;\n\nCREATE SOURCE JSONAccessLogSource USING FileReader(\n  directory:'@TEST-DATA-PATH@',\n  wildcard:'iterator2.json'\n)\nparse using JSONParser (\n) OUTPUT TO jsonSourceStream;\n\nend flow sourceflow;\n\n-- ******ARRAY LIST****** --\ncreate flow processFlow;\n\ncreate type cacheType (bankID string key, bankName string);\nCREATE cache dsvcache USING FileReader (\ndirectory:'@TEST-DATA-PATH@',\nwildcard:'banks.csv',\nblocksize: 10240,\npositionByEOF:false\n)\nPARSE USING DSVParser (\nheader:No,\ntrimquote:false\n) QUERY (keytomap:'bankID') OF cacheType;\n\nCREATE TYPE listType (id integer KEY, bankname string, lst java.util.List);\nCREATE TYPE listStoreType (id integer KEY, bankname string, lst java.util.List, lstoflst java.util.List);\n\nCREATE STREAM listStream of listType partition by bankname;\n\nCREATE JUMPING WINDOW listJWindow\nOVER listStream\nkeep 3 rows;\n\nCREATE WINDOW listWindow\nOVER listStream\nkeep 3 rows;\n\nCREATE WACTIONSTORE listStore CONTEXT OF listStoreType EVENT TYPES (listStoreType ) \n@PERSIST-TYPE@\n\ncreate cq updatelistStream\ninsert into listStream\nselect TO_INT(bankID), bankName, makelist(bankID,bankName) as lst \nfrom dsvcache;\n\ncreate cq updatelistStore \ninsert into listStore\nselect ID, bankName, lst, makelist(lst,lst) from listStream\nLINK SOURCE EVENT;\n\ncreate stream listTargetStream( str String);\n\ncreate cq updateListTarget\ninsert into listTargetStream\nselect itr\nfrom listStream, iterator(listStream.lst) itr order by cast(itr as java.lang.Comparable);\n\n-- CREATE TARGET listout USING SYSOUT(name:\"list\") input from listStream;\n\n-- ******JsonNode****** --\n\n--CREATE TYPE jsonType (id integer KEY,  lst com.fasterxml.jackson.databind.JsonNode);\nCREATE TYPE jsonType (int integer, bankname string, lst com.fasterxml.jackson.databind.JsonNode key);\n\nCREATE STREAM jsonStream of jsonType partition by bankname;\n\nCREATE JUMPING WINDOW jsonJWindow\nOVER jsonStream\nkeep 3 rows\npartition by bankname;\n\nCREATE WINDOW jsonWindow\nOVER jsonStream\nkeep 3 rows;\n\nCREATE WACTIONSTORE jsonStore CONTEXT OF jsonType EVENT TYPES (jsonType ) \n@PERSIST-TYPE@\n\ncreate cq updateJsonStream\ninsert into jsonStream\n--select TO_INT(MATCH(data[0], '.*\\\\\\\\s([0-9]+)')) , makeJSON('[{\"x\":\"a\"},{\"x\":\"b\",\"y\":\"c\"},{\"y\":\"c\"}]') as lst \nselect TO_INT(y.bankID), y.bankName, x.data \nfrom JSONSourceStream x, dsvcache y;\n\ncreate cq updateJsonStore \ninsert into jsonStore\nselect * from jsonStream\nLINK SOURCE EVENT;\n\ncreate stream jsonTargetStream( str String);\n\ncreate cq updateJsonTarget\ninsert into jsonTargetStream\nselect to_string(itr.StringJSON)\nfrom jsonStream, iterator(jsonStream.lst) itr order by to_string(itr.StringJSON);\nCREATE TARGET jsonout USING SYSOUT(name:\"jlist\") input from jsonStream;\n\nend flow processflow; \n\nend application iteratorapp;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "STOP APPLICATION @WRITERAPPNAME@;\nUNDEPLOY APPLICATION @WRITERAPPNAME@;\nDROP APPLICATION @WRITERAPPNAME@ CASCADE;\ncreate or replace PROPERTYVARIABLE SRC_PASSWORD='@SOURCE_PASS@';\nCREATE APPLICATION @WRITERAPPNAME@ RECOVERY 10 SECOND INTERVAL USE EXCEPTIONSTORE;\n\nCREATE SOURCE @SOURCE@ USING OracleReader\n(\nFetchSize:1,\nUsername:'@SOURCE_USER@',\nPassword:'$SRC_PASSWORD',\nConnectionURL:'@CONNECTION_URL@',\nTables:'@SOURCE_TABLE@',\npassword_encrypted: 'true'\n)\nOUTPUT TO @STREAM1@;\n\nCREATE OR REPLACE TYPE @TYPE@( \ndatae java.util.HashMap , \nTABLE_NAME java.lang.String , \nOPS_NAME java.lang.String , \nDB_TIMESTAMP java.lang.String  ,\nCOMMITSCN java.lang.String ,\nSCN java.lang.String ,\nREC_INS_TIME java.lang.String );\n\nCREATE CQ @CQ1@\nINSERT INTO @STREAM2@\nSELECT  \nCASE WHEN (META(c,\"OperationName\").toString() == \"DELETE\")\nTHEN putUserData(c, 'isDelete', 'true') \nELSE\nputUserData(c,'isDelete', 'false')\nEND\nFROM @STREAM1@ c;\n\nCREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;\n\nCREATE OR REPLACE CQ @CQ2@ \nINSERT INTO @STREAM3@\nSELECT \ndata(e),\nMETA(e,\"TableName\").toString() as TABLE_NAME,\nMETA(e, \"OperationName\").toString() as OPS_NAME,\nMETA(e, \"TimeStamp\").toString() as DB_TIMESTAMP,\nMETA(e,\"COMMITSCN\").toString() as COMMITSCN ,\nMETA(e,\"SCN\").toString() as  SCN ,\nDNOW().toString() as REC_INS_TIME\nFROM @STREAM1@ e;\n\ncreate Target @TARGET1@ using KafkaWriter VERSION @kafkaAdpVersion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC1',\nParallelThreads:'',\nPartitionKey:'@metadata(TableName)',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING dsvFormatter ()\ninput from @STREAM1@;\n\n\ncreate Target @TARGET2@ using KafkaWriter VERSION @kafkaAdpVersion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC2',\nParallelThreads:'2',\nPartitionKey:'TABLE_NAME',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING jsonFormatter ()\ninput from @STREAM3@;\n\ncreate Target @TARGET3@ using KafkaWriter VERSION @kafkaAdpVersion@ (\nbrokerAddress:'localhost:9099',\nTopic:'@WRITERAPPNAME@_TOPIC3',\nParallelThreads:'',\nPartitionKey:'@userdata(isDelete)',\nKafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')\nFORMAT USING avroFormatter (\nschemaFileName:'KafkaAvroTest.avsc')\ninput from @STREAM2@;\n\nend application @WRITERAPPNAME@;\ndeploy application @WRITERAPPNAME@;\nstart @WRITERAPPNAME@;\nstop application @READERAPPNAME@;\nundeploy application @READERAPPNAME@;\ndrop application @READERAPPNAME@ cascade;\nCREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;\n\n\nCREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafkaAdpVersion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC1',\n        startOffset:0       \n)\nPARSE USING DSVParser (\n)\nOUTPUT TO KafkaReaderStream1;\n\n\nCREATE TARGET kafkaDumpDSV USING FileWriter(\nname:kafkaOuputDSV,\nfilename:'@READERAPPNAME@_RT_DSV')\nFORMAT USING DSVFormatter()\nINPUT FROM KafkaReaderStream1;\n\nCREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafkaAdpVersion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC2',\n        startOffset:0          \n)\nPARSE USING JSONParser (\n\teventType:''\n)\nOUTPUT TO KafkaReaderStream2;\n\n\nCREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafkaAdpVersion@ (\n        brokerAddress:'localhost:9099',\n        Topic:'@WRITERAPPNAME@_TOPIC3',\n        startOffset:0          \n)\nPARSE USING AvroParser (\n\tschemaFileName:'KafkaAvroTest.avsc'\n)\nOUTPUT TO KafkaReaderStream3;\n\n\nend application @READERAPPNAME@;\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant, returning correct TQL content."}, {"role": "user", "content": "Please share the TQL content."}, {"role": "assistant", "content": "undeploy application RedshiftColmap;\nalter application RedshiftColmap;\n\nCREATE OR REPLACE SOURCE OracleSource USING OracleReader  (\n  Username: '@LOGMINER-UNAME@',\n  Password: '@LOGMINER-PASSWORD@',\n  ConnectionURL: '@LOGMINER-URL@',\n  Tables: '@SOURCE_TABLES@',\n  FetchSize: 1\n ) Output To LogminerStream;\n     \nCREATE OR REPLACE TARGET RedshiftTarget USING RedshiftWriter\n\t(\n\t  ConnectionURL: '@TARGET-URL@',\n\t  Username: '@TARGET-UNAME@',\n\t  Password: '@TARGET-PASSWORD@',\n\t  bucketname: '@BUCKETNAME@',\n\t  --accesskeyId: '@ACCESS-KEY-ID@',\n\t  --secretaccesskey: '@SECRET-ACCESS-KEY@',\n\t  S3IAMRole:'@IAMROLE@',\n\t  Tables: '@TARGET-TABLES@',\n\t  uploadpolicy:'eventcount:1,interval:5s',\n\t  Mode:'incremental'\n\t) INPUT FROM LogminerStream;\n\t\nEND APPLICATION RedshiftColmap;\nALTER APPLICATION RedshiftColmap RECOMPILE;\ndeploy application RedshiftColmap;\nSTART application RedshiftColmap;\n"}]}
