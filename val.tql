stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'Default',
  ProcessCopyBookFileAs: 'SingleEvent',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
  filename: '%@metadata(FileName)%',
  directory: '@DIR@',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter()
INPUT FROM CCBStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.XMLNodeEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (
  ProviderName: '',
  UserName: '',
  transactionpolicy: '',
  Provider: '',
  Ctx: '',
  EnableTransaction: '',
  QueueName: '',
  Topic:'',
  Password: '' )
PARSE USING XMLParserV2 (
  rootnode:'/JMSXMLIN'
  )
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

CREATE CQ @APPNAME@CQ
INSERT INTO @APPNAME@XMLStream
SELECT
  data.element("companyName").attributeValue("merchantId") as merchantId,
  data.element("companyName").getText() as companyName
FROM @APPNAME@PersistStream@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING Global.FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@XMLStream;

END APPLICATION @APPNAME@;

--
-- Crash Recovery Test 1 on Four node all server cluster 
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP N4S4CR1Tester.N4S4CRTest1;
UNDEPLOY APPLICATION N4S4CR1Tester.N4S4CRTest1;
DROP APPLICATION N4S4CR1Tester.N4S4CRTest1 CASCADE;
CREATE APPLICATION N4S4CRTest1 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest1;

CREATE SOURCE CsvSourceN4S4CRTest1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest1;

CREATE FLOW DataProcessingN4S4CRTest1;

CREATE TYPE WactionTypeN4S4CRTest1 (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE WactionsN4S4CRTest1 CONTEXT OF WactionTypeN4S4CRTest1
EVENT TYPES ( WactionTypeN4S4CRTest1 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN4S4CRTest1
INSERT INTO WactionsN4S4CRTest1
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END FLOW DataProcessingN4S4CRTest1;

END APPLICATION N4S4CRTest1;

STOP APPLICATION @AppName@;
UNDEPLOY APPLICATION @AppName@;
DROP APPLICATION @AppName@ CASCADE;
CREATE APPLICATION @AppName@ recovery 1 second interval;
CREATE SOURCE @AppName@_Source USING FileReader (
	WildCard: 'posdata100.csv',
 directory: '@dir@',
	positionByEOF:false
	)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO @AppName@_Stream;

CREATE TYPE cdctypestream(
 id int,
 name String
);

create stream @StreamType@ of cdctypestream persist using Global.DefaultKafkaProperties;
CREATE OR REPLACE CQ CsvToPosData
INSERT INTO @StreamType@
SELECT
TO_INT(TO_STRING(data[0]).replaceAll("COMPANY ", "")),
data[1]
FROM @AppName@_Stream;
Deploy application @AppName@;
Start @AppName@;

--
-- Recovery Test 50 is a simple Kafka test
-- Nicholas Keene WebAction, Inc.
--
-- S -> CQ -> KSu -> JWc10 -> WS
--

STOP Recov50Tester.RecovTest50;
UNDEPLOY APPLICATION Recov50Tester.RecovTest50;
DROP APPLICATION Recov50Tester.RecovTest50 CASCADE;
CREATE APPLICATION RecovTest50 RECOVERY 5 SECOND INTERVAL;

CREATE or REPLACE TYPE KafkaType(
  value java.lang.Long KEY  
);

CREATE SOURCE KafkaSource USING NumberSource ( 
  lowValue: '1',
  highValue: '1003',
  delayMillis: '10',
  delayNanos: '0',
  repeat: 'false'
 ) 
OUTPUT TO NumberSourceOut;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaStream OF KafkaType using KafkaProps;

CREATE OR REPLACE CQ KafkaStreamPopulate 
INSERT INTO KafkaStream
SELECT data[1]
FROM NumberSourceOut;

CREATE JUMPING WINDOW SizeTenWindow
OVER KafkaStream KEEP 10 ROWS;


CREATE WACTIONSTORE Wactions CONTEXT of KafkaType
@PERSIST-TYPE@

CREATE CQ WactionsPopulate
INSERT INTO Wactions
SELECT * FROM SizeTenWindow;

END APPLICATION RecovTest50;

STOP APPLICATION KafkaWPTester.KWApp;
STOP APPLICATION KafkaWPTester.KRApp;
UNDEPLOY APPLICATION KafkaWPTester.KWApp;
UNDEPLOY APPLICATION KafkaWPTester.KRApp;
DROP APPLICATION KafkaWPTester.KWApp CASCADE;
DROP APPLICATION KafkaWPTester.KRApp CASCADE;

CREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;
GRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;
CONNECT KafkaWPTester KafkaWPTester;


CREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE CSVSource USING FileReader (
	directory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',
    WildCard:'access_log_half',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;

CREATE TYPE AccessLogType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;

CREATE CQ AceeslogCQ
INSERT INTO TypedAccessLogStream
SELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]
FROM FileStream;

create Target DsvWriter using KafkaWriter VERSION '0.11.0' ( 
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V11dsvExistTopic',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream;


create Target JsonWriter using KafkaWriter VERSION '0.11.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V11jsonExistTopic',
	Mode:'Sync'
    )
 
FORMAT USING JSONFormatter ()
input from TypedAccessLogStream;


create Target AvroWriter using KafkaWriter VERSION '0.11.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V11avroExistTopic',
    Mode:'Sync'
    )
 
FORMAT USING AvroFormatter (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')
input from TypedAccessLogStream;


END APPLICATION KWApp;

DEPLOY APPLICATION KWApp on any in default;


-- Kafka Reader Apps

CREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;


CREATE TYPE AccessLogType2(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE Stream KafkaDSVReaderStream of AccessLogType2;
CREATE Stream KafkaJsonReaderStream of AccessLogType2;
CREATE Stream KafkaAvroReaderStream of AccessLogType2;


CREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '0.11.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V11dsvExistTopic',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;




CREATE CQ DSVtoKafkaDSVReaderStream
INSERT INTO KafkaDSVReaderStream
SELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]
FROM KafkaReaderStream1;

CREATE TARGET DSVDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V11_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaDSVReaderStream;

CREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '0.11.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V11jsonExistTopic',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;




CREATE CQ DSVtoKafkaJsonReaderStream
INSERT INTO KafkaJsonReaderStream
SELECT 
data.get('Col1').textValue(),
data.get('Col2').textValue(),
data.get('Col3').textValue(),
data.get('Col4').textValue(),
data.get('Col5').textValue(),
data.get('Col6').textValue(),
data.get('Col7').textValue(),
data.get('Col8').textValue(),
data.get('Col9').textValue(),
data.get('Col10').textValue()
FROM KafkaReaderStream2;

CREATE TARGET JSONDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V11_RT_JSON')
FORMAT USING DSVFormatter()
INPUT FROM KafkaJsonReaderStream;

CREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '0.11.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V11avroExistTopic',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'
)
OUTPUT TO KafkaReaderStream3;



CREATE Stream KRTypedStream of AccessLogType;

CREATE CQ DSVtoKafkaAvroReaderStream
INSERT INTO KafkaAvroReaderStream
SELECT    
data.get("Col1").toString(), 
data.get("Col2").toString(), 
data.get("Col3").toString(), 
data.get("Col4").toString(),
data.get("Col5").toString(), 
data.get("Col6").toString(), 
data.get("Col7").toString(), 
data.get("Col8").toString(),
data.get("Col9").toString(), 
data.get("Col10").toString()
FROM KafkaReaderStream3;

CREATE TARGET AVRODump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V11_RT_AVRO')
FORMAT USING DSVFormatter()
INPUT FROM KafkaAvroReaderStream;

END APPLICATION KRApp;
DEPLOY APPLICATION KRApp on any in default;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	charset:'@charset@'
)
parse using AvroParser (
	schemaFileName:'@fname@',
	schemaRegistryURI:'@evty@'
)
OUTPUT TO CsvStream;
/*
create Target FileTarget using FileWriter(
    rolloverpolicy:'eventcount:100',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using AvroFormatter (
schemaFileName:'@fname@'
)
input from CsvStream;
*/
end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

--
-- Recovery Test 20 with two sources going to one wactionstore
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> CQ1 -> WS
-- S2 -> CQ2 -> WS
--

STOP KStreamRecov20Tester.KStreamRecovTest20;
UNDEPLOY APPLICATION KStreamRecov20Tester.KStreamRecovTest20;
DROP APPLICATION KStreamRecov20Tester.KStreamRecovTest20 CASCADE;
DROP USER KStreamRecov20Tester;
DROP NAMESPACE KStreamRecov20Tester CASCADE;
CREATE USER KStreamRecov20Tester IDENTIFIED BY KStreamRecov20Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov20Tester;
CONNECT KStreamRecov20Tester KStreamRecov20Tester;

CREATE APPLICATION KStreamRecovTest20 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions1
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream1;

CREATE CQ InsertWactions2
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream2;

END APPLICATION KStreamRecovTest20;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE OR REPLACE SOURCE @parquetsrc@ USING Global.HDFSReader (
  wildcard: '',
  directory: '',
  hadoopurl: '',
  hadoopconfigurationpath: '',
  positionbyeof: false )
  PARSE USING ParquetParser (
   )
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: 'ParquetFileSchema'
)
INPUT FROM @appname@CqOut;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using ParquetFormatter (
schemaFileName: 'ParquetS3Schema'
)
input from @appname@CqOut;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using ParquetFormatter (
schemaFileName: 'ParquetAzureSchema'
)
INPUT FROM @appname@CqOut;

CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using ParquetFormatter (
schemaFileName: 'ParquetGCSSchema'
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application RedshiftColmap;
undeploy application RedshiftColmap;
drop application RedshiftColmap CASCADE;
create application RedshiftColmap recovery 1 second interval;
CREATE OR REPLACE SOURCE OracleSource USING OracleReader  (
  Username: 'miner',
  Password: 'miner',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.redshift_emp',
  FetchSize: 1
 )
OUTPUT TO LogminerStream;
CREATE  TARGET RedshiftTarget USING RedshiftWriter  (
  ConnectionURL: '@URL@',
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  bucketname: 'striim-s3',
  --accesskeyId: 'AKIAJFHRZ7VIH2GHON5A',
  --secretaccesskey: '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',
  S3IAMRole:'@IAMROLE@',
  Tables: 'QATEST.redshift_emp,qatest.EMPLOYEE COLUMNMAP(E_DOJ = DOJ, E_NAME = NAME,E_ID = ID)',
  uploadpolicy: 'eventcount:3,interval:5s',
  QuoteCharacter: '@QUOTECHARACTER@',
  Mode: 'incremental',
  ColumnDelimiter: '|'
 )
INPUT FROM LogminerStream;
END APPLICATION RedshiftColmap;
deploy application RedshiftColmap;
START application RedshiftColmap;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;


CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',
					acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

CREATE OR REPLACE STREAM @APPNAME@_stream OF Global.waevent persist using @APPNAME@_KafkaPropset;

CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @APPNAME@_stream2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNEC4TION_URL@',
  Tables: '$table1',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING SnowflakeWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'QATEST.ORCTOSFPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING SnowflakeWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo) input from @STREAM@;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @APPNAME@_stream3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '$table2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@4',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING SnowflakeWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'QATEST.ORCTOSFPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING SnowflakeWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 )
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@3;

END APPLICATION @APPNAME@2;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@ recovery 5 SECOND Interval;
CREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (
  FetchSize: 1,
  ConnectionURL: '@ORACLE-URL@',
  Tables: '@SOURCE-TABLES@',
  Username: '@ORACLE-USERNAME@',
  Password: '@ORACLE-PASSWORD@'
) OUTPUT TO @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt USING SnowflakeWriter

(
  ConnectionURL:'@SNOWFLAKE-URL@',
  username:'@SNOWFLAKE-USERNAME@',
  appendOnly:'false',
  Tables:'@TARGET-TABLES@',
  uploadpolicy:'eventcount:3,interval:10s',
  externalStageType:'local'
)
INPUT FROM @APP_NAME@_Stream;


END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

--
-- Kafka Stream Recovery Test 10 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same key
-- Bert Hashemi and Nicholas Keene WebAction, Inc.
--
-- S1 -> KS -> CQ -> CW(p) -> CQ -> WS
--

STOP KStreamRecov10Tester.KStreamRecovTest10;
UNDEPLOY APPLICATION KStreamRecov10Tester.KStreamRecovTest10;
DROP APPLICATION KStreamRecov10Tester.KStreamRecovTest10 CASCADE;
DROP USER KStreamRecov10Tester;
DROP NAMESPACE KStreamRecov10Tester CASCADE;
CREATE USER KStreamRecov10Tester IDENTIFIED BY KStreamRecov10Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov10Tester;
CONNECT KStreamRecov10Tester KStreamRecov10Tester;

CREATE APPLICATION KStreamRecovTest10 RECOVERY 1 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTest10Data.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  partKey String KEY,
  serialNumber int
);

CREATE STREAM DataStream OF CsvData PARTITION BY partKey;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    TO_INT(data[1])
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStreamTwoItems
OVER DataStream KEEP 2 ROWS
PARTITION BY partKey;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    first(partKey),
    to_int(first(serialNumber))
FROM DataStreamTwoItems
GROUP BY partKey;

END APPLICATION KStreamRecovTest10;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

create Source waSrcName
        Using HPNonStopSQLMXReader
  (AgentPortNo:'7012',
   AgentIPaddress:'$NSKADDR',
   PortNo:'5013',
   -- ipaddress: '$WAADDR',
   Name:'S00',
   ReturnDateTimeAs: 'String',
   Tables:'sncat.snsch.DBWriterTest1')
Output To waAppName_Stream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from waAppName_Stream;

END APPLICATION DataGenSampleApp;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;

 CREATE OR REPLACE SOURCE TS USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01;striim.test02;striim.test03',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream ;

  CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

create target T using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream;


CREATE TARGET log USING LOGWRITER
(
 name:log,
 filename:'LOGFILENAME'
)
INPUT FROM data_stream;


END APPLICATION IR;

deploy application IR;
start IR;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 1 second interval;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@',
  recordSelector: 'OH:MOH-SEG-ID=OH,OH2:OH2-SEG-ID=OH2,OHU:OHU-SEG-ID=OHU,OR1:OR1-SEG-ID=OR1,OR2:OR2-SEG-ID=OR2,OR3:OR3-SEG-ID=OR3,OR3:OR3-SEG-ID=OR3,OHM:OHM-SEG-ID=OR1,OD:OD-SEG-ID=OD,ODU:ODU-SEG-ID=ODU,OD1:OD1-SEG-ID=OD1,ODM:ODM-SEG-ID=ODM,OT:OT-SEG-ID=OT'
)
OUTPUT TO @APPNAME@Stream;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;

/*
create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:1,Interval:5',
  CommitPolicy: 'EventCount:1,Interval:5',
  Tables: 'QATEST.@table@'
)
input from @APPNAME@Stream;*/
end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

STOP APPLICATION KafkaWPTester.KWApp;
STOP APPLICATION KafkaWPTester.KRApp;
UNDEPLOY APPLICATION KafkaWPTester.KWApp;
UNDEPLOY APPLICATION KafkaWPTester.KRApp;
DROP APPLICATION KafkaWPTester.KWApp CASCADE;
DROP APPLICATION KafkaWPTester.KRApp CASCADE;

CREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;
GRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;
CONNECT KafkaWPTester KafkaWPTester;


CREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE CSVSource USING FileReader (
	directory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',
    WildCard:'access_log_half',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;

CREATE TYPE AccessLogType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;

CREATE CQ AceeslogCQ
INSERT INTO TypedAccessLogStream
SELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]
FROM FileStream;

create Target DsvWriter using KafkaWriter VERSION '0.8.0' ( 
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V8dsvExistTopic',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream;


create Target JsonWriter using KafkaWriter VERSION '0.8.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V8jsonExistTopic',
	Mode:'Sync'
    )
 
FORMAT USING JSONFormatter ()
input from TypedAccessLogStream;


create Target AvroWriter using KafkaWriter VERSION '0.8.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V8avroExistTopic',
    Mode:'Sync'
    )
 
FORMAT USING AvroFormatter (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')
input from TypedAccessLogStream;


END APPLICATION KWApp;

DEPLOY APPLICATION KWApp on any in default;


-- Kafka Reader Apps

CREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;


CREATE TYPE AccessLogType2(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE Stream KafkaDSVReaderStream of AccessLogType2;
CREATE Stream KafkaJsonReaderStream of AccessLogType2;
CREATE Stream KafkaAvroReaderStream of AccessLogType2;


CREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '0.8.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V8dsvExistTopic',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;




CREATE CQ DSVtoKafkaDSVReaderStream
INSERT INTO KafkaDSVReaderStream
SELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]
FROM KafkaReaderStream1;

CREATE TARGET DSVDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaDSVReaderStream;

CREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '0.8.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V8jsonExistTopic',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;




CREATE CQ DSVtoKafkaJsonReaderStream
INSERT INTO KafkaJsonReaderStream
SELECT 
data.get('Col1').textValue(),
data.get('Col2').textValue(),
data.get('Col3').textValue(),
data.get('Col4').textValue(),
data.get('Col5').textValue(),
data.get('Col6').textValue(),
data.get('Col7').textValue(),
data.get('Col8').textValue(),
data.get('Col9').textValue(),
data.get('Col10').textValue()
FROM KafkaReaderStream2;

CREATE TARGET JSONDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8_RT_JSON')
FORMAT USING DSVFormatter()
INPUT FROM KafkaJsonReaderStream;

CREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '0.8.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V8avroExistTopic',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'
)
OUTPUT TO KafkaReaderStream3;



CREATE Stream KRTypedStream of AccessLogType;

CREATE CQ DSVtoKafkaAvroReaderStream
INSERT INTO KafkaAvroReaderStream
SELECT    
data.get("Col1").toString(), 
data.get("Col2").toString(), 
data.get("Col3").toString(), 
data.get("Col4").toString(),
data.get("Col5").toString(), 
data.get("Col6").toString(), 
data.get("Col7").toString(), 
data.get("Col8").toString(),
data.get("Col9").toString(), 
data.get("Col10").toString()
FROM KafkaReaderStream3;

CREATE TARGET AVRODump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8_RT_AVRO')
FORMAT USING DSVFormatter()
INPUT FROM KafkaAvroReaderStream;

END APPLICATION KRApp;
DEPLOY APPLICATION KRApp on any in default;

stop PatternMatchingTcp.CSV;
undeploy application PatternMatchingTcp.CSV;
drop application PatternMatchingTCp.CSV cascade;

create application CSV;

create source TCPSource using TCPReader
(
  IpAddress:'127.0.0.1',
  PortNo:'3549'
)

PARSE USING DSVParser
(
header:'false',
metadata:'@TEST-DATA-PATH@/ctest-TCP.csv',endian : false

)
OUTPUT TO TcpStream;

create Target t1 using SysOut(name:Typed1) input from TcpStream;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  TO_INT(data[0]) as UserId,
	    TO_INT(data[1]) as temp1,
        TO_DOUBLE(data[2]) as temp2,
	    TO_STRING(data[3]) as temp3
FROM TcpStream;

-- scenario 1.1 check pattern using timer within 10 seconds and wait
CREATE CQ TypeConversionTCPCQ1
INSERT INTO TypedStream1
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN T A (W | B | C)
define T = timer(interval 10 second),
A = UserDataStream(temp1 >= 20), B= UserDataStream(temp2 < 30.40), C= UserDataStream(temp3 = 'Bret'), W = wait(T)
PARTITION BY UserId;

-- scenario 1.2 check pattern using timer within 20 seconds
CREATE CQ TypeConversionTCPCQ2
INSERT INTO TypedStream2
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN T A C
define T = timer(interval 20 second), B= UserDataStream(temp2 < 30.40), C= UserDataStream(temp3 = 'zalak'),
A = UserDataStream(temp1 >= 20)
PARTITION BY UserId;

-- scenario 1.3 check pattern using timer within 5 seconds with between values
CREATE CQ TypeConversionTCPCQ3
INSERT INTO TypedStream3
SELECT UserId as typeduserid,
	   A.temp1 as typedtemp1
from UserDataStream
MATCH_PATTERN T A
define T = timer(interval 5 second),
A = UserDataStream(temp1 between 10 and 40)
PARTITION BY UserId;

-- scenario 1.4 check pattern using timer which match no events
CREATE CQ TypeConversionTCPCQ4
INSERT INTO TypedStream4
SELECT UserId as typeduserid
from UserDataStream
MATCH_PATTERN T W
define T = timer(interval 50 second), W = wait(T)
PARTITION BY UserId;

-- scenario 1.5 check pattern using stop timer
CREATE CQ TypeConversionTCPCQ5
INSERT INTO TypedStream5
SELECT UserId as typeduserid,
       A.temp1 as typedtemp1,
       B.temp2 as typedtemp2
from UserDataStream
MATCH_PATTERN T A C T2 B
define
T = timer(interval 50 second),
A = UserDataStream(temp1 between 10 and 40),
C = stoptimer(T),
T2 = timer(interval 30 second),
B = UserDataStream(temp2 >= 20)
PARTITION BY UserId;

CREATE WACTIONSTORE UserActivityInfoTcp1
CONTEXT OF TypedStream1_Type
EVENT TYPES ( TypedStream1_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTcp2
CONTEXT OF TypedStream2_Type
EVENT TYPES ( TypedStream2_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTcp3
CONTEXT OF TypedStream3_Type
EVENT TYPES ( TypedStream3_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTcp4
CONTEXT OF TypedStream4_Type
EVENT TYPES ( TypedStream4_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTcp5
CONTEXT OF TypedStream5_Type
EVENT TYPES ( TypedStream5_Type )
@PERSIST-TYPE@

create Target t2 using SysOut(name:Typed2) input from TypedStream1;

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction1
INSERT INTO UserActivityInfoTcp1
SELECT * FROM TypedStream1
LINK SOURCE EVENT;

CREATE CQ UserWaction2
INSERT INTO UserActivityInfoTcp2
SELECT * FROM TypedStream2
LINK SOURCE EVENT;

CREATE CQ UserWaction3
INSERT INTO UserActivityInfoTcp3
SELECT * FROM TypedStream3
LINK SOURCE EVENT;

CREATE CQ UserWaction4
INSERT INTO UserActivityInfoTcp4
SELECT * FROM TypedStream4
LINK SOURCE EVENT;

CREATE CQ UserWaction5
INSERT INTO UserActivityInfoTcp5
SELECT * FROM TypedStream5
LINK SOURCE EVENT;

end application CSV;
deploy application csv;
start csv;

stop IR;
undeploy application IR;
drop application IR cascade;

CREATE APPLICATION IR;

CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.autotest01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.autotest01=id',
 startPosition: 'striim.autotest01=2',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream;

  create type AutoType(
  ID int,
  col1 int,
  col2 string,
  t1 DateTime,
  t2 DateTime
);

CREATE STREAM CDCdata_stream OF AutoType;

CREATE CQ Lookup
INSERT INTO CDCdata_stream
select data[0],data[1],data[2],data[3],data[4] from data_stream;

CREATE  TARGET AzureSQLDWHTarget1 USING AzureSQLDWHWriter  ( 
  ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
  username: 'striim',
  password: 'W3b@ct10n',
  storageaccount: 'striimqatestdonotdelete',
  accesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
  Tables: 'STRIIM.AUTOTEST01',
  uploadpolicy: 'eventcount:1,interval:10s'
 ) 
INPUT FROM CDCdata_stream;

CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM CDCdata_stream;


END APPLICATION IR;
deploy application IR;
start IR;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

Create Source dataGenSrc USING OracleReader  ( 
  FetchSize: 1,
  QueueSize: 2048,
  CommittedTransactions: true,
  Compression: false,
  Username: 'qatest',
  Password: 'r+g6o0bDETs=',
  ConnectionURL: 'localhost:1521:xe',
  FilterTransactionState: true,
  DictionaryMode: 'OnlineCatalog',
  ReaderType: 'LogMiner',
  Tables: '@tableNames@',
  Password_encrypted: true
 ) 
 Output To LCRStream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;

END APPLICATION DataGenSampleApp;

CREATE APPLICATION tungstenAppNoComments;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)

PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;


CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startingTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
@PERSIST-TYPE@


CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false,
  trimwhitespace:true
) 
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'zip') OF USAddressData;


CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;
        
END APPLICATION tungstenAppNoComments;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;
Create Source OS Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;

CREATE TARGET T using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

STOP OuterJoinTester.OJApp;
UNDEPLOY APPLICATION OuterJoinTester.OJApp;
DROP APPLICATION OuterJoinTester.OJApp cascade;

CREATE APPLICATION OJApp;

CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;


CREATE source wsSource2 USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'bankCards.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO stream2;



CREATE TYPE bankData
(
bankID Integer KEY,
bankName String
);

CREATE TYPE cardData
(
cardID Integer KEY,
cardName String
);

CREATE STREAM wsStream OF bankData;
CREATE STREAM wsStream2 OF cardData;


CREATE CACHE cache1 USING CsvReader(
directory: '@TEST-DATA-PATH@',
wildcard: 'banks.csv',
header: No,
columndelimiter: ',',
trimquote: false
) QUERY (keytomap:'bankID') OF bankData;


CREATE CACHE cache2 USING CsvReader(
directory: '@TEST-DATA-PATH@',
wildcard: 'bankCards.csv',
header: No,
columndelimiter: ',',
trimquote: false
) QUERY (keytomap:'cardID') OF cardData;


CREATE WACTIONSTORE oneWS CONTEXT OF bankData
EVENT TYPES(bankData )
@PERSIST-TYPE@

CREATE WACTIONSTORE twoWS CONTEXT OF cardData
EVENT TYPES(cardData )
@PERSIST-TYPE@



CREATE CQ csvTobankData
INSERT INTO oneWS
SELECT TO_INT(data[0]), data[1] FROM QaStream;



CREATE CQ csvTobankData2
INSERT INTO wsStream
SELECT TO_INT(data[0]), data[1] FROM QaStream;

CREATE CQ csvTobankData3
INSERT INTO wsStream2
SELECT TO_INT(data[0]), data[1] FROM stream2;

CREATE CQ csvTobankData4
INSERT INTO twoWS
SELECT TO_INT(data[0]), data[1] FROM stream2;


CREATE JUMPING WINDOW win1 OVER wsStream KEEP 20 rows;


CREATE JUMPING WINDOW win2 OVER wsStream2 KEEP 4 rows;



END APPLICATION OJApp;

--
-- Crash Recovery Test 6 with Jumping window and partitioned on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> KafkaStream -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION N2S2CR6Tester.N2S2CRTest6;
UNDEPLOY APPLICATION N2S2CR6Tester.N2S2CRTest6;
DROP APPLICATION N2S2CR6Tester.N2S2CRTest6 CASCADE;
CREATE APPLICATION N2S2CRTest6 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest6;

CREATE SOURCE CsvSourceN2S2CRTest6 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;




CREATE TYPE CsvDataTypeN2S2CRTest6 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream of CsvDataTypeN2S2CRTest6 using KafkaProps;

CREATE CQ TransferToKafka
INSERT INTO KafkaCsvStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;








END FLOW DataAcquisitionN2S2CRTest6;

CREATE FLOW DataProcessingN2S2CRTest6;

CREATE STREAM DataStream OF CsvDataTypeN2S2CRTest6 PARTITION BY merchantId;

CREATE CQ CsvToDataN2S2CRTest6
INSERT INTO DataStream
SELECT
    *
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsN2S2CRTest6 CONTEXT OF CsvDataTypeN2S2CRTest6
EVENT TYPES ( CsvDataTypeN2S2CRTest6 )
@PERSIST-TYPE@

CREATE CQ DataToWactionN2S2CRTest6
INSERT INTO WactionsN2S2CRTest6
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN2S2CRTest6;

END APPLICATION N2S2CRTest6;

-- The PosApp sample application demonstrates how a credit card
-- payment processor might use Striim to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.


STOP admin.PosApp;
UNDEPLOY APPLICATION admin.PosApp;
Drop Application admin.PosApp cascade;
CREATE APPLICATION PosApp;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING FileReader (
  directory:'/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/appData',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells Striim that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells Striim to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvToPosData

CREATE CQ CsvToPosData
INSERT INTO PosDataStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP.
--
-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as
-- the types to be used in PosDataStream, which is created automatically.

-- The DATETIME field from the source is converted to both a dateTime
-- value, used as the event timestamp by the application, and (via the
-- function) an integer hourValue, which is used to look up
-- historical hourly averages from the HourlyAveLookup cache,
-- discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)
-- 17 (hourValue)
-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateOnly
--
-- The PosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: '/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/appData',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;


-- WAction store MerchantActivity
--
-- The following group of statements create and populate the MerchantActivity
-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced
-- with merchant details from NameLookup cache and with latitude and longitude
-- values from the USAddressData cache.

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
PERSIST EVERY 10 second USING ( JDBC_DRIVER:'org.apache.derby.jdbc.ClientDriver',  JDBC_URL:'jdbc:derby://192.168.1.5:1527/wactionrepos', JDBC_USER:'waction', JDBC_PASSWORD:'w@ct10n', pu_name:derby, DDL_GENERATION:'create-or-extend-tables',  LOGGING_LEVEL:'SEVERE' );


CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/appData',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false,
  trimwhitespace:true
) 
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: '/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/appData',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'zip') OF USAddressData;


CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

-- CQ GenerateAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;

END APPLICATION PosApp;

CREATE DASHBOARD USING "/Users/senthilkumar/Product/IntegrationTests/../Samples/Customer/PosApp/PosAppDashboard.json";

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.waevent PERSIST USING @APPNAME@KafkaPropset;

CREATE TYPE @APPNAME@posDataType (
 merchantId java.lang.String,
 companyName java.lang.String
 );

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (
  ProviderName: '',
  UserName: '',
  transactionpolicy: '',
  Provider: '',
  Ctx: '',
  EnableTransaction: '',
  QueueName: '',
  Topic:'',
  Password: '' )
PARSE USING XMLParser (
  rootnode:'/JMSXMLIN'
  )
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

CREATE STREAM @APPNAME@TypedStream OF @APPNAME@posDataType;

CREATE CQ @APPNAME@CQ
INSERT INTO @APPNAME@TypedStream
SELECT TO_STRING(data[0]) as merchantId,
  TO_STRING(data[1]) as companyName
FROM @APPNAME@PersistStream@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING Global.FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING JSONFormatter  ()
INPUT FROM @APPNAME@TypedStream;

END APPLICATION @APPNAME@;

STOP APPLICATION App1;
UNDEPLOY APPLICATION App1;
DROP APPLICATION App1 CASCADE;
CREATE APPLICATION App1;
CREATE FLOW AgentFlow;
CREATE OR REPLACE SOURCE App1_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App1_SampleStream;
END FLOW AgentFlow;
CREATE FLOW ServerFlow;
CREATE OR REPLACE TARGET App1_NullTarget using NullWriter()
INPUT FROM App1_SampleStream;
END FLOW ServerFlow;
END APPLICATION App1;
deploy application App1 on any in ServerDG1 with AgentFlow on any in Agents, ServerFlow on any in ServerDG1;
START APPLICATION App1;

STOP APPLICATION App2;
UNDEPLOY APPLICATION App2;
DROP APPLICATION App2 CASCADE;
CREATE APPLICATION App2;
CREATE FLOW AgentFlow2;
CREATE OR REPLACE SOURCE App2_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App2_SampleStream;
END FLOW AgentFlow2;
CREATE FLOW ServerFlow2;
CREATE OR REPLACE TARGET App2_NullTarget using NullWriter()
INPUT FROM App2_SampleStream;
END FLOW ServerFlow2;
END APPLICATION App2;
deploy application App2 on any in ServerDG1 with AgentFlow2 on any in Agents, ServerFlow2 on any in ServerDG1;
START APPLICATION App2;

STOP APPLICATION App3;
UNDEPLOY APPLICATION App3;
DROP APPLICATION App3 CASCADE;
CREATE APPLICATION App3;
CREATE OR REPLACE SOURCE App3_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App3_SampleStream;
CREATE OR REPLACE TARGET App3_NullTarget using NullWriter()
INPUT FROM App3_SampleStream;
END APPLICATION App3;
DEPLOY APPLICATION App3;
START APPLICATION App3;

STOP APPLICATION App4;
UNDEPLOY APPLICATION App4;
DROP APPLICATION App4 CASCADE;
CREATE APPLICATION App4;
CREATE OR REPLACE SOURCE App4_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App4_SampleStream;
CREATE OR REPLACE TARGET App4_NullTarget using NullWriter()
INPUT FROM App4_SampleStream;
END APPLICATION App4;
DEPLOY APPLICATION App4 ON ONE IN ServerDG1;
START APPLICATION App4;

STOP APPLICATION App5;
UNDEPLOY APPLICATION App5;
DROP APPLICATION App5 CASCADE;
CREATE APPLICATION App5;
CREATE OR REPLACE SOURCE App5_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App5_SampleStream;
CREATE OR REPLACE TARGET App5_NullTarget using NullWriter()
INPUT FROM App5_SampleStream;
END APPLICATION App5;
DEPLOY APPLICATION App5 ON ONE IN ServerDG1;
START APPLICATION App5;

STOP APPLICATION App6;
UNDEPLOY APPLICATION App6;
DROP APPLICATION App6 CASCADE;
CREATE APPLICATION App6;
CREATE OR REPLACE SOURCE App6_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App6_SampleStream;
CREATE OR REPLACE TARGET App6_NullTarget using NullWriter()
INPUT FROM App6_SampleStream;
END APPLICATION App6;
DEPLOY APPLICATION App6 ON ONE IN ServerDG1;
START APPLICATION App6;

STOP APPLICATION App7;
UNDEPLOY APPLICATION App7;
DROP APPLICATION App7 CASCADE;
CREATE APPLICATION App7;
CREATE OR REPLACE SOURCE App7_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App7_SampleStream;
CREATE OR REPLACE TARGET App7_NullTarget using NullWriter()
INPUT FROM App7_SampleStream;
END APPLICATION App7;
DEPLOY APPLICATION App7 ON ONE IN ServerDG1;
START APPLICATION App7;

STOP APPLICATION App8;
UNDEPLOY APPLICATION App8;
DROP APPLICATION App8 CASCADE;
CREATE APPLICATION App8;
CREATE OR REPLACE SOURCE App8_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App8_SampleStream;
CREATE OR REPLACE TARGET App8_NullTarget using NullWriter()
INPUT FROM App8_SampleStream;
END APPLICATION App8;
DEPLOY APPLICATION App8 ON ONE IN ServerDG1;
START APPLICATION App8;


STOP APPLICATION App9;
UNDEPLOY APPLICATION App9;
DROP APPLICATION App9 CASCADE;
CREATE APPLICATION App9;
CREATE OR REPLACE SOURCE App9_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App9_SampleStream;
CREATE OR REPLACE TARGET App9_NullTarget using NullWriter()
INPUT FROM App9_SampleStream;
END APPLICATION App9;
DEPLOY APPLICATION App9;
START APPLICATION App9;

STOP APPLICATION App10;
UNDEPLOY APPLICATION App10;
DROP APPLICATION App10 CASCADE;
CREATE APPLICATION App10;
CREATE OR REPLACE SOURCE App10_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App10_SampleStream;
CREATE OR REPLACE TARGET App10_NullTarget using NullWriter()
INPUT FROM App10_SampleStream;
END APPLICATION App10;
DEPLOY APPLICATION App10;
START APPLICATION App10;

STOP APPLICATION App11;
UNDEPLOY APPLICATION App11;
DROP APPLICATION App11 CASCADE;
CREATE APPLICATION App11;
CREATE OR REPLACE SOURCE App11_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App11_SampleStream;
CREATE OR REPLACE TARGET App11_NullTarget using NullWriter()
INPUT FROM App11_SampleStream;
END APPLICATION App11;
DEPLOY APPLICATION App11;
START APPLICATION App11;

STOP APPLICATION App12;
UNDEPLOY APPLICATION App12;
DROP APPLICATION App12 CASCADE;
CREATE APPLICATION App12;
CREATE OR REPLACE SOURCE App12_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App12_SampleStream;
CREATE OR REPLACE TARGET App12_NullTarget using NullWriter()
INPUT FROM App12_SampleStream;
END APPLICATION App12;
DEPLOY APPLICATION App12;
START APPLICATION App12;

STOP APPLICATION App13;
UNDEPLOY APPLICATION App13;
DROP APPLICATION App13 CASCADE;
CREATE APPLICATION App13;
CREATE FLOW AgentFlow13;
CREATE OR REPLACE SOURCE App13_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App13_SampleStream;
END FLOW AgentFlow13;
CREATE FLOW ServerFlow13;
CREATE OR REPLACE TARGET App13_NullTarget using NullWriter()
INPUT FROM App13_SampleStream;
END FLOW ServerFlow13;
END APPLICATION App13;
deploy application App13 on any in ServerDG1 with AgentFlow13 on any in Agents, ServerFlow13 on any in ServerDG1;
START APPLICATION App13;

STOP APPLICATION HW ;
undeploy application HW ;
drop application HW cascade;

CREATE APPLICATION HW Recovery 5 second interval;

CREATE  SOURCE S USING OracleReader  ( 
  Username: 'miner',
  Password: '@miner',
  ConnectionURL: '@conn-url@',
  Tables: '@src@',
  FetchSize: 1) 
OUTPUT TO hivestream;

Create Target T using HiveWriter (
  ConnectionURL:'@hive-url@',
  Username:'@uname@', 
            Password:'@pwd@',
            hadoopurl:'hdfs://dockerhost:9000/',
	        Mode:'incremental',
	        mergepolicy: 'eventcount:5,interval:1s',
            Tables:'@tgt-table@',
            hadoopConfigurationPath:'/Users/saranyad/Documents/hello/'
 )
INPUT FROM hivestream;


END APPLICATION HW;
deploy application HW on all in default;

Start application HW;

CREATE APPLICATION FileSource;

CREATE  TYPE FileStr2_Type  ( seq java.lang.Integer
 );

CREATE STREAM FileStr2 OF FileStr2_Type;

CREATE OR REPLACE JUMPING WINDOW FileDataAggregation OVER FileStr2 KEEP 1000000 ROWS;

CREATE OR REPLACE SOURCE FileSource USING FileReader  (
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '@TEST-DATA-PATH@/Validate-Striim',
  skipbom: true,
  wildcard: 'FiletoRead.txt'
 )
 PARSE USING DSVParser  (
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: '-1',
  nocolumndelimiter: true,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: false,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 )
OUTPUT TO FileStr1 ;

CREATE OR REPLACE CQ FileDataConvert
INSERT INTO FileStr2
SELECT TO_INT(data[0]) as seq
FROM FileStr1;

CREATE  TYPE FileStr3_Type  ( SUMFileDataAggregationseq java.lang.Long );

CREATE STREAM FileStr3 OF FileStr3_Type;

CREATE OR REPLACE TARGET FileWrite USING FileWriter  (
  filename: 'SourceResults',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:10000,interval:30',
  adapterName: 'FileWriter',
  directory: '@FEATURE-DIR@/logs',
  rolloverpolicy: 'eventcount:10000,interval:30s'
 )
FORMAT USING DSVFormatter  (   nullvalue: 'NULL',
  standard: 'none',
  handler: 'com.webaction.proc.DSVFormatter',
  formatterName: 'DSVFormatter',
  usequotes: 'false',
  rowdelimiter: '\n',
  quotecharacter: '\"',
  header: 'false',
  columndelimiter: ','
 )
INPUT FROM FileStr3;

CREATE OR REPLACE CQ SumAggregat
INSERT INTO FileStr3
SELECT SUM(FileDataAggregation.seq)
FROM FileDataAggregation;

END APPLICATION FileSource;

create flow @STREAM@Flow1;
create type @STREAM@type (id string, name string);

CREATE OR REPLACE STREAM @STREAM@2 OF @STREAM@type;
CREATE OR REPLACE STREAM @STREAM@3 OF @STREAM@type;
CREATE OR REPLACE STREAM @STREAM@4 OF @STREAM@type;

create cq @STREAM@cq1
insert into @STREAM@2
select 
TO_STRING(data[0]).replaceAll("COMPANY ", ""),
data[1]
from @STREAM@;

end flow @STREAM@Flow1;


create flow @STREAM@Flow2;

create cq @STREAM@cq2
insert into @STREAM@3
select * 
from @STREAM@2 where id is not null ;

create cq @STREAM@cq3
insert into @STREAM@4
select * 
from @STREAM@3 where id is not null ;

end flow @STREAM@Flow2;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING FileWriter( 
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
   ) 
FORMAT USING dsvFormatter  ()
INPUT FROM @STREAM@4;

CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @APPNAME@_src USING Global.GCSReader (
  _h_GCSQueryCoolingTime: '1',
  ServiceAccountKey: '',
  ProjectId: '',
  DownloadPolicy: '',
  BucketName: '',
  FolderName: '',
  ObjectFilter: '',
  ObjectDetectionMode: '',
  IncludeSubfolders: false )
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE CQ @APPNAME@_CQ
INSERT INTO @APPNAME@_CQOut
SELECT
    data[0] as BusinessName,
    data[1] as MerchantId,
    data[2] as PosDataCode,
    data[3] as AccNumber,
    data[4] as DateTime,
    data[5] as ExpDate,
    data[6] as CurrencyCode,
    data[7] as AuthAmount,
    data[8] as TerminalId,
    data[9] as Zip,
    data[10] as City
FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_KafkaTarget USING Global.KafkaWriter VERSION @KAFKA_VERSION@(
  brokerAddress: '',
  Topic: '',
  Mode: 'Sync' )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_HDFSTarget USING Global.HDFSWriter (
  flushpolicy: '',
  rolloverpolicy: '',
  directory: '',
  hadoopurl: '',
  hadoopconfigurationpath: '',
  filename: '')
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_BlobTarget USING Global.AzureBlobWriter (
  containername: '',
  blobname: '',
  accountaccesskey: '',
  accountname: '',
  foldername: '' )
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_DWHTarget USING Global.BigQueryWriter (
  Tables: '',
  BatchPolicy: '',
  projectId: '',
  ServiceAccountKey: '' )
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_NoSqlTarget USING Global.MongoDBWriter (
  AuthDB: 'admin',
  ConnectionURL: '',
  Username: '',
  collections: '',
  Password: '' )
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_FileTarget USING Global.FileWriter (
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '',
  filename: '' )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_OLTPTarget USING Global.DatabaseWriter (
  ConnectionURL: '',
  Password: '',
  Username: '',
  Tables: '' )
INPUT FROM @APPNAME@_CQOut;

CREATE SOURCE @APPNAME@_KafkaSource USING KafkaReader VERSION @KAFKA_VERSION@ (
  brokerAddress: '',
  Topic: '',
  startOffset: '0' )
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@_Stream2;

CREATE OR REPLACE TARGET @APPNAME@_KafkaFileTarget USING Global.FileWriter (
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '',
  filename: '' )
FORMAT USING JSONFormatter(
members:'data')
INPUT FROM @APPNAME@_Stream2;

END APPLICATION @APPNAME@;

stop APPLICATION @APPNAME@_ExceptionStore;;
undeploy APPLICATION @APPNAME@_ExceptionStore;
drop APPLICATION @APPNAME@_ExceptionStore cascade;

CREATE APPLICATION @APPNAME@_ExceptionStore;

CREATE TYPE @APPNAME@2_CDCStreams_Type  (
 exceptionType java.lang.String,
  action java.lang.String,
  appName java.lang.String,
  entityType java.lang.String,
  entityName java.lang.String,
  className java.lang.String,
  message java.lang.String,
  relatedEntity java.lang.String,
  exceptionCode java.lang.String
 );

CREATE STREAM @APPNAME@2_CDCStreams OF @APPNAME@2_CDCStreams_Type;

CREATE CQ @APPNAME@2_ReadFromExpStore
INSERT INTO @APPNAME@2_CDCStreams
select s.exceptionType,s.action,s.appName,s.entityType,s.entityName,s.className,s.message,s.relatedEntity,s.exceptionCode from @APPNAME@_App_ExceptionStore [jumping @WINDOWINTERVAL@ seconds] s;

CREATE OR REPLACE TARGET @APPNAME@2_WriteToFileAsJSON USING FileWriter  (
  filename: 'expEvent.log',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:5',
  adapterName: 'FileWriter',
  directory: 'ExpStore_logs',
  rolloverpolicy: 'eventcount:5'
 )
FORMAT USING JSONFormatter  (
  handler: 'com.webaction.proc.JSONFormatter',
  formatterName: 'JSONFormatter',
  jsonMemberDelimiter: '\n',
  jsonobjectdelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true'
 )
INPUT FROM @APPNAME@2_CDCStreams;

END APPLICATION @APPNAME@_ExceptionStore;

deploy application @APPNAME@_ExceptionStore;
start @APPNAME@_ExceptionStore;

STOP Mssqltobigquery;
UNDEPLOY APPLICATION Mssqltobigquery;
DROP APPLICATION Mssqltobigquery CASCADE;

CREATE APPLICATION Mssqltobigquery RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE Mssqltobigquery_source USING MSSqlReader
(
Username:'qatest',
Password:'w3b@ct10n',
DatabaseName:'qatest',
ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',
Tables:'qatest.srctb',
ConnectionPoolSize:1,
Compression:'true'
)
OUTPUT TO SS;


CREATE or replace TARGET Mssqltobigquery_Target USING BigQueryWriter (
ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  projectId:'striimqa-214712',
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:5,Interval:120',
  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', 
  AllowQuotedNewLines: 'false', 
  adapterName: 'BigQueryWriter', 
  optimizedMerge: 'true', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', 
  StandardSQL: 'true', 
  QuoteCharacter: '\"', 
  Tables: 'QATEST.srctb,.oratobqtgt',
  Mode: 'MERGE',
  StandardSQL: 'true',
  QuoteCharacter: '\"'
) INPUT FROM SS;

END APPLICATION Mssqltobigquery;
DEPLOY APPLICATION Mssqltobigquery;
START APPLICATION Mssqltobigquery;

--
-- Recovery Test 12 with two sources, two jumping attribute windows, one wactionstore with recovery, and another wactionstore without -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W/p -> CQ1 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS2 (no persists)
--

STOP Recov12Tester.RecovTest12;
UNDEPLOY APPLICATION Recov12Tester.RecovTest12;
DROP APPLICATION Recov12Tester.RecovTest12 CASCADE;
CREATE APPLICATION RecovTest12 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsNoPersist CONTEXT OF WactionData
EVENT TYPES ( CsvData )
		PERSIST NONE USING ( ) ;

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWactionNoPersist
INSERT INTO WactionsNoPersist
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION RecovTest12;

stop Oracle_IRLogWriter;
undeploy application Oracle_IRLogWriter;
drop application Oracle_IRLogWriter cascade;

CREATE APPLICATION Oracle_IRLogWriter;

CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.autotest01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.autotest01=id',
 startPosition: 'striim.autotest01=2',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream;

  create type AutoType(
  ID string,
  name string,
  company string,
  country string
);

CREATE STREAM CDCdata_stream OF AutoType;

CREATE CQ Lookup
INSERT INTO CDCdata_stream
select data[0],data[1],data[2],data[3] from data_stream;

CREATE  TARGET AzureSQLDWHTarget USING AzureSQLDWHWriter  ( 
  ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
  username: 'striim',
  password: 'W3b@ct10n',
   accountname: 'striimqatestdonotdelete',
   Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
   Tables: 'STRIIM.AUTOTEST01',
  uploadpolicy: 'eventcount:1,interval:10s'
 ) 
INPUT FROM CDCdata_stream;

CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM CDCdata_stream;


END APPLICATION Oracle_IRLogWriter;
deploy application Oracle_IRLogWriter;
start Oracle_IRLogWriter;

--
-- Recovery Test 3
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> JW -> CQ(aggregate) -> WS
--

STOP Recov3Tester.RecovTest3;
UNDEPLOY APPLICATION Recov3Tester.RecovTest3;
DROP APPLICATION Recov3Tester.RecovTest3 CASCADE;
CREATE APPLICATION RecovTest3 RECOVERY 1 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount int,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_INT(TO_DOUBLE(data[7])),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

END APPLICATION RecovTest3;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ recovery 5 second interval;

CREATE OR REPLACE SOURCE @SOURCENAME@ USING IncrementalBatchReader  (
  FetchSize: 1000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: @startPosition@,
  PollingInterval: '20sec'
  )
  OUTPUT TO @STREAM@;

  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;

  CREATE TARGET @targetName@ USING DatabaseWriter(
    ConnectionURL:'@READER-URL@',
    Username:'@READER-UNAME@',
    Password:'@READER-PASSWORD@',
    BatchPolicy:'Eventcount:1000,Interval:1000',
    CommitPolicy:'Eventcount:1000,Interval:1000',
    Checkpointtable:'CHKPOINT',
    Tables:'@WATABLES@,@WATABLES@_target'
  ) INPUT FROM @STREAM@;

  END APPLICATION @APPNAME@;

  DEPLOY APPLICATION @APPNAME@;
  start application @APPNAME@;

--
-- Recovery Test 22 with two sources, two sliding attribute windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sa5W -> CQ1 -> WS
-- S2 -> Sa6W -> CQ2 -> WS
--

STOP Recov22Tester.RecovTest22;
UNDEPLOY APPLICATION Recov22Tester.RecovTest22;
DROP APPLICATION Recov22Tester.RecovTest22 CASCADE;
CREATE APPLICATION RecovTest22 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2;

END APPLICATION RecovTest22;

stop @AppName@;
Undeploy application @AppName@;
alter application @AppName@;
CREATE FLOW CP_Agent_flow;
CREATE OR REPLACE SOURCE CP_Oracle_source USING OracleReader ( 
  ConnectionURL: '@url@', 
  Tables: '@tables@', 
  Username: '@Username@', 
  Password: '@password@' ) 
OUTPUT TO CP_EndToEnd_SF_Adapter_Stream;
End Flow CP_Agent_flow;
alter application @AppName@ recompile;
DEPLOY APPLICATION @AppName@ with CP_Agent_flow on any in AGENTS;
start application @AppName@;

--
-- Recovery Test 8
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov8Tester.RecovTest8;
UNDEPLOY APPLICATION Recov8Tester.RecovTest8;
DROP APPLICATION Recov8Tester.RecovTest8 CASCADE;
CREATE APPLICATION RecovTest8 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END APPLICATION RecovTest8;

drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

CREATE TYPE @APPNAME@type1 (
 companyName java.lang.String,
 merchantId java.lang.String,
 city java.lang.String);

CREATE STREAM @APPNAME@TypedStream OF @APPNAME@type1 PARTITION BY city;

CREATE OR REPLACE SOURCE @APPNAME@_src USING FileReader (
  wildcard: '',
  positionByEOF: false,
  directory: ''
  )
PARSE USING DSVParser (
header:'true'
)
OUTPUT TO @APPNAME@Stream;

CREATE OR REPLACE CQ @APPNAME@CQ
INSERT INTO @APPNAME@TypedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantID,
TO_STRING(data[10]) as city
FROM @APPNAME@Stream;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@TypedStream;

END APPLICATION @APPNAME@;

--
-- Crash Recovery Test 5 with Jumping window and partitioned on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION N4S4CR5Tester.N4S4CRTest5;
UNDEPLOY APPLICATION N4S4CR5Tester.N4S4CRTest5;
DROP APPLICATION N4S4CR5Tester.N4S4CRTest5 CASCADE;
CREATE APPLICATION N4S4CRTest5 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest5;

CREATE SOURCE CsvSourceN4S4CRTest5 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest5;

CREATE FLOW DataProcessingN4S4CRTest5;

CREATE TYPE CsvDataN4S4CRTest5 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvDataN4S4CRTest5 PARTITION BY merchantId;

CREATE CQ CsvToDataN4S4CRTest5
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsN4S4CRTest5 CONTEXT OF CsvDataN4S4CRTest5
EVENT TYPES ( CsvDataN4S4CRTest5 )
@PERSIST-TYPE@

CREATE CQ DataToWactionN4S4CRTest5
INSERT INTO WactionsN4S4CRTest5
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN4S4CRTest5;

END APPLICATION N4S4CRTest5;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@  RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @srcName@ USING OracleReader (
 Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@'
)
OUTPUT TO @outstreamname@;

CREATE  TARGET @tgtName@ USING Global.KinesisWriter ( 
  BatchPolicy: 'Size:900000,Interval:1', 
  streamName: '@streamname@', 
  accesskeyid: '@accesskeyid@', 
  Mode: 'Sync', 
  regionName: '@region@', 
  secretaccesskey: '@secretaccesskey@', 
  adapterName: 'KinesisWriter' ) 
FORMAT USING Global.DSVFormatter  ( 
  quotecharacter: '\"', 
  handler: 'com.webaction.proc.DSVFormatter', 
  columndelimiter: ',', 
  formatterName: 'DSVFormatter', 
  nullvalue: 'NULL', 
  usequotes: 'false', 
  rowdelimiter: '\n', 
  standard: 'none', 
  header: 'false' ) 
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

--CREATE APPLICATION @APPNAME@;
create application @APPNAME@ Recovery 5 second Interval;

--create or replace flow @APPNAME@_agentflow;

CREATE OR REPLACE SOURCE @APPNAME@_source USING MariaDBReader 
(
Username: '@READER-UNAME@',
Password: '@READER-PASSWORD@',
ConnectionURL: '@CDC-READER-URL@',
Tables: @WATABLES@,
sendBeforeImage:'true',
FilterTransactionBoundaries: 'true'
) 
OUTPUT TO @APPNAME@_stream ;

--end flow @APPNAME@_@APPNAME@_agentflow;

CREATE OR REPLACE TARGET @APPNAME@_target USING CassandraCosmosDBWriter  (
  AccountEndpoint: 'cassandracosmostest.cassandra.cosmos.azure.com',
  AccountKey: 'pqDZvVgbdSCg7VzIzD77dAhPG2odGRZPLhAQA1qnZbAKoIDk6RuQX5r2phbRQFnR1l54qxOcvBXNdz8DeijYIg==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  Tables: 'QATEST.Source1,test.target1',
  adapterName: 'CassandraCosmosDBWriter'
 )
 INPUT FROM @APPNAME@_stream;

 END APPLICATION @APPNAME@;

deploy application @APPNAME@;
 --deploy application @APPNAME@ with agentflow in agents;
 start application @APPNAME@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.test01',
	FetchSize: '1',
	connectionRetryPolicy:'timeOut=00,retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(id,col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'Eventcount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true		
) INPUT FROM @APPNAME@_ss;
--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 SECOND Interval;;

Create Source @SourceName@ Using Ojet

(
  Username:'c##qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:oci:@//localhost:1529/orcl',
  Tables:'CDB$ROOT."C##QATEST".ojet_src;ORCLPDB.QATEST.ojet_src',
  _h_useClassic:false,
  Fetchsize:1,
  Compression: true,
  SupportPDB:true,
  ReplicationSlotName:'null'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orcl',
  Username:'c##qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'CDB$ROOT."C##QATEST".ojet_src,CDB$ROOT."C##QATEST".ojet_tgt'
) INPUT FROM @SRCINPUTSTREAM@;

CREATE TARGET @targetName1@ USING DatabaseWriter
(
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1529/orclpdb',
  Username:'qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
  Tables:'ORCLPDB.QATEST.ojet_src,ORCLPDB.QATEST.ojet_tgt'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP APPLICATION @Appname@;
UNDEPLOY APPLICATION @Appname@;
DROP APPLICATION @Appname@ CASCADE;

CREATE APPLICATION @Appname@ RECOVERY 10 SECOND INTERVAL;

CREATE  SOURCE @Appname@source USING MySQLReader
(
Username: '',
Password: '',
ConnectionURL: '',
Tables: 'waction.TABLE_TEST_%',
FetchSize:1
)
OUTPUT TO @Appname@MasterStream;

CREATE OR REPLACE ROUTER @Appname@Rs1 INPUT FROM @Appname@MasterStream s CASE
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_1' THEN ROUTE TO @Appname@Typed1,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_2' THEN ROUTE TO @Appname@Typed2,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_3' THEN ROUTE TO @Appname@Typed3,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_4' THEN ROUTE TO @Appname@Typed4,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_5' THEN ROUTE TO @Appname@Typed5,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_6' THEN ROUTE TO @Appname@Typed6,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_7' THEN ROUTE TO @Appname@Typed7,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_8' THEN ROUTE TO @Appname@Typed8,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_9' THEN ROUTE TO @Appname@Typed9,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_10' THEN ROUTE TO @Appname@Typed10,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_11' THEN ROUTE TO @Appname@Typed11,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_12' THEN ROUTE TO @Appname@Typed12,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_13' THEN ROUTE TO @Appname@Typed13,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_14' THEN ROUTE TO @Appname@Typed14,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_15' THEN ROUTE TO @Appname@Typed15,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_16' THEN ROUTE TO @Appname@Typed16,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_17' THEN ROUTE TO @Appname@Typed17,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_18' THEN ROUTE TO @Appname@Typed18,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_19' THEN ROUTE TO @Appname@Typed19,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_20' THEN ROUTE TO @Appname@Typed20,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_21' THEN ROUTE TO @Appname@Typed21,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_22' THEN ROUTE TO @Appname@Typed22,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_23' THEN ROUTE TO @Appname@Typed23,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_24' THEN ROUTE TO @Appname@Typed24,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_25' THEN ROUTE TO @Appname@Typed25,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_26' THEN ROUTE TO @Appname@Typed26,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_27' THEN ROUTE TO @Appname@Typed27,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_28' THEN ROUTE TO @Appname@Typed28,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_29' THEN ROUTE TO @Appname@Typed29,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_30' THEN ROUTE TO @Appname@Typed30,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_31' THEN ROUTE TO @Appname@Typed31,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_32' THEN ROUTE TO @Appname@Typed32,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_33' THEN ROUTE TO @Appname@Typed33,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_34' THEN ROUTE TO @Appname@Typed34,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_35' THEN ROUTE TO @Appname@Typed35,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_36' THEN ROUTE TO @Appname@Typed36,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_37' THEN ROUTE TO @Appname@Typed37,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_38' THEN ROUTE TO @Appname@Typed38,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_39' THEN ROUTE TO @Appname@Typed39,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_40' THEN ROUTE TO @Appname@Typed40,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_41' THEN ROUTE TO @Appname@Typed41,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_42' THEN ROUTE TO @Appname@Typed42,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_43' THEN ROUTE TO @Appname@Typed43,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_44' THEN ROUTE TO @Appname@Typed44,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_45' THEN ROUTE TO @Appname@Typed45,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_46' THEN ROUTE TO @Appname@Typed46,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_47' THEN ROUTE TO @Appname@Typed47,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_48' THEN ROUTE TO @Appname@Typed48,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_49' THEN ROUTE TO @Appname@Typed49,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_50' THEN ROUTE TO @Appname@Typed50,
/*WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_51' THEN ROUTE TO @Appname@Typed51,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_52' THEN ROUTE TO @Appname@Typed52,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_53' THEN ROUTE TO @Appname@Typed53,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_54' THEN ROUTE TO @Appname@Typed54,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_55' THEN ROUTE TO @Appname@Typed55,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_56' THEN ROUTE TO @Appname@Typed56,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_57' THEN ROUTE TO @Appname@Typed57,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_58' THEN ROUTE TO @Appname@Typed58,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_59' THEN ROUTE TO @Appname@Typed59,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_60' THEN ROUTE TO @Appname@Typed60,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_61' THEN ROUTE TO @Appname@Typed61,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_62' THEN ROUTE TO @Appname@Typed62,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_63' THEN ROUTE TO @Appname@Typed63,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_64' THEN ROUTE TO @Appname@Typed64,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_65' THEN ROUTE TO @Appname@Typed65,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_66' THEN ROUTE TO @Appname@Typed66,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_67' THEN ROUTE TO @Appname@Typed67,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_68' THEN ROUTE TO @Appname@Typed68,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_69' THEN ROUTE TO @Appname@Typed69,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_70' THEN ROUTE TO @Appname@Typed70,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_71' THEN ROUTE TO @Appname@Typed71,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_72' THEN ROUTE TO @Appname@Typed72,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_73' THEN ROUTE TO @Appname@Typed73,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_74' THEN ROUTE TO @Appname@Typed74,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_75' THEN ROUTE TO @Appname@Typed75,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_76' THEN ROUTE TO @Appname@Typed76,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_77' THEN ROUTE TO @Appname@Typed77,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_78' THEN ROUTE TO @Appname@Typed78,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_79' THEN ROUTE TO @Appname@Typed79,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_80' THEN ROUTE TO @Appname@Typed80,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_81' THEN ROUTE TO @Appname@Typed81,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_82' THEN ROUTE TO @Appname@Typed82,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_83' THEN ROUTE TO @Appname@Typed83,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_84' THEN ROUTE TO @Appname@Typed84,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_85' THEN ROUTE TO @Appname@Typed85,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_86' THEN ROUTE TO @Appname@Typed86,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_87' THEN ROUTE TO @Appname@Typed87,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_88' THEN ROUTE TO @Appname@Typed88,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_89' THEN ROUTE TO @Appname@Typed89,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_90' THEN ROUTE TO @Appname@Typed90,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_91' THEN ROUTE TO @Appname@Typed91,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_92' THEN ROUTE TO @Appname@Typed92,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_93' THEN ROUTE TO @Appname@Typed93,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_94' THEN ROUTE TO @Appname@Typed94,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_95' THEN ROUTE TO @Appname@Typed95,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_96' THEN ROUTE TO @Appname@Typed96,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_97' THEN ROUTE TO @Appname@Typed97,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_98' THEN ROUTE TO @Appname@Typed98,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_99' THEN ROUTE TO @Appname@Typed99,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_1000100' THEN ROUTE TO @Appname@Typed100,*/
ELSE ROUTE TO @Appname@TypedElse;



CREATE OR REPLACE cq @Appname@cq1
INSERT INTO combinedstream
select s FROM @Appname@Typed1 s;

CREATE OR REPLACE cq @Appname@cq2
INSERT INTO combinedstream
select s FROM @Appname@Typed2 s;

CREATE OR REPLACE cq @Appname@cq3
INSERT INTO combinedstream
select s FROM @Appname@Typed3 s;

CREATE OR REPLACE cq @Appname@cq4
INSERT INTO combinedstream
select s FROM @Appname@Typed4 s;

CREATE OR REPLACE cq @Appname@cq5
INSERT INTO combinedstream
select s FROM @Appname@Typed5 s;

CREATE OR REPLACE cq @Appname@cq6
INSERT INTO combinedstream
select s FROM @Appname@Typed6 s;

CREATE OR REPLACE cq @Appname@cq7
INSERT INTO combinedstream
select s FROM @Appname@Typed7 s;

CREATE OR REPLACE cq @Appname@cq8
INSERT INTO combinedstream
select s FROM @Appname@Typed8 s;

CREATE OR REPLACE cq @Appname@cq9
INSERT INTO combinedstream
select s FROM @Appname@Typed9 s;

CREATE OR REPLACE cq @Appname@cq10
INSERT INTO combinedstream
select s FROM @Appname@Typed10 s;


CREATE OR REPLACE cq @Appname@cq11
INSERT INTO combinedstream
select s FROM @Appname@Typed11 s;

CREATE OR REPLACE cq @Appname@cq12
INSERT INTO combinedstream
select s FROM @Appname@Typed12 s;

CREATE OR REPLACE cq @Appname@cq13
INSERT INTO combinedstream
select s FROM @Appname@Typed13 s;

CREATE OR REPLACE cq @Appname@cq14
INSERT INTO combinedstream
select s FROM @Appname@Typed14 s;

CREATE OR REPLACE cq @Appname@cq15
INSERT INTO combinedstream
select s FROM @Appname@Typed15 s;

CREATE OR REPLACE cq @Appname@cq16
INSERT INTO combinedstream
select s FROM @Appname@Typed16 s;

CREATE OR REPLACE cq @Appname@cq117
INSERT INTO combinedstream
select s FROM @Appname@Typed17 s;

CREATE OR REPLACE cq @Appname@cq18
INSERT INTO combinedstream
select s FROM @Appname@Typed18 s;

CREATE OR REPLACE cq @Appname@cq19
INSERT INTO combinedstream
select s FROM @Appname@Typed19 s;

CREATE OR REPLACE cq @Appname@cq20
INSERT INTO combinedstream
select s FROM @Appname@Typed20 s;

CREATE OR REPLACE cq @Appname@cq21
INSERT INTO combinedstream
select s FROM @Appname@Typed21 s;

CREATE OR REPLACE cq @Appname@cq22
INSERT INTO combinedstream
select s FROM @Appname@Typed22 s;

CREATE OR REPLACE cq @Appname@cq23
INSERT INTO combinedstream
select s FROM @Appname@Typed23 s;

CREATE OR REPLACE cq @Appname@cq24
INSERT INTO combinedstream
select s FROM @Appname@Typed24 s;

CREATE OR REPLACE cq @Appname@cq25
INSERT INTO combinedstream
select s FROM @Appname@Typed25 s;

CREATE OR REPLACE cq @Appname@cq26
INSERT INTO combinedstream
select s FROM @Appname@Typed26 s;

CREATE OR REPLACE cq @Appname@cq27
INSERT INTO combinedstream
select s FROM @Appname@Typed27 s;

CREATE OR REPLACE cq @Appname@cq28
INSERT INTO combinedstream
select s FROM @Appname@Typed28 s;

CREATE OR REPLACE cq @Appname@cq29
INSERT INTO combinedstream
select s FROM @Appname@Typed29 s;

CREATE OR REPLACE cq @Appname@cq30
INSERT INTO combinedstream
select s FROM @Appname@Typed30 s;

CREATE OR REPLACE cq @Appname@cq31
INSERT INTO combinedstream
select s FROM @Appname@Typed31 s;

CREATE OR REPLACE cq @Appname@cq32
INSERT INTO combinedstream
select s FROM @Appname@Typed32 s;

CREATE OR REPLACE cq @Appname@cq33
INSERT INTO combinedstream
select s FROM @Appname@Typed33 s;

CREATE OR REPLACE cq @Appname@cq34
INSERT INTO combinedstream
select s FROM @Appname@Typed34 s;

CREATE OR REPLACE cq @Appname@cq35
INSERT INTO combinedstream
select s FROM @Appname@Typed35 s;

CREATE OR REPLACE cq @Appname@cq36
INSERT INTO combinedstream
select s FROM @Appname@Typed36 s;

CREATE OR REPLACE cq @Appname@cq37
INSERT INTO combinedstream
select s FROM @Appname@Typed37 s;

CREATE OR REPLACE cq @Appname@cq38
INSERT INTO combinedstream
select s FROM @Appname@Typed38 s;

CREATE OR REPLACE cq @Appname@cq39
INSERT INTO combinedstream
select s FROM @Appname@Typed39 s;

CREATE OR REPLACE cq @Appname@cq40
INSERT INTO combinedstream
select s FROM @Appname@Typed40 s;

CREATE OR REPLACE cq @Appname@cq41
INSERT INTO combinedstream
select s FROM @Appname@Typed41 s;

CREATE OR REPLACE cq @Appname@cq42
INSERT INTO combinedstream
select s FROM @Appname@Typed42 s;

CREATE OR REPLACE cq @Appname@cq43
INSERT INTO combinedstream
select s FROM @Appname@Typed43 s;

CREATE OR REPLACE cq @Appname@cq44
INSERT INTO combinedstream
select s FROM @Appname@Typed44 s;

CREATE OR REPLACE cq @Appname@cq45
INSERT INTO combinedstream
select s FROM @Appname@Typed45 s;

CREATE OR REPLACE cq @Appname@cq46
INSERT INTO combinedstream
select s FROM @Appname@Typed46 s;

CREATE OR REPLACE cq @Appname@cq47
INSERT INTO combinedstream
select s FROM @Appname@Typed47 s;

CREATE OR REPLACE cq @Appname@cq48
INSERT INTO combinedstream
select s FROM @Appname@Typed48 s;

CREATE OR REPLACE cq @Appname@cq49
INSERT INTO combinedstream
select s FROM @Appname@Typed49 s;

CREATE OR REPLACE cq @Appname@cq50
INSERT INTO combinedstream
select s FROM @Appname@Typed50 s;
/*
CREATE OR REPLACE cq @Appname@cq51
INSERT INTO combinedstream
select s FROM @Appname@Typed51 s;

CREATE OR REPLACE cq @Appname@cq52
INSERT INTO combinedstream
select s FROM @Appname@Typed52 s;

CREATE OR REPLACE cq @Appname@cq53
INSERT INTO combinedstream
select s FROM @Appname@Typed53 s;

CREATE OR REPLACE cq @Appname@cq54
INSERT INTO combinedstream
select s FROM @Appname@Typed54 s;

CREATE OR REPLACE cq @Appname@cq55
INSERT INTO combinedstream
select s FROM @Appname@Typed55 s;

CREATE OR REPLACE cq @Appname@cq56
INSERT INTO combinedstream
select s FROM @Appname@Typed56 s;

CREATE OR REPLACE cq @Appname@cq57
INSERT INTO combinedstream
select s FROM @Appname@Typed57 s;

CREATE OR REPLACE cq @Appname@cq58
INSERT INTO combinedstream
select s FROM @Appname@Typed58 s;

CREATE OR REPLACE cq @Appname@cq59
INSERT INTO combinedstream
select s FROM @Appname@Typed59 s;

CREATE OR REPLACE cq @Appname@cq60
INSERT INTO combinedstream
select s FROM @Appname@Typed60 s;

CREATE OR REPLACE cq @Appname@cq61
INSERT INTO combinedstream
select s FROM @Appname@Typed61 s;

CREATE OR REPLACE cq @Appname@cq62
INSERT INTO combinedstream
select s FROM @Appname@Typed62 s;

CREATE OR REPLACE cq @Appname@cq63
INSERT INTO combinedstream
select s FROM @Appname@Typed63 s;

CREATE OR REPLACE cq @Appname@cq64
INSERT INTO combinedstream
select s FROM @Appname@Typed64 s;

CREATE OR REPLACE cq @Appname@cq65
INSERT INTO combinedstream
select s FROM @Appname@Typed65 s;

CREATE OR REPLACE cq @Appname@cq66
INSERT INTO combinedstream
select s FROM @Appname@Typed66 s;

CREATE OR REPLACE cq @Appname@cq67
INSERT INTO combinedstream
select s FROM @Appname@Typed67 s;

CREATE OR REPLACE cq @Appname@cq68
INSERT INTO combinedstream
select s FROM @Appname@Typed68 s;

CREATE OR REPLACE cq @Appname@cq69
INSERT INTO combinedstream
select s FROM @Appname@Typed69 s;

CREATE OR REPLACE cq @Appname@cq70
INSERT INTO combinedstream
select s FROM @Appname@Typed70 s;

CREATE OR REPLACE cq @Appname@cq71
INSERT INTO combinedstream
select s FROM @Appname@Typed71 s;

CREATE OR REPLACE cq @Appname@cq72
INSERT INTO combinedstream
select s FROM @Appname@Typed72 s;

CREATE OR REPLACE cq @Appname@cq73
INSERT INTO combinedstream
select s FROM @Appname@Typed73 s;

CREATE OR REPLACE cq @Appname@cq74
INSERT INTO combinedstream
select s FROM @Appname@Typed74 s;

CREATE OR REPLACE cq @Appname@cq75
INSERT INTO combinedstream
select s FROM @Appname@Typed75 s;

CREATE OR REPLACE cq @Appname@cq76
INSERT INTO combinedstream
select s FROM @Appname@Typed76 s;

CREATE OR REPLACE cq @Appname@cq77
INSERT INTO combinedstream
select s FROM @Appname@Typed77 s;

CREATE OR REPLACE cq @Appname@cq78
INSERT INTO combinedstream
select s FROM @Appname@Typed78 s;

CREATE OR REPLACE cq @Appname@cq79
INSERT INTO combinedstream
select s FROM @Appname@Typed79 s;

CREATE OR REPLACE cq @Appname@cq80
INSERT INTO combinedstream
select s FROM @Appname@Typed80 s;

CREATE OR REPLACE cq @Appname@cq81
INSERT INTO combinedstream
select s FROM @Appname@Typed81 s;

CREATE OR REPLACE cq @Appname@cq82
INSERT INTO combinedstream
select s FROM @Appname@Typed82 s;

CREATE OR REPLACE cq @Appname@cq83
INSERT INTO combinedstream
select s FROM @Appname@Typed83 s;

CREATE OR REPLACE cq @Appname@cq84
INSERT INTO combinedstream
select s FROM @Appname@Typed84 s;

CREATE OR REPLACE cq @Appname@cq85
INSERT INTO combinedstream
select s FROM @Appname@Typed85 s;

CREATE OR REPLACE cq @Appname@cq86
INSERT INTO combinedstream
select s FROM @Appname@Typed86 s;

CREATE OR REPLACE cq @Appname@cq87
INSERT INTO combinedstream
select s FROM @Appname@Typed87 s;

CREATE OR REPLACE cq @Appname@cq88
INSERT INTO combinedstream
select s FROM @Appname@Typed88 s;

CREATE OR REPLACE cq @Appname@cq89
INSERT INTO combinedstream
select s FROM @Appname@Typed89 s;

CREATE OR REPLACE cq @Appname@cq90
INSERT INTO combinedstream
select s FROM @Appname@Typed90 s;

CREATE OR REPLACE cq @Appname@cq91
INSERT INTO combinedstream
select s FROM @Appname@Typed91 s;

CREATE OR REPLACE cq @Appname@cq92
INSERT INTO combinedstream
select s FROM @Appname@Typed92 s;

CREATE OR REPLACE cq @Appname@cq93
INSERT INTO combinedstream
select s FROM @Appname@Typed93 s;

CREATE OR REPLACE cq @Appname@cq94
INSERT INTO combinedstream
select s FROM @Appname@Typed94 s;

CREATE OR REPLACE cq @Appname@cq95
INSERT INTO combinedstream
select s FROM @Appname@Typed95 s;

CREATE OR REPLACE cq @Appname@cq96
INSERT INTO combinedstream
select s FROM @Appname@Typed96 s;

CREATE OR REPLACE cq @Appname@cq97
INSERT INTO combinedstream
select s FROM @Appname@Typed97 s;

CREATE OR REPLACE cq @Appname@cq98
INSERT INTO combinedstream
select s FROM @Appname@Typed98 s;

CREATE OR REPLACE cq @Appname@cq99
INSERT INTO combinedstream
select s FROM @Appname@Typed99 s;

CREATE OR REPLACE cq @Appname@cq100
INSERT INTO combinedstream
select s FROM @Appname@Typed100 s;

CREATE OR REPLACE cq @Appname@cq101
INSERT INTO combinedstream
select s FROM @Appname@TypedElse s;*/

CREATE OR REPLACE ROUTER Rs2 INPUT FROM combinedstream s CASE
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_1' THEN ROUTE TO New@Appname@Typed1,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_2' THEN ROUTE TO New@Appname@Typed2,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_3' THEN ROUTE TO New@Appname@Typed3,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_4' THEN ROUTE TO New@Appname@Typed4,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_5' THEN ROUTE TO New@Appname@Typed5,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_6' THEN ROUTE TO New@Appname@Typed6,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_7' THEN ROUTE TO New@Appname@Typed7,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_8' THEN ROUTE TO New@Appname@Typed8,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_9' THEN ROUTE TO New@Appname@Typed9,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_10' THEN ROUTE TO New@Appname@Typed10,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_11' THEN ROUTE TO New@Appname@Typed11,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_12' THEN ROUTE TO New@Appname@Typed12,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_13' THEN ROUTE TO New@Appname@Typed13,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_14' THEN ROUTE TO New@Appname@Typed14,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_15' THEN ROUTE TO New@Appname@Typed15,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_16' THEN ROUTE TO New@Appname@Typed16,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_17' THEN ROUTE TO New@Appname@Typed17,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_18' THEN ROUTE TO New@Appname@Typed18,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_19' THEN ROUTE TO New@Appname@Typed19,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_20' THEN ROUTE TO New@Appname@Typed20,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_21' THEN ROUTE TO New@Appname@Typed21,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_22' THEN ROUTE TO New@Appname@Typed22,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_23' THEN ROUTE TO New@Appname@Typed23,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_24' THEN ROUTE TO New@Appname@Typed24,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_25' THEN ROUTE TO New@Appname@Typed25,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_26' THEN ROUTE TO New@Appname@Typed26,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_27' THEN ROUTE TO New@Appname@Typed27,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_28' THEN ROUTE TO New@Appname@Typed28,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_29' THEN ROUTE TO New@Appname@Typed29,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_30' THEN ROUTE TO New@Appname@Typed30,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_31' THEN ROUTE TO New@Appname@Typed31,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_32' THEN ROUTE TO New@Appname@Typed32,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_33' THEN ROUTE TO New@Appname@Typed33,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_34' THEN ROUTE TO New@Appname@Typed34,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_35' THEN ROUTE TO New@Appname@Typed35,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_36' THEN ROUTE TO New@Appname@Typed36,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_37' THEN ROUTE TO New@Appname@Typed37,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_38' THEN ROUTE TO New@Appname@Typed38,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_39' THEN ROUTE TO New@Appname@Typed39,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_40' THEN ROUTE TO New@Appname@Typed40,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_41' THEN ROUTE TO New@Appname@Typed41,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_42' THEN ROUTE TO New@Appname@Typed42,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_43' THEN ROUTE TO New@Appname@Typed43,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_44' THEN ROUTE TO New@Appname@Typed44,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_45' THEN ROUTE TO New@Appname@Typed45,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_46' THEN ROUTE TO New@Appname@Typed46,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_47' THEN ROUTE TO New@Appname@Typed47,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_48' THEN ROUTE TO New@Appname@Typed48,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_49' THEN ROUTE TO New@Appname@Typed49,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_50' THEN ROUTE TO New@Appname@Typed50,
/*WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_51' THEN ROUTE TO New@Appname@Typed51,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_52' THEN ROUTE TO New@Appname@Typed52,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_53' THEN ROUTE TO New@Appname@Typed53,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_54' THEN ROUTE TO New@Appname@Typed54,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_55' THEN ROUTE TO New@Appname@Typed55,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_56' THEN ROUTE TO New@Appname@Typed56,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_57' THEN ROUTE TO New@Appname@Typed57,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_58' THEN ROUTE TO New@Appname@Typed58,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_59' THEN ROUTE TO New@Appname@Typed59,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_60' THEN ROUTE TO New@Appname@Typed60,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_61' THEN ROUTE TO New@Appname@Typed61,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_62' THEN ROUTE TO New@Appname@Typed62,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_63' THEN ROUTE TO New@Appname@Typed63,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_64' THEN ROUTE TO New@Appname@Typed64,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_65' THEN ROUTE TO New@Appname@Typed65,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_66' THEN ROUTE TO New@Appname@Typed66,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_67' THEN ROUTE TO New@Appname@Typed67,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_68' THEN ROUTE TO New@Appname@Typed68,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_69' THEN ROUTE TO New@Appname@Typed69,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_70' THEN ROUTE TO New@Appname@Typed70,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_71' THEN ROUTE TO New@Appname@Typed71,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_72' THEN ROUTE TO New@Appname@Typed72,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_73' THEN ROUTE TO New@Appname@Typed73,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_74' THEN ROUTE TO New@Appname@Typed74,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_75' THEN ROUTE TO New@Appname@Typed75,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_76' THEN ROUTE TO New@Appname@Typed76,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_77' THEN ROUTE TO New@Appname@Typed77,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_78' THEN ROUTE TO New@Appname@Typed78,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_79' THEN ROUTE TO New@Appname@Typed79,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_80' THEN ROUTE TO New@Appname@Typed80,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_81' THEN ROUTE TO New@Appname@Typed81,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_82' THEN ROUTE TO New@Appname@Typed82,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_83' THEN ROUTE TO New@Appname@Typed83,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_84' THEN ROUTE TO New@Appname@Typed84,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_85' THEN ROUTE TO New@Appname@Typed85,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_86' THEN ROUTE TO New@Appname@Typed86,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_87' THEN ROUTE TO New@Appname@Typed87,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_88' THEN ROUTE TO New@Appname@Typed88,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_89' THEN ROUTE TO New@Appname@Typed89,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_90' THEN ROUTE TO New@Appname@Typed90,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_91' THEN ROUTE TO New@Appname@Typed91,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_92' THEN ROUTE TO New@Appname@Typed92,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_93' THEN ROUTE TO New@Appname@Typed93,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_94' THEN ROUTE TO New@Appname@Typed94,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_95' THEN ROUTE TO New@Appname@Typed95,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_96' THEN ROUTE TO New@Appname@Typed96,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_97' THEN ROUTE TO New@Appname@Typed97,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_98' THEN ROUTE TO New@Appname@Typed98,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_99' THEN ROUTE TO New@Appname@Typed99,
WHEN meta(s,"TableName").toString()='waction.TABLE_TEST_1000100' THEN ROUTE TO New@Appname@Typed100,*/
ELSE ROUTE TO New@Appname@TypedElse;

CREATE OR REPLACE cq New@Appname@cq1
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed1 s;

CREATE OR REPLACE cq New@Appname@cq2
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed2 s;

CREATE OR REPLACE cq New@Appname@cq3
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed3 s;

CREATE OR REPLACE cq New@Appname@cq4
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed4 s;

CREATE OR REPLACE cq New@Appname@cq5
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed5 s;

CREATE OR REPLACE cq New@Appname@cq6
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed6 s;

CREATE OR REPLACE cq New@Appname@cq7
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed7 s;

CREATE OR REPLACE cq New@Appname@cq8
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed8 s;

CREATE OR REPLACE cq New@Appname@cq9
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed9 s;

CREATE OR REPLACE cq New@Appname@cq10
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed10 s;


CREATE OR REPLACE cq New@Appname@cq11
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed11 s;

CREATE OR REPLACE cq New@Appname@cq12
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed12 s;

CREATE OR REPLACE cq New@Appname@cq13
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed13 s;

CREATE OR REPLACE cq New@Appname@cq14
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed14 s;

CREATE OR REPLACE cq New@Appname@cq15
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed15 s;

CREATE OR REPLACE cq New@Appname@cq16
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed16 s;

CREATE OR REPLACE cq New@Appname@cq117
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed17 s;

CREATE OR REPLACE cq New@Appname@cq18
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed18 s;

CREATE OR REPLACE cq New@Appname@cq19
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed19 s;

CREATE OR REPLACE cq New@Appname@cq20
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed20 s;

CREATE OR REPLACE cq New@Appname@cq21
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed21 s;

CREATE OR REPLACE cq New@Appname@cq22
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed22 s;

CREATE OR REPLACE cq New@Appname@cq23
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed23 s;

CREATE OR REPLACE cq New@Appname@cq24
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed24 s;

CREATE OR REPLACE cq New@Appname@cq25
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed25 s;

CREATE OR REPLACE cq New@Appname@cq26
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed26 s;

CREATE OR REPLACE cq New@Appname@cq27
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed27 s;

CREATE OR REPLACE cq New@Appname@cq28
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed28 s;

CREATE OR REPLACE cq New@Appname@cq29
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed29 s;

CREATE OR REPLACE cq New@Appname@cq30
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed30 s;

CREATE OR REPLACE cq New@Appname@cq31
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed31 s;

CREATE OR REPLACE cq New@Appname@cq32
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed32 s;

CREATE OR REPLACE cq New@Appname@cq33
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed33 s;

CREATE OR REPLACE cq New@Appname@cq34
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed34 s;

CREATE OR REPLACE cq New@Appname@cq35
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed35 s;

CREATE OR REPLACE cq New@Appname@cq36
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed36 s;

CREATE OR REPLACE cq New@Appname@cq37
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed37 s;

CREATE OR REPLACE cq New@Appname@cq38
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed38 s;

CREATE OR REPLACE cq New@Appname@cq39
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed39 s;

CREATE OR REPLACE cq New@Appname@cq40
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed40 s;

CREATE OR REPLACE cq New@Appname@cq41
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed41 s;

CREATE OR REPLACE cq New@Appname@cq42
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed42 s;

CREATE OR REPLACE cq New@Appname@cq43
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed43 s;

CREATE OR REPLACE cq New@Appname@cq44
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed44 s;

CREATE OR REPLACE cq New@Appname@cq45
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed45 s;

CREATE OR REPLACE cq New@Appname@cq46
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed46 s;

CREATE OR REPLACE cq New@Appname@cq47
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed47 s;

CREATE OR REPLACE cq New@Appname@cq48
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed48 s;

CREATE OR REPLACE cq New@Appname@cq49
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed49 s;

CREATE OR REPLACE cq New@Appname@cq50
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed50 s;
/*
CREATE OR REPLACE cq New@Appname@cq51
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed51 s;

CREATE OR REPLACE cq New@Appname@cq52
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed52 s;

CREATE OR REPLACE cq New@Appname@cq53
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed53 s;

CREATE OR REPLACE cq New@Appname@cq54
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed54 s;

CREATE OR REPLACE cq New@Appname@cq55
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed55 s;

CREATE OR REPLACE cq New@Appname@cq56
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed56 s;

CREATE OR REPLACE cq New@Appname@cq57
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed57 s;

CREATE OR REPLACE cq New@Appname@cq58
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed58 s;

CREATE OR REPLACE cq New@Appname@cq59
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed59 s;

CREATE OR REPLACE cq New@Appname@cq60
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed60 s;

CREATE OR REPLACE cq New@Appname@cq61
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed61 s;

CREATE OR REPLACE cq New@Appname@cq62
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed62 s;

CREATE OR REPLACE cq New@Appname@cq63
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed63 s;

CREATE OR REPLACE cq New@Appname@cq64
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed64 s;

CREATE OR REPLACE cq New@Appname@cq65
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed65 s;

CREATE OR REPLACE cq New@Appname@cq66
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed66 s;

CREATE OR REPLACE cq New@Appname@cq67
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed67 s;

CREATE OR REPLACE cq New@Appname@cq68
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed68 s;

CREATE OR REPLACE cq New@Appname@cq69
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed69 s;

CREATE OR REPLACE cq New@Appname@cq70
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed70 s;

CREATE OR REPLACE cq New@Appname@cq71
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed71 s;

CREATE OR REPLACE cq New@Appname@cq72
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed72 s;

CREATE OR REPLACE cq New@Appname@cq73
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed73 s;

CREATE OR REPLACE cq New@Appname@cq74
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed74 s;

CREATE OR REPLACE cq New@Appname@cq75
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed75 s;

CREATE OR REPLACE cq New@Appname@cq76
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed76 s;

CREATE OR REPLACE cq New@Appname@cq77
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed77 s;

CREATE OR REPLACE cq New@Appname@cq78
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed78 s;

CREATE OR REPLACE cq New@Appname@cq79
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed79 s;

CREATE OR REPLACE cq New@Appname@cq80
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed80 s;

CREATE OR REPLACE cq New@Appname@cq81
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed81 s;

CREATE OR REPLACE cq New@Appname@cq82
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed82 s;

CREATE OR REPLACE cq New@Appname@cq83
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed83 s;

CREATE OR REPLACE cq New@Appname@cq84
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed84 s;

CREATE OR REPLACE cq New@Appname@cq85
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed85 s;

CREATE OR REPLACE cq New@Appname@cq86
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed86 s;

CREATE OR REPLACE cq New@Appname@cq87
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed87 s;

CREATE OR REPLACE cq New@Appname@cq88
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed88 s;

CREATE OR REPLACE cq New@Appname@cq89
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed89 s;

CREATE OR REPLACE cq New@Appname@cq90
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed90 s;

CREATE OR REPLACE cq New@Appname@cq91
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed91 s;

CREATE OR REPLACE cq New@Appname@cq92
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed92 s;

CREATE OR REPLACE cq New@Appname@cq93
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed93 s;

CREATE OR REPLACE cq New@Appname@cq94
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed94 s;

CREATE OR REPLACE cq New@Appname@cq95
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed95 s;

CREATE OR REPLACE cq New@Appname@cq96
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed96 s;

CREATE OR REPLACE cq New@Appname@cq97
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed97 s;

CREATE OR REPLACE cq New@Appname@cq98
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed98 s;

CREATE OR REPLACE cq New@Appname@cq99
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed99 s;

CREATE OR REPLACE cq New@Appname@cq100
INSERT INTO Newcombinedstream
select s FROM New@Appname@Typed100 s;

CREATE OR REPLACE cq New@Appname@cq101
INSERT INTO Newcombinedstream
select s FROM New@Appname@TypedElse s;*/

create target systar using sysout(name:'out')INPUT FROM Newcombinedstream;

CREATE OR REPLACE TARGET @Appname@Target USING DatabaseWriter (
ConnectionURL:'jdbc:mysql://162.222.179.3:3306/waction',
Username:'root',
Password:'w@ct10n',
Tables:'waction.TABLE_TEST_%;waction.RESULTTABLE',
IgnorableExceptioncode : 'NO_OP_DELETE,NO_OP_UPDATE,NO_OP_PKUPDATE',
CommitPolicy:'Eventcount:1000,Interval:3600',
BatchPolicy:'eventCount:1000,Interval:3600'
)
INPUT FROM Newcombinedstream;

end application @Appname@;
deploy application @Appname@;
start application @Appname@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;
CREATE  SOURCE @TableSourceName@ USING DatabaseReader  ( 
  ConnectionURL: '@SourceConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  Tables: '@SourceTable@',
  ReplicationSlotName: 'null'
 ) OUTPUT TO @SRCTableINPUTSTREAM@;

 CREATE  SOURCE @QuerySourceName@ USING DatabaseReader  ( 
  ConnectionURL: '@SourceConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  Query: '@SourceQuery@',
  ReplicationSlotName: 'null'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@;

CREATE  TARGET @TabletargetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
 ) 
 INPUT FROM @SRCTableINPUTSTREAM@;

 CREATE  TARGET @QuerytargetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@ recovery 1 second interval;

CREATE SOURCE @parquetsrc@ USING FileReader (
  directory: '',
  positionByEOF: false,
  WildCard: '' )
PARSE USING ParquetParser (
 )
OUTPUT TO @appname@Streams;

CREATE OR REPLACE TARGET @kafkatarget@ USING Global.KafkaWriter VERSION @KAFKAVERSION@(
     brokerAddress: '',
     Topic: '',
     KafkaConfigValueSeparator: '=',
     MessageKey: '',
     MessageHeader: '',
     KafkaConfigPropertySeparator: ';',
     Mode: 'Sync',
     KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000' )
format using AvroFormatter (
formatAs: 'Default',
  schemaregistryurl: 'http://localhost:8081/',
  SchemaRegistrySubjectName: '',
  formatterName: 'AvroFormatter',
  schemaregistryConfiguration: ''
)
INPUT FROM @appname@Streams;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @srcName@ USING OracleReader (
 Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@'
)
OUTPUT TO @outstreamname@;

CREATE OR REPLACE TARGET @tgtName@ USING DatabaseWriter
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'@tgtusername@',
  BatchPolicy:'EventCount:1,Interval:0',
  CommitPolicy:'EventCount:1,Interval:0',
  ConnectionURL:'@tgturl@',
  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
  Password:'@tgtpassword@'
)
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @dsvsrc@ USING FileReader ( 
  directory: '', 
  wildcard: '', 
  positionbyeof: false ) 
PARSE USING DSVParser ()
OUTPUT TO @appname@OUT;

CREATE TARGET @prqttrgt@ USING FileWriter ( 
  filename: '', 
  directory: '', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING ParquetFormatter  ( 
  schemaFileName: '',
  compressiontype: 'GZIP',
  members:'data')
INPUT FROM @appname@OUT;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;
Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream; 


create Target FileTarget using FileWriter(
    rolloverpolicy:'@UPLOAD-SIZE@',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using AvroFormatter (
schemafilename:'@charset@',
formatAs:'@mem@',
schemaregistryurl:'@head@'

)
input from TypedCSVStream;

end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

Stop application DEV21349.app1;
Undeploy application DEV21349.app1;
Drop application DEV21349.app1 cascade;
Drop namespace DEV21349 cascade;
Create namespace DEV21349;
Use DEV21349;

CREATE APPLICATION app1;

CREATE CQ cq1 INSERT INTO cq1_outputstream select * from global.exceptionsstream exStream;

CREATE WACTIONSTORE exceptionsWS CONTEXT OF cq1_outputstream_Type @PERSIST-TYPE@;

CREATE CQ cq2 INSERT INTO exceptionsWS SELECT * FROM cq1_outputstream c;

CREATE OR REPLACE CQ readingWS_cq INSERT INTO outputstream2 select * from exceptionsWS;

CREATE OR REPLACE TARGET ExTarget USING Global.FileWriter  (
DataEncryptionKeyPassphrase: '',
  filename: 'ExceptionsOutPutFile',
  flushpolicy: 'EventCount:10000,Interval:30s',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  directory: '@TEST-DATA-PATH@',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  ( jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n' )
INPUT FROM outputstream2;

END APPLICATION app1;

create namespace ns1;
create namespace ns2;

use ns1;

create application group group1;

CREATE or REPLACE  APPLICATION testgroupapp1 RECOVERY 15 SECOND INTERVAL;
end application testgroupapp1;

CREATE or REPLACE  APPLICATION testgroupapp2 RECOVERY 15 SECOND INTERVAL;
end application testgroupapp2;

use ns2;
create application group group2;

CREATE or REPLACE  APPLICATION testgroupapp1 RECOVERY 15 SECOND INTERVAL;
end application testgroupapp1;

CREATE or REPLACE  APPLICATION testgroupapp2 RECOVERY 15 SECOND INTERVAL;
end application testgroupapp2;

alter application group group1 add ns1.testgroupapp1,testgroupapp1;

use ns1;
alter application group group2 add testgroupapp2,ns2.testgroupapp2;

use admin;

create user PosTester identified by PosTester;

drop role PosTester.enduser; drop role PosTester.useradmin;
drop role PosTester.dev; drop user PosTester cascade;
drop namespace PosTester;
create user PosTester identified by PosTester;
drop role PosTester.enduser; drop role PosTester.useradmin;
drop role PosTester.dev; drop user PosTester cascade;
drop namespace PosTester;

CREATE SOURCE @SOURCE_NAME@ USING Global.incREmEnTALBatchrEADer (
  StartPosition: '@startPosition@',
  ConnectionURL: '@sourceURL@',
  Username: '@userName@',
  Tables: '@tables@',
  CheckColumn: '@checkColum@',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  Password: '@password@',
  Password_encrypted: 'false',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

stop application DualGen;
undeploy application DualGen;
drop application DualGen cascade;
CREATE APPLICATION DualGen;

CREATE OR REPLACE TYPE DualEvent (
    Dummy DateTime,
    PhoneNo java.lang.String
);

CREATE OR REPLACE STREAM DualEvents OF DualEvent;

CREATE OR REPLACE CQ GenDual 
INSERT INTO DualEvents
SELECT
  TO_DATEF('28-FEB-22',"dd-MMM-yy"),maskPhoneNumber('44 844 493 0787', "(\\\\d{0,4}\\\\s)(\\\\d{0,4}\\\\s)([0-9 ]+)", 1, 2)  as Dummy
FROM
   heartbeat(interval @INTERVAL@ second) h;


CREATE OR REPLACE TARGET DualSys USING SysOut  ( 
  name: 'heartbeat_out'
 ) 
INPUT FROM DualEvents;


CREATE TARGET DSVFormatterOut using FileWriter(
 filename:'HeartBeat_Output.log',
 flushpolicy:'EventCount:6',
 rolloverpolicy:'interval:97s')
FORMAT USING DSVFormatter ()
INPUT FROM DualEvents;

END APPLICATION DualGen;
deploy application DualGen;
start application DualGen;

--
-- Recovery Test 26 with two sources, two jumping attribute windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W -> CQ1 -> WS
-- S2 -> Ja6W -> CQ2 -> WS
--

STOP Recov26Tester.RecovTest26;
UNDEPLOY APPLICATION Recov26Tester.RecovTest26;
DROP APPLICATION Recov26Tester.RecovTest26 CASCADE;
CREATE APPLICATION RecovTest26 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION RecovTest26;

--
-- Crash Recovery Test 4 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS
-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS
--

STOP APPLICATION KStreamN2S2CR4Tester.KStreamN2S2CRTest4;
UNDEPLOY APPLICATION KStreamN2S2CR4Tester.KStreamN2S2CRTest4;
DROP APPLICATION KStreamN2S2CR4Tester.KStreamN2S2CRTest4 CASCADE;

DROP USER KStreamN2S2CR4Tester;
DROP NAMESPACE KStreamN2S2CR4Tester CASCADE;
CREATE USER KStreamN2S2CR4Tester IDENTIFIED BY KStreamN2S2CR4Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR4Tester;
CONNECT KStreamN2S2CR4Tester KStreamN2S2CR4Tester;

CREATE APPLICATION KStreamN2S2CRTest4 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest4;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest4 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest4;

CREATE FLOW DataProcessingKStreamN2S2CRTest4;

CREATE TYPE CsvDataKStreamN2S2CRTest4 (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionTypeKStreamN2S2CRTest4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream OF CsvDataKStreamN2S2CRTest4;

CREATE CQ CsvToDataKStreamN2S2CRTest4
INSERT INTO DataStream
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest4 CONTEXT OF WactionTypeKStreamN2S2CRTest4
EVENT TYPES ( CsvDataKStreamN2S2CRTest4 )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO WactionsKStreamN2S2CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO WactionsKStreamN2S2CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END FLOW DataProcessingKStreamN2S2CRTest4;

END APPLICATION KStreamN2S2CRTest4;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ RECOVERY 1 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @SourceName@ USING PostgreSQLReader  ( 
 ReplicationSlotName: 'striim_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src'
 ) 
OUTPUT TO @SRCINPUTSTREAM@ ;


CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy:'EventCount:1000,Interval:60',
CommitPolicy:'EventCount:1000,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM @SRCINPUTSTREAM@;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  (
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: '@CDC_URL@',
  DatabaseName:'@DatabaseName@',
  Tables: '@Source1Tables@' )
OUTPUT TO @APPNAME@cdcStream;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@1 (
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
  FetchSize: 20,
  DatabaseProviderType: 'Default',
  Table: '@Source3Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')
OF @APPNAME@cachetype;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@2 (
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
  FetchSize: 10,
  DatabaseProviderType: 'Default',
  Table: '@Source2Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')
OF @APPNAME@cachetype;

STOP APPLICATION OneAgentMultiSourceCQTester.OneAgentWithMultiSourceCQApp;
UNDEPLOY APPLICATION OneAgentMultiSourceCQTester.OneAgentWithMultiSourceCQApp;
DROP APPLICATION OneAgentMultiSourceCQTester.OneAgentWithMultiSourceCQApp cascade;

create Application OneAgentWithMultiSourceCQApp;
CREATE FLOW AgentFlow;

create source XMLSource using FileReader (
  Directory:'@TEST-DATA-PATH@',
  WildCard:'books.xml',
  positionByEOF:false
)
parse using XMLParser (
  RootNode:'/catalog/book',
  columnlist:'book/@id,book/author,book/title,book/genre,book/price,book/publish_date,book/description'
)
OUTPUT TO XmlStream;

CREATE TYPE MyTypeXml(
id String KEY,
author String,
title String,
genre String,
price String,
publish_date String,
description String
);

CREATE STREAM TypedStreamXml of MyTypeXml;

CREATE CQ TypeConversionCQXml
INSERT INTO TypedStreamXml
SELECT
data[0],
data[1],
data[2],
data[3],
data[4],
data[5],
data[6]
from XmlStream;

create source DSVCSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'customerdetails-agent.csv',
  charset: 'UTF-8',
  positionByEOF:false
)
parse using DSVParser (
  header:'yes',
  minexpectedcolumns:'8'
)
OUTPUT TO DSVCsvStream;

CREATE TYPE MyTypeCsv(
PAN String,
FNAME String KEY,
LNAME String,
ADDRESS String,
CITY String,
STATE String,
ZIP String,
GENDER String
);

CREATE STREAM TypedStreamCsv of MyTypeCsv;

CREATE CQ TypeConversionCQCsv
INSERT INTO TypedStreamCsv
SELECT
data[0],
data[1],
data[2],
data[3],
data[4],
data[5],
data[6],
data[7]
from DSVCsvStream;

-- Read from File

create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'StoreNames.csv',
  columndelimiter:',',
  positionByEOF:false
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CSVStream;

CREATE TYPE MyType (
Store_Id String KEY,
Store_Name String
);

CREATE STREAM TypedStream of MyType;

CREATE CQ TypeConversionCQ
INSERT INTO TypedStream
SELECT data[0], data[1]
from CsvStream;

END FLOW AgentFlow;

CREATE FLOW ServerFlow;

CREATE WACTIONSTORE StoreInfo CONTEXT OF MyType
EVENT TYPES ( MyType )
@PERSIST-TYPE@

CREATE CQ StoreWaction
INSERT INTO StoreInfo
SELECT * FROM TypedStream
LINK SOURCE EVENT;

CREATE WACTIONSTORE StoreInfoXml CONTEXT OF MyTypeXml
EVENT TYPES ( MyTypeXml )
@PERSIST-TYPE@

CREATE CQ StoreWactionXml
INSERT INTO StoreInfoXml
SELECT * FROM TypedStreamXml
LINK SOURCE EVENT;

CREATE WACTIONSTORE StoreInfoCsv CONTEXT OF MyTypeCsv
EVENT TYPES ( MyTypeCsv )
@PERSIST-TYPE@

CREATE CQ StoreWactionCsv
INSERT INTO StoreInfoCsv
SELECT * FROM TypedStreamCsv
LINK SOURCE EVENT;


END FLOW ServerFlow;
end Application OneAgentWithMultiSourceCQApp;

DEPLOY APPLICATION OneAgentWithMultiSourceCQApp with AgentFlow in AGENTS, ServerFlow on any in default;
start OneAgentWithMultiSourceCQApp;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW ;

 

CREATE OR REPLACE SOURCE DBSource USING DatabaseReader  ( 
  Username: '@SOURCE_USER@',
  Password_encrypted: false,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SOURCE_PASS@'
 ) 
OUTPUT TO Oracle_ChangeDataStream;


CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1000,Interval:0',
  CommitPolicy: 'EventCount:1000,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

stop application GCSWriterTest;
undeploy application GCSWriterTest;
drop application GCSWriterTest cascade;
create application GCSWriterTest recovery 1 second interval;
create source GCS_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

create Target GCSTarget using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@',
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from CsvStream;

end application GCSWriterTest;
deploy application GCSWriterTest on all in default;
start application GCSWriterTest;

STOP APPLICATION testApp;
UNDEPLOY APPLICATION testApp;
DROP APPLICATION testApp CASCADE;
-- DROP EXCEPTIONSTORE testApp_exceptionstore;

CREATE APPLICATION testApp WITH ENCRYPTION RECOVERY 10 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE OR REPLACE SOURCE testApp_Source USING GGTrailReader ( 

  SupportColumnCharset: false, 
  TrailFilePattern: 'd1*', 
  DefinitionFile: '/Users/gopinaths/Product/IntegrationTests/TestData/OGG/alldatatype/alldtype.def', 
  Compression: false, 
  TrailDirectory: '/Users/gopinaths/Product/IntegrationTests/TestData/OGG/alldatatype', 
  Tables: 'QATEST.ALLDTYPE', 
  FilterTransactionBoundaries: true, 
  ExcludedTables:'waction.CHKPOINT',
  TrailByteOrder: 'LittleEndian' ) 
OUTPUT TO testApp_Stream;

CREATE OR REPLACE TARGET testApp_Target USING BigQueryWriter  (
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  projectId:'striimqa-214712',
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:5,Interval:120',
  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', 
  AllowQuotedNewLines: 'false', 
  adapterName: 'BigQueryWriter', 
  optimizedMerge: 'true', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', 
  StandardSQL: 'true', 
  QuoteCharacter: '\"', 
  Tables: 'QATEST.ALLDTYPE,.oratobqtgt',
  Mode: 'MERGE',
  StandardSQL: 'true',
  QuoteCharacter: '\"'
  ) INPUT FROM testApp_Stream;

CREATE OR REPLACE TARGET testApp_SysOut USING Global.SysOut (name: 'wa') INPUT FROM testApp_Stream;

END APPLICATION testApp;
DEPLOY APPLICATION testApp;
START testApp;

create or replace type @STREAM@details(
C_CUSTKEY int,
C_MKTSEGMENT String,
C_NATIONKEY int,
C_NAME String,
C_ADDRESS String,
C_PHONE String,
C_ACCTBAL int,
C_COMMENT String
);

create or replace stream @STREAM@_TYPED of @STREAM@details;

Create or replace CQ @STREAM@detailsCQ
insert into @STREAM@_TYPED
select 
to_int(data[0]),data[1],to_int(data[2]),data[3],data[4],data[5],to_int(data[6]),data[7]
from @STREAM@;

CREATE WINDOW @STREAM@_DBRWindow
OVER @STREAM@_TYPED
KEEP 1000 ROWS;

create or replace stream @STREAM@_TYPED2 of @STREAM@details;

Create or replace CQ @STREAM@detailsCQ2
insert into @STREAM@_TYPED2
select 
to_int(C_CUSTKEY),C_MKTSEGMENT,to_int(C_NATIONKEY),C_NAME,C_ADDRESS,C_PHONE,to_int(C_ACCTBAL),C_COMMENT
from @STREAM@_DBRWindow;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING FileWriter  ( 
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
 ) Format using DSVFormatter()
INPUT FROM @STREAM@_DBRWindow;

stop application app1PS;
undeploy application app1PS;
drop application app1PS cascade;

create application app1PS;

create target File_TargerPS using FileWriter
(
directory : '',
filename : ''
)
format using DSVFormatter()
input from KPSRss1;

end application app1PS;

deploy application app1PS;
start application app1PS;

stop application MSSQLTransactionSupportAutoDisableCdcTrue;
undeploy application MSSQLTransactionSupportAutoDisableCdcTrue;
drop application MSSQLTransactionSupportAutoDisableCdcTrue cascade;

CREATE APPLICATION MSSQLTransactionSupportAutoDisableCdcTrue recovery 1 second interval;

Create Source ReadFromMSSQL3
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
AutoDisableTableCDC:'true',
FetchTransactionMetadata:'true',
FilterTransactionBoundaries: false,
Compression:'false',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportAutoDisableCdcTrueStream;


CREATE TARGET WriteToMSSQL3 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC,@@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportAutoDisableCdcTrueStream;

CREATE TARGET MSSqlReaderOutput3 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportAutoDisableCdcTrueStream; 


CREATE OR REPLACE TARGET MSSQLFileOut3 USING FileWriter  ( 
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  directory:'@FEATURE-DIR@/logs/',
  filename: 'TransactionSupportAutoDisableTableCdcTrue.txt'
 ) 
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 ) 
INPUT FROM MSSQLTransactionSupportAutoDisableCdcTrueStream;

END APPLICATION MSSQLTransactionSupportAutoDisableCdcTrue;
deploy application MSSQLTransactionSupportAutoDisableCdcTrue;
start application MSSQLTransactionSupportAutoDisableCdcTrue;

STOP bq;
UNDEPLOY APPLICATION bq;
DROP APPLICATION bq CASCADE;

CREATE APPLICATION bq RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE DEV_22964_source USING MSSqlReader
(
	Username: 'qatest',
	Password: 'w3b@ct10n',
	ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
	Tables: 'QATEST.BitToBoolean',
	FetchTransactionMetadata: true, 
	FetchSize: '1'
)
OUTPUT TO SS;


CREATE or replace TARGET DEV_22964_target USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
    Tables:'QATEST.TABLE_TEST_1000001,qatest.% keycolumns(RONUM)',
    mode:'Appendonly',
    datalocation: 'US',
	nullmarker: 'NULL',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:100,Interval:10'	
) INPUT FROM ss;

END APPLICATION bq;
DEPLOY APPLICATION bq;
START APPLICATION bq;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@ recovery 1 second interval;

CREATE SOURCE @parquetsrc@ USING FileReader (
  directory: '',
  positionByEOF: false,
  WildCard: '' )
PARSE USING ParquetParser (
 )
OUTPUT TO @appname@Streams;

CREATE OR REPLACE CQ @appname@CQOrder3
INSERT INTO @appname@Stream3
SELECT
PUTUSERDATA(s,'schemaName',s.data.getSchema().getName())
FROM @appname@Streams s;

CREATE TARGET @adlstarget@ USING Global.ADLSGen2Writer (
    accountname:'',
  	sastoken:'',
  	filesystemname:'',
  	filename:'',
  	directory:'',
  	uploadpolicy:'eventcount:10' )

format using AvroFormatter (
)
INPUT FROM @appname@Stream3;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

--
-- Crash Recovery Test 2 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS
--

STOP APPLICATION N2S2CR2Tester.N2S2CRTest2;
UNDEPLOY APPLICATION N2S2CR2Tester.N2S2CRTest2;
DROP APPLICATION N2S2CR2Tester.N2S2CRTest2 CASCADE;
CREATE APPLICATION N2S2CRTest2 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest2;

CREATE SOURCE CsvSourceN2S2CRTest2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN2S2CRTest2;

CREATE FLOW DataProcessingN2S2CRTest2;

CREATE TYPE WactionTypeN2S2CRTest2 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionTypeN2S2CRTest2;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN2S2CRTest2 CONTEXT OF WactionTypeN2S2CRTest2
EVENT TYPES ( WactionTypeN2S2CRTest2 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN2S2CRTest2
INSERT INTO WactionsN2S2CRTest2
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN2S2CRTest2;

END APPLICATION N2S2CRTest2;

stop @appname@;
undeploy application @appname@;
DROP APPLICATION @appname@ CASCADE;
CREATE APPLICATION @appname@;

CREATE SOURCE @appname@_src USING databaseReader  (
  Username: '@@',
  Password: '@@',
  ConnectionURL: '@@',
  Tables: '@@',
  FetchSize: '100'
 )
OUTPUT TO @appname@_ss;

--CREATE JUMPING WINDOW @appname@_win OVER @appname@_ss KEEP @winsize@ ROWS;

CREATE TYPE @appname@_MapType
    (   
       id INTEGER,
        name STRING,
        city  STRING
    );
    
CREATE EXTERNAL CACHE @appname@_cach (
  AdapterName: 'DatabaseReader',
    ConnectionURL: '@url@',
    UserName: '@uname@',
    Password: '@pwd@',
   Table: '@tablename@',
  FetchSize: 100,
  Columns: 'id,name,city',
  trimquote: false,
  KeyToMap: '@key@'
 )
 OF @appname@_MapType;
 
CREATE TYPE @appname@_MapTypenew
    (   id_t            INTEGER,
        name_t           STRING,
        city_t            STRING,
        id_c            INTEGER,
        name_c            STRING,
        city_c            STRING
    );
    
CREATE STREAM @appname@_JoinedData OF @appname@_MapTypenew;

CREATE CQ @appname@_JoinDataCQ
INSERT INTO @appname@_JoinedData
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_STRING(f.data[2]),
        z.id,
        z.name,
        z.city
FROM @appname@_ss f, @appname@_cach z
where TO_INT(f.data[0]) = z.id
@Ex@;

CREATE TARGET @appname@_tgt USING DatabaseWriter
(
  ConnectionURL:'@@',
  Username:'@@',
  Password:'@@',
  BatchPolicy:'Eventcount:10000,Interval:1',
  CommitPolicy:'Interval:1,Eventcount:10000',
  Tables:'@@'
) 
INPUT FROM @appname@_JoinedData;

END APPLICATION @appname@;
deploy application @appname@;
start @appname@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.e1ptest%',
	FetchSize: '1'
)
OUTPUT TO @APPNAME@_SS;


CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.e1ptest%,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12)',
Mode:'merge',
optimizedMerge: 'true',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:2',
StandardSQL:true		
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING FileReader  (
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '/Users/jenniffer/Downloads',
  skipbom: true,
  wildcard: 'dk000000000'
 )
 PARSE USING GGTrailParser  (
  handler: 'com.webaction.proc.GGTrailParser_1_0',
  metadata: '@METADATA@',
  FilterTransactionBoundaries: true,
  TrailByteOrder: '@BYTEORDDER@',
  Tables: '@TABLES@',
  parserName: 'GGTrailParser',
  _h_ReturnDateTimeAs: '@DATETIME@',
  Compression:'@COMPRESSION@'
 )OUTPUT to @STREAM@;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW ;

CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: 'QATEST.orac_1000COL',
  adapterName: 'OracleReader',
  Password: 'qatest',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'qatest',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:60',
  CommitPolicy: 'EventCount:1,Interval:60',
  ConnectionURL: '@TARGET_URL@',
  Tables: 'QATEST.orac_1000COL,test.cassandra_1500col columnmap(field1=f1,field2=f2,field3=f3,field4=f4,field5=f5,field6=f6,field7=f7,field8=f8,field9=f9,field10=f10,field11=f11,field12=f12,field13=f13,field14=f14,field15=f15,field16=f16,field17=f17,field18=f18,field19=f19,field20=f20,field21=f21,field22=f22,field23=f23,field24=f24,field25=f25,field26=f26,field27=f27,field28=f28,field29=f29,field30=f30,field31=f31,field32=f32,field33=f33,field34=f34,field35=f35,field36=f36,field37=f37,field38=f38,field39=f39,field40=f40,field41=f41,field42=f42,field43=f43,field44=f44,field45=f45,field46=f46,field47=f47,field48=f48,field49=f49,field50=f50,field51=f51,field52=f52,field53=f53,field54=f54,field55=f55,field56=f56,field57=f57,field58=f58,field59=f59,field60=f60,field61=f61,field62=f62,field63=f63,field64=f64,field65=f65,field66=f66,field67=f67,field68=f68,field69=f69,field70=f70,field71=f71,field72=f72,field73=f73,field74=f74,field75=f75,field76=f76,field77=f77,field78=f78,field79=f79,field80=f80,field81=f81,field82=f82,field83=f83,field84=f84,field85=f85,field86=f86,field87=f87,field88=f88,field89=f89,field90=f90,field91=f91,field92=f92,field93=f93,field94=f94,field95=f95,field96=f96,field97=f97,field98=f98,field99=f99,field100=f100,field101=f101,field102=f102,field103=f103,field104=f104,field105=f105,field106=f106,field107=f107,field108=f108,field109=f109,field110=f110,field111=f111,field112=f112,field113=f113,field114=f114,field115=f115,field116=f116,field117=f117,field118=f118,field119=f119,field120=f120,field121=f121,field122=f122,field123=f123,field124=f124,field125=f125,field126=f126,field127=f127,field128=f128,field129=f129,field130=f130,field131=f131,field132=f132,field133=f133,field134=f134,field135=f135,field136=f136,field137=f137,field138=f138,field139=f139,field140=f140,field141=f141,field142=f142,field143=f143,field144=f144,field145=f145,field146=f146,field147=f147,field148=f148,field149=f149,field150=f150,field151=f151,field152=f152,field153=f153,field154=f154,field155=f155,field156=f156,field157=f157,field158=f158,field159=f159,field160=f160,field161=f161,field162=f162,field163=f163,field164=f164,field165=f165,field166=f166,field167=f167,field168=f168,field169=f169,field170=f170,field171=f171,field172=f172,field173=f173,field174=f174,field175=f175,field176=f176,field177=f177,field178=f178,field179=f179,field180=f180,field181=f181,field182=f182,field183=f183,field184=f184,field185=f185,field186=f186,field187=f187,field188=f188,field189=f189,field190=f190,field191=f191,field192=f192,field193=f193,field194=f194,field195=f195,field196=f196,field197=f197,field198=f198,field199=f199,field200=f200,field201=f201,field202=f202,field203=f203,field204=f204,field205=f205,field206=f206,field207=f207,field208=f208,field209=f209,field210=f210,field211=f211,field212=f212,field213=f213,field214=f214,field215=f215,field216=f216,field217=f217,field218=f218,field219=f219,field220=f220,field221=f221,field222=f222,field223=f223,field224=f224,field225=f225,field226=f226,field227=f227,field228=f228,field229=f229,field230=f230,field231=f231,field232=f232,field233=f233,field234=f234,field235=f235,field236=f236,field237=f237,field238=f238,field239=f239,field240=f240,field241=f241,field242=f242,field243=f243,field244=f244,field245=f245,field246=f246,field247=f247,field248=f248,field249=f249,field250=f250,field251=f251,field252=f252,field253=f253,field254=f254,field255=f255,field256=f256,field257=f257,field258=f258,field259=f259,field260=f260,field261=f261,field262=f262,field263=f263,field264=f264,field265=f265,field266=f266,field267=f267,field268=f268,field269=f269,field270=f270,field271=f271,field272=f272,field273=f273,field274=f274,field275=f275,field276=f276,field277=f277,field278=f278,field279=f279,field280=f280,field281=f281,field282=f282,field283=f283,field284=f284,field285=f285,field286=f286,field287=f287,field288=f288,field289=f289,field290=f290,field291=f291,field292=f292,field293=f293,field294=f294,field295=f295,field296=f296,field297=f297,field298=f298,field299=f299,field300=f300,field301=f301,field302=f302,field303=f303,field304=f304,field305=f305,field306=f306,field307=f307,field308=f308,field309=f309,field310=f310,field311=f311,field312=f312,field313=f313,field314=f314,field315=f315,field316=f316,field317=f317,field318=f318,field319=f319,field320=f320,field321=f321,field322=f322,field323=f323,field324=f324,field325=f325,field326=f326,field327=f327,field328=f328,field329=f329,field330=f330,field331=f331,field332=f332,field333=f333,field334=f334,field335=f335,field336=f336,field337=f337,field338=f338,field339=f339,field340=f340,field341=f341,field342=f342,field343=f343,field344=f344,field345=f345,field346=f346,field347=f347,field348=f348,field349=f349,field350=f350,field351=f351,field352=f352,field353=f353,field354=f354,field355=f355,field356=f356,field357=f357,field358=f358,field359=f359,field360=f360,field361=f361,field362=f362,field363=f363,field364=f364,field365=f365,field366=f366,field367=f367,field368=f368,field369=f369,field370=f370,field371=f371,field372=f372,field373=f373,field374=f374,field375=f375,field376=f376,field377=f377,field378=f378,field379=f379,field380=f380,field381=f381,field382=f382,field383=f383,field384=f384,field385=f385,field386=f386,field387=f387,field388=f388,field389=f389,field390=f390,field391=f391,field392=f392,field393=f393,field394=f394,field395=f395,field396=f396,field397=f397,field398=f398,field399=f399,field400=f400,field401=f401,field402=f402,field403=f403,field404=f404,field405=f405,field406=f406,field407=f407,field408=f408,field409=f409,field410=f410,field411=f411,field412=f412,field413=f413,field414=f414,field415=f415,field416=f416,field417=f417,field418=f418,field419=f419,field420=f420,field421=f421,field422=f422,field423=f423,field424=f424,field425=f425,field426=f426,field427=f427,field428=f428,field429=f429,field430=f430,field431=f431,field432=f432,field433=f433,field434=f434,field435=f435,field436=f436,field437=f437,field438=f438,field439=f439,field440=f440,field441=f441,field442=f442,field443=f443,field444=f444,field445=f445,field446=f446,field447=f447,field448=f448,field449=f449,field450=f450,field451=f451,field452=f452,field453=f453,field454=f454,field455=f455,field456=f456,field457=f457,field458=f458,field459=f459,field460=f460,field461=f461,field462=f462,field463=f463,field464=f464,field465=f465,field466=f466,field467=f467,field468=f468,field469=f469,field470=f470,field471=f471,field472=f472,field473=f473,field474=f474,field475=f475,field476=f476,field477=f477,field478=f478,field479=f479,field480=f480,field481=f481,field482=f482,field483=f483,field484=f484,field485=f485,field486=f486,field487=f487,field488=f488,field489=f489,field490=f490,field491=f491,field492=f492,field493=f493,field494=f494,field495=f495,field496=f496,field497=f497,field498=f498,field499=f499,field500=f500,field501=f501,field502=f502,field503=f503,field504=f504,field505=f505,field506=f506,field507=f507,field508=f508,field509=f509,field510=f510,field511=f511,field512=f512,field513=f513,field514=f514,field515=f515,field516=f516,field517=f517,field518=f518,field519=f519,field520=f520,field521=f521,field522=f522,field523=f523,field524=f524,field525=f525,field526=f526,field527=f527,field528=f528,field529=f529,field530=f530,field531=f531,field532=f532,field533=f533,field534=f534,field535=f535,field536=f536,field537=f537,field538=f538,field539=f539,field540=f540,field541=f541,field542=f542,field543=f543,field544=f544,field545=f545,field546=f546,field547=f547,field548=f548,field549=f549,field550=f550,field551=f551,field552=f552,field553=f553,field554=f554,field555=f555,field556=f556,field557=f557,field558=f558,field559=f559,field560=f560,field561=f561,field562=f562,field563=f563,field564=f564,field565=f565,field566=f566,field567=f567,field568=f568,field569=f569,field570=f570,field571=f571,field572=f572,field573=f573,field574=f574,field575=f575,field576=f576,field577=f577,field578=f578,field579=f579,field580=f580,field581=f581,field582=f582,field583=f583,field584=f584,field585=f585,field586=f586,field587=f587,field588=f588,field589=f589,field590=f590,field591=f591,field592=f592,field593=f593,field594=f594,field595=f595,field596=f596,field597=f597,field598=f598,field599=f599,field600=f600,field601=f601,field602=f602,field603=f603,field604=f604,field605=f605,field606=f606,field607=f607,field608=f608,field609=f609,field610=f610,field611=f611,field612=f612,field613=f613,field614=f614,field615=f615,field616=f616,field617=f617,field618=f618,field619=f619,field620=f620,field621=f621,field622=f622,field623=f623,field624=f624,field625=f625,field626=f626,field627=f627,field628=f628,field629=f629,field630=f630,field631=f631,field632=f632,field633=f633,field634=f634,field635=f635,field636=f636,field637=f637,field638=f638,field639=f639,field640=f640,field641=f641,field642=f642,field643=f643,field644=f644,field645=f645,field646=f646,field647=f647,field648=f648,field649=f649,field650=f650,field651=f651,field652=f652,field653=f653,field654=f654,field655=f655,field656=f656,field657=f657,field658=f658,field659=f659,field660=f660,field661=f661,field662=f662,field663=f663,field664=f664,field665=f665,field666=f666,field667=f667,field668=f668,field669=f669,field670=f670,field671=f671,field672=f672,field673=f673,field674=f674,field675=f675,field676=f676,field677=f677,field678=f678,field679=f679,field680=f680,field681=f681,field682=f682,field683=f683,field684=f684,field685=f685,field686=f686,field687=f687,field688=f688,field689=f689,field690=f690,field691=f691,field692=f692,field693=f693,field694=f694,field695=f695,field696=f696,field697=f697,field698=f698,field699=f699,field700=f700,field701=f701,field702=f702,field703=f703,field704=f704,field705=f705,field706=f706,field707=f707,field708=f708,field709=f709,field710=f710,field711=f711,field712=f712,field713=f713,field714=f714,field715=f715,field716=f716,field717=f717,field718=f718,field719=f719,field720=f720,field721=f721,field722=f722,field723=f723,field724=f724,field725=f725,field726=f726,field727=f727,field728=f728,field729=f729,field730=f730,field731=f731,field732=f732,field733=f733,field734=f734,field735=f735,field736=f736,field737=f737,field738=f738,field739=f739,field740=f740,field741=f741,field742=f742,field743=f743,field744=f744,field745=f745,field746=f746,field747=f747,field748=f748,field749=f749,field750=f750,field751=f751,field752=f752,field753=f753,field754=f754,field755=f755,field756=f756,field757=f757,field758=f758,field759=f759,field760=f760,field761=f761,field762=f762,field763=f763,field764=f764,field765=f765,field766=f766,field767=f767,field768=f768,field769=f769,field770=f770,field771=f771,field772=f772,field773=f773,field774=f774,field775=f775,field776=f776,field777=f777,field778=f778,field779=f779,field780=f780,field781=f781,field782=f782,field783=f783,field784=f784,field785=f785,field786=f786,field787=f787,field788=f788,field789=f789,field790=f790,field791=f791,field792=f792,field793=f793,field794=f794,field795=f795,field796=f796,field797=f797,field798=f798,field799=f799,field800=f800,field801=f801,field802=f802,field803=f803,field804=f804,field805=f805,field806=f806,field807=f807,field808=f808,field809=f809,field810=f810,field811=f811,field812=f812,field813=f813,field814=f814,field815=f815,field816=f816,field817=f817,field818=f818,field819=f819,field820=f820,field821=f821,field822=f822,field823=f823,field824=f824,field825=f825,field826=f826,field827=f827,field828=f828,field829=f829,field830=f830,field831=f831,field832=f832,field833=f833,field834=f834,field835=f835,field836=f836,field837=f837,field838=f838,field839=f839,field840=f840,field841=f841,field842=f842,field843=f843,field844=f844,field845=f845,field846=f846,field847=f847,field848=f848,field849=f849,field850=f850,field851=f851,field852=f852,field853=f853,field854=f854,field855=f855,field856=f856,field857=f857,field858=f858,field859=f859,field860=f860,field861=f861,field862=f862,field863=f863,field864=f864,field865=f865,field866=f866,field867=f867,field868=f868,field869=f869,field870=f870,field871=f871,field872=f872,field873=f873,field874=f874,field875=f875,field876=f876,field877=f877,field878=f878,field879=f879,field880=f880,field881=f881,field882=f882,field883=f883,field884=f884,field885=f885,field886=f886,field887=f887,field888=f888,field889=f889,field890=f890,field891=f891,field892=f892,field893=f893,field894=f894,field895=f895,field896=f896,field897=f897,field898=f898,field899=f899,field900=f900,field901=f901,field902=f902,field903=f903,field904=f904,field905=f905,field906=f906,field907=f907,field908=f908,field909=f909,field910=f910,field911=f911,field912=f912,field913=f913,field914=f914,field915=f915,field916=f916,field917=f917,field918=f918,field919=f919,field920=f920,field921=f921,field922=f922,field923=f923,field924=f924,field925=f925,field926=f926,field927=f927,field928=f928,field929=f929,field930=f930,field931=f931,field932=f932,field933=f933,field934=f934,field935=f935,field936=f936,field937=f937,field938=f938,field939=f939,field940=f940,field941=f941,field942=f942,field943=f943,field944=f944,field945=f945,field946=f946,field947=f947,field948=f948,field949=f949,field950=f950,field951=f951,field952=f952,field953=f953,field954=f954,field955=f955,field956=f956,field957=f957,field958=f958,field959=f959,field960=f960,field961=f961,field962=f962,field963=f963,field964=f964,field965=f965,field966=f966,field967=f967,field968=f968,field969=f969,field970=f970,field971=f971,field972=f972,field973=f973,field974=f974,field975=f975,field976=f976,field977=f977,field978=f978,field979=f979,field980=f980,field981=f981,field982=f982,field983=f983,field984=f984,field985=f985,field986=f986,field987=f987,field988=f988,field989=f989,field990=f990,field991=f991,field992=f992,field993=f993,field994=f994,field995=f995,field996=f996,field997=f997,field998=f998,field999=f999,field1000=f1000,field1001=f501,field1002=f2,field1003=f3,field1004=f4,field1005=f5,field1006=f6,field1007=f7,field1008=f8,field1009=f9,field1010=f10,field1011=f11,field1012=f12,field1013=f13,field1014=f14,field1015=f15,field1016=f16,field1017=f17,field1018=f18,field1019=f19,field1020=f20,field1021=f21,field1022=f22,field1023=f23,field1024=f24,field1025=f25,field1026=f26,field1027=f27,field1028=f28,field1029=f29,field1030=f30,field1031=f31,field1032=f32,field1033=f33,field1034=f34,field1035=f35,field1036=f36,field1037=f37,field1038=f38,field1039=f39,field1040=f40,field1041=f41,field1042=f42,field1043=f43,field1044=f44,field1045=f45,field1046=f46,field1047=f47,field1048=f48,field1049=f49,field1050=f50,field1051=f51,field1052=f52,field1053=f53,field1054=f54,field1055=f55,field1056=f56,field1057=f57,field1058=f58,field1059=f59,field1060=f60,field1061=f61,field1062=f62,field1063=f63,field1064=f64,field1065=f65,field1066=f66,field1067=f67,field1068=f68,field1069=f69,field1070=f70,field1071=f71,field1072=f72,field1073=f73,field1074=f74,field1075=f75,field1076=f76,field1077=f77,field1078=f78,field1079=f79,field1080=f80,field1081=f81,field1082=f82,field1083=f83,field1084=f84,field1085=f85,field1086=f86,field1087=f87,field1088=f88,field1089=f89,field1090=f90,field1091=f91,field1092=f92,field1093=f93,field1094=f94,field1095=f95,field1096=f96,field1097=f97,field1098=f98,field1099=f99,field1100=f100,field1101=f101,field1102=f102,field1103=f103,field1104=f104,field1105=f105,field1106=f106,field1107=f107,field1108=f108,field1109=f109,field1110=f110,field1111=f111,field1112=f112,field1113=f113,field1114=f114,field1115=f115,field1116=f116,field1117=f117,field1118=f118,field1119=f119,field1120=f120,field1121=f121,field1122=f122,field1123=f123,field1124=f124,field1125=f125,field1126=f126,field1127=f127,field1128=f128,field1129=f129,field1130=f130,field1131=f131,field1132=f132,field1133=f133,field1134=f134,field1135=f135,field1136=f136,field1137=f137,field1138=f138,field1139=f139,field1140=f140,field1141=f141,field1142=f142,field1143=f143,field1144=f144,field1145=f145,field1146=f146,field1147=f147,field1148=f148,field1149=f149,field1150=f150,field1151=f151,field1152=f152,field1153=f153,field1154=f154,field1155=f155,field1156=f156,field1157=f157,field1158=f158,field1159=f159,field1160=f160,field1161=f161,field1162=f162,field1163=f163,field1164=f164,field1165=f165,field1166=f166,field1167=f167,field1168=f168,field1169=f169,field1170=f170,field1171=f171,field1172=f172,field1173=f173,field1174=f174,field1175=f175,field1176=f176,field1177=f177,field1178=f178,field1179=f179,field1180=f180,field1181=f181,field1182=f182,field1183=f183,field1184=f184,field1185=f185,field1186=f186,field1187=f187,field1188=f188,field1189=f189,field1190=f190,field1191=f191,field1192=f192,field1193=f193,field1194=f194,field1195=f195,field1196=f196,field1197=f197,field1198=f198,field1199=f199,field1200=f200,field1201=f201,field1202=f202,field1203=f203,field1204=f204,field1205=f205,field1206=f206,field1207=f207,field1208=f208,field1209=f209,field1210=f210,field1211=f211,field1212=f212,field1213=f213,field1214=f214,field1215=f215,field1216=f216,field1217=f217,field1218=f218,field1219=f219,field1220=f220,field1221=f221,field1222=f222,field1223=f223,field1224=f224,field1225=f225,field1226=f226,field1227=f227,field1228=f228,field1229=f229,field1230=f230,field1231=f231,field1232=f232,field1233=f233,field1234=f234,field1235=f235,field1236=f236,field1237=f237,field1238=f238,field1239=f239,field1240=f240,field1241=f241,field1242=f242,field1243=f243,field1244=f244,field1245=f245,field1246=f246,field1247=f247,field1248=f248,field1249=f249,field1250=f250,field1251=f251,field1252=f252,field1253=f253,field1254=f254,field1255=f255,field1256=f256,field1257=f257,field1258=f258,field1259=f259,field1260=f260,field1261=f261,field1262=f262,field1263=f263,field1264=f264,field1265=f265,field1266=f266,field1267=f267,field1268=f268,field1269=f269,field1270=f270,field1271=f271,field1272=f272,field1273=f273,field1274=f274,field1275=f275,field1276=f276,field1277=f277,field1278=f278,field1279=f279,field1280=f280,field1281=f281,field1282=f282,field1283=f283,field1284=f284,field1285=f285,field1286=f286,field1287=f287,field1288=f288,field1289=f289,field1290=f290,field1291=f291,field1292=f292,field1293=f293,field1294=f294,field1295=f295,field1296=f296,field1297=f297,field1298=f298,field1299=f299,field1300=f300,field1301=f301,field1302=f302,field1303=f303,field1304=f304,field1305=f305,field1306=f306,field1307=f307,field1308=f308,field1309=f309,field1310=f310,field1311=f311,field1312=f312,field1313=f313,field1314=f314,field1315=f315,field1316=f316,field1317=f317,field1318=f318,field1319=f319,field1320=f320,field1321=f321,field1322=f322,field1323=f323,field1324=f324,field1325=f325,field1326=f326,field1327=f327,field1328=f328,field1329=f329,field1330=f330,field1331=f331,field1332=f332,field1333=f333,field1334=f334,field1335=f335,field1336=f336,field1337=f337,field1338=f338,field1339=f339,field1340=f340,field1341=f341,field1342=f342,field1343=f343,field1344=f344,field1345=f345,field1346=f346,field1347=f347,field1348=f348,field1349=f349,field1350=f350,field1351=f351,field1352=f352,field1353=f353,field1354=f354,field1355=f355,field1356=f356,field1357=f357,field1358=f358,field1359=f359,field1360=f360,field1361=f361,field1362=f362,field1363=f363,field1364=f364,field1365=f365,field1366=f366,field1367=f367,field1368=f368,field1369=f369,field1370=f370,field1371=f371,field1372=f372,field1373=f373,field1374=f374,field1375=f375,field1376=f376,field1377=f377,field1378=f378,field1379=f379,field1380=f380,field1381=f381,field1382=f382,field1383=f383,field1384=f384,field1385=f385,field1386=f386,field1387=f387,field1388=f388,field1389=f389,field1390=f390,field1391=f391,field1392=f392,field1393=f393,field1394=f394,field1395=f395,field1396=f396,field1397=f397,field1398=f398,field1399=f399,field1400=f400,field1401=f401,field1402=f402,field1403=f403,field1404=f404,field1405=f405,field1406=f406,field1407=f407,field1408=f408,field1409=f409,field1410=f410,field1411=f411,field1412=f412,field1413=f413,field1414=f414,field1415=f415,field1416=f416,field1417=f417,field1418=f418,field1419=f419,field1420=f420,field1421=f421,field1422=f422,field1423=f423,field1424=f424,field1425=f425,field1426=f426,field1427=f427,field1428=f428,field1429=f429,field1430=f430,field1431=f431,field1432=f432,field1433=f433,field1434=f434,field1435=f435,field1436=f436,field1437=f437,field1438=f438,field1439=f439,field1440=f440,field1441=f441,field1442=f442,field1443=f443,field1444=f444,field1445=f445,field1446=f446,field1447=f447,field1448=f448,field1449=f449,field1450=f450,field1451=f451,field1452=f452,field1453=f453,field1454=f454,field1455=f455,field1456=f456,field1457=f457,field1458=f458,field1459=f459,field1460=f460,field1461=f461,field1462=f462,field1463=f463,field1464=f464,field1465=f465,field1466=f466,field1467=f467,field1468=f468,field1469=f469,field1470=f470,field1471=f471,field1472=f472,field1473=f473,field1474=f474,field1475=f475,field1476=f476,field1477=f477,field1478=f478,field1479=f479,field1480=f480,field1481=f481,field1482=f482,field1483=f483,field1484=f484,field1485=f485,field1486=f486,field1487=f487,field1488=f488,field1489=f489,field1490=f490,field1491=f491,field1492=f492,field1493=f493,field1494=f494,field1495=f495,field1496=f496,field1497=f497,field1498=f498,field1499=f499,field1500=f500)',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

use PosTester;
DROP CACHE HourlyAveLookup;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;


create application @appname@ recovery 5 second interval;

CREATE OR REPLACE SOURCE @cobolsrc@ USING FileReader (
  wildcard: '',
  positionbyeof: false,
  directory: ''
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: 'ProcessRecordAsEvent',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'SingleEvent',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'Level01',
  copybookFileName: ''
   )
OUTPUT TO @appname@Stream;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
  filename: '',
  directory: '',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter  (
  members: 'data',
  EventsAsArrayOfJsonObjects: 'true'
 )
INPUT FROM @appname@Stream;

end application @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	charset:'@charset@'
)
parse using JSONParser (
	eventType:'@evty@',
	fieldName:'@fname@'
)
OUTPUT TO CsvStream;

create Target FileTarget using FileWriter(
    rolloverpolicy:'eventcount:100',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JSONFormatter (
)
input from CsvStream;

end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

STOP APPLICATION LongRunningQueryTester.LongRunningApp;
UNDEPLOY APPLICATION LongRunningQueryTester.LongRunningApp;
DROP APPLICATION LongRunningQueryTester.LongRunningApp cascade;

CREATE APPLICATION LongRunningApp;


 --COMMENT::   Modify type attributes as desired.
 --COMMENT::   Type must be created first before creating a source using ranReader

CREATE TYPE RandomData(
  myName String,
  streetAddress String,
  bankName String,
  bankNumber int KEY,
  bankAmount double
);

CREATE source ranDataSource USING ranReader(
  OutputType:'LongRunningQueryTester.RandomData',
  TimeInterval:5,
  NoLimit:true,
  SampleSize:10000,
  DataKey:bankName,
  NumberOfUniqueKeys:500
) OUTPUT TO CSVDataStream;


CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4])
FROM CSVDataStream;


CREATE JUMPING WINDOW RandomData10Rows
OVER RandomDataStream KEEP 10 ROWS
PARTITION BY bankNumber;


CREATE TYPE myData(
  myName String,
  myAddress String,
  myBankName String,
  myBankNumber int KEY,
  myBankAmount double
);

CREATE STREAM myDataStream OF myData;

CREATE CQ GetMyData
INSERT INTO MyDataStream
SELECT myName, streetAddress, bankName, bankNumber, bankAmount
FROM RandomData10Rows WHERE bankNumber > 20000 AND bankNumber < 20300;


CREATE WACTIONSTORE MyDataActivity CONTEXT OF MyData
EVENT TYPES(myData )
PERSIST EVERY 60 second USING (
JDBC_DRIVER:'@WASTORE-DRIVER@',
JDBC_URL:'@WASTORE-URL@',
JDBC_USER:'@WASTORE-UNAME@',
JDBC_PASSWORD:'@WASTORE-PASSWORD@',
DDL_GENERATION:'drop-and-CREATE-tables'
);


Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
Select * from myDataStream
LINK SOURCE EVENT;


END APPLICATION LongRunningApp;

--
-- Recovery Test 11
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS1
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS2
--

STOP Recov11Tester.RecovTest11;
UNDEPLOY APPLICATION Recov11Tester.RecovTest11;
DROP APPLICATION Recov11Tester.RecovTest11 CASCADE;
CREATE APPLICATION RecovTest11 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions1 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions2 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions1
INSERT INTO Wactions1
SELECT
    *
FROM DataStream5Minutes;

CREATE CQ InsertWactions2
INSERT INTO Wactions2
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION RecovTest11;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;


CREATE OR REPLACE SOURCE @APPNAME@CDC_Source1 USING MySQLReader( 
  Compression: true,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',
  Tables: 'waction.crash_type',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root'
 ) 
OUTPUT TO @APPNAME@AppStream1;


CREATE OR REPLACE TARGET @APPNAME@sap_target USING DatabaseWriter( 
  DatabaseProviderType:'SAPHANA',
  ConnectionRetryPolicy: 'retryInterval=30,maxRetries=3',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'SYSTEM',
  Password_encrypted: 'false',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:sap://10.77.21.116:39013/?databaseName=striim&currentSchema=QA',
  Tables: 'waction.crash_type,QA.CRASH_TYPES',
  adapterName: 'DatabaseWriter',
  --IgnorableExceptionCode: '',
  Password: 'Striim_SAP@123'
 ) 
INPUT FROM @APPNAME@AppStream1;


create or replace target @APPNAME@sys_tgt using sysout(
name:Foo2
)input from @APPNAME@AppStream1;

END APPLICATION @APPNAME@;

deploy application @APPNAME@;
start application @APPNAME@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING CassandraWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

--
-- Recovery Test 42 with two sources and two WactionStores. A variety of partitioned windows in between
-- assure that we are testing a complicated recovery scenario.
--
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Stream -> JWc5 -> WS1
--   S2 -> Stream -> JWc10 -> WS2
--

STOP Recov42Tester.RecovTest42;
UNDEPLOY APPLICATION Recov42Tester.RecovTest42;
DROP APPLICATION Recov42Tester.RecovTest42 CASCADE;
CREATE APPLICATION RecovTest42 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10242,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10242,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM DataStreamTop OF CsvData using KafkaProps;

CREATE CQ Csv1ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ Csv2ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;










CREATE JUMPING WINDOW LeftJWc5
OVER DataStreamTop KEEP 5 ROWS;

CREATE JUMPING WINDOW RightJWc10
OVER DataStreamTop KEEP 10 ROWS;



CREATE WACTIONSTORE WactionsLeft CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsRight CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ ToWactionsLeft
INSERT INTO WactionsLeft
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM LeftJWc5 p;

CREATE CQ ToWactionsRight
INSERT INTO WactionsRight
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM RightJWc10 p;

END APPLICATION RecovTest42;

CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( 
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: '@CDC_URL@',
  Tables: '@Source1Tables@' ) 
OUTPUT TO @APPNAME@cdcStream;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@1 ( 
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  FetchSize: 20,
  DatabaseProviderType: 'Default',
  Table: '@Source3Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')  
OF @APPNAME@cachetype;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@2 ( 
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  FetchSize: 10,
  DatabaseProviderType: 'Default',
  Table: '@Source2Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')  
OF @APPNAME@cachetype;

UNDEPLOY APPLICATION admin.LongRunningAppOracle;
DROP APPLICATION admin.LongRunningAppOracle cascade;

CREATE APPLICATION LongRunningAppOracle;


 --COMMENT::   Modify type attributes as desired.
 --COMMENT::   Type must be created first before creating a source using ranReader

CREATE TYPE RandomData(
  myName String,
  streetAddress String,
  bankName String,
  bankNumber int KEY,
  bankAmount double
);

CREATE source ranDataSource USING ranReader(
  OutputType:'LongRunningAppOracle.RandomData',
  TimeInterval:5,
  NoLimit:true,
  SampleSize:10000,
  DataKey:bankName,
  NumberOfUniqueKeys:500
) OUTPUT TO CSVDataStream;


CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4])
FROM CSVDataStream;


CREATE JUMPING WINDOW RandomData10Rows
OVER RandomDataStream KEEP 10 ROWS
PARTITION BY bankNumber;


CREATE TYPE myData(
  myName String,
  myAddress String,
  myBankName String,
  myBankNumber int KEY,
  myBankAmount double
);

CREATE STREAM myDataStream OF myData;

CREATE CQ GetMyData
INSERT INTO MyDataStream
SELECT myName, streetAddress, bankName, bankNumber, bankAmount
FROM RandomData10Rows WHERE bankNumber > 20000 AND bankNumber < 20300;


CREATE WACTIONSTORE MyDataActivity CONTEXT OF MyData
EVENT TYPES(myData )
PERSIST EVERY 10 second USING (
JDBC_DRIVER:'@WASTORE-DRIVER@',
JDBC_URL:'@WASTORE-URL@',
JDBC_USER:'@WASTORE-UNAME@',
JDBC_PASSWORD:'@WASTORE-PASSWORD@',
pu_name:oracle,
DDL_GENERATION:'drop-and-CREATE-tables'
);

Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
Select * from myDataStream
LINK SOURCE EVENT;


END APPLICATION LongRunningAppOracle;

STOP APPLICATION @APPNAME@_app;
UNDEPLOY APPLICATION @APPNAME@_app;
DROP APPLICATION @APPNAME@_app CASCADE;
-- DROP EXCEPTIONSTORE @APPNAME@_exceptionstore;

CREATE APPLICATION @APPNAME@_app RECOVERY 120 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @APPNAME@_Source USING @SOURCE_ADAPTER@  (
  Username:'qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: '',
  ) OUTPUT TO @APPNAME@_Stream  ;

CREATE OR REPLACE TARGET @APPNAME@_Target USING BigQueryWriter  (
  Tables                        : '',
  projectId                    : '',
  ServiceAccountKey            : '',
  Mode                         : 'APPENDONLY',
  BatchPolicy                  : 'EventCount:1, Interval:60',
  ) INPUT FROM @APPNAME@_Stream;

-- CREATE OR REPLACE TARGET @APPNAME@_SysOut USING Global.SysOut (name: 'wa') INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@_app;
DEPLOY APPLICATION @APPNAME@_app;
START APPLICATION @APPNAME@_app;

stop application GCSWriterTest;
undeploy application GCSWriterTest;
drop application GCSWriterTest cascade;
create application GCSWriterTest recovery 1 second interval;
create source GCS_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;
create Target GCSTarget using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@',
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
--members:'data'
)
input from TypedCSVStream;

end application GCSWriterTest;
deploy application GCSWriterTest on all in default;
start application GCSWriterTest;

--
-- Recovery Test 7
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov7Tester.RecovTest7;
UNDEPLOY APPLICATION Recov7Tester.RecovTest7;
DROP APPLICATION Recov7Tester.RecovTest7 CASCADE;
CREATE APPLICATION RecovTest7 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM wacStream OF WactionType;

CREATE CQ InsertWacStream
INSERT INTO wacStream
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE WINDOW waWindow
OVER wacStream KEEP WITHIN 1 SECOND ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT *
FROM waWindow
LINK SOURCE EVENT;

END APPLICATION RecovTest7;

STOP APPLICATION oraddl;
UNDEPLOY APPLICATION oraddl;
DROP APPLICATION oraddl CASCADE;
CREATE APPLICATION oraddl recovery 5 second interval;
 
Create Source Ora Using OracleReader 
(
 Username:'@user-name@',
 Password:'@password@',
 ConnectionURL:'src_url',
 Tables:'QATEST.ORACLEDDL%',
 DictionaryMode:OfflineCatalog,
 DDLCaptureMode : 'All',
 FetchSize:1
) Output To LogminerStream;

Create Target tgt using DatabaseWriter 
(
 Username:'@username@',
 Password:'@password@',
 ConnectionURL:'TGT_URL',
 BatchPolicy:'EventCount:1,Interval:1',
 CommitPolicy:'EventCount:1,Interval:1',
 IgnorableExceptionCode: '1,2290,942',
 Tables :'QATEST.ORACLEDDL%,QATEST2.%'
) input from LogminerStream;

end application oraddl;
deploy application oraddl;
start application oraddl;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

create source @SourceName@ using MySQLReader
  (ConnectionURL: '@SourceConnectionURL@',
   Username:'@UserName@',
   Password:'@Password@',
   Tables: '@SourceTableName@'
)
output to @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
   ConnectionURL:'@TargetConnectionURL@',
   Username:'@UserName@',
   Password:'@Password@',
   BatchPolicy:'EventCount:1,Interval:0',
   Tables: '@SourceTableName@,@TargetTableName@'
 
 ) INPUT FROM @SRCINPUTSTREAM@;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;


CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using PostgreSQLReader
(
   adapterName: PostgreSQLReader,
   CDDLAction: Quiesce_Cascade,
   CDDLCapture: true,
   CDDLTrackingTable:'striim.ddlcapturetable',
   ConnectionURL: jdbc:postgresql://localhost:5432/qatest,
   FilterTransactionBoundaries: true,
   Password: w@ct10n,
   ReplicationSlotName:'test_slot',
   Tables: public.sample,
   Username: sa,
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE APPLICATION @APPNAME@ WITH ENCRYPTION @Recovery@ USE EXCEPTIONSTORE;
CREATE SOURCE @APPNAME@_s USING FileReader(
  directory:'Samples/AppData',
  wildcard:'PO.JSON',
  positionByEOF:false
)
parse using JSONParser (
) OUTPUT TO @APPNAME@_ss1;

--Using JSONNew() to create empty json node (j1)
CREATE CQ @APPNAME@_cq1 INSERT INTO @APPNAME@_ss2 SELECT JSONNew() j1,* FROM @APPNAME@_ss1 ;
--Using JSONSet() to set value to the empty json node (j1) and naming it as (j2)
CREATE CQ @APPNAME@_cq2 INSERT INTO @APPNAME@_ss3 SELECT JSONSet(j1, "Mission","Test") j2,* FROM @APPNAME@_ss2 ;
--Using JSONFrom() to create new json node (j3) using string
CREATE CQ @APPNAME@_cq3 INSERT INTO @APPNAME@_ss4 SELECT JSONFrom('{"PONumber":1600,"Reference":"ABULL-20140421","Requestor":"Alexis Bull","User":"ABULL","CostCenter":"A50","ShippingInstructions":{"name":"Alexis Bull","Address":{"street":"200 Sporting Green","city":"South San Francisco","state":"CA","zipCode":99236,"country":"United States of America"},"Phone":[{"type":"Office","number":"909-555-7307"},{"type":"Mobile","number":"415-555-1234"}]},"Special Instructions":null,"AllowPartialShipment":false,"LineItems":[{"ItemNumber":1,"Part":{"Description":"One Magic Christmas","UnitPrice":19.95,"UPCCode":13131092899},"Quantity":9,"Barcodes":[{"type":"Home","number":"9-555-7307"},{"type":"Office","number":"9-555-1234"}]},{"ItemNumber":2,"Part":{"Description":"Lethal Weapon","UnitPrice":19.95,"UPCCode":85391628927},"Quantity":5,"Barcodes":[{"type":"Home","number":"10-555-7307"},{"type":"Office","number":"10-555-1234"}]}]}')j3,* FROM @APPNAME@_ss3 ;
--Using JSONSet() to set new element to already existing node (data) - naming it as j4
CREATE CQ @APPNAME@_cq4 INSERT INTO @APPNAME@_ss5 SELECT JSONSet(data, "FileName","Dummy") j4,* FROM @APPNAME@_ss4 ;
--Using JSONArrayAdd() to Add element to existing JsonArray - naming it as j5
CREATE CQ @APPNAME@_cq5 INSERT INTO @APPNAME@_ss6 SELECT JSONArrayAdd(data.get("ShippingInstructions").get("Phone"),"work") j5,* FROM @APPNAME@_ss5 ;
--Using JSONArrayInsert() to Add element to existing JsonArray by specifying Index - naming it as j6
CREATE CQ @APPNAME@_cq6 INSERT INTO @APPNAME@_ss7 SELECT JSONArrayInsert(data.get("ShippingInstructions").get("Phone"),1,"residence") j6,* FROM @APPNAME@_ss6 ;
--Using JSONRemove() to Remove element from existing Json node 'data' - naming it as j7
CREATE CQ @APPNAME@_cq7 INSERT INTO @APPNAME@_ss8 SELECT JSONRemove(data,"Requestor") j7,* FROM @APPNAME@_ss7 ;
--Using JSONSetAll() to Add collection of objects (from metadata) to existing Json node 'data' - naming it as j8
CREATE CQ @APPNAME@_cq8 INSERT INTO @APPNAME@_ss9 SELECT JSONSetAll(j4, metadata) j8,* FROM @APPNAME@_ss8;
--Using JSONSetAll() in empty json node j1 - naming it as j9
CREATE CQ @APPNAME@_cq9 INSERT INTO @APPNAME@_ss10 SELECT JSONSetAll(j1, metadata) j9,* FROM @APPNAME@_ss9;
--Using JSONSetAll() with simple json node j2 - naming it as j10
CREATE CQ @APPNAME@_cq10 INSERT INTO @APPNAME@_ss11 SELECT JSONSetAll(j2, metadata) j10,* FROM @APPNAME@_ss10;
--Using JSONSetAll() with json node which has the same field/values to verify overriding
CREATE CQ @APPNAME@_cq11 INSERT INTO @APPNAME@_ss12 SELECT JSONSetAll(j10, metadata) j11,* FROM @APPNAME@_ss11;
create target @APPNAME@_t using sysout (name:ss1) input from @APPNAME@_ss12;

CREATE Stream @APPNAME@_str1 (
     PODetails com.fasterxml.jackson.databind.JsonNode,
     Phone com.fasterxml.jackson.databind.JsonNode,
     LineItems com.fasterxml.jackson.databind.JsonNode,
     BarCodes com.fasterxml.jackson.databind.JsonNode,
     Addr com.fasterxml.jackson.databind.JsonNode
);

CREATE CQ @APPNAME@_cq12
INSERT into @APPNAME@_str1
    select
    j8,
    j8.get('ShippingInstructions').get('Phone'),
    j8.get('LineItems'),
    j8.get('LineItems').get('Barcodes'),
    j8.get('ShippingInstructions').get('Address')
from @APPNAME@_ss12;

CREATE Stream @APPNAME@_str2(
     PONumber Integer,
     Reference String,
     Usr String,
     CostCenter String,
     Name String,
     street String,
     city String,
     state String,
     zipCode Integer,
     country String,
     officephone String,
     mobilephone String,
     SplInstruction String,
     AllowPartialShipment boolean,
     PhoneType String,
     PhoneNo String,
     ItemNumber Integer,
     ItemDesc String,
     UnitProce Double,
     UPCCode Double,
     Quantity Integer,
     BarcodeType String,
     BarcodeNo String
);

CREATE CQ @APPNAME@_cq13
INSERT into @APPNAME@_str2
SELECT

    /* PO Details */
    JSONGetInteger(x.PODetails,"PONumber"),
    JSONGetString(x.PODetails,"Reference"),
    JSONGetString(x.PODetails,"User"),
    JSONGetString(x.PODetails,"CostCenter"),

    /* Shipping Details */
    JSONGetString(x.PODetails.get('ShippingInstructions'),"name"),
    JSONGetString(x.Addr,"street"),
    JSONGetString(x.Addr,"city"),
    JSONGetString(x.Addr,"state"),
    JSONGetInteger(x.Addr,"zipCode"),
    JSONGetString(x.Addr,"country"),

    /* Phone nos by array position*/
    JSONGetString(x.Phone.get(0),"number"),
    JSONGetString(x.Phone.get(1),"number"),

    /*Others*/
    JSONGetString(x.PODetails,"Special Instructions"),
    JSONGetboolean(x.PODetails,"AllowPartialShipment"),

    /* Phone nos using iterator*/
    JSONGetString(pho,"type"),
    JSONGetString(pho,"number"),

    /*Item Specific one */
    JSONGetInteger(Items,"ItemNumber"),
    JSONGetString(Items.get('Part'),"Description"),
    JSONGetDouble(Items.get('Part'),"UnitPrice"),
    JSONGetDouble(Items.get('Part'),"UPCCode"),
    JSONGetInteger(Items,"Quantity"),

    /*Barcode specific values */
    JSONGetString(Barcode,"type"),
    JSONGetString(Barcode,"number")

from @APPNAME@_str1 x, iterator(x.LineItems) Items, iterator(x.Phone) pho, iterator(Items.Barcodes) Barcode;

create Target @APPNAME@_JSONNew_T using FileWriter (
filename:'@APPNAME@_JSONNew_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j1)
input from @APPNAME@_ss2;

create Target @APPNAME@_JSONSet1_T using FileWriter (
filename:'@APPNAME@_JSONSet1_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j2)
input from @APPNAME@_ss3;

create Target @APPNAME@_JSONFrom_T using FileWriter (
filename:'@APPNAME@_JSONFrom_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j3)
input from @APPNAME@_ss4;

create Target @APPNAME@_JSONSet2_T using FileWriter (
filename:'@APPNAME@_JSONSet2_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j4)
input from @APPNAME@_ss5;

create Target @APPNAME@_JSONArrayAdd_T using FileWriter (
filename:'@APPNAME@_JSONArrayAdd_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j5)
input from @APPNAME@_ss6;

create Target @APPNAME@_JSONArrayInsert_T using FileWriter (
filename:'@APPNAME@_JSONArrayInsert_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j6)
input from @APPNAME@_ss7;

create Target @APPNAME@_JSONRemove_T using FileWriter (
filename:'@APPNAME@_JSONRemove_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j7)
input from @APPNAME@_ss8;

create Target @APPNAME@_JSONSetAll1_T using FileWriter (
filename:'@APPNAME@_JSONSetAll1_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j8)
input from @APPNAME@_ss9;

create Target @APPNAME@_JSONSetAll2_T using FileWriter (
filename:'@APPNAME@_JSONSetAll2_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j9)
input from @APPNAME@_ss10;

create Target @APPNAME@_JSONSetAll3_T using FileWriter (
filename:'@APPNAME@_JSONSetAll3_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j10)
input from @APPNAME@_ss11;

create Target @APPNAME@_JSONSetAll4_T using FileWriter (
filename:'@APPNAME@_JSONSetAll4_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000,interval:1s'
)
format using jsonFormatter(members:j11)
input from @APPNAME@_ss12;

create Target @APPNAME@_PO_T using FileWriter (
filename:'@APPNAME@_PO_RT',
directory:'@LOGDIR@',
rolloverpolicy:'eventcount:5000000'
)
format using dsvFormatter()
input from @APPNAME@_str2;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

create application JuniperLog;
create source JuniperLogSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'juniper-NSM*',
	charset:'UTF-8',
	positionByEOF:false
) PARSE USING JuniperNSMLogParser (
	trimwhitespace: yes
)
OUTPUT TO JuniperLogStream;
create Target JuniperDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/junipernsm_log') input from JuniperLogStream;
end application JuniperLog;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter
  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1'
 )

CREATE OR REPLACE APPLICATION @AppName@;

CREATE OR REPLACE SOURCE CP_Oracle_source USING OracleReader (
  ConnectionURL: '',
  Tables: '',
  Username: '',
  Password: '',
  Fetchsize: 1 )
OUTPUT TO CP_EndToEnd_SF_Adapter_Stream;

CREATE OR REPLACE TARGET CP_SF_Target USING Global.SnowflakeWriter (
  connectionProfileName: '',
  streamingUpload: 'false',
  StreamingConfiguration: 'MaxParallelRequests=5, MaxRequestSizeInMB=5, MaxRecordsPerRequest=10000',
  useConnectionProfile: 'true',
  externalStageConnectionProfileName: '',
  uploadPolicy: 'eventcount:10000,interval:5m',
  Tables: 'QATEST.Test_CP,SANJAYPRATAP.SAMPLESCHEMA.SAMPLE_PK')
INPUT FROM CP_EndToEnd_SF_Adapter_Stream;

END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

--This is a dummy file.
--Framework doesn't support to create an empty resource directory.
--Purpose of creating resource dir here is to store test logs in specific test related directory.

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;
CREATE  SOURCE @SOURCENAME@ USING OracleReader  (
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.emp',
  --OnlineCatalog: true,
  FetchSize: @FETCHSIZE@
 )
OUTPUT TO DataStream;

CREATE TARGET @TARGETNAME@ USING DatabaseWriter(
ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',
Username:'qatest',
PassWord:'w3b@ct10n',
Tables: 'qatest.emp,dbo.emp',
CHECKPOINTTABLE : 'qatest.CHKPOINT',
ConnectionRetryPolicy: '@RETRY_INTERVAL@,@MAX_RETRY@',
BatchPolicy:'EventCount:@BATCH_EVENT@,Interval:@BATCH_INTERVAL@',
CommitPolicy:'EventCount:@COMMIT_EVENT@,Interval:@COMMIT_INTERVAL@'
) INPUT FROM DataStream;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

--spool on to '/Users/jeyaselvan/Product/spoolfile.log';
select * from Oracle12C_To_Oracle12CApp_ExceptionStore;
--spool off;

--
-- Recovery Test 4
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS
-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS
--

STOP Recov4Tester.RecovTest4;
UNDEPLOY APPLICATION Recov4Tester.RecovTest4;
DROP APPLICATION Recov4Tester.RecovTest4 CASCADE;
CREATE APPLICATION RecovTest4 RECOVERY 50 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream OF CsvData;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION RecovTest4;

CREATE TARGET @TARGET_NAME@ using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkuPDaTehAnDliNgmOdE:'DELETEANDINSERT',
tables: 'QATEST.ORCLALLDATATYPES,@TARGET_TABLE@ ColumnMap(Log_Id=LOG_ID,Description=Description,LogChar=LogChar,LogVarchar2=LogVarchar2,LogNumber=LogNumber,LogBinaryFloat=LogBinaryFloat,LogBinaryDouble=LogBinaryDouble,LogFloat=LogFloat,LogDate=LogDate,LogTimestamp=LogTimestamp,LogNchar=LogNchar,LogNvarchar2=LogNvarchar2,LogTimezone=LogTimezone,LogTimelocal=LogTimelocal,LogInterval=LogInterval,LogInterval2=LogInterval2,LogInteger=LogInteger)',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM @STREAM@;

STOP APPLICATION KafkaWPTester.KWApp;
STOP APPLICATION KafkaWPTester.KRApp;
UNDEPLOY APPLICATION KafkaWPTester.KWApp;
UNDEPLOY APPLICATION KafkaWPTester.KRApp;
DROP APPLICATION KafkaWPTester.KWApp CASCADE;
DROP APPLICATION KafkaWPTester.KRApp CASCADE;

CREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;
GRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;
CONNECT KafkaWPTester KafkaWPTester;


CREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE CSVSource USING FileReader (
	directory:'/Users/bhushan/git/BBProduct/product/IntegrationTests/TestData/multiLog',
    WildCard:'access_log_half',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;

CREATE TYPE AccessLogType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE STREAM TypedAccessLogStream OF AccessLogType partition by Col4 ;

CREATE CQ AceeslogCQ
INSERT INTO TypedAccessLogStream
SELECT data[0],data[1],data[2], data[3],data[4], data[5], data[6], data[7],data[8],data[9]
FROM FileStream;

create Target DsvWriter using KafkaWriter VERSION '0.9.0' ( 
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V9dsvExistTopic',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream;


create Target JsonWriter using KafkaWriter VERSION '0.9.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V9jsonExistTopic',
	Mode:'Sync'
    )
 
FORMAT USING JSONFormatter ()
input from TypedAccessLogStream;


create Target AvroWriter using KafkaWriter VERSION '0.9.0' ( 
	brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
	Topic:'V9avroExistTopic',
    Mode:'Sync'
    )
 
FORMAT USING AvroFormatter (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc')
input from TypedAccessLogStream;


END APPLICATION KWApp;

DEPLOY APPLICATION KWApp on any in default;


-- Kafka Reader Apps

CREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;


CREATE TYPE AccessLogType2(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String,
Col6 String,
Col7 String,
Col8 String,
Col9 String,
Col10 String
);

CREATE Stream KafkaDSVReaderStream of AccessLogType2;
CREATE Stream KafkaJsonReaderStream of AccessLogType2;
CREATE Stream KafkaAvroReaderStream of AccessLogType2;


CREATE SOURCE KafkaDSVSource USING KafkaReader VERSION '0.9.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V9dsvExistTopic',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;




CREATE CQ DSVtoKafkaDSVReaderStream
INSERT INTO KafkaDSVReaderStream
SELECT data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10]
FROM KafkaReaderStream1;

CREATE TARGET DSVDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V9_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaDSVReaderStream;

CREATE SOURCE KafkaJsonSource USING KafkaReader VERSION '0.9.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V9jsonExistTopic',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;




CREATE CQ DSVtoKafkaJsonReaderStream
INSERT INTO KafkaJsonReaderStream
SELECT 
data.get('Col1').textValue(),
data.get('Col2').textValue(),
data.get('Col3').textValue(),
data.get('Col4').textValue(),
data.get('Col5').textValue(),
data.get('Col6').textValue(),
data.get('Col7').textValue(),
data.get('Col8').textValue(),
data.get('Col9').textValue(),
data.get('Col10').textValue()
FROM KafkaReaderStream2;

CREATE TARGET JSONDump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V9_RT_JSON')
FORMAT USING DSVFormatter()
INPUT FROM KafkaJsonReaderStream;

CREATE SOURCE KafkaAvroSource USING KafkaReader VERSION '0.9.0' (
        brokerAddress:'localhost:9092, localhost:9093,localhost:9094, localhost:9095',
        Topic:'V9avroExistTopic',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/kafkae1pAvroTestParsed1.avsc'
)
OUTPUT TO KafkaReaderStream3;



CREATE Stream KRTypedStream of AccessLogType;

CREATE CQ DSVtoKafkaAvroReaderStream
INSERT INTO KafkaAvroReaderStream
SELECT    
data.get("Col1").toString(), 
data.get("Col2").toString(), 
data.get("Col3").toString(), 
data.get("Col4").toString(),
data.get("Col5").toString(), 
data.get("Col6").toString(), 
data.get("Col7").toString(), 
data.get("Col8").toString(),
data.get("Col9").toString(), 
data.get("Col10").toString()
FROM KafkaReaderStream3;

CREATE TARGET AVRODump USING FileWriter(
name:testOuput,
rolloverpolicy:'filesize:500M',
filename:'/Users/bhushan/git/BBProduct/product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V9_RT_AVRO')
FORMAT USING DSVFormatter()
INPUT FROM KafkaAvroReaderStream;

END APPLICATION KRApp;
DEPLOY APPLICATION KRApp on any in default;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;


 CREATE SOURCE DBSource USING MySQLReader (
 Compression: true,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: '@CONNECTION_URL@',
  DatabaseName: 'testcassandra',
  Tables: '@SOURCE_TABLE@',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)
OUTPUT TO Mysql_ChangeDataStream;
CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'test.chkpoint',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  IgnorableExceptionCode:'PRIMARY KEY',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM Mysql_ChangeDataStream;
create Target t2 using SysOut(name:Foo2) input from Mysql_ChangeDataStream;

END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start application DBRTOCW;

create source @SOURCE_NAME@
USING MariaDbXpandReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@ConnectionURL@',
 Tables:'@Tables@',
 FetchSize:10000,
 QueueSize:2048
)Output To @STREAM@;

STOP APPLICATION routerApp;
UNDEPLOY APPLICATION routerApp;
DROP APPLICATION routerApp CASCADE;


CREATE APPLICATION routerApp;

CREATE  SOURCE OraSource USING OracleReader  (
Username: 'qatest',
Password: 'qatest',
ConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',
Tables: 'QATEST.TGT_T%',
 FetchSize:'100'
)
OUTPUT TO MasterStream1;

CREATE OR REPLACE ROUTER tablerouter1 INPUT FROM MasterStream1 s CASE
WHEN meta(s,"TableName").toString()='QATEST.TGT_T1' THEN ROUTE TO ss1,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T2' THEN ROUTE TO ss2,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T3' THEN ROUTE TO ss3,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T4' THEN ROUTE TO ss4,
ELSE ROUTE TO ss_else;

create Target FileTarget_1 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'
)
FORMAT USING dsvFormatter ()
input from ss1;

create Target FileTarget_2 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'
)
FORMAT USING dsvFormatter ()
input from ss2;

create Target FileTarget_3 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from ss3;

create Target FileTarget_4 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from ss4;


create Target FileTarget_5 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from ss_else;


end application routerApp;
deploy application routerApp;
start routerApp;

stop application @appName@;
undeploy application @appName@;
drop application @appName@ cascade;

CREATE OR REPLACE APPLICATION @appName@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @appName@_Source USING PostgreSQLReader (
Username: '@Username@',
ConnectionURL: '@ConnectionURL@',
ReplicationSlotName: '@ReplicationSlotName@',
Tables: '@srcTable@',
Password: '@Password@' )
OUTPUT TO @appName@_Stream;

CREATE OR REPLACE TARGET @appName@_Target USING Global.BigQueryWriter (
ColumnDelimiter: '|',
NullMarker: 'NULL',
streamingUpload: 'false',
projectId: '@projectId@',
Encoding: 'UTF-8',
ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 ,
maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30',
AllowQuotedNewLines: 'false',
CDDLAction: 'Process',
Tables: '@srcTable@,@tgtTable@',
optimizedMerge: 'false',
TransportOptions: 'connectionTimeout=300,
readTimeout=120',
adapterName: 'BigQueryWriter',
Mode: 'APPENDONLY',   StandardSQL: 'true',
ServiceAccountKey: '@GCS-AuthPath@',
BatchPolicy: 'eventCount:100',
QuoteCharacter: '\"' )
INPUT FROM @appName@_Stream;
END APPLICATION @appName@;

IMPORT cern.colt.list.DoubleArrayList;

IMPORT com.webaction.runtime.LocationInfo;

IMPORT com.webaction.runtime.LocationArrayList;

UNDEPLOY APPLICATION FraudTester.FraudDetectionApp;
DROP APPLICATION FraudTester.FraudDetectionApp cascade;

CREATE APPLICATION FraudDetectionApp;

CREATE SOURCE CsvDataSource USING CSVReader
(
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'fraudPosData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE PosData
(
  merchantId String,
  pan String,
  datetime DateTime,
  hourValue int,
  amount double,
  zip String
);

CREATE STREAM PosDataStream OF PosData PARTITION BY merchantId;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT data[1], data[2], TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]), data[9]
FROM CsvStream;

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON datetime
PARTITION BY merchantId;

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE ZipLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  header: Yes,
  columndelimiter: '	',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

CREATE TYPE MerchantNameData(
  merchantId		String KEY,
  companyName		String
);

CREATE CACHE NameLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'MerchantNames.csv',
  header: Yes,
  columndelimiter: ',',
  trimquote:false
) QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE TYPE CustomerDetails(
  pan String KEY,
  fname String,
  lname String,
  address String,
  city String,
  state String,
  zip String,
  gender String
);

CREATE CACHE CustomerLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'customerdetails.csv',
  header: Yes,
  columndelimiter: ',',
  trimquote:false
) QUERY(keytomap:'pan') OF CustomerDetails;

CREATE TYPE PanWithMerchantData(
wKey String KEY,
pan String,
count int,
datetime DateTime,
locations com.webaction.runtime.LocationArrayList);

CREATE STREAM PanWithMerchantDataStream of PanWithMerchantData PARTITION BY wKey;

CREATE CQ CsvToPanWithMerchantData
INSERT into PanWithMerchantDataStream
SELECT p.pan+p.datetime, p.pan , count(*), p.datetime, locInfoList(z.latVal, z.longVal, z.city, z.zip, n.companyName)
FROM PosData5Minutes p, ZipLookup z, NameLookup n
WHERE p.zip = z.zip AND p.merchantId = n.merchantId group by p.pan having count(*) > 1;

CREATE TYPE PanWithMerchantAndCustomerData(
wKey String KEY,
pan String,
fname String,
lname String,
address String,
city String,
state String,
zip String,
gender String,
count int,
datetime DateTime,
locations com.webaction.runtime.LocationArrayList);

CREATE STREAM PanWithMerchantAndCustomerDataStream of PanWithMerchantAndCustomerData PARTITION BY wKey;

CREATE CQ PanWithMerchantToMerchantAndCustomerData
INSERT into PanWithMerchantAndCustomerDataStream SELECT ds.wKey, ds.pan, c.fname, c.lname, c.address, c.city, c.state, c.zip, c.gender, ds.count, ds.datetime, ds.locations
FROM PanWithMerchantDataStream ds, CustomerLookup c
WHERE ds.pan = c.pan;

CREATE TYPE AlertRecord(
  name String,
  keyVal String,
  severity String,
  flag String ,
  message String
);

CREATE STREAM AlertStream OF AlertRecord;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT p.pan, p.wKey,
       CASE
         WHEN p.count <= 2  THEN 'info'
      WHEN p.count > 2 THEN 'warning'
         ELSE 'error' END,
       CASE
         WHEN p.count > 1 THEN 'raise'
         ELSE 'cancel' END,
       CASE
         WHEN p.count <= 2 THEN textFromFields('PAN $2 has been used $10 times in a 5 minute period',p)
         WHEN p.count > 2 THEN textFromFields('[Abnormal Usage] PAN $2 has been used $10 times in a 5 minute period',p)
         ELSE ''
         END
FROM PanWithMerchantAndCustomerDataStream p;

CREATE TARGET SendAlert USING AlertSender(
 name: unusualActivity
) INPUT FROM AlertStream;

CREATE WACTIONSTORE FraudActivity CONTEXT OF PanWithMerchantAndCustomerData
EVENT TYPES ( PanWithMerchantAndCustomerData )
@PERSIST-TYPE@


Create CQ TrackFraudActivity
INSERT INTO FraudActivity
Select * from PanWithMerchantAndCustomerDataStream
LINK SOURCE EVENT;


END APPLICATION FraudDetectionApp;

CREATE APPLICATION applicationApiSaas;

CREATE FLOW SaasSourceFlow;

CREATE SOURCE SaasLog4JSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'SaasMonitorLog4JOutput.xml',
  positionByEOF:false
)
PARSE USING XMLParser(
  rootnode:'/log4j:event',
  columnlist:'log4j:event/@timestamp,log4j:event/@level,log4j:event/log4j:message,log4j:event/log4j:locationInfo/@method'
)
OUTPUT TO SaasRawXMLStream;

CREATE TYPE SaasLog4JEvent (
    ts DateTime,
    level String,
    message String,
    method String
);
CREATE STREAM SaasLog4JStream OF SaasLog4JEvent;

CREATE CQ SaasGetLog4JData
INSERT INTO SaasLog4JStream
SELECT TO_DATE(TO_LONG(data[0])), data[1], data[2], data[3]
FROM SaasRawXMLStream;

CREATE STREAM SaasErrorStream OF SaasLog4JEvent;
CREATE STREAM SaasWarningStream OF SaasLog4JEvent;
CREATE STREAM SaasInfoStream OF SaasLog4JEvent;

CREATE CQ SaasGetErrors
INSERT INTO SaasErrorStream
SELECT log4j
FROM SaasLog4JStream log4j WHERE log4j.level = 'ERROR';

CREATE CQ SaasGetWarnings
INSERT INTO SaasWarningStream
SELECT log4j
FROM SaasLog4JStream log4j WHERE log4j.level = 'WARN';

CREATE CQ SaasGetInfo
INSERT INTO SaasInfoStream
SELECT log4j
FROM SaasLog4JStream log4j WHERE log4j.level = 'INFO';

END FLOW SaasSourceFlow;


CREATE FLOW SaasErrorFlow;

CREATE STREAM SaasErrorAlertStream OF Global.AlertEvent;

CREATE CQ SaasSendErrorAlerts
INSERT INTO SaasErrorAlertStream
SELECT 'ErrorAlert', ''+ts, 'error', 'raise', 'Error in log ' + message
FROM SaasErrorStream;

CREATE SUBSCRIPTION SaasErrorAlertSub USING WebAlertAdapter( ) INPUT FROM SaasErrorAlertStream;

END FLOW SaasErrorFlow;


CREATE FLOW SaasWarningFlow;

CREATE JUMPING WINDOW SaasWarningWindow
OVER SaasWarningStream KEEP WITHIN 30 SECOND ON ts;

CREATE STREAM SaasWarningAlertStream OF Global.AlertEvent;

CREATE CQ SaasSendWarningAlerts
INSERT INTO SaasWarningAlertStream
SELECT 'WarningAlert', ''+ts, 'warning', 'raise',
        COUNT(ts) + ' Warnings in log for api ' + method
FROM SaasWarningWindow
GROUP BY method
HAVING count(ts) > 25;
CREATE SUBSCRIPTION SaasWarningAlertSub USING WebAlertAdapter( ) INPUT FROM SaasWarningAlertStream;

END FLOW SaasWarningFlow;


CREATE FLOW SaasInfoFlow;

CREATE TYPE SaasApiCall (
  userId String,
  apiCall String,
  sobject String,
  ts DateTime,
  userName String,
  company String,
  userZip String,
  companyZip String
);
CREATE STREAM SaasApiStream OF SaasApiCall;

CREATE CQ SaasGetApiDetails
INSERT INTO SaasApiStream(userId,apiCall,sobject,ts)
SELECT MATCH(i.message, '\\\\[user=([a-zA-Z0-9]*)\\\\]'),
       MATCH(i.message, '\\\\[api=([a-zA-Z0-9]*)\\\\]'),
       MATCH(i.message, '\\\\[sobject=([a-zA-Z0-9]*)\\\\]'),
       i.ts
FROM SaasInfoStream i;

CREATE STREAM SaasApiEnrichedStream OF SaasApiCall;

CREATE TYPE SaasUserInfo (
  userId String,
  userName String,
  company String,
  userZip String,
  companyZip String
);
CREATE CACHE SaasUserLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'userLookup.txt',
  header: Yes,
  columndelimiter: ','
) QUERY (keytomap:'userId') OF SaasUserInfo;


CREATE CQ SaasGetUserDetails
INSERT INTO SaasApiEnrichedStream
SELECT a.userId, a.apiCall, a.sobject, a.ts, u.userName, u.company, u.userZip, u.companyZip
FROM SaasApiStream a, SaasUserLookup u
WHERE a.userId = u.userId;

END FLOW SaasInfoFlow;


CREATE FLOW SaasUserApiFlow;

CREATE JUMPING WINDOW SaasUserApiWindow
OVER SaasApiEnrichedStream KEEP WITHIN 5 SECOND ON ts
PARTITION BY userId;

CREATE TYPE SaasUserApiUsage (
  userId String key,
  userName String,
  userZip String,
  userLat double,
  userLong double,
  company String,
  apiCall String,
  count integer,
  state String,
  topObject String,
  ts DateTime
);
CREATE STREAM SaasUserApiUsageStream OF SaasUserApiUsage;

CREATE TYPE SaasUSAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);
CREATE CACHE SaasZipLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  header: Yes,
  columndelimiter: '	'
) QUERY (keytomap:'zip') OF SaasUSAddressData;


CREATE CQ SaasGetUserApiUsage
INSERT INTO SaasUserApiUsageStream
SELECT a.userId, a.userName, a.userZip, z.latVal, z.longVal, a.company,
       a.apiCall, COUNT(a.sobject),
       CASE WHEN COUNT(a.sobject) > 175 THEN 'HOT'
            ELSE 'COLD' END,
       MAXOCCURS(a.sobject),
       FIRST(a.ts)
FROM SaasUserApiWindow a, SaasZipLookup z
WHERE a.userZip = z.zip
GROUP BY a.userId, a.apiCall
HAVING FIRST(a.ts) IS NOT NULL;

CREATE TYPE SaasUserApiContext (
  userId String key,
  userName String,
  userZip String,
  userLat double,
  userLong double,
  company String,
  count integer,
  state String,
  topObject String,
  ts DateTime
);
CREATE WACTIONSTORE SaasUserApiActivity
CONTEXT OF SaasUserApiContext
EVENT TYPES ( SaasUserApiUsage )
@PERSIST-TYPE@

CREATE JUMPING WINDOW SaasUserWindow
OVER SaasUserApiUsageStream KEEP WITHIN 5 SECOND ON ts
PARTITION BY userId;

CREATE CQ SaasTrackUserApiSummary
INSERT INTO SaasUserApiActivity
SELECT a.userId, a.userName, a.userZip, a.userLat, a.userLong,
       a.company, SUM(count),
       CASE WHEN SUM(count) > 1000 THEN 'HOT'
            ELSE 'COLD' END,
       MAXOCCURS(a.topObject),
       FIRST(a.ts)
FROM SaasUserWindow a
GROUP BY a.userId
LINK SOURCE EVENT;

CREATE STREAM SaasUserApiAlertStream OF Global.AlertEvent;

CREATE CQ SaasSendUserApiAlerts
INSERT INTO SaasUserApiAlertStream
SELECT 'UserAPIAlert', ''+ts, 'warning', 'raise',
       'User ' + userName + ' has used api ' + apiCall + ' ' + count + ' times for ' + topObject
FROM SaasUserApiUsageStream
WHERE state = 'HOT' AND count > 300;

CREATE SUBSCRIPTION SaasUserApiAlertSub USING WebAlertAdapter( ) INPUT FROM SaasUserApiAlertStream;

END FLOW SaasUserApiFlow;


CREATE FLOW SaasCompanyApiFlow;

CREATE JUMPING WINDOW SaasCompanyApiWindow
OVER SaasApiEnrichedStream KEEP WITHIN 5 SECOND ON ts
PARTITION BY company;

CREATE TYPE SaasCompanyApiUsage (
  company String key,
  companyZip String,
  companyLat double,
  companyLong double,
  apiCall String,
  count integer,
  state String,
  topObject String,
  ts DateTime
);
CREATE STREAM SaasCompanyApiUsageStream OF SaasCompanyApiUsage;

CREATE CQ SaasGetCompanyApiUsage
INSERT INTO SaasCompanyApiUsageStream
SELECT a.company, a.companyZip, z.latVal, z.longVal,
       a.apiCall, COUNT(a.sobject),
       CASE WHEN COUNT(a.sobject) > 3000 THEN 'HOT'
            ELSE 'COLD' END,
       MAXOCCURS(a.sobject),
       FIRST(a.ts)
FROM SaasCompanyApiWindow a, SaasZipLookup z
WHERE a.companyZip = z.zip
GROUP BY a.company, a.apiCall
HAVING FIRST(a.ts) IS NOT NULL;

CREATE TYPE SaasCompanyApiContext (
  company String key,
  companyZip String,
  companyLat double,
  companyLong double,
  count integer,
  state String,
  topObject String,
  ts DateTime
);
CREATE JUMPING WINDOW SaasCompanyWindow
OVER SaasCompanyApiUsageStream KEEP WITHIN 5 SECOND ON ts
PARTITION BY company;

CREATE WACTIONSTORE SaasCompanyApiActivity
CONTEXT OF SaasCompanyApiContext
EVENT TYPES ( SaasCompanyApiUsage )
@PERSIST-TYPE@


CREATE CQ SaasTrackCompanyApiSummary
INSERT INTO SaasCompanyApiActivity
SELECT a.company, a.companyZip, a.companyLat, a.companyLong,
       SUM(a.count),
       CASE WHEN SUM(a.count) > 15000 THEN 'HOT'
            ELSE 'COLD' END,
       MAXOCCURS(a.topObject),
       FIRST(a.ts)
FROM SaasCompanyWindow a
GROUP BY a.company
LINK SOURCE EVENT;

END FLOW SaasCompanyApiFlow;


CREATE FLOW SaasApiFlow;

CREATE JUMPING WINDOW SaasApiWindow
OVER SaasApiEnrichedStream KEEP WITHIN 5 SECOND ON ts
PARTITION BY apiCall;

CREATE TYPE SaasApiUsage (
  apiCall String key,
  sobject String,
  count integer,
  state String,
  ts DateTime
);
CREATE STREAM SaasApiUsageStream OF SaasApiUsage;

CREATE CQ SaasGetApiUsage
INSERT INTO SaasApiUsageStream
SELECT a.apiCall, a.sobject,
       COUNT(a.userId),
       CASE WHEN COUNT(a.userId) > 3000 THEN 'HOT'
            ELSE 'COLD' END,
       FIRST(a.ts)
FROM SaasApiWindow a
GROUP BY a.apiCall, a.sobject
HAVING FIRST(a.ts) IS NOT NULL;

CREATE TYPE SaasApiContext (
  apiCall String key,
  count integer,
  state String,
  topObject String,
  ts DateTime
);

CREATE JUMPING WINDOW SaasApiAggWindow
OVER SaasApiUsageStream KEEP WITHIN 5 SECOND ON ts
PARTITION BY apiCall;

CREATE WACTIONSTORE SaasApiActivity
CONTEXT OF SaasApiContext
EVENT TYPES ( SaasApiUsage )
@PERSIST-TYPE@

CREATE CQ SaasTrackApiSummary
INSERT INTO SaasApiActivity
SELECT a.apiCall,
       SUM(a.count),
       CASE WHEN SUM(a.count) > 20000 THEN 'HOT'
            ELSE 'COLD' END,
       MAXOCCURS(a.sobject),
       first(a.ts)
FROM SaasApiAggWindow a
GROUP BY a.apiCall
LINK SOURCE EVENT;

END FLOW SaasApiFlow;

END APPLICATION applicationApiSaas;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[0]) != '2';

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

stop application Postgres_To_Filewriter;
undeploy application Postgres_To_Filewriter;
drop application Postgres_To_Filewriter cascade;

CREATE APPLICATION Postgres_To_Filewriter RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE Postgres_Src USING PostgreSQLReader  ( 
  ReplicationSlotName: '',
  FilterTransactionBoundaries: 'true',
  Username: '',
  Password_encrypted: false,
  ConnectionURL: '',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: '',
  Tables: ''
 ) 
OUTPUT TO Change_Data_Stream;


CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Change_Data_Stream;
 
 CREATE TARGET BinaryDump USING LogWriter(
  name: 'PostgresCDCData',
  filename:'PostgresCDCData.log'
)INPUT FROM Change_Data_Stream;
 

end application Postgres_To_Filewriter;
deploy application Postgres_To_Filewriter;
start Postgres_To_Filewriter;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@;

create flow agentflow;
CREATE OR REPLACE SOURCE @APPNAME@_Src USING SpannerBatchReader  (
  DatabaseProviderType: 'Default',
  pollingInterval: '5ms',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  ConnectionURL: 'jdbc:cloudspanner:/projects/bigquerywritertest/instances/testspanner/databases/spannertestdb?credentials=/Users/jenniffer/Downloads/abc.json',
  Tables: 'Recovery_Timestam%',
  --_h_mode:'InitialLoad',
--  VendorConfiguration:'_h_SpannerReadStaleness=MAX_STALENESS 20s',
  adapterName: 'SpannerBatchReader',
    StartPosition: '%=0',
  CheckColumn: '%=id'
 )
OUTPUT TO @APPNAME@_Output_Stream;
end flow agentflow;

CREATE TARGET @APPNAME@_tgt USING SpannerWriter (
	Tables: 'spannersource,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	BatchPolicy: 'EventCount: 1; Interval: 1s',
	instanceId: 'qatest'
) INPUT FROM @APPNAME@_Output_Stream;

create Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_Output_Stream;

end application @APPNAME@;
deploy application @APPNAME@ with agentflow in agents;
start application @APPNAME@;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;
DROP EXCEPTIONSTORE @APP_NAME@_EXCEPTIONSTORE;

CREATE APPLICATION @APP_NAME@ @APP_PROPERTY@ USE EXCEPTIONSTORE;

CREATE OR REPLACE STREAM @APP_NAME@_DataStream OF Global.WAEvent;

Create Source @APP_NAME@_Source Using @SOURCE_ADAPTER@ (

) OUTPUT TO @APP_NAME@_DataStream;

CREATE TARGET @APP_NAME@_Target USING @TARGET_ADAPTER@( 

) INPUT FROM @APP_NAME@_DataStream;

CREATE OR REPLACE TARGET @APP_NAME@_SysOut USING Global.SysOut  ( 
	name: '@APP_NAME@_SysOutWA' 
) INPUT FROM @APP_NAME@_DataStream;

END APPLICATION @APP_NAME@;

DEPLOY APPLICATION @APP_NAME@ IN DEFAULT;
START APPLICATION @APP_NAME@;

use PosTester;
DROP WINDOW PosData5Minutes;

--
-- Canon Test W40
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for an unpartitioned jumping attribute window
--
-- S -> JWa5u -> CQ -> WS
--


UNDEPLOY APPLICATION NameW40.W40;
DROP APPLICATION NameW40.W40 CASCADE;
CREATE APPLICATION W40 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW40;


CREATE SOURCE CsvSourceW40 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW40;


END FLOW DataAcquisitionW40;




CREATE FLOW DataProcessingW40;

CREATE TYPE DataTypeW40 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW40 OF DataTypeW40;

CREATE CQ CSVStreamW40_to_DataStreamW40
INSERT INTO DataStreamW40
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW40;

CREATE JUMPING WINDOW JWa5uW40
OVER DataStreamW40
KEEP WITHIN 5 SECOND ON dateTime;

CREATE WACTIONSTORE WactionStoreW40 CONTEXT OF DataTypeW40
EVENT TYPES ( DataTypeW40 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWa5uW40_to_WactionStoreW40
INSERT INTO WactionStoreW40
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWa5uW40;

END FLOW DataProcessingW40;



END APPLICATION W40;

stop @appname@;
undeploy application @appname@;
DROP APPLICATION @appname@ CASCADE;
CREATE APPLICATION @appname@;

CREATE SOURCE @appname@_src USING databaseReader  (
  Username: '@@',
  Password: '@@',
  ConnectionURL: '@@',
  Tables: '@@',
  FetchSize: '100'
 )
OUTPUT TO @appname@_ss;

CREATE JUMPING WINDOW @appname@_win OVER @appname@_ss KEEP @winsize@ ROWS;

CREATE TYPE @appname@_MapType
    (   
       id INTEGER,
        name STRING,
        city  STRING
    );
    
CREATE EXTERNAL CACHE @appname@_cach (
  AdapterName: 'DatabaseReader',
    ConnectionURL: '@url@',
    UserName: '@uname@',
    Password: '@pwd@',
   Table: '@tablename@',
  FetchSize: 100,
  skipinvalid : @valid@,
  Columns: 'id,name,city',
  trimquote: false,
  KeyToMap: '@key@'
 )
 OF @appname@_MapType;
 
CREATE TYPE @appname@_MapTypenew
    (   id_t            INTEGER,
        name_t           STRING,
        city_t            STRING,
        id_c            INTEGER,
        name_c            STRING,
        city_c            STRING
    );
    
CREATE STREAM @appname@_JoinedData OF @appname@_MapTypenew;

CREATE CQ @appname@_JoinDataCQ
INSERT INTO @appname@_JoinedData
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_STRING(f.data[2]),
        z.id,
        z.name,
        z.city
FROM @appname@_win f, @appname@_cach z
where TO_INT(f.data[0]) = z.id
@Ex@;

CREATE TARGET @appname@_tgt USING DatabaseWriter
(
  ConnectionURL:'@@',
  Username:'@@',
  Password:'@@',
  BatchPolicy:'Eventcount:10000,Interval:1',
  CommitPolicy:'Interval:1,Eventcount:10000',
  Tables:'@@'
) 
INPUT FROM @appname@_JoinedData;

END APPLICATION @appname@;
deploy application @appname@;
start @appname@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@  RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @srcName@ USING OracleReader (
 Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@'
)
OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@ USING AzureEventHubWriter (
  SASKey:'@saasKey@',
  EventHubNamespace:'@eventhubNamespace@',
  ConsumerGroup:'@consumerGrp@',
  SASPolicyName:'RootManageSharedAccessKey',
  E1P:'true',
  OperationTimeout:'1m',
  ConnectionRetryPolicy:'Retries:0,RetryBackOff:1m',
  EventHubName:'@eventhub@',
  BatchPolicy:'Size:1000000,Interval:1m'
)
FORMAT USING JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  members: 'data',
  jsonobjectdelimiter: '\n' )
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP LngIS2noder.LongRunningISStreamApp;
UNDEPLOY APPLICATION LngIS2noder.LongRunningISStreamApp;
DROP APPLICATION LngIS2noder.LongRunningISStreamApp CASCADE;

CREATE APPLICATION LongRunningISStreamApp;

CREATE FLOW ISFLOW1;
----------------------------------------------------
CREATE source implicitSOurce USING StreamReader
(
   OutputType: 'LngIS2noder.Atm',
   noLimit: 'true',
   maxRows: 1000,
   iterationDelay: 50,
   StringSet: 'productID[001-002-003-004],stateID[AS-CA-WA-NY],currentDate[2014-2015]',
   NumberSet: 'productWeight[1-10]R,quantity[50.5-160.1]G,size[763872-4778823]L'
) OUTPUT TO CsvStream;


CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity double,
  size long,
  currentDate org.joda.time.DateTime);

END FLOW ISFLOW1;
----------------------------------------------------

CREATE FLOW ISFLOW2;

CREATE CACHE cache1 USING CsvReader(
  directory: '@TEST-DATA-PATH@',
  wildcard: 'ISdata.csv',
  header: false,
  columndelimiter: ',',
  trimquote: false
  ) QUERY (keytomap:'productID') OF Atm;


CREATE STREAM newStream OF Atm;


CREATE CQ newCQ
INSERT INTO newStream
SELECT data[0], data[1], TO_INT(data[2]), TO_DOUBLE(data[3]), TO_LONG(data[4]), TO_DATE(data[5]) FROM
CsvStream;

CREATE WINDOW win1
OVER newStream
KEEP 50 rows;


CREATE CQ newCQ2
INSERT INTO newStream2
SELECT productID as A , stateID AS B, productWeight AS C, quantity AS D, size AS E, currentDate AS F FROM
newStream;


CREATE CQ newCQ3
INSERT INTO newStream3 PARTITION BY A
SELECT A,B,C,D,E,F FROM newStream2 order by C,D
link source event;

CREATE CQ newCQ4
INSERT INTO newStream4
SELECT count(productID),currentDate FROM newStream ORDER BY currentDate
link source event;

CREATE CQ newCQ5
INSERT INTO newStream5
SELECT x.*, y.* from cache1 x, newStream y WHERE x.productweight > 6 ORDER BY x.currentDate;


CREATE WACTIONSTORE WS1 CONTEXT OF Atm
EVENT TYPES(Atm );

CREATE CQ newCQ6
INSERT INTO WS1
SELECT * FROM newStream WHERE productID = '001';

CREATE CQ newCQ7
INSERT INTO newStream6
SELECT aa.productID FROM WS1 [push] aa, cache1 bb;


CREATE CQ newCQ8
INSERT INTO newStream7
SELECT Sum(X.size) FROM (Select size from win1 where productweight > 5) X;


END FLOW ISFLOW2;
----------------------------------------------------

END APPLICATION LongRunningISStreamApp;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '',
  region: '')
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

END APPLICATION @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
  directory: '',
  positionByEOF: false,
  WildCard: '')
PARSE USING ParquetParser ()
OUTPUT TO @appname@Streams;

CREATE CQ @appname@CQ1
INSERT INTO @appname@out
SELECT convertParquetEventToWAEvent(o) FROM @appname@Streams o;

CREATE TARGET @filetarget@ USING Global.FileWriter (
  directory: '',
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000,Interval:30s',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  filename: '',
  DataEncryptionKeyPassphrase_encrypted: 'true',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  members: 'data',
  jsonobjectdelimiter: '\n' )
INPUT FROM @appname@out;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

STOP APPLICATION snow2pg;
UNDEPLOY APPLICATION snow2pg;
DROP APPLICATION snow2pg CASCADE;


CREATE OR REPLACE APPLICATION snow2pg;

CREATE OR REPLACE SOURCE snow_pg USING Global.ServiceNowReader (
  Mode: 'InitialLoad',
  ServiceNow.ConnectionTimeOut: 60,
  ServiceNow.MaxConnections: 20,
  ServiceNow.FetchSize: 10000,
  ThreadPoolCount: '10',
  ServiceNow.ConnectionRetries: 3,
  PollingInterval: '1',
  ClientSecret: '6Wa-cv`I7x',
  Password: '^Pre&$EMO%6O.e_{96h+$R?rJd,=[4Vt=K)Szh?6g<J9D3,3zs8R;hpZqh]-3?C&.u-@GvSakPXH1:2eygbBDI>ou-z#GjBw[u8x',
  ServiceNow.Tables: 'u_empl',
  UserName: 'snr',
  ClientID: 'ce4fd5af894a11103d2c5c3a8fe075e1',
  adapterName: 'ServiceNowReader',
  ServiceNow.BatchAPI: false,
  ServiceNow.ConnectionUrl: 'https://dev84954.service-now.com/' )
OUTPUT TO sn;

CREATE TARGET pg USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'w@ct10n',
  Tables: 'u_empl,u_empl ColumnMap(name=u_name,age=u_age,address=u_address,sys_id=sys_id)',
  ParallelThreads: '',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  CommitPolicy: 'EventCount:1000,Interval:60',
  StatementCacheSize: '50',
  Username: 'waction',
  DatabaseProviderType: 'Postgres',
  BatchPolicy: 'EventCount:1000,Interval:60',
  PreserveSourceTransactionBoundary: 'false' )
INPUT FROM sn;

END APPLICATION snow2pg;
deploy application snow2pg;
start snow2pg;

CREATE OR REPLACE PROPERTYVARIABLE RetryPolicy='timeOut=10,retryInterval=10,maxRetries=30';
CREATE OR REPLACE PROPERTYVARIABLE KafkaBrokerAddress='localhost:9099';
--CREATE OR REPLACE PROPERTYVARIABLE KafkaConfig='batch.size=1048576;linger.ms=30000';

STOP @WRITERAPPNAME@;
UNDEPLOY APPLICATION @WRITERAPPNAME@;
DROP APPLICATION @WRITERAPPNAME@ CASCADE;

CREATE APPLICATION @WRITERAPPNAME@ Recovery 5 second interval;
create flow @WRITERAPPNAME@AgentFlow;
CREATE SOURCE @WRITERAPPNAME@S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.oracle_kw_quiesce_simple_test%',
	FetchSize: '1',
	connectionRetryPolicy:'$RetryPolicy',
	DictionaryMode: onlineCatalog
)
OUTPUT TO @WRITERAPPNAME@SS;
end flow @WRITERAPPNAME@AgentFlow;
create flow @WRITERAPPNAME@serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;
create stream @WRITERAPPNAME@out_cq_select_SS_1 of global.waevent;

CREATE OR REPLACE CQ @WRITERAPPNAME@cq_select_SS1 
INSERT INTO @WRITERAPPNAME@out_cq_select_SS_1
select *
from @WRITERAPPNAME@ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_QUIESCE_SIMPLE_TEST';


create Target @WRITERAPPNAME@TARGET1 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@WRITERAPPNAME@_dsv_sync_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(COMMIT_TIMESTAMP)',
Mode:'sync',
KafkaConfig: 'batch.size=1048576;linger.ms=300000;')
FORMAT USING dsvFormatter ()
input from @WRITERAPPNAME@ss;

create Target @WRITERAPPNAME@TARGET2 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@WRITERAPPNAME@_json_sync_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(COMMIT_TIMESTAMP)',
Mode:'sync',
KafkaConfig: 'batch.size=1048576;linger.ms=300000;')
FORMAT USING jsonFormatter ()
input from @WRITERAPPNAME@out_cq_select_SS_1;

create Target @WRITERAPPNAME@TARGET3 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@WRITERAPPNAME@_avro_sync_CQ',
--ParallelThreads:'2',
PartitionKey:'@metadata(COMMIT_TIMESTAMP)',
Mode:'sync',
KafkaConfig: 'batch.size=1048576;linger.ms=300000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTestQuiesce_sync_CQ.avsc')
input from @WRITERAPPNAME@out_cq_select_SS_1;

end flow @WRITERAPPNAME@serverFlow;
end application @WRITERAPPNAME@;
@DEPLOYAPP@;
start @WRITERAPPNAME@;


stop application @READERAPPNAME@;
undeploy application @READERAPPNAME@;
drop application @READERAPPNAME@ cascade;
CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @READERAPPNAME@_DSV_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@WRITERAPPNAME@_dsv_sync_CQ',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO @READERAPPNAME@KafkaReaderStream1;

CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
rolloverpolicy:'filesize:500M',
filename:'@READERAPPNAME@_dsv_sync_CQ')
FORMAT USING DSVFormatter()
INPUT FROM @READERAPPNAME@KafkaReaderStream1;

CREATE SOURCE @READERAPPNAME@_JSON_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@WRITERAPPNAME@_json_sync_CQ',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO @READERAPPNAME@KafkaReaderStream2;

CREATE SOURCE @READERAPPNAME@_AVRO_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@WRITERAPPNAME@_avro_sync_CQ',
        startOffset:0          
)
PARSE USING AvroParser (
schemaFileName:'KafkaAvroTestQuiesce_sync_CQ.avsc')
OUTPUT TO @READERAPPNAME@KafkaReaderStream3;


end application @READERAPPNAME@;
deploy application @READERAPPNAME@;

STOP APPLICATION EH;
UNDEPLOY APPLICATION EH;
DROP APPLICATION EH CASCADE;
CREATE APPLICATION EH @Recovery@;
CREATE SOURCE s USING FileReader (
	directory:'Product/IntegrationTests/TestData/',
    WildCard:'posdata.csv',
	positionByEOF:false
	)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO ss;

CREATE TYPE userType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String
);

CREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;

CREATE CQ cq1
INSERT INTO userDefinedTypedStream
SELECT data[0],data[1],data[2], data[3],data[4]
FROM ss;

create Target t1 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	--Partitionkey:'@metadata(RecordOffset)',
	ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
jsonMemberDelimiter: '\n',
jsonobjectdelimiter: '\n',
EventsAsArrayOfJsonObjects: 'true')
input from ss;

create Target t2 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	--Partitionkey:'Col1',
	ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
jsonMemberDelimiter: '\n',
jsonobjectdelimiter: '\n',
EventsAsArrayOfJsonObjects: 'true')
input from userDefinedTypedStream;

END APPLICATION EH;
DEPLOY APPLICATION EH;
start application EH;

STOP APPLICATION ER;
UNDEPLOY APPLICATION ER;
DROP APPLICATION ER CASCADE;
CREATE APPLICATION ER;
CREATE SOURCE ER_S1 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING jsonParser (
)OUTPUT TO ER_SS1;


CREATE SOURCE ER_S2 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING jsonParser (
)OUTPUT TO ER_SS2;

create Type CustType1 
(writerdata com.fasterxml.jackson.databind.JsonNode
--TopicName java.lang.String,
--PartitionID java.lang.String
);

Create Stream datastream1 of CustType1;

create Type CustType2
(Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String
--TopicName String,
--PartitionID String
);

Create Stream datastream2 of CustType2;

CREATE CQ CustCQ1
INSERT INTO datastream1
SELECT data.data
--metadata.get("TopicName").toString() AS TopicName,
--metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS1;

CREATE CQ CustCQ2
INSERT INTO datastream2
SELECT 
data.get('Col1').textValue(),
data.get('Col2').textValue(),
data.get('Col3').textValue(),
data.get('Col4').textValue(),
data.get('Col5').textValue()
--metadata.get("TopicName").toString() AS TopicName,
--metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS2;

create Target ER_t1 using FileWriter (
filename:'FT1_5L_JSON_RT',
directory:'FEATURE-DIR/logs/',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from datastream1;

create Target ER_t2 using FileWriter (
filename:'FT2_5L_JSON_RT',
directory:'FEATURE-DIR/logs/',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from datastream2;
end application ER;
deploy application ER;

use PosTester;
alter application PosApp;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvStream;

end application PosApp;

alter application PosApp recompile;

use PosTester;
alter application PosApp;

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

end application PosApp;

alter application PosApp recompile;

stop APPLICATION RollOverTester.DSV;

UNDEPLOY APPLICATION RollOverTester.DSV;
DROP APPLICATION RollOverTester.DSV CASCADE;
CREATE APPLICATION DSV;

create source JsonSource using FileReader (
        directory:'@TEST-DATA-PATH@',
        WildCard:'TextInput.txt',
        positionByEOF:false,
        charset:'UTF-8'
)
 PARSE USING JSONParser ( 
  eventType: ''
 ) 
OUTPUT TO activeguard_rawstream;


CREATE OR REPLACE TYPE activeguard_parsed (
	-- Enriched, or not in AGI left navigation
	hash_key String KEY,
	client_id Long,
	event_type_id Long,
	type_code String,
	rule_id Long,
	rule_name String,
	rule_action String,
	sce_id  Long,
	message String,
	ref_list_match String,
	event_date DateTime,
	log_count Long,
	
	-- As seen in AGI
	-- frequently used, but not duplicated
	received_date DateTime,
	event_sub_source String,
					
	-- database
	sol_action String,
	comment String,
	comment_text String,
	database_name String,
	database_user_name String,
	entry_id String,
	obj_name String,
	os_user_name String,
	owner String,
	priv_used String,
	return_code String,
	session_id String,
	
	-- geo
	destination_country_code String,
	source_country_code String,
	
	-- network
	data_size String,
	destination String,
	destination_object String,
	destination_port String,
	sol_domain String,
	event_manager String,
	event_path String,
	event_source String,
	msg_no String,
	protocol_id String,
	service String,
	sol_source String,
	source_mac_address String,
	source_object String,
	source_port String,
	status_code String,
	target_path String,
	
	-- other
	command String,
	
	-- security
	affected_file String,
	object_action String,
	object_action_result String,
	object_id String,
	object_name String,
	object_path String,
	process_name String,
	
	-- user
	caller_user_name String,
	logon_type String,
	target_user_name String,
	user_agent String,
	user_name String,
	workstation String,
	
	-- web
	web_file String,
	web_method String,
	web_page String,
	web_protocol String
);


CREATE OR REPLACE STREAM activeguard_stream OF activeguard_parsed;

CREATE CQ Parseactiveguard_stream 
INSERT INTO activeguard_stream
SELECT

	-- Enriched, or not in AGI left navigation
	data.get('hash_key').textValue() as hash_key,
	data.get('client_id').longValue() as client_id,
	data.get('event_type_id').longValue() as event_type_id,
	data.get('type_code').textValue() as type_code,
	data.get('rule_id').longValue() as rule_id,
	data.get('rule_name').textValue() as rule_name,
	data.get('rule_action').textValue() as rule_action,
	data.get('sce_id').longValue() as sce_id,
	data.get('message').textValue() as message,
	data.get('ref_list_match').textValue() as ref_list_match,
	TO_DATE(data.get('event_date').textValue()) as event_date,
	data.get('log_count').longValue() as log_count,
	
	-- As seen in AGI
	-- frequently used, but not duplicated
	TO_DATE(data.get('received_date').textValue()) as received_date,
	data.get('event_sub_source').textValue() as event_sub_source,
					
	-- database
	data.get('sol_action').textValue() as sol_action,
	data.get('comment').textValue() as comment,
	data.get('comment_text').textValue() as comment_text,
	data.get('database_name').textValue() as database_name,
	data.get('database_user_name').textValue() as database_user_name,
	data.get('entry_id').textValue() as entry_id,
	data.get('obj_name').textValue() as obj_name,
	data.get('os_user_name').textValue() as os_user_name,
	data.get('owner').textValue() as owner,
	data.get('priv_used').textValue() as priv_used,
	data.get('return_code').textValue() as return_code,
	data.get('session_id').textValue() as session_id,
	
	-- geo
	data.get('destination_country_code').textValue() as destination_country_code,
	data.get('source_country_code').textValue() as source_country_code,
	
	-- network
	data.get('data_size').textValue() as data_size,
	data.get('destination').textValue() as destination,
	data.get('destination_object').textValue() as destination_object,
	data.get('destination_port').textValue() as destination_port,
	data.get('sol_domain').textValue() as sol_domain,
	data.get('event_manager').textValue() as event_manager,
	data.get('event_path').textValue() as event_path,
	data.get('event_source').textValue() as event_source,
	data.get('msg_no').textValue() as msg_no,
	data.get('protocol_id').textValue() as protocol_id,
	data.get('service').textValue() as service,
	data.get('sol_source').textValue() as sol_source,
	data.get('source_mac_address').textValue() as source_mac_address,
	data.get('source_object').textValue() as source_object,
	data.get('source_port').textValue() as source_port,
	data.get('status_code').textValue() as status_code,
	data.get('target_path').textValue() as target_path,
	
	-- other
	data.get('command').textValue() as command,
	
	-- security
	data.get('affected_file').textValue() as affected_file,
	data.get('object_action').textValue() as object_action,
	data.get('object_action_result').textValue() as object_action_result,
	data.get('object_id').textValue() as object_id,
	data.get('object_name').textValue() as object_name,
	data.get('object_path').textValue() as object_path,
	data.get('process_name').textValue() as process_name,
	
	-- user
	data.get('caller_user_name').textValue() as caller_user_name,
	data.get('logon_type').textValue() as logon_type,
	data.get('target_user_name').textValue() as target_user_name,
	data.get('user_agent').textValue() as user_agent,
	data.get('user_name').textValue() as user_name,
	data.get('workstation').textValue() as workstation,
	
	-- web
	data.get('web_file').textValue() as web_file,
	data.get('web_method').textValue() as web_method,
	data.get('web_page').textValue() as web_page,
	data.get('web_protocol').textValue() as web_protocol
	
FROM activeguard_rawstream;


CREATE TARGET alertf USING FileWriter ( directory: '@FEATURE-DIR@/logs', filename: 'TestOutput.txt', rolloverpolicy:'EventCount:10000,Interval:30s' )
 FORMAT USING JSONFormatter (
		members:'hash_key,client_id,event_type_id,type_code,rule_id,rule_name,rule_action,sce_id,message,ref_list_match,event_date,log_count,received_date,event_sub_source,sol_action,comment,comment_text,database_name,database_user_name,entry_id,obj_name,os_user_name,owner,priv_used,return_code,session_id,destination_country_code,source_country_code,data_size,destination,destination_object,destination_port,sol_domain,event_manager,event_path,event_source,msg_no,protocol_id,service,sol_source,source_mac_address,source_object,source_port,status_code,target_path,command,affected_file,object_action,object_action_result,object_id,object_name,object_path,process_name,caller_user_name,logon_type,target_user_name,user_agent,user_name,workstation,web_file,web_method,web_page,web_protocol',
 		rowdelimiter:'\n'
 )
--format using dsvformatter()
 INPUT FROM activeguard_stream;

 
END APPLICATION DSV;

stop application iteratortester.iteratorapp;
undeploy application iteratortester.iteratorapp;
drop application iteratortester.iteratorapp cascade;

CREATE APPLICATION iteratorapp;

create flow sourceFlow;

CREATE SOURCE JSONAccessLogSource USING FileReader(
  directory:'@TEST-DATA-PATH@',
  wildcard:'iterator.json',
  positionByEOF:'false'
)
parse using JSONParser (
) OUTPUT TO jsonSourceStream;

end flow sourceflow;

-- ******ARRAY LIST****** --
create flow processFlow;

create type cacheType (bankID string key, bankName string);
CREATE cache dsvcache USING FileReader (
directory:'@TEST-DATA-PATH@',
wildcard:'banks.csv',
blocksize: 10240,
positionByEOF:false
)
PARSE USING DSVParser (
header:No,
trimquote:false
) QUERY (keytomap:'bankID') OF cacheType;

CREATE TYPE listType (id integer KEY, bankname string, lst java.util.List);
CREATE TYPE listStoreType (id integer KEY, bankname string, lst java.util.List, lstoflst java.util.List);

CREATE STREAM listStream of listType partition by bankname;

CREATE JUMPING WINDOW listJWindow
OVER listStream
keep 3 rows;

CREATE WINDOW listWindow
OVER listStream
keep 3 rows;

CREATE WACTIONSTORE listStore CONTEXT OF listStoreType EVENT TYPES (listStoreType ) 
@PERSIST-TYPE@

create cq updatelistStream
insert into listStream
select TO_INT(bankID), bankName, makelist(bankID,bankName) as lst 
from dsvcache;

create cq updatelistStore 
insert into listStore
select ID, bankName, lst, makelist(lst,lst) from listStream
LINK SOURCE EVENT;

create stream listTargetStream( str String);

create cq updateListTarget
insert into listTargetStream
select itr
from listStream, iterator(listStream.lst) itr order by cast(itr as java.lang.Comparable);

-- CREATE TARGET listout USING SYSOUT(name:"list") input from listStream;

-- ******JsonNode****** --

--CREATE TYPE jsonType (id integer KEY,  lst com.fasterxml.jackson.databind.JsonNode);
CREATE TYPE jsonType (int integer, bankname string, lst com.fasterxml.jackson.databind.JsonNode key);

CREATE STREAM jsonStream of jsonType partition by bankname;

CREATE JUMPING WINDOW jsonJWindow
OVER jsonStream
keep 3 rows
partition by bankname;

CREATE WINDOW jsonWindow
OVER jsonStream
keep 3 rows;

CREATE WACTIONSTORE jsonStore CONTEXT OF jsonType EVENT TYPES (jsonType ) 
@PERSIST-TYPE@

create cq updateJsonStream
insert into jsonStream
--select TO_INT(MATCH(data[0], '.*\\\\s([0-9]+)')) , makeJSON('[{"x":"a"},{"x":"b","y":"c"},{"y":"c"}]') as lst 
select TO_INT(y.bankID), y.bankName, x.data 
from JSONSourceStream x, dsvcache y;

create cq updateJsonStore 
insert into jsonStore
select * from jsonStream
LINK SOURCE EVENT;

create stream jsonTargetStream( str String);

create cq updateJsonTarget
insert into jsonTargetStream
select to_string(itr.StringJSON)
from jsonStream, iterator(jsonStream.lst) itr order by to_string(itr.StringJSON);
CREATE TARGET jsonout USING SYSOUT(name:"jlist") input from jsonStream;

end flow processflow; 

end application iteratorapp;

drop application dropDeployApp cascade;
create application dropDeployApp;
deploy application admin.dropDeployApp on any in default;
drop application dropDeployApp;

STOP application AlterTester.DSV;
undeploy application AlterTester.DSV;
drop application AlterTester.DSV cascade;


create application DSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;


end application DSV;

create application KinesisTest 
 RECOVERY 10 SECOND INTERVAL
;
create source CSVSource using FileReader (
	directory:'/home/dz/src/product/Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  companyName String,
  merID String,
  primNum String,
  posDataCode int,
  dateTime String,
  expDate String,
  curCode String,
  Amnt String,
  termId String,
  zip String,
  city String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],data[2],TO_INT(data[3]),
       data[4],data[5],data[6],
       data[7],data[8],
       data[9],data[10]
FROM CsvStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM'
)
format using DSVFormatter (
)
input from TypedCSVStream;
end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;

drop propertyset OrcToSpnPlatfm_App_KafkaPropset;
drop stream  OrcToSpnPlatfm_Stream CASCADE;


--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;

CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNEC4TION_URL@',
  Tables: '$table1',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING SpannerWriter (
  Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'QATEST.ORCTOSPNPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING SpannerWriter (
  Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
 )
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '$table2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@4',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING SpannerWriter  (
  Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'QATEST.ORCTOSPNPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING SpannerWriter  (
  Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
 )
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;

END APPLICATION @APPNAME@2;

stop APPLICATION @AppName@;
Undeploy APPLICATION @AppName@;
drop APPLICATION @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @AgentFlow@;

CREATE OR REPLACE SOURCE @SourceName@ USING Global.MSJet (
  Tables: 'dbo.compsrc',
  username: 'qatest',
  DatabaseName: 'qatest',
  FetchTransactionMetadata: true,
  filterTransactionBoundaries: true,
  compression: false,
   Mode: '@mode@',
  ConnectionURL: '@ConnectionURL@',
  CommittedTransactions: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
  SendBeforeImage: true,
  password: 'w3b@ct10n' )
OUTPUT TO @StreamName@;
END FLOW @AgentFlow@;

CREATE TARGET @SysTarget@ USING Global.SysOut (
  name: 'MS_CDC_SYSOUT' )
INPUT FROM @StreamName@;

CREATE FLOW @ServerFlow@;

CREATE TARGET @TargetName@ USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ParallelThreads: '',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: '@ConnectionURL@',
  StatementCacheSize: '50',
  DatabaseProviderType: 'Default',
  Username: 'qatest',
  Tables: 'dbo.compsrc,dbo.comptar',
  Password: 'w3b@ct10n',
  PreserveSourceTransactionBoundary: 'false',
  BatchPolicy: 'EventCount:1,Interval:60' )
INPUT FROM @StreamName@;

create Target @TargetName@_File using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'',
  rolloverpolicy:'EventCount:10000,Interval:30s'
)
format using DSVFormatter (

)
INPUT FROM @StreamName@;

END FLOW @ServerFlow@;

END APPLICATION @AppName@;
DEPLOY APPLICATION @AppName@ with @AgentFlow@ in AGENTS ,@ServerFlow@ on any in default;
START APPLICATION @AppName@;

create application HPTippingLog;
create source DHCPLogSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'hp*',
	charset:'UTF-8',
	positionByEOF:false 
) PARSE USING HPTippingPointLogParser (
) OUTPUT TO HPLogStream;
create Target HPDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/hp_log') input from HPLogStream;
end application HPTippingLog;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW ;

 create flow myagentflow;

CREATE OR REPLACE SOURCE DBSource USING DatabaseReader  ( 
  Username: '@SOURCE_USER@',
  Password_encrypted: false,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Query: "SELECT * FROM QATEST.OracToCql_alldatatypes",
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SOURCE_PASS@'
 ) 
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource2 USING DatabaseReader  ( 
  Username: '@SOURCE_USER@',
  Password_encrypted: false,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Query: "SELECT * FROM QATEST.OracToCql_alldatatypes2",
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SOURCE_PASS@'
 ) 
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource3 USING DatabaseReader  ( 
  Username: '@SOURCE_USER@',
  Password_encrypted: false,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Query: "SELECT * FROM QATEST.OracToCql_alldatatypes3",
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SOURCE_PASS@'
 ) 
OUTPUT TO Oracle_ChangeDataStream;

end flow myagentflow;

create flow myserverflow;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  Tables: '',
  Password: 'cassandra',
  Password_encrypted: false
 ) 
INPUT FROM Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

end flow myserverflow;

END APPLICATION DBRTOCW;

deploy application DBRTOCW on ALL in default with myagentflow on all in Agents, myserverflow on all in  default;

start DBRTOCW;

UNDEPLOY APPLICATION RollOverTester.DSV;
DROP APPLICATION RollOverTester.DSV CASCADE;
CREATE APPLICATION DSV;


create source JsonSource using FileReader (
        directory:'@TEST-DATA-PATH@',
        WildCard:'TestInput.txt',
        positionByEOF:false,
        charset:'UTF-8'
)
 PARSE USING JSONParser (
  eventType: ''
 )
OUTPUT TO activeguard_rawstream;


CREATE OR REPLACE TYPE activeguard_parsed (
	-- Enriched, or not in AGI left navigation
	hash_key String KEY,
	client_id Long,
	event_type_id Long,
	type_code String,
	rule_id Long,
	rule_name String,
	rule_action String,
	sce_id  Long,
	message String,
	ref_list_match String,
	event_date DateTime,
	log_count Long,

	-- As seen in AGI
	-- frequently used, but not duplicated
	received_date DateTime,
	event_sub_source String,

	-- database
	sol_action String,
	comment String,
	comment_text String,
	database_name String,
	database_user_name String,
	entry_id String,
	obj_name String,
	os_user_name String,
	owner String,
	priv_used String,
	return_code String,
	session_id String,

	-- geo
	destination_country_code String,
	source_country_code String,

	-- network
	data_size String,
	destination String,
	destination_object String,
	destination_port String,
	sol_domain String,
	event_manager String,
	event_path String,
	event_source String,
	msg_no String,
	protocol_id String,
	service String,
	sol_source String,
	source_mac_address String,
	source_object String,
	source_port String,
	status_code String,
	target_path String,

	-- other
	command String,

	-- security
	affected_file String,
	object_action String,
	object_action_result String,
	object_id String,
	object_name String,
	object_path String,
	process_name String,

	-- user
	caller_user_name String,
	logon_type String,
	target_user_name String,
	user_agent String,
	user_name String,
	workstation String,

	-- web
	web_file String,
	web_method String,
	web_page String,
	web_protocol String
);


CREATE OR REPLACE STREAM activeguard_stream OF activeguard_parsed;

CREATE CQ Parseactiveguard_stream
INSERT INTO activeguard_stream
SELECT

	-- Enriched, or not in AGI left navigation
	data.get('hash_key').textValue() as hash_key,
	data.get('client_id').longValue() as client_id,
	data.get('event_type_id').longValue() as event_type_id,
	data.get('type_code').textValue() as type_code,
	data.get('rule_id').longValue() as rule_id,
	data.get('rule_name').textValue() as rule_name,
	data.get('rule_action').textValue() as rule_action,
	data.get('sce_id').longValue() as sce_id,
	data.get('message').textValue() as message,
	data.get('ref_list_match').textValue() as ref_list_match,
	TO_DATE(data.get('event_date').textValue()) as event_date,
	data.get('log_count').longValue() as log_count,

	-- As seen in AGI
	-- frequently used, but not duplicated
	TO_DATE(data.get('received_date').textValue()) as received_date,
	data.get('event_sub_source').textValue() as event_sub_source,

	-- database
	data.get('sol_action').textValue() as sol_action,
	data.get('comment').textValue() as comment,
	data.get('comment_text').textValue() as comment_text,
	data.get('database_name').textValue() as database_name,
	data.get('database_user_name').textValue() as database_user_name,
	data.get('entry_id').textValue() as entry_id,
	data.get('obj_name').textValue() as obj_name,
	data.get('os_user_name').textValue() as os_user_name,
	data.get('owner').textValue() as owner,
	data.get('priv_used').textValue() as priv_used,
	data.get('return_code').textValue() as return_code,
	data.get('session_id').textValue() as session_id,

	-- geo
	data.get('destination_country_code').textValue() as destination_country_code,
	data.get('source_country_code').textValue() as source_country_code,

	-- network
	data.get('data_size').textValue() as data_size,
	data.get('destination').textValue() as destination,
	data.get('destination_object').textValue() as destination_object,
	data.get('destination_port').textValue() as destination_port,
	data.get('sol_domain').textValue() as sol_domain,
	data.get('event_manager').textValue() as event_manager,
	data.get('event_path').textValue() as event_path,
	data.get('event_source').textValue() as event_source,
	data.get('msg_no').textValue() as msg_no,
	data.get('protocol_id').textValue() as protocol_id,
	data.get('service').textValue() as service,
	data.get('sol_source').textValue() as sol_source,
	data.get('source_mac_address').textValue() as source_mac_address,
	data.get('source_object').textValue() as source_object,
	data.get('source_port').textValue() as source_port,
	data.get('status_code').textValue() as status_code,
	data.get('target_path').textValue() as target_path,

	-- other
	data.get('command').textValue() as command,

	-- security
	data.get('affected_file').textValue() as affected_file,
	data.get('object_action').textValue() as object_action,
	data.get('object_action_result').textValue() as object_action_result,
	data.get('object_id').textValue() as object_id,
	data.get('object_name').textValue() as object_name,
	data.get('object_path').textValue() as object_path,
	data.get('process_name').textValue() as process_name,

	-- user
	data.get('caller_user_name').textValue() as caller_user_name,
	data.get('logon_type').textValue() as logon_type,
	data.get('target_user_name').textValue() as target_user_name,
	data.get('user_agent').textValue() as user_agent,
	data.get('user_name').textValue() as user_name,
	data.get('workstation').textValue() as workstation,

	-- web
	data.get('web_file').textValue() as web_file,
	data.get('web_method').textValue() as web_method,
	data.get('web_page').textValue() as web_page,
	data.get('web_protocol').textValue() as web_protocol

FROM activeguard_rawstream;



CREATE TARGET alertf USING FileWriter (
  filename: 'JParser',
  flushinterval: '0',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy: 'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00'
 )
 FORMAT USING JSONFormatter (
		members:'hash_key,client_id,event_type_id,type_code,rule_id,rule_name,rule_action,sce_id,message,ref_list_match,event_date,log_count,received_date,event_sub_source,sol_action,comment,comment_text,database_name,database_user_name,entry_id,obj_name,os_user_name,owner,priv_used,return_code,session_id,destination_country_code,source_country_code,data_size,destination,destination_object,destination_port,sol_domain,event_manager,event_path,event_source,msg_no,protocol_id,service,sol_source,source_mac_address,source_object,source_port,status_code,target_path,command,affected_file,object_action,object_action_result,object_id,object_name,object_path,process_name,caller_user_name,logon_type,target_user_name,user_agent,user_name,workstation,web_file,web_method,web_page,web_protocol',
 		rowdelimiter:'\n'
 )
INPUT FROM activeguard_stream;

create Target z using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/Target_JParser_actual.log') input from activeguard_stream;


END APPLICATION DSV;

stop application APP_KAFKA_DATASOURCES;
undeploy application APP_KAFKA_DATASOURCES;
alter application APP_KAFKA_DATASOURCES;

CREATE OR REPLACE SOURCE SRC_FR_KAFKA_HOURLYTOTALS USING Global.FileReader (
  adapterName: 'FileReader',
  rolloverstyle: 'Default',
  blocksize: 64,
  skipbom: true,
  wildcard: 'kafka_hourly_total_20210316.txt',
  directory: '@confDir@',
  includesubdirectories: false,
  positionbyeof: false ) 
PARSE USING Global.DSVParser (
  trimwhitespace: false,
  linenumber: '-1',
  columndelimittill: '-1',
  trimquote: true,
  ignoreemptycolumn: false,
  parserName: 'DSVParser',
  quoteset: '\"',
  handler: 'com.webaction.proc.DSVParser_1_0',
  charset: 'UTF-8',
  columndelimiter: ':',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  separator: ',',
  header: false,
  blockascompleterecord: false,
  rowdelimiter: '\n',
  nocolumndelimiter: false,
  headerlineno: 0 )
OUTPUT TO STREAM_SRC_FR_KAFKA_HOURLYTOTALS;

alter application APP_KAFKA_DATASOURCES recompile;

CREATE OR REPLACE PROPERTYVARIABLE RetryPolicy='timeOut=00,retryInterval=1,maxRetries=3';
CREATE OR REPLACE PROPERTYVARIABLE KafkaBrokerAddress='localhost:9099';
CREATE OR REPLACE PROPERTYVARIABLE KafkaConfig='request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;';

STOP @WRITERAPPNAME@@RECOVSTATUS@;
UNDEPLOY APPLICATION @WRITERAPPNAME@@RECOVSTATUS@;
DROP APPLICATION @WRITERAPPNAME@@RECOVSTATUS@ CASCADE;

CREATE APPLICATION @WRITERAPPNAME@@RECOVSTATUS@ @Recovery@;
create flow AgentFlow;
CREATE SOURCE S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.oracle_kw_test%',
	FetchSize: '1',
	connectionRetryPolicy:'$RetryPolicy'
)
OUTPUT TO SS;
end flow AgentFlow;
create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;
create stream out_cq_select_SS_1 of global.waevent;
create stream out_cq_select_SS_2 of global.waevent;
create stream out_cq_select_SS_3 of global.waevent;
create stream out_cq_select_SS_4 of global.waevent;
create stream out_cq_select_SS_5 of global.waevent;
create stream out_cq_select_SS_6 of global.waevent;

CREATE OR REPLACE CQ cq_select_SS1 
INSERT INTO out_cq_select_SS_1
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST1';

CREATE OR REPLACE CQ cq_select_SS2 
INSERT INTO out_cq_select_SS_2
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST2';

CREATE OR REPLACE CQ cq_select_SS3 
INSERT INTO out_cq_select_SS_3
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST3';

CREATE OR REPLACE CQ cq_select_SS4 
INSERT INTO out_cq_select_SS_4
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST4';

CREATE OR REPLACE CQ cq_select_SS5 
INSERT INTO out_cq_select_SS_5
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST5';

CREATE OR REPLACE CQ cq_select_SS6 
INSERT INTO out_cq_select_SS_6
select *
from ss o
where TO_STRING(meta(o,'OperationType'))='DML' and  TO_STRING(meta(o,'TableName'))='QATEST.ORACLE_KW_TEST6';

create Target TARGET1 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_sync_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: '$KafkaConfig')
FORMAT USING dsvFormatter ()
input from out_cq_select_SS_1;

create Target TARGET2 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_sync_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: '$KafkaConfig')
FORMAT USING jsonFormatter ()
input from out_cq_select_SS_2;

create Target TARGET3 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_sync_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: '$KafkaConfig')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_sync_CQ.avsc')
input from out_cq_select_SS_3;

create Target TARGET4 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_Async_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(ROWID)',
Mode:'Async',
KafkaConfig: '$KafkaConfig')
FORMAT USING dsvFormatter ()
input from out_cq_select_SS_4;

create Target TARGET5 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_Async_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(ROWID)',
Mode:'Async',
KafkaConfig: '$KafkaConfig')
FORMAT USING jsonFormatter ()
input from out_cq_select_SS_5;

create Target TARGET6 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_Async_CQ',
ParallelThreads:'2',
PartitionKey:'@metadata(ROWID)',
Mode:'Async',
KafkaConfig: '$KafkaConfig')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_Async_CQ.avsc')
input from out_cq_select_SS_6;

create Target TARGET7 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_sync',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: '$KafkaConfig')
FORMAT USING dsvFormatter ()
input from ss;

create Target TARGET8 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_sync',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: '$KafkaConfig')
FORMAT USING jsonFormatter ()
input from ss;

create Target TARGET9 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_sync',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'sync',
KafkaConfig: '$KafkaConfig')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_sync.avsc')
input from ss;

create Target TARGET10 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_dsv_Async',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'Async',
KafkaConfig: '$KafkaConfig')
FORMAT USING dsvFormatter ()
input from ss;

create Target TARGET11 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_json_Async',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'Async',
KafkaConfig: '$KafkaConfig')
FORMAT USING jsonFormatter ()
input from ss;

create Target TARGET12 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'@SOURCE@_avro_Async',
ParallelThreads:'2',
PartitionKey:'@metadata(TableName)',
Mode:'Async',
KafkaConfig: '$KafkaConfig')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest_Async.avsc')
input from ss;

end flow serverFlow;
end application @WRITERAPPNAME@@RECOVSTATUS@;
deploy application @WRITERAPPNAME@@RECOVSTATUS@;
start @WRITERAPPNAME@@RECOVSTATUS@;



stop application @READERAPPNAME@@RECOVSTATUS@;
undeploy application @READERAPPNAME@@RECOVSTATUS@;
drop application @READERAPPNAME@@RECOVSTATUS@ cascade;
CREATE APPLICATION @READERAPPNAME@@RECOVSTATUS@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @SOURCE@_DSV_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_sync_CQ',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;

CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
rolloverpolicy:'filesize:500M',
filename:'@READERAPPNAME@@RECOVSTATUS@_@SOURCE@_dsv_sync_CQ')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream1;

CREATE SOURCE @SOURCE@_JSON_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_sync_CQ',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;

CREATE SOURCE @SOURCE@_AVRO_sync_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_sync_CQ',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_sync_CQ.avsc'
)
OUTPUT TO KafkaReaderStream3;

CREATE SOURCE @SOURCE@_DSV_Async_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_Async_CQ',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream4;

CREATE SOURCE @SOURCE@_JSON_Async_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_Async_CQ',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream5;

CREATE SOURCE @SOURCE@_AVRO_Async_CQ USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_Async_CQ',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_Async_CQ.avsc'
)
OUTPUT TO KafkaReaderStream6;

CREATE SOURCE @SOURCE@_DSV_sync USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_sync',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream7;

CREATE TARGET kafkaDumpDSV_rawstream USING FileWriter(
name:kafkaOuputDSV_rawstream,
rolloverpolicy:'filesize:500M',
filename:'@READERAPPNAME@@RECOVSTATUS@_@SOURCE@_dsv_sync')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream7;

CREATE SOURCE @SOURCE@_JSON_sync USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_sync',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream8;

CREATE SOURCE @SOURCE@_AVRO_sync USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_sync',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_sync.avsc'
)
OUTPUT TO KafkaReaderStream9;

CREATE SOURCE @SOURCE@_DSV_Async USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_dsv_Async',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream10;

CREATE SOURCE @SOURCE@_JSON_Async USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_json_Async',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream11;

CREATE SOURCE @SOURCE@_AVRO_Async USING KafkaReader VERSION '2.1.0' (
        brokerAddress:'$KafkaBrokerAddress',
        Topic:'@SOURCE@_avro_Async',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest_Async.avsc'
)
OUTPUT TO KafkaReaderStream12;

end application @READERAPPNAME@@RECOVSTATUS@;
deploy application @READERAPPNAME@@RECOVSTATUS@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ Recovery 5 second interval;

create stream @APPNAME@_UserdataStream of Global.WAEvent;

create type @APPNAME@_Order_type(
id int,
order_id int,
zipcode int,
category String,
tablename string
);

CREATE OR REPLACE SOURCE @APPNAME@S1 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.order_%'
)
OUTPUT TO @APPNAME@_OrdersStream;

CREATE OR REPLACE SOURCE @APPNAME@S2 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_2',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.second_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream2;

CREATE OR REPLACE SOURCE @APPNAME@S3 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_3',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.third_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream3;

CREATE OR REPLACE SOURCE @APPNAME@S4 USING PostgreSQLReader
(
  ReplicationSlotName: 'test_slot_4',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.fourth_order_%'
)
OUTPUT TO @APPNAME@_OrdersStream4;


Create CQ @APPNAME@_CQUser
insert into @APPNAME@_UserdataStream
select 
putuserdata (data,'Fileowner','FIRST_ORDER') from @APPNAME@_OrdersStream data;


Create CQ @APPNAME@_CQUser2
insert into @APPNAME@_UserdataStream
select 
putuserdata (data2,'Fileowner','SECOND_ORDER') from @APPNAME@_OrdersStream2 data2;


Create CQ @APPNAME@_CQUser3
insert into @APPNAME@_UserdataStream
select 
putuserdata (data3,'Fileowner','THIRD_ORDER') from @APPNAME@_OrdersStream3 data3;


Create CQ @APPNAME@_CQUser4
insert into @APPNAME@_UserdataStream
select 
putuserdata (data4,'Fileowner','FOURTH_ORDER') from @APPNAME@_OrdersStream4 data4;

create stream @APPNAME@_OrderTypedStream1 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream2 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream3 of @APPNAME@_Order_type;
create stream @APPNAME@_OrderTypedStream4 of @APPNAME@_Order_type;

CREATE CQ @APPNAME@_fin_cq
INSERT INTO @APPNAME@_OrderTypedStream1
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FIRST_ORDER';

CREATE CQ @APPNAME@_fin_cq2
INSERT INTO @APPNAME@_OrderTypedStream2
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'SECOND_ORDER';

CREATE CQ @APPNAME@_fin_cq3
INSERT INTO @APPNAME@_OrderTypedStream3
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'THIRD_ORDER';

CREATE CQ @APPNAME@_fin_cq4
INSERT INTO @APPNAME@_OrderTypedStream4
SELECT to_int(data[0]),
to_int(data[1]),
to_int(data[2]),
USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString(),
META(@APPNAME@_UserdataStream,'TableName').toString()
FROM @APPNAME@_UserdataStream where USERDATA(@APPNAME@_UserdataStream,'Fileowner').toString()== 'FOURTH_ORDER';


create Target @APPNAME@T1 using ADLSGen2Writer(
        filename:'event_data.csv',
        directory:'%category%/%tablename%',
       	accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        uploadpolicy:'eventcount:8,interval:20s'
)
format using DSVFormatter (
    header:'true'
)
input from @APPNAME@_OrderTypedStream1; 

create Target @APPNAME@T2 using ADLSGen2Writer(
        filename:'event_data.xml',
        directory:'%category%/%tablename%',
		accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
   		uploadpolicy:'eventcount:8,interval:20s'
)
format using XMLFormatter (
  elementtuple: 'Order_id:id:order_id:zipcode:category:text=tablename',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from @APPNAME@_OrderTypedStream2; 

create Target @APPNAME@T3 using ADLSGen2Writer(
        filename:'event_data.avro',
        directory:'%category%/%tablename%',
		accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
  		uploadpolicy:'eventcount:8,interval:20s'
)
format using AvroFormatter (
  formatAs: 'Default',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA-FILE@'
)
input from @APPNAME@_OrderTypedStream3; 


create Target @APPNAME@T4 using ADLSGen2Writer(
        filename:'event_data.json',
        directory:'%category%/%tablename%',
        accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
  		uploadpolicy:'eventcount:8,interval:20s'
)
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@_OrderTypedStream4;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

Create Source @SOURCE_NAME@ Using MSJet
(
 Username:'@READER-UNAME@',
 Password:'@READER-PASSWORD@',
 DatabaseName:'@DB_NAME@',
 ConnectionURL:'@CONN_URL@',
 Tables:@WATABLES@,
 compression:'@COMP@'
)
OUTPUT TO @STREAM@;

use PosTester;
DROP STREAM MerchantTxRateOnlyStream;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
filename:'Events',
directory:'@FEATURE-DIR@/logs/',
rolloverpolicy:'eventcount:200,sequence:00'
)
format using DSVFormatter (

)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetEventCount_actual.log') input from TypedCSVStream;

end application DSV;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING YugabyteReader  (
 ReplicationSlotName: 'slotname',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 )
OUTPUT TO @STREAM@ ;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.OJet (
  PrimaryDatabasePassword: '@PRIMARY_PASSWORD@',
  ConnectionURL: '@DOWNSTREAM_URL@',
  PrimaryDatabaseUsername: '@PRIMARY_USER@',
  Password: '@DOWNSTREAM_PASSWORD@',
  DownstreamCaptureMode: 'REAL_TIME',
  DownstreamCapture: true,
  PrimaryDatabaseConnectionURL: '@PRIMARY_URL@',
  Tables: '@SOURCE_TABLES@',
  CDDLCapture: true,
  CDDLAction: 'Process',
  Username: '@DOWNSTREAM_USER@' )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET_SYS@ USING Global.SysOut (
  name: 'Out' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter (
  ConnectionURL: '@TARGET_URL@',
  Username: '@TARGET_USER@',
  Password: '@TARGET_PASSWORD@',
  CheckPointTable: 'CHKPOINT',
  CommitPolicy: 'EventCount:1',
  Tables: '@TARGET_TABLES@',
  BatchPolicy: 'EventCount:1' )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

Stop Oracle_Oracle_IRLogWriterLogWriter;
Undeploy application Oracle_Oracle_IRLogWriterLogWriter;
drop application Oracle_Oracle_IRLogWriterLogWriter cascade;

CREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;

CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( 
 
  FetchSize: 5000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=t1',
 startPosition: '%=-1',
  PollingInterval: '20sec'
  )
  OUTPUT TO data_stream;
create target AzureSQLDWHTarget using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:0,interval:0s'
) INPUT FROM data_stream;
  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

END APPLICATION Oracle_IRLogWriter;
deploy application Oracle_IRLogWriter;
start Oracle_IRLogWriter;

STOP APPLICATION orrs;
UNDEPLOY APPLICATION orrs;
DROP APPLICATION orrs CASCADE;
CREATE APPLICATION orrs;
Create Type CSVType (
  merchantName String,
  companyname String
);

Create Stream TypedFileStream of CSVType;

create source CSVSource using FileReader (
	directory:'@DIRECTORY@',
	WildCard:'posdata5L.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	
)
OUTPUT TO FileStream;

CREATE CQ CsvToPosData
INSERT INTO TypedFileStream
SELECT TO_STRING(data[1]),TO_STRING(data[0])
FROM FileStream;

CREATE TARGET RSTarget USING RedshiftWriter
(
  ConnectionURL: '@TARGET_URL@',
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  bucketname: '@BUCKET-NAME@',
  --accesskeyId: '@ACCESS-KEY-ID@',
  --secretaccesskey: '@SECRET-ACCESS-KEY@',
   S3IAMRole:'@IAMROLE@',
  Tables: '@TABLES@',
  uploadpolicy: 'eventcount:10000, interval:1m'
) INPUT FROM TypedFileStream;
DEPLOY APPLICATION orrs;
START APPLICATION orrs;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCENAME1@ USING IncrementalBatchReader  (
  FetchSize: 10,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: '@startPosition@',
  PollingInterval: '20sec'
  )
  OUTPUT TO @STREAM@;

  CREATE OR REPLACE SOURCE @SOURCENAME2@ USING IncrementalBatchReader  (
    FetchSize: 10,
    Username: 'striim',
    Password: 'striim',
    ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
    Tables: 'striim.test01',
    adapterName: 'IncrementalBatchReader',
    CheckColumn: @checkColumn1@,
    startPosition: '@startPosition1@',
    PollingInterval: '20sec'
    )
    OUTPUT TO @STREAM@;

  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;

  CREATE TARGET @targetName@ USING DatabaseWriter(
    ConnectionURL:'@READER-URL@',
    Username:'@READER-UNAME@',
    Password:'@READER-PASSWORD@',
    BatchPolicy:'Eventcount:1,Interval:1',
    CommitPolicy:'Eventcount:1,Interval:1',
    Checkpointtable:'CHKPOINT',
    Tables:'@WATABLES@,@WATABLES@_target'
  ) INPUT FROM @STREAM@;


  END APPLICATION @APPNAME@;

  DEPLOY APPLICATION @APPNAME@;
  start application @APPNAME@;

stop PatternMatching1.CSV;
undeploy application PatternMatching1.CSV;
drop application PatternMatching1.CSV cascade;

create application CSV;

create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'posdata.csv',
  columndelimiter:',',
  positionByEOF:false
)
OUTPUT TO CsvStream;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  TO_STRING(data[0]) as temp1,
        TO_STRING(data[1]) as temp2,
        TO_DOUBLE(data[2]) as temp3
FROM CsvStream;

-- scenario 1.1 check pattern using PosApp data with alteration and quantifier(0 or 1)
CREATE CQ TypeConversionPosAppCQ1
INSERT INTO TypedStream1
SELECT A.temp1 as typeduserid,
       B.temp2 as typedtemp1,
       C.temp3 as typedtemp2
from UserDataStream
MATCH_PATTERN A | B ? C
define A = UserDataStream(temp1 = 'COMPANY 4'),
       B = UserDataStream(temp2 = 'OFp6pKTMg26n1iiFY00M9uSqh9ZfMxMBRf1'),
       C = UserDataStream(temp3 = 520236368216865619l)
PARTITION BY temp1;

-- scenario 1.2 check pattern using PosApp data with permutation and quantifier(0 or 1)
CREATE CQ TypeConversionPosAppCQ2
INSERT INTO TypedStream2
SELECT A.temp1 as typeduserid,
       B.temp2 as typedtemp1,
       C.temp3 as typedtemp2
from UserDataStream
MATCH_PATTERN A & B ? C
define A = UserDataStream(temp1 = 'COMPANY 100'),
       B = UserDataStream(temp2 = 'IYuqAbAQ07NS3lZO74VGPldfAUAGKwzR2k3'),
       C = UserDataStream(temp3 = 6388500771470313223l)
PARTITION BY temp3;

CREATE WACTIONSTORE UserActivityInfoPosApp1
CONTEXT OF TypedStream1_Type
EVENT TYPES ( TypedStream1_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoPosApp2
CONTEXT OF TypedStream2_Type
EVENT TYPES ( TypedStream2_Type )
@PERSIST-TYPE@

create Target t2 using SysOut(name:AgentTyped) input from TypedStream1;
create Target t3 using SysOut(name:AgentTyped1) input from TypedStream2;

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction1
INSERT INTO UserActivityInfoPosApp1
SELECT * FROM TypedStream1
LINK SOURCE EVENT;

CREATE CQ UserWaction2
INSERT INTO UserActivityInfoPosApp2
SELECT * FROM TypedStream2
LINK SOURCE EVENT;

end application CSV;
deploy application csv;
start csv;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[0]) = '1' ;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@@RECOVERY_PROP@;

CREATE OR REPLACE SOURCE @APP_NAME@_src USING FileReader (
directory:'@DIRECTORY@',
WildCard:'posdata5L.csv',
positionByEOF:false
)
parse using DSVParser (
header:true
)
OUTPUT TO @APP_NAME@_Stream;


CREATE OR REPLACE CQ @APP_NAME@_CQ
INSERT INTO @APP_NAME@_Stream2
SELECT data[0] as BUSINESS_NAME,data[1] as MERCHANT_ID,data[2] as PRIMARY_ACCOUNT,
data[3] as POS_DATA_CODE,data[4] as DATE_TIME,data[5] as EXP_DATE,data[6] as CURRENCY_CODE,data[7] as AUTH_AMOUNT,
data[8] as TERMINAL_ID,data[9] as ZIP,data[10] as CITY
FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt USING KuduWriter (
  kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
  pkupdatehandlingmode:'@MODE@',
  tables: '@TARGET_TABLES@',
  batchpolicy: 'EventCount:1,Interval:0'
 )
INPUT FROM @APP_NAME@_Stream2;


END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

Create Source dataGenSrc Using MSSqlReader
(
 Username:'qatest',
 Password:'w3b@ct10n',
 DatabaseName:'qatest',
 ConnectionURL:'localhost:1433',
 Tables:'@tableNames@', 
 ConnectionPoolSize:1,
 StartPosition:'EOF'
 )
 Output To LCRStream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;

END APPLICATION DataGenSampleApp;

CREATE APPLICATION  @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE  @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'JsonNodeEvent.json',
positionByEOF:false
)
PARSE USING Global.JSONParser (
 )  OUTPUT TO  @AppName@_rawstream;


CREATE CQ @BuiltinFunc@CQ
INSERT INTO  @BuiltinFunc@_Stream
SELECT @BuiltinFunc@(x, 'Sno', data.get("_id"), 'Name', data.get("firstname"))
FROM @AppName@_rawstream x;

CREATE OR REPLACE TARGET  @AppName@_FileTarget USING Global.FileWriter (
  flushpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
 directory: '@logs@',
  filename: '@BuiltinFunc@_JsonNodeEventData',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  (
  handler: 'com.webaction.proc.JSONFormatter',
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  formatterName: 'JSONFormatter',
  jsonobjectdelimiter: '\n' )
INPUT FROM @BuiltinFunc@_Stream;

End application  @AppName@;
Deploy application  @AppName@;
Start application  @AppName@;

Stop application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

CREATE APPLICATION DSV;

CREATE FLOW HTTPsource;

CREATE SOURCE HTTPSOURCE USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'http.txt',
  skipbom: true,
  rolloverpolicy: 'DefaultFileComparator',
  blocksize: '64',
  charset: 'UTF-8',
  positionbyeof: false
 )
 PARSE USING DSVParser (
  trimquote: true,
  columndelimiter: ' ',
  rowdelimiter: '\n',
  header: false,
  quoteset: '{}'
 )
OUTPUT TO RawHTTPStream;

END FLOW HTTPSource;

CREATE FLOW main;

CREATE TYPE HTTPLogEntry (
     start_time String,
     srcIp      String KEY,
     port       String,
     method     String,
     url        String,
     error_num  String,
     status     String,
     end_time   String,
     host       String
 );

CREATE STREAM HTTPStream OF HTTPLogEntry;

CREATE CQ ParseHTTPLog
  INSERT INTO HTTPStream
  SELECT  MATCH(data[1],'start\\s+(\\w+.\\w+)'),
          matchIP(data[2]),
          MATCH(data[3],'port\\W+(\\w+)'),
          MATCH(data[4],'method\\W+(\\w+)'),
          data[5],
          data[7],
          data[8],
          data[9],
          data[10]
  FROM RawHTTPStream;

create Target t using FileWriter(
  filename:'XmlTrimQuote',
  sequence:'00',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:10s,sequence:00'
)
format using DSVFormatter (

)
input from HTTPStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TrimQuoteTest_actual.log') input from HTTPStream;

END FLOW main;

END APPLICATION DSV;

create application CSVToXML;
create source CSVSource using FileReader (
	directory:'Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
	filename:'posdata_XML',
	rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'
)
format using XMLFormatter (
	rootelement:'document',
	elementtuple:'MerchantName:merchantid:text=merchantname'
)
input from TypedCSVStream;
end application CSVToXML;

deploy application CSVToXML;
start application CSVToXML;

STOP Sliding1Tester.Sliding1;
UNDEPLOY APPLICATION Sliding1Tester.Sliding1;
DROP APPLICATION Sliding1Tester.Sliding1 CASCADE;
CREATE APPLICATION Sliding1;

create source CsvSource1 using FileReader
(
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'WindowsTest.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
)
 parse using DSVParser
(
	header:'yes',
	columndelimiter:','
)
OUTPUT TO CsvStream1;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);
CREATE TYPE CsvData1 (
  zip double
);

CREATE TYPE WactionData1 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData2 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData3 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData5 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData6 (
  zip double
);
CREATE TYPE WactionData7 (
  zip double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY companyName;

CREATE STREAM DataStream2 OF CsvData
PARTITION BY city;

CREATE STREAM DataStream3 OF CsvData;
CREATE STREAM DataStream4 OF CsvData;
CREATE STREAM DataStream5 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData3
INSERT INTO DataStream3
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData4
INSERT INTO DataStream4
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData5
INSERT INTO DataStream5
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData6
INSERT INTO DataStream6
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

CREATE CQ CsvToData7
INSERT INTO DataStream7
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

-- Count based sliding window
CREATE WINDOW DataStreamCount
OVER DataStream1 KEEP 5 ROWS
PARTITION BY companyName;

-- Time based jumping window
CREATE WINDOW DataStreamTime OVER DataStream2 KEEP
within 350 second
PARTITION BY companyName,city;

-- Attribute based sliding window
CREATE WINDOW DataStreamAtrribute
OVER DataStream3 KEEP
range 180 second
ON dateTime;

-- Count + time based sliding window
CREATE WINDOW DataStreamCountTime
OVER DataStream4 KEEP
5 rows
within 155 second;

-- Attribute + time based sliding window
CREATE WINDOW DataStreamAttributeTime
OVER DataStream5 KEEP
range 50 second
ON dateTime
within 400 second;

CREATE WACTIONSTORE Wactions1 CONTEXT OF WactionData1
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions2 CONTEXT OF WactionData2
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions3 CONTEXT OF WactionData3
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions4 CONTEXT OF WactionData4
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions5 CONTEXT OF WactionData5
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions6 CONTEXT OF WactionData6
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions7 CONTEXT OF WactionData7
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE CQ Data1ToWaction
INSERT INTO Wactions1
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamCount p
group by companyName;

CREATE CQ Data2ToWaction
INSERT INTO Wactions2
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamTime p
group by companyName,city;

CREATE CQ Data3ToWaction
INSERT INTO Wactions3
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamAtrribute p;

CREATE CQ Data4ToWaction
INSERT INTO Wactions4
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamCountTime p;

CREATE CQ Data5ToWaction
INSERT INTO Wactions5
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamAttributeTime p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions6
SELECT
    count(*)
FROM DataStreamCount p;

CREATE CQ Data7ToWaction
INSERT INTO Wactions7
SELECT
    count(*)
FROM DataStreamTime p;

END APPLICATION Sliding1;

stop application JMSWriter.JMS;
undeploy application JMSWriter.JMS;
drop application JMSWriter.JMS cascade;

create application JMS;
create source JMSCSVSource using FileReader (
        directory:'@TEST-DATA-PATH@',
        WildCard:'AdhocQueryData2.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target JmsTarget  using JMSWriter (
		Provider:'@PROVIDER@',
		Ctx:'@CONTEXT@',
		messagetype: @MESSAGETYPE@,
		UserName:'@USERNAME@',
		Password:'@PASSWORD@',
		Queuename:'dynamicQueues/Test.bar')
format using dsvformatter (
)
input from TypedCSVStream;

end Application Jms;
deploy application jms;

start jms;

create Target @TARGET_NAME@ using ADLSGen2Writer(
          accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'%@metadata(TableName)%',
        filename:'table.csv',
        uploadpolicy:'filesize:10M'
)
FORMAT USING dsvFormatter()
input from @STREAM@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY;

Create Source @APPNAME@_src Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1000,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
   _h_ReturnDateTimeAs:'ZonedDateTime'
) Output To @APPNAME@_stream;

create Target @APPNAME@_tgt using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_stream;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

stop application @appname@Out;
undeploy application @appname@Out;
drop application @appname@Out cascade;

drop stream @appname@KafkaStream;
CREATE OR REPLACE PROPERTYSET @appname@KafkaPropset (zk.address:@keeper@, bootstrap.brokers:@broker@, partitions:'50');
CREATE STREAM @appname@KafkaStream OF Global.parquetevent PERSIST USING @appname@KafkaPropset;
 
CREATE APPLICATION @appname@;

CREATE OR REPLACE SOURCE @parquetsrc@ USING Global.FileReader ( 
  directory: '', 
  wildcard: '',
  positionbyeof: false ) 
PARSE USING Global.ParquetParser () 
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@KafkaStream
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;
    
END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE APPLICATION @appname@Out;

CREATE OR REPLACE TARGET @filetarget@ USING Global.FileWriter ( 
  directory: '', 
  filename: '' 
)
FORMAT USING Global.ParquetFormatter  ( 
  schemaFileName: 'parquetSchema' 
) 
INPUT FROM @appname@KafkaStream;

END APPLICATION @appname@Out;
deploy application @appname@Out on all in default;
start application @appname@Out;

CREATE APPLICATION @AppName@ RECOVERY 1 MINUTE INTERVAL AUTORESUME MAXRETRIES 2 RETRYINTERVAL 60;

CREATE source @AppName@_PosData USING FileReader (
  WildCard: 'posdata100.csv',
  directory: '@TestDir@',
positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO @AppName@_CsvStream;

CREATE OR REPLACE CQ CsvPosAppCq 
INSERT INTO FromCsvPosAppCq 
SELECT TO_STRING(data[1]) as merchantId,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM @AppName@_CsvStream;


CREATE TARGET FileWriterTarget USING Global.FileWriter ( 
 
  flushpolicy: 'EventCount:1000,Interval:30s', 
  directory: '@logs@',
  filename: '@Filename@', 
  rolloverpolicy: 'EventCount:1000,Interval:30s' ) 
FORMAT USING Global.DSVFormatter  ( 
  quotecharacter: '\"', 
  columndelimiter: ',', 
  nullvalue: 'NULL', 
  usequotes: 'false', 
  rowdelimiter: '\n', 
  standard: 'none', 
  header: 'false' ) 
INPUT FROM FromCsvPosAppCq;

end application @AppName@;
deploy application @AppName@;
start application @AppName@;

stop application Postgres_To_PostgresApp;
undeploy application Postgres_To_PostgresApp;
drop application Postgres_To_PostgresApp cascade;
CREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;
create type pkFlag_type
(
TableName String,
PK_UPDATE String,
OperationName String
);
CREATE STREAM Postgres_TypedStream of pkFlag_type;
CREATE OR REPLACE SOURCE Postgres_Src USING PostgreSQLReader  ( 
  ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgres_src'
 ) 
OUTPUT TO Postgres_Change_Data_Stream;
create CQ Cqfilter 
insert into Postgres_TypedStream
select 
META(u,'TableName').toString(),
META(u,'PK_UPDATE').toString(),
META(u,'OperationName').toString()
from Postgres_Change_Data_Stream u;
CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'postgres_PK_Out'
 ) INPUT FROM Postgres_TypedStream;
CREATE  TARGET Postgres_FW USING FileWriter  ( 
  filename: 'Postgres_PKOut.log',
  directory: '/Users/jenniffer/Product2/IntegrationTests/target/test-classes/testNG/PostgreSQLReader/logs'
 ) 
FORMAT USING DSVFormatter  (  ) 
INPUT FROM Postgres_TypedStream;
end application Postgres_To_PostgresApp;
deploy application Postgres_To_PostgresApp;
start Postgres_To_PostgresApp;

STOP APPLICATION App1;
UNDEPLOY APPLICATION App1;
DROP APPLICATION App1 CASCADE;
CREATE APPLICATION App1;
CREATE FLOW AgentFlow;
CREATE OR REPLACE SOURCE App1_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App1_SampleStream;
END FLOW AgentFlow;
CREATE OR REPLACE TARGET App1_NullTarget using NullWriter()
INPUT FROM App1_SampleStream;
END APPLICATION App1;
deploy application App1 with AgentFlow on any in AGENTS;
START APPLICATION App1;

STOP APPLICATION App2;
UNDEPLOY APPLICATION App2;
DROP APPLICATION App2 CASCADE;
CREATE APPLICATION App2;
CREATE FLOW AgentFlow2;
CREATE OR REPLACE SOURCE App2_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App2_SampleStream;
END FLOW AgentFlow2;
CREATE OR REPLACE TARGET App2_NullTarget using NullWriter()
INPUT FROM App2_SampleStream;
END APPLICATION App2;
deploy application App2 with AgentFlow2 on any in AGENTS;
START APPLICATION App2;

STOP APPLICATION App3;
UNDEPLOY APPLICATION App3;
DROP APPLICATION App3 CASCADE;
CREATE APPLICATION App3;
CREATE OR REPLACE SOURCE App3_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App3_SampleStream;
CREATE OR REPLACE TARGET App3_NullTarget using NullWriter()
INPUT FROM App3_SampleStream;
END APPLICATION App3;
DEPLOY APPLICATION App3;
START APPLICATION App3;

STOP APPLICATION App4;
UNDEPLOY APPLICATION App4;
DROP APPLICATION App4 CASCADE;
CREATE APPLICATION App4;
CREATE OR REPLACE SOURCE App4_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App4_SampleStream;
CREATE OR REPLACE TARGET App4_NullTarget using NullWriter()
INPUT FROM App4_SampleStream;
END APPLICATION App4;
DEPLOY APPLICATION App4;
START APPLICATION App4;

STOP APPLICATION App5;
UNDEPLOY APPLICATION App5;
DROP APPLICATION App5 CASCADE;
CREATE APPLICATION App5;
CREATE OR REPLACE SOURCE App5_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App5_SampleStream;
CREATE OR REPLACE TARGET App5_NullTarget using NullWriter()
INPUT FROM App5_SampleStream;
END APPLICATION App5;
DEPLOY APPLICATION App5;
START APPLICATION App5;

STOP APPLICATION App6;
UNDEPLOY APPLICATION App6;
DROP APPLICATION App6 CASCADE;
CREATE APPLICATION App6;
CREATE OR REPLACE SOURCE App6_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App6_SampleStream;
CREATE OR REPLACE TARGET App6_NullTarget using NullWriter()
INPUT FROM App6_SampleStream;
END APPLICATION App6;
DEPLOY APPLICATION App6;
START APPLICATION App6;

STOP APPLICATION App7;
UNDEPLOY APPLICATION App7;
DROP APPLICATION App7 CASCADE;
CREATE APPLICATION App7;
CREATE OR REPLACE SOURCE App7_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App7_SampleStream;
CREATE OR REPLACE TARGET App7_NullTarget using NullWriter()
INPUT FROM App7_SampleStream;
END APPLICATION App7;
DEPLOY APPLICATION App7;
START APPLICATION App7;

STOP APPLICATION App8;
UNDEPLOY APPLICATION App8;
DROP APPLICATION App8 CASCADE;
CREATE APPLICATION App8;
CREATE OR REPLACE SOURCE App8_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App8_SampleStream;
CREATE OR REPLACE TARGET App8_NullTarget using NullWriter()
INPUT FROM App8_SampleStream;
END APPLICATION App8;
DEPLOY APPLICATION App8;
START APPLICATION App8;


STOP APPLICATION App9;
UNDEPLOY APPLICATION App9;
DROP APPLICATION App9 CASCADE;
CREATE APPLICATION App9;
CREATE OR REPLACE SOURCE App9_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App9_SampleStream;
CREATE OR REPLACE TARGET App9_NullTarget using NullWriter()
INPUT FROM App9_SampleStream;
END APPLICATION App9;
DEPLOY APPLICATION App9;

create application KinesisTest RECOVERY 1 SECOND INTERVAL;
CREATE OR REPLACE SOURCE ora_reader USING OracleReader (
  Username: 'miner',
  Password: 'miner',
  ConnectionURL: '192.168.1.113:1521:ORCL',
  TABLES: 'QATEST.H_REGION;QATEST.H_NATION;QATEST.H_CUSTOMER',
  FetchSize: '1'
 )
OUTPUT TO DDLCDCStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	accesskeyid:'ACCESS_KEY',
	secretaccesskey:'SECRET_KEY'
)
format using JSONFormatter (
)
input from DDLCDCStream;

create or replace Target t2 using KinesisWriter (
  regionName:'TARGET_REGION',
  streamName:'TARGET_STREAM',
  accesskeyid:'ACCESS_KEY',
  secretaccesskey:'SECRET_KEY'
)
format using JSONFormatter (
)
input from DDLCDCStream;

end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

STOP APPLICATION orrs;
UNDEPLOY APPLICATION orrs;
DROP APPLICATION orrs CASCADE;
CREATE APPLICATION orrs recovery 5 second interval;
Create Source OraSource Using OracleReader 
	(
	 Username:'user-name',	
	 Password:'password',
	 ConnectionURL: 'src_url',
	 Tables:'src_table',
	 FilterTransactionBoundaries:true,
	 FetchSize:'fetch-size'
	) Output To LCRStream;
	
	CREATE TARGET RSTarget USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: '@ACCESS-KEY-ID@',
	  --secretaccesskey: '@SECRET-ACCESS-KEY@',
	  S3IAMRole:'@IAMROLE@',
	  Tables: 'tgt_table',
	  uploadpolicy:'eventcount:300,interval:1m'
	) INPUT FROM LCRStream;
END APPLICATION orrs;
DEPLOY APPLICATION orrs;
START APPLICATION orrs;

CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'data.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO @AppName@_rawstream;

CREATE OR REPLACE STREAM @BuiltinFunc@_Stream OF Global.WAEVent;
CREATE OR REPLACE STREAM CombineStream OF Global.WAEVent;

CREATE OR REPLACE CQ cq1
INSERT INTO @BuiltinFunc@_Stream
SELECT
@BuiltinFunc@(s1, 'city',data[5])
FROM @AppName@_rawstream s1;

CREATE OR REPLACE CQ cq2
INSERT INTO CombineStream
Select *
FROM @BuiltinFunc@_Stream s4;

CREATE OR REPLACE CQ cq3
INSERT INTO CombineStream
select *
FROM @AppName@_rawstream s5;

CREATE OR REPLACE TARGET @AppName@_FileTarget USING Global.FileWriter ( 
  flushpolicy: 'EventCount:10000,Interval:30s', 
  adapterName: 'FileWriter', 
  directory: '@logs@',
  filename: '@BuiltinFunc@_Data', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING Global.JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter', 
  jsonMemberDelimiter: '\n', 
  EventsAsArrayOfJsonObjects: 'true', 
  formatterName: 'JSONFormatter', 
  jsonobjectdelimiter: '\n' ) 
INPUT FROM CombineStream;

End application @AppName@;
Deploy application @AppName@; 
Start application @AppName@;

CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( 
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: '@CDC_URL@',
  Tables: '@Source1Tables@',
  FetchSize: 1) 
OUTPUT TO @APPNAME@cdcStream;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@1 ( 
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  FetchSize: 20,
  DatabaseProviderType: 'Default',
  Table: '@Source3Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')  
OF @APPNAME@cachetype;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@2 ( 
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  FetchSize: 10,
  DatabaseProviderType: 'Default',
  Table: '@Source2Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')  
OF @APPNAME@cachetype;

-- Wactionstore has been moved to DSWaction.tql

CREATE APPLICATION MyPosApp;

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


CREATE TYPE PosData(
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);
CREATE STREAM PosDataStream OF PosData PARTITION BY merchantId;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue int,
  hourlyAve int
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;


CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       p.zip,
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;


CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE CACHE NameLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false
)
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;

END APPLICATION MyPosApp;
deploy application MyPosApp;
start application MyPosApp;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;
Create Source oracSource Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;

CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

STOP APPLICATION @AppName@_App1;
UNDEPLOY APPLICATION @AppName@_App1;
DROP APPLICATION @AppName@_App1 CASCADE;
CREATE APPLICATION @AppName@_App1 recovery 1 second interval;


CREATE SOURCE @AppName@_FileSource USING FileReader (
  directory:'@TestdataDir@',
    WildCard:'banks*',
  positionByEOF:false
  )
PARSE USING DSVParser (
  header:no
)OUTPUT TO FileStream;


CREATE OR REPLACE ROUTER filerouter1 INPUT FROM FileStream s CASE
WHEN meta(s,"FileName").toString()='banks1.csv' THEN ROUTE TO stream1,
WHEN meta(s,"FileName").toString()='banks2.csv' THEN ROUTE TO stream2,
WHEN meta(s,"FileName").toString()='banks3.csv' THEN ROUTE TO stream3,
WHEN meta(s,"FileName").toString()='banks4.csv' THEN ROUTE TO stream4,
ELSE ROUTE TO ss_else;

CREATE TYPE banks1(
  id int,
  name String ,
Filename String
);

Create stream cdctypestream1 of banks1;

CREATE CQ cdcstreamcq1
INSERT INTO cdctypestream1
SELECT TO_INT(p.data[0]), 
       TO_STRING(p.data[1]), TO_STRING(META(p,'FileName'))
FROM stream1 p;


CREATE OR REPLACE TARGET @AppName@_DataBaset1 USING DatabaseWriter  ( 
ConnectionURL:'@url@',
Username:'@userName@',
Password:'@password@',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.NEWBANKS1') 
INPUT FROM cdctypestream1;

End application @AppName@_App1;
Deploy application @AppName@_App1;
start application @AppName@_App1;

stop application @AppName@_App2;
undeploy application @AppName@_App2;
drop application @AppName@_App2 CASCADE;
create application @AppName@_App2 recovery 1 second interval;

CREATE OR REPLACE TARGET @AppName@_DataBaset2 USING DatabaseWriter  ( 
ConnectionURL:'@url@',
Username:'@userName@',
Password:'@password@',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.BANKS1') 
INPUT FROM cdctypestream1;

end application @AppName@_App2;
deploy application @AppName@_App2;
start application @AppName@_App2;

stop application MySQLToSQLServer;
undeploy application MySQLToSQLServer;
drop application MySQLToSQLServer cascade;

CREATE APPLICATION MySQLToSQLServer recovery 1 second interval;

create source Src1ReadFromMySQL USING MySQLReader (
Username: 'root',
  Password: 'w@ct10n',
  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',
--Tables: 'waction.mytable1;waction.mytable2;qatest.mysqlmarker',
Tables: 'waction.Parent%;waction.Child%;',
--Tables: 'waction.mytable%',
BiDirectionalMarkerTable: 'waction.mysqlmarker',
compression: 'True',
sendBeforeImage:True
) OUTPUT TO App1Stream;


CREATE  TARGET Src1ReadFromMySQL_Out USING FileWriter  (
  filename: 'Src1ReadFromMySQL_Out.log',
flushpolicy: 'eventcount:1',
rolloverpolicy: 'eventcount:10000'
 )
FORMAT USING JSONFORMATTER  (
 )
INPUT FROM App1Stream;


CREATE TARGET WriteToMSSQL1 USING DatabaseWriter(
ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',
Username:'qatest',
Password:'w3b@ct10n',
BatchPolicy:'EventCount:8,Interval:5',
CommitPolicy:'EventCount:10,Interval:5',
BiDirectionalMarkerTable: 'qatest.mysqlmarker',
--Tables: 'waction.mytable1,qatest.mytable1;'
Tables: 'waction.Parent%,qatest.%;waction.Child%,qatest.%;'
--Tables: 'waction.mytable%,qatest.%'
)
INPUT FROM App1Stream;


--MSSQl tp MySQL

Create Source SrcReadFromMSSQL
Using MSSqlReader
(
Username:'qatest',
Password:'w3b@ct10n',
DatabaseName:'qatest',
ConnectionURL:'localhost:1433',
--Tables:'qatest.mytable1;qatest.mytable2;qatest.mysqlmarker;qatest.mysqlmarker2',
Tables:'qatest.Parent%;qatest.Child%',
--Tables:'qatest.mytable%',
BiDirectionalMarkerTable: 'qatest.mysqlmarker',
TransactionSupport: 'true',
FilterTransactionBoundaries: true,
Compression: 'True',
ConnectionPoolSize:1
)
Output To App2Stream;


CREATE  TARGET SrcReadFromMSSQL_Out USING FileWriter  (
  filename: 'SrcReadFromMSSQL_Out.log',
flushpolicy: 'eventcount:1',
rolloverpolicy: 'eventcount:10000'
 )
FORMAT USING JSONFORMATTER  (
 )
INPUT FROM App2Stream;


CREATE TARGET WriteToMySQL USING DatabaseWriter(
ConnectionURL:'jdbc:mysql://localhost:3306/waction',
Username:'root',
Password:'w@ct10n',
BatchPolicy:'EventCount:10,Interval:10',
CommitPolicy: 'EventCount:15,Interval:12',
BiDirectionalMarkerTable: 'waction.mysqlmarker',
--Tables: 'qatest.mytable1,waction.mytable1;qatest.mytable2,waction.mytable3;'
Tables: 'qatest.Parent_1,waction.Parent_1;qatest.Parent_2,waction.Parent_2;qatest.Child_1,waction.Child_1;qatest.Child_2,waction.Child_2;'
--Tables: 'qatest.mytable%,waction.%'
)
INPUT FROM App2Stream;

/*
CREATE OR REPLACE TARGET MSSQLDDLFileOut USING FileWriter  (
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  filename: 'MSSQL.txt'
 )
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 )
INPUT FROM App2Stream;

*/

END APPLICATION MySQLToSQLServer;
deploy application MySQLToSQLServer;
start application MySQLToSQLServer;

STOP application SecurityApper.MultiLogDashboard;
undeploy application SecurityApper.MultiLogDashboard;
drop application SecurityApper.MultiLogDashboard cascade;

CREATE APPLICATION MultiLogDashboard;

-- This sample application shows how WebAction could be used monitor and correlate logs
-- from web and application server logs from the same web application. See the discussion
-- in the "Sample Applications" section of the WebAction documentation for additional
-- discussion.


CREATE FLOW MonitorLogs;

-- MonitorLogs sets up the two log sources used by this application. In a real-world
--implementation, each source could be reading many logs from many servers.

-- The web server logs are in Apache NCSA extended/ combined log format plus response time:
-- "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-agent}i\" %D"
-- See apache.org for more information.

CREATE SOURCE AccessLogSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'access_log',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO RawAccessStream;

CREATE TYPE AccessLogEntry (
    srcIp String KEY,
    userId String,
    sessionId String,
    accessTime DateTime,
    request String,
    code int,
    size int,
    referrer String,
    userAgent String,
    responseTime int
);
CREATE STREAM AccessStream OF AccessLogEntry;

CREATE CQ ParseAccessLog
INSERT INTO AccessStream
SELECT data[0], data[2], MATCH(data[4], ".*jsessionId=(.*) "),
       TO_DATE(data[3], "dd/MMM/yyyy:HH:mm:ss.SSS Z"), data[4], TO_INT(data[5]), TO_INT(data[6]),
       data[7], data[8], TO_INT(data[9])
FROM RawAccessStream;

-- The application server logs are in Apache's Log4J format.

CREATE SOURCE Log4JSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'log4jLog.xml',
  positionByEOF:false
)
PARSE USING XMLParser(
  rootnode:'/log4j:event',
  columnlist:'log4j:event/@timestamp,log4j:event/@level,log4j:event/log4j:message,log4j:event/log4j:throwable,log4j:event/log4j:locationInfo/@class,log4j:event/log4j:locationInfo/@method,log4j:event/log4j:locationInfo/@file,log4j:event/log4j:locationInfo/@line'
)
OUTPUT TO RawXMLStream;

CREATE TYPE Log4JEntry (
  logTime DateTime,
  level String,
  message String,
  api String,
  sessionId String,
  userId String,
  sobject String,
  xception String,
  className String,
  method String,
  fileName String,
  lineNum String
);
CREATE STREAM Log4JStream OF Log4JEntry;

CREATE CQ ParseLog4J
INSERT INTO Log4JStream
SELECT TO_DATE(TO_LONG(data[0])), data[1], data[2],
       MATCH(data[2], '\\\\[api=([a-zA-Z0-9]*)\\\\]'),
       MATCH(data[2], '\\\\[session=([a-zA-Z0-9\\-]*)\\\\]'),
       MATCH(data[2], '\\\\[user=([a-zA-Z0-9\\-]*)\\\\]'),
       MATCH(data[2], '\\\\[sobject=([a-zA-Z0-9]*)\\\\]'),
       data[3], data[4], data[5], data[6], data[7]
FROM RawXMLStream;

END FLOW MonitorLogs;


CREATE FLOW ErrorsAndWarnings;

-- ErrorsAndWarnings creates a sliding window (Log4JErrorWarningActivity) containing
-- the 300 most recent errors and warnings in the application server log. The
-- ZeroContentCheck and LargeRTCheck flows join events from this window with access log
-- events.

-- The type Log4JEntry was already defined by the MonitorLogs flow.
CREATE STREAM Log4ErrorWarningStream OF Log4JEntry;

CREATE CQ GetLog4JErrorWarning
INSERT INTO Log4ErrorWarningStream
SELECT l FROM Log4JStream l
WHERE l.level = 'ERROR' OR l.level = 'WARN';

CREATE WINDOW Log4JErrorWarningActivity
OVER Log4ErrorWarningStream KEEP 300 ROWS;

END FLOW ErrorsAndWarnings;


-- HackerCheck sends an alert when an access log srcIp value is on a blacklist.

CREATE FLOW HackerCheck;

CREATE TYPE IPEntry (
    ip String
);

/* CREATE CACHE BlackListLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'multiLogBlackList.txt',
  header: No,
  columndelimiter: ','
) QUERY (keytomap:'ip') OF IPEntry; */

CREATE CACHE BlackListLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'multiLogBlackList.txt'
)
PARSE USING DSVParser ( )
QUERY (keytomap:'ip') OF IPEntry;


CREATE STREAM HackerStream OF AccessLogEntry;

CREATE CQ FindHackers
INSERT INTO HackerStream
SELECT ale
FROM AccessStream ale, BlackListLookup bll
WHERE ale.srcIp = bll.ip;

CREATE TYPE UnusualContext (
    typeOfActivity String,
    accessTime DateTime,
    accessSessionId String,
    srcIp String KEY,
    userId String,
    country String,
    city String,
    lat double,
    lon double
);
CREATE TYPE MergedEntry (
    accessTime DateTime,
    accessSessionId String,
    srcIp String KEY,
    userId String,
    request String,
    code int,
    size int,
    referrer String,
    userAgent String,
    responseTime int,
    logTime DateTime,
    logSessionId String,
    level String,
    message String,
    api String,
    sobject String,
    xception String,
    className String,
    method String,
    fileName String,
    lineNum String
);
CREATE WACTIONSTORE UnusualActivity
CONTEXT OF UnusualContext
EVENT TYPES (MergedEntry,AccessLogEntry)
PERSIST NONE USING ( );

CREATE CQ GenerateHackerContext
INSERT INTO UnusualActivity
SELECT 'HackAttempt', accessTime, sessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM HackerStream
LINK SOURCE EVENT;

CREATE STREAM HackingAlertStream OF Global.AlertEvent;

CREATE CQ SendHackingAlerts
INSERT INTO HackingAlertStream
SELECT 'HackingAlert', ''+accessTime, 'warning', 'raise',
        'Possible Hacking Attempt from ' + srcIp + ' in ' + IP_COUNTRY(srcIp)
FROM HackerStream;

CREATE SUBSCRIPTION HackingAlertSub USING WebAlertAdapter( ) INPUT FROM HackingAlertStream;

END FLOW HackerCheck;


-- LargeRTCheck sends an alert when an access log responseTime value exceeds 2000
-- microseconds.

CREATE FLOW LargeRTCheck;

CREATE STREAM LargeRTStream of AccessLogEntry;

CREATE CQ FindLargeRT
INSERT INTO LargeRTStream
SELECT ale
FROM AccessStream ale
WHERE ale.responseTime > 2000;

CREATE WINDOW LargeRTActivity
OVER LargeRTStream KEEP 100 ROWS;

CREATE STREAM LargeRTAPIStream OF MergedEntry;

CREATE CQ MergeLargeRTAPI
INSERT INTO LargeRTAPIStream
SELECT lrt.accessTime, lrt.sessionId, lrt.srcIp, lrt.userId, lrt.request,
       lrt.code, lrt.size, lrt.referrer, lrt.userAgent, lrt.responseTime,
       log4j.logTime, log4j.sessionId, log4j.level, log4j.message, log4j.api, log4j.sobject, log4j.xception,
       log4j.className, log4j.method, log4j.fileName, log4j.lineNum
FROM LargeRTActivity lrt, Log4JErrorWarningActivity log4j
WHERE lrt.sessionId = log4j.sessionId
      AND lrt.accessTime = log4j.logTime;

CREATE CQ GenerateLargeRTContext
INSERT INTO UnusualActivity
SELECT 'LargeResponseTime', accessTime, accessSessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM LargeRTAPIStream
LINK SOURCE EVENT;

CREATE STREAM LargeRTAlertStream OF Global.AlertEvent;

CREATE CQ SendLargeRTAlerts
INSERT INTO LargeRTAlertStream
SELECT 'LargeRTAlert', ''+accessTime, 'warning', 'raise',
        'Long response time for call from ' + userId + ' api ' + api + ' message ' + message
FROM LargeRTAPIStream;

CREATE SUBSCRIPTION LargeRTAlertSub USING WebAlertAdapter( ) INPUT FROM LargeRTAlertStream;

END FLOW LargeRTCheck;


-- ProxyCheck sends an alert when an access log srcIP value is on a list of suspicious
-- proxies.

CREATE FLOW ProxyCheck;

/* CREATE CACHE ProxyLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'multiLogProxies.txt',
  header: No,
  columndelimiter: ','
) QUERY (keytomap:'ip') OF IPEntry; */

CREATE CACHE ProxyLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'multiLogProxies.txt'
)
PARSE USING DSVParser ( )
QUERY (keytomap:'ip') OF IPEntry;


CREATE STREAM ProxyStream OF AccessLogEntry;

CREATE CQ FindProxies
INSERT INTO ProxyStream
SELECT ale
FROM AccessStream ale, ProxyLookup pl
WHERE ale.srcIp = pl.ip;

CREATE CQ GenerateProxyContext
INSERT INTO UnusualActivity
SELECT 'ProxyAccess', accessTime, sessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM ProxyStream
LINK SOURCE EVENT;


CREATE STREAM ProxyAlertStream OF Global.AlertEvent;

CREATE CQ SendProxyAlerts
INSERT INTO ProxyAlertStream
SELECT 'ProxyAlert', ''+accessTime, 'warning', 'raise',
        'Possible use of Proxy from ' + srcIp + ' in ' + IP_COUNTRY(srcIp) + ' for user ' + userId
FROM ProxyStream;

CREATE SUBSCRIPTION ProxyAlertSub USING WebAlertAdapter( ) INPUT FROM ProxyAlertStream;

END FLOW ProxyCheck;


-- ZeroContentCheck sends an alert when an access log entry's code value is 200 (that is,
-- the HTTP request succeeded) but the size value is 0 (the return had no content).

CREATE FLOW ZeroContentCheck;

CREATE STREAM ZeroContentStream of AccessLogEntry;

CREATE CQ FindZeroContent
INSERT INTO ZeroContentStream
SELECT ale
FROM AccessStream ale
WHERE ale.code = 200 AND ale.size = 0;

CREATE WINDOW ZeroContentActivity
OVER ZeroContentStream KEEP 100 ROWS;

CREATE STREAM ZeroContentAPIStream OF MergedEntry;

CREATE CQ MergeZeroContentAPI
INSERT INTO ZeroContentAPIStream
SELECT zcs.accessTime, zcs.sessionId, zcs.srcIp, zcs.userId, zcs.request,
       zcs.code, zcs.size, zcs.referrer, zcs.userAgent, zcs.responseTime,
       log4j.logTime, log4j.sessionId, log4j.level, log4j.message, log4j.api, log4j.sobject, log4j.xception,
       log4j.className, log4j.method, log4j.fileName, log4j.lineNum
FROM ZeroContentActivity zcs, Log4JErrorWarningActivity log4j
WHERE zcs.sessionId = log4j.sessionId
      AND zcs.accessTime = log4j.logTime;

CREATE CQ GenerateZeroContentContext
INSERT INTO UnusualActivity
SELECT 'ZeroContent', accessTime, accessSessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM ZeroContentAPIStream
LINK SOURCE EVENT;


CREATE TYPE ZeroContentEventListType (
    srcIp String KEY,
    code int,
    size int,
    level String,
    message String,
    xception String);

CREATE WACTIONSTORE ZeroContentEventList
CONTEXT OF ZeroContentEventListType
EVENT TYPES (ZeroContentEventListType )
PERSIST NONE USING ( );


CREATE CQ GenerateZeroContentEventList
INSERT INTO ZeroContentEventList
SELECT srcIp, code, size, level, message, xception
FROM ZeroContentAPIStream;


CREATE STREAM ZeroContentAlertStream OF Global.AlertEvent;

CREATE CQ SendZeroContentAlerts
INSERT INTO ZeroContentAlertStream
SELECT 'ZeroContentAlert', ''+accessTime, 'warning', 'raise',
        'Zero content returned in call from ' + userId + ' api ' + api + ' message ' + message
FROM ZeroContentAPIStream;

CREATE SUBSCRIPTION ZeroContentAlertSub USING WebAlertAdapter( ) INPUT FROM ZeroContentAlertStream;

END FLOW ZeroContentCheck;


-- ErrorHandling is functionally identical to ErrorFlow.SaasMonitorApp. It sends an alert
-- when an error message appears in the application server log.

CREATE FLOW ErrorHandling;

CREATE STREAM ErrorStream OF Log4JEntry;

CREATE CQ GetErrors
INSERT INTO ErrorStream
SELECT log4j
FROM Log4ErrorWarningStream log4j WHERE log4j.level = 'ERROR';

CREATE STREAM ErrorAlertStream OF Global.AlertEvent;

CREATE CQ SendErrorAlerts
INSERT INTO ErrorAlertStream
SELECT 'ErrorAlert', ''+logTime, 'error', 'raise', 'Error in log ' + message
FROM ErrorStream;

CREATE SUBSCRIPTION ErrorAlertSub USING WebAlertAdapter( ) INPUT FROM ErrorAlertStream;

END FLOW ErrorHandling;


-- WarningHandling is a minor variation on WarningFlow.SaasMonitorApp. It sends an alert
-- once an hour with the count of warnings for each api call for which there has been at
-- least one alert.

CREATE FLOW WarningHandling;

CREATE STREAM WarningStream OF Log4JEntry;

CREATE CQ GetWarnings
INSERT INTO WarningStream
SELECT log4j
FROM Log4ErrorWarningStream log4j WHERE log4j.level = 'WARN';

CREATE JUMPING WINDOW WarningWindow
OVER WarningStream KEEP WITHIN 60 MINUTE ON logTime;

CREATE STREAM WarningAlertStream OF Global.AlertEvent;

CREATE CQ SendWarningAlerts
INSERT INTO WarningAlertStream
SELECT 'WarningAlert', ''+logTime, 'warning', 'raise',
        COUNT(logTime) + ' Warnings in log for api ' + api
FROM WarningWindow
GROUP BY api
HAVING count(logTime) > 1;

CREATE SUBSCRIPTION WarningAlertSub USING WebAlertAdapter( ) INPUT FROM WarningAlertStream;

END FLOW WarningHandling;


-- InfoFlow is functionally similar to InfoFlow.SaasMonitorApp. Its output is used by
-- ApiFlow, CompanyApiFlow, and UserApiFlow.

CREATE FLOW InfoFlow;

CREATE TYPE UserInfo (
  userId String,
  userName String,
  company String,
  userZip String,
  companyZip String
);

/* CREATE CACHE MLogUserLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'multiLogUser.csv',
  header: No,
  columndelimiter: ','
) QUERY (keytomap:'userId') OF UserInfo; */

CREATE CACHE MLogUserLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'multiLogUser.csv'
)
PARSE USING DSVParser ( )
QUERY (keytomap:'userId') OF UserInfo;

CREATE STREAM InfoStream OF Log4JEntry;

CREATE CQ GetInfo
INSERT INTO InfoStream
SELECT log4j
FROM Log4JStream log4j WHERE log4j.level = 'INFO';

CREATE TYPE ApiCall (
  userId String,
  api String,
  sobject String,
  logTime DateTime,
  userName String,
  company String,
  userZip String,
  companyZip String
);
CREATE STREAM ApiEnrichedStream OF ApiCall;

CREATE CQ GetUserDetails
INSERT INTO ApiEnrichedStream
SELECT a.userId, a.api, a.sobject, a.logTime, u.userName, u.company, u.userZip, u.companyZip
FROM InfoStream a, MLogUserLookup u
WHERE a.userId = u.userId;

END FLOW InfoFlow;


-- ApiFlow populates the dashboard's Detail - ApiActivity page and the pie chart on the
-- Overview page.

CREATE FLOW ApiFlow;

CREATE TYPE ApiUsage (
  api String key,
  sobject String,
  count int,
  logTime DateTime
);

CREATE TYPE ApiContext (
  api String key,
  count int,
  logTime DateTime
);

CREATE WACTIONSTORE ApiActivity
CONTEXT OF ApiContext
EVENT TYPES (ApiUsage )
PERSIST NONE USING ( );

CREATE JUMPING WINDOW ApiWindow
OVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY api;

CREATE STREAM ApiUsageStream OF ApiUsage;

CREATE CQ GetApiUsage
INSERT INTO ApiUsageStream
SELECT a.api, a.sobject,
       COUNT(a.userId), FIRST(a.logTime)
FROM ApiWindow a
GROUP BY a.api, a.sobject HAVING FIRST(a.logTime) IS NOT NULL;

CREATE JUMPING WINDOW ApiSummaryWindow
OVER ApiUsageStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY api;

CREATE CQ GetApiSummaryUsage
INSERT INTO ApiActivity
SELECT a.api,
       sum(a.count), first(a.logTime)
FROM ApiSummaryWindow a
GROUP BY a.api
LINK SOURCE EVENT;

END FLOW ApiFlow;


-- CompanyApiFlow populates the dashboard's Detail - CompanyApiActivity page and the bar
-- chart on the Overview page. It also sends an alert when an API call is used by a
-- company more than 1500 times during the flow's one-hour jumping window.

CREATE FLOW CompanyApiFlow;

CREATE TYPE CompanyApiUsage (
  company String key,
  companyZip String,
  companyLat double,
  companyLong double,
  api String,
  count int,
  unusual int,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE TYPE CompanyApiContext (
  company String key,
  companyZip String,
  companyLat double,
  companyLong double,
  count int,
  unusual int,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE WACTIONSTORE CompanyApiActivity
CONTEXT OF CompanyApiContext
EVENT TYPES ( CompanyApiUsage )
PERSIST NONE USING ( );

CREATE JUMPING WINDOW CompanyApiWindow
OVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY company;

CREATE STREAM CompanyApiUsageStream OF CompanyApiUsage;

CREATE TYPE MLogUSAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

/*CREATE CACHE MLogZipLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  header: Yes,
  columndelimiter: ','
) QUERY (keytomap:'zip') OF MLogUSAddressData; */

CREATE CACHE MLogZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt'
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false

)
QUERY (keytomap:'zip') OF MLogUSAddressData;


CREATE CQ GetCompanyApiUsage
INSERT INTO CompanyApiUsageStream
SELECT a.company, a.companyZip, z.latVal, z.longVal,
       a.api, COUNT(a.sobject),
       CASE WHEN COUNT(a.sobject) > 1500 THEN 1
            ELSE 0 END,
       CASE WHEN COUNT(a.sobject) > 1500 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.sobject),
       FIRST(a.logTime)
FROM CompanyApiWindow a, MLogZipLookup z
WHERE a.companyZip = z.zip
GROUP BY a.company, a.api HAVING FIRST(a.logTime) IS NOT NULL;

CREATE JUMPING WINDOW CompanyWindow
OVER CompanyApiUsageStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY company;

CREATE CQ GetCompanyUsage
INSERT INTO CompanyApiActivity
SELECT a.company, a.companyZip, a.companyLat, a.companyLong,
       SUM(a.count), SUM(a.unusual),
       CASE WHEN SUM(a.unusual) > 0 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.topObject),
       FIRST(a.logTime)
FROM CompanyWindow a
GROUP BY a.company
LINK SOURCE EVENT;

CREATE STREAM CompanyAlertStream OF Global.AlertEvent;

CREATE CQ SendCompanyApiAlerts
INSERT INTO CompanyAlertStream
SELECT 'CompanyAPIAlert', ''+logTime, 'warning', 'raise',
       'Company ' + company + ' has used api ' + api + ' ' + count + ' times for ' + topObject
FROM CompanyApiUsageStream
WHERE unusual = 1;

CREATE SUBSCRIPTION CompanyAlertSub USING WebAlertAdapter( ) INPUT FROM CompanyAlertStream;

END FLOW CompanyApiFlow;


-- UserApiFlow populates the dashboard's Detail - UserApiActivity page and the US map on
-- the Overview page. It also sends an alert when an API call is used by a user more than
-- 125 times during the flow's one-hour window.

CREATE FLOW UserApiFlow;

CREATE TYPE UserApiUsage (
  userId String key,
  userName String,
  userZip String,
  userLat double,
  userLong double,
  company String,
  api String,
  count int,
  unusual int,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE TYPE UserApiContext (
  userId String key,
  userName String,
  userZip String,
  userLat double,
  userLong double,
  company String,
  count int,
  unusual int,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE WACTIONSTORE UserApiActivity
CONTEXT OF UserApiContext
EVENT TYPES (  UserApiUsage )
PERSIST NONE USING ( );

CREATE JUMPING WINDOW UserApiWindow
OVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY userId;

CREATE STREAM UserApiUsageStream OF UserApiUsage;

CREATE CQ GetUserApiUsage
INSERT INTO UserApiUsageStream
SELECT a.userId, a.userName, a.userZip, z.latVal, z.longVal, a.company,
       a.api, COUNT(a.sobject),
       CASE WHEN COUNT(a.sobject) > 125 THEN 1
            ELSE 0 END,
       CASE WHEN COUNT(a.sobject) > 125 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.sobject),
       FIRST(a.logTime)
FROM UserApiWindow a, MLogZipLookup z
WHERE a.userZip = z.zip
GROUP BY a.userId, a.api HAVING FIRST(a.logTime) IS NOT NULL;

CREATE JUMPING WINDOW UserWindow
OVER UserApiUsageStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY userId;

CREATE CQ GetUserUsage
INSERT INTO UserApiActivity
SELECT a.userId, a.userName, a.userZip, a.userLat, a.userLong,
       a.company, SUM(a.count), SUM(a.unusual),
       CASE WHEN SUM(a.unusual) > 0 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.topObject),
       FIRST(a.logTime)
FROM UserWindow a
GROUP BY a.userId
LINK SOURCE EVENT;

CREATE STREAM UserAlertStream OF Global.AlertEvent;

CREATE CQ SendUserApiAlerts
INSERT INTO UserAlertStream
SELECT 'UserAPIAlert', ''+logTime, 'warning', 'raise',
       'User ' + userName + ' has used api ' + api + ' ' + count + ' times for ' + topObject
FROM UserApiUsageStream
WHERE unusual = 1;

CREATE SUBSCRIPTION UserAlertSub USING WebAlertAdapter( ) INPUT FROM UserAlertStream;

END FLOW UserApiFlow;


CREATE VISUALIZATION MultiLogApp "@FEATURE-DIR@/tql/MultiLogApp_visualization_settings.json";

END APPLICATION MultiLogDashboard;

create dashboard using "@FEATURE-DIR@/tql/MultiLogAppDashboard.json";

CREATE TARGET @TARGET_NAME@ USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
) INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@ USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
) INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@2 USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
) INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@3 USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
) INPUT FROM @STREAM@;


CREATE TARGET @TARGET_NAME@4 USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
) INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;;

create Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;

--
-- Recovery Test 6 with sliding window and partitioned feature
-- Nicholas Keene, Bert Hashemi WebAction, Inc.
--
-- S -> CQ -> SW(partitioned) -> CQ(no aggregate) -> WS
--

STOP KStreamRecov6Tester.KStreamRecovTest6;
UNDEPLOY APPLICATION KStreamRecov6Tester.KStreamRecovTest6;
DROP APPLICATION KStreamRecov6Tester.KStreamRecovTest6 CASCADE;
DROP USER KStreamRecov6Tester;
DROP NAMESPACE KStreamRecov6Tester CASCADE;
CREATE USER KStreamRecov6Tester IDENTIFIED BY KStreamRecov6Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov6Tester;
CONNECT KStreamRecov6Tester KStreamRecov6Tester;

CREATE APPLICATION KStreamRecovTest6 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvData PARTITION BY merchantId;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION KStreamRecovTest6;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE FLOW @APP_NAME@SrcFlow;

CREATE OR REPLACE SOURCE @APP_NAME@_src USING FileReader (
directory:'@DIRECTORY@',
WildCard:'posdata5L.csv',
positionByEOF:false
)
parse using DSVParser (
header:'no'
)
OUTPUT TO @APP_NAME@_Stream;
END FLOW @APP_NAME@SrcFlow;

CREATE FLOW @APP_NAME@TgtFlow;

CREATE OR REPLACE TYPE @APP_NAME@_Type  ( BUSINESS_NAME java.lang.String KEY,
MERCHANT_ID java.lang.String,
PRIMARY_ACCOUNT_NUMBER java.lang.String
 ) ;

CREATE OR REPLACE STREAM @APP_NAME@_Stream2 OF @APP_NAME@_Type;
CREATE OR REPLACE CQ @APP_NAME@_CQ
INSERT INTO @APP_NAME@_Stream2
SELECT data[0],data[1],data[2]
FROM @APP_NAME@_Stream;

CREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.DeltaLakeWriter (
  personalAccessToken: '',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.student',
  adapterName: 'DeltaLakeWriter',
  personalAccessToken_encrypted: 'false',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:50000,interval:50s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @APP_NAME@_Stream2;

END FLOW @APP_NAME@TgtFlow;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@ WITH @APP_NAME@SrcFlow IN agents,@APP_NAME@TgtFlow IN default;
START APPLICATION @APP_NAME@;

stop application GCSWriterTest;
undeploy application GCSWriterTest;
drop application GCSWriterTest cascade;
create application GCSWriterTest recovery 1 second interval;
create source GCS_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'true'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;
create Target GCSTarget using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@',
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
--members:'data'
)
input from TypedCSVStream;

end application GCSWriterTest;
deploy application GCSWriterTest on all in default;
start application GCSWriterTest;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @SourceName@ USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 ) 
OUTPUT TO @SRCINPUTSTREAM@ ;


CREATE OR REPLACE TARGET @targetsys@ USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM @SRCINPUTSTREAM@;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR;

Create Source s1 Using IncrementalBatchReader (
 FetchSize: 1,
  Username: 'striim',
  Password: 'o4l1uMpwIDQ=',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.autotest01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.autotest01=id',
  startPosition: '%=0'
 )
OUTPUT TO data_stream1;

create source s2 using IncrementalBatchReader (
FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.autotest02',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.autotest02=id',
  startPosition: '%=0'
 )
OUTPUT TO data_stream2;

create source s3 using IncrementalBatchReader (
FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.autotest03',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.autotest03=id',
  startPosition: '%=0'
 )
OUTPUT TO data_stream3;

Create Type EventType (
ID int,
PIN int
);

CREATE STREAM insertData1  of EventType;
CREATE STREAM deleteData1 of EventType;
CREATE STREAM joinData1 of EventType;
CREATE STREAM joinData2 of EventType;
CREATE STREAM deleteData2 of EventType;
CREATE STREAM OutStream of EventType;

CREATE CQ cq1 INSERT INTO insertData1  SELECT TO_INT(data[0]),TO_INT(data[1]) FROM data_stream1;

CREATE CQ cq2 INSERT INTO deleteData1 SELECT TO_INT(data[0]),TO_INT(data[1]) FROM data_stream2;

CREATE CQ cq3 INSERT INTO joinData1 SELECT TO_INT(data[0]),TO_INT(data[1]) FROM data_stream3;

CREATE JUMPING WINDOW DataWin1 OVER deleteData1 KEEP 1 ROWS;

CREATE CQ TypedEventKeyCQ6 INSERT INTO deleteData2 SELECT * from DataWin1;

CREATE JUMPING WINDOW DataWin2 OVER joinData1 KEEP 1 ROWS;

CREATE CQ TypedEventKeyCQ5 INSERT INTO joinData2 SELECT * from DataWin2;

CREATE EVENTTABLE ETABLE1 using STREAM ( NAME: 'insertData1 ' )
DELETE using STREAM ( NAME: 'deleteData1')
QUERY (keytomap:"ID", persistPolicy: 'true') OF EventType;

CREATE CQ cq4 INSERT INTO OutStream SELECT B.ID,B.PIN FROM joinData2 A, ETABLE1 B where A.ID=B.ID;

CREATE TARGET EventTableFW USING FileWriter
(filename:'BasicIR_RT.log',
 rolloverpolicy: 'EventCount:1000000')
FORMAT USING DSVFormatter () INPUT FROM OutStream;

create target Target_Azure using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'STRIIM',
        password: 'W3b@ct10n',
        AccountName: 'striimqatestdonotdelete',
        accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables:'dbo.autotest01',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM OutStream;

END APPLICATION IR;
deploy application IR in default;
start IR;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadpolicy:'EventCount:7'
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from @STREAM@;

end flow @APPNAME@_serverflow;

end application @APPNAME@;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;


CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using OracleReader
(
  Compression:true,
  StartTimestamp:'null',
  FetchSize:1,
  CommittedTransactions:true,
  QueueSize:2048,
  FilterTransactionBoundaries:true,
  Password_encrypted:'false',
  SendBeforeImage:true,
  XstreamTimeOut:600,
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE',
  adapterName:'OracleReader',
  Password:'qatest',
  DictionaryMode:'OfflineCatalog',
  FilterTransactionState:true,
  connectionRetryPolicy:'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType:'LogMiner',
  Username:'qatest',
  OutboundServerProcessName:'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic:true,
  CDDLAction:'Quiesce_Cascade',
  CDDLCapture:'true'
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING BigqueryWriter  
(
  serviceAccountKey:'/Users/hariharasudhan/Downloads/google-gcs.json',
  projectId:'striimqa-214712',
  datalocation:'US',
  Tables:'public.dbr_pg234567890123456789source1,public.dbr_pg234567890123456789Target1;public.dbr_pg234567890123456789source2,public.dbr_pg234567890123456789Target2',
  BatchPolicy:"eventCount:1,Interval:90",
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
  directory: '',
  positionByEOF: false,
  WildCard: '' )
PARSE USING ParquetParser (
 )
OUTPUT TO @appname@Streams;

CREATE STREAM @appname@Stream3 OF Global.ParquetEvent;

CREATE STREAM @appname@Stream4 OF Global.ParquetEvent;

CREATE OR REPLACE CQ @appname@CQOrder3
INSERT INTO @appname@Stream3
SELECT
PUTUSERDATA(s,'schemaName',s.data.getSchema().getName())
FROM @appname@Streams s;

CREATE OR REPLACE CQ @appname@CQOrder4
INSERT INTO @appname@Stream4
SELECT
PUTUSERDATA(s2, 'customFilename', Userdata(s2, 'schemaName').toString().concat(".test"))
FROM @appname@Stream3 s2;

CREATE OR REPLACE TARGET @filetarget@ USING Global.FileWriter (
  flushpolicy: 'EventCount:10000,Interval:30s',
  directory: '',
  filename: '',
  rolloverpolicy: 'eventcount:10' )
FORMAT USING ParquetFormatter  (
schemafilename:''
)
INPUT FROM @appname@Stream4;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.OJet (
  PrimaryDatabasePassword: '@PRIMARY_PASSWORD@',
  ConnectionURL: '@DOWNSTREAM_URL@',
  PrimaryDatabaseUsername: '@PRIMARY_USER@',
  Password: '@DOWNSTREAM_PASSWORD@',
  DownstreamCaptureMode: 'REAL_TIME',
  DownstreamCapture: true,
  PrimaryDatabaseConnectionURL: '@PRIMARY_URL@',
  Tables: '@SOURCE_TABLES@',
  CDDLCapture: true,
  Username: '@DOWNSTREAM_USER@' )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET_SYS@ USING Global.SysOut (
  name: 'Out' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter (
  ConnectionURL: '@TARGET_URL@',
  Username: '@TARGET_USER@',
  Password: '@TARGET_PASSWORD@',
  CheckPointTable: 'CHKPOINT',
  CommitPolicy: 'EventCount:1',
  Tables: '@TARGET_TABLES@',
  BatchPolicy: 'EventCount:1' )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

stop Quiesce_CDC;
undeploy application Quiesce_CDC;
alter application Quiesce_CDC;
CREATE or replace FLOW Quiesce_CDC_flow;
Create or replace Source Quiesce_CDC_Oraclesrc Using oraclereader(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',
 Tables:'QATEST.QUIESCE_TABLE1',
 _h_fetchexactrowcount: 'true'
)
Output To Quiesce_CDC_OrcStrm;
END FLOW Quiesce_CDC_flow;
alter application Quiesce_CDC recompile;
DEPLOY APPLICATION Quiesce_CDC;
start application Quiesce_CDC;

stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@;

CREATE SOURCE @SourceName@ USING MySQLReader  ( 
ReaderType: 'LogMiner', 
  Password_encrypted: 'false', 
  DatabaseName: 'qatest',
  SupportPDB: false, 
  QuiesceMarkerTable: 'QUIESCEMARKER', 
  QueueSize: 2048, 
  CommittedTransactions: true, 
  Username: '@UserName@', 
  TransactionBufferType: 'Memory', 
  TransactionBufferDiskLocation: '.striim/LargeBuffer', 
  OutboundServerProcessName: 'WebActionXStream', 
  Password: '@Password@', 
  DDLCaptureMode: 'All', 
  Compression: false, 
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', 
  FetchSize: 1, 
  Tables: '@SourceTables@', 
  DictionaryMode: 'OnlineCatalog', 
  XstreamTimeOut: 600, 
  TransactionBufferSpilloverSize: '1MB', 
  FilterTransactionBoundaries: true, 
  StartSCN: 'null', 
  ConnectionURL: '@ConnectionURL@', 
  SendBeforeImage: true ) 
OUTPUT TO @AppStream@  ;

CREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream SELECT data[2], to_string(to_date(data[2]), "dd-MMM-yy hh.mm.ss") FROM @AppStream@ o ;

CREATE  TARGET @targetsys@ USING Global.SysOut  ( 
name: 'ora1_sys' ) 
INPUT FROM admin.ZDT_cq_stream;

create Target @TargetFile@ using FileWriter(
  filename:'toStringOut.log',
  directory:'@FilePath@',
  rolloverpolicy:'eventcount:1000'
)
format using DSVFormatter (

)
input from admin.ZDT_cq_stream;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadpolicy:'EventCount:7'
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from @STREAM@;
end application @APPNAME@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@;

-----------------------------------
stop application SourceAgentApp1;
undeploy application SourceAgentApp1;

stop application SourceAgentApp2;
undeploy application SourceAgentApp2;

stop application TargetServerApp;
undeploy application TargetServerApp;

drop application SourceAgentApp1 cascade;
drop application SourceAgentApp2 cascade;
drop application TargetServerApp cascade;


CREATE APPLICATION SourceAgentApp1;
create flow flow1;
create source CSVSource1 using FileReader (
directory: '@TEST-DATA-PATH@/tmp',
WildCard:'mybanks*',
positionByEOF: true,
charset:'UTF-8'
) parse using DSVParser (header:'no')
OUTPUT TO CsvStream;
end flow flow1;

--CREATE TARGET T USING Sysout(name:'sysout1') INPUT FROM CsvStream;

END APPLICATION SourceAgentApp1;

DEPLOY APPLICATION SourceAgentApp1 with flow1 in AGENTS;


CREATE APPLICATION SourceAgentApp2;

create flow flow2;
create source CSVSource2 using FileReader (
directory: '@TEST-DATA-PATH@/tmp',
WildCard:'mybanks*',
positionByEOF: true,
charset:'UTF-8'
) parse using DSVParser (header:'no')
OUTPUT TO CsvStream;
end flow flow2;

--CREATE TARGET T USING Sysout(name:'sysout2') INPUT FROM CsvStream;

END APPLICATION SourceAgentApp2;

DEPLOY APPLICATION SourceAgentApp2 with flow2 in AGENTS;


-- One app consuming from stream from 2 sources running in agent
CREATE APPLICATION TargetServerApp;
create flow flow3;

CREATE TARGET T5 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')
FORMAT USING JSONFormatter ()
INPUT FROM CsvStream;
end flow flow3;

END APPLICATION TargetServerApp;
deploy application TargetServerApp with flow3 in default;

stop application admin.@APPNAME@;
undeploy application admin.@APPNAME@;
alter application admin.@APPNAME@ AUTORESUME MAXRETRIES 5 RETRYINTERVAL 5;
Alter application admin.@APPNAME@ RECOMPILE;
Alter application admin.@APPNAME@;
create or replace source admin.@APPNAME@s using FileReader (
        directory:'Product/IntegrationTests/TestData/',
        wildcard:'posdata5L.csv',
        positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:',',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  header: true,
  separator:'~'
)
OUTPUT TO admin.@APPNAME@in_memory_rawStream;

Alter application admin.@APPNAME@ RECOMPILE;
deploy application admin.@APPNAME@;
start admin.@APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@ recovery 1 second interval;

CREATE OR REPLACE SOURCE @parquetsrc@ USING Global.HDFSReader (
  eofdelay: 100,
  wildcard: '@File@',
  rolloverstyle: 'Default',
  directory: '@DIR@',
  adapterName: 'HDFSReader',
  hadoopurl: 'hdfs://dockerhost:9000',
  hadoopconfigurationpath: '@CONF@',
  skipbom: true,
  includesubdirectories: false,
  positionbyeof: false )
  PARSE USING ParquetParser (
   )
OUTPUT TO @appname@ParquetStreams;

CREATE OR REPLACE CQ @appname@CQOrder3
INSERT INTO @appname@newStream3
SELECT
PUTUSERDATA(s,'schemaName',s.data.getSchema().getName())
FROM @appname@ParquetStreams s;

CREATE TARGET @avrotarget@ USING Global.FileWriter (
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000,Interval:30s',
  directory: '@FOLDER@',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  filename: 'AvroOutFile',
  DataEncryptionKeyPassphrase_encrypted: 'true',
  flushpolicy: 'EventCount:10,Interval:30s',
  rolloverpolicy: 'EventCount:10,Interval:30s' )
FORMAT USING Global.AvroFormatter  (
  schemaFileName: 'AvroSchema',
  formatAs: 'default',
  schemaregistryConfiguration: '' )
INPUT FROM @appname@newStream3;

CREATE TARGET @parquettarget@ USING Global.FileWriter (
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000,Interval:30s',
  directory: '@FOLDER@',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  filename: 'AvroOutFile',
  DataEncryptionKeyPassphrase_encrypted: 'true',
  flushpolicy: 'EventCount:10,Interval:30s',
  rolloverpolicy: 'EventCount:10,Interval:30s' )
FORMAT USING Global.ParquetFormatter  (
  blocksize: '128000000',
  compressiontype: 'UNCOMPRESSED',
  formatAs: 'Default',
  handler: 'com.webaction.proc.ParquetFormatter',
  formatterName: 'ParquetFormatter' )
INPUT FROM @appname@newStream3;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

use PosTester;
alter application PosApp;

CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

end application PosApp;
alter application PosApp recompile;

DROP APPLICATION SliderSorter CASCADE;

CREATE APPLICATION SliderSorter;

CREATE OR REPLACE TYPE OrdersDataPARSED_TP (
    tDateTime       org.joda.time.DateTime KEY,
    sBusinessName   java.lang.String KEY,
    sMerchantID     java.lang.String KEY,
    sOrderId        java.lang.String,
    sZip            java.lang.String,
    lTerminalID     java.lang.Long,
    fPaidAmount     java.lang.Float  
);

CREATE OR REPLACE STREAM OrdersDataPARSED_ST OF OrdersDataPARSED_TP;

CREATE OR REPLACE TYPE ReturnsDataPARSED_TP (
    tDateTime org.joda.time.DateTime KEY,
    sOrderId java.lang.String KEY,
    fReturnedAmount java.lang.Float
);

CREATE OR REPLACE STREAM ReturnsDataPARSED_ST OF ReturnsDataPARSED_TP;

CREATE OR REPLACE STREAM OrdersDataSORTED_ST OF OrdersDataPARSED_TP;
CREATE OR REPLACE STREAM ReturnsDataSORTED_ST OF ReturnsDataPARSED_TP;
CREATE OR REPLACE STREAM Errors_ST OF Global.WAEvent;

CREATE SORTER MySorter OVER
OrdersDataPARSED_ST  ON tDateTime OUTPUT TO OrdersDataSORTED_ST,
ReturnsDataPARSED_ST ON tDateTime OUTPUT TO ReturnsDataSORTED_ST
WITHIN 2 MINUTE
OUTPUT ERRORS TO Errors_ST;

END APPLICATION SliderSorter;

CREATE OR REPLACE PROPERTYVARIABLE RetryPolicy='timeOut=00,retryInterval=1,maxRetries=3';
CREATE OR REPLACE PROPERTYVARIABLE KafkaBrokerAddress='localhost:9099';
CREATE OR REPLACE PROPERTYVARIABLE KafkaConfig='request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;';

STOP APPLICATION @Appname@;
UNDEPLOY APPLICATION @Appname@;
DROP APPLICATION @Appname@ CASCADE;
CREATE APPLICATION @Appname@ @Recovery@;
CREATE FLOW @Appname@AgentFlow;
--Partitioning source stream with meta (tablename)
Create Source @Appname@s1 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
 Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss1 partition by meta(@Appname@ss1,'TableName');
-- (id integer,name1 string,name2 string) partition by sleft(name1,1)
-- select TO_INT(data[0]),TO_STRING(data[1]),TO_STRING(data[2]);


--Partitioning source stream using meta (STARTSCN)
Create Source @Appname@s2 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss2 partition by meta(@Appname@ss2,'STARTSCN');



CREATE TYPE @Appname@OpTableDataType(
  TableName String,
  STARTSCN String,
  data java.util.HashMap
);

--Partitioning typed stream inline while creation using expr
CREATE STREAM @Appname@OracleTypedStream OF @Appname@OpTableDataType partition by sright(STARTSCN,2);
Create Source @Appname@s3 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss3 partition by meta(@Appname@ss3,'STARTSCN');

CREATE CQ @Appname@ParseOracleRawStream
  INSERT INTO @Appname@OracleTypedStream
  SELECT META(@Appname@ss3, 'TableName').toString(),META(@Appname@ss3, 'STARTSCN').toString(),
    DATA(@Appname@ss3)
  FROM @Appname@ss3;

--Partitioning source stream with types defined inline using meta expr
Create Source @Appname@s4 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss4 (id integer,NAME string,COL1 string)
-- partition by meta(ss4,'STARTSCN')
select TO_INT(data[0]),TO_STRING(data[2]),TO_STRING(data[3]);

CREATE CQ @Appname@cqss4
  INSERT INTO @Appname@ss4new Partition by sright(id,1)
  SELECT * FROM @Appname@ss4;

--Partitioning source stream -(waevent-s5) with field that has null value

CREATE STREAM @Appname@s5 of Global.WAEvent PARTITION BY meta(@Appname@s5, 'TransactionName');

Create Source @Appname@src5 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss5;

END FLOW @Appname@AgentFlow;


CREATE FLOW @Appname@ServerFlow;
--CREATE STREAM modifyStream OF Global.WAEvent partition by meta(modifyStream,'STARTSCN');
CREATE STREAM @Appname@modifyStream OF Global.WAEvent partition by sleft(data[0],1);
CREATE CQ @Appname@modifycq INSERT INTO @Appname@modifyStream
SELECT * FROM @Appname@ss5
MODIFY
(
data[0] = (TO_INT(data[0]) * 10000) / 100
);


--Partitioning source stream -(waevent) with modified fields
Create Source @Appname@s6 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss6;

CREATE CQ @Appname@putUserDatacq1
INSERT INTO @Appname@newss6
PARTITION BY userdata(@Appname@newss6, 'Modified_Date')
SELECT
putUserData(x,'Modified_Date',TO_STRING(DNOW(),'yyyy-MM-dd HH:mm:ss'))
FROM @Appname@ss6 x;

--Partitioning stream with userdata
CREATE STREAM @Appname@newss7 of Global.WAEvent PARTITION BY userdata(@Appname@newss7, 'Modified_Date');

Create Source @Appname@s7 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss7;

CREATE CQ @Appname@putUserDatacq2
INSERT INTO @Appname@newss7
SELECT
putUserData(x,'Modified_Date',TO_STRING(DNOW(),'yyyy-MM-dd HH:mm:ss'))
FROM @Appname@ss7 x;


create Target @Appname@KW1 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr01',
Mode:'Async',
KafkaConfig: '$KafkaConfig'
)
FORMAT USING jsonFormatter ()
input from @Appname@ss1;

create Target @Appname@KW2 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr02',
Mode:'Async',
KafkaConfig: '$KafkaConfig'
        )
FORMAT USING jsonFormatter ()
input from @Appname@ss2;

create Target @Appname@KW3 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr03',
Mode:'Async',
KafkaConfig: '$KafkaConfig'
        )
FORMAT USING jsonFormatter ()
input from @Appname@OracleTypedStream;

create Target @Appname@KW4 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr04',
Mode:'sync',
KafkaConfig: '$KafkaConfig'
        )
FORMAT USING jsonFormatter ()
input from @Appname@ss4new;

create Target @Appname@KW5 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr05',
Mode:'sync',
KafkaConfig: '$KafkaConfig'
        )
FORMAT USING jsonFormatter ()
input from @Appname@modifyStream;

create Target @Appname@KW6 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr06',
Mode:'sync',
KafkaConfig: '$KafkaConfig'
        )
FORMAT USING jsonFormatter ()
input from @Appname@newss6;

create Target @Appname@KW7 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr07',
Mode:'sync',
KafkaConfig: '$KafkaConfig'
        )
FORMAT USING jsonFormatter ()
input from @Appname@newss7;
END FLOW @Appname@ServerFlow;
end application @Appname@;
--deploy application @Appname@ with @Appname@AgentFlow in Agents, @Appname@ServerFlow in default;
deploy application @Appname@;
start @Appname@;

stop application @Appname@KR;
undeploy application @Appname@KR;
drop application @Appname@KR cascade;
create application @Appname@KR;
CREATE STREAM @Appname@KafkaStream of Global.jsonnodeEvent;
alter stream @Appname@KafkaStream PARTITION BY meta(@Appname@KafkaStream, 'PartitionID');
CREATE SOURCE @Appname@KafkaSource1 USING KafkaReader Version '2.1.0'
(
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr01',
startOffset:0
)
PARSE USING jsonParser ()
OUTPUT TO @Appname@KafkaStream;

create Target @Appname@t2 using filewriter(
	filename:'%n%',
	rolloverpolicy:'EventCount:5000000',
    directory:'FEATURE-DIR/logs/%@metadata(TopicName)%/%@metadata(PartitionID)%'
)
format using jsonFormatter (
)
input from @Appname@KafkaStream;

end application @Appname@KR;
deploy application @Appname@KR;
--start @Appname@KR;

STOP APPLICATION @Appname@FR;
UNDEPLOY APPLICATION @Appname@FR;
DROP APPLICATION @Appname@FR CASCADE;
CREATE APPLICATION @Appname@FR;
CREATE SOURCE @Appname@FS0 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS0;

CREATE SOURCE @Appname@FS1 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS1;

CREATE SOURCE @Appname@FS2 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS2;

CREATE SOURCE @Appname@FS3 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS3;

CREATE SOURCE @Appname@FS4 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS4;

CREATE SOURCE @Appname@FS5 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS5;

CREATE SOURCE @Appname@FS6 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS6;
end application @Appname@FR;
deploy application @Appname@FR;

STOP APPLICATION DBRTOCW;
UNDEPLOY APPLICATION DBRTOCW;
DROP APPLICATION DBRTOCW CASCADE;
CREATE APPLICATION DBRTOCW;

create source CSVSource using FileReader (
	directory:'/Users/jenniffer/Product2/IntegrationTests/TestData/',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	
)
OUTPUT TO FileStream
(
id String,
ename String
)
select 
data[2],
data[0];

create Target t2 using SysOut(name:OrgData) input from FileStream;
CREATE OR REPLACE Target DBTarget USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1000,Interval:60',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  Tables: 'test.employee',
  Password: 'cassandra',
  Password_encrypted: false
 )INPUT FROM FileStream;

END APPLICATION DBRTOCW;
DEPLOY APPLICATION DBRTOCW;
START APPLICATION DBRTOCW;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;
CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'TargetPosDataXmlEC',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'eventcount:2000,sequence:00'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:zip:text=merchantname'
)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlEC_actual.log') input from TypedCSVStream;

end application DSV;

STOP APPLICATION @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;
CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9099', kafkaversion:'0.11');

CREATE FLOW @APPNAME@_AgentFlow;

CREATE STREAM @APPNAME@_PS_Stream1 OF Global.waevent persist using  @APPNAME@_KafkaPropset;
CREATE STREAM @APPNAME@_PS_Stream2 OF Global.waevent persist using  @APPNAME@_KafkaPropset;
CREATE STREAM @APPNAME@_PS_Stream3 OF Global.waevent persist using  @APPNAME@_KafkaPropset;
CREATE STREAM @APPNAME@_PS_Stream4 OF Global.waevent persist using  @APPNAME@_KafkaPropset;


create source @APPNAME@_source using FileReader (
directory:'/Users/jenniffer/Product2/IntegrationTests/TestData/OGG/Recovery',
WildCard:'lg*.gz',
positionByEOF:false,
compressiontype:'gzip',
recoveryInterval: 5
) parse using GGTrailParser (
metadata:'@META-FILE@'
)
OUTPUT TO @APPNAME@_Stream;

END FLOW @APPNAME@_AgentFlow;

CREATE CQ @APPNAME@_CQ1
INSERT INTO @APPNAME@_PS_Stream1
SELECT *
FROM @APPNAME@_Stream sm
WHERE META(sm, 'TableName').toString() = 'MINER.CUSTOMER';

CREATE CQ @APPNAME@_CQ2
INSERT INTO @APPNAME@_PS_Stream2
SELECT *
FROM @APPNAME@_Stream sm
WHERE META(sm, 'TableName').toString() = 'MINER.CUSTOMER';

CREATE CQ @APPNAME@_CQ3
INSERT INTO @APPNAME@_PS_Stream3
SELECT *
FROM @APPNAME@_Stream sm
WHERE META(sm, 'TableName').toString() = 'MINER.CUSTOMER';

CREATE CQ @APPNAME@_CQ4
INSERT INTO @APPNAME@_PS_Stream4
SELECT *
FROM @APPNAME@_Stream sm
WHERE META(sm, 'TableName').toString() = 'MINER.CUSTOMER';


CREATE FLOW @APPNAME@_ServerFlow;

CREATE TARGET @APPNAME@_target1 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'MINER.CUSTOMER,QATEST.CUSTOMER_TARGET_AG4'
) INPUT FROM @APPNAME@_PS_Stream1;

CREATE TARGET @APPNAME@_target2 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'MINER.CUSTOMER,QATEST.CUSTOMER_TARGET_AG4'
) INPUT FROM @APPNAME@_PS_Stream2;

CREATE TARGET @APPNAME@_target3 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'MINER.CUSTOMER,QATEST.CUSTOMER_TARGET_AG4'
) INPUT FROM @APPNAME@_PS_Stream3;

CREATE TARGET @APPNAME@_target4 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'MINER.CUSTOMER,QATEST.CUSTOMER_TARGET_AG4'
) INPUT FROM @APPNAME@_PS_Stream4;

END FLOW @APPNAME@_ServerFlow;

end application @APPNAME@;

deploy application @APPNAME@ with @APPNAME@_AgentFlow in AGENTS, @APPNAME@_ServerFlow on any in default;

start application @APPNAME@;

STOP APPLICATION DatabaseWriterTester.DatabaseReaderApp;
UNDEPLOY APPLICATION DatabaseWriterTester.DatabaseReaderApp;
DROP APPLICATION DatabaseWriterTester.DatabaseReaderApp CASCADE;
STOP APPLICATION DatabaseWriterTester.DatabaseWriterApp;
UNDEPLOY APPLICATION DatabaseWriterTester.DatabaseWriterApp;
DROP APPLICATION DatabaseWriterTester.DatabaseWriterApp CASCADE;

CREATE APPLICATION DatabaseWriterApp;

CREATE SOURCE CSVSource USING FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'banks.csv',
positionByEOF:false
)
PARSE USING DSVParser (
header:'yes'
)
OUTPUT TO Orders;

CREATE TYPE OrdersData(
  id String Key,
  name String);

CREATE STREAM OrdersDataStream OF OrdersData;

CREATE CQ CsvToOrdersData
INSERT INTO OrdersDataStream (id, name)
SELECT data[0], data[1]
FROM Orders;

CREATE TARGET WriteJpaOracle USING DatabaseWriter(
DriverName:'oracle.jdbc.OracleDriver',
ConnectionURL:'@READER-URL@',
        Username:'@READER-UNAME@',
        Password:'@READER-PASSWORD@',
Tables:'QATEST.DB_WRITER_TEST',
eventType:'DatabaseWriterTester.OrdersData'
) INPUT FROM OrdersDataStream;

END APPLICATION DatabaseWriterApp;
DEPLOY APPLICATION DatabaseWriterApp;
START APPLICATION DatabaseWriterApp;

CREATE APPLICATION DatabaseReaderApp;

create source DatabaseReaderSource USING DatabaseReader
(
ConnectionURL:'@READER-URL@',
        Username:'@READER-UNAME@',
        Password:'@READER-PASSWORD@',
Tables:'QATEST.DB_WRITER_TEST',
   FetchSize: 1

)
OUTPUT TO DatabaseReaderTestStream;

create Target t using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/Databasewritertarget_RT') input from DatabaseReaderTestStream;


END APPLICATION DatabaseReaderApp;
DEPLOY APPLICATION DatabaseReaderApp;

CREATE OR REPLACE APPLICATION OtelMonitoringApp;

CREATE OR REPLACE TYPE MonitorBatchEvent_Type (
items java.util.ArrayList
);
CREATE OR REPLACE STREAM MonitorBatchStream OF MonitorBatchEvent_Type;

CREATE OR REPLACE TYPE MonitorEvent_Type (
item com.webaction.runtime.monitor.MonitorEvent
);
CREATE OR REPLACE STREAM MonitorEventStream OF MonitorEvent_Type;


CREATE OR REPLACE CQ MonitorBatchCQ
INSERT INTO MonitorBatchStream
select filterMonEventsForOtelWriter(ms,'INPUT','MEMORY_USED_PERCENT','DISK_FREE','CPU_PER_CORE_PCT','LAG_END2END', 'STATUS_CHANGE') from global.MonitoringSourceStream ms;

CREATE CQ MonitorEventCQ
INSERT INTO MonitorEventStream
SELECT ri FROM MonitorBatchStream m, ITERATOR(m.items) ri;

CREATE OR REPLACE Target OTelWriter1 USING StriimOTelWriter INPUT FROM MonitorEventStream;

END APPLICATION OtelMonitoringApp;
deploy application OtelMonitoringApp;
start OtelMonitoringApp;

Stop @APPNAME@_App;
undeploy application @APPNAME@_App;
drop application @APPNAME@_App cascade;

CREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;
CREATE SOURCE @APPNAME@_Source USING Global.OracleReader

(
  FetchSize:'1',
  Username:'@SOURCE_USER@',
  Password:'@SOURCE_PASSWORD@',
  ConnectionURL:'@SOURCE_URL@',
  Tables:'@SOURCE_TABLES@'
)
OUTPUT TO @APPNAME@_Stream;

CREATE TARGET @APPNAME@_SYS USING SysOut (
  name: '@APPNAME@_SYS' )
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_CDB_Target USING DatabaseWriter

(
  Username:'@TARGET_CDB_USER@',
  ConnectionURL:'@TARGET_CDB_URL@',
  Tables:'@TARGET_CDB_TABLES@',
  Password:'@TARGET_CDB_PASSWORD@',
  BatchPolicy:'EventCount:1,Interval:1',
  CommitPolicy:'EventCount:1,Interval:1'
)
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_PDB_Target USING DatabaseWriter

(
  Username:'@TARGET_PDB_USER@',
  ConnectionURL:'@TARGET_PDB_URL@',
  Tables:'@TARGET_PDB_TABLES@',
  Password:'@TARGET_PDB_PASSWORD@',
  BatchPolicy:'EventCount:1,Interval:1',
  CommitPolicy:'EventCount:1,Interval:1'
)
INPUT FROM @APPNAME@_Stream;

End APPLICATION @APPNAME@_App;

CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @APPNAME@_DBSource USING Global.OracleReader (
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  Compression: false,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Password_encrypted: 'false',
  SupportPDB: false,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  FetchSize: 1000,
  CDDLAction: 'Process',
  ConnectionURL: '192.168.56.3:1521:orcl',
  DictionaryMode: 'OnlineCatalog',
  QueueSize: 2048,
  CommittedTransactions: true,
  SetConservativeRange: false,
  CDDLCapture: false,
  Username: 'fan',
  Tables: 'FAN.S_BLOB',
  TransactionBufferType: 'Disk',
  Password: '9S5GnbGmBQNDD5c/baD0Tw==',
  TransactionBufferSpilloverSize: '100MB',
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  DatabaseRole: 'Primary' )
OUTPUT TO @APPNAME@_stream;

CREATE OR REPLACE TARGET @APPNAME@_target USING Global.BigQueryWriter (
  ColumnDelimiter: '|',
  projectId: 'striim-support',
  BatchPolicy: 'eventCount:1, Interval:1',
  NullMarker: 'NULL',
  streamingUpload: 'false',
  ServiceAccountKey: '/Users/fzhang/fan/u01/app/product/striim/striim_latest/UploadedFiles/striim-support-286429beb74d.json',
  Encoding: 'UTF-8',
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30',
  AllowQuotedNewLines: 'false',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  Tables: 'FAN.S_BLOB,Fan.s_blob columnmap(A=A,B=B,C=C,D=C,E=@metadata(OperationName));',
  TransportOptions: 'connectionTimeout=300, readTimeout=120',
  adapterName: 'BigQueryWriter',
  Mode: 'MERGE',
  StandardSQL: 'true',
  includeInsertId: 'true',
  QuoteCharacter: '\"' )
INPUT FROM @APPNAME@_stream;


END APPLICATION @APPNAME@;

STOP bq;
UNDEPLOY APPLICATION bq;
DROP APPLICATION bq CASCADE;

CREATE APPLICATION bq RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:orcl',
	Tables: 'QATEST.TABLE_TEST_1000100',
	DictionaryMode: offlineCatalog,
	FetchSize: '1'
)
OUTPUT TO SS;


CREATE or replace TARGET T USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
    Tables:'QATEST.TABLE_TEST_1000100,qatest.% keycolumns(RONUM)',
    mode:'Appendonly',
    datalocation: 'US',
	nullmarker: 'defaultNULL',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:100,Interval:10'	
) INPUT FROM ss;

END APPLICATION bq;
DEPLOY APPLICATION bq;
START APPLICATION bq;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '@GRPPOLICY@',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'MultipleEvents',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
  filename: '%@metadata(FileName)%',
  directory: '@DIR@',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter()
INPUT FROM CCBStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

CREATE APPLICATION jsonapp;

CREATE TYPE JSONAccessLogEntry (
    srcIp String KEY,
    userId String,
    sessionId String,
    accessTime long,
    request String,
    code int,
    size int,
    referrer String,
    userAgent String,
    responseTime int
);
CREATE STREAM JSONSourceStream OF JSONAccessLogEntry;

CREATE SOURCE JSONAccessLogSource USING FileReader(
  directory:'@TEST-DATA-PATH@',
  wildcard:'FileWithJSONTest.json',
  positionbyeof:false
)
parse using JSONParser (
  eventType:'admin.JSONAccessLogEntry'
) OUTPUT TO JSONSourceStream;

create Target JSONDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/jsondata') input FROM JSONSourceStream;

END APPLICATION jsonapp;

--
-- Recovery Test 31 with two sources, two sliding count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5W/p -> CQ1 -> WS
-- S2 -> Sc6W/p -> CQ2 -> WS
--

STOP Recov31Tester.RecovTest31;
UNDEPLOY APPLICATION Recov31Tester.RecovTest31;
DROP APPLICATION Recov31Tester.RecovTest31 CASCADE;
CREATE APPLICATION RecovTest31 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION RecovTest31;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW;
CREATE OR REPLACE PROPERTYSET KafkaProps(zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.9');
CREATE STREAM Oracle_ChangeDataStream1 of Global.WAEvent PERSIST USING KafkaProps;
CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  QuiesceMarkeRTable:'QATEST.QUIESCEMARKER',
  Password_encrypted: false,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//127.0.0.1:1521/xe',
  Tables: 'QATEST.OracToCql_alldatatypes',
  adapterName: 'OracleReader',
  Password: 'miner',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: 'miner',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream1;
CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'test.chkpoint',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1,Interval:60',
  CommitPolicy: 'EventCount:1,Interval:60',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  Tables: 'QATEST.OracToCql_alldatatypes,test.oractocq_alldatatypes',
  Password: 'cassandra',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream1;
create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream1;
END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]);

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

CREATE OR REPLACE EMBEDDINGGENERATOR @EMB_NAME@ USING @MODEL@ (
modelProvider: '@MODEL@',
modelName: '@MODEL_NAME@',
apiKey: '@API_KEY@'
);

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (
  ConnectionURL: '@CONN_URL@',
  Tables: '@TABLES@',
  Username: '@USERNAME@',
  Password: '@PASSWORD@'
) OUTPUT TO @APP_NAME@_Stream;


CREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.FabricDataWarehouseWriter (
  Tables: '@TABLES@',
  ConnectionURL: '@CONN_URL@',
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  uploadpolicy: 'eventcount:1',
  AccountName: '@ACCOUNTNAME@')
INPUT FROM @APP_NAME@_Stream;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

create Target FileTarget using FileWriter(
    rolloverpolicy:'@UPLOAD-SIZE@',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using DSVFormatter (
charset:'@charset@',
members:'@mem@',
header:'@head@'

)
input from CsvStream;

end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE OR REPLACE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;

CREATE OR REPLACE SOURCE @SOURCE@ USING SalesForceReader (
  autoAuthTokenRenewal: 'true',
  Username: '@userName@',
  securityToken: '@securityToken@',
  sObjects: '@srcObjectName@',
  pollingInterval: '1 min',
  Password_encrypted: 'false',
  securityToken_encrypted: 'false',
  customObjects: 'False',
  consumerKey: '@consumerKey@',
  startTimestamp: '',
  apiEndPoint: 'https://ap2.salesforce.com',
  mode: 'Incremental',
  consumerSecret: '@consumerSecert@',
  consumerSecret_encrypted: 'false',
  Password: '@Password@' )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@ USING Global.BigQueryWriter (
  ColumnDelimiter: '|',
  NullMarker: 'NULL',
  ConnectionRetryPolicy: 'retryInterval=30,\n maxRetries=3',
  streamingUpload: 'false',
  Mode: 'Merge',
  projectId: '@ProjectId@',
  Encoding: 'UTF-8',
  TransportOptions: 'connectionTimeout=300,\n readTimeout=120',
  Tables: '@srcObjectName@,@TargetTableName@ columnmap(ID=ID,checkbool__c=checkbool__c,dt__c=dt__c,percnt__c=percnt__c,phn__c=phn__c,txtlong__c=txtlong__c,url1__c=url1__c);',
  AllowQuotedNewlines: 'false',
  CDDLAction: 'Process',
  adapterName: 'BigQueryWriter',
  serviceAccountKey: '@GCS-AuthPath@',
  optimizedMerge: 'true',
  StandardSQL: 'true',
  includeInsertId: 'true',
  QuoteCharacter: '\"',
  BatchPolicy: 'eventCount:1000,Interval:10' )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE OR REPLACE TYPE @appname@CQOUT1_Type (
 companyName java.lang.String,
 merchantId java.lang.String,
 dateTime org.joda.time.DateTime,
 hourValue java.lang.String,
 amount java.lang.String,
 zip java.lang.String,
 FileName java.lang.String);

CREATE SOURCE @parquetsrc@ USING S3Reader (
    bucketname:'',
    objectnameprefix:''
     )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE OR REPLACE STREAM @appname@CQOUT1 OF @appname@CQOUT1_Type;
CREATE OR REPLACE CQ @appname@CQ_PQEvent
INSERT INTO @appname@CQOUT1
    Select
    data.get("companyName").toString(),
    data.get("merchantId").toString(),
    TO_DATE(data.get("dateTime").toString()),
    data.get("hourValue").toString(),
    data.get("amount").toString(),
    data.get("zip").toString(),
    metadata.get("FileName").toString()
    FROM @appname@Stream p;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using AvroFormatter (
schemaFileName: 'AvroS3Schema'
)
input from @appname@CQOUT1;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using JSONFormatter ()
INPUT FROM @appname@CQOUT1;


CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using JSONFormatter (
)
INPUT FROM @appname@CQOUT1;

CREATE OR REPLACE TARGET @dbtarget@ USING DatabaseWriter (
  Tables: '',
  ConnectionURL:'',
  Username:'',
  Password:'',
  CommitPolicy: 'EventCount:1,Interval:0',
  BatchPolicy:'EventCount:1,Interval:0'
)
INPUT FROM @appname@CQOUT1;

CREATE TARGET @bqtarget@ USING BigQueryWriter (
  Tables: '',
  projectId:'',
  BatchPolicy: 'eventCount:1, Interval:1',
  ServiceAccountKey: '',
   )
INPUT FROM @appname@CQOUT1;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

create application CSVToJSON;
create source CSVSource using FileReader (
	directory:'Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
	filename:'posdata_JSON',
	rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'
)
format using JSONFormatter (
	members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)

input from TypedCSVStream;
end application CSVToJSON;

deploy application CSVToJSON;
start application CSVToJSON;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE TYPE @appname@CQOUT1_Type (
 companyName java.lang.String,
 merchantId java.lang.String,
 dateTime org.joda.time.DateTime,
 hourValue java.lang.String,
 amount java.lang.String,
 zip java.lang.String,
 FileName java.lang.String);

CREATE SOURCE @parquetsrc@ USING S3Reader (
    bucketname:'',
    objectnameprefix:'',
    foldername:''
     )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE OR REPLACE STREAM @appname@CQOUT1 OF @appname@CQOUT1_Type;
CREATE OR REPLACE CQ @appname@CQ_PQEvent
INSERT INTO @appname@CQOUT1
    Select
    data.get("companyName").toString(),
    data.get("merchantId").toString(),
    TO_DATE(data.get("dateTime").toString()),
    data.get("hourValue").toString(),
    data.get("amount").toString(),
    data.get("zip").toString(),
    metadata.get("FileName").toString()
    FROM @appname@Stream p;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using DSVFormatter ()
input from @appname@CQOUT1;

END APPLICATION @appname@;
deploy application @appname@ on @node@ in default;
start application @appname@;

CREATE  SOURCE @SOURCE_NAME@ USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@1;

CREATE  SOURCE @SOURCE_NAME@2 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@2;

CREATE  SOURCE @SOURCE_NAME@3 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@3;

CREATE  SOURCE @SOURCE_NAME@4 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@4;

CREATE  SOURCE @SOURCE_NAME@5 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@5;

CREATE  SOURCE @SOURCE_NAME@6 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@6;

CREATE  SOURCE @SOURCE_NAME@7 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@7;

CREATE  SOURCE @SOURCE_NAME@8 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@8;

CREATE  SOURCE @SOURCE_NAME@9 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@9;

CREATE  SOURCE @SOURCE_NAME@10 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@10;

CREATE OR REPLACE TARGET @TARGET_NAME@ using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
		BatchPolicy: 'EventCount:1',
  		CommitPolicy: 'EventCount:1',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@2 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
  		BatchPolicy: 'Interval:10',
  		CommitPolicy: 'Interval:10',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@3 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
		BatchPolicy: 'eventCount:100000,Interval:20',
		CommitPolicy: 'eventCount:100000,Interval:20',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@4 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
  		BatchPolicy: 'EventCount:1',
		CommitPolicy: 'EventCount:1',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10,interval:30s'
) INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;;

create Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

CREATE APPLICATION DSV;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE  SOURCE CSVSource USING FileReader (
  directory: '@TEST-DATA-PATH@',
  WildCard: 'posdata.csv',
  positionByEOF: false,
  charset: 'UTF-8'
 )
 PARSE USING DSVParser (
  header: 'yes'
 )
OUTPUT TO CsvStream;

CREATE OR REPLACE TARGET t USING FileWriter (
  filename: 'TargetDefault',
  directory:'@FEATURE-DIR@/logs/',
  sequence:'00',
  --flushinterval: '0',
  rolloverpolicy:'EventCount:5000000,Interval:200s'
 )
 format using DSVFormatter (

)
INPUT FROM TypedCSVStream;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetDefaultPD_actual.log') input from TypedCSVStream;

END APPLICATION DSV;

create or replace type @STREAM@details(
C_CUSTKEY int,
C_MKTSEGMENT String,
C_NATIONKEY int,
C_NAME String,
C_ADDRESS String,
C_PHONE String,
C_ACCTBAL int,
C_COMMENT String
);

create or replace stream @STREAM@_TYPED of @STREAM@details;

Create or replace CQ @STREAM@detailsCQ
insert into @STREAM@_TYPED
select 
to_int(data[0]),data[1],to_int(data[2]),data[3],data[4],data[5],to_int(data[6]),data[7]
from @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 ) 
INPUT FROM @STREAM@_TYPED;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE OR REPLACE SOURCE @parquetsrc@ USING Global.HDFSReader (
  wildcard: '',
  directory: '',
  hadoopurl: '',
  hadoopconfigurationpath: '',
  positionbyeof: false )
  PARSE USING ParquetParser (
   )
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING AvroFormatter  (
schemaFileName: 'AvroFileSchema'
)
INPUT FROM @appname@CqOut;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using AvroFormatter (
schemaFileName: 'AvroS3Schema'
)
input from @appname@CqOut;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using AvroFormatter (
schemaFileName: 'AvroAzureSchema'
)
INPUT FROM @appname@CqOut;

CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using AvroFormatter (
schemaFileName: 'AvroGCSSchema'
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;