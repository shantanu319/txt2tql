stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@;

CREATE SOURCE @SourceName@ USING PostgreSQLReader  ( 
ReaderType: 'LogMiner', 
  Password_encrypted: 'false', 
  SupportPDB: false, 
  ReplicationSlotName: 'test_slot',
  QuiesceMarkerTable: 'QUIESCEMARKER', 
  QueueSize: 2048, 
  CommittedTransactions: true, 
  Username: '@UserName@', 
  TransactionBufferType: 'Memory', 
  TransactionBufferDiskLocation: '.striim/LargeBuffer', 
  OutboundServerProcessName: 'WebActionXStream', 
  Password: '@Password@', 
  DDLCaptureMode: 'All', 
  Compression: false, 
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', 
  FetchSize: 1, 
  Tables: '@SourceTables@', 
  DictionaryMode: 'OnlineCatalog', 
  XstreamTimeOut: 600, 
  TransactionBufferSpilloverSize: '1MB', 
  FilterTransactionBoundaries: true, 
  ConnectionURL: '@ConnectionURL@', 
  SendBeforeImage: true ) 
OUTPUT TO @AppStream@  ;

CREATE OR REPLACE CQ @cqName@ INSERT INTO admin.ZDT_cq_stream SELECT data[2], to_string(to_date(data[2]), "dd-MMM-yy hh.mm.ss") FROM @AppStream@ o ;

CREATE  TARGET @targetsys@ USING Global.SysOut  ( 
name: 'ora1_sys' ) 
INPUT FROM admin.ZDT_cq_stream;

create Target @TargetFile@ using FileWriter(
  filename:'toStringOut.log',
  directory:'@FilePath@',
  rolloverpolicy:'eventcount:1000'
)
format using DSVFormatter (

)
input from admin.ZDT_cq_stream;

END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

CREATE  SOURCE @QuerySource@1 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@1;
create Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;
 CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@1;

CREATE  SOURCE @QuerySource@2 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@2;
create Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;
 CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@2;

CREATE  SOURCE @QuerySource@3 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@3;
create Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;
 CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@3;

CREATE  SOURCE @QuerySource@4 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@4;
create Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;
 CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@4;

CREATE  SOURCE @QuerySource@5 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@5;
create Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;
 CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@5;

CREATE  SOURCE @QuerySource@6 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@6;
create Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;
 CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@6;

CREATE  SOURCE @QuerySource@7 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@7;
create Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;
 CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@7;

CREATE  SOURCE @QuerySource@8 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@8;
create Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;
 CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@8;

CREATE  SOURCE @QuerySource@9 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@9;
create Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;
 CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@9;

CREATE  SOURCE @QuerySource@10 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@10;
create Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;
 CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@10;
end application @APPNAME@;

stop OracleReaderToDBWriter;
Undeploy application OracleReaderToDBWriter;
alter application OracleReaderToDBWriter;
CREATE OR REPLACE SOURCE Oraclesrc USING OracleReader  ( 
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Tables: 'QATEST.Orcalesrc',
  FetchSize: '3'
 ) 
OUTPUT TO OrcStrm;
alter application OracleReaderToDBWriter recompile;
DEPLOY APPLICATION OracleReaderToDBWriter with Hz_Agent_flow on any in AGENTS;
start application OracleReaderToDBWriter;

create application KinesisTest;
CREATE OR REPLACE SOURCE ora_reader USING OracleReader (
  Username: 'miner',
  Password: 'miner',
  ConnectionURL: '192.168.1.113:1521:ORCL',
  TABLES: 'QATEST.H_REGION;QATEST.H_NATION;QATEST.H_CUSTOMER',
  FetchSize: '1'
 )
OUTPUT TO DDLCDCStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	accesskeyid:'ACCESS_KEY',
	secretaccesskey:'SECRET_KEY'
)
format using JSONFormatter (
)
input from DDLCDCStream;

end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING DatabaseReader  (
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  Tables: @SOURCE_TABLE@,
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true'
 )
OUTPUT TO @STREAM@;

CREATE ROUTER @SOURCE_NAME@_ROUTER INPUT FROM @STREAM@ cs CASE
WHEN meta(cs,"OperationName").toString()='SELECT' THEN ROUTE TO @STREAM@1, ELSE ROUTE TO @STREAM@2;

STOP APPLICATION bq;
UNDEPLOY APPLICATION bq;
DROP APPLICATION bq CASCADE;
CREATE APPLICATION bq;

CREATE SOURCE s USING FileReader
(
  directory:'/Users/sujith_syk/MyTasks/Analyse_BQtableScan_on_partitioned_table_merge_query',
  WildCard:'testdata1.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO FileStream;

CREATE TYPE cdctype(
    STORE_ID  String,
    NAME  String,
    CITY  String,
    STATE  String,
    ZIP  String,
    CUSTOMER_ACCOUNT_NUMBER  String,
    ORDER_ID  String,
    SKU  String,
    ORDER_AMOUNT  String,
    DATETIME  String
);

CREATE STREAM cdctypestream OF cdctype;

CREATE CQ cdcstreamcq
INSERT INTO cdctypestream
SELECT TO_STRING(p.data[0]),
       TO_STRING(p.data[1]),
       TO_STRING(p.data[2]),
       TO_STRING(p.data[3]),
       TO_STRING(p.data[4]),
       TO_STRING(p.data[5]),
       TO_STRING(p.data[6]),
       TO_STRING(p.data[7]),
       TO_STRING(p.data[8]),
       TO_STRING(p.data[9])
FROM FileStream p;


CREATE TARGET t1 USING BigQueryWriter
(
  ServiceAccountKey:'/Users/sujith_syk/Documents/striimdev-creds.json',
  projectId:'striimdev',
  Tables:'qatest.FILETOBQHEAPTEST1',
  datalocation:'US',
  nullmarker:'NOTNULL',
  columnDelimiter:'|',
  BatchPolicy:'eventCount:100, Interval:180',
  Mode: 'MERGE'
) INPUT FROM cdctypestream;

CREATE TARGET t2 USING BigQueryWriter
(
  ServiceAccountKey:'/Users/sujith_syk/Documents/striimdev-creds.json',
  projectId:'striimdev',
  Tables:'qatest.FILETOBQHEAPTEST2',
  datalocation:'US',
  nullmarker:'NOTNULL',
  columnDelimiter:'|',
  BatchPolicy:'eventCount:100, Interval:180',
  Mode: 'MERGE'
) INPUT FROM cdctypestream;

CREATE TARGET t3 USING BigQueryWriter
(
  ServiceAccountKey:'/Users/sujith_syk/Documents/striimdev-creds.json',
  projectId:'striimdev',
  Tables:'qatest.FILETOBQHEAPTEST3',
  datalocation:'US',
  nullmarker:'NOTNULL',
  columnDelimiter:'|',
  BatchPolicy:'eventCount:100, Interval:180',
  Mode: 'MERGE'
) INPUT FROM cdctypestream;

END APPLICATION bq;
DEPLOY APPLICATION bq;
START APPLICATION bq;

CREATE APPLICATION ExportApp2_API;

CREATE OR REPLACE SOURCE ExportApp2_API_FileSource USING FileReader ( 
  wildcard: 'posdata.csv', 
  positionByEOF: false, 
  directory: '@testdata_dir@' ) 
PARSE USING DSVParser ( 
 ) 
OUTPUT TO ExportApp2_API_SampleStream;

CREATE OR REPLACE TARGET ExportApp2_API_NullTarget USING NullWriter ( 
 ) 
INPUT FROM ExportApp2_API_SampleStream;

END APPLICATION ExportApp2_API;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;

drop propertyset OrcToSpnPlatfm_App_KafkaPropset;
drop stream  OrcToSpnPlatfm_Stream CASCADE;


--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;

CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNEC4TION_URL@',
  Tables: '$table1',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING SpannerWriter (
  Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'QATEST.ORCTOSPNPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING SpannerWriter (
  Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
 )
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '$table2',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using OracleReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@4',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING SpannerWriter  (
  Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'QATEST.ORCTOSPNPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING SpannerWriter  (
  Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
 )
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;

END APPLICATION @APPNAME@2;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@ recovery 1 second interval;

CREATE SOURCE @parquetsrc@ USING FileReader (
  directory: '',
  positionByEOF: false,
  WildCard: '' )
PARSE USING ParquetParser ()
OUTPUT TO @appname@Streams;

CREATE OR REPLACE CQ @appname@CQOrder3
INSERT INTO @appname@Stream3
SELECT
PUTUSERDATA(s,'schemaName',s.data.getSchema().getName())
FROM @appname@Streams s;

CREATE TARGET @adlstarget@ USING Global.ADLSGen2Writer (
    accountname:'',
  	sastoken:'',
  	filesystemname:'',
  	filename:'',
  	directory:'',
  	uploadpolicy:'eventcount:10' )
format using ParquetFormatter (
schemaFileName: 'ParquetSchema'
)
INPUT FROM @appname@Stream3;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING MariaDbXpandReader
(
Username: '@READER-UNAME@',
Password: '@READER-PASSWORD@',
ConnectionURL: '@CDC-READER-URL@',
Tables: @WATABLES@,
sendBeforeImage:'true',
FilterTransactionBoundaries: 'true'
) 
OUTPUT TO @STREAM@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using googlepubsubwriter(
    ServiceAccountKey:'@SAS-KEY@',
ProjectId:'@PROJECTID@',
topic:'@topic@',
BatchPolicy:'@BATCHPOLICY@'
)
format using DSVFormatter (
)
input from @STREAM@;

end application @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

CREATE SOURCE @SourceName@ USING MySqlReader  ( 
TransactionSupport: false, 
  FetchTransactionMetadata: false, 
  Compression: false, 
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', 
  Password_encrypted: 'false', 
  ConnectionURL: '@ConnectionURL@', 
  Fetchsize: 0, 
  ConnectionPoolSize: 10, 
  Username: '@UN@', 
  cdcRoleName: 'STRIIM_READER', 
  Password: '@PWD@', 
  Tables: 'qatest.%', 
  FilterTransactionBoundaries: true, 
  SendBeforeImage: true, 
  AutoDisableTableCDC: false ) 
OUTPUT TO @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;


CREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'), 'OpTime',META(x, 'TimeStamp'))) FROM @SRCINPUTSTREAM@ x; ;

CREATE TARGET @targetName@ USING DatabaseWriter ( 
ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  ParallelThreads: '', 
  CheckPointTable: 'CHKPOINT', 
  Password_encrypted: 'false', 
  ConnectionURL: '@ConnectionURL@', 
  CommitPolicy: 'EventCount:1,Interval:60', 
  StatementCacheSize: '50', 
  DatabaseProviderType: 'Default', 
  Username: '@UN@', 
  Password: '@PWD@', 
  PreserveSourceTransactionBoundary: 'false', 
  BatchPolicy: 'EventCount:1,Interval:60', 
  Tables: 'qatest.%, dbo.% columnmap(opt_type=@USERDATA(OpType),opt_time=@USERDATA(OpTime));' ) 
INPUT FROM admin.sqlreader_cq_out;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

DROP APPLICATION SliderSorter CASCADE;

CREATE APPLICATION SliderSorter;

CREATE OR REPLACE TYPE OrdersDataPARSED_TP (
    tDateTime       org.joda.time.DateTime KEY,
    sBusinessName   java.lang.String KEY,
    sMerchantID     java.lang.String KEY,
    sOrderId        java.lang.String,
    sZip            java.lang.String,
    lTerminalID     java.lang.Long,
    fPaidAmount     java.lang.Float  
);

CREATE OR REPLACE STREAM OrdersDataPARSED_ST OF OrdersDataPARSED_TP;

CREATE OR REPLACE TYPE ReturnsDataPARSED_TP (
    tDateTime org.joda.time.DateTime KEY,
    sOrderId java.lang.String KEY,
    fReturnedAmount java.lang.Float
);

CREATE OR REPLACE STREAM ReturnsDataPARSED_ST OF ReturnsDataPARSED_TP;

CREATE OR REPLACE STREAM OrdersDataSORTED_ST OF OrdersDataPARSED_TP;
CREATE OR REPLACE STREAM ReturnsDataSORTED_ST OF ReturnsDataPARSED_TP;
CREATE OR REPLACE STREAM Errors_ST OF Global.WAEvent;

CREATE SORTER MySorter OVER
OrdersDataPARSED_ST  ON tDateTime OUTPUT TO OrdersDataSORTED_ST,
ReturnsDataPARSED_ST ON tDateTime OUTPUT TO ReturnsDataSORTED_ST
WITHIN 2 MINUTE
OUTPUT ERRORS TO Errors_ST;

END APPLICATION SliderSorter;

--
-- Recovery Test 34 with two sources, two sliding time-count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Sc5a9W/p  -> CQ1 -> WS
-- S2 -> Sc6a11W/p -> CQ2 -> WS
--

STOP Recov34Tester.RecovTest34;
UNDEPLOY APPLICATION Recov34Tester.RecovTest34;
DROP APPLICATION Recov34Tester.RecovTest34 CASCADE;
CREATE APPLICATION RecovTest34 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE WINDOW DataStream5Minutes1
OVER DataStream1 KEEP 5 ROWS WITHIN 9 MINUTE
PARTITION BY merchantId;

CREATE WINDOW DataStream5Minutes2
OVER DataStream2 KEEP 6 ROWS WITHIN 11 MINUTE
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction1
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes1
GROUP BY merchantId;

CREATE CQ DataToWaction2
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes2
GROUP BY merchantId;

END APPLICATION RecovTest34;

stop application logminer;
undeploy application logminer;
drop application logminer cascade;

create application logminer;

Create Source Rac11g Using OracleReader
(
 --StartTimestamp:'15-JAN-2015 13:00:40',
 Username:'miner',
 Password:'miner',
 ConnectionURL:'10.1.110.128:1521:orcl',
 --Tables:'SCOTT.SIMPLETEST',
 Tables:'QATEST.SAMPLETEST2',
 OnlineCatalog:true,
 FetchSize:1,
 QueueSize:2148,
 CommittedTransactions:false,
 Compression:false,
 FilterTransactionState:false
)
Output To LCRStream;

create target myout using sysout(name: logminer) input from LCRStream;
create Target y using logwriter(name:GitCommitInfo,filename:logminer) input from LCRStream;

end application logminer;
deploy application logminer;
start application logminer;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@ recovery 1 second interval;

CREATE OR REPLACE SOURCE @parquetsrc@ USING Global.HDFSReader (
  eofdelay: 100,
  wildcard: '@File@',
  rolloverstyle: 'Default',
  directory: '@DIR@',
  adapterName: 'HDFSReader',
  hadoopurl: 'hdfs://dockerhost:9000',
  hadoopconfigurationpath: '@CONF@',
  skipbom: true,
  includesubdirectories: false,
  positionbyeof: false )
  PARSE USING ParquetParser (
   )
OUTPUT TO @appname@ParquetStreams;

CREATE OR REPLACE CQ @appname@CQOrder3
INSERT INTO @appname@newStream3
SELECT
PUTUSERDATA(s,'schemaName',s.data.getSchema().getName())
FROM @appname@ParquetStreams s;

CREATE TARGET @avrotarget@ USING Global.FileWriter (
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000,Interval:30s',
  directory: '@FOLDER@',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  filename: 'AvroOutFile',
  DataEncryptionKeyPassphrase_encrypted: 'true',
  flushpolicy: 'EventCount:10,Interval:30s',
  rolloverpolicy: 'EventCount:10,Interval:30s' )
FORMAT USING Global.AvroFormatter  (
  schemaFileName: 'AvroSchema',
  formatAs: 'default',
  schemaregistryConfiguration: '' )
INPUT FROM @appname@newStream3;

CREATE TARGET @parquettarget@ USING Global.FileWriter (
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000,Interval:30s',
  directory: '@FOLDER@',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  filename: 'AvroOutFile',
  DataEncryptionKeyPassphrase_encrypted: 'true',
  flushpolicy: 'EventCount:10,Interval:30s',
  rolloverpolicy: 'EventCount:10,Interval:30s' )
FORMAT USING Global.ParquetFormatter  (
  blocksize: '128000000',
  compressiontype: 'UNCOMPRESSED',
  formatAs: 'Default',
  handler: 'com.webaction.proc.ParquetFormatter',
  formatterName: 'ParquetFormatter' )
INPUT FROM @appname@newStream3;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ (
  positionbyeof: false
)
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING ParquetFormatter (
  schemaFileName: 'parquetSchema'
)
INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @srcName@ USING OracleReader (
 Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@'
)
OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@ USING DatabaseWriter (
  CheckPointTable: 'CHKPOINT', 
  ReplicationSlotName:'test_slot',
  Username:'@tgtusername@',
  Password:'@tgtpassword@',
  ConnectionURL:'@tgturl@',
  adapterName:'PostgreSQLReader',
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@'
)
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP noParser;
UNDEPLOY APPLICATION noParser;
DROP APPLICATION noParser CASCADE;

CREATE APPLICATION noParser;

CREATE TYPE Atm(
productID String KEY,
stateID String,
productWeight int,
quantity double,
size long,
currentDate DateTime);


CREATE CACHE cache1 USING FileReader(
  directory: '@TEST-DATA-PATH@',
  wildcard: 'ISdata.csv',
  header: false,
  columndelimiter: ',',
  trimquote: false
  ) QUERY (keytomap:'productID') OF Atm;

END APPLICATION noParser;

Create Source @SOURCE_NAME@ Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: true,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SCHEMANAME@.@TABLENAME@_copy;@SCHEMANAME@.@TABLENAME@_copy',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  SetConservativeRange: true
) Output To @STREAM@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@ Using Ojet
(
 Username:'@OJET-UNAME@',
 Password:'@OJET-PASSWORD@',
 ConnectionURL:'@OCI-URL@',
 Tables:'@SourceTable@'
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:XE',
  Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using DatabaseReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@;
CREATE  SOURCE @SourceName@ USING DatabaseReader  ( 
  Username: '@UserName@',
  Password: '@Password@',
  DatabaseName: 'qatest',
  ConnectionURL: '@SourceConnectionURL@',
  Tables: 'qatest.@SourceTable@',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @SRCINPUTSTREAM@;
create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;
CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:1',
  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
 ) 
INPUT FROM @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
stop application @APPNAME3@;
undeploy application @APPNAME3@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;
drop application @APPNAME3@ cascade;

CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using OracleReader
(
  Compression:true,
  StartTimestamp:'null',
  CommittedTransactions:true,
  FilterTransactionBoundaries:true,
  Password_encrypted:'false',
  SendBeforeImage:true,
  XstreamTimeOut:600,
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',
  Tables:'qatest.oraMultiDownstream_src',
  adapterName:'OracleReader',
  Password:'qatest',
  DictionaryMode:'OfflineCatalog',
  FilterTransactionState:true,
  connectionRetryPolicy:'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType:'LogMiner',
  FetchSize: 1,
  Username:'qatest',
  OutboundServerProcessName:'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic:true,
  CDDLAction:'Quiesce_Cascade',
  CDDLCapture:'true'
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;


CREATE APPLICATION @APPNAME3@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName1@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME3@;
DEPLOY APPLICATION @APPNAME3@;
START APPLICATION @APPNAME3@;

create application KinesisTest;
create source CSVSource using FileReader (
	directory:'/home/dz/src/product/Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  companyName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0], data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM'
)
format using DSVFormatter (
)
input from TypedCSVStream;
end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'data.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO @AppName@_rawstream;

CREATE OR REPLACE STREAM @BuiltinFunc@_Stream OF Global.WAEVent;
CREATE OR REPLACE STREAM CombineStream OF Global.WAEVent;

CREATE OR REPLACE CQ cq1
INSERT INTO @BuiltinFunc@_Stream
SELECT
@BuiltinFunc@(s1, 'city',data[5])
FROM @AppName@_rawstream s1;

CREATE OR REPLACE CQ cq2
INSERT INTO CombineStream
Select *
FROM @BuiltinFunc@_Stream s4;

CREATE OR REPLACE CQ cq3
INSERT INTO CombineStream
select *
FROM @AppName@_rawstream s5;

CREATE OR REPLACE TARGET @AppName@_FileTarget USING Global.FileWriter ( 
  flushpolicy: 'EventCount:10000,Interval:30s', 
  adapterName: 'FileWriter', 
  directory: '@logs@',
  filename: '@BuiltinFunc@_Data', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING Global.JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter', 
  jsonMemberDelimiter: '\n', 
  EventsAsArrayOfJsonObjects: 'true', 
  formatterName: 'JSONFormatter', 
  jsonobjectdelimiter: '\n' ) 
INPUT FROM CombineStream;

End application @AppName@;
Deploy application @AppName@; 
Start application @AppName@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using S3Writer(
    bucketname:'@BUCKET@',
   objectname:'upgradeData.csv',
   foldername:'upgradefolder',
  uploadpolicy:'EventCount : 10000,Interval :1m '
)
format using DSVFormatter (
)
input from @STREAM@;

end flow @APPNAME@_serverflow;

end application @APPNAME@;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING DatabaseReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

END APPLICATION @APPNAME@;

stop @appName@;
undeploy application @appName@;
drop application @appName@ cascade;
CREATE APPLICATION @appName@ RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE @appName@_Source USING OracleReader
(
  FilterTransactionBoundaries:true,
  ConnectionURL:'@ConnectionURL@',
  Tables:'@OrcTable@',
  Password:'@Password@',
  fetchsize:'1',
  connectionRetryPolicy:'timeOut=30, retryInterval=30, maxRetries=3',
  Username:'@Username@'
)
OUTPUT TO @appName@_Stream;

CREATE OR REPLACE TARGET @appName@_Target1 USING Global.BigQueryWriter (
  ColumnDelimiter: '|',
  NullMarker: 'NULL',
  ConnectionRetryPolicy: 'retryInterval=30,\n maxRetries=3',
  streamingUpload: 'false',
  Mode: 'Merge',
  projectId: '@ProjectId@',
  Encoding: 'UTF-8',
  TransportOptions: 'connectionTimeout=300,\n readTimeout=120',
  Tables: '@OrcTable@,@BqTable@',
  AllowQuotedNewlines: 'false',
  CDDLAction: 'Process',
  adapterName: 'BigQueryWriter',
  serviceAccountKey: '@GCS-AuthPath@',
  optimizedMerge: 'true',
  StandardSQL: 'true',
  includeInsertId: 'true',
  QuoteCharacter: '\"',
  BatchPolicy: 'eventCount:100,Interval:10' )
INPUT FROM @appName@_Stream;

End application @appName@;

stop @appname@;
undeploy application @appname@;
DROP APPLICATION @appname@ CASCADE;
CREATE APPLICATION @appname@;

CREATE SOURCE @appname@_src USING databaseReader  (
  Username: '@@',
  Password: '@@',
  ConnectionURL: '@@',
  Tables: '@@',
  FetchSize: '100'
 )
OUTPUT TO @appname@_ss;

CREATE JUMPING WINDOW @appname@_win OVER @appname@_ss KEEP @winsize@ ROWS;

CREATE TYPE @appname@_MapType
    (   
       id INTEGER,
        name STRING,
        city  STRING
    );
    
CREATE EXTERNAL CACHE @appname@_cach (
  AdapterName: 'DatabaseReader',
    ConnectionURL: '@url@',
    UserName: '@uname@',
    Password: '@pwd@',
   Table: '@tablename@',
  FetchSize: 100,
  skipinvalid : @valid@,
  Columns: 'id,name,city',
  trimquote: false,
  KeyToMap: '@key@'
 )
 OF @appname@_MapType;
 
CREATE TYPE @appname@_MapTypenew
    (   id_t            INTEGER,
        name_t           STRING,
        city_t            STRING,
        id_c            INTEGER,
        name_c            STRING,
        city_c            STRING
    );
    
CREATE STREAM @appname@_JoinedData OF @appname@_MapTypenew;

CREATE CQ @appname@_JoinDataCQ
INSERT INTO @appname@_JoinedData
SELECT  TO_INT(f.data[0]),
        TO_STRING(f.data[1]),
        TO_STRING(f.data[2]),
        z.id,
        z.name,
        z.city
FROM @appname@_win f, @appname@_cach z
where TO_INT(f.data[0]) = z.id
@Ex@;

CREATE TARGET @appname@_tgt USING DatabaseWriter
(
  ConnectionURL:'@@',
  Username:'@@',
  Password:'@@',
  BatchPolicy:'Eventcount:10000,Interval:1',
  CommitPolicy:'Interval:1,Eventcount:10000',
  Tables:'@@'
) 
INPUT FROM @appname@_JoinedData;

END APPLICATION @appname@;
deploy application @appname@;
start @appname@;

create or replace type @STREAM@details(
C_CUSTKEY int,
C_MKTSEGMENT String,
C_NATIONKEY int,
C_NAME String,
C_ADDRESS String,
C_PHONE String,
C_ACCTBAL int,
C_COMMENT String
);

create or replace stream @STREAM@_TYPED of @STREAM@details;

Create or replace CQ @STREAM@detailsCQ
insert into @STREAM@_TYPED
select 
to_int(data[0]),data[1],to_int(data[2]),data[3],data[4],data[5],to_int(data[6]),data[7]
from @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 ) 
INPUT FROM @STREAM@_TYPED;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.JsonNodeEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (
  ProviderName: '',
  Provider: '',
  Ctx: '',
  QueueName: '',
  Topic:'',
  UserName: '',
  Password: '',
  EnableTransaction: '',
  transactionpolicy: ''
 )
PARSE USING JSONParser ()
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING JSONFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream@RANDOM@;

END APPLICATION @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'SingleEvent',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
  filename: '%@metadata(FileName)%',
  directory: '@DIR@',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter  ()
INPUT FROM CCBStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

stop PatternMatchingTimer.CSV;
undeploy application PatternMatchingTimer.CSV;
drop application PatternMatchingTimer.CSV cascade;

create application CSV;

create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'ctest.csv',
  columndelimiter:',',
  positionByEOF:false
)
OUTPUT TO CsvStream;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  TO_INT(data[0]) as UserId,
	    TO_INT(data[1]) as temp1,
        TO_DOUBLE(data[2]) as temp2,
	    TO_STRING(data[3]) as temp3
FROM CsvStream;

-- scenario 1.1 check pattern using timer within 10 seconds and wait
CREATE CQ TypeConversionTimerCQ1
INSERT INTO TypedStream1
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN T A (W | B | C)
define T = timer(interval 10 second),
A = UserDataStream(temp1 >= 20), B= UserDataStream(temp2 < 30.40), C= UserDataStream(temp3 = 'Bret'), W = wait(T)
PARTITION BY UserId;

-- scenario 1.2 check pattern using timer within 20 seconds
CREATE CQ TypeConversionTimerCQ2
INSERT INTO TypedStream2
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN T A C
define T = timer(interval 20 second), B= UserDataStream(temp2 < 30.40), C= UserDataStream(temp3 = 'zalak'),
A = UserDataStream(temp1 >= 20)
PARTITION BY UserId;

-- scenario 1.3 check pattern using timer within 5 seconds with between values
CREATE CQ TypeConversionTimerCQ3
INSERT INTO TypedStream3
SELECT UserId as typeduserid,
	   A.temp1 as typedtemp1
from UserDataStream
MATCH_PATTERN T A
define T = timer(interval 5 second),
A = UserDataStream(temp1 between 10 and 40)
PARTITION BY UserId;

-- scenario 1.4 check pattern using timer which match no events
CREATE CQ TypeConversionTimerCQ4
INSERT INTO TypedStream4
SELECT UserId as typeduserid
from UserDataStream
MATCH_PATTERN T W
define T = timer(interval 50 second), W = wait(T)
PARTITION BY UserId;

-- scenario 1.5 check pattern using stop timer
CREATE CQ TypeConversionTimerCQ5
INSERT INTO TypedStream5
SELECT UserId as typeduserid,
       A.temp1 as typedtemp1,
       B.temp2 as typedtemp2
from UserDataStream
MATCH_PATTERN T A C T2 B
define
T = timer(interval 50 second),
A = UserDataStream(temp1 between 10 and 40),
C = stoptimer(T),
T2 = timer(interval 30 second),
B = UserDataStream(temp2 >= 20)
PARTITION BY UserId;

CREATE WACTIONSTORE UserActivityInfoTimer1
CONTEXT OF TypedStream1_Type
EVENT TYPES ( TypedStream1_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTimer2
CONTEXT OF TypedStream2_Type
EVENT TYPES ( TypedStream2_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTimer3
CONTEXT OF TypedStream3_Type
EVENT TYPES ( TypedStream3_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTimer4
CONTEXT OF TypedStream4_Type
EVENT TYPES ( TypedStream4_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTimer5
CONTEXT OF TypedStream5_Type
EVENT TYPES ( TypedStream5_Type )
@PERSIST-TYPE@

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction1
INSERT INTO UserActivityInfoTimer1
SELECT * FROM TypedStream1
LINK SOURCE EVENT;

CREATE CQ UserWaction2
INSERT INTO UserActivityInfoTimer2
SELECT * FROM TypedStream2
LINK SOURCE EVENT;

CREATE CQ UserWaction3
INSERT INTO UserActivityInfoTimer3
SELECT * FROM TypedStream3
LINK SOURCE EVENT;

CREATE CQ UserWaction4
INSERT INTO UserActivityInfoTimer4
SELECT * FROM TypedStream4
LINK SOURCE EVENT;

CREATE CQ UserWaction5
INSERT INTO UserActivityInfoTimer5
SELECT * FROM TypedStream5
LINK SOURCE EVENT;

end application CSV;
deploy application csv;
start csv;

STOP APPLICATION eh;
UNDEPLOY APPLICATION eh;
DROP APPLICATION eh CASCADE;
CREATE APPLICATION eh @Recovery@;
create flow AgentFlow;
CREATE OR REPLACE SOURCE s USING IncrementalBatchReader  ( 
  FetchSize: 1,
  StartPosition:'QATEST.IBR01=-1;QATEST.IBR02=-1;QATEST.IBR03=0;QATEST.IBR04=0;QATEST.IBR05=1;QATEST.IBR06=2018-09-20 06:43:59;QATEST.%=-1',
  Username: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@//dockerhost:1521/orcl',
  Tables: 'QATEST.IBR%',
  CheckColumn: 'QATEST.IBR01=id;QATEST.IBR03=id;QATEST.IBR05=id;QATEST.%=t1',
  Password: 'qatest' ) 
OUTPUT TO sourcestream ;


CREATE TYPE cdctype(
  id int,
  name String  
);

CREATE STREAM cdctypestream OF cdctype;

CREATE CQ cdcstreamcq
INSERT INTO cdctypestream
SELECT TO_INT(p.data[0]), 
       TO_STRING(p.data[1])
FROM sourcestream p;

end flow AgentFlow;

create flow serverFlow;

create Target t1_dsv using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_01',
	E1P:'true',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
	PartitionKey:'id',
	OperationTimeoutMS:'120000',
    BatchPolicy:'Size:256000,Interval:30s',
    ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
	--ParallelThreads:'2'
)
format using DSVFormatter ( 
)
input from cdctypestream;

create Target t2_json using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_02',
	E1P:'true',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
	PartitionKey:'id',
	OperationTimeoutMS:'140000',
	BatchPolicy:'Size:200000,Interval:1m',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m',
	ConsumerGroup:'reader')
format using JSONFormatter ( 
)
input from cdctypestream;

create Target t3_avro using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_03',
	E1P:'true',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'fxC8n39mA2gyvbgJ67URl1MJYP/s96d8f1hNTCknrC4=',
	PartitionKey:'id',
	OperationTimeoutMS:'140000',
	BatchPolicy:'Size:256000,Interval:1h',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m',
	ConsumerGroup:'reader')
format using AvroFormatter (
schemaFileName:'kafkaAvroTest_Agent_ibr_orcl.avsc'
)input from cdctypestream;
end flow serverFlow;

END APPLICATION eh;
--deploy application eh;
deploy application eh with AgentFlow in Agents, ServerFlow in default;

start application eh;

STOP APPLICATION ER;
UNDEPLOY APPLICATION ER;
DROP APPLICATION ER CASCADE;
CREATE APPLICATION ER;
CREATE STREAM ER_SS2 OF Global.JsonNodeEvent;

CREATE SOURCE ER_S1 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING dsvParser (
)OUTPUT TO ER_SS1;

CREATE SOURCE ER_S2 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING jsonparser (members:'data')
OUTPUT TO ER_SS2;


CREATE SOURCE ER_S3 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING avroParser (
schemaFileName:'kafkaAvroTest_Agent_ibr_orcl.avsc'
)OUTPUT TO ER_SS3;


create Type CustType 
(writerdata com.fasterxml.jackson.databind.JsonNode,
TopicName java.lang.String,
PartitionID java.lang.String);

Create Stream datastream3 of CustType;

CREATE CQ CustCQ3
INSERT INTO datastream3
SELECT AvroToJson(s3.data),
metadata.get("TopicName").toString() AS TopicName,
metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS3 s3;



create Target ER_t1 using FileWriter (
filename:'FT1_5L_AVRO_RT',
directory:'FEATURE-DIR/logs/',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from ER_SS1;

create Target ER_t2 using FileWriter (
filename:'FT2_JSON_RT',
directory:'FEATURE-DIR/logs/',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000'	
)
format using jsonFormatter(members: 'data' )
input from ER_SS2;

create Target ER_t3 using FileWriter (
filename:'FT2_JSON_RT',
directory:'FEATURE-DIR/logs/',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from datastream3;


end application ER;
deploy application ER;

stop application app2PS;
undeploy application app2PS;
drop application app2PS cascade;

create application app2PS;

create target File_TargerPS2 using FileWriter
(
directory : '',
filename : ''
)
format using DSVFormatter()
input from Recoveryss2;

end application app2PS;

deploy application app2PS;
start application app2PS;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ recovery 5 second Interval;
create source @srcName@ USING MySQLReader
(
  Username:'@srcusername@',
  Password:'@srcpassword@',
  ConnectionURL:'@srcurl@',
  Tables:'@srcschema@.@srctable@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries:'true'
) 
OUTPUT TO @outstreamname@;

CREATE OR REPLACE TARGET @tgtName@ USING DatabaseWriter
(
  CheckPointTable:'CHKPOINT',
  Username:'@tgtusername@',
  Password:'@tgtpassword@',
  BatchPolicy:'EventCount:1,Interval:0',
  CommitPolicy:'EventCount:1,Interval:0',
  ConnectionURL:'@tgturl@',
  Tables:'@srcschema@.@srctable@,@tgtschema@.@tgttable@'
) 
INPUT FROM @instreamname@;


End APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.test01',
	FetchSize: '1',
	connectionRetryPolicy:'timeOut=00,retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'Eventcount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true		
) INPUT FROM @APPNAME@_ss;
--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING OracleReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '',
  region: '')
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

END APPLICATION @APPNAME@;

stop application RedshiftColmap;
undeploy application RedshiftColmap;
drop application RedshiftColmap CASCADE;
create application RedshiftColmap recovery 1 second interval;

CREATE OR REPLACE SOURCE OracleSource USING OracleReader  (
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: '@SOURCE_TABLES@',
  FetchSize: 1
 ) Output To LogminerStream;
 
--create Target t2 using SysOut(name:Foo2) input from LogminerStream; 
 
CREATE TARGET RedshiftTarget USING RedshiftWriter
	(
	  ConnectionURL: '@TARGET-URL@',
	  Username: '@TARGET-UNAME@',
	  Password: '@TARGET-PASSWORD@',
	  bucketname: '@BUCKETNAME@',
	  --accesskeyId: '@ACCESS-KEY-ID@',
	  --secretaccesskey: '@SECRET-ACCESS-KEY@',
	  S3IAMRole:'@IAMROLE@',
	  Tables: '@TARGET-TABLES@',
	  uploadpolicy:'eventcount:5,interval:10s',
	  Mode:'incremental'
	) INPUT FROM LogminerStream;
	
END APPLICATION RedshiftColmap;
deploy application RedshiftColmap;
START application RedshiftColmap;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ RECOVERY 1 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @SourceName@ USING PostgreSQLReader  ( 
 ReplicationSlotName: 'striim_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src'
 ) 
OUTPUT TO @SRCINPUTSTREAM@ ;


CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy:'EventCount:1000,Interval:60',
CommitPolicy:'EventCount:1000,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM @SRCINPUTSTREAM@;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop ADW;
undeploy application ADW;
drop application ADW cascade;

CREATE APPLICATION ADW recovery 5 second interval;
create flow agentflow;
CREATE OR REPLACE SOURCE CSVPoller USING FileReader (
directory:'@DIRECTORY@',
WildCard:'posdata5L.csv',
positionByEOF:false
)
parse using DSVParser (
header:'no'
)
OUTPUT TO CsvStream;
end flow agentflow;

create flow targetflow;
CREATE OR REPLACE TYPE CSVStream_Type  ( BUSINESS_NAME java.lang.String KEY,
MERCHANT_ID java.lang.String,
PRIMARY_ACCOUNT_NUMBER java.lang.String
 ) ;

CREATE OR REPLACE STREAM CSVTypeStream OF CSVStream_Type;
CREATE OR REPLACE CQ CQ1
INSERT INTO CSVTypeStream
SELECT data[0],data[1],data[2]
FROM CsvStream;

create target WriteToAzureSQLWH using AzureSQLDWHWriter (
       ConnectionURL: '@CONNECTION-URL@',
       username: '@USERNAME@',
       password: '@PASSWORD@',
	   AccountName: '@STORAGE-ACCOUNT@',
       AccountAccessKey: '@ACCESS-KEY@',
      Tables: '@TARGET-TABLE@',
      uploadpolicy:'eventcount:20000,interval:1m'
) INPUT FROM CSVTypeStream;
end flow targetflow;
END APPLICATION ADW;
deploy application ADW with agentflow in agents,targetflow in default;
start ADW;

--
-- Kafka Stream Recovery Test 10 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same key
-- Bert Hashemi and Nicholas Keene WebAction, Inc.
--
-- S1 -> KS -> CQ -> CW(p) -> CQ -> WS
--

STOP KStreamRecov10Tester.KStreamRecovTest10;
UNDEPLOY APPLICATION KStreamRecov10Tester.KStreamRecovTest10;
DROP APPLICATION KStreamRecov10Tester.KStreamRecovTest10 CASCADE;
DROP USER KStreamRecov10Tester;
DROP NAMESPACE KStreamRecov10Tester CASCADE;
CREATE USER KStreamRecov10Tester IDENTIFIED BY KStreamRecov10Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov10Tester;
CONNECT KStreamRecov10Tester KStreamRecov10Tester;

CREATE APPLICATION KStreamRecovTest10 RECOVERY 1 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTest10Data.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  partKey String KEY,
  serialNumber int
);

CREATE STREAM DataStream OF CsvData PARTITION BY partKey;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    TO_INT(data[1])
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStreamTwoItems
OVER DataStream KEEP 2 ROWS
PARTITION BY partKey;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    first(partKey),
    to_int(first(serialNumber))
FROM DataStreamTwoItems
GROUP BY partKey;

END APPLICATION KStreamRecovTest10;

STOP APPLICATION @APPNAME@_app;
UNDEPLOY APPLICATION @APPNAME@_app;
DROP APPLICATION @APPNAME@_app CASCADE;
-- DROP EXCEPTIONSTORE @APPNAME@_exceptionstore;

CREATE APPLICATION @APPNAME@_app RECOVERY 120 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @APPNAME@_Source USING @SOURCE_ADAPTER@  (
  Username:'qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: '',
  ) OUTPUT TO @APPNAME@_Stream  ;

CREATE OR REPLACE TARGET @APPNAME@_Target USING BigQueryWriter  (
  Tables                        : '',
  projectId                    : '',
  ServiceAccountKey            : '',
  Mode                         : 'APPENDONLY',
  BatchPolicy                  : 'EventCount:1, Interval:60',
  ) INPUT FROM @APPNAME@_Stream;

-- CREATE OR REPLACE TARGET @APPNAME@_SysOut USING Global.SysOut (name: 'wa') INPUT FROM @APPNAME@_Stream;

END APPLICATION @APPNAME@_app;
DEPLOY APPLICATION @APPNAME@_app;
START APPLICATION @APPNAME@_app;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING MSSqlReader
(
  Compression: false,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.test01',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
)
OUTPUT TO @APPNAME@_SS;


CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(id,name) COLUMNMAP(id=id,col1=col11,col2=col12)',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;;

Create Source OracleSource1 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source OracleSource2 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source OracleSource3 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source OracleSource4 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

Create Source OracleSource5 Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;

create target AzureTarget using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',  
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;
END APPLICATION ADW;
deploy application ADW;
start application ADW;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE OR REPLACE TYPE @appname@CQOUT1_Type (
 companyName java.lang.String,
 merchantId java.lang.String,
 dateTime org.joda.time.DateTime,
 hourValue java.lang.String,
 amount java.lang.String,
 zip java.lang.String,
 FileName java.lang.String);

CREATE SOURCE @parquetsrc@ USING FILEReader (
    wildcard: '',
    directory: '',
    positionbyeof: false
 )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE OR REPLACE STREAM @appname@CQOUT1 OF @appname@CQOUT1_Type;
CREATE OR REPLACE CQ @appname@CQ_PQEvent
INSERT INTO @appname@CQOUT1
    Select
    data.get("companyName").toString(),
    data.get("merchantId").toString(),
    TO_DATE(data.get("dateTime").toString()),
    data.get("hourValue").toString(),
    data.get("amount").toString(),
    data.get("zip").toString(),
    metadata.get("FileName").toString()
    FROM @appname@Stream p;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using JSONFormatter ()
input from @appname@CQOUT1;

CREATE OR REPLACE TARGET @dbtarget@ USING DatabaseWriter (
  Tables: '',
  ConnectionURL:'',
  Username:'',
  Password:'',
  CommitPolicy: 'EventCount:10,Interval:0',
  BatchPolicy:'EventCount:10,Interval:0'
)
INPUT FROM @appname@CQOUT1;

create Target @jsontarget@ using FileWriter(
filename: '',
directory: '',
flushpolicy: 'EventCount:10,Interval:30s',
rolloverpolicy: 'EventCount:10,Interval:30s' )
format using JSONFormatter ()
INPUT FROM @appname@CQOUT1;

create Target @xmltarget@ using FileWriter(
filename: '',
directory: '',
flushpolicy: 'EventCount:10,Interval:30s',
rolloverpolicy: 'EventCount:10,Interval:30s' )
format using XMLFormatter (
    rootelement:'',
    elementtuple:'',
    charset:'UTF-8'
)
INPUT FROM @appname@CQOUT1;

create Target @dsvtarget@ using FileWriter(
filename: '',
directory: '',
flushpolicy: 'EventCount:10,Interval:30s',
rolloverpolicy: 'EventCount:10,Interval:30s' )
format using DSVFormatter ()
INPUT FROM @appname@CQOUT1;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

STOP APPLICATION App1;
UNDEPLOY APPLICATION App1;
DROP APPLICATION App1 CASCADE;
CREATE APPLICATION App1;
CREATE FLOW AgentFlow;
CREATE OR REPLACE SOURCE App1_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App1_SampleStream;
END FLOW AgentFlow;
CREATE OR REPLACE TARGET App1_NullTarget using NullWriter()
INPUT FROM App1_SampleStream;
END APPLICATION App1;
deploy application App1 with AgentFlow on any in AGENTS;
START APPLICATION App1;

STOP APPLICATION App2;
UNDEPLOY APPLICATION App2;
DROP APPLICATION App2 CASCADE;
CREATE APPLICATION App2;
CREATE FLOW AgentFlow2;
CREATE OR REPLACE SOURCE App2_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App2_SampleStream;
END FLOW AgentFlow2;
CREATE OR REPLACE TARGET App2_NullTarget using NullWriter()
INPUT FROM App2_SampleStream;
END APPLICATION App2;
deploy application App2 with AgentFlow2 on any in AGENTS;
START APPLICATION App2;

STOP APPLICATION App3;
UNDEPLOY APPLICATION App3;
DROP APPLICATION App3 CASCADE;
CREATE APPLICATION App3;
CREATE OR REPLACE SOURCE App3_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App3_SampleStream;
CREATE OR REPLACE TARGET App3_NullTarget using NullWriter()
INPUT FROM App3_SampleStream;
END APPLICATION App3;
DEPLOY APPLICATION App3;
START APPLICATION App3;

STOP APPLICATION App4;
UNDEPLOY APPLICATION App4;
DROP APPLICATION App4 CASCADE;
CREATE APPLICATION App4;
CREATE OR REPLACE SOURCE App4_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App4_SampleStream;
CREATE OR REPLACE TARGET App4_NullTarget using NullWriter()
INPUT FROM App4_SampleStream;
END APPLICATION App4;
DEPLOY APPLICATION App4;
START APPLICATION App4;

STOP APPLICATION App5;
UNDEPLOY APPLICATION App5;
DROP APPLICATION App5 CASCADE;
CREATE APPLICATION App5;
CREATE OR REPLACE SOURCE App5_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App5_SampleStream;
CREATE OR REPLACE TARGET App5_NullTarget using NullWriter()
INPUT FROM App5_SampleStream;
END APPLICATION App5;
DEPLOY APPLICATION App5;
START APPLICATION App5;

STOP APPLICATION App6;
UNDEPLOY APPLICATION App6;
DROP APPLICATION App6 CASCADE;
CREATE APPLICATION App6;
CREATE OR REPLACE SOURCE App6_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App6_SampleStream;
CREATE OR REPLACE TARGET App6_NullTarget using NullWriter()
INPUT FROM App6_SampleStream;
END APPLICATION App6;
DEPLOY APPLICATION App6;
START APPLICATION App6;

STOP APPLICATION App7;
UNDEPLOY APPLICATION App7;
DROP APPLICATION App7 CASCADE;
CREATE APPLICATION App7;
CREATE OR REPLACE SOURCE App7_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App7_SampleStream;
CREATE OR REPLACE TARGET App7_NullTarget using NullWriter()
INPUT FROM App7_SampleStream;
END APPLICATION App7;
DEPLOY APPLICATION App7;
START APPLICATION App7;

STOP APPLICATION App8;
UNDEPLOY APPLICATION App8;
DROP APPLICATION App8 CASCADE;
CREATE APPLICATION App8;
CREATE OR REPLACE SOURCE App8_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App8_SampleStream;
CREATE OR REPLACE TARGET App8_NullTarget using NullWriter()
INPUT FROM App8_SampleStream;
END APPLICATION App8;
DEPLOY APPLICATION App8;
START APPLICATION App8;


STOP APPLICATION App9;
UNDEPLOY APPLICATION App9;
DROP APPLICATION App9 CASCADE;
CREATE APPLICATION App9;
CREATE OR REPLACE SOURCE App9_FileSource USING FileReader (
directory:'@dirPath@',
wildcard:'posdata.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO App9_SampleStream;
CREATE OR REPLACE TARGET App9_NullTarget using NullWriter()
INPUT FROM App9_SampleStream;
END APPLICATION App9;
DEPLOY APPLICATION App9;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCENAME1@ USING IncrementalBatchReader  (
  FetchSize: 10,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: '@startPosition@',
  PollingInterval: '20sec'
  )
  OUTPUT TO @STREAM@;

  CREATE OR REPLACE SOURCE @SOURCENAME2@ USING IncrementalBatchReader  (
    FetchSize: 10,
    Username: 'striim',
    Password: 'striim',
    ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
    Tables: 'striim.test01',
    adapterName: 'IncrementalBatchReader',
    CheckColumn: @checkColumn1@,
    startPosition: '@startPosition1@',
    PollingInterval: '20sec'
    )
    OUTPUT TO @STREAM@;

  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;

  CREATE TARGET @targetName@ USING DatabaseWriter(
    ConnectionURL:'@READER-URL@',
    Username:'@READER-UNAME@',
    Password:'@READER-PASSWORD@',
    BatchPolicy:'Eventcount:1,Interval:1',
    CommitPolicy:'Eventcount:1,Interval:1',
    Checkpointtable:'CHKPOINT',
    Tables:'@WATABLES@,@WATABLES@_target'
  ) INPUT FROM @STREAM@;


  END APPLICATION @APPNAME@;

  DEPLOY APPLICATION @APPNAME@;
  start application @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_source USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: true,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 ) 
OUTPUT TO @APPNAME@_Stream ;

CREATE TARGET @APPNAME@_Target USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	BatchPolicy: 'EventCount: 1; Interval: 1s',
	instanceId: 'qatest'
) INPUT FROM @APPNAME@_stream;

create Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_stream;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@;

create flow agentflow;
CREATE OR REPLACE SOURCE @APPNAME@_Src USING SpannerBatchReader  (
  DatabaseProviderType: 'Default',
  pollingInterval: '5ms',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  ConnectionURL: 'jdbc:cloudspanner:/projects/bigquerywritertest/instances/testspanner/databases/spannertestdb?credentials=/Users/jenniffer/Downloads/abc.json',
  Tables: 'Recovery_Timestam%',
  --_h_mode:'InitialLoad',
--  VendorConfiguration:'_h_SpannerReadStaleness=MAX_STALENESS 20s',
  adapterName: 'SpannerBatchReader',
    StartPosition: '%=0',
  CheckColumn: '%=id'
 )
OUTPUT TO @APPNAME@_Output_Stream;
end flow agentflow;

CREATE TARGET @APPNAME@_tgt USING SpannerWriter (
	Tables: 'spannersource,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	BatchPolicy: 'EventCount: 1; Interval: 1s',
	instanceId: 'qatest'
) INPUT FROM @APPNAME@_Output_Stream;

create Target @APPNAME@_sysout using SysOut(name:Foo2) input from @APPNAME@_Output_Stream;

end application @APPNAME@;
deploy application @APPNAME@ with agentflow in agents;
start application @APPNAME@;

CREATE or replace TARGET @TARGET_NAME@ USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:10000,Interval:600',
StandardSQL:true	
) INPUT FROM @STREAM@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING AvroParser ()
OUTPUT TO @APPNAME@_Stream;

CREATE CQ @APPNAME@_CQ
INSERT INTO @APPNAME@_CQOut
SELECT AvroToJson(data,false) FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING JSONFormatter ()
INPUT FROM @APPNAME@_CQOut;

END APPLICATION @APPNAME@;

STOP APPLICATION oracletokudu;
UNDEPLOY APPLICATION oracletokudu;
DROP APPLICATION oracletokudu CASCADE;
CREATE APPLICATION oracletokudu;
Create Type CSVType (
	companyname String,
  merchantName String
);

Create Stream TypedFileStream of CSVType;

create source CSVSource using FileReader (
	directory:'/Users/jenniffer/Product2/IntegrationTests/TestData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	
)
OUTPUT TO FileStream;

CREATE CQ CsvToPosData
INSERT INTO TypedFileStream
SELECT TO_STRING(data[0]),TO_STRING(data[1])
FROM FileStream;

CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:10000,Interval:20s')
INPUT FROM TypedFileStream;
DEPLOY APPLICATION oracletokudu;
START APPLICATION oracletokudu;

STOP APPLICATION oraddl;
UNDEPLOY APPLICATION oraddl;
DROP APPLICATION oraddl CASCADE;
CREATE APPLICATION oraddl recovery 5 second interval;
 
Create Source Ora Using OracleReader 
(
 Username:'@user-name@',
 Password:'@password@',
 ConnectionURL:'src_url',
 Tables:'QATEST.ORACLEDDL%',
 DictionaryMode:OfflineCatalog,
 DDLCaptureMode : 'All',
 FetchSize:1
) Output To LogminerStream;

Create Target tgt using DatabaseWriter 
(
 Username:'@username@',
 Password:'@password@',
 ConnectionURL:'TGT_URL',
 BatchPolicy:'EventCount:1,Interval:1',
 CommitPolicy:'EventCount:1,Interval:1',
 IgnorableExceptionCode: '1,2290,942',
 Tables :'QATEST.ORACLEDDL%,QATEST2.%'
) input from LogminerStream;

end application oraddl;
deploy application oraddl;
start application oraddl;

Stop bq;
Undeploy application bq;
alter application bq;
--CREATE CQ cq1
--INSERT INTO OpsStream
--SELECT  
--CASE WHEN (META(a,"OperationName").toString() == "INSERT")
--THEN putUserData(a, 'ops_name', 'I') 
--Else putUserData(a, 'ops_name', 'NOT_INSERT') 
--END
--FROM ss a;
CREATE OR REPLACE TARGET T USING BigQueryWriter  ( 
  serviceAccountKey: '/Users/saranyad/Product/IntegrationTests/TestData/google-gcs.json',
  projectId: 'bigquerywritertest',
  Tables: '@TABLE@',
  datalocation: 'US',
  nullmarker: 'null',
  columnDelimiter: '|',
  Mode: 'Merge',
  _h_throwExceptionOnTableNotFound:'false',
  IgnorableExceptioncode:'TABLE_NOT_FOUND',
  BatchPolicy: 'eventcount:100,interval:10'
 ) INPUT FROM ss;
alter application bq recompile;
deploy application bq;
Start bq;

STOP ModifyBeforeDataTester.ModifyBeforeData;
UNDEPLOY APPLICATION ModifyBeforeDataTester.ModifyBeforeData;
DROP APPLICATION ModifyBeforeDataTester.ModifyBeforeData CASCADE;

CREATE APPLICATION ModifyBeforeData;

create source GGTrailSource using FileReader (
    directory:'@TEST-DATA-PATH@/OGG/oracle_alltypes_122',
    WildCard:'ld*',
    positionByEOF:false
) parse using GGTrailParser (
    FilterTransactionBoundaries: true,
    metadata:'@TEST-DATA-PATH@/OGG/oracle_alltypes_122/def_oracle_alltypes122.def',
    compression:false
)
OUTPUT TO SourceStream;

CREATE STREAM ModifiedBeforeStream OF Global.WAEvent;
CREATE STREAM ModifiedDataStream OF Global.WAEvent;

CREATE OR REPLACE CQ ModifierBeforeCQ
INSERT INTO ModifiedBeforeStream
SELECT * FROM SourceStream
where not(before is null)
modify(before[1] = data[1].toString().replaceAll(".", "B"));

CREATE OR REPLACE CQ ModifierDataCQ
INSERT INTO ModifiedDataStream
SELECT * FROM SourceStream
where not(before is null)
modify(data[1] = before[1].toString().replaceAll(".", "D"));

-- Generate the final filtered stream to write to the waction store.
CREATE OR REPLACE CQ CopyBeforeCQ
INSERT INTO FilteredBeforeStream
SELECT *
FROM (SELECT TO_STRING(before[1]) as tcolBefore
      FROM ModifiedBeforeStream) as src
where src.tcolBefore like 'BBBBBB%';

CREATE OR REPLACE CQ CopyDataCQ
INSERT INTO FilteredDataStream
SELECT *
FROM (SELECT TO_STRING(data[1]) as tcolData
      FROM ModifiedDataStream) as src
where src.tcolData like "DDDDDD%";

-- Need duplicate types due to a limitation of Waction Stores: two Waction Stores
-- should not share the same type for CONTEXT OF.
CREATE TYPE WactionType1 (
  colTest String KEY
);

CREATE TYPE WactionType2 (
  colTest String KEY
);

CREATE WACTIONSTORE WactionsBefore CONTEXT OF WactionType1
EVENT TYPES ( WactionType1 )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsData CONTEXT OF WactionType2
EVENT TYPES ( WactionType2 )
@PERSIST-TYPE@

CREATE CQ InsertWactions1
INSERT INTO WactionsBefore
SELECT tcolBefore as colTest
FROM FilteredBeforeStream;

CREATE CQ InsertWactions2
INSERT INTO WactionsData
SELECT tcolData as colTest
FROM FilteredDataStream;

END APPLICATION ModifyBeforeData;

CREATE  SOURCE @SOURCE_NAME@ USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@1;

CREATE  SOURCE @SOURCE_NAME@2 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@2;

CREATE  SOURCE @SOURCE_NAME@3 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@3;

CREATE  SOURCE @SOURCE_NAME@4 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@4;

CREATE  SOURCE @SOURCE_NAME@5 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@5;

CREATE  SOURCE @SOURCE_NAME@6 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@6;

CREATE  SOURCE @SOURCE_NAME@7 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@7;

CREATE  SOURCE @SOURCE_NAME@8 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@8;

CREATE  SOURCE @SOURCE_NAME@9 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@9;

CREATE  SOURCE @SOURCE_NAME@10 USING DatabaseReader  (
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1522/orcl',
  Username: 'qatest',
  Password: 'qatest',
  Query: '@SourceQuery@'
 )
OUTPUT TO @STREAM@10;

--
-- Canon Master Test, Unpartitioned
-- Nicholas Keene, WebAction, Inc.
--


CREATE APPLICATION MasterUnpartitioned
RECOVERY 5 SECOND INTERVAL
;

CREATE OR REPLACE TYPE N100k_JUc100_NoPersist_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100k_JUc100_NoPersist_WS  CONTEXT OF N100k_JUc100_NoPersist_WS_Type
 PERSIST NONE USING ( 
 ) ;

CREATE OR REPLACE TYPE R10_SPc10_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE SOURCE N50k USING NumberSource ( 
  lowValue: '1',
  highValue: '500000',
  delayMillis: '0',
  delayNanos: '0',
  repeat: 'false'
 ) 
OUTPUT TO NumberStream2_Out;

CREATE OR REPLACE SOURCE N100k USING NumberSource ( 
  lowValue: '1',
  highValue: '1000000',
  delayMillis: '0',
  delayNanos: '0',
  repeat: 'false'
 ) 
OUTPUT TO N100k_Out;

CREATE OR REPLACE TYPE ComplexNumberType  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY , 
valueAsDateTime org.joda.time.DateTime , 
valueMod10 java.lang.Long , 
valueMod7 java.lang.Long , 
valueMod13 java.lang.Long  
 );

CREATE OR REPLACE TYPE R10_JPc5_x7_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE TYPE N100k_JUc100_SUc10_WS  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE TYPE N100K_N50K_TwoInputs_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100K_N50K_TwoInputs  CONTEXT OF N100K_N50K_TwoInputs_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE N100K_TwoInputs_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100K_TwoInputs_WS  CONTEXT OF N100K_TwoInputs_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE N100k_JUc100_M_JUc200_JUc15_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100k_JUc100_M_JUc200_JUc15_WS  CONTEXT OF N100k_JUc100_M_JUc200_JUc15_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE N100k_JUc100_Dupe_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100k_JUc100_Dupe  CONTEXT OF N100k_JUc100_Dupe_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE N100k_JUc100_SUc10_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY , 
count java.lang.Long  
 );

CREATE OR REPLACE WACTIONSTORE N100k_JUc100_SUc10_WS  CONTEXT OF N100k_JUc100_SUc10_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE N100k_JUc100_SUc10na_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE TYPE ComplexNumberType2  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY , 
valueAsTimeStamp org.joda.time.DateTime , 
valueMod10 java.lang.Long , 
valueMod7 java.lang.Long , 
valueMod13 java.lang.Long  
 );

CREATE OR REPLACE TYPE Type1  ( f java.lang.String KEY  
 );

CREATE OR REPLACE TYPE N50k_JUc100_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N50k_JUc100_WS  CONTEXT OF N50k_JUc100_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE Type2  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY , 
valueAsDateTime org.joda.time.DateTime , 
valueMod10 java.lang.Long , 
valueMod3 java.lang.Long , 
valueMod7 java.lang.Long  
 );

CREATE OR REPLACE TYPE N100k_JUc100_JUc10_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100k_JUc100_JUc10_WS  CONTEXT OF N100k_JUc100_JUc10_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE OneNumberType  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE STREAM N100k_Stream OF OneNumberType;

CREATE OR REPLACE CQ N100k_Convert 
INSERT INTO N100k_Stream
SELECT TO_DATE(data[0]), data[1]
 	 
FROM N100k_Out;

CREATE OR REPLACE JUMPING WINDOW N100k_JUc100 OVER N100k_Stream KEEP 100 ROWS;

CREATE OR REPLACE CQ N100k_JUc100_Pull_Dupe 
INSERT INTO N100k_JUc100_Dupe
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100 w;

CREATE OR REPLACE CQ N100k_JUc100_Pull3 
INSERT INTO N100k_JUc100_NoPersist_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100 w;

CREATE OR REPLACE CQ N100k_Merge_JUc100_Pull 
INSERT INTO N100K_TwoInputs_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100 w;

CREATE OR REPLACE JUMPING WINDOW N100k_JUc200 OVER N100k_Stream KEEP 200 ROWS;

CREATE OR REPLACE CQ N100K_JUc200_TwoInputs_Pull 
INSERT INTO N100K_N50K_TwoInputs
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc200 w;

CREATE OR REPLACE STREAM N50k_Stream OF OneNumberType;

CREATE OR REPLACE JUMPING WINDOW N50k_JUc100 OVER N50k_Stream KEEP 100 ROWS;

CREATE OR REPLACE CQ N50K_JUc100_TwoInputs_Pull 
INSERT INTO N100K_N50K_TwoInputs
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N50k_JUc100 w;

CREATE OR REPLACE CQ N50k_JUc100_Pull 
INSERT INTO N50k_JUc100_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N50k_JUc100 w;

CREATE OR REPLACE CQ N50k_Convert 
INSERT INTO N50k_Stream
SELECT TO_DATE(data[0]), data[1]
 	 
FROM NumberStream2_Out;

CREATE OR REPLACE STREAM N100k_JUc100_Out OF OneNumberType;

CREATE OR REPLACE JUMPING WINDOW N100k_JUc100_JUc10 OVER N100k_JUc100_Out KEEP 10 ROWS;

CREATE OR REPLACE CQ N100k_JUc100_JUc10_Pull 
INSERT INTO N100k_JUc100_JUc10_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100_JUc10 w;

CREATE OR REPLACE CQ N100K_TwoInputs_CQ2 
INSERT INTO N100K_TwoInputs_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100_JUc10 w;

CREATE OR REPLACE CQ N100k_JUc100_Pull2 
INSERT INTO N100k_JUc100_Out
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100 w;

CREATE OR REPLACE WINDOW N100k_JUc100_SUc10 OVER N100k_JUc100_Out KEEP 10 ROWS;

CREATE OR REPLACE CQ N100k_JUc100_SUc10_Pull 
INSERT INTO N100k_JUc100_SUc10_WS
SELECT FIRST(w.timestamp), FIRST(w.value), COUNT(w)
 	 
FROM N100k_JUc100_SUc10 w;

CREATE OR REPLACE STREAM N100k_JUc100_M_JUc200 OF OneNumberType;

CREATE OR REPLACE JUMPING WINDOW N100k_JUc100_M_JUc200_JUc15 OVER N100k_JUc100_M_JUc200 KEEP 15 ROWS;

CREATE OR REPLACE CQ N100k_JUc100_M_JUc200_JUc15_Pull 
INSERT INTO N100k_JUc100_M_JUc200_JUc15_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100_M_JUc200_JUc15 w;

CREATE OR REPLACE CQ N100k_Merge_JUc200 
INSERT INTO N100k_JUc100_M_JUc200
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc200 w;

CREATE OR REPLACE CQ N100k_Merge_JUc100 
INSERT INTO N100k_JUc100_M_JUc200
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100 w;

CREATE OR REPLACE STREAM N50K_x100_SUc5_Stream OF OneNumberType;

CREATE OR REPLACE WINDOW N50K_x100_SUc5_SUc10 OVER N50K_x100_SUc5_Stream KEEP 10 ROWS;

CREATE OR REPLACE STREAM N50K_x100_Stream OF OneNumberType;

CREATE OR REPLACE WINDOW N50K_x100_SUc5 OVER N50K_x100_Stream KEEP 5 ROWS;

CREATE OR REPLACE CQ N50K_x100_SUc5_Stream_Pull2 
INSERT INTO N50K_x100_SUc5_Stream
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N50K_x100_SUc5 w;

CREATE OR REPLACE CQ N50K_x100_Pop 
INSERT INTO N50K_x100_Stream
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N50k_Stream w
WHERE (w.value % 100) == 0;

CREATE OR REPLACE TYPE R10_JPc10_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE TYPE N100k_JUc100_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100k_JUc100_WS  CONTEXT OF N100k_JUc100_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE CQ N100k_JUc100_Pull 
INSERT INTO N100k_JUc100_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100 w;

CREATE OR REPLACE TYPE ComplexNumberType3  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY , 
valueAsDateTime org.joda.time.DateTime , 
valueMod10 java.lang.Long , 
valueMod7 java.lang.Long , 
valueMod3 java.lang.Long  
 );

CREATE OR REPLACE TYPE N50K_x100_SUc5_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N50K_x100_SUc5_WS  CONTEXT OF N50K_x100_SUc5_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE CQ N50K_x100_SUc5_Pull 
INSERT INTO N50K_x100_SUc5_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N50K_x100_SUc5 w;





END APPLICATION MasterUnpartitioned;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE OR REPLACE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL USE EXCEPTIONSTORE TTL : '7d' ;

CREATE OR REPLACE SOURCE @SOURCE@ USING SalesForceReader (
  autoAuthTokenRenewal: 'true',
  Username: '@userName@',
  securityToken: '@securityToken@',
  sObjects: '@SourceObj@',
  pollingInterval: '1 min',
  Password_encrypted: 'false',
  securityToken_encrypted: 'false',
  customObjects: 'False',
  consumerKey: '@consumerKey@',
  startTimestamp: '',
  apiEndPoint: 'https://ap2.salesforce.com',
  mode: 'Incremental',
  consumerSecret: '@consumerSecert@',
  consumerSecret_encrypted: 'false',
  Password: '@Password@' )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@ USING SnowflakeWriter (
  connectionUrl: '@tgtConnectionUrl@',
  optimizedMerge: 'true',
  password: '@TgtPassword@',
  username: '@TgtUserName@',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10,interval:1m',
  tables: '@SourceObj@,@TargetTableName@ columnmap(ID=ID,checkbool__c=checkbool__c,dt__c=dt__c,percnt__c=percnt__c,phn__c=phn__c,txtlong__c=txtlong__c,url1__c=url1__c)'
 )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

--
-- Canon Test W10
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for an unpartitioned sliding count window
--
-- S -> SWc5u -> CQ -> WS
--


UNDEPLOY APPLICATION NameW10.W10;
DROP APPLICATION NameW10.W10 CASCADE;
CREATE APPLICATION W10 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW10;

CREATE SOURCE CsvSourceW10 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW10;

END FLOW DataAcquisitionW10;


CREATE FLOW DataProcessingW10;

CREATE TYPE DataTypeW10 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW10 OF DataTypeW10;

CREATE CQ CSVStreamW10_to_DataStreamW10
INSERT INTO DataStreamW10
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW10;

CREATE WINDOW SWc5uW10
OVER DataStreamW10
KEEP 5 ROWS;

CREATE WACTIONSTORE WactionStoreW10 CONTEXT OF DataTypeW10
EVENT TYPES ( DataTypeW10 KEY(word) )
@PERSIST-TYPE@

CREATE CQ SWc5uW10_to_WactionStoreW10
INSERT INTO WactionStoreW10
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM SWc5uW10;

END FLOW DataProcessingW10;



END APPLICATION W10;

stop application Postgres_To_PostgresApp;
undeploy application Postgres_To_PostgresApp;
drop application Postgres_To_PostgresApp cascade;

CREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE Postgres_src USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 ) 
OUTPUT TO Change_Data_Stream ;


CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET Postgres_tgt USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

end application Postgres_To_PostgresApp;
deploy application Postgres_To_PostgresApp;
start Postgres_To_PostgresApp;

stop application ADW2;
undeploy application ADW2;
drop application ADW2 cascade;
CREATE APPLICATION ADW2;

CREATE  SOURCE SqlServerInitialLoad2 USING DatabaseReader  
 (
 Username:'src_username',
 Password:'src_password',
 ConnectionURL: 'src_url',
 Tables:'@SOURCE-TABLES@',
 FetchSize:2000
) 
OUTPUT TO InitialLoadStream2;

CREATE TARGET AzureDWInitialLoad2 USING AzureSQLDWHWriter(
ConnectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',
        uploadpolicy:'@EVENT-COUNT@'
)
INPUT FROM InitialLoadStream2;

END APPLICATION ADW2;
deploy application ADW2;
start application ADW2;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '@GRPPOLICY@',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'Text',
  ProcessCopyBookFileAs: 'MultipleEvents',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
  filename: '%@metadata(FileName)%',
  directory: '@DIR@',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter  ()
INPUT FROM CCBStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

--create application @APPNAME@ Recovery 5 Second Interval;
create application @APPNAME@;

create or replace type @APPNAME@emp_type(
Sno integer,
Empname string,
Doj string,
Country string,
CompanyName string
);

CREATE OR REPLACE SOURCE @APPNAME@File_Source1 using Filereader(
	directory:'@DIRECTORY@',
  wildcard:'File_empdata.csv',
  positionByEOF:false
)parse using dsvParser(
    header:'yes'
)
OUTPUT TO @APPNAME@FileSource_Stream1,
OUTPUT TO @APPNAME@FileSource_Stream1_automap MAP(filename:'File_empdata.csv');

CREATE OR REPLACE SOURCE @APPNAME@Init_Source1 USING DatabaseReader  (
  Username: 'qatest',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.EMP_INIT',
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: 'qatest'
 )
OUTPUT TO 	@APPNAME@InitialLoad_Stream1;

CREATE OR REPLACE SOURCE @APPNAME@CDC_Source1 USING OracleReader  (
  StartTimestamp: 'null',
  SupportPDB: false,
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  FetchSize: 1,
  DDLCaptureMode: 'All',
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.EMP',
  adapterName: 'OracleReader',
  Password: 'qatest',
  TransactionBufferType: 'Memory',
  DictionaryMode: 'OnlineCatalog',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: null,
  ReaderType: 'LogMiner',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '1MB',
  compression: true,
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO @APPNAME@CDC_Stream1 ;

create or replace stream @APPNAME@FileSource_cdc_init_TypedStream of @APPNAME@emp_type;
create or replace cq @APPNAME@file_typed_streamcq
insert into @APPNAME@FileSource_cdc_init_TypedStream
SELECT to_int(data[0]),
data[1],
data[2],
data[3],
data[4]
from @APPNAME@FileSource_Stream1;

create or replace cq @APPNAME@cdc_typed_streamcq
insert into @APPNAME@FileSource_cdc_init_TypedStream
SELECT to_int(data[0]),
data[1],
data[2],
data[3],
data[4]
from @APPNAME@CDC_Stream1;

create or replace cq @APPNAME@init_typed_streamcq
insert into @APPNAME@FileSource_cdc_init_TypedStream
SELECT to_int(data[0]),
data[1],
data[2],
data[3],
data[4]
from @APPNAME@InitialLoad_Stream1;

CREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target1 USING CassandraCosmosDBWriter  (
  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --CheckPointTable: '',
  FlushPolicy: 'EventCount:1000,Interval:60',
  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',
  AccountKey:'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  --ConsistencyLevel:'fh',
  --port:'10505',
  Tables: 'test.file_emp',
  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',
  adapterName: 'CassandraCosmosDBWriter'
 )
INPUT FROM @APPNAME@FileSource_Stream1_automap;

CREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target2 USING CassandraCosmosDBWriter  (
  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --CheckPointTable: '',
  FlushPolicy: 'EventCount:1000,Interval:60',
  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',
  AccountKey: 'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  --ConsistencyLevel:'fh',
  --port:'10505',
  Tables: 'QATEST.EMP,test.cdc_emp',
  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',
  adapterName: 'CassandraCosmosDBWriter'
 )
INPUT FROM @APPNAME@CDC_Stream1;

CREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target3 USING CassandraCosmosDBWriter  (
  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --CheckPointTable: '',
  FlushPolicy: 'EventCount:1000,Interval:60',
  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',
  AccountKey: 'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  --ConsistencyLevel:'fh',
  --port:'10505',
  Tables: 'QATEST.EMP_INIT,test.initialload_emp',
  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',
  adapterName: 'CassandraCosmosDBWriter'
 )
INPUT FROM @APPNAME@InitialLoad_Stream1;


CREATE OR REPLACE TARGET @APPNAME@cosmoscassandra_target4 USING CassandraCosmosDBWriter  (
  --ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --CheckPointTable: '',
  FlushPolicy: 'EventCount:1000,Interval:60',
  AccountEndpoint: 'qacassandracosmos.cassandra.cosmos.azure.com',
  AccountKey: 'e4f5HlfwP26Udlob0v9z8NKCOVtRzOyq11Pjo62rpQvW5KXBaLJizEd999qLfTU7sIUkK8i9VYCUcuLzRh3DXQ==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  --ConsistencyLevel:'fh',
  --port:'10505',
  Tables: 'test.file_cdc_emp',
  OverloadRetryPolicy:'retryInterval=30,maxRetries=3',
  adapterName: 'CassandraCosmosDBWriter'
 )
INPUT FROM @APPNAME@FileSource_cdc_init_TypedStream;

create or replace target @APPNAME@sys_file_tgt using sysout(
name:'foo_file'
)input from @APPNAME@FileSource_Stream1;

create or replace target @APPNAME@sys_cdc_tgt using sysout(
name:'foo_cdc'
)input from @APPNAME@CDC_Stream1;

create or replace target @APPNAME@sys_init_tgt using sysout(
name:'foo_init'
)input from @APPNAME@InitialLoad_Stream1;

End Application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

stop APPLICATION @APPNAME@_ExceptionStore;;
undeploy APPLICATION @APPNAME@_ExceptionStore;
drop APPLICATION @APPNAME@_ExceptionStore cascade;

CREATE APPLICATION @APPNAME@_ExceptionStore;

CREATE TYPE @APPNAME@2_CDCStreams_Type  (
 exceptionType java.lang.String,
  action java.lang.String,
  appName java.lang.String,
  entityType java.lang.String,
  entityName java.lang.String,
  className java.lang.String,
  message java.lang.String,
  relatedEntity java.lang.String,
  exceptionCode java.lang.String
 );

CREATE STREAM @APPNAME@2_CDCStreams OF @APPNAME@2_CDCStreams_Type;

CREATE CQ @APPNAME@2_ReadFromExpStore
INSERT INTO @APPNAME@2_CDCStreams
select s.exceptionType,s.action,s.appName,s.entityType,s.entityName,s.className,s.message,s.relatedEntity,s.exceptionCode from @APPNAME@_App_ExceptionStore [jumping @WINDOWINTERVAL@ seconds] s;

CREATE OR REPLACE TARGET @APPNAME@2_WriteToFileAsJSON USING FileWriter  (
  filename: 'expEvent.log',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:5',
  adapterName: 'FileWriter',
  directory: 'ExpStore_logs',
  rolloverpolicy: 'eventcount:5'
 )
FORMAT USING JSONFormatter  (
  handler: 'com.webaction.proc.JSONFormatter',
  formatterName: 'JSONFormatter',
  jsonMemberDelimiter: '\n',
  jsonobjectdelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true'
 )
INPUT FROM @APPNAME@2_CDCStreams;

END APPLICATION @APPNAME@_ExceptionStore;

deploy application @APPNAME@_ExceptionStore;
start @APPNAME@_ExceptionStore;

STOP APPLICATION tpcc;
UNDEPLOY APPLICATION tpcc;
DROP APPLICATION tpcc CASCADE;

CREATE APPLICATION tpcc RECOVERY 5 SECOND Interval;

CREATE SOURCE Ojet_Source USING Ojet
(
    Username: '@Username@',
    Password: '@Password@',
    ConnectionURL: '@ConnectionURL@',
    Tables: '@Tables@',
)

OUTPUT TO SourceStream ;

create Target Ojet_FileWriter using FileWriter(
  filename:'qatar.csv',
  directory:'',
  flushpolicy: 'EventCount:10000,Interval:60s',
  rolloverpolicy: 'EventCount:10000,Interval:60s'
)
format using DSVFormatter (

)
input from SourceStream;

create Target t2 using SysOut(name:Foo2) input from SourceStream;

END APPLICATION tpcc;

DEPLOY APPLICATION tpcc;
START APPLICATION tpcc;

--
-- Canon Test W20
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for an unpartitioned sliding attribute window
--
-- S -> SWa5u -> CQ -> WS
--


UNDEPLOY APPLICATION NameW20.W20;
DROP APPLICATION NameW20.W20 CASCADE;
CREATE APPLICATION W20 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW20;

CREATE SOURCE CsvSourceW20 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW20;

END FLOW DataAcquisitionW20;



CREATE FLOW DataProcessingW20;

CREATE TYPE DataTypeW20 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW20 OF DataTypeW20;

CREATE CQ CSVStreamW20_to_DataStreamW20
INSERT INTO DataStreamW20
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW20;

CREATE WINDOW SWa5uW20
OVER DataStreamW20
KEEP WITHIN 5 SECOND ON dateTime;

CREATE WACTIONSTORE WactionStoreW20 CONTEXT OF DataTypeW20
EVENT TYPES ( DataTypeW20 KEY(word) )
@PERSIST-TYPE@

CREATE CQ SWa5uW20_to_WactionStoreW20
INSERT INTO WactionStoreW20
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM SWa5uW20;

END FLOW DataProcessingW20;



END APPLICATION W20;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

Create Source dataGenSrc USING OracleReader  ( 
  FetchSize: 1,
  QueueSize: 2048,
  CommittedTransactions: true,
  Compression: false,
  Username: 'qatest',
  Password: 'r+g6o0bDETs=',
  ConnectionURL: 'localhost:1521:xe',
  FilterTransactionState: true,
  DictionaryMode: 'OnlineCatalog',
  ReaderType: 'LogMiner',
  Tables: '@tableNames@',
  Password_encrypted: true
 ) 
 Output To LCRStream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;

END APPLICATION DataGenSampleApp;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
	filename:'JsonTargetAdd',
	directory:'@FEATURE-DIR@/logs/',
	sequence:'00',
	rolloverpolicy:'FileSizeRollingPolicy,filesize:33 + 44M'
)
format using JSONFormatter (
	members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)

input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizeAdd_actual.log') input from TypedCSVStream;
end application DSV;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING Global.FabricDataWarehouseWriter (
  Tables: '',
  StorageAccessDriverType: 'WASBS',
  ConnectionURL: '',
  Username: '',
  AccountAccessKey: '',
  Mode: 'APPENDONLY',
  Password: '',
  uploadpolicy: 'eventcount:1,interval:10s',
  AccountName: '')
INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;;

create Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;

stop application Postgres_To_PostgresApp;
undeploy application Postgres_To_PostgresApp;
drop application Postgres_To_PostgresApp cascade;

CREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE Postgres_src USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 ) 
OUTPUT TO Change_Data_Stream ;


CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Change_Data_Stream;

CREATE OR REPLACE TARGET Postgres_tgt USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:100,Interval:60',
CommitPolicy: 'EventCount:100,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Change_Data_Stream;

end application Postgres_To_PostgresApp;
deploy application Postgres_To_PostgresApp;
start Postgres_To_PostgresApp;

CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (
  ConnectionURL: @sourceURL@,
  Username: '@userName',
  Tables: '@tables@',
  CheckColumn: '@checkColumn',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  Password: '@password',
  Password_encrypted: 'false',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

STOP APPLICATION @appName@;
UNDEPLOY APPLICATION @appName@;
DROP APPLICATION @appName@ CASCADE;

CREATE APPLICATION @appName@;

CREATE OR REPLACE TYPE @EventType@ (
 CC_Number java.lang.String,
 Amount java.lang.String,
 TXN_Type java.lang.String,
 TXN_Timestamp java.lang.String);

CREATE SOURCE @sourceComp@ USING Global.FileReader (
  rolloverstyle: 'Default',
  blocksize: 64,
  wildcard: '@sourceFileName@',
  skipbom: true,
  directory: '@sourceFileDir@',
  includesubdirectories: false,
  positionbyeof: false )
PARSE USING Global.DSVParser (
  trimwhitespace: true,
  commentcharacter: '',
  linenumber: '-1',
  columndelimiter: ',',
  trimquote: true,
  columndelimittill: '-1',
  eventtype: '@EventType@',
  ignoreemptycolumn: false,
  separator: ':',
  quoteset: '\"',
  charset: 'UTF-8',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  blockascompleterecord: false,
  rowdelimiter: '\n',
  nocolumndelimiter: false,
  headerlineno: 0,
  header: false )
OUTPUT TO @sourceOutStream@;

CREATE OR REPLACE STREAM @sourceOutStream@ OF @EventType@;

CREATE CQ @match_patternCQ@
INSERT INTO @patternOutStream@
SELECT
LIST(A,B) as events,
COUNT(B) as count
FROM @sourceOutStream@ t
MATCH_PATTERN T A+ (W|B)
DEFINE
	A = t(TXN_Type = 'AUTH/HOLD'),
	B = t(TXN_Type = 'CHARGE'),
	T = TIMER(interval 3 minute),
	W = WAIT(T)
PARTITION BY t.CC_Number;;

CREATE CQ @cqForTarget1@
INSERT INTO @target1InStream@
SELECT events FROM @patternOutStream@ a
WHERE a.count > 0;;

CREATE CQ @cqForTarget2@
INSERT INTO @target2InStream@
SELECT events FROM @patternOutStream@ a
WHERE a.count = 0;;

CREATE TARGET @target1@ USING Global.FileWriter (
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  rolloverpolicy: 'EventCount:250000',
  directory: '@targetFilesDir@',
  filename: '@targetFile1@' )
FORMAT USING Global.JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n' )
INPUT FROM @target1InStream@;

CREATE TARGET @target2@ USING Global.FileWriter (
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  directory: '@targetFilesDir@',
  filename: '@targetFile2@',
  rolloverpolicy: 'EventCount:250000' )
FORMAT USING Global.JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n' )
INPUT FROM @target2InStream@;

END APPLICATION @appName@;

stop tpcc;
undeploy application tpcc;
drop application tpcc cascade;
CREATE APPLICATION tpcc;

Create Source oracSource
 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'localhost:1521:orcl',
 Tables:'QATEST.TIMETEST',
 Fetchsize:1
)
Output To DataStream;

CREATE TARGET WriteCDCOracle USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:orcl',
  Username:'qatest',
  Password:'qatest',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'QATEST.TIMETEST,QATEST.TIMETEST_TGT'
) INPUT FROM DataStream;

create Target t2 using SysOut(name:Foo2) input from DataStream;

END APPLICATION tpcc;
deploy application tpcc in default;
start tpcc;

STOP APPLICATION LongRunningQueryTester.LongRunningQuery;
UNDEPLOY APPLICATION LongRunningQueryTester.LongRunningQuery;
DROP APPLICATION LongRunningQueryTester.LongRunningQuery cascade;

CREATE APPLICATION LongRunningQuery;




CREATE TYPE RandomData(
myName String,
streetAddress String,
bankName String,
bankNumber int KEY,
bankAmount double
);


CREATE SOURCE ranDataSource using StreamReader(
OutputType: 'LongRunningQueryTester.RandomData',
noLimit: 'false',
maxRows: 0,
iterations: 2,
iterationDelay: 1000,
StringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',
NumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]G'
)OUTPUT TO CSVDataStream;

CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4])
FROM CSVDataStream;




CREATE TYPE myData(
myName String,
myAddress String,
myBankName String,
myBankNumber int KEY,
myBankAmount double
);

CREATE STREAM myDataStream OF myData;

CREATE CQ GetMyData
INSERT INTO MyDataStream
SELECT myName, streetAddress, bankName, bankNumber, bankAmount
FROM RandomDataStream;


CREATE WACTIONSTORE MyDataActivity
CONTEXT OF MyData
EVENT TYPES(myData )
@PERSIST-TYPE@

Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
Select * from myDataStream
LINK SOURCE EVENT;


END APPLICATION LongRunningQuery;

create application SybaseJaguarApp;

create source SybaseJaguarSource using FileReader (
        directory:'@TEST-DATA-PATH@',
        WildCard:'SybaseJaguar.log',
        charset:'ISO-8859-1',
        positionByEOF:false
)
parse using FreeformTextParser (
        -- TimeStamp:'%E %mon %d %H:%M:%S %y %z',
        RecordBegin:'%H:%M:%S:%sss %yyyy%mm%d:1234567890:\n[TRACE]',
        RecordEnd:')\n',
	IgnoreMultipleRecordBegin: false,	
        regex:'((?<=logonname\\()[a-z,0-9]*)|((?<=localHostName\\()[a-z,A-Z,0-9]*)|((?<=localIP\\()[0-9,.]*)',
        separator:'~'
)
OUTPUT TO SybaseJaguarStream;

CREATE TYPE LoginInfo (
        userName String,
        hostName String,
        hostIP  String,
        eventTime long
);

create stream LoginInfoStream of LoginInfo;

create cq LoginInfoCQ
insert into LoginInfoStream
select
        data[0],
        data[1],
        data[2],
        TO_LONG(META(x,'OriginTimestamp'))
from SybaseJaguarStream x;


create Target SybaseJaguarDump using CSVWriter(filename:'@FEATURE-DIR@/logs/SybaseJaguar.log') input from LoginInfoStream;
end application SybaseJaguarApp;

STOP application admin.app1;
undeploy application admin.app1;
drop application admin.app1 cascade;


CREATE APPLICATION app1;

CREATE SOURCE S1 USING Global.OracleReader (
  Tables: 'QATEST.TEST01',
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  OutboundServerProcessName: 'WebActionXStream',
  Password: 'qatest',
  Compression: false,
  ReaderType: 'LogMiner',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  FetchSize: 1,
  Password_encrypted: 'false',
  SupportPDB: false,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  CDDLAction: 'Process',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  DictionaryMode: 'OnlineCatalog',
  QueueSize: 2048,
  CommittedTransactions: true,
  XstreamTimeOut: 600,
  CDDLCapture: false,
  TransactionBufferType: 'Disk',
  Username: 'qatest',
  TransactionBufferSpilloverSize: '100MB',
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  DatabaseRole: 'Primary' )
OUTPUT TO buffer;

CREATE TARGET t USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'qatest',
  ParallelThreads: '',
  DatabaseProviderType: 'Oracle',
  CheckPointTable: 'CHKPOINT',
  Password_encrypted: 'false',
  CDDLAction: 'Process',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.TEST01,QATEST.TEST02',
  CommitPolicy: 'EventCount:1000,Interval:60',
  StatementCacheSize: '50',
  Username: 'qatest',
  BatchPolicy: 'EventCount:1000,Interval:60',
  PreserveSourceTransactionBoundary: 'false' )
INPUT FROM buffer;

END APPLICATION app1;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using S3Writer(
    bucketname:'@BUCKET@',
   objectname:'upgradeData.csv',
   foldername:'upgradefolder',
  uploadpolicy:'EventCount : 10000,Interval :1m '
)
format using DSVFormatter (
)
input from @STREAM@;

end application @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @APPNAME@_src Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
) Output To @APPNAME@_stream;

create type @APPNAME@_type(
id int,
name String,
cost float,
Lvalue long,
PreviousPaid double,
dateOrder DateTime,
DwoTime Datetime,
DeliverDate Datetime,
TableName string,
OperationName String
);

create or replace stream @APPNAME@_typed_Stream of @APPNAME@_type;

Create CQ @APPNAME@_TypedCQ
insert into @APPNAME@_typed_Stream
select
to_int(data[0]),data[1],to_float(data[2]),to_long(data[3]),to_double(data[4]),
to_Date(data[5]),to_Date(data[6]),to_Date(data[7]),
meta(@APPNAME@_stream,'TableName'),
Meta(@APPNAME@_stream,'OperationName') from @APPNAME@_stream;


create Target @APPNAME@_tgt using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'eventCount:5',
    ServiceAccountKey:'@file-path@'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_typed_Stream;




end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;


CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using MysqlReader
(
   adapterName: MysqlReader,
   CDDLAction: Process,
   CDDLCapture: false,
   Compression: false,
   ConnectionURL: jdbc:mysql://localhost:3306/waction,
   FilterTransactionBoundaries: true,
   Password: ReaderPassword,
   SendBeforeImage: true,
   Tables: srcTable,
   Username: ReaderUsername
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;

stop DBRTOCW;
 undeploy application DBRTOCW;
 drop application DBRTOCW cascade;
 CREATE APPLICATION DBRTOCW;

 Create Source MSSQLSource Using MSSqlReader
(
Username:'qatest',
Password:'w@ct10n',
DatabaseName:'qatest',
ConnectionURL:'10.77.61.30:1433',
Tables:'qatest.MssqlTocql_Alldatatypes',
ConnectionPoolSize:1,
Compression:'true'
)
OUTPUT TO Oracle_ChangeDataStream;

 CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM Oracle_ChangeDataStream;

 create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

 END APPLICATION DBRTOCW;

 deploy application DBRTOCW in default;

 start DBRTOCW;

STOP APPLICATION @APPNAME@app1;
STOP APPLICATION @APPNAME@app2;
STOP APPLICATION @APPNAME@app3;
STOP APPLICATION @APPNAME@app4;
STOP APPLICATION @APPNAME@app5;
UNDEPLOY APPLICATION @APPNAME@app1;
UNDEPLOY APPLICATION @APPNAME@app2;
UNDEPLOY APPLICATION @APPNAME@app3;
UNDEPLOY APPLICATION @APPNAME@app4;
UNDEPLOY APPLICATION @APPNAME@app5;
DROP APPLICATION @APPNAME@app1 CASCADE;
DROP APPLICATION @APPNAME@app2 CASCADE;
DROP APPLICATION @APPNAME@app3 CASCADE;
DROP APPLICATION @APPNAME@app4 CASCADE;
DROP APPLICATION @APPNAME@app5 CASCADE;

CREATE APPLICATION @APPNAME@app1 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;

create flow @APPNAME@agentflowps;
CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',
acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');


create type @APPNAME@type1(
  id String,
  name String,
  city string
);

CREATE STREAM @APPNAME@sourcestream OF Global.waevent persist using @APPNAME@KafkaPropset;
CREATE STREAM @APPNAME@kps_typedStream OF @APPNAME@type1 partition by city persist using @APPNAME@KafkaPropset;


CREATE OR REPLACE SOURCE @APPNAME@s USING oracleReader  ( 
  Username:'qatest',
  Password:'qatest',
  ConnectionURL:'localhost:1521/xe',
  Tables:'QATEST.test01',
  FetchSize:1
 ) 
OUTPUT TO @APPNAME@rawstream;

create cq @APPNAME@cq1
INSERT INTO @APPNAME@sourcestream
SELECT * from @APPNAME@rawstream;

CREATE CQ @APPNAME@cq2
INSERT INTO @APPNAME@kps_typedStream
SELECT TO_STRING(data[0]),
TO_STRING(data[1]),
TO_STRING(data[2])FROM @APPNAME@rawstream;
end flow @APPNAME@agentflowps;
end application @APPNAME@app1;
--deploy application app1;
--deploy application app1 with agentflowps on AGENTS;
@DEPLOY@;

CREATE APPLICATION @APPNAME@app2 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE TARGET @APPNAME@app2_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS1'
) INPUT FROM @APPNAME@sourcestream;


end application @APPNAME@app2;
deploy application @APPNAME@app2;


CREATE APPLICATION @APPNAME@app3 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE TARGET @APPNAME@app3_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS2'
) INPUT FROM @APPNAME@sourcestream;

end application @APPNAME@app3;
--deploy application app3;


CREATE APPLICATION @APPNAME@app4 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE TARGET @APPNAME@app4_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS3'
) INPUT FROM @APPNAME@sourcestream;

end application @APPNAME@app4;
--deploy application app4;


CREATE APPLICATION @APPNAME@app5 RECOVERY 1 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE TARGET @APPNAME@app5_target1 USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'snappy1',
KafkaConfig:'compression.type=snappy'
) 
FORMAT USING DSVFormatter ()
INPUT FROM @APPNAME@kps_typedStream;

CREATE TARGET @APPNAME@app5_target2 USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'gzip1',
KafkaConfig:'compression.type=gzip'
) 
FORMAT USING DSVFormatter ()
INPUT FROM @APPNAME@sourcestream;

CREATE TARGET @APPNAME@app5_target3 USING KafkaWriter VERSION '0.11.0'(
brokeraddress:'localhost:9092',
topic:'lz41',
KafkaConfig:'compression.type=lz4'
) 
FORMAT USING DSVFormatter ()
INPUT FROM @APPNAME@sourcestream;

end application @APPNAME@app5;
--deploy application app5;

--
-- Recovery Test 99
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov99Tester.RecovTest99;
UNDEPLOY APPLICATION Recov99Tester.RecovTest99;
DROP APPLICATION Recov99Tester.RecovTest99 CASCADE;
CREATE APPLICATION RecovTest99 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW BasicComponentTestsFlow;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

-- unpartitioned windows

CREATE WINDOW DataStreamSliding3U
OVER DataStream KEEP 3 ROWS;

CREATE WINDOW DataStreamSliding11U
OVER DataStream KEEP 11 ROWS;

CREATE JUMPING WINDOW DataStreamJumping5U
OVER DataStream KEEP 5 ROWS;

CREATE JUMPING WINDOW DataStreamJumping7U
OVER DataStream KEEP 7 ROWS;

CREATE JUMPING WINDOW DataStream5MinutesU
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6MinutesU
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;


-- partitioned windows

CREATE WINDOW DataStreamSliding3P
OVER DataStream KEEP 3 ROWS
PARTITION BY merchantId;

CREATE WINDOW DataStreamSliding11P
OVER DataStream KEEP 11 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStreamJumping5P
OVER DataStream KEEP 5 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStreamJumping7P
OVER DataStream KEEP 7 ROWS
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream5MinutesP
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6MinutesP
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;





-- THE TEST OUTPUTS


-- Wactions01 receives data straight through from the Source
-- should contain the exact data emitted
CREATE WACTIONSTORE Wactions_S1 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions01
INSERT INTO Wactions01
SELECT *
FROM DataStream;








-- test test boundaries for jumping/sliding partitioned/unpartitioned aggregate/non-aggregate

-- SLIDING

-- Wactions_SW3UN data goes through an unpartitioned sliding window then a non-aggregate CQ

CREATE WACTIONSTORE Wactions_SW3UN CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_SW3UN
INSERT INTO Wactions_SW3UN
SELECT *
FROM DataStreamSliding3U;


-- Wactions_SW3UN data goes through a partitioned sliding window then a non-aggregate CQ

CREATE WACTIONSTORE Wactions_SW3PN CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_SW3PN
INSERT INTO Wactions_SW3PN
SELECT *
FROM DataStreamSliding3P;


-- Wactions_SW3UN data goes through an unpartitioned sliding window then an aggregate CQ

CREATE WACTIONSTORE Wactions_SW3UA CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_SW3UA
INSERT INTO Wactions_SW3UA
SELECT FIRST(*)
FROM DataStreamSliding3U;


-- Wactions_SW3UN data goes through a partitioned sliding window then an aggregate CQ

CREATE WACTIONSTORE Wactions_SW3PA CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_SW3PA
INSERT INTO Wactions_SW3PA
SELECT FIRST(*)
FROM DataStreamSliding3P;



-- JUMPING


-- Wactions_JW3UN data goes through an unpartitioned jumping window then a non-aggregate CQ

CREATE WACTIONSTORE Wactions_JW5UN CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_JW5UN
INSERT INTO Wactions_JW5UN
SELECT *
FROM DataStreamJumping5U;


-- Wactions_JW5UN data goes through a partitioned sliding jumping then a non-aggregate CQ

CREATE WACTIONSTORE Wactions_JW5PN CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_JW5PN
INSERT INTO Wactions_JW5PN
SELECT *
FROM DataStreamJumping5P;


-- Wactions_JW5UN data goes through an unpartitioned jumping window then an aggregate CQ

CREATE WACTIONSTORE Wactions_JW5UA CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_JW5UA
INSERT INTO Wactions_JW5UA
SELECT FIRST(*)
FROM DataStreamJumping5U;


-- Wactions_JW5UN data goes through a partitioned jumping window then an aggregate CQ

CREATE WACTIONSTORE Wactions_JW5PA CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions_JW5PA
INSERT INTO Wactions_JW5PA
SELECT FIRST(*)
FROM DataStreamJumping5P;







END FLOW BasicComponentTestsFlow;










CREATE FLOW ComplexScenariosFlow;





CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;








-- Wactions03 data goes through an unpartitioned jumping window then an aggregate CQ
-- should contain one waction for every 5 events
CREATE WACTIONSTORE Wactions03 CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions03
INSERT INTO Wactions03
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStreamJumping5 p;


-- Wactions04 data goes through a size-6 unpartitioned jumping window then an aggregate CQ
-- should contain one waction for every 6 events
CREATE WACTIONSTORE Wactions04 CONTEXT OF WactionType
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data6ToWactios04
INSERT INTO Wactions04
SELECT
    FIRST(p.companyName),
    FIRST(p.merchantId),
    FIRST(p.dateTime),
    SUM(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;





END FLOW ComplexScenariosFlow;






END APPLICATION RecovTest99;

CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( 
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: '@CDC_URL@',
  Tables: '@Source1Tables@',
  FetchSize: 1) 
OUTPUT TO @APPNAME@cdcStream;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@1 ( 
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  FetchSize: 20,
  DatabaseProviderType: 'Default',
  Table: '@Source3Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')  
OF @APPNAME@cachetype;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@2 ( 
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  FetchSize: 10,
  DatabaseProviderType: 'Default',
  Table: '@Source2Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')  
OF @APPNAME@cachetype;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;

CREATE APPLICATION @APP_NAME@;
CREATE OR REPLACE SOURCE @APP_NAME@_src USING Global.OracleReader (
  ConnectionURL: '@CONN_URL@',
  Tables: '@TABLES@',
  Username: '@USERNAME@',
  Password: '@PASSWORD@'
) OUTPUT TO @APP_NAME@_Stream;


CREATE OR REPLACE TARGET @APP_NAME@_tgt USING Global.FabricDataWarehouseWriter (
  Tables: '@TABLES@',
  ConnectionURL: '@CONN_URL@',
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  uploadpolicy: 'eventcount:1',
  AccountName: '@ACCOUNTNAME@')
INPUT FROM @APP_NAME@_Stream;

END APPLICATION @APP_NAME@;
DEPLOY APPLICATION @APP_NAME@;
START APPLICATION @APP_NAME@;

--
-- Recovery Test 27 with two sources, two jumping time windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jt1W -> CQ1 -> WS
--   S2 -> Jt2W -> CQ2 -> WS
--

STOP KStreamRecov27Tester.KStreamRecovTest27;
UNDEPLOY APPLICATION KStreamRecov27Tester.KStreamRecovTest27;
DROP APPLICATION KStreamRecov27Tester.KStreamRecovTest27 CASCADE;
DROP USER KStreamRecov27Tester;
DROP NAMESPACE KStreamRecov27Tester CASCADE;
CREATE USER KStreamRecov27Tester IDENTIFIED BY KStreamRecov27Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov27Tester;
CONNECT KStreamRecov27Tester KStreamRecov27Tester;

CREATE APPLICATION KStreamRecovTest27 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream1Second
OVER DataStream1 KEEP WITHIN 1 SECOND;

CREATE JUMPING WINDOW DataStream2Second
OVER DataStream2 KEEP WITHIN 2 SECOND;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data1ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream1Second p;

CREATE CQ Data2ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream2Second p;

END APPLICATION KStreamRecovTest27;

create application HTTPtest;
create source DSVCSVSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'customerdetails.csv',
	charset: 'UTF-8',
	positionByEOF:false
)
parse using DSVParser (
	header:'no'
)
OUTPUT TO DSVCsvStream;
create Target DSVDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/customerdetails') input from DSVCsvStream;
end application HTTPtest;

stop ROLLUPMON_CDC;
undeploy application ROLLUPMON_CDC;
alter application ROLLUPMON_CDC;
CREATE or replace FLOW ROLLUPMON_CDC_flow;
Create or replace Source ROLLUPMON_CDC_Oraclesrc Using oraclereader(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',
 Tables:'QATEST.ROLLUPMON_TABLE1;QATEST.ROLLUPMON_TABLE2;QATEST.ROLLUPMON_TABLE3;QATEST.ROLLUPMON_TABLE4;QATEST.ROLLUPMON_TABLE5',
 Fetchsize:1000,
 connectionRetryPolicy:'maxRetries=4',
 TransactionBufferSpilloverSize:'200MB',
 _h_fetchexactrowcount: 'true'
)
Output To ROLLUPMON_CDC_OrcStrm;
END FLOW ROLLUPMON_CDC_flow;
alter application ROLLUPMON_CDC recompile;
DEPLOY APPLICATION ROLLUPMON_CDC;
start application ROLLUPMON_CDC;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 1 second interval;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@',
  CopybookFileFormat:'USE_COLS_6_TO_80'
  --recordSelector: '@PROP8@'
)
OUTPUT TO @APPNAME@Stream;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;

/*
create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:1,Interval:5',
  CommitPolicy: 'EventCount:1,Interval:5',
  Tables: 'QATEST.@table@'
)
input from @APPNAME@Stream;*/
end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

CREATE OR REPLACE PROPERTYSET LDAP1 ( PROVIDER_URL:"ldap://10.77.12.210:389", SECURITY_AUTHENTICATION:simple, SECURITY_PRINCIPAL: "praveen,dc=qa,dc=webaction,dc=com" , SECURITY_CREDENTIALS:InvalidPwd, USER_BASE_DN:"dc=qa,dc=webaction,dc=com", User_userId:cn );

--
-- Recovery Test 25 with two sources, two jumping count windows, and one wactionstore -- all with no partitioning
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5W -> CQ1 -> WS
--   S2 -> Jc6W -> CQ2 -> WS
--

STOP Recov25Tester.RecovTest25;
UNDEPLOY APPLICATION Recov25Tester.RecovTest25;
DROP APPLICATION Recov25Tester.RecovTest25 CASCADE;
CREATE APPLICATION RecovTest25 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream1 OF CsvData;
CREATE STREAM DataStream2 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP 5 ROWS;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP 6 ROWS;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION RecovTest25;

create flow AgentFlow;

CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (
  StartPosition: '@startPosition@',
  ConnectionURL: '@sourceURL@',
  Username: '@userName@',
  Tables: '@tables@',
  CheckColumn: '@checkColum@',
  FetchSize: 1,
  Password: '@password@',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

end flow AgentFlow;

--
-- Canon Test W80
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for a partitioned jumping attribute window
--
-- S -> JWa5p -> CQ -> WS
--


UNDEPLOY APPLICATION NameW80.W80;
DROP APPLICATION NameW80.W80 CASCADE;
CREATE APPLICATION W80 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW80;

CREATE SOURCE CsvSourceW80 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW80;

END FLOW DataAcquisitionW80;



CREATE FLOW DataProcessingW80;

CREATE TYPE DataTypeW80 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW80 OF DataTypeW80;

CREATE CQ CSVStreamW80_to_DataStreamW80
INSERT INTO DataStreamW80
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW80;

CREATE JUMPING WINDOW JWa5pW80
OVER DataStreamW80
KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY word;

CREATE WACTIONSTORE WactionStoreW80 CONTEXT OF DataTypeW80
EVENT TYPES ( DataTypeW80 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWa5pW80_to_WactionStoreW80
INSERT INTO WactionStoreW80
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWa5pW80
GROUP BY word;

END FLOW DataProcessingW80;



END APPLICATION W80;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
wildcard: '',
directory: '',
positionbyeof: false )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: ''
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
wildcard: '',
directory: '',
positionbyeof: false )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: 'ParquetFileSchema'
)
INPUT FROM @appname@CqOut;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using ParquetFormatter (
schemaFileName: 'ParquetS3Schema'
)
input from @appname@CqOut;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using ParquetFormatter (
schemaFileName: 'ParquetAzureSchema'
)
INPUT FROM @appname@CqOut;

CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using ParquetFormatter (
schemaFileName: 'ParquetGCSSchema'
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

STOP @Appname@;
UNDEPLOY APPLICATION @Appname@;
DROP APPLICATION @Appname@ CASCADE;

CREATE APPLICATION @Appname@ @Recovery@ use exceptionstore;

CREATE SOURCE @Appname@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:orcl',
	Tables: 'QATEST.TABLE_TEST_%',
	FetchSize: '1'
)
OUTPUT TO @Appname@_SS;


CREATE or replace TARGET @Appname@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:10',
StandardSQL:true	
) INPUT FROM @Appname@_ss;

END APPLICATION @Appname@;
DEPLOY APPLICATION @Appname@;
START APPLICATION @Appname@;

use PosTester;
alter application PosApp;

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

end application PosApp;

alter application PosApp recompile;

stop application DualGen;
undeploy application DualGen;
drop application DualGen cascade;
CREATE APPLICATION DualGen;

CREATE OR REPLACE TYPE DualEvent (
    Dummy DateTime,
    PhoneNo java.lang.String
);

CREATE OR REPLACE STREAM DualEvents OF DualEvent;

CREATE OR REPLACE CQ GenDual 
INSERT INTO DualEvents
SELECT
  TO_DATEF('28-FEB-22',"dd-MMM-yy"),maskPhoneNumber('44 844 493 0787', "(\\\\d{0,4}\\\\s)(\\\\d{0,4}\\\\s)([0-9 ]+)", 1, 2)  as Dummy
FROM
   heartbeat(interval @INTERVAL@ second) h;


CREATE OR REPLACE TARGET DualSys USING SysOut  ( 
  name: 'heartbeat_out'
 ) 
INPUT FROM DualEvents;


CREATE TARGET DSVFormatterOut using FileWriter(
 filename:'HeartBeat_Output.log',
 flushpolicy:'EventCount:6',
 rolloverpolicy:'interval:97s')
FORMAT USING DSVFormatter ()
INPUT FROM DualEvents;

END APPLICATION DualGen;
deploy application DualGen;
start application DualGen;

STOP APPLICATION persistencetester.ranGenAppES;
UNDEPLOY APPLICATION persistencetester.ranGenAppES;
DROP APPLICATION persistencetester.ranGenAppES cascade;

CREATE APPLICATION ranGenAppES;

CREATE TYPE Atm(
  productID String KEY,
  stateID String,
  productWeight int,
  quantity int,
  size int,
  myDate datetime
);

CREATE  SOURCE liveSource USING StreamReader (
  OutputType: 'Atm',
  noLimit: 'false',
  maxRows: 20,
  iterations: 0,
  iterationDelay: 2000,
  StringSet: 'productID[1001-1002-1003-1004],stateID[BofA-WellsFargo-Chase-NYBank]',
  NumberSet: 'productWeight[3-3]R,quantity[0-200]R,size[250-1250]R'
 )
OUTPUT TO CsvStream;

CREATE STREAM ranDataStream OF Atm;

CREATE CQ ranToRanData
INSERT INTO ranDataStream
SELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4]), TO_DATE(data[5])
FROM CsvStream;

CREATE WACTIONSTORE RanGenDataStore
CONTEXT OF Atm
EVENT TYPES ( Atm )
@PERSIST-TYPE@

CREATE CQ TrackRanGenDataActivity
INSERT INTO RanGenDataStore
Select * from ranDataStream
LINK SOURCE EVENT;


END APPLICATION ranGenAppES;

stop APPLICATION MultiLogApp;
undeploy APPLICATION MultiLogApp;
drop APPLICATION MultiLogApp cascade;

CREATE APPLICATION MultiLogApp;

-- This sample application shows how Striim could be used monitor and correlate logs 
-- from web and application server logs from the same web application. See the discussion 
-- in the "Sample Applications" section of the Striim documentation for additional 
-- discussion.


CREATE FLOW MonitorLogs;

-- MonitorLogs sets up the two log sources used by this application. In a real-world
--implementation, each source could be reading many logs from many servers.

-- The web server logs are in Apache NCSA extended/ combined log format plus response time:
-- "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-agent}i\" %D"
-- See apache.org for more information.

CREATE SOURCE AccessLogSource USING FileReader (
  directory:'Samples/Customer/MultiLogApp/appData',
  wildcard:'access_log',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~',
  trimwhitespace:true
)
OUTPUT TO RawAccessStream;

CREATE TYPE AccessLogEntry (
    srcIp String KEY,
    userId String,
    sessionId String,
    accessTime DateTime,
    request String,
    code integer,
    size integer,
    referrer String,
    userAgent String,
    responseTime integer
);
CREATE STREAM AccessStream OF AccessLogEntry;

CREATE CQ ParseAccessLog 
INSERT INTO AccessStream
SELECT data[0], data[2], MATCH(data[4], ".*jsessionId=(.*) "),
       TO_DATE(data[3], "dd/MMM/yyyy:HH:mm:ss.SSS Z"), data[4], TO_INT(data[5]), TO_INT(data[6]),
       data[7], data[8], TO_INT(data[9])
FROM RawAccessStream;

-- The application server logs are in Apache's Log4J format.

CREATE SOURCE Log4JSource USING FileReader (
  directory:'Samples/Customer/MultiLogApp/appData',
  wildcard:'log4jLog.xml',
  positionByEOF:false
) 
PARSE USING XMLParser(
  rootnode:'/log4j:event',
  columnlist:'log4j:event/@timestamp,log4j:event/@level,log4j:event/log4j:message,log4j:event/log4j:throwable,log4j:event/log4j:locationInfo/@class,log4j:event/log4j:locationInfo/@method,log4j:event/log4j:locationInfo/@file,log4j:event/log4j:locationInfo/@line'
)
OUTPUT TO RawXMLStream;

CREATE TYPE Log4JEntry (
  logTime DateTime,
  level String,
  message String,
  api String,
  sessionId String,
  userId String,
  sobject String,
  xception String,
  className String,
  method String,
  fileName String,
  lineNum String
);
CREATE STREAM Log4JStream OF Log4JEntry;

CREATE CQ ParseLog4J
INSERT INTO Log4JStream
SELECT TO_DATE(TO_LONG(data[0])), data[1], data[2], 
       MATCH(data[2], '\\\\[api=([a-zA-Z0-9]*)\\\\]'),
       MATCH(data[2], '\\\\[session=([a-zA-Z0-9\\-]*)\\\\]'),
       MATCH(data[2], '\\\\[user=([a-zA-Z0-9\\-]*)\\\\]'),
       MATCH(data[2], '\\\\[sobject=([a-zA-Z0-9]*)\\\\]'),
       data[3], data[4], data[5], data[6], data[7]
FROM RawXMLStream;

END FLOW MonitorLogs;


CREATE FLOW ErrorsAndWarnings;

-- ErrorsAndWarnings creates a sliding window (Log4JErrorWarningActivity) containing 
-- the 300 most recent errors and warnings in the application server log. The 
-- ZeroContentCheck and LargeRTCheck flows join events from this window with access log 
-- events.

-- The type Log4JEntry was already defined by the MonitorLogs flow.
CREATE STREAM Log4ErrorWarningStream OF Log4JEntry;

CREATE CQ GetLog4JErrorWarning
INSERT INTO Log4ErrorWarningStream
SELECT l FROM Log4JStream l
WHERE l.level = 'ERROR' OR l.level = 'WARN';

CREATE WINDOW Log4JErrorWarningActivity 
OVER Log4ErrorWarningStream KEEP 300 ROWS;

END FLOW ErrorsAndWarnings;


-- HackerCheck sends an alert when an access log srcIp value is on a blacklist.

CREATE FLOW HackerCheck;

CREATE TYPE IPEntry (
    ip String
);

/* CREATE CACHE BlackListLookup using CSVReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'multiLogBlackList.txt',
  header: No,
  columndelimiter: ','
) QUERY (keytomap:'ip') OF IPEntry; */

CREATE CACHE BlackListLookup using FileReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'multiLogBlackList.txt'
)
PARSE USING DSVParser ( )
QUERY (keytomap:'ip') OF IPEntry;


CREATE STREAM HackerStream OF AccessLogEntry;

CREATE CQ FindHackers
INSERT INTO HackerStream
SELECT ale 
FROM AccessStream ale, BlackListLookup bll
WHERE ale.srcIp = bll.ip;

CREATE TYPE UnusualContext (
    typeOfActivity String,
    accessTime DateTime,
    accessSessionId String,
    srcIp String KEY,
    userId String,
    country String,
    city String,
    lat double,
    lon double
);
CREATE TYPE MergedEntry (
    accessTime DateTime,
    accessSessionId String,
    srcIp String KEY,
    userId String,
    request String,
    code integer,
    size integer,
    referrer String,
    userAgent String,
    responseTime integer,
    logTime DateTime,
    logSessionId String,
    level String,
    message String,
    api String,
    sobject String,
    xception String,
    className String,
    method String,
    fileName String,
    lineNum String
);
CREATE WACTIONSTORE UnusualActivity 
CONTEXT OF UnusualContext 
EVENT TYPES (MergedEntry, AccessLogEntry)
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ GenerateHackerContext
INSERT INTO UnusualActivity
SELECT 'HackAttempt', accessTime, sessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM HackerStream
LINK SOURCE EVENT;

CREATE STREAM HackingAlertStream OF Global.AlertEvent;

CREATE CQ SendHackingAlerts 
INSERT INTO HackingAlertStream 
SELECT 'HackingAlert', ''+accessTime, 'warning', 'raise', 
        'Possible Hacking Attempt from ' + srcIp + ' in ' + IP_COUNTRY(srcIp)
FROM HackerStream; 

CREATE SUBSCRIPTION HackingAlertSub USING WebAlertAdapter( ) INPUT FROM HackingAlertStream;

END FLOW HackerCheck;


-- LargeRTCheck sends an alert when an access log responseTime value exceeds 2000 
-- microseconds.

CREATE FLOW LargeRTCheck;

CREATE STREAM LargeRTStream of AccessLogEntry;

CREATE CQ FindLargeRT
INSERT INTO LargeRTStream
SELECT ale
FROM AccessStream ale
WHERE ale.responseTime > 2000;

CREATE WINDOW LargeRTActivity 
OVER LargeRTStream KEEP 100 ROWS;

CREATE STREAM LargeRTAPIStream OF MergedEntry;

CREATE CQ MergeLargeRTAPI
INSERT INTO LargeRTAPIStream
SELECT lrt.accessTime, lrt.sessionId, lrt.srcIp, lrt.userId, lrt.request, 
       lrt.code, lrt.size, lrt.referrer, lrt.userAgent, lrt.responseTime,
       log4j.logTime, log4j.sessionId, log4j.level, log4j.message, log4j.api, log4j.sobject, log4j.xception, 
       log4j.className, log4j.method, log4j.fileName, log4j.lineNum
FROM LargeRTActivity lrt, Log4JErrorWarningActivity log4j
WHERE lrt.sessionId = log4j.sessionId
      AND lrt.accessTime = log4j.logTime;   

CREATE CQ GenerateLargeRTContext
INSERT INTO UnusualActivity
SELECT 'LargeResponseTime', accessTime, accessSessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM LargeRTAPIStream
LINK SOURCE EVENT;

CREATE STREAM LargeRTAlertStream OF Global.AlertEvent;

CREATE CQ SendLargeRTAlerts 
INSERT INTO LargeRTAlertStream 
SELECT 'LargeRTAlert', ''+accessTime, 'warning', 'raise', 
        'Long response time for call from ' + userId + ' api ' + api + ' message ' + message
FROM LargeRTAPIStream; 

CREATE SUBSCRIPTION LargeRTAlertSub USING WebAlertAdapter( ) INPUT FROM LargeRTAlertStream;

END FLOW LargeRTCheck;


-- ProxyCheck sends an alert when an access log srcIP value is on a list of suspicious 
-- proxies.

CREATE FLOW ProxyCheck;

/* CREATE CACHE ProxyLookup using CSVReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'multiLogProxies.txt',
  header: No,
  columndelimiter: ','
) QUERY (keytomap:'ip') OF IPEntry; */

CREATE CACHE ProxyLookup using FileReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'multiLogProxies.txt'
)
PARSE USING DSVParser ( )
QUERY (keytomap:'ip') OF IPEntry; 


CREATE STREAM ProxyStream OF AccessLogEntry;

CREATE CQ FindProxies
INSERT INTO ProxyStream
SELECT ale 
FROM AccessStream ale, ProxyLookup pl
WHERE ale.srcIp = pl.ip;

CREATE CQ GenerateProxyContext
INSERT INTO UnusualActivity
SELECT 'ProxyAccess', accessTime, sessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM ProxyStream
LINK SOURCE EVENT;


CREATE STREAM ProxyAlertStream OF Global.AlertEvent;

CREATE CQ SendProxyAlerts 
INSERT INTO ProxyAlertStream 
SELECT 'ProxyAlert', ''+accessTime, 'warning', 'raise', 
        'Possible use of Proxy from ' + srcIp + ' in ' + IP_COUNTRY(srcIp) + ' for user ' + userId 
FROM ProxyStream; 

CREATE SUBSCRIPTION ProxyAlertSub USING WebAlertAdapter( ) INPUT FROM ProxyAlertStream;

END FLOW ProxyCheck;


-- ZeroContentCheck sends an alert when an access log entry's code value is 200 (that is,
-- the HTTP request succeeded) but the size value is 0 (the return had no content).

CREATE FLOW ZeroContentCheck;

CREATE STREAM ZeroContentStream of AccessLogEntry;

CREATE CQ FindZeroContent
INSERT INTO ZeroContentStream
SELECT ale
FROM AccessStream ale
WHERE ale.code = 200 AND ale.size = 0;

CREATE WINDOW ZeroContentActivity 
OVER ZeroContentStream KEEP 100 ROWS;

CREATE STREAM ZeroContentAPIStream OF MergedEntry;

CREATE CQ MergeZeroContentAPI
INSERT INTO ZeroContentAPIStream
SELECT zcs.accessTime, zcs.sessionId, zcs.srcIp, zcs.userId, zcs.request, 
       zcs.code, zcs.size, zcs.referrer, zcs.userAgent, zcs.responseTime,
       log4j.logTime, log4j.sessionId, log4j.level, log4j.message, log4j.api, log4j.sobject, log4j.xception, 
       log4j.className, log4j.method, log4j.fileName, log4j.lineNum
FROM ZeroContentActivity zcs, Log4JErrorWarningActivity log4j
WHERE zcs.sessionId = log4j.sessionId
      AND zcs.accessTime = log4j.logTime;   

CREATE CQ GenerateZeroContentContext
INSERT INTO UnusualActivity
SELECT 'ZeroContent', accessTime, accessSessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM ZeroContentAPIStream
LINK SOURCE EVENT;


CREATE TYPE ZeroContentEventListType (
    srcIp String KEY,
    code integer,
    size integer,
    level String,
    message String,
    xception String);
    
CREATE WACTIONSTORE ZeroContentEventList
CONTEXT OF ZeroContentEventListType 
EVENT TYPES (ZeroContentEventListType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );


CREATE CQ GenerateZeroContentEventList
INSERT INTO ZeroContentEventList
SELECT srcIp, code, size, level, message, xception
FROM ZeroContentAPIStream;


CREATE STREAM ZeroContentAlertStream OF Global.AlertEvent;

CREATE CQ SendZeroContentAlerts 
INSERT INTO ZeroContentAlertStream 
SELECT 'ZeroContentAlert', ''+accessTime, 'warning', 'raise', 
        'Zero content returned in call from ' + userId + ' api ' + api + ' message ' + message
FROM ZeroContentAPIStream; 

CREATE SUBSCRIPTION ZeroContentAlertSub USING WebAlertAdapter( ) INPUT FROM ZeroContentAlertStream;

END FLOW ZeroContentCheck;


-- ErrorHandling is functionally identical to ErrorFlow.SaasMonitorApp. It sends an alert 
-- when an error message appears in the application server log.

CREATE FLOW ErrorHandling;

CREATE STREAM ErrorStream OF Log4JEntry;

CREATE CQ GetErrors 
INSERT INTO ErrorStream 
SELECT log4j 
FROM Log4ErrorWarningStream log4j WHERE log4j.level = 'ERROR';

CREATE STREAM ErrorAlertStream OF Global.AlertEvent;

CREATE CQ SendErrorAlerts 
INSERT INTO ErrorAlertStream 
SELECT 'ErrorAlert', ''+logTime, 'error', 'raise', 'Error in log ' + message 
FROM ErrorStream;

CREATE SUBSCRIPTION ErrorAlertSub USING WebAlertAdapter( ) INPUT FROM ErrorAlertStream;

END FLOW ErrorHandling;


-- WarningHandling is a minor variation on WarningFlow.SaasMonitorApp. It sends an alert 
-- once an hour with the count of warnings for each api call for which there has been at 
-- least one alert.

CREATE FLOW WarningHandling;

CREATE STREAM WarningStream OF Log4JEntry;

CREATE CQ GetWarnings 
INSERT INTO WarningStream 
SELECT log4j 
FROM Log4ErrorWarningStream log4j WHERE log4j.level = 'WARN';

CREATE JUMPING WINDOW WarningWindow 
OVER WarningStream KEEP WITHIN 60 MINUTE ON logTime;

CREATE STREAM WarningAlertStream OF Global.AlertEvent;

CREATE CQ SendWarningAlerts 
INSERT INTO WarningAlertStream 
SELECT 'WarningAlert', ''+logTime, 'warning', 'raise', 
        COUNT(logTime) + ' Warnings in log for api ' + api 
FROM WarningWindow 
GROUP BY api 
HAVING count(logTime) > 1;

CREATE SUBSCRIPTION WarningAlertSub USING WebAlertAdapter( ) INPUT FROM WarningAlertStream;

END FLOW WarningHandling;


-- InfoFlow is functionally similar to InfoFlow.SaasMonitorApp. Its output is used by 
-- ApiFlow, CompanyApiFlow, and UserApiFlow.

CREATE FLOW InfoFlow;

CREATE TYPE UserInfo (
  userId String, 
  userName String, 
  company String,  
  userZip String,  
  companyZip String
);

/* CREATE CACHE MLogUserLookup using CSVReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'multiLogUser.csv',
  header: No,
  columndelimiter: ','
) QUERY (keytomap:'userId') OF UserInfo; */

CREATE CACHE MLogUserLookup using FileReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'multiLogUser.csv'
)
PARSE USING DSVParser ( )
QUERY (keytomap:'userId') OF UserInfo;

CREATE STREAM InfoStream OF Log4JEntry;

CREATE CQ GetInfo 
INSERT INTO InfoStream 
SELECT log4j 
FROM Log4JStream log4j WHERE log4j.level = 'INFO';

CREATE TYPE ApiCall (
  userId String, 
  api String, 
  sobject String, 
  logTime DateTime, 
  userName String, 
  company String,  
  userZip String,  
  companyZip String
);
CREATE STREAM ApiEnrichedStream OF ApiCall;

CREATE CQ GetUserDetails 
INSERT INTO ApiEnrichedStream 
SELECT a.userId, a.api, a.sobject, a.logTime, u.userName, u.company, u.userZip, u.companyZip 
FROM InfoStream a, MLogUserLookup u 
WHERE a.userId = u.userId;

END FLOW InfoFlow;


-- ApiFlow populates the dashboard's Detail - ApiActivity page and the pie chart on the 
-- Overview page.

CREATE FLOW ApiFlow;

CREATE TYPE ApiUsage (
  api String key, 
  sobject String, 
  count integer, 
  logTime DateTime
);

CREATE TYPE ApiContext (
  api String key, 
  count integer, 
  logTime DateTime
);

CREATE WACTIONSTORE ApiActivity 
CONTEXT OF ApiContext 
EVENT TYPES (ApiUsage )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE JUMPING WINDOW ApiWindow 
OVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime 
PARTITION BY api;

CREATE STREAM ApiUsageStream OF ApiUsage;

CREATE CQ GetApiUsage 
INSERT INTO ApiUsageStream 
SELECT a.api, a.sobject, 
       COUNT(a.userId), FIRST(a.logTime) 
FROM ApiWindow a 
GROUP BY a.api, a.sobject HAVING FIRST(a.logTime) IS NOT NULL;

CREATE JUMPING WINDOW ApiSummaryWindow 
OVER ApiUsageStream KEEP WITHIN 1 HOUR ON logTime 
PARTITION BY api;

CREATE CQ GetApiSummaryUsage 
INSERT INTO ApiActivity 
SELECT a.api,  
       sum(a.count), first(a.logTime)
FROM ApiSummaryWindow a 
GROUP BY a.api
LINK SOURCE EVENT;

END FLOW ApiFlow;


-- CompanyApiFlow populates the dashboard's Detail - CompanyApiActivity page and the bar 
-- chart on the Overview page. It also sends an alert when an API call is used by a 
-- company more than 1500 times during the flow's one-hour jumping window.

CREATE FLOW CompanyApiFlow;

CREATE TYPE CompanyApiUsage (
  company String key, 
  companyZip String, 
  companyLat double, 
  companyLong double, 
  api String, 
  count integer, 
  unusual integer,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE TYPE CompanyApiContext (
  company String key, 
  companyZip String, 
  companyLat double, 
  companyLong double, 
  count integer, 
  unusual integer,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE WACTIONSTORE CompanyApiActivity 
CONTEXT OF CompanyApiContext 
EVENT TYPES (CompanyApiUsage )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE JUMPING WINDOW CompanyApiWindow 
OVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime 
PARTITION BY company;

CREATE STREAM CompanyApiUsageStream OF CompanyApiUsage;

CREATE TYPE MLogUSAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

/*CREATE CACHE MLogZipLookup using CSVReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'USAddresses.txt',
  header: Yes,
  columndelimiter: ','
) QUERY (keytomap:'zip') OF MLogUSAddressData; */

CREATE CACHE MLogZipLookup using FileReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'USAddresses.txt'
)
PARSE USING DSVParser (
  header: Yes
)
QUERY (keytomap:'zip') OF MLogUSAddressData;


CREATE CQ GetCompanyApiUsage 
INSERT INTO CompanyApiUsageStream 
SELECT a.company, a.companyZip, z.latVal, z.longVal, 
       a.api, COUNT(a.sobject), 
       CASE WHEN COUNT(a.sobject) > 1500 THEN 1
            ELSE 0 END,
       CASE WHEN COUNT(a.sobject) > 1500 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.sobject),
       FIRST(a.logTime) 
FROM CompanyApiWindow a, MLogZipLookup z 
WHERE a.companyZip = z.zip 
GROUP BY a.company, a.api HAVING FIRST(a.logTime) IS NOT NULL;

CREATE JUMPING WINDOW CompanyWindow 
OVER CompanyApiUsageStream KEEP WITHIN 1 HOUR ON logTime 
PARTITION BY company;

CREATE CQ GetCompanyUsage 
INSERT INTO CompanyApiActivity 
SELECT a.company, a.companyZip, a.companyLat, a.companyLong, 
       SUM(a.count), SUM(a.unusual), 
       CASE WHEN SUM(a.unusual) > 0 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.topObject),
       FIRST(a.logTime) 
FROM CompanyWindow a 
GROUP BY a.company
LINK SOURCE EVENT;

CREATE STREAM CompanyAlertStream OF Global.AlertEvent;

CREATE CQ SendCompanyApiAlerts 
INSERT INTO CompanyAlertStream 
SELECT 'CompanyAPIAlert', ''+logTime, 'warning', 'raise', 
       'Company ' + company + ' has used api ' + api + ' ' + count + ' times for ' + topObject 
FROM CompanyApiUsageStream 
WHERE unusual = 1;

CREATE SUBSCRIPTION CompanyAlertSub USING WebAlertAdapter( ) INPUT FROM CompanyAlertStream;

END FLOW CompanyApiFlow;


-- UserApiFlow populates the dashboard's Detail - UserApiActivity page and the US map on 
-- the Overview page. It also sends an alert when an API call is used by a user more than 
-- 125 times during the flow's one-hour window.

CREATE FLOW UserApiFlow;

CREATE TYPE UserApiUsage (
  userId String key, 
  userName String, 
  userZip String, 
  userLat double, 
  userLong double, 
  company String, 
  api String, 
  count integer, 
  unusual integer,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE TYPE UserApiContext (
  userId String key, 
  userName String, 
  userZip String, 
  userLat double, 
  userLong double, 
  company String, 
  count integer, 
  unusual integer,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE WACTIONSTORE UserApiActivity
CONTEXT OF UserApiContext 
EVENT TYPES (  UserApiUsage ) 
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE JUMPING WINDOW UserApiWindow 
OVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY userId;

CREATE STREAM UserApiUsageStream OF UserApiUsage;

CREATE CQ GetUserApiUsage 
INSERT INTO UserApiUsageStream 
SELECT a.userId, a.userName, a.userZip, z.latVal, z.longVal, a.company,
       a.api, COUNT(a.sobject), 
       CASE WHEN COUNT(a.sobject) > 125 THEN 1
            ELSE 0 END,
       CASE WHEN COUNT(a.sobject) > 125 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.sobject),
       FIRST(a.logTime) 
FROM UserApiWindow a, MLogZipLookup z 
WHERE a.userZip = z.zip 
GROUP BY a.userId, a.api HAVING FIRST(a.logTime) IS NOT NULL;

CREATE JUMPING WINDOW UserWindow 
OVER UserApiUsageStream KEEP WITHIN 1 HOUR ON logTime 
PARTITION BY userId;

CREATE CQ GetUserUsage 
INSERT INTO UserApiActivity 
SELECT a.userId, a.userName, a.userZip, a.userLat, a.userLong, 
       a.company, SUM(a.count), SUM(a.unusual), 
       CASE WHEN SUM(a.unusual) > 0 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.topObject),
       FIRST(a.logTime) 
FROM UserWindow a 
GROUP BY a.userId
LINK SOURCE EVENT;

CREATE STREAM UserAlertStream OF Global.AlertEvent;

CREATE CQ SendUserApiAlerts 
INSERT INTO UserAlertStream 
SELECT 'UserAPIAlert', ''+logTime, 'warning', 'raise', 
       'User ' + userName + ' has used api ' + api + ' ' + count + ' times for ' + topObject 
FROM UserApiUsageStream 
WHERE unusual = 1;

CREATE SUBSCRIPTION UserAlertSub USING WebAlertAdapter( ) INPUT FROM UserAlertStream;

END FLOW UserApiFlow;

END APPLICATION MultiLogApp;

Deploy application MultiLogApp;
Start application MultiLogApp;

stop ADW;
undeploy application ADW;
DROP APPLICATION ADW CASCADE;
CREATE APPLICATION ADW recovery 5 second interval;;

Create Source OracleSource Using OracleReader
(
 Username:'@ORACLE-USERNAME',
 Password:'@ORACLE-PASSWORD',
 ConnectionURL: '@ORACLE-IP@',
 Tables: '@SOURCE-TABLES@',
 FetchSize:'@FETCH-SIZE@'
) 
Output To str;


create target AzureTarget1 using AzureSQLDWHWriter (
		CoNNectionURL: '@SQLDW-URL@',
        username: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@',  
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;


create target AzureTarget2 using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        UserName: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@', 
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;


create target AzureTarget3 using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        UserName: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@', 
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

create target AzureTarget4 using AzureSQLDWHWriter (
		ConnectionURL: '@SQLDW-URL@',
        UserName: '@SQLDW-USERNAME@',
        password: '@SQLDW-PASSWORD@',
        AccountName: '@STORAGEACCOUNT@',
        AccountAccessKey: '@ACCESSKEY@',
        Tables: '@TARGET-TABLES@', 
        uploadpolicy:'@EVENT-COUNT@'
) INPUT FROM str;

END APPLICATION ADW;
deploy application ADW;
start application ADW;

Stop Oracle_IRLogWriter;
Undeploy application Oracle_IRLogWriter;
drop application Oracle_IRLogWriter cascade;

CREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;

CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( 
 
  FetchSize: 5000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=t1',
 startPosition: '%=-1',
  PollingInterval: '20sec'
  )
  OUTPUT TO data_stream;
create target AzureSQLDWHTarget using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10000,interval:300s'
) INPUT FROM data_stream;
  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

END APPLICATION Oracle_IRLogWriter;
deploy application Oracle_IRLogWriter;
start Oracle_IRLogWriter;

Stop Oracle_IRLogWriter2;
Undeploy application Oracle_IRLogWriter2;
drop application Oracle_IRLogWriter2 cascade;

CREATE APPLICATION Oracle_IRLogWriter2 recovery 5 second interval;

CREATE OR REPLACE SOURCE Oracle_IRSource2 USING IncrementalBatchReader  ( 
 
  FetchSize: 5000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=t1',
 startPosition: '%=-1',
  PollingInterval: '20sec'
  )
  OUTPUT TO data_stream2;
create target AzureSQLDWHTarget2 using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1 COLUMNMAP(dwh_id=id,dwh_col1=col1,dwh_col2=col2,dwh_t1=t1,dwh_t2=t2)',
		uploadpolicy:'eventcount:10000,interval:300s'
) INPUT FROM data_stream2;
  CREATE OR REPLACE TARGET TeraSys2 USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream2;

END APPLICATION Oracle_IRLogWriter2;
deploy application Oracle_IRLogWriter2;
start Oracle_IRLogWriter2;

stop application reconnect;
undeploy application reconnect;
drop application reconnect cascade;
CREATE APPLICATION reconnect recovery 1 second interval;

CREATE  SOURCE mssqlsource USING MssqlReader  ( 
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  ConnectionURL: '@URL@',
  Tables: '@TABLE@',
  FetchSize: 1
 ) 
OUTPUT TO sqlstream;

CREATE TARGET dbtarget USING DatabaseWriter(
  ConnectionURL:'@URL@',
  Username:'@USERNAME@',
  Password:'@PASSWORD@',
  ConnectionRetryPolicy: 'retryInterval=15s,maxRetries=2',
  BatchPolicy:'EventCount:5,Interval:30',
  CommitPolicy:'EventCount:5,Interval:30',
  Tables: '@TABLES@'
 ) INPUT FROM sqlstream;

 create Target tSysOut using Sysout(name:OrgData) input from sqlstream;
 end application reconnect;
 deploy application reconnect;
 start application reconnect;

stop application @APPNAME@app4;
undeploy application @APPNAME@app4;
alter application @APPNAME@app4;
CREATE or replace TARGET @APPNAME@app4_targetNew USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS4_Alter'
) INPUT FROM @APPNAME@sourcestream;
alter application @APPNAME@app4 recompile;
deploy application @APPNAME@app4;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;
CREATE  SOURCE @SOURCENAME@ USING OracleReader  (
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.emp',
  --OnlineCatalog: true,
  FetchSize: @FETCHSIZE@
 )
OUTPUT TO DataStream;

CREATE TARGET @TARGETNAME@ USING DatabaseWriter(
ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',
Username:'qatest',
PassWord:'w3b@ct10n',
Tables: 'qatest.emp,dbo.emp',
CHECKPOINTTABLE : 'qatest.CHKPOINT',
ConnectionRetryPolicy: '@RETRY_INTERVAL@,@MAX_RETRY@',
BatchPolicy:'EventCount:@BATCH_EVENT@,Interval:@BATCH_INTERVAL@',
CommitPolicy:'EventCount:@COMMIT_EVENT@,Interval:@COMMIT_INTERVAL@'
) INPUT FROM DataStream;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

--
-- Recovery Test 4
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS
-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS
--

STOP KStreamRecov4Tester.KStreamRecovTest4;
UNDEPLOY APPLICATION KStreamRecov4Tester.KStreamRecovTest4;
DROP APPLICATION KStreamRecov4Tester.KStreamRecovTest4 CASCADE;
DROP USER KStreamRecov4Tester;
DROP NAMESPACE KStreamRecov4Tester CASCADE;
CREATE USER KStreamRecov4Tester IDENTIFIED BY KStreamRecov4Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov4Tester;
CONNECT KStreamRecov4Tester KStreamRecov4Tester;

CREATE APPLICATION KStreamRecovTest4 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream OF CsvData;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END APPLICATION KStreamRecovTest4;

STOP OneAgentTester.CSV;
UNDEPLOY APPLICATION OneAgentTester.CSV;
DROP APPLICATION OneAgentTester.CSV CASCADE;
CONNECT ADMIN abc;
DROP USER OneAgentTester;
USE ADMIN;
DROP NAMESPACE OneAgentTester CASCADE;
drop dg AGENTS;
drop dg LocalServer;

Create Source @SOURCE_NAME@ Using MSSqlReader
(
 Username:'@READER-UNAME@',
 Password:'@READER-PASSWORD@',
 DatabaseName:'qatest',
 ConnectionURL:'@CDC-READER-URL@',
 Tables:@WATABLES@,
 ConnectionPoolSize:2,
 Compression:false,
 StartPosition:'EOF'
)
Output To @STREAM@;

--
-- Crash Recovery Test 6 with Jumping window and partitioned on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> KafkaStream -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION KStreamN2S2CR6Tester.KStreamN2S2CRTest6;
UNDEPLOY APPLICATION KStreamN2S2CR6Tester.KStreamN2S2CRTest6;
DROP APPLICATION KStreamN2S2CR6Tester.KStreamN2S2CRTest6 CASCADE;
DROP USER KStreamN2S2CR6Tester;
DROP NAMESPACE KStreamN2S2CR6Tester CASCADE;
CREATE USER KStreamN2S2CR6Tester IDENTIFIED BY KStreamN2S2CR6Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR6Tester;
CONNECT KStreamN2S2CR6Tester KStreamN2S2CR6Tester;

CREATE APPLICATION KStreamN2S2CRTest6 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest6;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest6 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest6;

CREATE FLOW DataProcessingKStreamN2S2CRTest6;

CREATE TYPE CsvDataTypeKStreamN2S2CRTest6 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvDataTypeKStreamN2S2CRTest6 PARTITION BY merchantId;

CREATE CQ CsvToDataKStreamN2S2CRTest6
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest6 CONTEXT OF CsvDataTypeKStreamN2S2CRTest6
EVENT TYPES ( CsvDataTypeKStreamN2S2CRTest6 )
@PERSIST-TYPE@

CREATE CQ DataToWactionKStreamN2S2CRTest6
INSERT INTO WactionsKStreamN2S2CRTest6
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingKStreamN2S2CRTest6;

END APPLICATION KStreamN2S2CRTest6;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	accesskeyid:'ACCESS_KEY',
	secretaccesskey:'SECRET_KEY'
)
format using JSONFormatter (
)
input from DDLCDCStream;

stop Oracle_IRLogWriter;
undeploy application Oracle_IRLogWriter;
drop application Oracle_IRLogWriter cascade;
CREATE APPLICATION Oracle_IRLogWriter;

Create Source s1 Using IncrementalBatchReader (
 FetchSize: 1,
  Username: 'striim',
  Password: 'o4l1uMpwIDQ=',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.autotest01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.autotest01=id',
  startPosition: '%=0'
 )
OUTPUT TO data_stream1;

create source s2 using IncrementalBatchReader (
FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.autotest02',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.autotest02=id',
  startPosition: '%=0'
 )
OUTPUT TO data_stream2;

create source s3 using IncrementalBatchReader (
FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025',
  Tables: 'striim.autotest03',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.autotest03=id',
  startPosition: '%=0'
 )
OUTPUT TO data_stream3;

Create Type EventType (
ID int,
NAME string,
COMPANY string,
COUNTRY string
);

CREATE STREAM insertData1  of EventType;
CREATE STREAM deleteData1 of EventType;
CREATE STREAM joinData1 of EventType;
CREATE STREAM joinData2 of EventType;
CREATE STREAM deleteData2 of EventType;
CREATE STREAM OutStream of EventType;

CREATE CQ cq1 INSERT INTO insertData1  SELECT TO_INT(data[0]),data[1],data[2],data[3] FROM data_stream1;

CREATE CQ cq2 INSERT INTO deleteData1 SELECT TO_INT(data[0]),data[1],data[2],data[3] FROM data_stream2;

CREATE CQ cq3 INSERT INTO joinData1 SELECT TO_INT(data[0]),data[1],data[2],data[3] FROM data_stream3;

CREATE JUMPING WINDOW DataWin1 OVER deleteData1 KEEP 1 ROWS;

CREATE CQ TypedEventKeyCQ6 INSERT INTO deleteData2 SELECT * from DataWin1;

CREATE JUMPING WINDOW DataWin2 OVER joinData1 KEEP 1 ROWS;

CREATE CQ TypedEventKeyCQ5 INSERT INTO joinData2 SELECT * from DataWin2;

CREATE EVENTTABLE ETABLE1 using STREAM ( NAME: 'insertData1 ' )
--DELETE using STREAM ( NAME: 'deleteData1')
QUERY (keytomap:"ID", persistPolicy: 'true') OF EventType;

CREATE CQ cq4 INSERT INTO OutStream SELECT B.ID,B.NAME,B.COMPANY,B.COUNTRY FROM joinData2 A, ETABLE1 B where A.ID=B.ID;

CREATE TARGET EventTableFW USING FileWriter
(filename:'BasicOracle_IRLogWriter_RT.log',
 rolloverpolicy: 'EventCount:1000000')
FORMAT USING DSVFormatter () INPUT FROM OutStream;

create target Target_Azure using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'STRIIM',
        password: 'W3b@ct10n',
        AccountName: 'striimqatestdonotdelete',
        accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables:'dbo.autotest01',
        uploadpolicy:'eventcount:0,interval:0s'
) INPUT FROM OutStream;

END APPLICATION Oracle_IRLogWriter;
deploy application Oracle_IRLogWriter in default;
start Oracle_IRLogWriter;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
directory:'@FEATURE-DIR@/logs/',
filename:'PosData',
rolloverpolicy:'EventCount:5000000,Interval:60s'
)
format using DSVFormatter (

)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizePosDataDefault_actual.log') input from TypedCSVStream;

end application DSV;

create source @SOURCE_NAME@
USING MariaDbXpandReader
(
 Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@ConnectionURL@',
 Tables:'@Tables@',
 FetchSize:10000,
 QueueSize:2048
)Output To @STREAM@;

UNDEPLOY APPLICATION FileToKWriterUpgrade;

DROP APPLICATION FileToKWriterUpgrade cascade;

CREATE APPLICATION FileToKWriterUpgrade;

CREATE TYPE Type1
(
 City String,
 Col2 String,
 Col3 String,
 Col4 String
);

-- Create a stream of type Type1

CREATE STREAM TypedStream OF Type1;

-- Create a source using FileReader

CREATE SOURCE KafkaCSVSource USING FILEREADER
(
 directory:'@DIRECTORY@',
 WildCard:'city*.dsv',
 positionByEOF:false,
 charset:'UTF-8'
)
PARSE USING DSVPARSER
(
 columndelimiter:',',
 ignoreemptycolumn:'Yes'
)
OUTPUT TO FileStream;

-- Read from raw stream to typed stream using CQ

CREATE CQ RawStreamCQ
INSERT INTO TypedStream
SELECT 
 data[0],
 data[1],
 data[2],
 data[3] 
FROM FileStream;

-- Load the KafkaWriter from TypedStream

CREATE TARGET KWriter USING KAFKAWRITER VERSION '0.9.0'
(
 brokerAddress:'localhost:9092',
 Topic:'@TOPIC@',
 KafkaMessageFormatVersion:v2
)
FORMAT USING DSVFORMATTER()
INPUT FROM TypedStream;


CREATE SOURCE KReader USING KAFKAREADER VERSION '0.9.0'
(
 brokerAddress:'localhost:9092',
 Topic:'@TOPIC@',
 charset : 'UTF-8',
 KafkaConfig:'retry.backoff.ms=5000',
 startOffset:0
)
PARSE USING DSVParser (
)

OUTPUT TO KafkaReaderStream;

CREATE TARGET LogKafkaReaderStream USING LOGWRITER
(
 name:KafkaLOuput,
 filename:'@LOGFILENAME@',
 flushpolicy : 'flushcount:1',
 rolloverpolicy : 'EventCount:10000,Interval:30s'
)
INPUT FROM KafkaReaderStream;


END APPLICATION FileToKWriterUpgrade;

DEPLOY APPLICATION FileToKWriterUpgrade;

START APPLICATION FileToKWriterUpgrade;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

create Source waSrcName
        Using HPNonStopSQLMXReader
  (AgentPortNo:'7012',
   AgentIPaddress:'$NSKADDR',
   PortNo:'5013',
   -- ipaddress: '$WAADDR',
   Name:'S00',
   ReturnDateTimeAs: 'String',
   Tables:'sncat.snsch.DBWriterTest1')
Output To waAppName_Stream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from waAppName_Stream;

END APPLICATION DataGenSampleApp;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;


CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using OracleReader
(
  Compression:true,
  StartTimestamp:'null',
  CommittedTransactions:true,
  FilterTransactionBoundaries:true,
  Password_encrypted:'false',
  SendBeforeImage:true,
  XstreamTimeOut:600,
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE',
  adapterName:'OracleReader',
  Password:'qatest',
  DictionaryMode:'OfflineCatalog',
  FilterTransactionState:true,
  connectionRetryPolicy:'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType:'LogMiner',
  FetchSize: 1,
  Username:'qatest',
  OutboundServerProcessName:'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic:true,
  CDDLAction:'Quiesce_Cascade',
  CDDLCapture:'true'
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;

stop bankApp;
undeploy application bankApp;

alter application bankApp;
CREATE OR REPLACE TYPE wsData  ( bankID java.lang.Integer KEY, 
nameOfBank java.lang.String
);

CREATE OR REPLACE WACTIONSTORE oneWS  CONTEXT OF wsData
EVENT TYPES(wsData )
@PERSIST-TYPE@;



alter application bankApp recompile;
deploy application bankApp;
start application bankApp;

stop application GGTrailReaderApp;
undeploy application GGTrailReaderApp;
drop application GGTrailReaderApp cascade;

create application GGTrailReaderApp recovery 5 second interval;

create source GGTrailSource using GGTrailReader (
tRaildIrectory:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario15',
tRAilfilepattern:'15*',
positionByEOF:false,
FilterTransactionBoundaries: true,
DefinitionFile:'/Users/karthikmurugan/Bugs/tested/OGG/cDDL/Scenario15/Scn15_beforeddl.def',
captureCDdl: true,
CDDLAction:'Process',
--CDDLAction:'ignore',
--CDDLAction:'quiesce',
--cddlAction:'Error',
TrailByTeOrder:'LittleEndian',
recoveryInterval: 5,
TABLES:'QATEST.INT2;QATEST.INT1'
)
OUTPUT TO GGTrailStream;

create Target t2 using SysOut(name:Foo2) input from GGTrailStream;

CREATE TARGET WriteCDCOracle1 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost/ORCL',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:1,Interval:1',
CommitPolicy:'Eventcount:1,Interval:1',
Checkpointtable:'RGRN_CHKPOINT',
Tables:'QATEST.GGDDL5,QATEST.GGDDL5_TGT'
) INPUT FROM GGTrailStream;


end application GGTrailReaderApp;

deploy application GGTrailReaderApp;
start application GGTrailReaderApp;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE OR REPLACE APPLICATION @APPNAME@ recovery 5 second interval;

CREATE FLOW @APPNAME@_Agent_flow;

CREATE OR REPLACE SOURCE @SourceName@1 USING Global.MSJet (
  Tables: 'dbo.compsrc',
    username: 'qatest',
    DatabaseName: 'qatest',
    FetchTransactionMetadata: true,
    filterTransactionBoundaries: true,
    compression: false,
    ConnectionURL: '10.211.55.3:1433',
    CommittedTransactions: true,
    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
    SendBeforeImage: true,
    password: 'w3b@ct10n' )
OUTPUT TO @SRCINPUTSTREAM@;

END FLOW @APPNAME@_Agent_flow;

CREATE FLOW @APPNAME@_Agent_flow1;

CREATE OR REPLACE SOURCE @SourceName@2 USING Global.MSJet (
  Tables: 'dbo.compsrc',
    username: 'qatest',
    DatabaseName: 'qatest',
    FetchTransactionMetadata: true,
    filterTransactionBoundaries: true,
    compression: false,
    ConnectionURL: '10.211.55.3:1433',
    CommittedTransactions: true,
    connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
    SendBeforeImage: true,
    password: 'w3b@ct10n' )
OUTPUT TO @SRCINPUTSTREAM@1;

END FLOW @APPNAME@_Agent_flow1;

CREATE FLOW @APPNAME@_server_flow;

CREATE OR REPLACE TARGET @targetName@1 USING Global.DatabaseWriter
(
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  BatchPolicy:'EventCount:10,Interval:60',
  CommitPolicy:'EventCount:10,Interval:60',
  ParallelThreads:'',
  CheckPointTable:'CHKPOINT',
  Password_encrypted:'false',
  Tables:'qatest.MSJEtsrc1,qatest.MSJEtar1;qatest.MSJEtsrc2,qatest.MSJEtar2;',
  CDDLAction:'Process',
  Password:'w3b@ct10n',
  StatementCacheSize:'50',
  ConnectionURL:'jdbc:sqlserver://10.211.55.3:1433;databaseName=qatest',
  DatabaseProviderType:'Default',
  Username:'qatest',
  PreserveSourceTransactionBoundary:'false',
  adapterName:'DatabaseWriter'
)
INPUT FROM @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@2 USING Global.DatabaseWriter
(
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  BatchPolicy:'EventCount:10,Interval:60',
  CommitPolicy:'EventCount:10,Interval:60',
  ParallelThreads:'',
  CheckPointTable:'CHKPOINT',
  Password_encrypted:'false',
  Tables:'qatest.MSJEtsrc1,qatest.MSJEtar1;qatest.MSJEtsrc2,qatest.MSJEtar2;',
  CDDLAction:'Process',
  Password:'w3b@ct10n',
  StatementCacheSize:'50',
  ConnectionURL:'jdbc:sqlserver://10.211.55.3:1433;databaseName=qatest',
  DatabaseProviderType:'Default',
  Username:'qatest',
  PreserveSourceTransactionBoundary:'false',
  adapterName:'DatabaseWriter'
)
INPUT FROM @SRCINPUTSTREAM@1;

CREATE TARGET @targetsys@ USING Global.SysOut (
  name: '@targetsys@' )
INPUT FROM @SRCINPUTSTREAM@;

END FLOW @APPNAME@_server_flow;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@ with @APPNAME@_Agent_flow in AGENTS, @APPNAME@_Agent_flow1 in AGENTS ,@APPNAME@_server_flow on any in default;
START APPLICATION @APPNAME@;

--
-- Crash Recovery Test 1 on Four node all server cluster 
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP N4S4CR1Tester.N4S4CRTest1;
UNDEPLOY APPLICATION N4S4CR1Tester.N4S4CRTest1;
DROP APPLICATION N4S4CR1Tester.N4S4CRTest1 CASCADE;
CREATE APPLICATION N4S4CRTest1 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest1;

CREATE SOURCE CsvSourceN4S4CRTest1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest1;

CREATE FLOW DataProcessingN4S4CRTest1;

CREATE TYPE WactionTypeN4S4CRTest1 (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE WactionsN4S4CRTest1 CONTEXT OF WactionTypeN4S4CRTest1
EVENT TYPES ( WactionTypeN4S4CRTest1 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN4S4CRTest1
INSERT INTO WactionsN4S4CRTest1
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END FLOW DataProcessingN4S4CRTest1;

END APPLICATION N4S4CRTest1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@;

-----------------------------------
stop application SourceAgentApp1;
undeploy application SourceAgentApp1;

stop application SourceAgentApp2;
undeploy application SourceAgentApp2;

stop application TargetServerApp;
undeploy application TargetServerApp;

drop application SourceAgentApp1 cascade;
drop application SourceAgentApp2 cascade;
drop application TargetServerApp cascade;


CREATE APPLICATION SourceAgentApp1;
create flow flow1;
create source CSVSource1 using FileReader (
directory: '@TEST-DATA-PATH@/tmp',
WildCard:'mybanks*',
positionByEOF: true,
charset:'UTF-8'
) parse using DSVParser (header:'no')
OUTPUT TO CsvStream;
end flow flow1;

--CREATE TARGET T USING Sysout(name:'sysout1') INPUT FROM CsvStream;

END APPLICATION SourceAgentApp1;

DEPLOY APPLICATION SourceAgentApp1 with flow1 in AGENTS;


CREATE APPLICATION SourceAgentApp2;

create flow flow2;
create source CSVSource2 using FileReader (
directory: '@TEST-DATA-PATH@/tmp',
WildCard:'mybanks*',
positionByEOF: true,
charset:'UTF-8'
) parse using DSVParser (header:'no')
OUTPUT TO CsvStream;
end flow flow2;

--CREATE TARGET T USING Sysout(name:'sysout2') INPUT FROM CsvStream;

END APPLICATION SourceAgentApp2;

DEPLOY APPLICATION SourceAgentApp2 with flow2 in AGENTS;


-- One app consuming from stream from 2 sources running in agent
CREATE APPLICATION TargetServerApp;
create flow flow3;

CREATE TARGET T5 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')
FORMAT USING JSONFormatter ()
INPUT FROM CsvStream;
end flow flow3;

END APPLICATION TargetServerApp;
deploy application TargetServerApp with flow3 in default;

STOP APPLICATION EH;
UNDEPLOY APPLICATION EH;
DROP APPLICATION EH CASCADE;
CREATE APPLICATION EH @Recovery@;
CREATE SOURCE s USING FileReader (
	directory:'Product/IntegrationTests/TestData/',
    WildCard:'posdata.csv',
	positionByEOF:false
	)
PARSE USING DSVParser (
	header:yes
)OUTPUT TO ss;

CREATE TYPE userType(
Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String
);

CREATE STREAM userDefinedTypedStream OF userType partition by Col1 ;

CREATE CQ cq1
INSERT INTO userDefinedTypedStream
SELECT data[0],data[1],data[2], data[3],data[4]
FROM ss;

create Target t1 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	--Partitionkey:'@metadata(RecordOffset)',
	ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
jsonMemberDelimiter: '\n',
jsonobjectdelimiter: '\n',
EventsAsArrayOfJsonObjects: 'true')
input from ss;

create Target t2 using AzureEventHubWriter (
	EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	--Partitionkey:'Col1',
	ParallelThreads:'2',
	ConsumerGroup:'reader',
	E1P:'true',
	OperationTimeout:'500000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
jsonMemberDelimiter: '\n',
jsonobjectdelimiter: '\n',
EventsAsArrayOfJsonObjects: 'true')
input from userDefinedTypedStream;

END APPLICATION EH;
DEPLOY APPLICATION EH;
start application EH;

STOP APPLICATION ER;
UNDEPLOY APPLICATION ER;
DROP APPLICATION ER CASCADE;
CREATE APPLICATION ER;
CREATE SOURCE ER_S1 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING jsonParser (
)OUTPUT TO ER_SS1;


CREATE SOURCE ER_S2 USING AzureEventHubReader (
    EventHubNamespace:'qatestnov9',
	EventHubName:'test_2_partitions',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'Default',
	startSeqNo:'0'	
	)
PARSE USING jsonParser (
)OUTPUT TO ER_SS2;

create Type CustType1 
(writerdata com.fasterxml.jackson.databind.JsonNode
--TopicName java.lang.String,
--PartitionID java.lang.String
);

Create Stream datastream1 of CustType1;

create Type CustType2
(Col1 String,
Col2 String,
Col3 String,
Col4 String,
Col5 String
--TopicName String,
--PartitionID String
);

Create Stream datastream2 of CustType2;

CREATE CQ CustCQ1
INSERT INTO datastream1
SELECT data.data
--metadata.get("TopicName").toString() AS TopicName,
--metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS1;

CREATE CQ CustCQ2
INSERT INTO datastream2
SELECT 
data.get('Col1').textValue(),
data.get('Col2').textValue(),
data.get('Col3').textValue(),
data.get('Col4').textValue(),
data.get('Col5').textValue()
--metadata.get("TopicName").toString() AS TopicName,
--metadata.get("PartitionID").toString() AS PartitionID
FROM ER_SS2;

create Target ER_t1 using FileWriter (
filename:'FT1_5L_JSON_RT',
directory:'FEATURE-DIR/logs/',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from datastream1;

create Target ER_t2 using FileWriter (
filename:'FT2_5L_JSON_RT',
directory:'FEATURE-DIR/logs/',
flushpolicy:'eventcount:1',
rolloverpolicy:'eventcount:5000000'	
)
format using dsvFormatter()
input from datastream2;
end application ER;
deploy application ER;

STOP APPLICATION MysqltoBQ;
UNDEPLOY APPLICATION MysqltoBQ;
DROP APPLICATION MysqltoBQ CASCADE;
CREATE APPLICATION MysqltoBQ recovery 5 SECOND Interval;
CREATE OR REPLACE SOURCE MysqltoBQ_Source USING MySQLReader 
(
  Username:'root',
  Password:'w@ct10n',
  connectionURL:'jdbc:mysql://localhost:3306/waction',
  Tables:'waction.sourceTable',
  sendBeforeImage:'true',
  FilterTransactionBoundaries:'true',
  ExcludedTables:'waction.CHKPOINT',
  useSSL:true
) 
OUTPUT TO MysqltoBQ_Stream;

CREATE OR REPLACE TARGET MysqltoBQ_Target USING BigQueryWriter  (
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  projectId:'striimqa-214712',
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:5,Interval:120',
  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', 
  AllowQuotedNewLines: 'false', 
  adapterName: 'BigQueryWriter', 
  optimizedMerge: 'true', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', 
  StandardSQL: 'true', 
  QuoteCharacter: '\"', 
  Tables: 'waction.sourceTable,.oratobqtgt',
  Mode: 'MERGE',
  StandardSQL: 'true',
  QuoteCharacter: '\"'
  ) INPUT FROM MysqltoBQ_Stream;

CREATE OR REPLACE TARGET MysqltoBQ_SysOut USING Global.SysOut (name: 'wa') INPUT FROM MysqltoBQ_Stream;

END APPLICATION MysqltoBQ;
DEPLOY APPLICATION MysqltoBQ;
START MysqltoBQ;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;

 CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01;striim.test02;striim.test03',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',
 startPosition: 'striim.test01=1;striim.test02=-1;%=0',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream ;

  CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

create target AzureSQLDWHTarget1 using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'STRIIM.AUTO_LENGTHY_TABLE_NAME_FROM_INCREMENTAL_READER_TO_AZURE_STRUCTURED_QUERY_LANGUAGE_DATABASE_WAREHOUSE_WITH_128_CHARACTER_MAXIMUM,DBO.AUTOTEST_LENGTHY1 COLUMNMAP(Field1=AUTO_LENGTHY_COLUMNNAME_FROM_INCREMENTAL_READER_TO_AZURE_STRUCTURED_QUERY_LANGUAGE_DATABASE_WAREHOUSE_WITH_128_CHARACTER_MAXIMUM)',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream;

create target AzureSQLDWHTarget2 using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        Accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'STRIIM.AUTO_LENGTHY_TABLE_NAME_FROM_INCREMENTAL_READER_TO_AZURE_STRUCTURED_QUERY_LANGUAGE_DATABASE_WAREHOUSE_WITH_128_CHARACTER_MAXIMUM,DBO.AUTOTEST_LENGTHY2 COLUMNMAP(Field1=AUTO_LENGTHY_COLUMNNAME_FROM_INCREMENTAL_READER_TO_AZURE_STRUCTURED_QUERY_LANGUAGE_DATABASE_WAREHOUSE_WITH_128_CHARACTER_MAXIMUM,Field2=Field1026,Field3=Field1027,Field4=Field1028,Field5=Field1029,Field6=Field1030,Field7=Field1031,Field8=Field1032,Field9=Field1033,Field10=Field1034,Field11=Field1035,Field12=Field1036,Field13=Field1037,Field14=Field1038,Field15=Field1039,Field16=Field1040,Field17=Field1041,Field18=Field1042,Field19=Field1043,Field20=Field1044,Field21=Field1045,Field22=Field1046,Field23=Field1047,Field24=Field1048,Field25=Field1049,Field26=Field1050,Field27=Field1051,Field28=Field1052,Field29=Field1053,Field30=Field1054,Field31=Field1055,Field32=Field1056,Field33=Field1057,Field34=Field1058,Field35=Field1059,Field36=Field1060,Field37=Field1061,Field38=Field1062,Field39=Field1063,Field40=Field1064,Field41=Field1065,Field42=Field1066,Field43=Field1067,Field44=Field1068,Field45=Field1069,Field46=Field1070,Field47=Field1071,Field48=Field1072,Field49=Field1073,Field50=Field1074,Field51=Field1075,Field52=Field1076,Field53=Field1077,Field54=Field1078,Field55=Field1079,Field56=Field1080,Field57=Field1081,Field58=Field1082,Field59=Field1083,Field60=Field1084,Field61=Field1085,Field62=Field1086,Field63=Field1087,Field64=Field1088,Field65=Field1089,Field66=Field1090,Field67=Field1091,Field68=Field1092,Field69=Field1093,Field70=Field1094,Field71=Field1095,Field72=Field1096,Field73=Field1097,Field74=Field1098,Field75=Field1099,Field76=Field1100,Field77=Field1101,Field78=Field1102,Field79=Field1103,Field80=Field1104,Field81=Field1105,Field82=Field1106,Field83=Field1107,Field84=Field1108,Field85=Field1109,Field86=Field1110,Field87=Field1111,Field88=Field1112,Field89=Field1113,Field90=Field1114,Field91=Field1115,Field92=Field1116,Field93=Field1117,Field94=Field1118,Field95=Field1119,Field96=Field1120,Field97=Field1121,Field98=Field1122,Field99=Field1123,Field100=Field1124,Field101=Field1125,Field102=Field1126,Field103=Field1127,Field104=Field1128,Field105=Field1129,Field106=Field1130,Field107=Field1131,Field108=Field1132,Field109=Field1133,Field110=Field1134,Field111=Field1135,Field112=Field1136,Field113=Field1137,Field114=Field1138,Field115=Field1139,Field116=Field1140,Field117=Field1141,Field118=Field1142,Field119=Field1143,Field120=Field1144,Field121=Field1145,Field122=Field1146,Field123=Field1147,Field124=Field1148,Field125=Field1149,Field126=Field1150,Field127=Field1151,Field128=Field1152,Field129=Field1153,Field130=Field1154,Field131=Field1155,Field132=Field1156,Field133=Field1157,Field134=Field1158,Field135=Field1159,Field136=Field1160,Field137=Field1161,Field138=Field1162,Field139=Field1163,Field140=Field1164,Field141=Field1165,Field142=Field1166,Field143=Field1167,Field144=Field1168,Field145=Field1169,Field146=Field1170,Field147=Field1171,Field148=Field1172,Field149=Field1173,Field150=Field1174,Field151=Field1175,Field152=Field1176,Field153=Field1177,Field154=Field1178,Field155=Field1179,Field156=Field1180,Field157=Field1181,Field158=Field1182,Field159=Field1183,Field160=Field1184,Field161=Field1185,Field162=Field1186,Field163=Field1187,Field164=Field1188,Field165=Field1189,Field166=Field1190,Field167=Field1191,Field168=Field1192,Field169=Field1193,Field170=Field1194,Field171=Field1195,Field172=Field1196,Field173=Field1197,Field174=Field1198,Field175=Field1199,Field176=Field1200,Field177=Field1201,Field178=Field1202,Field179=Field1203,Field180=Field1204,Field181=Field1205,Field182=Field1206,Field183=Field1207,Field184=Field1208,Field185=Field1209,Field186=Field1210,Field187=Field1211,Field188=Field1212,Field189=Field1213,Field190=Field1214,Field191=Field1215,Field192=Field1216,Field193=Field1217,Field194=Field1218,Field195=Field1219,Field196=Field1220,Field197=Field1221,Field198=Field1222,Field199=Field1223,Field200=Field1224,Field201=Field1225,Field202=Field1226,Field203=Field1227,Field204=Field1228,Field205=Field1229,Field206=Field1230,Field207=Field1231,Field208=Field1232,Field209=Field1233,Field210=Field1234,Field211=Field1235,Field212=Field1236,Field213=Field1237,Field214=Field1238,Field215=Field1239,Field216=Field1240,Field217=Field1241,Field218=Field1242,Field219=Field1243,Field220=Field1244,Field221=Field1245,Field222=Field1246,Field223=Field1247,Field224=Field1248,Field225=Field1249,Field226=Field1250,Field227=Field1251,Field228=Field1252,Field229=Field1253,Field230=Field1254,Field231=Field1255,Field232=Field1256,Field233=Field1257,Field234=Field1258,Field235=Field1259,Field236=Field1260,Field237=Field1261,Field238=Field1262,Field239=Field1263,Field240=Field1264,Field241=Field1265,Field242=Field1266,Field243=Field1267,Field244=Field1268,Field245=Field1269,Field246=Field1270,Field247=Field1271,Field248=Field1272,Field249=Field1273,Field250=Field1274,Field251=Field1275,Field252=Field1276,Field253=Field1277,Field254=Field1278,Field255=Field1279,Field256=Field1280,Field257=Field1281,Field258=Field1282,Field259=Field1283,Field260=Field1284,Field261=Field1285,Field262=Field1286,Field263=Field1287,Field264=Field1288,Field265=Field1289,Field266=Field1290,Field267=Field1291,Field268=Field1292,Field269=Field1293,Field270=Field1294,Field271=Field1295,Field272=Field1296,Field273=Field1297,Field274=Field1298,Field275=Field1299,Field276=Field1300,Field277=Field1301,Field278=Field1302,Field279=Field1303,Field280=Field1304,Field281=Field1305,Field282=Field1306,Field283=Field1307,Field284=Field1308,Field285=Field1309,Field286=Field1310,Field287=Field1311,Field288=Field1312,Field289=Field1313,Field290=Field1314,Field291=Field1315,Field292=Field1316,Field293=Field1317,Field294=Field1318,Field295=Field1319,Field296=Field1320,Field297=Field1321,Field298=Field1322,Field299=Field1323,Field300=Field1324,Field301=Field1325,Field302=Field1326,Field303=Field1327,Field304=Field1328,Field305=Field1329,Field306=Field1330,Field307=Field1331,Field308=Field1332,Field309=Field1333,Field310=Field1334,Field311=Field1335,Field312=Field1336,Field313=Field1337,Field314=Field1338,Field315=Field1339,Field316=Field1340,Field317=Field1341,Field318=Field1342,Field319=Field1343,Field320=Field1344,Field321=Field1345,Field322=Field1346,Field323=Field1347,Field324=Field1348,Field325=Field1349,Field326=Field1350,Field327=Field1351,Field328=Field1352,Field329=Field1353,Field330=Field1354,Field331=Field1355,Field332=Field1356,Field333=Field1357,Field334=Field1358,Field335=Field1359,Field336=Field1360,Field337=Field1361,Field338=Field1362,Field339=Field1363,Field340=Field1364,Field341=Field1365,Field342=Field1366,Field343=Field1367,Field344=Field1368,Field345=Field1369,Field346=Field1370,Field347=Field1371,Field348=Field1372,Field349=Field1373,Field350=Field1374,Field351=Field1375,Field352=Field1376,Field353=Field1377,Field354=Field1378,Field355=Field1379,Field356=Field1380,Field357=Field1381,Field358=Field1382,Field359=Field1383,Field360=Field1384,Field361=Field1385,Field362=Field1386,Field363=Field1387,Field364=Field1388,Field365=Field1389,Field366=Field1390,Field367=Field1391,Field368=Field1392,Field369=Field1393,Field370=Field1394,Field371=Field1395,Field372=Field1396,Field373=Field1397,Field374=Field1398,Field375=Field1399,Field376=Field1400,Field377=Field1401,Field378=Field1402,Field379=Field1403,Field380=Field1404,Field381=Field1405,Field382=Field1406,Field383=Field1407,Field384=Field1408,Field385=Field1409,Field386=Field1410,Field387=Field1411,Field388=Field1412,Field389=Field1413,Field390=Field1414,Field391=Field1415,Field392=Field1416,Field393=Field1417,Field394=Field1418,Field395=Field1419,Field396=Field1420,Field397=Field1421,Field398=Field1422,Field399=Field1423,Field400=Field1424,Field401=Field1425,Field402=Field1426,Field403=Field1427,Field404=Field1428,Field405=Field1429,Field406=Field1430,Field407=Field1431,Field408=Field1432,Field409=Field1433,Field410=Field1434,Field411=Field1435,Field412=Field1436,Field413=Field1437,Field414=Field1438,Field415=Field1439,Field416=Field1440,Field417=Field1441,Field418=Field1442,Field419=Field1443,Field420=Field1444,Field421=Field1445,Field422=Field1446,Field423=Field1447,Field424=Field1448,Field425=Field1449,Field426=Field1450,Field427=Field1451,Field428=Field1452,Field429=Field1453,Field430=Field1454,Field431=Field1455,Field432=Field1456,Field433=Field1457,Field434=Field1458,Field435=Field1459,Field436=Field1460,Field437=Field1461,Field438=Field1462,Field439=Field1463,Field440=Field1464,Field441=Field1465,Field442=Field1466,Field443=Field1467,Field444=Field1468,Field445=Field1469,Field446=Field1470,Field447=Field1471,Field448=Field1472,Field449=Field1473,Field450=Field1474,Field451=Field1475,Field452=Field1476,Field453=Field1477,Field454=Field1478,Field455=Field1479,Field456=Field1480,Field457=Field1481,Field458=Field1482,Field459=Field1483,Field460=Field1484,Field461=Field1485,Field462=Field1486,Field463=Field1487,Field464=Field1488,Field465=Field1489,Field466=Field1490,Field467=Field1491,Field468=Field1492,Field469=Field1493,Field470=Field1494,Field471=Field1495,Field472=Field1496,Field473=Field1497,Field474=Field1498,Field475=Field1499,Field476=Field1500,Field477=Field1501,Field478=Field1502,Field479=Field1503,Field480=Field1504,Field481=Field1505,Field482=Field1506,Field483=Field1507,Field484=Field1508,Field485=Field1509,Field486=Field1510,Field487=Field1511,Field488=Field1512,Field489=Field1513,Field490=Field1514,Field491=Field1515,Field492=Field1516,Field493=Field1517,Field494=Field1518,Field495=Field1519,Field496=Field1520,Field497=Field1521,Field498=Field1522,Field499=Field1523,Field500=Field1524,Field501=Field1525,Field502=Field1526,Field503=Field1527,Field504=Field1528,Field505=Field1529,Field506=Field1530,Field507=Field1531,Field508=Field1532,Field509=Field1533,Field510=Field1534,Field511=Field1535,Field512=Field1536,Field513=Field1537,Field514=Field1538,Field515=Field1539,Field516=Field1540,Field517=Field1541,Field518=Field1542,Field519=Field1543,Field520=Field1544,Field521=Field1545,Field522=Field1546,Field523=Field1547,Field524=Field1548,Field525=Field1549,Field526=Field1550,Field527=Field1551,Field528=Field1552,Field529=Field1553,Field530=Field1554,Field531=Field1555,Field532=Field1556,Field533=Field1557,Field534=Field1558,Field535=Field1559,Field536=Field1560,Field537=Field1561,Field538=Field1562,Field539=Field1563,Field540=Field1564,Field541=Field1565,Field542=Field1566,Field543=Field1567,Field544=Field1568,Field545=Field1569,Field546=Field1570,Field547=Field1571,Field548=Field1572,Field549=Field1573,Field550=Field1574,Field551=Field1575,Field552=Field1576,Field553=Field1577,Field554=Field1578,Field555=Field1579,Field556=Field1580,Field557=Field1581,Field558=Field1582,Field559=Field1583,Field560=Field1584,Field561=Field1585,Field562=Field1586,Field563=Field1587,Field564=Field1588,Field565=Field1589,Field566=Field1590,Field567=Field1591,Field568=Field1592,Field569=Field1593,Field570=Field1594,Field571=Field1595,Field572=Field1596,Field573=Field1597,Field574=Field1598,Field575=Field1599,Field576=Field1600,Field577=Field1601,Field578=Field1602,Field579=Field1603,Field580=Field1604,Field581=Field1605,Field582=Field1606,Field583=Field1607,Field584=Field1608,Field585=Field1609,Field586=Field1610,Field587=Field1611,Field588=Field1612,Field589=Field1613,Field590=Field1614,Field591=Field1615,Field592=Field1616,Field593=Field1617,Field594=Field1618,Field595=Field1619,Field596=Field1620,Field597=Field1621,Field598=Field1622,Field599=Field1623,Field600=Field1624,Field601=Field1625,Field602=Field1626,Field603=Field1627,Field604=Field1628,Field605=Field1629,Field606=Field1630,Field607=Field1631,Field608=Field1632,Field609=Field1633,Field610=Field1634,Field611=Field1635,Field612=Field1636,Field613=Field1637,Field614=Field1638,Field615=Field1639,Field616=Field1640,Field617=Field1641,Field618=Field1642,Field619=Field1643,Field620=Field1644,Field621=Field1645,Field622=Field1646,Field623=Field1647,Field624=Field1648,Field625=Field1649,Field626=Field1650,Field627=Field1651,Field628=Field1652,Field629=Field1653,Field630=Field1654,Field631=Field1655,Field632=Field1656,Field633=Field1657,Field634=Field1658,Field635=Field1659,Field636=Field1660,Field637=Field1661,Field638=Field1662,Field639=Field1663,Field640=Field1664,Field641=Field1665,Field642=Field1666,Field643=Field1667,Field644=Field1668,Field645=Field1669,Field646=Field1670,Field647=Field1671,Field648=Field1672,Field649=Field1673,Field650=Field1674,Field651=Field1675,Field652=Field1676,Field653=Field1677,Field654=Field1678,Field655=Field1679,Field656=Field1680,Field657=Field1681,Field658=Field1682,Field659=Field1683,Field660=Field1684,Field661=Field1685,Field662=Field1686,Field663=Field1687,Field664=Field1688,Field665=Field1689,Field666=Field1690,Field667=Field1691,Field668=Field1692,Field669=Field1693,Field670=Field1694,Field671=Field1695,Field672=Field1696,Field673=Field1697,Field674=Field1698,Field675=Field1699,Field676=Field1700,Field677=Field1701,Field678=Field1702,Field679=Field1703,Field680=Field1704,Field681=Field1705,Field682=Field1706,Field683=Field1707,Field684=Field1708,Field685=Field1709,Field686=Field1710,Field687=Field1711,Field688=Field1712,Field689=Field1713,Field690=Field1714,Field691=Field1715,Field692=Field1716,Field693=Field1717,Field694=Field1718,Field695=Field1719,Field696=Field1720,Field697=Field1721,Field698=Field1722,Field699=Field1723,Field700=Field1724,Field701=Field1725,Field702=Field1726,Field703=Field1727,Field704=Field1728,Field705=Field1729,Field706=Field1730,Field707=Field1731,Field708=Field1732,Field709=Field1733,Field710=Field1734,Field711=Field1735,Field712=Field1736,Field713=Field1737,Field714=Field1738,Field715=Field1739,Field716=Field1740,Field717=Field1741,Field718=Field1742,Field719=Field1743,Field720=Field1744,Field721=Field1745,Field722=Field1746,Field723=Field1747,Field724=Field1748,Field725=Field1749,Field726=Field1750,Field727=Field1751,Field728=Field1752,Field729=Field1753,Field730=Field1754,Field731=Field1755,Field732=Field1756,Field733=Field1757,Field734=Field1758,Field735=Field1759,Field736=Field1760,Field737=Field1761,Field738=Field1762,Field739=Field1763,Field740=Field1764,Field741=Field1765,Field742=Field1766,Field743=Field1767,Field744=Field1768,Field745=Field1769,Field746=Field1770,Field747=Field1771,Field748=Field1772,Field749=Field1773,Field750=Field1774,Field751=Field1775,Field752=Field1776,Field753=Field1777,Field754=Field1778,Field755=Field1779,Field756=Field1780,Field757=Field1781,Field758=Field1782,Field759=Field1783,Field760=Field1784,Field761=Field1785,Field762=Field1786,Field763=Field1787,Field764=Field1788,Field765=Field1789,Field766=Field1790,Field767=Field1791,Field768=Field1792,Field769=Field1793,Field770=Field1794,Field771=Field1795,Field772=Field1796,Field773=Field1797,Field774=Field1798,Field775=Field1799,Field776=Field1800,Field777=Field1801,Field778=Field1802,Field779=Field1803,Field780=Field1804,Field781=Field1805,Field782=Field1806,Field783=Field1807,Field784=Field1808,Field785=Field1809,Field786=Field1810,Field787=Field1811,Field788=Field1812,Field789=Field1813,Field790=Field1814,Field791=Field1815,Field792=Field1816,Field793=Field1817,Field794=Field1818,Field795=Field1819,Field796=Field1820,Field797=Field1821,Field798=Field1822,Field799=Field1823,Field800=Field1824,Field801=Field1825,Field802=Field1826,Field803=Field1827,Field804=Field1828,Field805=Field1829,Field806=Field1830,Field807=Field1831,Field808=Field1832,Field809=Field1833,Field810=Field1834,Field811=Field1835,Field812=Field1836,Field813=Field1837,Field814=Field1838,Field815=Field1839,Field816=Field1840,Field817=Field1841,Field818=Field1842,Field819=Field1843,Field820=Field1844,Field821=Field1845,Field822=Field1846,Field823=Field1847,Field824=Field1848,Field825=Field1849,Field826=Field1850,Field827=Field1851,Field828=Field1852,Field829=Field1853,Field830=Field1854,Field831=Field1855,Field832=Field1856,Field833=Field1857,Field834=Field1858,Field835=Field1859,Field836=Field1860,Field837=Field1861,Field838=Field1862,Field839=Field1863,Field840=Field1864,Field841=Field1865,Field842=Field1866,Field843=Field1867,Field844=Field1868,Field845=Field1869,Field846=Field1870,Field847=Field1871,Field848=Field1872,Field849=Field1873,Field850=Field1874,Field851=Field1875,Field852=Field1876,Field853=Field1877,Field854=Field1878,Field855=Field1879,Field856=Field1880,Field857=Field1881,Field858=Field1882,Field859=Field1883,Field860=Field1884,Field861=Field1885,Field862=Field1886,Field863=Field1887,Field864=Field1888,Field865=Field1889,Field866=Field1890,Field867=Field1891,Field868=Field1892,Field869=Field1893,Field870=Field1894,Field871=Field1895,Field872=Field1896,Field873=Field1897,Field874=Field1898,Field875=Field1899,Field876=Field1900,Field877=Field1901,Field878=Field1902,Field879=Field1903,Field880=Field1904,Field881=Field1905,Field882=Field1906,Field883=Field1907,Field884=Field1908,Field885=Field1909,Field886=Field1910,Field887=Field1911,Field888=Field1912,Field889=Field1913,Field890=Field1914,Field891=Field1915,Field892=Field1916,Field893=Field1917,Field894=Field1918,Field895=Field1919,Field896=Field1920,Field897=Field1921,Field898=Field1922,Field899=Field1923,Field900=Field1924,Field901=Field1925,Field902=Field1926,Field903=Field1927,Field904=Field1928,Field905=Field1929,Field906=Field1930,Field907=Field1931,Field908=Field1932,Field909=Field1933,Field910=Field1934,Field911=Field1935,Field912=Field1936,Field913=Field1937,Field914=Field1938,Field915=Field1939,Field916=Field1940,Field917=Field1941,Field918=Field1942,Field919=Field1943,Field920=Field1944,Field921=Field1945,Field922=Field1946,Field923=Field1947,Field924=Field1948,Field925=Field1949,Field926=Field1950,Field927=Field1951,Field928=Field1952,Field929=Field1953,Field930=Field1954,Field931=Field1955,Field932=Field1956,Field933=Field1957,Field934=Field1958,Field935=Field1959,Field936=Field1960,Field937=Field1961,Field938=Field1962,Field939=Field1963,Field940=Field1964,Field941=Field1965,Field942=Field1966,Field943=Field1967,Field944=Field1968,Field945=Field1969,Field946=Field1970,Field947=Field1971,Field948=Field1972,Field949=Field1973,Field950=Field1974,Field951=Field1975,Field952=Field1976,Field953=Field1977,Field954=Field1978,Field955=Field1979,Field956=Field1980,Field957=Field1981,Field958=Field1982,Field959=Field1983,Field960=Field1984,Field961=Field1985,Field962=Field1986,Field963=Field1987,Field964=Field1988,Field965=Field1989,Field966=Field1990,Field967=Field1991,Field968=Field1992,Field969=Field1993,Field970=Field1994,Field971=Field1995,Field972=Field1996,Field973=Field1997,Field974=Field1998,Field975=Field1999,Field976=Field2000,Field977=Field2001,Field978=Field2002,Field979=Field2003,Field980=Field2004,Field981=Field2005,Field982=Field2006,Field983=Field2007,Field984=Field2008,Field985=Field2009,Field986=Field2010,Field987=Field2011,Field988=Field2012,Field989=Field2013,Field990=Field2014,Field991=Field2015,Field992=Field2016,Field993=Field2017,Field994=Field2018,Field995=Field2019,Field996=Field2020,Field997=Field2021,Field998=Field2022,Field999=Field2023,Field1000=Field2024,Field1001=Field2025,Field1002=Field2026,Field1003=Field2027,Field1004=Field2028,Field1005=Field2029,Field1006=Field2030,Field1007=Field2031,Field1008=Field2032,Field1009=Field2033,Field1010=Field2034,Field1011=Field2035,Field1012=Field2036,Field1013=Field2037,Field1014=Field2038,Field1015=Field2039,Field1016=Field2040,Field1017=Field2041,Field1018=Field2042,Field1019=Field2043,Field1020=Field2044,Field1021=Field2045,Field1022=Field2046)',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream;


CREATE TARGET log USING LOGWRITER
(
 name:log,
 filename:'LOGFILENAME'
)
INPUT FROM data_stream;


END APPLICATION IR;
deploy application IR;
start application IR;

CREATE APPLICATION @AppName@;

CREATE OR REPLACE SOURCE Filereader_Src USING FileReader  (
   WildCard: 'posdata100.csv',
  directory: '@SrcDir@',
  positionbyeof: false)
 PARSE USING DSVParser  (
 )
OUTPUT TO CsvStream ;

CREATE OR REPLACE SOURCE initialLoad_Src USING Global.DatabaseReader (
  QuiesceOnILCompletion: false,
  Tables: '@SrcTableName@',
  adapterName: 'DatabaseReader',
  Password: '@Password@',
  Username: '@UserName@',
  ConnectionURL: '@Srcurl@',
   FetchSize: 10000)
OUTPUT TO ILStream;

Create Type CSVType (
  companyid String,
  merchantId String
);

CREATE STREAM CommonTypedStream OF CSVType;


CREATE OR REPLACE  CQ CsvToPosData
INSERT INTO CommonTypedStream
SELECT
TO_STRING(data[0]).replaceAll("COMPANY ", ""),
data[1]
FROM CsvStream;

CREATE CQ cq1
INSERT INTO CommonTypedStream
SELECT data[0],data[1]
FROM ILStream;


CREATE OR REPLACE TARGET Postgres_Trg USING Global.DatabaseWriter (
  ConnectionURL: '@trgUrl@',
  Username: '@trgUsrName@',
  Tables: '@trgTable@',
  Password: '@trgPswd@',
  CommitPolicy: 'EventCount:10000,Interval:60',
  adapterName: 'DatabaseWriter' )
INPUT FROM CommonTypedStream;

CREATE TARGET filewriter_tgt USING Global.FileWriter (
 directory:'@trgDir@',
  filename: '@fileName@',
  flushpolicy: 'EventCount:10000,Interval:30s',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n' )
INPUT FROM CommonTypedStream;

CREATE OR REPLACE TARGET BigQuery_Target USING Global.BigQueryWriter (
  streamingUpload: 'false',
  projectId: '@projectID@',
  Tables: '@BQTableName@',
  optimizedMerge: 'false',
  ServiceAccountKey: '@ServiceAccountKey@',
  BatchPolicy: 'EventCount:1000000,Interval:90',
  Mode: 'APPENDONLY' )
INPUT from CommonTypedStream;

END APPLICATION @AppName@;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	charset:'@charset@'
)
parse using JSONParser (
	eventType:'@evty@',
	fieldName:'@fname@'
)
OUTPUT TO CsvStream;

create Target FileTarget using FileWriter(
    rolloverpolicy:'eventcount:100',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JSONFormatter (
)
input from CsvStream;

end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using AzureblobWriter(
    accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:7'
)
format using DSVFormatter (
)
input from @STREAM@;

end application @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@ recovery 1 second interval;

CREATE SOURCE @parquetsrc@ USING FileReader (
  directory: '',
  positionByEOF: false,
  WildCard: '' )
PARSE USING ParquetParser (
 )
OUTPUT TO @appname@Streams;

CREATE OR REPLACE TARGET @kafkatarget@ USING Global.KafkaWriter VERSION @KAFKAVERSION@(
     brokerAddress: '',
     Topic: '',
     KafkaConfigValueSeparator: '=',
     MessageKey: '',
     MessageHeader: '',
     KafkaConfigPropertySeparator: ';',
     Mode: 'Sync',
     KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000' )
format using AvroFormatter (
formatAs: 'Default',
  schemaregistryurl: 'http://localhost:8081/',
  SchemaRegistrySubjectName: '',
  formatterName: 'AvroFormatter',
  schemaregistryConfiguration: ''
)
INPUT FROM @appname@Streams;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

--
-- Recovery Test 2
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS
--

STOP Recov2Tester.RecovTest2;
UNDEPLOY APPLICATION Recov2Tester.RecovTest2;
DROP APPLICATION Recov2Tester.RecovTest2 CASCADE;
CREATE APPLICATION RecovTest2 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION RecovTest2;

CREATE OR REPLACE APPLICATION @AppName@;
Create connectionProfile @CP@ TYPE Snowflake(ConnectionURL:'@SFurl@',username:'@SFUsername@',Password:'@SFPassword@',authenticationType:'Password');

Create Source @AppName@_source Using OracleReader(
  Username:'@username@',
  Password:'@password@',
  ConnectionURL:'@url@',
  Tables:'@tableName@',
  Fetchsize:1
)
Output To @AppName@_Stream;
CREATE OR REPLACE TARGET @AppName@_Target USING Global.SnowflakeWriter (
  streamingUpload: 'false',
  useConnectionProfile:'true',
  connectionProfileName: 'admin.@CP@',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  columnDelimiter: '|',
  tables: '@tableName@,SANJAYPRATAP.SAMPLESCHEMA.SAMPLE_PK',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:1,interval:5m',
  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',
  externalStageType: 'Local',
  adapterName: 'SnowflakeWriter',
  fileFormatOptions: 'null_if = \"\"' )
INPUT FROM @AppName@_Stream;
END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;
DROP EXCEPTIONSTORE @APP_NAME@_EXCEPTIONSTORE;

CREATE APPLICATION @APP_NAME@ @APP_PROPERTY@ USE EXCEPTIONSTORE;

CREATE SOURCE @APP_NAME@_Source Using @SOURCE_ADAPTER@ (

) OUTPUT TO @APP_NAME@DataStream;


CREATE TARGET @TARGETNAME@ USING @TARGET_ADAPTER@ (

) INPUT FROM @APP_NAME@DataStream;

END APPLICATION @APP_NAME@;

DEPLOY APPLICATION @APP_NAME@ IN DEFAULT;
START APPLICATION @APP_NAME@;

STOP application AlterTester.DSV;
undeploy application AlterTester.DSV;
drop application AlterTester.DSV cascade;


create application DSV;

create flow myFlowDSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

end flow myFlowDSV;
end application DSV;

STOP AdhocTester.ws_one;
UNDEPLOY APPLICATION AdhocTester.ws_one;
DROP APPLICATION AdhocTester.ws_one cascade;

CREATE APPLICATION ws_one;

CREATE SOURCE wsSource USING CSVReader  ( 
  blocksize: 10240,
  charset: 'UTF-8',
  positionByEOF: false,
  columndelimiter: ',',
  directory: '@TEST-DATA-PATH@',
  eofdelay: 100,
  wildcard: 'sampleByData.csv',
  expected_column_count: 0,
  rowdelimiter: '\n',
  header: true,
  adapterName: 'CSVReader',
  quoteset: '\"',
  trimquote: true,
  skipbom: true
 ) 
OUTPUT TO QaStream ;

CREATE TYPE sampleS_Type  ( vID java.lang.Integer KEY, 
vInt java.lang.Integer , 
vDouble java.lang.Double , 
vLong java.lang.Long , 
vShort java.lang.Short , 
vFloat java.lang.Float  
 ) ;

CREATE STREAM sampleS OF sampleS_Type;

CREATE CQ csvTowsData 
INSERT INTO sampleS
SELECT  to_int(data[0]) as vID,
to_int(data[1]) as vInt,
to_double(data[1]) as vDouble,
to_long(data[1]) as vLong,
to_short(data[1]) as vShort,
to_float(data[1]) as vFloat
FROM QaStream
;

CREATE WACTIONSTORE oneWS  CONTEXT OF sampleS_Type
EVENT TYPES(sampleS_Type )
@PERSIST-TYPE@

CREATE CQ wsToWaction 
INSERT INTO oneWS
SELECT * FROM sampleS s
LINK SOURCE EVENT;

END APPLICATION ws_one;
DEPLOY APPLICATION ws_one on any in default;
START ws_one;

Stop Oracle_IRLogWriter;
Undeploy application Oracle_IRLogWriter;
drop application Oracle_IRLogWriter cascade;
CREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;
create flow AgentFlow;
CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( 
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.119/DBS_PORT=1025',
  Tables: 'striim.upgrade01',
  CheckColumn:'striim.upgrade01=t1',
  startPosition:'striim.upgrade01=2018-09-20 06:43:59',
  ReturnDateTimeAs:'string'
  )
OUTPUT TO data_stream1;
end flow AgentFlow;

create flow serverFlow;
create target AzureSQLDWHTarget using AzureSQLDWHWriter(
ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        Accountaccesskey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test26,dbo.test26',
        uploadpolicy:'eventcount:10000,interval:10s'
) INPUT FROM data_stream1;

end flow serverFlow;
END APPLICATION Oracle_IRLogWriter;
deploy application Oracle_IRLogWriter with AgentFlow in Agents, ServerFlow in default;
start application Oracle_IRLogWriter;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING DatabaseReader  (
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  Tables: @SOURCE_TABLE@,
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true'
 )
OUTPUT TO @STREAM@;


CREATE OR REPLACE SOURCE @SOURCE_NAME@2 USING DatabaseReader  (
  Username: '@READER-UNAME@',
  Password: '@READER-PASSWORD@',
  ConnectionURL: '@CDC-READER-URL@',
  Tables: @SOURCE_TABLE@,
  sendBeforeImage:'true',
  FilterTransactionBoundaries: 'true'
 )
OUTPUT TO @STREAM@;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

Create Source dataGenSrc using MySQLReader
(
 Username:'root',
 Password:'w@ct10n',
 ConnectionURL: 'mysql://127.0.0.1:3306/waction',
 Tables:'@tableNames@'
 )
 Output To LCRStream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;

END APPLICATION DataGenSampleApp;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using MySQLReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

STOP APPLICATION SystemTimeTester.SystemTimeWindows;
UNDEPLOY APPLICATION SystemTimeTester.SystemTimeWindows;
DROP APPLICATION SystemTimeTester.SystemTimeWindows cascade;


CREATE APPLICATION SystemTimeWindows;

CREATE TYPE RandomData(
bankNumber int KEY,
bankName String
);

CREATE  SOURCE ranDataSource USING StreamReader (
  OutputType: 'SystemTimeTester.RandomData',
  noLimit: 'false',
  isSeeded: 'true',
  maxRows: 0,
  iterations: 30,
  iterationDelay: 1000,
  StringSet: 'myName[Michael-Jason-Abel-Contavius],streetAddress[adfsfa-safda-asdfasd-fasdf],bankName[bofa-chase-wellsfargo]',
  NumberSet: 'bankNumber[250-350]R,bankAmount[20.5-50.5]R'
 )
OUTPUT TO CSVDataStream;

CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT data[0], data[1]
FROM CSVDataStream;

CREATE @WINDOWTYPE@ WINDOW tierone OVER RandomDataStream keep within 20 second;

CREATE STREAM onetwostream OF RandomData;

CREATE CQ onetwocq
INSERT INTO onetwostream
SELECT bankNumber,bankName
FROM tierone
where  bankName LIKE 'bofa'
order by bankName;

CREATE WACTIONSTORE MyDataActivity  CONTEXT OF RandomData
EVENT TYPES ( RandomData  )
@PERSIST-TYPE@

Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
SELECT bankNumber,bankName from @FROMSTREAM@
where  bankName LIKE '%fa%'
order by bankName
LINK SOURCE EVENT;

END APPLICATION SystemTimeWindows;
deploy application SystemTimeWindows;
start application SystemTimeWindows;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.e1ptest%',
	FetchSize: '1'
)
OUTPUT TO @APPNAME@_SS;


CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.e1ptest%,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12)',
Mode:'merge',
optimizedMerge: 'true',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1000,Interval:2',
StandardSQL:true		
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

stop @appName@;
undeploy application @appName@;
drop application @appName@ cascade;

CREATE APPLICATION @appName@ USE EXCEPTIONSTORE TTL : '7d' AUTORESUME MAXRETRIES 1000 RETRYINTERVAL 60;

CREATE OR REPLACE SOURCE @appName@_Source USING Global.MysqlReader (
  ConnectionURL: '@ConnectionURL@',
  Tables: '@dbTable@',
  Password: '@Password@',
  Username: '@Username@',
  adapterName: 'MysqlReader')
OUTPUT TO @appName@st1;

CREATE CQ @appName@CQ
INSERT INTO @appName@st2
SELECT data[0] as Id,data[1] as Description FROM @appName@st1 s;;

create target @appName@_tgt using NullWriter() input from @appName@st2;

END APPLICATION @appName@;

STOP APPLICATION OneAgentMultiSourceCQTester.OneAgentWithMultiSourceCQApp;
UNDEPLOY APPLICATION OneAgentMultiSourceCQTester.OneAgentWithMultiSourceCQApp;
DROP APPLICATION OneAgentMultiSourceCQTester.OneAgentWithMultiSourceCQApp cascade;

create Application OneAgentWithMultiSourceCQApp;
CREATE FLOW AgentFlow;

create source XMLSource using FileReader (
  Directory:'@TEST-DATA-PATH@',
  WildCard:'books.xml',
  positionByEOF:false
)
parse using XMLParser (
  RootNode:'/catalog/book',
  columnlist:'book/@id,book/author,book/title,book/genre,book/price,book/publish_date,book/description'
)
OUTPUT TO XmlStream;

CREATE TYPE MyTypeXml(
id String KEY,
author String,
title String,
genre String,
price String,
publish_date String,
description String
);

CREATE STREAM TypedStreamXml of MyTypeXml;

CREATE CQ TypeConversionCQXml
INSERT INTO TypedStreamXml
SELECT
data[0],
data[1],
data[2],
data[3],
data[4],
data[5],
data[6]
from XmlStream;

create source DSVCSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'customerdetails-agent.csv',
  charset: 'UTF-8',
  positionByEOF:false
)
parse using DSVParser (
  header:'yes',
  minexpectedcolumns:'8'
)
OUTPUT TO DSVCsvStream;

CREATE TYPE MyTypeCsv(
PAN String,
FNAME String KEY,
LNAME String,
ADDRESS String,
CITY String,
STATE String,
ZIP String,
GENDER String
);

CREATE STREAM TypedStreamCsv of MyTypeCsv;

CREATE CQ TypeConversionCQCsv
INSERT INTO TypedStreamCsv
SELECT
data[0],
data[1],
data[2],
data[3],
data[4],
data[5],
data[6],
data[7]
from DSVCsvStream;

-- Read from File

create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'StoreNames.csv',
  columndelimiter:',',
  positionByEOF:false
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CSVStream;

CREATE TYPE MyType (
Store_Id String KEY,
Store_Name String
);

CREATE STREAM TypedStream of MyType;

CREATE CQ TypeConversionCQ
INSERT INTO TypedStream
SELECT data[0], data[1]
from CsvStream;

END FLOW AgentFlow;

CREATE FLOW ServerFlow;

CREATE WACTIONSTORE StoreInfo CONTEXT OF MyType
EVENT TYPES ( MyType )
@PERSIST-TYPE@

CREATE CQ StoreWaction
INSERT INTO StoreInfo
SELECT * FROM TypedStream
LINK SOURCE EVENT;

CREATE WACTIONSTORE StoreInfoXml CONTEXT OF MyTypeXml
EVENT TYPES ( MyTypeXml )
@PERSIST-TYPE@

CREATE CQ StoreWactionXml
INSERT INTO StoreInfoXml
SELECT * FROM TypedStreamXml
LINK SOURCE EVENT;

CREATE WACTIONSTORE StoreInfoCsv CONTEXT OF MyTypeCsv
EVENT TYPES ( MyTypeCsv )
@PERSIST-TYPE@

CREATE CQ StoreWactionCsv
INSERT INTO StoreInfoCsv
SELECT * FROM TypedStreamCsv
LINK SOURCE EVENT;


END FLOW ServerFlow;
end Application OneAgentWithMultiSourceCQApp;

DEPLOY APPLICATION OneAgentWithMultiSourceCQApp with AgentFlow in AGENTS, ServerFlow on any in default;
start OneAgentWithMultiSourceCQApp;

STOP bq;
UNDEPLOY APPLICATION bq;
DROP APPLICATION bq CASCADE;

CREATE APPLICATION bq RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE DEV_22964_source USING MSSqlReader
(
	Username: 'qatest',
	Password: 'w3b@ct10n',
	ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
	Tables: 'QATEST.BitToBoolean',
	FetchTransactionMetadata: true, 
	FetchSize: '1'
)
OUTPUT TO SS;


CREATE or replace TARGET DEV_22964_target USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
    Tables:'QATEST.TABLE_TEST_1000001,qatest.% keycolumns(RONUM)',
    mode:'Appendonly',
    datalocation: 'US',
	nullmarker: 'NULL',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:100,Interval:10'	
) INPUT FROM ss;

END APPLICATION bq;
DEPLOY APPLICATION bq;
START APPLICATION bq;

STOP APPLICATION orrs;
	UNDEPLOY APPLICATION orrs;
	DROP APPLICATION orrs CASCADE;
	CREATE APPLICATION orrs;
	Create Source OraSource Using OracleReader 
	(
	 Username:'user-name',	
	 Password:'password',
	 ConnectionURL: 'src_url',
	 Tables:'src_table',
	 FilterTransactionBoundaries:true,
	 FetchSize:'fetch-size'
	) Output To LCRStream;
	
	CREATE TARGET RSTarget USING RedshiftWriter
	(
	  ConnectionURL: 'tgt_url',
	  Username: 'tgt_username',
	  Password: 'tgt_pwrd',
	  bucketname: 'bucket_name',
	  --accesskeyId: '@ACCESS-KEY-ID@',
	  --secretaccesskey: '@SECRET-ACCESS-KEY@',
	  S3IAMRole:'@IAMROLE@',
	  Tables: 'tgt_table',
	  uploadpolicy:'eventcount:300,interval:1m'
	) INPUT FROM LCRStream;
	
	END APPLICATION orrs;
	deploy application orrs;
	START application orrs;

CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (
  StartPosition: '@startPosition@',
  ConnectionURL: '@sourceURL@',
  Username: '@userName@',
  Tables: '@tables@',
  CheckColumn: '@checkColum@',
  FetchSize: 1,
  Password: '@password@',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

 create type @SOURCE_NAME@_AutoType(
  ID int,
  name string,
  country string
);

CREATE STREAM @STREAM@_stream OF @SOURCE_NAME@_AutoType;

CREATE CQ Lookup
INSERT INTO @STREAM@_stream
select data[0],data[1],data[2] from @STREAM@;

stop CSVToHBase;
undeploy application CSVToHBase;
drop application CSVToHBase cascade;

CREATE APPLICATION CSVToHBase;

CREATE OR REPLACE SOURCE CSVPoller USING FileReader ( 
	directory:'/Users/ravipathak/webactionrepo/Product',
	WildCard:'smallpos.csv',
	positionByEOF:false
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

CREATE OR REPLACE TYPE CSVStream_Type  ( BUSINESS_NAME java.lang.String KEY, 
MERCHANT_ID java.lang.String, 
PRIMARY_ACCOUNT_NUMBER java.lang.String  
 ) ;

CREATE OR REPLACE STREAM CSVTypeStream OF CSVStream_Type;

CREATE OR REPLACE CQ CQ1 
INSERT INTO CSVTypeStream
SELECT data[0],data[1],data[2]
FROM CsvStream;

CREATE OR REPLACE TARGET Target1 USING SysOut ( 
  name: "dstream"
 ) 
INPUT FROM CsvStream;

CREATE OR REPLACE TARGET Target2 using HBaseWriter(
 HBaseConfigurationPath:"/Users/ravipathak/soft/hbase-1.1.5/conf/hbase-site.xml",
  Tables: "maprtest.maprdata",
  --FamilyNames: "maprdata",
  BatchPolicy: "eventCount:1")
INPUT FROM CSVTypeStream;

END APPLICATION CSVToHBase;

deploy application CSVToHBase;
start CSVToHBase;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'smallposdata10rows.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


@@FEATURE-DIR@/tql/TQLwithinTQL3.tql;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@;
CREATE SOURCE  @srcName@  USING Global.OJet ( 
  Username:'@srcusername@',
  Password:'@srcpassword@', 
  Tables:'@srcschema@.@srctable@',
  ConnectionURL:'@srcurl@',
  Tables:'@srcschema@.@srctable@',
  sendBeforeImage:'true',
  FilterTransactionBoundaries:'true'
) 
OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@ USING DatabaseWriter (
  CheckPointTable: 'CHKPOINT', 
  ReplicationSlotName:'test_slot',
  Username:'@tgtusername@',
  Password:'@tgtpassword@',
  ConnectionURL:'@tgturl@',
  adapterName:'PostgreSQLReader',
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@'
)
INPUT FROM @outstreamname@;

End APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

--
-- Recovery Test 38 with two sources, two jumping time-count windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> Jc5a3W/p -> CQ1 -> WS
--   S2 -> Jc6a4W/p -> CQ2 -> WS
--

STOP KStreamRecov38Tester.KStreamRecovTest38;
UNDEPLOY APPLICATION KStreamRecov38Tester.KStreamRecovTest38;
DROP APPLICATION KStreamRecov38Tester.KStreamRecovTest38 CASCADE;

DROP USER KStreamRecov38Tester;
DROP NAMESPACE KStreamRecov38Tester CASCADE;
CREATE USER KStreamRecov38Tester IDENTIFIED BY KStreamRecov38Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov38Tester;
CONNECT KStreamRecov38Tester KStreamRecov38Tester;

CREATE APPLICATION KStreamRecovTest38 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE JUMPING WINDOW DataStream5Rows3Seconds
OVER DataStream1 KEEP 5 ROWS WITHIN 3 SECOND
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Rows4Seconds
OVER DataStream2 KEEP 6 ROWS WITHIN 4 SECOND
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataStream5Rows3Seconds
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Rows3Seconds p
GROUP BY p.merchantId;

CREATE CQ DataStream6Rows4Seconds
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Rows4Seconds p
GROUP BY p.merchantId;

END APPLICATION KStreamRecovTest38;

stop application HDFSDSV;
undeploy application HDFSDSV;
drop application HDFSDSV cascade;

create application HDFSDSV;
create source HDFSCSVSource using HDFSReader (
	hadoopurl:'@HDFSREADERHADOOPURL@/home/hadoop/input/',
    WildCard:'posdata.csv',
	charset:'UTF-8',
    positionByEOF:false
)
parse using @HDFSCSVSOURCEFORMATTERTYPE@ (
	@HDFSCSVSOURCEFORMATTERMEMBERS@
)
OUTPUT TO HDFSCsvStream;
create Target HDFSDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/posdata') input from HDFSCsvStream;
end application HDFSDSV;

stop APPLICATION @AppName@;
Undeploy APPLICATION @AppName@;
drop APPLICATION @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @AgentFlow@1;
CREATE OR REPLACE SOURCE @SourceName@1 USING Global.MSJet (
  Tables: 'dbo.compsrc',
  username: 'qatest',
  DatabaseName: 'qatest',
  FetchTransactionMetadata: true,
  filterTransactionBoundaries: true,
  compression: false,
  ConnectionURL: '@ConnectionURL@1',
  Mode: '@mode@',
  CommittedTransactions: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
  SendBeforeImage: true,
  password: 'w3b@ct10n' )
OUTPUT TO @StreamName@;
END FLOW @AgentFlow@1;

CREATE FLOW @AgentFlow@2;
CREATE OR REPLACE SOURCE @SourceName@2 USING Global.MSJet (
  Tables: 'dbo.compsrc',
  username: 'qatest',
  DatabaseName: 'qatest',
  FetchTransactionMetadata: true,
  filterTransactionBoundaries: true,
  compression: false,
  ConnectionURL: '@ConnectionURL@2',
  CaptureType: '@captureType@',
  CommittedTransactions: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
  SendBeforeImage: true,
  password: 'w3b@ct10n' )
OUTPUT TO @StreamName@;
END FLOW @AgentFlow@2;

CREATE TARGET @SysTarget@ USING Global.SysOut (
  name: 'MS_CDC_SYSOUT' )
INPUT FROM @StreamName@;

CREATE FLOW @ServerFlow@1;
CREATE TARGET @TargetName@1 USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ParallelThreads: '',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: '@ConnectionURL@',
  StatementCacheSize: '50',
  DatabaseProviderType: 'Default',
  Username: 'qatest',
  Tables: 'dbo.compsrc,dbo.comptar',
  Password: 'w3b@ct10n',
  PreserveSourceTransactionBoundary: 'false',
  BatchPolicy: 'EventCount:1,Interval:60' )
INPUT FROM @StreamName@;
END FLOW @ServerFlow@1;

CREATE FLOW @ServerFlow@2;
CREATE TARGET @TargetName@2 USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ParallelThreads: '',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: '@ConnectionURL@',
  StatementCacheSize: '50',
  DatabaseProviderType: 'Default',
  Username: 'qatest',
  Tables: 'dbo.compsrc,dbo.comptar',
  Password: 'w3b@ct10n',
  PreserveSourceTransactionBoundary: 'false',
  BatchPolicy: 'EventCount:1,Interval:60' )
INPUT FROM @StreamName@;
END FLOW @ServerFlow@2;

END APPLICATION @AppName@;
DEPLOY APPLICATION @AppName@ with @AgentFlow@1 in AGENTS, @AgentFlow@2 in AGENTS, @ServerFlow@1 on any in default, @ServerFlow@2 on any in default;
START APPLICATION @AppName@;

STOP APPLICATION OneAgentEncryptionTester.CSV;
UNDEPLOY APPLICATION OneAgentEncryptionTester.CSV;
DROP APPLICATION OneAgentEncryptionTester.CSV cascade;

create application CSV WITH ENCRYPTION;

CREATE FLOW AgentFlow;
create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails-agent.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStream1;

END FLOW AgentFlow;

CREATE FLOW ServerFlow;

CREATE TYPE MyTypeCsv(
PAN String,
FNAME String KEY,
LNAME String,
ADDRESS String,
CITY String,
STATE String,
ZIP String,
GENDER String
);

CREATE STREAM TypedStreamCsv of MyTypeCsv;

CREATE CQ TypeConversionCQCsv
INSERT INTO TypedStreamCsv
SELECT
data[0],
data[1],
data[2],
data[3],
data[4],
data[5],
data[6],
data[7]
from CsvStream1;

CREATE WACTIONSTORE StoreInfoCsv CONTEXT OF MyTypeCsv
EVENT TYPES ( MyTypeCsv )
@PERSIST-TYPE@

CREATE CQ StoreWactionCsv
INSERT INTO StoreInfoCsv
SELECT * FROM TypedStreamCsv
LINK SOURCE EVENT;


END FLOW ServerFlow;

end application CSV;

DEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;
START CSV;

Stop @APPNAME@_App;
undeploy application @APPNAME@_App;
drop application @APPNAME@_App cascade;

CREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;
CREATE SOURCE @APPNAME@_Source USING Global.OracleReader

(
  FetchSize:'1',
  Username:'@SOURCE_USER@',
  Password:'@SOURCE_PASSWORD@',
  ConnectionURL:'@SOURCE_URL@',
  Tables:'@SOURCE_TABLES@'
)
OUTPUT TO @APPNAME@_Stream;

CREATE TARGET @APPNAME@_SYS USING SysOut (
  name: '@APPNAME@_SYS' )
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_CDB_Target USING DatabaseWriter

(
  Username:'@TARGET_CDB_USER@',
  ConnectionURL:'@TARGET_CDB_URL@',
  Tables:'@TARGET_CDB_TABLES@',
  Password:'@TARGET_CDB_PASSWORD@',
  BatchPolicy:'EventCount:1,Interval:1',
  CommitPolicy:'EventCount:1,Interval:1'
)
INPUT FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_PDB_Target USING DatabaseWriter

(
  Username:'@TARGET_PDB_USER@',
  ConnectionURL:'@TARGET_PDB_URL@',
  Tables:'@TARGET_PDB_TABLES@',
  Password:'@TARGET_PDB_PASSWORD@',
  BatchPolicy:'EventCount:1,Interval:1',
  CommitPolicy:'EventCount:1,Interval:1'
)
INPUT FROM @APPNAME@_Stream;

End APPLICATION @APPNAME@_App;

stop application GCSWriterTest;
undeploy application GCSWriterTest;
drop application GCSWriterTest cascade;
create application GCSWriterTest recovery 1 second interval;
create source GCS_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'true'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;
create Target GCSTarget using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@',
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
--members:'data'
)
input from TypedCSVStream;

end application GCSWriterTest;
deploy application GCSWriterTest on all in default;
start application GCSWriterTest;

Stop Oracle_IRLogWriter;
Undeploy application Oracle_IRLogWriter;
drop application Oracle_IRLogWriter cascade;

CREATE APPLICATION Oracle_IRLogWriter recovery 5 second interval;

CREATE OR REPLACE SOURCE Oracle_IRSource USING IncrementalBatchReader  ( 
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.TDSOURCE',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.TEST01=ID;',
  PollingInterval: '5sec',
  ReturnDateTimeAs: 'String',
  startPosition:'striim.test01=0'
  )
  OUTPUT TO data_stream;

  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

CREATE TARGET BinaryDump USING LogWriter(
  name: 'TeraData',
  filename:'TeraData.log',
  flushpolicy:'EventCount:100,Interval:30s'
)INPUT FROM data_stream;

END APPLICATION Oracle_IRLogWriter;

deploy application Oracle_IRLogWriter in default;

start application Oracle_IRLogWriter;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@;
CREATE  SOURCE @SourceName@ USING MSSqlReader  ( 
  Username: '@UserName@',
  Password: '@Password@',
  DatabaseName: 'qatest',
  ConnectionURL: '@SourceConnectionURL@',
  Tables: 'qatest.@SourceTable@',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @SRCINPUTSTREAM@;
create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;
CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:1',
  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
 ) 
INPUT FROM @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

CREATE APPLICATION @AppName@;

CREATE OR REPLACE SOURCE CP_Oracle_source USING OracleReader (
  ConnectionURL: '',
  Tables: '',
  Username: '',
  Password: '',
  Fetchsize: 1 )
OUTPUT TO CP_EndToEnd_DB_Adapter_Stream;

CREATE OR REPLACE TARGET CP_DB_Target USING Global.DeltaLakeWriter (
  connectionProfileName: '',
  useConnectionProfile: 'true',
  externalStageConnectionProfileName: '',
  Tables: 'QATEST.Test_CP,qa_reg_CDDL_1702803133916.OrcToDLAltAppendtarget1',
  uploadPolicy: 'eventcount:100000,interval:60s'
)
INPUT FROM CP_EndToEnd_DB_Adapter_Stream;
END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

STOP OuterJoinTester.OJApp;
UNDEPLOY APPLICATION OuterJoinTester.OJApp;
DROP APPLICATION OuterJoinTester.OJApp cascade;

CREATE APPLICATION OJApp;


CREATE TYPE bankData
(
ucID Integer,
ucLong long,
ucDate DateTime,
ucDouble Double KEY
);


CREATE STREAM wsStream OF bankData;

CREATE CACHE cache1 USING CsvReader(
directory: '@TEST-DATA-PATH@',
wildcard: 'ucData.csv',
header: No,
columndelimiter: ',',
trimquote: false
) QUERY (keytomap:'ucDouble') OF bankData;



END APPLICATION OJApp;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

create Target FileTarget using FileWriter(
    rolloverpolicy:'@UPLOAD-SIZE@',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JSONFormatter (
charset:'@charset@',
members:'@mem@'
)
input from CsvStream;

end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @SourceName@ USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 ) 
OUTPUT TO @SRCINPUTSTREAM@ ;


CREATE OR REPLACE TARGET @targetsys@ USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:60',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgrestopostgres_src, public.postgrestopostgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM @SRCINPUTSTREAM@;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING ParquetParser (
  retryWait: '1m'
)
OUTPUT TO @APPNAME@_Stream;

CREATE CQ @APPNAME@_CQ
INSERT INTO @APPNAME@_CQOut
SELECT putUserData(x, 'folderName','Parquet') FROM @APPNAME@_Stream x;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING ParquetFormatter (
  schemaFileName: 'parquetSchema',
  members:'data'
)
INPUT FROM @APPNAME@_CQOut;

END APPLICATION @APPNAME@;

CREATE APPLICATION SourceRetailApp;

CREATE source RetailDataSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:no,
  wildcard:'retaildata2M.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO RetailOrders;

CREATE TARGET RetailSourceDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceRetailAppData') input from RetailOrders;

CREATE Source RetailHourlyStoreSales using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'storehourlyData.txt',
  header: no,
  columndelimiter: ',',
  positionByEOF:false
) OUTPUT TO RetailCacheSource1;

CREATE TARGET RetailCacheDump1 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceRetailCacheData1') input from RetailCacheSource1;

CREATE Source RetailStoreNameLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'StoreNames.csv',
  header: no,
  columndelimiter: ',',
  positionByEOF:false
) OUTPUT TO RetailCacheSource2;

CREATE TARGET RetailCacheDump2 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceRetailCacheData2') input from RetailCacheSource2;

CREATE Source RetailZipCodeLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  header: no,
  columndelimiter: '	',
  positionByEOF:false
) OUTPUT TO  RetailCacheSource3;

CREATE TARGET RetailCacheDump3 using CSVWriter(fileName:'@FEATURE-DIR@/logs/SourceRetailCacheData3') input from RetailCacheSource3;


END APPLICATION SourceRetailApp;

stop application MySQLAllDataTypes;
undeploy application MySQLAllDataTypes;
drop application MySQLAllDataTypes CASCADE;
create application MySQLAllDataTypes;

CREATE OR REPLACE SOURCE MySQLSource USING MySQLReader  (
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Database:'@DATABASE@',
  Tables: '@SOURCE_TABLES@'
 ) Output To MySQLStream;
 
--create Target t2 using SysOut(name:Foo2) input from MySQLStream; 
CREATE TARGET RedshiftTarget USING RedshiftWriter
	(
	  ConnectionURL: '@TARGET-URL@',
	  Username: '@TARGET-UNAME@',
	  Password: '@TARGET-PASSWORD@',
	  bucketname: '@BUCKETNAME@',
	  accesskeyId: '@ACCESS-KEY-ID@',
	  secretaccesskey: '@SECRET-ACCESS-KEY@',
	  Tables: '@TARGET-TABLES@',
	  uploadpolicy:'eventcount:1,interval:20s',
	  Mode:'incremental'
	) INPUT FROM MySQLStream;
	
END APPLICATION MySQLAllDataTypes;
deploy application MySQLAllDataTypes;
START application MySQLAllDataTypes;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

CREATE  SOURCE @QuerySource@1 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@1;
create Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;
 CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@1;

CREATE  SOURCE @QuerySource@2 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@2;
create Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;
 CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@2;

CREATE  SOURCE @QuerySource@3 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@3;
create Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;
 CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@3;

CREATE  SOURCE @QuerySource@4 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@4;
create Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;
 CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@4;

CREATE  SOURCE @QuerySource@5 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@5;
create Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;
 CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@5;

CREATE  SOURCE @QuerySource@6 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@6;
create Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;
 CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@6;

CREATE  SOURCE @QuerySource@7 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@7;
create Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;
 CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@7;

CREATE  SOURCE @QuerySource@8 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@8;
create Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;
 CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@8;

CREATE  SOURCE @QuerySource@9 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@9;
create Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;
 CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@9;

CREATE  SOURCE @QuerySource@10 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@10;
create Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;
 CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@10;
end application @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING FILEReader (
    wildcard: '',
    directory: '',
    positionbyeof: false
 )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:10,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: ''
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@;

CREATE SOURCE @SOURCE_NAME@2 USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@2;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;

 CREATE OR REPLACE SOURCE TS USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01;striim.test02;striim.test03',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream ;

  CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

create target T using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream;


CREATE TARGET log USING LOGWRITER
(
 name:log,
 filename:'LOGFILENAME'
)
INPUT FROM data_stream;


END APPLICATION IR;

deploy application IR;
start IR;

--
-- Canon Test W40
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for an unpartitioned jumping attribute window
--
-- S -> JWa5u -> CQ -> WS
--


UNDEPLOY APPLICATION NameW40.W40;
DROP APPLICATION NameW40.W40 CASCADE;
CREATE APPLICATION W40 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW40;


CREATE SOURCE CsvSourceW40 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW40;


END FLOW DataAcquisitionW40;




CREATE FLOW DataProcessingW40;

CREATE TYPE DataTypeW40 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW40 OF DataTypeW40;

CREATE CQ CSVStreamW40_to_DataStreamW40
INSERT INTO DataStreamW40
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW40;

CREATE JUMPING WINDOW JWa5uW40
OVER DataStreamW40
KEEP WITHIN 5 SECOND ON dateTime;

CREATE WACTIONSTORE WactionStoreW40 CONTEXT OF DataTypeW40
EVENT TYPES ( DataTypeW40 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWa5uW40_to_WactionStoreW40
INSERT INTO WactionStoreW40
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWa5uW40;

END FLOW DataProcessingW40;



END APPLICATION W40;

STOP APPLICATION admin.PosApp;
UNDEPLOY APPLICATION admin.PosApp;
DROP APPLICATION admin.PosApp cascade;
drop namedquery admin.PosAppMainPageMap;

drop namedquery admin.PosAppMainPageBar;

drop namedquery admin.PosAppMainPageBar2;
drop namedquery admin.PosAppMainPageScatter;

drop namedquery admin.PosAppHeatMapDrilldownHeatMap;
drop namedquery admin.PosAppHeatMapDrilldownDonuts;
drop namedquery admin.PosAppMainPageHeatMap;
drop namedquery admin.PosAppCompanyDrilldownCharts;
drop namedquery admin.PosAppHeatMapDrilldownMap;
drop namedquery admin.PosAppMainSearchBox;
drop namedquery admin.PosAppMcount;

drop user PosTester;

drop namespace PosTester cascade;
drop dashboard admin.PosAppDash;

STOP APPLICATION FileReaderWithBinaryParserApp;
UNDEPLOY APPLICATION FileReaderWithBinaryParserApp;
DROP APPLICATION FileReaderWithBinaryParserApp cascade;

CREATE APPLICATION FileReaderWithBinaryParserApp;

create source BinarySource using FileReader (
  directory:'@TEST-DATA-PATH@/binary',
  wildcard:'10rows.bin',
  positionByEOF:false
)
parse using BinaryParser (
  metadata:'@TEST-DATA-PATH@/binary/metadata.json',
  endian:true,
  StringTerminatedByNull:false
)
OUTPUT TO BinaryStream;

create Target DSVDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/binaryDetails') input from BinaryStream;
CREATE TARGET RawOut using SysOut(name: TCPRaw) INPUT FROM BinaryStream;


END APPLICATION FileReaderWithBinaryParserApp;
deploy application FileReaderWithBinaryParserApp;
start application FileReaderWithBinaryParserApp;

create application XML;
create source CSVSource using FileReader (
	directory:'Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
	filename:'posdata_XML',
	rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'
)
format using XMLFormatter (
	rootelement:'document',
	elementtuple:'MerchantName:merchantid:text=merchantname'
)
input from TypedCSVStream;
end application XML;

create or replace type @STREAM@details(
C_CUSTKEY int,
C_MKTSEGMENT String,
C_NATIONKEY int,
C_NAME String,
C_ADDRESS String,
C_PHONE String,
C_ACCTBAL int,
C_COMMENT String
);

create or replace stream @STREAM@_TYPED of @STREAM@details;

Create or replace CQ @STREAM@detailsCQ
insert into @STREAM@_TYPED
select 
to_int(data[0]),data[1],to_int(data[2]),data[3],data[4],data[5],to_int(data[6]),data[7]
from @STREAM@;

CREATE WINDOW @STREAM@_DBRWindow
OVER @STREAM@_TYPED
KEEP 1000 ROWS;

create or replace stream @STREAM@_TYPED2 of @STREAM@details;

Create or replace CQ @STREAM@detailsCQ2
insert into @STREAM@_TYPED2
select 
to_int(C_CUSTKEY),C_MKTSEGMENT,to_int(C_NATIONKEY),C_NAME,C_ADDRESS,C_PHONE,to_int(C_ACCTBAL),C_COMMENT
from @STREAM@_DBRWindow;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING FileWriter  ( 
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000,Interval:30s'
 ) Format using DSVFormatter()
INPUT FROM @STREAM@_DBRWindow;

UNDEPLOY APPLICATION RollOverTester.DSV;
DROP APPLICATION RollOverTester.DSV CASCADE;
CREATE APPLICATION DSV;


create source JsonSource using FileReader (
        directory:'@TEST-DATA-PATH@',
        WildCard:'TestInput.txt',
        positionByEOF:false,
        charset:'UTF-8'
)
 PARSE USING JSONParser (
  eventType: ''
 )
OUTPUT TO activeguard_rawstream;


CREATE OR REPLACE TYPE activeguard_parsed (
	-- Enriched, or not in AGI left navigation
	hash_key String KEY,
	client_id Long,
	event_type_id Long,
	type_code String,
	rule_id Long,
	rule_name String,
	rule_action String,
	sce_id  Long,
	message String,
	ref_list_match String,
	event_date DateTime,
	log_count Long,

	-- As seen in AGI
	-- frequently used, but not duplicated
	received_date DateTime,
	event_sub_source String,

	-- database
	sol_action String,
	comment String,
	comment_text String,
	database_name String,
	database_user_name String,
	entry_id String,
	obj_name String,
	os_user_name String,
	owner String,
	priv_used String,
	return_code String,
	session_id String,

	-- geo
	destination_country_code String,
	source_country_code String,

	-- network
	data_size String,
	destination String,
	destination_object String,
	destination_port String,
	sol_domain String,
	event_manager String,
	event_path String,
	event_source String,
	msg_no String,
	protocol_id String,
	service String,
	sol_source String,
	source_mac_address String,
	source_object String,
	source_port String,
	status_code String,
	target_path String,

	-- other
	command String,

	-- security
	affected_file String,
	object_action String,
	object_action_result String,
	object_id String,
	object_name String,
	object_path String,
	process_name String,

	-- user
	caller_user_name String,
	logon_type String,
	target_user_name String,
	user_agent String,
	user_name String,
	workstation String,

	-- web
	web_file String,
	web_method String,
	web_page String,
	web_protocol String
);


CREATE OR REPLACE STREAM activeguard_stream OF activeguard_parsed;

CREATE CQ Parseactiveguard_stream
INSERT INTO activeguard_stream
SELECT

	-- Enriched, or not in AGI left navigation
	data.get('hash_key').textValue() as hash_key,
	data.get('client_id').longValue() as client_id,
	data.get('event_type_id').longValue() as event_type_id,
	data.get('type_code').textValue() as type_code,
	data.get('rule_id').longValue() as rule_id,
	data.get('rule_name').textValue() as rule_name,
	data.get('rule_action').textValue() as rule_action,
	data.get('sce_id').longValue() as sce_id,
	data.get('message').textValue() as message,
	data.get('ref_list_match').textValue() as ref_list_match,
	TO_DATE(data.get('event_date').textValue()) as event_date,
	data.get('log_count').longValue() as log_count,

	-- As seen in AGI
	-- frequently used, but not duplicated
	TO_DATE(data.get('received_date').textValue()) as received_date,
	data.get('event_sub_source').textValue() as event_sub_source,

	-- database
	data.get('sol_action').textValue() as sol_action,
	data.get('comment').textValue() as comment,
	data.get('comment_text').textValue() as comment_text,
	data.get('database_name').textValue() as database_name,
	data.get('database_user_name').textValue() as database_user_name,
	data.get('entry_id').textValue() as entry_id,
	data.get('obj_name').textValue() as obj_name,
	data.get('os_user_name').textValue() as os_user_name,
	data.get('owner').textValue() as owner,
	data.get('priv_used').textValue() as priv_used,
	data.get('return_code').textValue() as return_code,
	data.get('session_id').textValue() as session_id,

	-- geo
	data.get('destination_country_code').textValue() as destination_country_code,
	data.get('source_country_code').textValue() as source_country_code,

	-- network
	data.get('data_size').textValue() as data_size,
	data.get('destination').textValue() as destination,
	data.get('destination_object').textValue() as destination_object,
	data.get('destination_port').textValue() as destination_port,
	data.get('sol_domain').textValue() as sol_domain,
	data.get('event_manager').textValue() as event_manager,
	data.get('event_path').textValue() as event_path,
	data.get('event_source').textValue() as event_source,
	data.get('msg_no').textValue() as msg_no,
	data.get('protocol_id').textValue() as protocol_id,
	data.get('service').textValue() as service,
	data.get('sol_source').textValue() as sol_source,
	data.get('source_mac_address').textValue() as source_mac_address,
	data.get('source_object').textValue() as source_object,
	data.get('source_port').textValue() as source_port,
	data.get('status_code').textValue() as status_code,
	data.get('target_path').textValue() as target_path,

	-- other
	data.get('command').textValue() as command,

	-- security
	data.get('affected_file').textValue() as affected_file,
	data.get('object_action').textValue() as object_action,
	data.get('object_action_result').textValue() as object_action_result,
	data.get('object_id').textValue() as object_id,
	data.get('object_name').textValue() as object_name,
	data.get('object_path').textValue() as object_path,
	data.get('process_name').textValue() as process_name,

	-- user
	data.get('caller_user_name').textValue() as caller_user_name,
	data.get('logon_type').textValue() as logon_type,
	data.get('target_user_name').textValue() as target_user_name,
	data.get('user_agent').textValue() as user_agent,
	data.get('user_name').textValue() as user_name,
	data.get('workstation').textValue() as workstation,

	-- web
	data.get('web_file').textValue() as web_file,
	data.get('web_method').textValue() as web_method,
	data.get('web_page').textValue() as web_page,
	data.get('web_protocol').textValue() as web_protocol

FROM activeguard_rawstream;



CREATE TARGET alertf USING FileWriter (
  filename: 'JParser',
  flushinterval: '0',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy: 'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00'
 )
 FORMAT USING JSONFormatter (
		members:'hash_key,client_id,event_type_id,type_code,rule_id,rule_name,rule_action,sce_id,message,ref_list_match,event_date,log_count,received_date,event_sub_source,sol_action,comment,comment_text,database_name,database_user_name,entry_id,obj_name,os_user_name,owner,priv_used,return_code,session_id,destination_country_code,source_country_code,data_size,destination,destination_object,destination_port,sol_domain,event_manager,event_path,event_source,msg_no,protocol_id,service,sol_source,source_mac_address,source_object,source_port,status_code,target_path,command,affected_file,object_action,object_action_result,object_id,object_name,object_path,process_name,caller_user_name,logon_type,target_user_name,user_agent,user_name,workstation,web_file,web_method,web_page,web_protocol',
 		rowdelimiter:'\n'
 )
INPUT FROM activeguard_stream;

create Target z using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/Target_JParser_actual.log') input from activeguard_stream;


END APPLICATION DSV;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallretaildata2M.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
storeId String,
nameId String,
city String,
state String

);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],
data[1],
data[2],
data[3]

FROM CsvStream;

create Target t using FileWriter(
filename:'EventNCDefault',
directory:'@FEATURE-DIR@/logs/',
sequence:'00',
--filelimit: '5',
rolloverpolicy:'eventcount:-100'
)
format using DSVFormatter (

)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetNegativeECDefault_actual.log') input from TypedCSVStream;
end application DSV;
DEPLOY APPLICATION DSV on any in default;
START DSV;
deploy application DSV;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using MSSqlReader
(
 Username:'@UserName@',
 Password:'@Password@',
 DatabaseName:'@DatabaseName@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'@SourceTable@',
 ConnectionPoolSize:1,
 StartPosition:'EOF'
 ) Output To @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;

create Target @targetFile@ using FileWriter(
  filename:'TestOut.log',
  directory:'@FileDirectoryPath@',
  rolloverpolicy:'eventcount:1000'
)
format using DSVFormatter (

)
input from @SRCINPUTSTREAM@;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE FLOW @appname@AgentFlow;
    CREATE SOURCE @parquetsrc@ USING FileReader (
    wildcard: '',
    directory: '',
    positionbyeof: false )
    PARSE USING ParquetParser (
    )
    OUTPUT TO @appname@Stream;
END FLOW @appname@AgentFlow;

CREATE FLOW @appname@serverFlow;
    CREATE CQ @appname@CQ
    INSERT INTO @appname@CqOut
        SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;
    
    CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
    filename: '',
    directory: '',
    flushpolicy: 'EventCount:1,Interval:30s',
    rolloverpolicy: 'EventCount:10000,Interval:30s' )
    FORMAT USING ParquetFormatter  (
    schemaFileName: ''
    )
    INPUT FROM @appname@CqOut;
END FLOW @appname@serverFlow;

END APPLICATION @appname@;
DEPLOY APPLICATION @appname@ with @appname@AgentFlow in Agents, @appname@ServerFlow in default;
start application @appname@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

CREATE APPLICATION ValidateFiles;

CREATE OR REPLACE TYPE AggregatorOutput_Type  ( SUMsthingSUMtthing java.lang.Long );

CREATE OR REPLACE STREAM AggregatorOutput OF AggregatorOutput_Type;

CREATE OR REPLACE TARGET DiffFile USING FileWriter  (
  filename: 'Difference',
  rolloveronddl: 'true',
  flushpolicy: 'eventcount:10000,interval:30',
  adapterName: 'FileWriter',
  directory: '@FEATURE-DIR@/logs',
  rolloverpolicy: 'eventcount:10000,interval:30s'
 )
FORMAT USING DSVFormatter  (   nullvalue: 'NULL',
  standard: 'none',
  handler: 'com.webaction.proc.DSVFormatter',
  formatterName: 'DSVFormatter',
  usequotes: 'false',
  rowdelimiter: '\n',
  quotecharacter: '\"',
  header: 'false',
  columndelimiter: ','
 )
INPUT FROM AggregatorOutput;

CREATE OR REPLACE TYPE ewfew_Type  ( thing java.lang.Integer );

CREATE OR REPLACE TYPE newTargetST_Type  ( thing java.lang.Integer );

CREATE OR REPLACE STREAM newTargetST OF newTargetST_Type;

CREATE OR REPLACE JUMPING WINDOW AllTargetSums OVER newTargetST KEEP WITHIN 15 SECOND;

CREATE OR REPLACE SOURCE TargetSums USING FileReader  (
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '@FEATURE-DIR@/logs',
  skipbom: true,
  wildcard: 'TargetResults.00'
 )
 PARSE USING DSVParser  (
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: '-1',
  nocolumndelimiter: true,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: false,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 )
OUTPUT TO compareTstream ;

CREATE OR REPLACE CQ GetDemTargetValues
INSERT INTO newTargetST
SELECT TO_LONG(data[0]) as thing
FROM compareTstream;

CREATE OR REPLACE SOURCE SourceSums USING FileReader  (
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '@FEATURE-DIR@/logs',
  skipbom: true,
  wildcard: 'SourceResults.00'
 )
 PARSE USING DSVParser  (
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: '-1',
  nocolumndelimiter: true,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: false,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 )
OUTPUT TO compareStream ;

CREATE OR REPLACE TYPE newST_Type  ( thing java.lang.Integer );

CREATE OR REPLACE STREAM newST OF newST_Type;

CREATE OR REPLACE JUMPING WINDOW AllSourceSums OVER newST KEEP WITHIN 15 SECOND;

CREATE OR REPLACE CQ Aggregator
INSERT INTO AggregatorOutput
SELECT SUM(s.thing) - SUM(t.thing)
FROM AllSourceSums s, AllTargetSums t;

CREATE TARGET output1 USING SysOut(name : SrcItem) input FROM compareStream;
CREATE TARGET output2 USING SysOut(name : TrgItem) input FROM compareTstream;
CREATE TARGET output3 USING SysOut(name : AggregatorItem) input FROM AggregatorOutput;

CREATE OR REPLACE CQ GetdemValues
INSERT INTO newST
SELECT TO_LONG(data[0]) as thing
FROM compareStream;

END APPLICATION ValidateFiles;

stop application ManyToManyADLSGen1;
undeploy application ManyToManyADLSGen1;
drop application ManyToManyADLSGen1 cascade;

create application ManyToManyADLSGen1 Recovery 5 second interval;


create type ADLSGen1csv_type(
id String,
name String,
seq String
);

create type ADLSGen1Order_type(
id String,
Name String,
Company String
);



create source ADLSGen1CSVSource_multi using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'Canon1000_All.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO ADLSGen1CsvStream_user;

create source ADLSGen1CSVSource2 using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'portfolio.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO ADLSGen1CsvStream;

create source ADLSGen1CSVSource3 using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'dynamicdirectory.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO ADLSGen1CsvStream;


CREATE SOURCE ADLSGen1OraSource1 USING OracleReader
(
 Username:'miner',
 Password:'miner',
 ConnectionURL:'localhost:1521:xe',
 Tables:'QATEST.CUSTOMER1,QATEST.CUSTOMER2,QATEST.CUSTOMER3',
 OnlineCatalog:true,
 FetchSize:10000,
 QueueSize:2148,
 CommittedTransactions:false,
 Compression:true
)
OUTPUT TO ADLSGen1OrdersStream;

CREATE SOURCE ADLSGen1OraSource2 USING OracleReader
(
 Username:'miner',
 Password:'miner',
 ConnectionURL:'localhost:1521:xe',
 Tables:'QATEST.CUSTOMER4,QATEST.CUSTOMER5',
 OnlineCatalog:true,
 FetchSize:10000,
 QueueSize:2148,
 CommittedTransactions:false,
 Compression:true
)
OUTPUT TO ADLSGen1OrdersStream;


create Target ADLSGen1_tgt1 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'%@metadata(FileName)%',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
		rolloverpolicy:'interval:30s'
)
FORMAT USING JSONFormatter()
input from ADLSGen1CsvStream; 

create Target ADLSGen1_tgt2 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'%@metadata(TableName)%',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
		rolloverpolicy:'filesize:10M',
		compressiontype: 'true'
)
format using DSVFormatter (
)
input from ADLSGen1OrdersStream; 

create stream ADLSGen1UserdataStream of Global.WAEvent;

Create CQ ADLSGen1CQUser
insert into ADLSGen1UserdataStream
select 
putuserdata (data1,'Fileowner',data[1]) from ADLSGen1CsvStream_user data1;

create stream ADLSGen1CSVTypedStream1 of csv_type;
create stream ADLSGen1CSVTypedStream2 of csv_type;
create stream ADLSGen1CSVTypedStream3 of csv_type;

CREATE CQ ADLSGen1cq1
INSERT INTO ADLSGen1CSVTypedStream1
SELECT data[0],
data[1],
data[2]
FROM ADLSGen1UserdataStream
WHERE USERDATA(ADLSGen1UserdataStream,'Fileowner').toString() == 'Lorem';

CREATE CQ ADLSGen1cq2
INSERT INTO ADLSGen1CSVTypedStream2
SELECT data[0],
data[1],
data[2]
FROM ADLSGen1UserdataStream
WHERE USERDATA(ADLSGen1UserdataStream,'Fileowner').toString() == 'doloremque';

CREATE CQ ADLSGen1cq3
INSERT INTO ADLSGen1CSVTypedStream3
SELECT data[0],
data[1],
data[2]
FROM ADLSGen1UserdataStream
WHERE USERDATA(ADLSGen1UserdataStream,'Fileowner').toString() == 'accusantium';

create Target ADLSGen1_tgt3 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'1_%name%',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
	rolloverpolicy:'eventcount:74'
)
format using XMLFormatter (
  elementtuple: 'Eventname:name:id:seq:text=name',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from ADLSGen1CSVTypedStream1; 

create Target ADLSGen1_tgt4 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'2_%name%',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
    rolloverpolicy:'eventcount:4,interval:50s'
)
format using XMLFormatter (
  elementtuple: 'Eventname:name:id:seq:text=name',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from ADLSGen1CSVTypedStream2; 

create Target ADLSGen1_tgt5 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'3_%name%',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
    rolloverpolicy:'eventcount:12'
)
format using XMLFormatter (
  elementtuple: 'Eventname:name:id:seq:text=name',
  formatterName: 'XMLFormatter',
  rowdelimiter: '\n',
  rootelement: 'document'
)
input from ADLSGen1CSVTypedStream3; 


create stream ADLSGen1OrderTypedStream1 of Order_type;
create stream ADLSGen1OrderTypedStream2 of Order_type;
create stream ADLSGen1OrderTypedStream3 of Order_type;

CREATE CQ ADLSGen1cq1_db
INSERT INTO ADLSGen1OrderTypedStream1
SELECT data[0],
data[1],
data[2]
FROM ADLSGen1OrdersStream
WHERE META(ADLSGen1OrdersStream,'TableName').toString() == 'QATEST.CUSTOMER1';

CREATE CQ ADLSGen1cq2_db
INSERT INTO ADLSGen1OrderTypedStream2
SELECT data[0],
data[1],
data[2]
FROM ADLSGen1OrdersStream
WHERE META(ADLSGen1OrdersStream,'TableName').toString() == 'QATEST.CUSTOMER2';

CREATE CQ ADLSGen1cq3_db
INSERT INTO ADLSGen1OrderTypedStream3
SELECT data[0],
data[1],
data[2]
FROM ADLSGen1OrdersStream
WHERE META(ADLSGen1OrdersStream,'TableName').toString() == 'QATEST.CUSTOMER3';


create Target ADLSGen1_tgt6 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'Customer1',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
	rolloverpolicy:'eventcount:10000'
)
format using AvroFormatter(
  formatAs: 'Default',
  handler: 'com.webaction.proc.AvroFormatter',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA@'
)
input from ADLSGen1OrderTypedStream1; 

create Target ADLSGen1_tgt7 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'Customer2',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
    rolloverpolicy:'eventcount:10000'
)
format using AvroFormatter(
  formatAs: 'Default',
  handler: 'com.webaction.proc.AvroFormatter',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA@'
)input from ADLSGen1OrderTypedStream2; 

create Target ADLSGen1_tgt8 using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'Customer3',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
    rolloverpolicy:'eventcount:10000'
)
format using AvroFormatter(
  formatAs: 'Default',
  handler: 'com.webaction.proc.AvroFormatter',
  formatterName: 'AvroFormatter',
  schemaFileName: '@SCHEMA@'
)
input from ADLSGen1OrderTypedStream3; 

end application ManyToManyADLSGen1;

deploy application ManyToManyADLSGen1;
start application ManyToManyADLSGen1;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 1 second interval;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@',
  recordSelector: 'OH:MOH-SEG-ID=OH,OH2:OH2-SEG-ID=OH2,OHU:OHU-SEG-ID=OHU,OR1:OR1-SEG-ID=OR1,OR2:OR2-SEG-ID=OR2,OR3:OR3-SEG-ID=OR3,OR3:OR3-SEG-ID=OR3,OHM:OHM-SEG-ID=OR1,OD:OD-SEG-ID=OD,ODU:ODU-SEG-ID=ODU,OD1:OD1-SEG-ID=OD1,ODM:ODM-SEG-ID=ODM,OT:OT-SEG-ID=OT'
)
OUTPUT TO @APPNAME@Stream;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;

/*
create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:1,Interval:5',
  CommitPolicy: 'EventCount:1,Interval:5',
  Tables: 'QATEST.@table@'
)
input from @APPNAME@Stream;*/
end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

stop application AzureApp2;
undeploy application AzureApp2;
drop application AzureApp2 cascade;

create application AzureApp2
RECOVERY 5 second interval;
create source CSVSource2 using FileReader (
	directory:'@DIR@',
	WildCard:'@WILDCARD@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream2;

Create Type CSVType2 (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream2 of CSVType2;

CREATE CQ CsvToPosData2
INSERT INTO TypedCSVStream2
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream2;

create Target BlobT2 using AzureBlobWriter(
	accountname:'@ACCNAME@',
	accountaccesskey:'@ACCKEY@',
	containername:'@CONT@',
        blobname:'@BLOB@',
	foldername:'@FOLDER@',
	uploadpolicy:'EventCount:100000'
)
format using DSVFormatter (
)
input from TypedCSVStream2;
end application AzureApp2;
deploy application AzureApp2 in default;
start application AzureApp2;

CREATE APPLICATION FtoKaf RECOVERY 2 SECOND INTERVAL;

CREATE OR REPLACE SOURCE FIletoRead USING FileReader  (
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '@TEST-DATA-PATH@/Validate-Striim',
  skipbom: true,
  wildcard: 'FiletoRead.txt'
 )
 PARSE USING DSVParser  (
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: '-1',
  nocolumndelimiter: true,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: false,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 )
OUTPUT TO FtoK1 ;

CREATE  TYPE FtoK2_Type  ( seq java.lang.Integer
 );

CREATE STREAM FtoK2 OF FtoK2_Type;

CREATE OR REPLACE CQ GetData
INSERT INTO FtoK2
SELECT TO_INT(data[0]) as seq
FROM FtoK1;

CREATE OR REPLACE TARGET KafkatoWrite USING KafkaWriter VERSION '0.11.0' (
  KafkaConfigPropertySeparator: ';',
  Mode: 'Sync',
  adapterName: 'KafkaWriter',
  Topic: 'kafkaTopic7',
  brokerAddress: 'localhost:9092',
  KafkaConfigValueSeparator: '=',
  KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000'
 )
FORMAT USING DSVFormatter  (   nullvalue: 'NULL',
  standard: 'none',
  handler: 'com.webaction.proc.DSVFormatter',
  formatterName: 'DSVFormatter',
  usequotes: 'false',
  rowdelimiter: '\n',
  quotecharacter: '\"',
  header: 'false',
  columndelimiter: ','
 )
INPUT FROM FtoK2;

END APPLICATION FtoKaf;

-- Wactionstore has been moved to DSWaction.tql

CREATE APPLICATION MyPosApp;

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


CREATE TYPE PosData(
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);
CREATE STREAM PosDataStream OF PosData PARTITION BY merchantId;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue int,
  hourlyAve int
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;


CREATE STREAM MerchantTxRateOnlyStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId,
       p.zip,
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

CREATE STREAM MerchantTxRateWithStatusStream OF MerchantTxRate PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId,
       zip,
       startTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateOnlyStream;


CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE CACHE NameLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false
)
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;

END APPLICATION MyPosApp;
deploy application MyPosApp;
start application MyPosApp;

stop @appName@;
undeploy application @appName@;
drop application @appName@ cascade;

CREATE OR REPLACE APPLICATION @appName@;

-x-

CREATE SOURCE @appName@_Source USING Global.MysqlReader (
  Username: '@UserName@',
  ConnectionURL: '@ConnectionURL@',
  Password: '@Password@',
  Tables: '@SourceTable@' )
OUTPUT TO @appName@_st1 ;

-x-

CREATE TYPE @appName@_cache_output_Type (
 product_id java.lang.Integer,
 product_name java.lang.String,
 discount java.lang.Integer);

CREATE TYPE @appName@_cache_type (
 p_id java.lang.Integer KEY,
 category java.lang.String,
 quantity java.lang.Integer,
 discount java.lang.Integer);

CREATE OR REPLACE STREAM @appName@_cache_output_st OF @appName@_cache_output_Type;

CREATE OR REPLACE CACHE @appName@_cache_comp USING DatabaseReader (
  Query: 'select * from @LookUpTableForCache@',
  username: '@UserName@',
  FetchSize: 1,
  ConnectionURL: '@ConnectionURL@',
  password: '@Password@' )
QUERY (
  keytomap: 'p_id'
  )
OF  @appName@_cache_type;

CREATE OR REPLACE CQ @appName@_cache_output_cq
INSERT INTO @appName@_cache_output_st
SELECT
      p.data[0] as product_id,
      p.data[1] as product_name,
      pcache.discount as discount
   FROM
   @appName@_st1 p INNER JOIN @appName@_cache_comp pcache ON TO_INT(p.data[0]) = pcache.p_id;

CREATE OR REPLACE TARGET @appName@_cacheTarget USING Global.DatabaseWriter (
  CommitPolicy: 'EventCount:10,Interval:10',
  BatchPolicy: 'EventCount:10,Interval:10',
  Username: '@UserName@',
  ConnectionURL: '@ConnectionURL@',
  Password: '@Password@',
  Tables: '@cache_target_table@' )
INPUT FROM @appName@_cache_output_st;

-x-

CREATE TYPE @appName@_external_cache_type (
 p_id java.lang.Integer KEY,
 category java.lang.String,
 quantity java.lang.Integer,
 discount java.lang.Integer);

CREATE EXTERNAL CACHE @appName@_external_cache_comp (
  Columns: 'p_id,category,quantity,discount',
  KeyToMap: 'p_id',
  AdapterName: 'DatabaseReader',
  Table: '@LookUpTableForExternalCache@',
  Username: '@UserName@',
  ConnectionURL: '@ConnectionURL@',
  Password: '@Password@' )
OF @appName@_external_cache_type;

CREATE CQ @appName@_external_cache_output_cq
INSERT INTO @appName@_external_cache_output_st
SELECT p.data[0] as product_id,p.data[1] as product_name,ecp.quantity as quantity
  FROM @appName@_st1  p inner join @appName@_external_cache_comp ecp
  on TO_INT(p.data[0]) = ecp.p_id;

CREATE OR REPLACE TARGET @appName@_externalCacheTarget USING Global.DatabaseWriter (
  CommitPolicy: 'EventCount:10,Interval:10',
  BatchPolicy: 'EventCount:10,Interval:10',
  Username: '@UserName@',
  ConnectionURL: '@ConnectionURL@',
  Password: '@Password@',
  Tables: '@externalCache_target_table@' )
INPUT FROM @appName@_external_cache_output_st;

-x-

END APPLICATION @appName@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;


create application @appname@ recovery 5 second interval;

CREATE OR REPLACE SOURCE @cobolsrc@ USING FileReader (
  wildcard: '',
  positionbyeof: false,
  directory: ''
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: 'ProcessRecordAsEvent',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'SingleEvent',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'Level01',
  copybookFileName: ''
   )
OUTPUT TO @appname@Stream;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
  filename: '',
  directory: '',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter  (
  members: 'data',
  EventsAsArrayOfJsonObjects: 'true'
 )
INPUT FROM @appname@Stream;

end application @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application MSSQLTransactionSupportMSSQLToMySQL1;
undeploy application MSSQLTransactionSupportMSSQLToMySQL1;
drop application MSSQLTransactionSupportMSSQLToMySQL1 cascade;

CREATE APPLICATION MSSQLTransactionSupportMSSQLToMySQL1 recovery 1 second interval;

Create Source ReadFromMSSQL6
Using MSSqlReader
(
Username:'@READER-NAME@',
Password:'@READER-PASSWORD@',
DatabaseName:'@SRCDB-NAME@',
ConnectionURL:'@CONN-URL@',
Tables:'@WATABLES-SRC@',
TransactionSupport: 'true',
FetchTransactionMetadata:'true',
FilterTransactionBoundaries: true,
Compression:'false',
ConnectionPoolSize:1
)
Output To MSSQLTransactionSupportMSSQLToMySQL1Stream;


CREATE TARGET WriteToMySQL6 USING DatabaseWriter(
ConnectionURL:'@TGT-URL@',
Username:'@WRITER-UNAME@',
Password:'@WRITER-PASSWORD@',
BatchPolicy:'EventCount:5,Interval:5',
CommitPolicy:'EventCount:1,Interval:1',
Tables: '@WATABLES-SRC@,@WATABLES-TGT@'
)
INPUT FROM MSSQLTransactionSupportMSSQLToMySQL1Stream;

CREATE TARGET MSSqlReaderOutput6 USING SysOut(name:MSSqlReaderOutput) INPUT FROM MSSQLTransactionSupportMSSQLToMySQL1Stream; 


CREATE OR REPLACE TARGET MSSQLFileOut6 USING FileWriter  ( 
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  directory:'@FEATURE-DIR@/logs/',
  filename: 'TransactionSupportMSSQLToMySQL.txt'
 ) 
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 ) 
INPUT FROM MSSQLTransactionSupportMSSQLToMySQL1Stream;

END APPLICATION MSSQLTransactionSupportMSSQLToMySQL1;
deploy application MSSQLTransactionSupportMSSQLToMySQL1;
start application MSSQLTransactionSupportMSSQLToMySQL1;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]);

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

STOP APPLICATION snow2FW;
UNDEPLOY APPLICATION snow2FW;
DROP APPLICATION snow2FW CASCADE;
CREATE OR REPLACE APPLICATION snow2FW;

CREATE OR REPLACE SOURCE snow_fw USING Global.ServiceNowReader (
  Mode: 'InitialLoad',
  ServiceNow.ConnectionTimeOut: 60,
  ServiceNow.MaxConnections: 20,
  ServiceNow.FetchSize: 10000,
  ThreadPoolCount: '10',
  ServiceNow.ConnectionRetries: 3,
  PollingInterval: '1',
  ClientSecret: '6Wa-cv`I7x',
  Password: '^Pre&$EMO%6O.e_{96h+$R?rJd,=[4Vt=K)Szh?6g<J9D3,3zs8R;hpZqh]-3?C&.u-@GvSakPXH1:2eygbBDI>ou-z#GjBw[u8x',
  ServiceNow.Tables: 'u_empl',
  UserName: 'snr',
  ClientID: 'ce4fd5af894a11103d2c5c3a8fe075e1',
  adapterName: 'ServiceNowReader',
  ServiceNow.BatchAPI: true,
  ServiceNow.ConnectionUrl: 'https://dev84954.service-now.com/' )
OUTPUT TO sn;


CREATE TARGET ft USING Global.FileWriter (
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000,Interval:30s',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  DataEncryptionKeyPassphrase_encrypted: 'true',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  filename: 'dt' )
FORMAT USING Global.JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n' )
INPUT FROM sn;

CREATE TARGET pg_target USING Global.DatabaseWriter (

  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
    Password: 'w@ct10n',
    Tables: 'u_empl,u_empl ColumnMap(name=u_name,age=u_age,address=u_address,sys_id=sys_id)',
    ParallelThreads: '',
    CheckPointTable: 'CHKPOINT',
    CDDLAction: 'Process',
    ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
    CommitPolicy: 'EventCount:1000,Interval:60',
    StatementCacheSize: '50',
    Username: 'waction',
    DatabaseProviderType: 'Postgres',
    BatchPolicy: 'EventCount:1000,Interval:60',
    PreserveSourceTransactionBoundary: 'false' )
  INPUT FROM sn;

END APPLICATION snow2FW;
deploy application snow2FW;
start snow2FW;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @APPNAME@_src Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
   _h_ReturnDateTimeAs:'ZonedDateTime'
) Output To @APPNAME@_stream;

create Target @APPNAME@_tgt using FileWriter(
  filename:'CompressedMerchant.gz',
  directory:'/logs/',
  rolloverpolicy:'EventCount:10000'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_stream;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

-- The PosAppAgent sample application demonstrates how a credit card
-- payment processor might use Striim to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.


CREATE APPLICATION PosAppAgent;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosAppAgent application.

-- source CsvAgentDataSource

CREATE FLOW AgentFlow;

CREATE source CsvAgentDataSource USING FileReader (
  directory:'/opt/striim/Samples/PosApp/appData',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvAgentStream;

END FLOW AgentFlow;

-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells Striim that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells Striim to start reading
-- from the beginning of the file. (In a real-world application 
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvAgentStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvAgentToPosDataCq

CREATE FLOW ProcessFlow;

CREATE CQ CsvAgentToPosDataCq
INSERT INTO PosDataAgentStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM CsvAgentStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP.
--
-- The TO_STRING, TO_DATEF, and TO_DOUBLE functions cast the fields as 
-- the types to be used in PosDataAgentStream, which is created automatically.

-- The DATETIME field from the source is converted to both a dateTime 
-- value, used as the event timestamp by the application, and (via the 
-- function) an integer hourValue, which is used to look up 
-- historical hourly averages from the HourlyAgentAveLookup cache, 
-- discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)
-- 17 (hourValue)
-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateAgentOnly
--
-- The AgentPosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAgentAveLookup cache. (Aggregate functions cannot be used and 
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAgentAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW AgentPosData5Minutes
OVER PosDataAgentStream KEEP WITHIN 5 MINUTE ON dateTime PARTITION BY merchantId;

CREATE TYPE MerchantAgentHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);
CREATE CACHE HourlyAgentAveLookup using FileReader (
  directory: 'Samples/PosApp/appData',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'merchantId') OF MerchantAgentHourlyAve;

CREATE TYPE MerchantTxRateAgent(
  merchantId String KEY,
  zip String,
  startTime DateTime,
  count integer,
  totalAmount double,
  hourlyAve integer,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);
CREATE STREAM MerchantTxRateAgentOnlyStream OF MerchantTxRateAgent PARTITION BY merchantId;

CREATE CQ GenerateMerchantTxRateAgentOnly
INSERT INTO MerchantTxRateAgentOnlyStream
SELECT p.merchantId,
       FIRST(p.zip),
       FIRST(p.dateTime),
       COUNT(p.merchantId),
       SUM(p.amount),
       FIRST(l.hourlyAve/12),
       FIRST(l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       FIRST(l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END),
       '<NOTSET>',
       '<NOTSET>'
FROM AgentPosData5Minutes p, HourlyAgentAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAgentAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateAgentWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE STREAM MerchantTxRateAgentWithStatusStream OF MerchantTxRateAgent;

CREATE CQ GenerateMerchantTxRateAgentWithStatus
INSERT INTO MerchantTxRateAgentWithStatusStream
SELECT merchantId,
       zip,
       startTime,
       count,
       totalAmount,
       hourlyAve,
       upperLimit,
       lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END
FROM MerchantTxRateAgentOnlyStream;


-- WAction store MerchantActivityAgent
--
-- The following group of statements create and populate the MerchantActivityAgent
-- WAction store. Data from the MerchantTxRateAgentWithStatusStream is enhanced
-- with merchant details from NameLookupAgent cache and with latitude and longitude
-- values from the USAddressDataAgent cache.

CREATE TYPE MerchantActivityAgentContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count integer,
  HourlyAve integer,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);

CREATE WACTIONSTORE MerchantActivityAgent CONTEXT OF MerchantActivityAgentContext
EVENT TYPES ( MerchantTxRateAgent )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );


CREATE TYPE MerchantAgentNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressDataAgent(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookupAgent using FileReader (
  directory:'Samples/PosApp/appData',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false,
  trimwhitespace:true
) 
QUERY(keytomap:'merchantId') OF MerchantAgentNameData;

CREATE CACHE ZipLookupAgent using FileReader (
  directory: 'Samples/PosApp/appData',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false,
  trimwhitespace:true
) QUERY (keytomap:'zip') OF USAddressDataAgent;


CREATE CQ GenerateWactionAgentContext
INSERT INTO MerchantActivityAgent
SELECT  m.merchantId,
        m.startTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateAgentWithStatusStream m, NameLookupAgent n, ZipLookupAgent z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;

-- CQ GenerateAgentAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertAgentStream OF Global.AlertEvent;

CREATE CQ GenerateAgentAlerts
INSERT INTO AlertAgentStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateAgentWithStatusStream m, NameLookupAgent n
WHERE m.merchantId = n.merchantId;

CREATE SUBSCRIPTION AgentAlertSub USING WebAlertAdapter( ) INPUT FROM AlertAgentStream;

END FLOW ProcessFlow;

END APPLICATION PosAppAgent;

DEPLOY APPLICATION PosAppAgent with AgentFlow in AGENTS, ProcessFlow in default;

-- CREATE DASHBOARD USING "Samples/PosApp/PosAppDashboard.json";

use RetailTester;
DROP FLOW RetailSourceFlow cascade;

STOP OuterJoinTester.OJApp;
UNDEPLOY APPLICATION OuterJoinTester.OJApp;
DROP APPLICATION OuterJoinTester.OJApp cascade;

CREATE APPLICATION OJApp;



CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;



CREATE source wsSource2 USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'bankCards.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO stream2;



CREATE TYPE bankData
(
bankID Integer KEY,
bankName String
);

CREATE TYPE cardData
(
cardID Integer KEY,
cardName String
);


CREATE STREAM wsStream OF bankData;
CREATE STREAM wsStream2 OF cardData;

CREATE JUMPING WINDOW win1 OVER wsStream KEEP 20 rows;


CREATE JUMPING WINDOW win2 OVER stream2 KEEP 4 rows;

CREATE WACTIONSTORE oneWS CONTEXT OF bankData
EVENT TYPES(bankData )
@PERSIST-TYPE@

CREATE WACTIONSTORE twoWS CONTEXT OF cardData
EVENT TYPES(cardData )
@PERSIST-TYPE@


--Select data from QaStream and insert into wsStream

CREATE CQ csvTobankData
INSERT INTO oneWS
SELECT TO_INT(data[0]), data[1] FROM QaStream;

CREATE CQ csvTobankData2
INSERT INTO twoWS
SELECT TO_INT(data[0]), data[1] FROM stream2;


END APPLICATION OJApp;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;
CREATE APPLICATION OracleToKudu RECOVERY 1 SECOND INTERVAL;
CREATE  SOURCE oracSource USING OracleReader  ( 
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@//192.168.56.102:1521/orcl',
  Tables: 'QATEST.oracle_alldatatypes',
  OnlineCatalog: true,
  FetchSize: 1
 ) 
OUTPUT TO DataStream;
CREATE  TARGET WriteintoKudu USING KuduWriter  ( 
  kuduclientconfig: 'master.addresses->192.168.56.101:7051;socketreadtimeout->240;operationtimeout->1200',
  pkupdatehandlingmode: 'DELETEANDINSERT',
  tables: 'QATEST.ORACLE_ALLDATATYPES,KUDU_ALLDATATYPES',
  batchpolicy: 'EventCount:1,Interval:0'
 ) 
INPUT FROM DataStream;
END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

--
-- Recovery Test 12 with two sources, two jumping attribute windows, one wactionstore with recovery, and another wactionstore without -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W/p -> CQ1 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS2 (no persists)
--

STOP Recov12Tester.RecovTest12;
UNDEPLOY APPLICATION Recov12Tester.RecovTest12;
DROP APPLICATION Recov12Tester.RecovTest12 CASCADE;
CREATE APPLICATION RecovTest12 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsNoPersist CONTEXT OF WactionData
EVENT TYPES ( CsvData )
		PERSIST NONE USING ( ) ;

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWactionNoPersist
INSERT INTO WactionsNoPersist
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION RecovTest12;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCENAME@ USING IncrementalBatchReader  (
  FetchSize: 10,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: '@startPosition@',
  PollingInterval: '20sec'
  )
  OUTPUT TO @STREAM@;

  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;

  CREATE TARGET @targetName1@ USING DatabaseWriter(
    ConnectionURL:'@READER-URL@',
    Username:'@READER-UNAME@',
    Password:'@READER-PASSWORD@',
    BatchPolicy:'Eventcount:1,Interval:1',
    CommitPolicy:'Eventcount:1,Interval:1',
    Checkpointtable:'CHKPOINT',
    Tables:'@WATABLES@,@WATABLES@_target'
  ) INPUT FROM @STREAM@;

  create Target @targetsys1@ using SysOut(name:@targetsys1@) input from @STREAM@;

    CREATE TARGET @targetName2@ USING DatabaseWriter(
      ConnectionURL:'@READER-URL@',
      Username:'@READER-UNAME@',
      Password:'@READER-PASSWORD@',
      BatchPolicy:'Eventcount:1,Interval:1',
      CommitPolicy:'Eventcount:1,Interval:1',
      Checkpointtable:'CHKPOINT',
      Tables:'@WATABLES@,@WATABLES@_target1'
    ) INPUT FROM @STREAM@;

  END APPLICATION @APPNAME@;

  DEPLOY APPLICATION @APPNAME@;
  start application @APPNAME@;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;

drop propertyset MysqlToMysqlPlatfm_App_KafkaPropset;
drop stream  MysqlToMysqlPlatfm_Stream CASCADE;


--CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

--CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;
					
CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using MySQLReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNEC4TION_URL@',
  Tables: '$table1',
  adapterName: 'MySQLReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using MySQLReader( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@2',
  adapterName: 'MySQLReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'WACTION.MYSQLTOMYSQLPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using MySQLReader( 
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '$table2',
  adapterName: 'MySQLReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using MySQLReader(
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'WACTION.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@TABLENAME@4',
  adapterName: 'MySQLReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OfflineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic: true
)OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'WACTION.MYSQLTOMYSQLPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;

END APPLICATION @APPNAME@2;

STOP TestAlertsSMS.TestAlertsSmsApp;
UNDEPLOY APPLICATION TestAlertsSMS.TestAlertsSmsApp;
DROP APPLICATION TestAlertsSMS.TestAlertsSmsApp CASCADE;

CREATE APPLICATION TestAlertsSmsApp;

CREATE source rawSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:No,
  wildcard:'alerts_csv.txt',
  coldelimiter:' ',
  positionByEOF:false
) OUTPUT TO rawStream;

CREATE STREAM MyAlertStream OF Global.AlertEvent;

CREATE CQ GenerateMyAlerts
INSERT INTO MyAlertStream (name, keyVal, severity, flag, message)
SELECT "Testing Alerts", trimStr(data[0]), trimStr(data[1]), trimStr(data[2]), trimStr(data[3])
FROM rawStream s;

CREATE TARGET output2 USING SysOut(name : alertsrecevied) input FROM MyAlertStream;

CREATE SUBSCRIPTION smsAlertSubscription4 USING ClickatellSMSAdapter
(
clickatelluserName:"@Alerts_smsuser@",
CLICKaTELLpasSWORD:"@Alerts_smspassword@",
clickatellapiID:"@Alerts_smsid@",
userIds:"@Alerts_smsuserid@",
threadCount:"@Alerts_smsthreadcount@",
senderNumber:"@Alerts_smssenderno@",
phoneNumberList:"@Alerts_smsphonelist@" ) INPUT FROM MyAlertStream;

END APPLICATION TestAlertsSmsApp;
DEPLOY APPLICATION TestAlertsSMS.TestAlertsSmsApp;
START TestAlertsSMS.TestAlertsSmsApp;

CREATE TARGET @TARGET_NAME@ USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
) INPUT FROM @STREAM@;

DROP APPLICATION ns1.OPExample cascade;
DROP NAMESPACE ns1 cascade;
CREATE OR REPLACE NAMESPACE ns1;
USE ns1;
CREATE APPLICATION OPExample;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'PosDataPreview.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
)
OUTPUT TO CsvStream;
 
CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue integer,
  hourlyAve integer
);

CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false,
  trimwhitespace:true
) 
QUERY (keytomap:'merchantId') 
OF MerchantHourlyAve;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream partition by merchantId
SELECT TO_STRING(data[1]) as merchantId,
  TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
  DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
  TO_DOUBLE(data[7]) as amount,
  TO_INT(data[9]) as zip
FROM CsvStream;
 
CREATE CQ cq2
INSERT INTO SendToOPStream
SELECT makeList(dateTime) as dateTime,
  makeList(zip) as zip
FROM PosDataStream;
 
CREATE TYPE ReturnFromOPStream_Type ( time DateTime , val Integer );
CREATE STREAM ReturnFromOPStream OF ReturnFromOPStream_Type;

CREATE TARGET OPExampleTarget 
USING FileWriter (filename: 'OPExampleOut') 
FORMAT USING JSONFormatter() 
INPUT FROM ReturnFromOPStream;
 
END APPLICATION OPExample;

stop APPLICATION @AppName@;
Undeploy APPLICATION @AppName@;
drop APPLICATION @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @AgentFlow@;

CREATE OR REPLACE SOURCE @SourceName@ USING Global.MSJet (
  Tables: 'dbo.compsrc',
  username: 'qatest',
  DatabaseName: 'qatest',
  FetchTransactionMetadata: true,
  filterTransactionBoundaries: true,
  compression: false,
  ConnectionURL: '@ConnectionURL@',
  Mode: '@mode@',
  CommittedTransactions: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
  SendBeforeImage: true,
  password: 'w3b@ct10n' )
OUTPUT TO @StreamName@;
END FLOW @AgentFlow@;

CREATE TARGET @SysTarget@ USING Global.SysOut (
  name: 'MS_CDC_SYSOUT' )
INPUT FROM @StreamName@;

CREATE FLOW @ServerFlow@;

CREATE TARGET @TargetName@ USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ParallelThreads: '',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: '@ConnectionURL@',
  StatementCacheSize: '50',
  DatabaseProviderType: 'Default',
  Username: 'qatest',
  Tables: 'dbo.compsrc,dbo.comptar',
  Password: 'w3b@ct10n',
  PreserveSourceTransactionBoundary: 'false',
  BatchPolicy: 'EventCount:1,Interval:60' )
INPUT FROM @StreamName@;

END FLOW @ServerFlow@;

END APPLICATION @AppName@;
DEPLOY APPLICATION @AppName@ with @AgentFlow@ in AGENTS ,@ServerFlow@ on any in default;
START APPLICATION @AppName@;

STOP APPLICATION testApp;
UNDEPLOY APPLICATION testApp;
DROP APPLICATION testApp CASCADE;
CREATE APPLICATION testApp recovery 5 SECOND Interval;


  CREATE OR REPLACE SOURCE testApp_Source Using PostgreSQLReader( 
  
  ReplicationSlotName:'test_slot',
  FilterTransactionBoundaries:'true',
  Username:'waction',
  Password_encrypted:false,
  ConnectionURL:'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  adapterName:'PostgreSQLReader',
  ConnectionRetryPolicy:'retryInterval=30, maxRetries=3',
  Password:'w@ct10n',
  Tables:'public.sourceTable',
  ExcludedTables:'public.chkpoint'
 ) OUTPUT TO PGtoBQ_Stream;


CREATE OR REPLACE TARGET testApp_Target USING BigQueryWriter  (
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  projectId:'striimqa-214712',
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:5,Interval:120',
  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', 
  AllowQuotedNewLines: 'false', 
  adapterName: 'BigQueryWriter', 
  optimizedMerge: 'true', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', 
  StandardSQL: 'true', 
  QuoteCharacter: '\"', 
  Tables: 'public.sourceTable,BQAllpl.oratobqtgt',
  Mode: 'MERGE',
  StandardSQL: 'true',
  QuoteCharacter: '\"'
  ) INPUT FROM PGtoBQ_Stream;

CREATE OR REPLACE TARGET testApp_SysOut USING Global.SysOut (name: 'wa') INPUT FROM PGtoBQ_Stream;

END APPLICATION testApp;
DEPLOY APPLICATION testApp;
START testApp;

--
-- Recovery Test 2
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS
--


STOP KStreamRecov2Tester.KStreamRecovTest2;
UNDEPLOY APPLICATION KStreamRecov2Tester.KStreamRecovTest2;
DROP APPLICATION KStreamRecov2Tester.KStreamRecovTest2 CASCADE;
DROP USER KStreamRecov2Tester;
DROP NAMESPACE KStreamRecov2Tester CASCADE;
CREATE USER KStreamRecov2Tester IDENTIFIED BY KStreamRecov2Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov2Tester;
CONNECT KStreamRecov2Tester KStreamRecov2Tester;

CREATE APPLICATION KStreamRecovTest2 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE TYPE WactionType (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionType;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION KStreamRecovTest2;

CREATE APPLICATION TestAPP
CREATE TYPE TestType(
    id java.lang.Long KEY,
    name java.lang.String
);

CREATE STREAM TestStream OF TestType;




CREATE CQ TestCQ
INSERT INTO TestStream
SELECT
    h.value,
    TO_STRING(DNOW())
FROM heartbeat(interval 1 second) h;

END APPLICATION TestAPP;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;
DROP EXCEPTIONSTORE @APP_NAME@_EXCEPTIONSTORE;

CREATE APPLICATION @APP_NAME@ WITH ENCRYPTION RECOVERY 2 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE SOURCE @APP_NAME@_Source USING @SOURCE_ADAPTER@(
)OUTPUT TO @APP_NAME@DataStream;


CREATE TARGET @APP_NAME@_Target USING @TARGET_ADAPTER@( 
) INPUT FROM @APP_NAME@DataStream;


CREATE OR REPLACE TARGET @APP_NAME@_SysOut USING Global.SysOut(
	name: 'waEvent'
) INPUT FROM @APP_NAME@DataStream;

END APPLICATION @APP_NAME@;

DEPLOY APPLICATION @APP_NAME@ in default;
START APPLICATION @APP_NAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE @SOURCE@ USING Ojet  (
  FilterTransactionBoundaries: true,
  ConnectionURL: '@OCI-URL@',
  Tables: '@SOURCE_TABLE@',
  Password: '@OJET-PASSWORD@',
  fetchsize: 1,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Username: '@OJET-UNAME@'
 )
OUTPUT TO @STREAM@;

CREATE OR REPLACE TARGET @TARGET@1 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'false',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@2 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'false',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'true',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@3 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'true',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

END APPLICATION @APPNAME@;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;
Create Source oracSource
 Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;

CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;


 CREATE SOURCE DBSource USING MySQLReader (
 Compression: true,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: '@CONNECTION_URL@',
  DatabaseName: 'testcassandra',
  Tables: '@SOURCE_TABLE@',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)
OUTPUT TO Mysql_ChangeDataStream;
CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'test.chkpoint',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  IgnorableExceptionCode:'PRIMARY KEY',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM Mysql_ChangeDataStream;
create Target t2 using SysOut(name:Foo2) input from Mysql_ChangeDataStream;

END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start application DBRTOCW;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW ;

 

CREATE OR REPLACE SOURCE DBSource USING DatabaseReader  ( 
  Username: '@SOURCE_USER@',
  Password_encrypted: false,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SOURCE_PASS@'
 ) 
OUTPUT TO Oracle_ChangeDataStream;


CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1000,Interval:0',
  CommitPolicy: 'EventCount:1000,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start DBRTOCW;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING PostgreSQLReader  (
  ReplicationSlotName: 'Slot_Name',
  FilterTransactionBoundaries: 'true',
  Username: 'User_Name',
  ConnectionURL: 'Connection_URL',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'Password',
  Tables: 'Tables',
  PostgresConfig:'{"ReplicationPluginConfig": {"Name": "WAL2JSON", "Format": "2"}}'
 )
OUTPUT TO @STREAM@ ;

CREATE TARGET @SOURCE_NAME@_sysout USING Global.SysOut (
  name: '@SOURCE_NAME@_sysout' )
INPUT FROM @STREAM@;

CREATE CQ @CQ_NAME@
INSERT INTO @EMB_STREAM@
@SELECT_QUERY@
FROM @STREAM@ e;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;
CREATE APPLICATION DSV;

Create type ScanResultType (
  timestamp1 String,
  rssi String
);

Create Stream ScanResultStream of ScanResultType;

CREATE  SOURCE CSVSource USING FileReader (
  directory: '@TEST-DATA-PATH@',
  WildCard: 'sample.json',
  positionByEOF: false
 )
 PARSE USING JSONParser (
  eventType: 'RollOverTester.ScanResultType',
  fieldName: 'scanresult'
 )
OUTPUT TO ScanResultStream;

CREATE TARGET KafkaSYSOUT USING FileWriter (
  filename: 'EventType',
  flushinterval: '0',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy: 'eventcount:1,sequence:00'
 )
format using JSONFormatter (
  members:'timestamp1,rssi'
)
INPUT FROM ScanResultStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/EventType_actual.log') input from ScanResultStream;

END APPLICATION DSV;