stop application MySQLToSQLServer;
undeploy application MySQLToSQLServer;
drop application MySQLToSQLServer cascade;

CREATE APPLICATION MySQLToSQLServer recovery 1 second interval;

create source Src1ReadFromMySQL USING MySQLReader (
Username: 'root',
  Password: 'w@ct10n',
  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',
--Tables: 'waction.mytable1;waction.mytable2;qatest.mysqlmarker',
Tables: 'waction.Parent%;waction.Child%;',
--Tables: 'waction.mytable%',
BiDirectionalMarkerTable: 'waction.mysqlmarker',
compression: 'True',
sendBeforeImage:True
) OUTPUT TO App1Stream;


CREATE  TARGET Src1ReadFromMySQL_Out USING FileWriter  (
  filename: 'Src1ReadFromMySQL_Out.log',
flushpolicy: 'eventcount:1',
rolloverpolicy: 'eventcount:10000'
 )
FORMAT USING JSONFORMATTER  (
 )
INPUT FROM App1Stream;


CREATE TARGET WriteToMSSQL1 USING DatabaseWriter(
ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',
Username:'qatest',
Password:'w3b@ct10n',
BatchPolicy:'EventCount:8,Interval:5',
CommitPolicy:'EventCount:10,Interval:5',
BiDirectionalMarkerTable: 'qatest.mysqlmarker',
--Tables: 'waction.mytable1,qatest.mytable1;'
Tables: 'waction.Parent%,qatest.%;waction.Child%,qatest.%;'
--Tables: 'waction.mytable%,qatest.%'
)
INPUT FROM App1Stream;


--MSSQl tp MySQL

Create Source SrcReadFromMSSQL
Using MSSqlReader
(
Username:'qatest',
Password:'w3b@ct10n',
DatabaseName:'qatest',
ConnectionURL:'localhost:1433',
--Tables:'qatest.mytable1;qatest.mytable2;qatest.mysqlmarker;qatest.mysqlmarker2',
Tables:'qatest.Parent%;qatest.Child%',
--Tables:'qatest.mytable%',
BiDirectionalMarkerTable: 'qatest.mysqlmarker',
TransactionSupport: 'true',
FilterTransactionBoundaries: true,
Compression: 'True',
ConnectionPoolSize:1
)
Output To App2Stream;


CREATE  TARGET SrcReadFromMSSQL_Out USING FileWriter  (
  filename: 'SrcReadFromMSSQL_Out.log',
flushpolicy: 'eventcount:1',
rolloverpolicy: 'eventcount:10000'
 )
FORMAT USING JSONFORMATTER  (
 )
INPUT FROM App2Stream;


CREATE TARGET WriteToMySQL USING DatabaseWriter(
ConnectionURL:'jdbc:mysql://localhost:3306/waction',
Username:'root',
Password:'w@ct10n',
BatchPolicy:'EventCount:10,Interval:10',
CommitPolicy: 'EventCount:15,Interval:12',
BiDirectionalMarkerTable: 'waction.mysqlmarker',
--Tables: 'qatest.mytable1,waction.mytable1;qatest.mytable2,waction.mytable3;'
Tables: 'qatest.Parent_1,waction.Parent_1;qatest.Parent_2,waction.Parent_2;qatest.Child_1,waction.Child_1;qatest.Child_2,waction.Child_2;'
--Tables: 'qatest.mytable%,waction.%'
)
INPUT FROM App2Stream;

/*
CREATE OR REPLACE TARGET MSSQLDDLFileOut USING FileWriter  (
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
  filename: 'MSSQL.txt'
 )
FORMAT USING JSONFormatter  (   jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  jsonobjectdelimiter: '\n'
 )
INPUT FROM App2Stream;

*/

END APPLICATION MySQLToSQLServer;
deploy application MySQLToSQLServer;
start application MySQLToSQLServer;

use PosTester;
DROP TYPE MerchantTxRate;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING @SOURCE@ ()
PARSE USING ParquetParser (
  retryWait: '1m'
)
OUTPUT TO @APPNAME@_Stream;

CREATE CQ @APPNAME@_CQ
INSERT INTO @APPNAME@_CQOut
SELECT putUserData(x, 'folderName','Parquet') FROM @APPNAME@_Stream x;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING @TARGET@ ()
FORMAT USING ParquetFormatter (
  schemaFileName: 'parquetSchema',
  members:'data'
)
INPUT FROM @APPNAME@_CQOut;

END APPLICATION @APPNAME@;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;
Create Source oracSource Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;

CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

STOP virtualTester.VirtualApp;
UNDEPLOY APPLICATION virtualTester.VirtualApp;
DROP APPLICATION virtualTester.VirtualApp cascade;

CREATE APPLICATION VirtualApp;


CREATE OR REPLACE SOURCE wsSource USING FileReader
(
  directory:'@TEST-DATA-PATH@',
  wildcard:'AdhocQueryData2.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser
(
   header:True,
   columndelimiter:',',
   trimquote:false
)OUTPUT TO QaStream;

CREATE OR REPLACE Target TheData using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/QaStream.log') input from QaStream;

CREATE OR REPLACE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE OR REPLACE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'AdhocQueryData.csv'
)
parse using DSVParser
(
  header:'yes',
  columndelimiter: '	',
  trimquote:false

)QUERY (keytomap:'zip') OF USAddressData;

Create OR REPLACE TYPE wsData(
  CompanyNum String,
  CompanyName String KEY,
  CompanyCode int,
  Zip String
);


CREATE OR REPLACE STREAM wsStream OF wsData;

--Select data from QaStream and insert into wsStream

CREATE OR REPLACE CQ csvTowsData
INSERT INTO wsStream
SELECT data[0],
       data[1],
       TO_INT(data[3]),
       data[9]
FROM QaStream;


CREATE OR REPLACE WACTIONSTORE oneWS CONTEXT OF wsData
  EVENT TYPES (wsData );


CREATE OR REPLACE CQ wsToWaction
  INSERT INTO oneWS
  SELECT * FROM wsStream
  LINK SOURCE EVENT;

END APPLICATION VirtualApp;

STOP APPLICATION OJETTOBIGQUERY;
UNDEPLOY APPLICATION OJETTOBIGQUERY;
DROP APPLICATION OJETTOBIGQUERY CASCADE;

--create application 
CREATE APPLICATION OJETTOBIGQUERY RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE ojetSrc USING Ojet (
 ConnectionURL: '192.168.123.12:1521/ORCL',
 Tables: 'QATEST.OJETTOBIGQALLDATATYPE',
 Username: 'qatest',
 Password: 'qatest',
 FetchSize:1
) OUTPUT TO CDCStream;

CREATE OR REPLACE TARGET bqtables using BigqueryWriter(
 serviceAccountKey:"/Users/karthikmurugan/Downloads/bqtest-540227c31980.json",
 projectId:"bqtest-158706",
 datalocation: 'US',
 Tables: "QATEST.OJETTOBIGQALLDATATYPE,QATEST.OJETTOBIGQALLDATATYPE",
 BatchPolicy: "eventCount:1,Interval:90")
INPUT FROM CDCStream;


CREATE OR REPLACE TARGET T1 using SysOut(name :Foo2out) INPUT FROM CDCStream;

END APPLICATION OJETTOBIGQUERY;

DEPLOY APPLICATION OJETTOBIGQUERY;
START APPLICATION OJETTOBIGQUERY;

--
-- Recovery Test 50 is a simple Kafka test
-- Nicholas Keene WebAction, Inc.
--
-- S -> CQ -> KSu -> JWc10 -> WS
--

STOP Recov50Tester.RecovTest50;
UNDEPLOY APPLICATION Recov50Tester.RecovTest50;
DROP APPLICATION Recov50Tester.RecovTest50 CASCADE;
CREATE APPLICATION RecovTest50 RECOVERY 5 SECOND INTERVAL;

CREATE or REPLACE TYPE KafkaType(
  value java.lang.Long KEY  
);

CREATE SOURCE KafkaSource USING NumberSource ( 
  lowValue: '1',
  highValue: '1003',
  delayMillis: '10',
  delayNanos: '0',
  repeat: 'false'
 ) 
OUTPUT TO NumberSourceOut;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaStream OF KafkaType using KafkaProps;

CREATE OR REPLACE CQ KafkaStreamPopulate 
INSERT INTO KafkaStream
SELECT data[1]
FROM NumberSourceOut;

CREATE JUMPING WINDOW SizeTenWindow
OVER KafkaStream KEEP 10 ROWS;


CREATE WACTIONSTORE Wactions CONTEXT of KafkaType
@PERSIST-TYPE@

CREATE CQ WactionsPopulate
INSERT INTO Wactions
SELECT * FROM SizeTenWindow;

END APPLICATION RecovTest50;

stop MySQLToHBase;
undeploy application MySQLToHBase;
drop application MySQLToHBase cascade;

CREATE APPLICATION MySQLToHBase RECOVERY 5 SECOND INTERVAL;;

CREATE SOURCE MySQLCDCIn USING MySQLReader (
Username:'root',
Password:'root',
ConnectionURL:'mysql://192.168.123.14:3306',
Database:'qatest',
Tables:'qatest.BUSINESS'
)
OUTPUT TO DataStream;

CREATE TARGET MySQLCDCOut
USING SysOut(name:MySQLCDC)
INPUT FROM DataStream;

CREATE OR REPLACE TARGET Target2 using HBaseWriter(
  HBaseConfigurationPath:"/Users/ravipathak/soft/hbase-1.1.5/conf/hbase-site.xml",
  Tables: "qatest.BUSINESS,PKTEST.data",
  PKUpdateHandlingMode: "DELETEANDINSERT",
  BatchPolicy: "eventCount:1")
INPUT FROM DataStream;

END APPLICATION MySQLToHBase;

deploy application MySQLToHBase;
start MySQLToHBase;

STOP APPLICATION @appname@routerApp;
UNDEPLOY APPLICATION @appname@routerApp;
DROP APPLICATION @appname@routerApp CASCADE;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'kafkaPropSetVersion', batch.size:'800000',
  acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

CREATE APPLICATION @appname@routerApp RECOVERY 10 SECOND INTERVAL;

CREATE  SOURCE @appname@OraSource USING OracleReader  (
Username: 'qatest',
Password: 'qatest',
ConnectionURL: 'jdbc:oracle:thin:@dockerhost:1521:xe',
Tables: 'QATEST.TGT_T%',
 FetchSize:'100'
)
OUTPUT TO @appname@MasterStream1;

-- CREATE STREAM @appname@ss1 OF Global.waevent persist using Global.DefaultKafkaProperties;
-- CREATE STREAM @appname@ss2 OF Global.waevent persist using Global.DefaultKafkaProperties;
-- CREATE STREAM @appname@ss3 OF Global.waevent persist using Global.DefaultKafkaProperties;

CREATE STREAM @appname@ss1 OF Global.waevent PERSIST USING KafkaPropset;
CREATE STREAM @appname@ss2 OF Global.waevent PERSIST USING KafkaPropset;
CREATE STREAM @appname@ss3 OF Global.waevent PERSIST USING KafkaPropset;

CREATE OR REPLACE ROUTER @appname@tablerouter1 INPUT FROM @appname@MasterStream1 s CASE
WHEN meta(s,"TableName").toString()='QATEST.TGT_T1' THEN ROUTE TO @appname@ss1,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T2' THEN ROUTE TO @appname@ss2,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T3' THEN ROUTE TO @appname@ss3,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T4' THEN ROUTE TO @appname@ss4,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T5' THEN ROUTE TO @appname@ss5,
WHEN meta(s,"TableName").toString()='QATEST.TGT_T6' THEN ROUTE TO @appname@ss6,
ELSE ROUTE TO @appname@ss_else;

create Target @appname@FileTarget_1 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'
)
FORMAT USING dsvFormatter ()
input from @appname@ss1;

create Target @appname@FileTarget_2 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'
)
FORMAT USING dsvFormatter ()
input from @appname@ss2;

create Target @appname@FileTarget_3 using FileWriter
(
directory: 'testSep17',
filename:'%@metadata(TableName)%'

)
FORMAT USING dsvFormatter ()
input from @appname@ss3;

CREATE OR REPLACE TARGET @appname@KafkaTarget_4 USING KafkaWriter VERSION '0.11.0' (
  brokerAddress: 'localhost:9092',
  Topic: 'target4'
 )
FORMAT USING JSONFormatter  (
 )
INPUT FROM @appname@ss4;

CREATE OR REPLACE TARGET @appname@KafkaTarget_5 USING KafkaWriter VERSION '0.11.0' (
  brokerAddress: 'localhost:9092',
  Topic: 'target5'
 )
FORMAT USING JSONFormatter  (
 )
INPUT FROM @appname@ss5;

CREATE OR REPLACE TARGET @appname@KafkaTarget_6 USING KafkaWriter VERSION '0.11.0' (
  brokerAddress: 'localhost:9092',
  Topic: 'target6'
 )
FORMAT USING JSONFormatter  (
 )
INPUT FROM @appname@ss6;




end application @appname@routerApp;
deploy application @appname@routerApp;
start @appname@routerApp;

STOP @appName@;
UNDEPLOY application @appName@;
DROP application @appName@ cascade;

CREATE APPLICATION @appName@ @recovery@;

CREATE SOURCE @appName@_MongoDBReader USING MongoDBReader
(
  QuiesceOnILCompletion:'true',
  collections:'',
  userName:'',
  password:'',
  connectionUrl:'',
  mode:''
)
OUTPUT TO @appName@_MOut;

CREATE CQ @appName@_MCQ1
INSERT INTO @appName@_MCQOut1
SELECT
data.get('_id').textValue() as _id,
data.get('index').toString() as index,
data.get('isActive').toString() as isActive,
data.get('age').toString() as age,
data.get('balance').toString() as balance,
data.get('name').textValue() as name,
data.get('gender').textValue() as gender,
data.get('company').textValue() as company,
data.get('email').textValue() as email,
data.get('phone').textValue() as phone,
data.get('registered').textValue() as registered,
data.get('latitude').toString() as latitude,
data.get('longitude').toString() as longitude
FROM @appName@_MOut m;;

CREATE TARGET @appName@_AzureEventHubWriter USING AzureEventHubWriter
(
  SASKey:'',
  EventHubName:'',
  EventHubNamespace:'',
  SASPolicyName:'',
  BatchPolicy:'Size:1000000,Interval:10s',
  ConsumerGroup:'default'
)
FORMAT USING JSONFormatter  (
 )
INPUT FROM @appName@_MCQOut1;

END APPLICATION @appName@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@ recovery 1 second interval;

create source @APPNAME@_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using CobolCopybookParser (
copybookFileName : '@TD@/@PROP1@',
  dataFileFont: '@PROP2@',
  copybookSplit: '@PROP3@',
  dataFileOrganization: '@PROP4@',
  copybookDialect: '@PROP5@', 
  skipIndent:'@PROP6@',
  DatahandlingScheme:'@PROP7@'
  --recordSelector: '@PROP8@'
)
OUTPUT TO @APPNAME@Stream;

create Target @APPNAME@Target using FileWriter(
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JsonFormatter (
)
input from @APPNAME@Stream;

CREATE TYPE test_type (
 account_no com.fasterxml.jackson.databind.JsonNode,
 first_name com.fasterxml.jackson.databind.JsonNode,
 last_name com.fasterxml.jackson.databind.JsonNode,
 addr1 com.fasterxml.jackson.databind.JsonNode,
Addr2 com.fasterxml.jackson.databind.JsonNode,
City com.fasterxml.jackson.databind.JsonNode,
State com.fasterxml.jackson.databind.JsonNode,
Zip com.fasterxml.jackson.databind.JsonNode
);

Create stream cqAsJSONNodeStream of test_type;

CREATE CQ GetPOAsJsonNodes
INSERT into cqAsJSONNodeStream
    select 
    data.get('ACCTS-RECORD').get('ACCOUNT-NO'),
data.get('ACCTS-RECORD').get('NAME').get('FIRST-NAME'),
data.get('ACCTS-RECORD').get('NAME').get('LAST-NAME'),
data.get('ACCTS-RECORD').get('ADDRESS1'),
data.get('ACCTS-RECORD').get('ADDRESS2'),
data.get('ACCTS-RECORD').get('ADDRESS3').get('CITY'),
data.get('ACCTS-RECORD').get('ADDRESS3').get('STATE'),
data.get('ACCTS-RECORD').get('ADDRESS3').get('ZIP-CODE')
from @APPNAME@Stream js;

create type finaldtype(
      ACCOUNT_NO String, 
      FIRST_NAME String,
      LAST_NAME String,
      ADDRESS1 String,
      ADDRESS2 String,
      CITY String,
      STATE String,
      ZIP_CODE String
);

CREATE CQ getdata
INSERT into getdataStream
    select account_no.toString(),
    first_name.toString(),
    last_name.toString(),
    addr1.toString(),
    Addr2.toString(),
    City.toString(),
    State.toString(),
    Zip.toString()
from cqAsJSONNodeStream x;

create Target @APPNAME@DBTarget using DatabaseWriter(
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@localhost:1521/xe',
  BatchPolicy: 'EventCount:10000',
  CommitPolicy: 'EventCount:10000',
  Tables: 'QATEST.@APPNAME@'
)
input from getdataStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  FetchSize:1,
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@;

--
-- Recovery Test 36 with two sources, two jumping attribute windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W/p -> CQ1 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS
--

STOP Recov36Tester.RecovTest36;
UNDEPLOY APPLICATION Recov36Tester.RecovTest36;
DROP APPLICATION Recov36Tester.RecovTest36 CASCADE;

DROP USER KStreamRecov36Tester;
DROP NAMESPACE KStreamRecov36Tester CASCADE;
CREATE USER KStreamRecov36Tester IDENTIFIED BY KStreamRecov36Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov36Tester;
CONNECT KStreamRecov36Tester KStreamRecov36Tester;

CREATE APPLICATION KStreamRecovTest36 RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM KafkaCsvStream2 OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM KafkaCsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION KStreamRecovTest36;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	charset:'@charset@'
)
parse using JSONParser (
	eventType:'@evty@',
	fieldName:'@fname@'
)
OUTPUT TO CsvStream;

create Target FileTarget using FileWriter(
    rolloverpolicy:'eventcount:100',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using JSONFormatter (
)
input from CsvStream;

end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

create Application UdpXml;
create source UdpXMLSource using UDPReader (
	IpAddress:'127.0.0.1',
	PortNo:'3546'
)
parse using XMLParser (
    RootNode:'/catalog/book'
)
OUTPUT TO UdpXMLStream;
create Target UdpDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/xmldata') input from UdpXMLStream;
end Application UdpXml;

--
-- Crash Recovery Test 5 with Jumping window and partitioned on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP APPLICATION N4S4CR5Tester.N4S4CRTest5;
UNDEPLOY APPLICATION N4S4CR5Tester.N4S4CRTest5;
DROP APPLICATION N4S4CR5Tester.N4S4CRTest5 CASCADE;
CREATE APPLICATION N4S4CRTest5 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest5;

CREATE SOURCE CsvSourceN4S4CRTest5 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest5;

CREATE FLOW DataProcessingN4S4CRTest5;

CREATE TYPE CsvDataN4S4CRTest5 (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvDataN4S4CRTest5 PARTITION BY merchantId;

CREATE CQ CsvToDataN4S4CRTest5
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE WactionsN4S4CRTest5 CONTEXT OF CsvDataN4S4CRTest5
EVENT TYPES ( CsvDataN4S4CRTest5 )
@PERSIST-TYPE@

CREATE CQ DataToWactionN4S4CRTest5
INSERT INTO WactionsN4S4CRTest5
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN4S4CRTest5;

END APPLICATION N4S4CRTest5;

create or replace PROPERTYVARIABLE SRC_PASSWORD='@PROP_VAR@';
CREATE APPLICATION @WRITERAPPNAME@ RECOVERY 10 SECOND INTERVAL;
-- USE EXCEPTIONSTORE;

CREATE SOURCE @SOURCE@ USING Ojet
(
FetchSize:1,
Username:'@SOURCE_USER@',
Password:'@pass@',
--Password:'$SRC_PASSWORD',
ConnectionURL:'@CONNECTION_URL@',
Tables:'@SOURCE_TABLE@',
)
OUTPUT TO @STREAM1@;

CREATE OR REPLACE TYPE @TYPE@( 
datae java.util.HashMap , 
TABLE_NAME java.lang.String , 
OPS_NAME java.lang.String , 
DB_TIMESTAMP java.lang.String  ,
COMMITSCN java.lang.String ,
SCN java.lang.String ,
REC_INS_TIME java.lang.String );

CREATE CQ @CQ1@
INSERT INTO @STREAM2@
SELECT  
CASE WHEN (META(c,"OperationName").toString() == "DELETE")
THEN putUserData(c, 'isDelete', 'true') 
ELSE
putUserData(c,'isDelete', 'false')
END
FROM @STREAM1@ c;

CREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;

CREATE OR REPLACE CQ @CQ2@ 
INSERT INTO @STREAM3@
SELECT 
data(e),
META(e,"TableName").toString() as TABLE_NAME,
META(e, "OperationName").toString() as OPS_NAME,
META(e, "TimeStamp").toString() as DB_TIMESTAMP,
META(e,"COMMITSCN").toString() as COMMITSCN ,
META(e,"SCN").toString() as  SCN ,
DNOW().toString() as REC_INS_TIME
FROM @STREAM1@ e;

create Target @TARGET1@ using KafkaWriter VERSION @kafakaversion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC1',
ParallelThreads:'',
PartitionKey:'@metadata(TableName)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from @STREAM1@;

create Target @TARGET2@ using KafkaWriter VERSION @kafakaversion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC2',
ParallelThreads:'2',
PartitionKey:'TABLE_NAME',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from @STREAM3@;

create Target @TARGET3@ using KafkaWriter VERSION @kafakaversion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC3',
ParallelThreads:'',
PartitionKey:'@userdata(isDelete)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest.avsc')
input from @STREAM2@;

end application @WRITERAPPNAME@;

CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafakaversion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC1',
        startOffset:0       
)
PARSE USING DSVParser ()
OUTPUT TO KafkaReaderStream1;

CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
filename:'@READERAPPNAME@_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream1;

CREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafakaversion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC2',
        startOffset:0          
)
PARSE USING JSONParser ()
OUTPUT TO KafkaReaderStream2;

CREATE TARGET kafkaDumpJSON USING FileWriter(
filename:'@READERAPPNAME@_RT_JSON')
FORMAT USING JSONFormatter()
INPUT FROM KafkaReaderStream2;

CREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafakaversion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC3',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest.avsc'
)
OUTPUT TO KafkaReaderStream3;

CREATE CQ CQAvro_Json 
INSERT INTO Avro_Json  
SELECT AvroToJSON(u.data) FROM KafkaReaderStream3 u;;

CREATE TARGET kafkaDumpAVRO USING FileWriter(
filename:'@READERAPPNAME@_RT_AVRO')
FORMAT USING Global.JSONFormatter  ()
INPUT FROM Avro_Json;

end application @READERAPPNAME@;

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'smallposdata10rows.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


@@FEATURE-DIR@/tql/TQLwithinTQL3.tql;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@2;

-- Creating a namespace ensures there won't be conflicts with the regular version of
-- PosApp. The only difference between this version and the regular version is
-- that the CQ that parses the source stream includes a PAUSE clauses that introduces a
-- 40-millisecond pause after each event is read, simulating the way the dashboard would
-- work with real-time data.
Stop PosAppImplicit.PosAppImplicit;
undeploy application PosAppImplicit.PosAppImplicit;
drop application PosAppImplicit.PosAppImplicit cascade;


-- The PosApp sample application demonstrates how a credit card
-- payment processor might use WebAction to generate reports on current
-- transaction activity by merchant and send alerts when transaction
-- counts for a merchant are higher or lower than average for the time
-- of day.

CREATE Application PosAppImplicit;

-- All CREATE statements between here and the END APPLICATION
-- statement will create objects in the PosApp application.

-- source CsvDataSource

CREATE source CsvDataSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'posdata.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO CsvStream;


-- This is the primary data source for this application.
-- In a real-world application, it would be real-time data. Here,
-- the data comes from a comma-delimited file, posdata.csv. The first
-- two lines of that file are:
--
-- BUSINESS NAME, MERCHANT ID, PRIMARY ACCOUNT NUMBER, POS DATA CODE, DATETIME, EXP DATE, CURRENCY CODE, AUTH AMOUNT, TERMINAL ID, ZIP, CITY
-- COMPANY 1,D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu,6705362103919221351,0,20130312173210,0916,USD,2.20,5150279519809946,41363,Quicksand
--
-- The "header:Yes" setting tells WebAction that the first line contains
-- field labels that should not be treated as data.
--
-- The "positionByEOF:false" setting tells WebAction to start reading
-- from the beginning of the file. (In a real-world application
-- reading real log files, you would typically use the default "true"
-- setting so that the application would read only new data.)
--
-- The OUTPUT TO clause automatically creates the stream
-- CsvStream using the WAEvent type associated with the CSVReader
-- adapater. The only field from WAEvent used by this application
-- is "data", an array containing the delimited fields.


-- CQ CsvToPosData
--
-- A stream's type must be declared before the stream, and a CQ's
-- output stream must be defined before the CQ. Hence type-stream-CQ
-- sequences like the following are very common.

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT TO_STRING(data[1]) as merchantId,
       TO_DATEF(data[4],'yyyyMMddHHmmss') as dateTime,
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')) as hourValue,
       TO_DOUBLE(data[7]) as amount,
       To_String(data[9]) as zip
FROM CsvStream;

-- Here, "data" refers to the array mentioned above, and the number
-- in brackets specifies a field from the array, counting from zero.
-- Thus data[1] is MERCHANT ID, data[4] is DATETIME, data[7] is AUTH
-- AMOUNT, and data[9] is ZIP. These correspond to the merchantId,
-- dateTime, hourValue, amount, and zip fields in PosDataStream, as
-- defined by the PosData type.
--
-- The DATETIME field from the source is converted to both a DateTime
-- value, used as the event timestamp by the application, and an int,
-- which is used to look up historical hourly averages from the
-- HourlyAveLookup cache, discussed below.
--
-- The other six fields are discarded. Thus the first line of data
-- from posdata.csv has at this point been reduced to five values:
--
-- D6RJPwyuLXoLqQRQcOcouJ26KGxJSf6hgbu (merchantId)
-- 20130312173210 (DateTime)

-- 2.20 (amount)
-- 41363 (zip)


-- CQ GenerateMerchantTxRateOnly
--
-- The PosData5Minutes window bounds the data so that the query can
-- use the COUNT, FIRST, and SUM functions and join data from the
-- HourlyAveLookup cache. (Aggregate functions cannot be used and
-- joins cannot be performed on unbound real-time data.)
--
-- The HourlyAveLookup cache provides historical average sales
-- amounts for the current hour for each merchant.

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE TYPE MerchantHourlyAve(
  merchantId String,
  hourValue int,
  hourlyAve int
);
CREATE CACHE HourlyAveLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'hourlyData.txt'
)
PARSE USING DSVParser (
  header: Yes,
  trimquote:false
) QUERY (keytomap:'merchantId') OF MerchantHourlyAve;

CREATE CQ GenerateMerchantTxRateOnly
INSERT INTO MerchantTxRateOnlyStream
SELECT p.merchantId as merchantId,
       p.zip as zip,
       FIRST(p.dateTime) as startingTime,
       COUNT(p.merchantId) as count,
       SUM(p.amount) as totalAmount,
       l.hourlyAve/12 as hourlyAve,
       l.hourlyAve/12 * CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END as upperLimit,
       l.hourlyAve/12 / CASE
         WHEN l.hourlyAve/12 >10000 THEN 1.15
         WHEN l.hourlyAve/12 > 800 THEN 1.2
         WHEN l.hourlyAve/12 >200 THEN 1.25
         ELSE 1.5 END as lowerLimit,
       '<NOTSET>' as category,
       '<NOTSET>' as status
FROM PosData5Minutes p, HourlyAveLookup l
WHERE p.merchantId = l.merchantId AND p.hourValue = l.hourValue
GROUP BY p.merchantId;

-- This query aggregates five minutes' worth of data for each
-- merchant, calculating the total transaction count and amount, and
-- calculates the upperLimit and lowerLimit values based on the
-- historical average transaction count for the current hour of the
-- day from the HourlyAveLookup cache. The category and status fields
-- are left unset to be populated by the next query.


-- CQ GenerateMerchantTxRateWithStatus
--
-- This query sets the count values used by the Dashboard map and the
-- status values used to trigger alerts.

CREATE CQ GenerateMerchantTxRateWithStatus
INSERT INTO MerchantTxRateWithStatusStream
SELECT merchantId as merchantId,
       zip as zip,
       startingTime as startingTime,
       count as count,
       totalAmount as totalAmount,
       hourlyAve as hourlyAve,
       upperLimit as upperLimit,
       lowerLimit as lowerLimit,
       CASE
         WHEN count >10000 THEN 'HOT'
         WHEN count > 800 THEN 'WARM'
         WHEN count >200 THEN 'COOL'
         ELSE 'COLD' END as category,
       CASE
         WHEN count > upperLimit THEN 'TOOHIGH'
         WHEN count < lowerLimit THEN 'TOOLOW'
         ELSE 'OK' END as status
FROM MerchantTxRateOnlyStream;


-- WAction store MerchantActivity
--
-- The following group of statements create and populate the MerchantActivity
-- WAction store. Data from the MerchantTxRateWithStatusStream is enhanced
-- with merchant details from NameLookup cache and with latitude and longitude
-- values from the USAddressData cache.

CREATE TYPE MerchantActivityContext(
  MerchantId String KEY,
  StartTime  DateTime,
  CompanyName String,
  Category String,
  Status String,
  Count int,
  HourlyAve int,
  UpperLimit double,
  LowerLimit double,
  Zip String,
  City String,
  State String,
  LatVal double,
  LongVal double
);
CREATE TYPE MerchantTxRate(
  merchantId String KEY,
  zip String,
  startingTime DateTime,
  count int,
  totalAmount double,
  hourlyAve int,
  upperLimit double,
  lowerLimit double,
  category String,
  status String
);

CREATE WACTIONSTORE MerchantActivity CONTEXT OF MerchantActivityContext
EVENT TYPES ( MerchantTxRate )
@PERSIST-TYPE@

CREATE TYPE MerchantNameData(
  merchantId String KEY,
  companyName String
);

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE NameLookup using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false
)
PARSE USING DSVParser (
  header:'yes',
  trimquote:false
)
QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE CACHE ZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  positionByEOF:false
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

CREATE CQ GenerateWactionContext
INSERT INTO MerchantActivity
SELECT  m.merchantId,
        m.startingTime,
        n.companyName,
        m.category,
        m.status,
        m.count,
        m.hourlyAve,
        m.upperLimit,
        m.lowerLimit,
        m.zip,
        z.city,
        z.state,
        z.latVal,
        z.longVal
FROM MerchantTxRateWithStatusStream m, NameLookup n, ZipLookup z
WHERE m.merchantId = n.merchantId AND m.zip = z.zip
LINK SOURCE EVENT;


-- CQ GenerateAlerts
--
-- This CQ sends an alert when a merchant's status value changes to
-- TOOHIGH or TOOLOW, then another alert when the value returns to OK.


CREATE STREAM AlertStream OF Global.AlertEvent;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT n.CompanyName,
       m.MerchantId,
       CASE
         WHEN m.Status = 'OK' THEN 'info'
         ELSE 'warning' END,
       CASE
         WHEN m.Status = 'OK' THEN 'cancel'
         ELSE 'raise' END,
       CASE
         WHEN m.Status = 'OK'      THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is back between '         + ROUND_DOUBLE(m.lowerLimit,0) + ' and ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOHIGH' THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is above upper limit of ' + ROUND_DOUBLE(m.upperLimit,0)
         WHEN m.Status = 'TOOLOW'  THEN 'Merchant ' + n.companyName + ' count of ' + m.count + ' is below lower limit of ' + ROUND_DOUBLE(m.lowerLimit,0)
         ELSE ''
         END
FROM MerchantTxRateWithStatusStream m, NameLookup n
WHERE m.merchantId = n.merchantId;


-- The following statement loads visualization (Dashboard) settings
-- from a file.


--CREATE TARGET output1 USING SysOut(name : rawinput) input FROM AlertStream;

CREATE SUBSCRIPTION AlertSub USING WebAlertAdapter( ) INPUT FROM AlertStream;


END APPLICATION PosAppImplicit;

CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @APPNAME@_DBSource USING Global.OracleReader (
  TransactionBufferDiskLocation: '.striim/LargeBuffer',
  Compression: false,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Password_encrypted: 'false',
  SupportPDB: false,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  FetchSize: 1000,
  CDDLAction: 'Process',
  ConnectionURL: '192.168.56.3:1521:orcl',
  DictionaryMode: 'OnlineCatalog',
  QueueSize: 2048,
  CommittedTransactions: true,
  SetConservativeRange: false,
  CDDLCapture: false,
  Username: 'fan',
  Tables: 'FAN.S_BLOB',
  TransactionBufferType: 'Disk',
  Password: '9S5GnbGmBQNDD5c/baD0Tw==',
  TransactionBufferSpilloverSize: '100MB',
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  DatabaseRole: 'Primary' )
OUTPUT TO @APPNAME@_stream;

CREATE OR REPLACE TARGET @APPNAME@_target USING Global.BigQueryWriter (
  ColumnDelimiter: '|',
  projectId: 'striim-support',
  BatchPolicy: 'eventCount:1, Interval:1',
  NullMarker: 'NULL',
  streamingUpload: 'false',
  ServiceAccountKey: '/Users/fzhang/fan/u01/app/product/striim/striim_latest/UploadedFiles/striim-support-286429beb74d.json',
  Encoding: 'UTF-8',
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30',
  AllowQuotedNewLines: 'false',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  Tables: 'FAN.S_BLOB,Fan.s_blob columnmap(A=A,B=B,C=C,D=C,E=@metadata(OperationName));',
  TransportOptions: 'connectionTimeout=300, readTimeout=120',
  adapterName: 'BigQueryWriter',
  Mode: 'MERGE',
  StandardSQL: 'true',
  includeInsertId: 'true',
  QuoteCharacter: '\"' )
INPUT FROM @APPNAME@_stream;


END APPLICATION @APPNAME@;

CREATE OR REPLACE APPLICATION @APPNAME@ @RECOVERY@;

CREATE OR REPLACE SOURCE @APPNAME@_src USING DatabaseReader (
  Tables: '',
  ConnectionURL: '',
  Password: '',
  Username: ''
  )
OUTPUT TO @APPNAME@stream;

CREATE OR REPLACE TARGET @APPNAME@_trgt USING S3Writer (
  bucketname: '',
  uploadpolicy: '',
  UploadConfiguration: '',
  objectname: '' )
FORMAT USING JSONFormatter (
members:'data'
)
INPUT FROM @APPNAME@stream;

END APPLICATION @APPNAME@;

stop @AppName@;
Undeploy application @AppName@;
alter application @AppName@;
CREATE FLOW CP_Agent_flow;
CREATE OR REPLACE SOURCE CP_Oracle_source USING OracleReader ( 
  ConnectionURL: '@url@', 
  Tables: '@tables@', 
  Username: '@Username@', 
  Password: '@password@' ) 
OUTPUT TO CP_EndToEnd_SF_Adapter_Stream;
End Flow CP_Agent_flow;
alter application @AppName@ recompile;
DEPLOY APPLICATION @AppName@ with CP_Agent_flow on any in AGENTS;
start application @AppName@;

CREATE OR REPLACE PROPERTYVARIABLE Mode='sync';
CREATE OR REPLACE PROPERTYVARIABLE BatchPolicy='Size:900000,Interval:1';
create application KinesisTest;
create source CSVSource using FileReader (
	directory:'/home/dz/src/product/Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  companyName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0], data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	BatchPolicy: '$BatchPolicy',
    Mode: '$Mode'	
)
format using AvroFormatter (
	schemaFileName:'/Users/shikhar_nahar/Product/testwaevent.avsc'
)
input from TypedCSVStream;
end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

stop application DS.DSAPP;
undeploy application DS.DSAPP;

stop application DS.MyPosApp;
undeploy application DS.MyPosApp;

stop application DS.DsPosApp;
undeploy application DS.DsPosApp;

stop application DSL.DSLAPP;
undeploy application DSL.DSLAPP;

unload cache DS.C1;
unload wactionstore DS.MerchantActivity;

drop namespace DS cascade;
drop user DS cascade;

drop namespace DSL cascade;
drop user DSL cascade;

CREATE OR REPLACE PROPERTYVARIABLE Mode='sync';
CREATE OR REPLACE PROPERTYVARIABLE BatchPolicy='Size:900000,Interval:1';
create application KinesisTest;
create source CSVSource using FileReader (
	directory:'/home/dz/src/product/Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  companyName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0], data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	BatchPolicy: '$BatchPolicy',
    Mode: '$Mode'
)
format using XMLFormatter (
	rootelement:'document',
	elementtuple:'CompanyName:merchantid:text=companyname,ZipCode:text=zip,Amount:text=amount'
)
input from TypedCSVStream;
end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;

 CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01;striim.test02;striim.test03',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',
 startPosition: 'striim.test01=1;striim.test02=-1;%=0',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream ;

  CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

create target AzureSQLDWHTarget using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream;


CREATE TARGET log USING LOGWRITER
(
 name:log,
 filename:'LOGFILENAME'
)
INPUT FROM data_stream;


END APPLICATION IR;

deploy application IR;
start IR;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.WAEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (
  ProviderName: '',
  Provider: '',
  Ctx: '',
  QueueName: '',
  Topic:'',
  UserName: '',
  Password: '',
  EnableTransaction: '',
  transactionpolicy: ''
 )
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING DSVFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream@RANDOM@;

END APPLICATION @APPNAME@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE FLOW @APPNAME@AgentFlow;

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.waevent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (
  ProviderName: '',
  UserName: '',
  transactionpolicy: '',
  Provider: '',
  Ctx: '',
  EnableTransaction: '',
  QueueName: '',
  Password: '' )
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@PersistStream@RANDOM@;
END FLOW @APPNAME@AgentFlow;

CREATE FLOW @APPNAME@serverFlow;
CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING JSONFormatter  (
  members: 'data' )
INPUT FROM @APPNAME@PersistStream@RANDOM@;
END FLOW @APPNAME@serverFlow;

END APPLICATION @APPNAME@;

--
-- Crash Recovery Test 4 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS
-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS
--

STOP APPLICATION KStreamN2S2CR4Tester.KStreamN2S2CRTest4;
UNDEPLOY APPLICATION KStreamN2S2CR4Tester.KStreamN2S2CRTest4;
DROP APPLICATION KStreamN2S2CR4Tester.KStreamN2S2CRTest4 CASCADE;

DROP USER KStreamN2S2CR4Tester;
DROP NAMESPACE KStreamN2S2CR4Tester CASCADE;
CREATE USER KStreamN2S2CR4Tester IDENTIFIED BY KStreamN2S2CR4Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR4Tester;
CONNECT KStreamN2S2CR4Tester KStreamN2S2CR4Tester;

CREATE APPLICATION KStreamN2S2CRTest4 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest4;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest4 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest4;

CREATE FLOW DataProcessingKStreamN2S2CRTest4;

CREATE TYPE CsvDataKStreamN2S2CRTest4 (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionTypeKStreamN2S2CRTest4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream OF CsvDataKStreamN2S2CRTest4;

CREATE CQ CsvToDataKStreamN2S2CRTest4
INSERT INTO DataStream
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest4 CONTEXT OF WactionTypeKStreamN2S2CRTest4
EVENT TYPES ( CsvDataKStreamN2S2CRTest4 )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO WactionsKStreamN2S2CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO WactionsKStreamN2S2CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END FLOW DataProcessingKStreamN2S2CRTest4;

END APPLICATION KStreamN2S2CRTest4;

stop OracleToKudu;
undeploy application OracleToKudu;
drop application OracleToKudu cascade;

CREATE APPLICATION OracleToKudu;
Create Source oracSource
 Using OracleReader
(
 Username:'@LOGMINER-UNAME@',
 Password:'@LOGMINER-PASSWORD@',
 ConnectionURL:'@LOGMINER-URL@',
 Tables:'@SOURCE_TABLES@',
 OnlineCatalog:true,
 FetchSize:1
) Output To DataStream;

CREATE TARGET WriteintoKudu using KuduWriter (
kuduclientconfig:'master.addresses->@CONFIG_ADDRESS@:@PORT@;socketreadtimeout->@SOCKET_TIMEOUT@;operationtimeout->@OPERATION_TIMEOUT@',
pkupdatehandlingmode:'@MODE@',
tables: '@TARGET_TABLES@',
batchpolicy: 'EventCount:1,Interval:0')
INPUT FROM DataStream;

END APPLICATION OracleToKudu;
deploy application OracleToKudu in default;
start OracleToKudu;

stop APPLICATION @AppName@;
Undeploy APPLICATION @AppName@;
drop APPLICATION @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;

CREATE FLOW @AgentFlow@1;
CREATE OR REPLACE SOURCE @SourceName@1 USING Global.MSJet (
  Tables: 'dbo.compsrc',
  username: 'qatest',
  DatabaseName: 'qatest',
  FetchTransactionMetadata: true,
  filterTransactionBoundaries: true,
  compression: false,
  ConnectionURL: '@ConnectionURL@1',
  Mode: '@mode@',
  CommittedTransactions: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
  SendBeforeImage: true,
  password: 'w3b@ct10n' )
OUTPUT TO @StreamName@;
END FLOW @AgentFlow@1;

CREATE FLOW @AgentFlow@2;
CREATE OR REPLACE SOURCE @SourceName@2 USING Global.MSJet (
  Tables: 'dbo.compsrc',
  username: 'qatest',
  DatabaseName: 'qatest',
  FetchTransactionMetadata: true,
  filterTransactionBoundaries: true,
  compression: false,
  ConnectionURL: '@ConnectionURL@2',
  CaptureType: '@captureType@',
  CommittedTransactions: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=5',
  SendBeforeImage: true,
  password: 'w3b@ct10n' )
OUTPUT TO @StreamName@;
END FLOW @AgentFlow@2;

CREATE TARGET @SysTarget@ USING Global.SysOut (
  name: 'MS_CDC_SYSOUT' )
INPUT FROM @StreamName@;

CREATE FLOW @ServerFlow@1;
CREATE TARGET @TargetName@1 USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ParallelThreads: '',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: '@ConnectionURL@',
  StatementCacheSize: '50',
  DatabaseProviderType: 'Default',
  Username: 'qatest',
  Tables: 'dbo.compsrc,dbo.comptar',
  Password: 'w3b@ct10n',
  PreserveSourceTransactionBoundary: 'false',
  BatchPolicy: 'EventCount:1,Interval:60' )
INPUT FROM @StreamName@;
END FLOW @ServerFlow@1;

CREATE FLOW @ServerFlow@2;
CREATE TARGET @TargetName@2 USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ParallelThreads: '',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: '@ConnectionURL@',
  StatementCacheSize: '50',
  DatabaseProviderType: 'Default',
  Username: 'qatest',
  Tables: 'dbo.compsrc,dbo.comptar',
  Password: 'w3b@ct10n',
  PreserveSourceTransactionBoundary: 'false',
  BatchPolicy: 'EventCount:1,Interval:60' )
INPUT FROM @StreamName@;
END FLOW @ServerFlow@2;

END APPLICATION @AppName@;
DEPLOY APPLICATION @AppName@ with @AgentFlow@1 in AGENTS, @AgentFlow@2 in AGENTS, @ServerFlow@1 on any in default, @ServerFlow@2 on any in default;
START APPLICATION @AppName@;

STOP OuterJoinTester.OJApp;
UNDEPLOY APPLICATION OuterJoinTester.OJApp;
DROP APPLICATION OuterJoinTester.OJApp cascade;

CREATE APPLICATION OJApp;

CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;


CREATE source wsSource2 USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'bankCards.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO stream2;



CREATE TYPE bankData
(
bankID Integer KEY,
bankName String
);

CREATE TYPE cardData
(
cardID Integer KEY,
cardName String
);

CREATE STREAM wsStream OF bankData;
CREATE STREAM wsStream2 OF cardData;


CREATE CACHE cache1 USING CsvReader(
directory: '@TEST-DATA-PATH@',
wildcard: 'banks.csv',
header: No,
columndelimiter: ',',
trimquote: false
) QUERY (keytomap:'bankID') OF bankData;


CREATE CACHE cache2 USING CsvReader(
directory: '@TEST-DATA-PATH@',
wildcard: 'bankCards.csv',
header: No,
columndelimiter: ',',
trimquote: false
) QUERY (keytomap:'cardID') OF cardData;


CREATE WACTIONSTORE oneWS CONTEXT OF bankData
EVENT TYPES(bankData )
@PERSIST-TYPE@

CREATE WACTIONSTORE twoWS CONTEXT OF cardData
EVENT TYPES(cardData )
@PERSIST-TYPE@



CREATE CQ csvTobankData
INSERT INTO oneWS
SELECT TO_INT(data[0]), data[1] FROM QaStream;



CREATE CQ csvTobankData2
INSERT INTO wsStream
SELECT TO_INT(data[0]), data[1] FROM QaStream;

CREATE CQ csvTobankData3
INSERT INTO wsStream2
SELECT TO_INT(data[0]), data[1] FROM stream2;

CREATE CQ csvTobankData4
INSERT INTO twoWS
SELECT TO_INT(data[0]), data[1] FROM stream2;


CREATE JUMPING WINDOW win1 OVER wsStream KEEP 20 rows;


CREATE JUMPING WINDOW win2 OVER wsStream2 KEEP 4 rows;



END APPLICATION OJApp;

stop @appName@;
undeploy application @appName@;
drop application @appName@ cascade;

CREATE APPLICATION @appName@ USE EXCEPTIONSTORE TTL : '7d' AUTORESUME MAXRETRIES 1000 RETRYINTERVAL 60;

CREATE OR REPLACE SOURCE @appName@_Source USING Global.MysqlReader (
  ConnectionURL: '@ConnectionURL@',
  Tables: '@dbTable@',
  Password: '@Password@',
  Username: '@Username@',
  adapterName: 'MysqlReader')
OUTPUT TO @appName@st1;

CREATE CQ @appName@CQ
INSERT INTO @appName@st2
SELECT data[0] as Id,data[1] as Description FROM @appName@st1 s;;

create target @appName@_tgt using NullWriter() input from @appName@st2;

END APPLICATION @appName@;

Alter application app1;

CREATE OR REPLACE SOURCE s USING oracleReader  ( 
  Username:'qatest',
  Password:'qatest',
  ConnectionURL:'localhost:1521/xe',
  Tables:'QATEST.test%',
  FetchSize:2
 ) 
OUTPUT TO rawstream;
Alter application app1 recompile;



Alter application app2;
CREATE or replace TARGET app2_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Interval:60',
CommitPolicy:'Interval:60',
Tables:'qatest.test01,QATEST.KPS1'
) INPUT FROM rawstream;
Alter application app2 recompile;




Alter application app3;

CREATE or replace TARGET app3_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Interval:60',
CommitPolicy:'Interval:60',
Tables:'qatest.test02,QATEST.KPS2'
) INPUT FROM rawstream;

Alter application app3 recompile;



Alter application app4;

CREATE or replace TARGET app4_target USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Interval:60',
CommitPolicy:'Interval:60',
Tables:'qatest.test03,QATEST.KPS3'
) INPUT FROM rawstream;

Alter application app4 recompile;

CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @APPNAME@_src USING Global.GCSReader (
  _h_GCSQueryCoolingTime: '1',
  ServiceAccountKey: '',
  ProjectId: '',
  DownloadPolicy: '',
  BucketName: '',
  FolderName: '',
  ObjectFilter: '',
  ObjectDetectionMode: '',
  IncludeSubfolders: false )
PARSE USING Global.DSVParser ()
OUTPUT TO @APPNAME@_Stream;

CREATE OR REPLACE CQ @APPNAME@_CQ
INSERT INTO @APPNAME@_CQOut
SELECT
    data[0] as BusinessName,
    data[1] as MerchantId,
    data[2] as PosDataCode,
    data[3] as AccNumber,
    data[4] as DateTime,
    data[5] as ExpDate,
    data[6] as CurrencyCode,
    data[7] as AuthAmount,
    data[8] as TerminalId,
    data[9] as Zip,
    data[10] as City
FROM @APPNAME@_Stream;

CREATE OR REPLACE TARGET @APPNAME@_KafkaTarget USING Global.KafkaWriter VERSION @KAFKA_VERSION@(
  brokerAddress: '',
  Topic: '',
  Mode: 'Sync' )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_HDFSTarget USING Global.HDFSWriter (
  flushpolicy: '',
  rolloverpolicy: '',
  directory: '',
  hadoopurl: '',
  hadoopconfigurationpath: '',
  filename: '')
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_BlobTarget USING Global.AzureBlobWriter (
  containername: '',
  blobname: '',
  accountaccesskey: '',
  accountname: '',
  foldername: '' )
FORMAT USING JSONFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_DWHTarget USING Global.BigQueryWriter (
  Tables: '',
  BatchPolicy: '',
  projectId: '',
  ServiceAccountKey: '' )
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_NoSqlTarget USING Global.MongoDBWriter (
  AuthDB: 'admin',
  ConnectionURL: '',
  Username: '',
  collections: '',
  Password: '' )
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_FileTarget USING Global.FileWriter (
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '',
  filename: '' )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@_CQOut;

CREATE OR REPLACE TARGET @APPNAME@_OLTPTarget USING Global.DatabaseWriter (
  ConnectionURL: '',
  Password: '',
  Username: '',
  Tables: '' )
INPUT FROM @APPNAME@_CQOut;

CREATE SOURCE @APPNAME@_KafkaSource USING KafkaReader VERSION @KAFKA_VERSION@ (
  brokerAddress: '',
  Topic: '',
  startOffset: '0' )
PARSE USING DSVParser ()
OUTPUT TO @APPNAME@_Stream2;

CREATE OR REPLACE TARGET @APPNAME@_KafkaFileTarget USING Global.FileWriter (
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '',
  filename: '' )
FORMAT USING JSONFormatter(
members:'data')
INPUT FROM @APPNAME@_Stream2;

END APPLICATION @APPNAME@;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;


CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using OracleReader
(
  Compression:true,
  StartTimestamp:'null',
  CommittedTransactions:true,
  FilterTransactionBoundaries:true,
  Password_encrypted:'false',
  SendBeforeImage:true,
  XstreamTimeOut:600,
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE',
  adapterName:'OracleReader',
  Password:'qatest',
  DictionaryMode:'OfflineCatalog',
  FilterTransactionState:true,
  connectionRetryPolicy:'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType:'LogMiner',
  FetchSize: 1,
  Username:'qatest',
  OutboundServerProcessName:'WebActionXStream',
  _h_ReturnDateTimeAs:'ZonedDateTime',
  _h_useClassic:true,
  CDDLAction:'Quiesce_Cascade',
  CDDLCapture:'true'
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;

STOP APPLICATION EH;
UNDEPLOY APPLICATION EH;
DROP APPLICATION EH CASCADE;
CREATE APPLICATION EH recovery 5 second interval;
create flow AgentFlow;
CREATE SOURCE EH_SOURCE USING MySQLReader (
  Compression: true,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: '@CONNECTION_URL@',
  DatabaseName: 'waction',
  Tables: 'waction.Test01',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)
OUTPUT TO EH_SS;
end flow AgentFlow;
create flow serverFlow;
create Target EH_TARGET using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_01',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_01_cg',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:1000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter()
input from EH_SS;
end flow serverFlow;

END APPLICATION EH;

--DEPLOY APPLICATION EH;
deploy application eh with AgentFlow in Agents, ServerFlow in default;

start application EH;

STOP APPLICATION ER;
UNDEPLOY APPLICATION ER;
DROP APPLICATION ER CASCADE;
CREATE APPLICATION ER;
CREATE STREAM ER_SS1 OF Global.JsonNodeEvent;

CREATE SOURCE ER_S1 USING AzureEventHubReader (
    EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_01',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
    ConsumerGroup: 'test_01_cg',
	startSeqNo:'0'
	)
PARSE USING jsonparser ()
OUTPUT TO ER_SS1;


create Target ER_t1 using FileWriter (
filename:'MYSQL_EH_ER_JSON_RT',
directory:'FEATURE-DIR/logs/',
rolloverpolicy:'eventcount:5000000'
)
format using jsonFormatter (members:'data')
input from ER_SS1;
end application ER;
deploy application ER;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING CosmosDBWriter  (
  BatchPolicy: 'EventCount:1000,Interval:1',
  ServiceEndpoint: 'https://souvik.documents.azure.com:443/',
  ConnectionPoolSize: '10',
  AccessKey: 'z1CfmzAy5QwB5MN7bWIinM9gYmJn7zWo1wOaadvaCErqaTCqlb7srpAx7muPYWhJwnYq3plOQoBNENn1xPmkfQ==',
  adapterName: 'CosmosDBWriter',
  Collections: 'QATEST.TEST,db2.TEST keycolumns(id)',
  ConnectionRetryPolicy: 'RetryInterval:1,MaxRetries:0',
  KeySeparator: ':',
  AccessKey_encrypted: false
 )
INPUT FROM @STREAM@;

stop application GCSWriterTest;
undeploy application GCSWriterTest;
drop application GCSWriterTest cascade;
create application GCSWriterTest recovery 1 second interval;
create source GCS_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;
create Target GCSTarget using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'@UPLOAD-SIZE@',
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
--members:'data'
)
input from TypedCSVStream;

end application GCSWriterTest;
deploy application GCSWriterTest on all in default;
start application GCSWriterTest;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_SourceFlow;
CREATE OR REPLACE SOURCE @SOURCE@ USING Ojet  (
  FilterTransactionBoundaries: true,
  ConnectionURL: '@OCI-URL@',
  Tables: '@SOURCE_TABLE@',
  Password: '@OJET-PASSWORD@',
  fetchsize: 1,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  Username: '@OJET-UNAME@'
 )
OUTPUT TO @STREAM@;
end flow @APPNAME@_SourceFlow;
create flow @APPNAME@_TargetFlow;
CREATE OR REPLACE TARGET @TARGET@1 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'false',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@2 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'false',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'true',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET@3 USING SnowflakeWriter (
  connectionUrl: 'jdbc:snowflake://striim.snowflakecomputing.com/?db=DEMO_DB',
  optimizedMerge: 'true',
  password: 'Web@ct10n',
  username: 'striim_regression2',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:10000,interval:5m',
  tables: 'QATEST.S,DEMO_DB.TESTSCHEMA.T',
 )
INPUT FROM @STREAM@;
end flow @APPNAME@_TargetFlow;
END APPLICATION @APPNAME@;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING FILEReader (
    wildcard: '',
    directory: '',
    positionbyeof: false
 )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:10,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: ''
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop ROLLUPMON_IL;
undeploy application ROLLUPMON_IL;
alter application ROLLUPMON_IL;
CREATE or replace FLOW ROLLUPMON_IL_flow;
Create or replace Source ROLLUPMON_IL_Oraclesrc Using databasereader(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'jdbc:oracle:thin:@//localhost:1522/orcl',
 Tables:'QATEST.ROLLUPMON_TABLE1;QATEST.ROLLUPMON_TABLE2;QATEST.ROLLUPMON_TABLE3;QATEST.ROLLUPMON_TABLE4;QATEST.ROLLUPMON_TABLE5',
 _h_fetchexactrowcount: 'true',
FetchSize:1000
)
Output To ROLLUPMON_IL_OrcStrm;
END FLOW ROLLUPMON_IL_flow;
alter application ROLLUPMON_IL recompile;
deploy application ROLLUPMON_IL;
start application ROLLUPMON_IL;

--
-- Recovery Test 12 with two sources, two jumping attribute windows, one wactionstore with recovery, and another wactionstore without -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W/p -> CQ1 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS2 (no persists)
--

STOP Recov12Tester.RecovTest12;
UNDEPLOY APPLICATION Recov12Tester.RecovTest12;
DROP APPLICATION Recov12Tester.RecovTest12 CASCADE;
CREATE APPLICATION RecovTest12 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream1 KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream2 KEEP WITHIN 6 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsNoPersist CONTEXT OF WactionData
EVENT TYPES ( CsvData )
		PERSIST NONE USING ( ) ;

CREATE CQ Data5ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream5Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWaction
INSERT INTO Wactions
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

CREATE CQ Data6ToWactionNoPersist
INSERT INTO WactionsNoPersist
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM DataStream6Minutes p
GROUP BY p.merchantId;

END APPLICATION RecovTest12;

STOP bq;
UNDEPLOY APPLICATION bq;
DROP APPLICATION bq CASCADE;

CREATE APPLICATION bq RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:orcl',
	Tables: 'QATEST.TABLE_TEST_1000100',
	DictionaryMode: offlineCatalog,
	FetchSize: '1'
)
OUTPUT TO SS;


CREATE or replace TARGET T USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
    Tables:'QATEST.TABLE_TEST_1000100,qatest.% keycolumns(RONUM)',
    mode:'Appendonly',
    datalocation: 'US',
	nullmarker: 'defaultNULL',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:100,Interval:10'	
) INPUT FROM ss;

END APPLICATION bq;
DEPLOY APPLICATION bq;
START APPLICATION bq;

STOP OuterJoinTester.OJApp;
UNDEPLOY APPLICATION OuterJoinTester.OJApp;
DROP APPLICATION OuterJoinTester.OJApp cascade;

CREATE APPLICATION OJApp;


CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;


CREATE source wsSource2 USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'bankCards.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO stream2;



CREATE TYPE bankData
(
bankID Integer KEY,
bankName String
);

CREATE TYPE cardData
(
cardID Integer KEY,
cardName String
);


CREATE STREAM wsStream OF bankData;
CREATE STREAM wsStream2 OF cardData;


--Select data from QaStream and insert into wsStream

CREATE CQ csvTobankData
INSERT INTO wsStream
SELECT TO_INT(data[0]), data[1] FROM QaStream;

CREATE CQ csvTobankData2
INSERT INTO wsStream2
SELECT TO_INT(data[0]), data[1] FROM stream2;

CREATE JUMPING WINDOW win1 OVER wsStream KEEP 20 rows;


CREATE JUMPING WINDOW win2 OVER wsStream2 KEEP 4 rows;

END APPLICATION OJApp;

CREATE OR REPLACE EMBEDDINGGENERATOR @EMB_NAME@ USING @MODEL@ (
modelProvider: '@MODEL@',
modelName: '@MODEL_NAME@',
apiKey: '@API_KEY@'
);

--
-- Recovery Test 5 with Jumping window and partitioned
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> JW(partitioned) -> CQ(no aggregate) -> WS
--

STOP Recov5Tester.RecovTest5;
UNDEPLOY APPLICATION Recov5Tester.RecovTest5;
DROP APPLICATION Recov5Tester.RecovTest5 CASCADE;
CREATE APPLICATION RecovTest5 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStream OF CsvData PARTITION BY merchantId;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY merchantId;

CREATE WACTIONSTORE Wactions CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    *
FROM DataStream5Minutes;

END APPLICATION RecovTest5;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.test01',
	FetchSize: '1',
	connectionRetryPolicy:'timeOut=00,retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'Eventcount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true		
) INPUT FROM @APPNAME@_ss;
--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

CREATE APPLICATION @AppName@ RECOVERY 1 MINUTE INTERVAL AUTORESUME MAXRETRIES 2 RETRYINTERVAL 60;

CREATE source @AppName@_PosData USING FileReader (
  WildCard: 'posdata100.csv',
  directory: '@TestDir@',
positionByEOF:false
)
PARSE USING DSVParser (
  header:Yes,
  trimquote:false
) OUTPUT TO @AppName@_CsvStream;

CREATE OR REPLACE CQ CsvPosAppCq 
INSERT INTO FromCsvPosAppCq 
SELECT TO_STRING(data[1]) as merchantId,
       TO_DOUBLE(data[7]) as amount,
       TO_STRING(data[9]) as zip
FROM @AppName@_CsvStream;


CREATE TARGET FileWriterTarget USING Global.FileWriter ( 
 
  flushpolicy: 'EventCount:1000,Interval:30s', 
  directory: '@logs@',
  filename: '@Filename@', 
  rolloverpolicy: 'EventCount:1000,Interval:30s' ) 
FORMAT USING Global.DSVFormatter  ( 
  quotecharacter: '\"', 
  columndelimiter: ',', 
  nullvalue: 'NULL', 
  usequotes: 'false', 
  rowdelimiter: '\n', 
  standard: 'none', 
  header: 'false' ) 
INPUT FROM FromCsvPosAppCq;

end application @AppName@;
deploy application @AppName@;
start application @AppName@;

CREATE APPLICATION @AppName@;

CREATE OR REPLACE SOURCE ReadHTTPPort USING Global.HTTPReader  ( 
deferresponsetimeout: '10000ms', 
  keystore: '@dataFile@server.keystore.jks', 
  deferresponse: true, 
  keystorepassword: 'w@ct10n', 
  ipaddress: 'localhost', 
  threadcount: 10, 
  keystorepassword_encrypted: 'false', 
  adapterName: 'HTTPReader', 
  keystoretype: 'JKS', 
  portno: '@port@', 
  authenticateclient: true ) 
OUTPUT TO HTTPReaderOUT  ;

CREATE OR REPLACE TARGET sysout USING Global.SysOut  ( 
name: 'sysout' ) 
INPUT FROM HTTPReaderOUT;

END APPLICATION @AppName@;
deploy application @AppName@;
Start @AppName@;

stop FileReaderToKuduWriter;
undeploy application FileReaderToKuduWriter;
drop application FileReaderToKuduWriter cascade;

CREATE APPLICATION FileReaderToKuduWriter recovery 5 second interval ;;

CREATE OR REPLACE SOURCE CSVPoller USING FileReader (
        directory:'/Users/Striim/',
        WildCard:'typetest.csv',
        positionByEOF:false
)
parse using DSVParser (
        header:'yes'
)
OUTPUT TO CsvStream;

CREATE OR REPLACE TYPE CSVStream_Type  ( ID1 String KEY,
ID2 String
);

CREATE OR REPLACE STREAM CSVTypeStream OF CSVStream_Type;

CREATE OR REPLACE CQ CQ1
INSERT INTO CSVTypeStream
SELECT TO_STRING(data[0]),TO_STRING(data[1])
FROM CsvStream;

CREATE TARGET WriteintoKudu using KuduWriter (
KuduClientConfig:'master.addresses->192.168.56.101:7051;socketreadtimeout->10000;operationtimeout->30000',
Tables: 'INTEGRATIONTEST',
BatchPolicy: 'EventCount:1,Interval:10') INPUT FROM CSVTypeStream;

END APPLICATION FileReaderToKuduWriter;
deploy application FileReaderToKuduWriter;
start FileReaderToKuduWriter;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@;

CREATE OR REPLACE TYPE @APPNAME@_Type (
 HEADER java.lang.String,
 DETAILNode com.fasterxml.jackson.databind.JsonNode
 );

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '@GRPPOLICY@',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'FixedLength',
  ProcessCopyBookFileAs: 'MultipleEvents',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE OR REPLACE CQ @APPNAME@_CQ
INSERT INTO @APPNAME@_CQOut
SELECT data.get('HEADER-RECORD').get('DSD-BATCH').textvalue() as HEADER,
data.get('DETAIL-RECORD') as DETAILNode FROM CCBStream c;

CREATE OR REPLACE CQ @APPNAME@_CQ1
INSERT INTO @APPNAME@_CQOut1
SELECT HEADER,
DETAILNode.get('REC-TYPE').textvalue() as DRECTYPE,
DETAILNode.get('AP-VENDOR').textvalue() as DAPVENDOR,
DETAILNode.get('FACILITY').textvalue() as DFACILITY,
DETAILNode.get('INVOICE-NUM').textvalue() as DINVOICENUM,
DETAILNode.get('DIV').textvalue() as DDIV,
DETAILNode.get('BILLING-COST').doubleValue() as DBILLINGCOST,
DETAILNode.get('BILLING-RETAIL').doubleValue() as DBILLINGRETAIL,
DETAILNode.get('TAX-AMOUNT').doubleValue() as DTAXAMOUNT,
DETAILNode.get('CASH-DISCOUNT').doubleValue() as DCASHDISCOUNT
FROM @APPNAME@_CQOut c, iterator(c.DETAILNode) DETAILNode;;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
  filename: '%@metadata(FileName)%',
  directory: '@DIR@',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter  ()
INPUT FROM @APPNAME@_CQOut1;

CREATE OR REPLACE TARGET OracleTarget USING DatabaseWriter (
  ConnectionURL: '', 
  Password: '', 
  Username: '',
  Tables: '',  
  CommitPolicy: 'EventCount:10,Interval:10', 
  BatchPolicy: 'EventCount:10,Interval:10'
  )
INPUT FROM @APPNAME@_CQOut1;

CREATE TARGET BigQueryTarget USING BigQueryWriter (
  Tables: '',
  projectId:'',
  BatchPolicy: 'eventCount:1, Interval:1',
  ServiceAccountKey: '',
)
INPUT FROM @APPNAME@_CQOut1;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

CREATE APPLICATION FtoKaf RECOVERY 2 SECOND INTERVAL;

CREATE OR REPLACE SOURCE FIletoRead USING FileReader  (
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '@TEST-DATA-PATH@/Validate-Striim',
  skipbom: true,
  wildcard: 'FiletoRead.txt'
 )
 PARSE USING DSVParser  (
  charset: 'UTF-8',
  handler: 'com.webaction.proc.DSVParser_1_0',
  linenumber: '-1',
  nocolumndelimiter: true,
  trimwhitespace: false,
  columndelimiter: ',',
  columndelimittill: '-1',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  parserName: 'DSVParser',
  separator: ':',
  blockascompleterecord: false,
  ignoreemptycolumn: false,
  rowdelimiter: '\n',
  header: false,
  headerlineno: 0,
  quoteset: '\"',
  trimquote: true
 )
OUTPUT TO FtoK1 ;

CREATE  TYPE FtoK2_Type  ( seq java.lang.Integer
 );

CREATE STREAM FtoK2 OF FtoK2_Type;

CREATE OR REPLACE CQ GetData
INSERT INTO FtoK2
SELECT TO_INT(data[0]) as seq
FROM FtoK1;

CREATE OR REPLACE TARGET KafkatoWrite USING KafkaWriter VERSION '0.11.0' (
  KafkaConfigPropertySeparator: ';',
  Mode: 'Sync',
  adapterName: 'KafkaWriter',
  Topic: 'kafkaTopic7',
  brokerAddress: 'localhost:9092',
  KafkaConfigValueSeparator: '=',
  KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000'
 )
FORMAT USING DSVFormatter  (   nullvalue: 'NULL',
  standard: 'none',
  handler: 'com.webaction.proc.DSVFormatter',
  formatterName: 'DSVFormatter',
  usequotes: 'false',
  rowdelimiter: '\n',
  quotecharacter: '\"',
  header: 'false',
  columndelimiter: ','
 )
INPUT FROM FtoK2;

END APPLICATION FtoKaf;

stop application AzureDLSGen1_sanity;
undeploy application AzureDLSGen1_sanity;
drop application AzureDLSGen1_sanity cascade;


create application AzureDLSGen1_sanity recovery 5 Second interval;
create source CSVSource using FileReader (
        directory:'./Samples/AppData/',
        WildCard:'dynamicdirectory.csv',
        positionByEOF:false,
        charset:'UTF-8'
)
parse using DSVParser (
        header:'no'
)
OUTPUT TO CsvStream;

create Target WriteToADLSGen1 using ADLSGen1Writer(
        filename:'',
        directory:'',
        datalakestorename:'',
        clientid:'',
        authtokenendpoint:'',
        clientkey:'',
		rolloverpolicy:'eventcount:100000'
)
format using DSVFormatter (
)
input from CsvStream; 

end application AzureDLSGen1_sanity;

deploy application AzureDLSGen1_sanity;
start application AzureDLSGen1_sanity;

STOP application SecurityApper.MultiLogDashboard;
undeploy application SecurityApper.MultiLogDashboard;
drop application SecurityApper.MultiLogDashboard cascade;

CREATE APPLICATION MultiLogDashboard;

-- This sample application shows how WebAction could be used monitor and correlate logs
-- from web and application server logs from the same web application. See the discussion
-- in the "Sample Applications" section of the WebAction documentation for additional
-- discussion.


CREATE FLOW MonitorLogs;

-- MonitorLogs sets up the two log sources used by this application. In a real-world
--implementation, each source could be reading many logs from many servers.

-- The web server logs are in Apache NCSA extended/ combined log format plus response time:
-- "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-agent}i\" %D"
-- See apache.org for more information.

CREATE SOURCE AccessLogSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'access_log',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO RawAccessStream;

CREATE TYPE AccessLogEntry (
    srcIp String KEY,
    userId String,
    sessionId String,
    accessTime DateTime,
    request String,
    code int,
    size int,
    referrer String,
    userAgent String,
    responseTime int
);
CREATE STREAM AccessStream OF AccessLogEntry;

CREATE CQ ParseAccessLog
INSERT INTO AccessStream
SELECT data[0], data[2], MATCH(data[4], ".*jsessionId=(.*) "),
       TO_DATE(data[3], "dd/MMM/yyyy:HH:mm:ss.SSS Z"), data[4], TO_INT(data[5]), TO_INT(data[6]),
       data[7], data[8], TO_INT(data[9])
FROM RawAccessStream;

-- The application server logs are in Apache's Log4J format.

CREATE SOURCE Log4JSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'log4jLog.xml',
  positionByEOF:false
)
PARSE USING XMLParser(
  rootnode:'/log4j:event',
  columnlist:'log4j:event/@timestamp,log4j:event/@level,log4j:event/log4j:message,log4j:event/log4j:throwable,log4j:event/log4j:locationInfo/@class,log4j:event/log4j:locationInfo/@method,log4j:event/log4j:locationInfo/@file,log4j:event/log4j:locationInfo/@line'
)
OUTPUT TO RawXMLStream;

CREATE TYPE Log4JEntry (
  logTime DateTime,
  level String,
  message String,
  api String,
  sessionId String,
  userId String,
  sobject String,
  xception String,
  className String,
  method String,
  fileName String,
  lineNum String
);
CREATE STREAM Log4JStream OF Log4JEntry;

CREATE CQ ParseLog4J
INSERT INTO Log4JStream
SELECT TO_DATE(TO_LONG(data[0])), data[1], data[2],
       MATCH(data[2], '\\\\[api=([a-zA-Z0-9]*)\\\\]'),
       MATCH(data[2], '\\\\[session=([a-zA-Z0-9\\-]*)\\\\]'),
       MATCH(data[2], '\\\\[user=([a-zA-Z0-9\\-]*)\\\\]'),
       MATCH(data[2], '\\\\[sobject=([a-zA-Z0-9]*)\\\\]'),
       data[3], data[4], data[5], data[6], data[7]
FROM RawXMLStream;

END FLOW MonitorLogs;


CREATE FLOW ErrorsAndWarnings;

-- ErrorsAndWarnings creates a sliding window (Log4JErrorWarningActivity) containing
-- the 300 most recent errors and warnings in the application server log. The
-- ZeroContentCheck and LargeRTCheck flows join events from this window with access log
-- events.

-- The type Log4JEntry was already defined by the MonitorLogs flow.
CREATE STREAM Log4ErrorWarningStream OF Log4JEntry;

CREATE CQ GetLog4JErrorWarning
INSERT INTO Log4ErrorWarningStream
SELECT l FROM Log4JStream l
WHERE l.level = 'ERROR' OR l.level = 'WARN';

CREATE WINDOW Log4JErrorWarningActivity
OVER Log4ErrorWarningStream KEEP 300 ROWS;

END FLOW ErrorsAndWarnings;


-- HackerCheck sends an alert when an access log srcIp value is on a blacklist.

CREATE FLOW HackerCheck;

CREATE TYPE IPEntry (
    ip String
);

/* CREATE CACHE BlackListLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'multiLogBlackList.txt',
  header: No,
  columndelimiter: ','
) QUERY (keytomap:'ip') OF IPEntry; */

CREATE CACHE BlackListLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'multiLogBlackList.txt'
)
PARSE USING DSVParser ( )
QUERY (keytomap:'ip') OF IPEntry;


CREATE STREAM HackerStream OF AccessLogEntry;

CREATE CQ FindHackers
INSERT INTO HackerStream
SELECT ale
FROM AccessStream ale, BlackListLookup bll
WHERE ale.srcIp = bll.ip;

CREATE TYPE UnusualContext (
    typeOfActivity String,
    accessTime DateTime,
    accessSessionId String,
    srcIp String KEY,
    userId String,
    country String,
    city String,
    lat double,
    lon double
);
CREATE TYPE MergedEntry (
    accessTime DateTime,
    accessSessionId String,
    srcIp String KEY,
    userId String,
    request String,
    code int,
    size int,
    referrer String,
    userAgent String,
    responseTime int,
    logTime DateTime,
    logSessionId String,
    level String,
    message String,
    api String,
    sobject String,
    xception String,
    className String,
    method String,
    fileName String,
    lineNum String
);
CREATE WACTIONSTORE UnusualActivity
CONTEXT OF UnusualContext
EVENT TYPES (MergedEntry,AccessLogEntry)
PERSIST NONE USING ( );

CREATE CQ GenerateHackerContext
INSERT INTO UnusualActivity
SELECT 'HackAttempt', accessTime, sessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM HackerStream
LINK SOURCE EVENT;

CREATE STREAM HackingAlertStream OF Global.AlertEvent;

CREATE CQ SendHackingAlerts
INSERT INTO HackingAlertStream
SELECT 'HackingAlert', ''+accessTime, 'warning', 'raise',
        'Possible Hacking Attempt from ' + srcIp + ' in ' + IP_COUNTRY(srcIp)
FROM HackerStream;

CREATE SUBSCRIPTION HackingAlertSub USING WebAlertAdapter( ) INPUT FROM HackingAlertStream;

END FLOW HackerCheck;


-- LargeRTCheck sends an alert when an access log responseTime value exceeds 2000
-- microseconds.

CREATE FLOW LargeRTCheck;

CREATE STREAM LargeRTStream of AccessLogEntry;

CREATE CQ FindLargeRT
INSERT INTO LargeRTStream
SELECT ale
FROM AccessStream ale
WHERE ale.responseTime > 2000;

CREATE WINDOW LargeRTActivity
OVER LargeRTStream KEEP 100 ROWS;

CREATE STREAM LargeRTAPIStream OF MergedEntry;

CREATE CQ MergeLargeRTAPI
INSERT INTO LargeRTAPIStream
SELECT lrt.accessTime, lrt.sessionId, lrt.srcIp, lrt.userId, lrt.request,
       lrt.code, lrt.size, lrt.referrer, lrt.userAgent, lrt.responseTime,
       log4j.logTime, log4j.sessionId, log4j.level, log4j.message, log4j.api, log4j.sobject, log4j.xception,
       log4j.className, log4j.method, log4j.fileName, log4j.lineNum
FROM LargeRTActivity lrt, Log4JErrorWarningActivity log4j
WHERE lrt.sessionId = log4j.sessionId
      AND lrt.accessTime = log4j.logTime;

CREATE CQ GenerateLargeRTContext
INSERT INTO UnusualActivity
SELECT 'LargeResponseTime', accessTime, accessSessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM LargeRTAPIStream
LINK SOURCE EVENT;

CREATE STREAM LargeRTAlertStream OF Global.AlertEvent;

CREATE CQ SendLargeRTAlerts
INSERT INTO LargeRTAlertStream
SELECT 'LargeRTAlert', ''+accessTime, 'warning', 'raise',
        'Long response time for call from ' + userId + ' api ' + api + ' message ' + message
FROM LargeRTAPIStream;

CREATE SUBSCRIPTION LargeRTAlertSub USING WebAlertAdapter( ) INPUT FROM LargeRTAlertStream;

END FLOW LargeRTCheck;


-- ProxyCheck sends an alert when an access log srcIP value is on a list of suspicious
-- proxies.

CREATE FLOW ProxyCheck;

/* CREATE CACHE ProxyLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'multiLogProxies.txt',
  header: No,
  columndelimiter: ','
) QUERY (keytomap:'ip') OF IPEntry; */

CREATE CACHE ProxyLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'multiLogProxies.txt'
)
PARSE USING DSVParser ( )
QUERY (keytomap:'ip') OF IPEntry;


CREATE STREAM ProxyStream OF AccessLogEntry;

CREATE CQ FindProxies
INSERT INTO ProxyStream
SELECT ale
FROM AccessStream ale, ProxyLookup pl
WHERE ale.srcIp = pl.ip;

CREATE CQ GenerateProxyContext
INSERT INTO UnusualActivity
SELECT 'ProxyAccess', accessTime, sessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM ProxyStream
LINK SOURCE EVENT;


CREATE STREAM ProxyAlertStream OF Global.AlertEvent;

CREATE CQ SendProxyAlerts
INSERT INTO ProxyAlertStream
SELECT 'ProxyAlert', ''+accessTime, 'warning', 'raise',
        'Possible use of Proxy from ' + srcIp + ' in ' + IP_COUNTRY(srcIp) + ' for user ' + userId
FROM ProxyStream;

CREATE SUBSCRIPTION ProxyAlertSub USING WebAlertAdapter( ) INPUT FROM ProxyAlertStream;

END FLOW ProxyCheck;


-- ZeroContentCheck sends an alert when an access log entry's code value is 200 (that is,
-- the HTTP request succeeded) but the size value is 0 (the return had no content).

CREATE FLOW ZeroContentCheck;

CREATE STREAM ZeroContentStream of AccessLogEntry;

CREATE CQ FindZeroContent
INSERT INTO ZeroContentStream
SELECT ale
FROM AccessStream ale
WHERE ale.code = 200 AND ale.size = 0;

CREATE WINDOW ZeroContentActivity
OVER ZeroContentStream KEEP 100 ROWS;

CREATE STREAM ZeroContentAPIStream OF MergedEntry;

CREATE CQ MergeZeroContentAPI
INSERT INTO ZeroContentAPIStream
SELECT zcs.accessTime, zcs.sessionId, zcs.srcIp, zcs.userId, zcs.request,
       zcs.code, zcs.size, zcs.referrer, zcs.userAgent, zcs.responseTime,
       log4j.logTime, log4j.sessionId, log4j.level, log4j.message, log4j.api, log4j.sobject, log4j.xception,
       log4j.className, log4j.method, log4j.fileName, log4j.lineNum
FROM ZeroContentActivity zcs, Log4JErrorWarningActivity log4j
WHERE zcs.sessionId = log4j.sessionId
      AND zcs.accessTime = log4j.logTime;

CREATE CQ GenerateZeroContentContext
INSERT INTO UnusualActivity
SELECT 'ZeroContent', accessTime, accessSessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM ZeroContentAPIStream
LINK SOURCE EVENT;


CREATE TYPE ZeroContentEventListType (
    srcIp String KEY,
    code int,
    size int,
    level String,
    message String,
    xception String);

CREATE WACTIONSTORE ZeroContentEventList
CONTEXT OF ZeroContentEventListType
EVENT TYPES (ZeroContentEventListType )
PERSIST NONE USING ( );


CREATE CQ GenerateZeroContentEventList
INSERT INTO ZeroContentEventList
SELECT srcIp, code, size, level, message, xception
FROM ZeroContentAPIStream;


CREATE STREAM ZeroContentAlertStream OF Global.AlertEvent;

CREATE CQ SendZeroContentAlerts
INSERT INTO ZeroContentAlertStream
SELECT 'ZeroContentAlert', ''+accessTime, 'warning', 'raise',
        'Zero content returned in call from ' + userId + ' api ' + api + ' message ' + message
FROM ZeroContentAPIStream;

CREATE SUBSCRIPTION ZeroContentAlertSub USING WebAlertAdapter( ) INPUT FROM ZeroContentAlertStream;

END FLOW ZeroContentCheck;


-- ErrorHandling is functionally identical to ErrorFlow.SaasMonitorApp. It sends an alert
-- when an error message appears in the application server log.

CREATE FLOW ErrorHandling;

CREATE STREAM ErrorStream OF Log4JEntry;

CREATE CQ GetErrors
INSERT INTO ErrorStream
SELECT log4j
FROM Log4ErrorWarningStream log4j WHERE log4j.level = 'ERROR';

CREATE STREAM ErrorAlertStream OF Global.AlertEvent;

CREATE CQ SendErrorAlerts
INSERT INTO ErrorAlertStream
SELECT 'ErrorAlert', ''+logTime, 'error', 'raise', 'Error in log ' + message
FROM ErrorStream;

CREATE SUBSCRIPTION ErrorAlertSub USING WebAlertAdapter( ) INPUT FROM ErrorAlertStream;

END FLOW ErrorHandling;


-- WarningHandling is a minor variation on WarningFlow.SaasMonitorApp. It sends an alert
-- once an hour with the count of warnings for each api call for which there has been at
-- least one alert.

CREATE FLOW WarningHandling;

CREATE STREAM WarningStream OF Log4JEntry;

CREATE CQ GetWarnings
INSERT INTO WarningStream
SELECT log4j
FROM Log4ErrorWarningStream log4j WHERE log4j.level = 'WARN';

CREATE JUMPING WINDOW WarningWindow
OVER WarningStream KEEP WITHIN 60 MINUTE ON logTime;

CREATE STREAM WarningAlertStream OF Global.AlertEvent;

CREATE CQ SendWarningAlerts
INSERT INTO WarningAlertStream
SELECT 'WarningAlert', ''+logTime, 'warning', 'raise',
        COUNT(logTime) + ' Warnings in log for api ' + api
FROM WarningWindow
GROUP BY api
HAVING count(logTime) > 1;

CREATE SUBSCRIPTION WarningAlertSub USING WebAlertAdapter( ) INPUT FROM WarningAlertStream;

END FLOW WarningHandling;


-- InfoFlow is functionally similar to InfoFlow.SaasMonitorApp. Its output is used by
-- ApiFlow, CompanyApiFlow, and UserApiFlow.

CREATE FLOW InfoFlow;

CREATE TYPE UserInfo (
  userId String,
  userName String,
  company String,
  userZip String,
  companyZip String
);

/* CREATE CACHE MLogUserLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'multiLogUser.csv',
  header: No,
  columndelimiter: ','
) QUERY (keytomap:'userId') OF UserInfo; */

CREATE CACHE MLogUserLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'multiLogUser.csv'
)
PARSE USING DSVParser ( )
QUERY (keytomap:'userId') OF UserInfo;

CREATE STREAM InfoStream OF Log4JEntry;

CREATE CQ GetInfo
INSERT INTO InfoStream
SELECT log4j
FROM Log4JStream log4j WHERE log4j.level = 'INFO';

CREATE TYPE ApiCall (
  userId String,
  api String,
  sobject String,
  logTime DateTime,
  userName String,
  company String,
  userZip String,
  companyZip String
);
CREATE STREAM ApiEnrichedStream OF ApiCall;

CREATE CQ GetUserDetails
INSERT INTO ApiEnrichedStream
SELECT a.userId, a.api, a.sobject, a.logTime, u.userName, u.company, u.userZip, u.companyZip
FROM InfoStream a, MLogUserLookup u
WHERE a.userId = u.userId;

END FLOW InfoFlow;


-- ApiFlow populates the dashboard's Detail - ApiActivity page and the pie chart on the
-- Overview page.

CREATE FLOW ApiFlow;

CREATE TYPE ApiUsage (
  api String key,
  sobject String,
  count int,
  logTime DateTime
);

CREATE TYPE ApiContext (
  api String key,
  count int,
  logTime DateTime
);

CREATE WACTIONSTORE ApiActivity
CONTEXT OF ApiContext
EVENT TYPES (ApiUsage )
PERSIST NONE USING ( );

CREATE JUMPING WINDOW ApiWindow
OVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY api;

CREATE STREAM ApiUsageStream OF ApiUsage;

CREATE CQ GetApiUsage
INSERT INTO ApiUsageStream
SELECT a.api, a.sobject,
       COUNT(a.userId), FIRST(a.logTime)
FROM ApiWindow a
GROUP BY a.api, a.sobject HAVING FIRST(a.logTime) IS NOT NULL;

CREATE JUMPING WINDOW ApiSummaryWindow
OVER ApiUsageStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY api;

CREATE CQ GetApiSummaryUsage
INSERT INTO ApiActivity
SELECT a.api,
       sum(a.count), first(a.logTime)
FROM ApiSummaryWindow a
GROUP BY a.api
LINK SOURCE EVENT;

END FLOW ApiFlow;


-- CompanyApiFlow populates the dashboard's Detail - CompanyApiActivity page and the bar
-- chart on the Overview page. It also sends an alert when an API call is used by a
-- company more than 1500 times during the flow's one-hour jumping window.

CREATE FLOW CompanyApiFlow;

CREATE TYPE CompanyApiUsage (
  company String key,
  companyZip String,
  companyLat double,
  companyLong double,
  api String,
  count int,
  unusual int,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE TYPE CompanyApiContext (
  company String key,
  companyZip String,
  companyLat double,
  companyLong double,
  count int,
  unusual int,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE WACTIONSTORE CompanyApiActivity
CONTEXT OF CompanyApiContext
EVENT TYPES ( CompanyApiUsage )
PERSIST NONE USING ( );

CREATE JUMPING WINDOW CompanyApiWindow
OVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY company;

CREATE STREAM CompanyApiUsageStream OF CompanyApiUsage;

CREATE TYPE MLogUSAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

/*CREATE CACHE MLogZipLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  header: Yes,
  columndelimiter: ','
) QUERY (keytomap:'zip') OF MLogUSAddressData; */

CREATE CACHE MLogZipLookup using FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt'
)
PARSE USING DSVParser (
  header: Yes,
  columndelimiter: '\t',
  trimquote:false

)
QUERY (keytomap:'zip') OF MLogUSAddressData;


CREATE CQ GetCompanyApiUsage
INSERT INTO CompanyApiUsageStream
SELECT a.company, a.companyZip, z.latVal, z.longVal,
       a.api, COUNT(a.sobject),
       CASE WHEN COUNT(a.sobject) > 1500 THEN 1
            ELSE 0 END,
       CASE WHEN COUNT(a.sobject) > 1500 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.sobject),
       FIRST(a.logTime)
FROM CompanyApiWindow a, MLogZipLookup z
WHERE a.companyZip = z.zip
GROUP BY a.company, a.api HAVING FIRST(a.logTime) IS NOT NULL;

CREATE JUMPING WINDOW CompanyWindow
OVER CompanyApiUsageStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY company;

CREATE CQ GetCompanyUsage
INSERT INTO CompanyApiActivity
SELECT a.company, a.companyZip, a.companyLat, a.companyLong,
       SUM(a.count), SUM(a.unusual),
       CASE WHEN SUM(a.unusual) > 0 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.topObject),
       FIRST(a.logTime)
FROM CompanyWindow a
GROUP BY a.company
LINK SOURCE EVENT;

CREATE STREAM CompanyAlertStream OF Global.AlertEvent;

CREATE CQ SendCompanyApiAlerts
INSERT INTO CompanyAlertStream
SELECT 'CompanyAPIAlert', ''+logTime, 'warning', 'raise',
       'Company ' + company + ' has used api ' + api + ' ' + count + ' times for ' + topObject
FROM CompanyApiUsageStream
WHERE unusual = 1;

CREATE SUBSCRIPTION CompanyAlertSub USING WebAlertAdapter( ) INPUT FROM CompanyAlertStream;

END FLOW CompanyApiFlow;


-- UserApiFlow populates the dashboard's Detail - UserApiActivity page and the US map on
-- the Overview page. It also sends an alert when an API call is used by a user more than
-- 125 times during the flow's one-hour window.

CREATE FLOW UserApiFlow;

CREATE TYPE UserApiUsage (
  userId String key,
  userName String,
  userZip String,
  userLat double,
  userLong double,
  company String,
  api String,
  count int,
  unusual int,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE TYPE UserApiContext (
  userId String key,
  userName String,
  userZip String,
  userLat double,
  userLong double,
  company String,
  count int,
  unusual int,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE WACTIONSTORE UserApiActivity
CONTEXT OF UserApiContext
EVENT TYPES (  UserApiUsage )
PERSIST NONE USING ( );

CREATE JUMPING WINDOW UserApiWindow
OVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY userId;

CREATE STREAM UserApiUsageStream OF UserApiUsage;

CREATE CQ GetUserApiUsage
INSERT INTO UserApiUsageStream
SELECT a.userId, a.userName, a.userZip, z.latVal, z.longVal, a.company,
       a.api, COUNT(a.sobject),
       CASE WHEN COUNT(a.sobject) > 125 THEN 1
            ELSE 0 END,
       CASE WHEN COUNT(a.sobject) > 125 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.sobject),
       FIRST(a.logTime)
FROM UserApiWindow a, MLogZipLookup z
WHERE a.userZip = z.zip
GROUP BY a.userId, a.api HAVING FIRST(a.logTime) IS NOT NULL;

CREATE JUMPING WINDOW UserWindow
OVER UserApiUsageStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY userId;

CREATE CQ GetUserUsage
INSERT INTO UserApiActivity
SELECT a.userId, a.userName, a.userZip, a.userLat, a.userLong,
       a.company, SUM(a.count), SUM(a.unusual),
       CASE WHEN SUM(a.unusual) > 0 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.topObject),
       FIRST(a.logTime)
FROM UserWindow a
GROUP BY a.userId
LINK SOURCE EVENT;

CREATE STREAM UserAlertStream OF Global.AlertEvent;

CREATE CQ SendUserApiAlerts
INSERT INTO UserAlertStream
SELECT 'UserAPIAlert', ''+logTime, 'warning', 'raise',
       'User ' + userName + ' has used api ' + api + ' ' + count + ' times for ' + topObject
FROM UserApiUsageStream
WHERE unusual = 1;

CREATE SUBSCRIPTION UserAlertSub USING WebAlertAdapter( ) INPUT FROM UserAlertStream;

END FLOW UserApiFlow;


CREATE VISUALIZATION MultiLogApp "@FEATURE-DIR@/tql/MultiLogApp_visualization_settings.json";

END APPLICATION MultiLogDashboard;

create dashboard using "@FEATURE-DIR@/tql/MultiLogAppDashboard.json";

stop application reconnect;
undeploy application reconnect;
drop application reconnect cascade;
CREATE APPLICATION reconnect recovery 1 second interval;

CREATE  SOURCE OracleSource USING OracleReader  ( 
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  ConnectionURL: '@URL@',
  Tables: 'qatest.ReconnectTestSource;qatest.ReconnectTestSourceDummy',
  FetchSize: '@FETCHSIZE@'
 )output to sqlstream;
 --output to sqlstream MAP (table:'qatest.ReconnectTestSource');

CREATE TARGET dbtarget USING DatabaseWriter(
  ConnectionURL:'@URL@',
  Username:'@USERNAME@',
  Password:'@PASSWORD@',
  ConnectionRetryPolicy: 'retryInterval=10s, maxRetries=4',
  BatchPolicy:'EventCount:5000,Interval:30',
  CommitPolicy:'EventCount:30000,Interval:30',
  Tables: '@TABLES@'
 ) INPUT FROM sqlstream;
 
 create target filetgt using filewriter(
 filename:'recovering_validation',
 rolloverpolicy:'eventcount:1000000,interval:100m',
 flushpolicy:'eventcount:1000000,interval:100m'
 )format using dsvformatter()
 input from sqlstream;

--create Target tSysOut using Sysout(name:OrgData) input from sqlstream;
 end application reconnect;
 deploy application reconnect;
 start application reconnect;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE APPLICATION @APPNAME@ RECOVERY 10 SECOND INTERVAL;

 create flow Mysqlflow;
CREATE SOURCE MysqlToDBRoutersource USING MysqlReader
(
  Username: '',
  Password: '',
  Tables: '',
  ConnectionURL: '',
  Password_encrypted: 'false',
  connectionRetryPolicy: 'retryInterval=30, maxRetries=3'
)
OUTPUT TO RouterTestMasterStream;

end flow Mysqlflow;

CREATE OR REPLACE ROUTER RouterTestRs1 INPUT FROM RouterTestMasterStream s CASE
WHEN meta(s,"TableName").toString()='waction.source1' THEN ROUTE TO RouterTestTyped1,
WHEN meta(s,"TableName").toString()='waction.source2' THEN ROUTE TO RouterTestTyped2,
ELSE ROUTE TO RouterTestTypedElse;

CREATE TARGET MysqlToDBRoutertarget1 USING DatabaseWriter(
   Username: '',
   Password: '',
   Tables: '',
   ConnectionURL: '',
   Password_encrypted: 'false',
   connectionRetryPolicy: 'retryInterval=30, maxRetries=3'
) INPUT FROM RouterTestTyped1;


CREATE TARGET MysqlToDBRoutertarget2 USING DatabaseWriter(
    Username: '',
    Password: '',
    Tables: '',
    ConnectionURL: '',
    Password_encrypted: 'false',
    connectionRetryPolicy: 'retryInterval=30, maxRetries=3'
) INPUT FROM RouterTestTyped2;


end application @APPNAME@;
deploy application @APPNAME@ with Mysqlflow in AGENTs;
start application @APPNAME@;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;


 CREATE SOURCE DBSource USING MySQLReader (
 Compression: true,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: '@CONNECTION_URL@',
  DatabaseName: 'testcassandra',
  Tables: '@SOURCE_TABLE@',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)
OUTPUT TO Mysql_ChangeDataStream;
CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'test.chkpoint',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  IgnorableExceptionCode:'PRIMARY KEY',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 ) 
INPUT FROM Mysql_ChangeDataStream;
create Target t2 using SysOut(name:Foo2) input from Mysql_ChangeDataStream;

END APPLICATION DBRTOCW;
deploy application DBRTOCW in default;
start application DBRTOCW;

stop application DualGen;
undeploy application DualGen;
drop application DualGen cascade;
CREATE APPLICATION DualGen;

CREATE OR REPLACE TYPE DualEvent (
     Dummy DateTime,
    PhoneNo java.lang.String
);

CREATE OR REPLACE STREAM DualEvents OF DualEvent;

CREATE OR REPLACE CQ GenDual 
INSERT INTO DualEvents
SELECT
    TO_DATEF('28-FEB-22',"dd-MMM-yy") as Dummy,
    maskPhoneNumber('44 844 493 0787', "(\\\\d{0,4}\\\\s)(\\\\d{0,4}\\\\s)([0-9 ]+)", 1, 2) as PhoneNo
FROM
   heartbeat(interval 10 second) h;

   CREATE OR REPLACE CQ GenDual2 
INSERT INTO DualEvents
SELECT
   TO_DATEF('12/JAN/32',"dd/MMM/yy") as Dummy,
   maskPhoneNumber('12 345 678 9101', "(\\\\d{0,4}\\\\s)(\\\\d{0,4}\\\\s)([0-9 ]+)", 1, 2) as PhoneNo
FROM
   heartbeat(interval 30 second) h;

CREATE  SOURCE Orac_Src USING OracleReader  ( 
  Compression: false,
  DictionaryMode: 'OnlineCatalog',
  StartTimestamp: 'null',
  SupportPDB: false,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  FetchSize: 1,
  QuiesceMarkerTable: 'QUIESCEMARKER',
  DDLCaptureMode: 'All',
  CommittedTransactions: true,
  QueueSize: 2048,
  ReaderType: 'LogMiner',
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Username: 'miner',
  Password: 'miner',
  Tables: 'QATEST.HEARTBEAT',
  OutboundServerProcessName: 'WebActionXStream',
  Password_encrypted: false
 ) 
OUTPUT TO CDC_Events ;

CREATE OR REPLACE CQ cdcdual 
INSERT INTO DualEvents
SELECT
   TO_DATE(data[0]) as Dummy,
   data[1] as PhoneNo
FROM
   CDC_Events;


CREATE OR REPLACE TARGET DualSys USING SysOut  ( 
  name: 'heartbeat_out'
 ) 
INPUT FROM DualEvents;

CREATE TARGET DSVFormatterOut using FileWriter(
 filename:'HeartBeat_Reader.log',
 flushpolicy:'EventCount:16',
 rolloverpolicy:'EventCount:16,interval:117s')
FORMAT USING DSVFormatter ()
INPUT FROM DualEvents;


END APPLICATION DualGen;
deploy application DualGen;
start application DualGen;

STOP APPLICATION MysqlToDbApp;
UNDEPLOY APPLICATION MysqlToDbApp;
DROP APPLICATION MysqlToDbApp CASCADE;


CREATE APPLICATION MysqlToDbApp RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE MysqlSource USING MysqlReader
(
  Username: '',
  Password: '',
  Tables: '',
  ConnectionURL: '',
  Password_encrypted: 'false',
  connectionRetryPolicy: 'retryInterval=30, maxRetries=3'
)
OUTPUT TO MysqlReaderOut;


CREATE TARGET DbTarget USING DatabaseWriter
(
  Username: '',
  Password: '',
  Tables: '',
  ConnectionURL: '',
  Password_encrypted: 'false',
  connectionRetryPolicy: 'retryInterval=30, maxRetries=3'
)
INPUT FROM MysqlReaderOut;




END APPLICATION MysqlToDbApp;
DEPLOY APPLICATION MysqlToDbApp;
START APPLICATION MysqlToDbApp;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

create application @appname@;

CREATE SOURCE @parquetsrc@ USING FileReader (
  directory: '',
  positionByEOF: false,
  WildCard: '')
PARSE USING ParquetParser ()
OUTPUT TO @appname@Streams;

CREATE CQ @appname@CQ1
INSERT INTO @appname@out
SELECT convertParquetEventToWAEvent(o) FROM @appname@Streams o;

CREATE TARGET @filetarget@ USING Global.FileWriter (
  directory: '',
  DataEncryptionKeyPassphrase: '',
  flushpolicy: 'EventCount:10000,Interval:30s',
  rolloveronddl: 'true',
  encryptionpolicy: '',
  filename: '',
  DataEncryptionKeyPassphrase_encrypted: 'true',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  (
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  members: 'data',
  jsonobjectdelimiter: '\n' )
INPUT FROM @appname@out;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

CREATE OR REPLACE PROPERTYVARIABLE Mode='sync';
CREATE OR REPLACE PROPERTYVARIABLE BatchPolicy='Size:900000,Interval:1';

create application KinesisTest;
create source CSVSource using FileReader (
	directory:'/home/dz/src/product/Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  companyName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0], data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create or replace Target t using KinesisWriter (
	regionName:'TARGET_REGION',
	streamName:'TARGET_STREAM',
	BatchPolicy: '$BatchPolicy',
    Mode: '$Mode'	
)
format using JSONFormatter (
)
input from TypedCSVStream;
end application KinesisTest;
deploy application KinesisTest in default;
start application KinesisTest;

CREATE SOURCE @SOURCE_NAME@ USING Global.IncrementalBatchReader (
  ConnectionURL: @sourceURL@,
  Username: '@userName',
  Tables: '@tables@',
  CheckColumn: '@checkColumn',
  FetchSize: 1,
  ReturnDateTimeAs: 'JODA',
  Password: '@password',
  Password_encrypted: 'false',
  DatabaseProviderType: 'Default',
  ThreadPoolSize: 5,
  pollingInterval: '2sec',
  ConnectionPoolSize: 1 )
OUTPUT TO @STREAM@;

CREATE OR REPLACE PROPERTYSET LDAP1 ( PROVIDER_URL:"ldap://10.77.12.210:389", SECURITY_AUTHENTICATION:simple, SECURITY_PRINCIPAL: "praveen,dc=qa,dc=webaction,dc=com" , SECURITY_CREDENTIALS:InvalidPwd, USER_BASE_DN:"dc=qa,dc=webaction,dc=com", User_userId:cn );

STOP APPLICATION VerticaTester.VW;
UNDEPLOY APPLICATION VerticaTester.VW;
DROP APPLICATION VerticaTester.VW CASCADE;
CREATE APPLICATION VW;

CREATE SOURCE CSVSource USING CSVReader
(
  directory:'@TEST-DATA-PATH@',
  header:No,
  wildcard:'trade.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStream;


CREATE TYPE VerticaStream_t
(
  NameField   String,
  ValueField  String
);

CREATE STREAM VerticaStream OF VerticaStream_t;

CREATE CQ VW_q
  INSERT INTO VerticaStream
  SELECT DATA[0],
         DATA[1]
  FROM CsvStream;

CREATE TARGET WriteToVertica USING VerticaWriter
(
  dbHostName:'@HOST@',
  dbUser:'@USER@',
  dbPassword: '@PASS@',
  dbName: '@DBNAME@',
  tableName: '@SCHEMA@.TRADE'
)
INPUT FROM VerticaStream;

END APPLICATION VW;

stop application PublishAPITester.NS;
undeploy application PublishAPITester.NS;
drop application PublishAPITester.NS cascade;

create application NS;

create Stream MyStream of Global.WAEvent;

CREATE TYPE bankData
(
bankID Integer KEY,
bankName String
);


CREATE TYPE moreBankData
(
bankID Integer KEY,
bankName String,
bankRouting long,
bankAmount double
);

create stream bankStream of bankData;

create stream dataStream of moreBankData;

--create Target t3 using SysOut(name:AgentOut) input from MyStream;

end application NS;

DEPLOY APPLICATION NS; 
start NS;

stop application SQLtoRedshift;
undeploy application SQLtoRedshift;
drop application SQLtoRedshift cascade;
CREATE APPLICATION SQLtoRedshift RECOVERY 1 SECOND INTERVAL;

CREATE  SOURCE SQLSource USING MSSqlReader  ( 
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  DatabaseName: '@DATABASE-NAME@',
  ConnectionURL: '@URL@',
  Tables: '@SOURCE-TABLES@'
 ) 
OUTPUT TO sqlstream ;

--CREATE  TARGET t2 USING SysOut  ( name: 'sqltors') INPUT FROM sqlstream;

CREATE TARGET RedshiftTarget USING RedshiftWriter
	(
	  ConnectionURL: '@TARGET-URL@',
	  Username: '@TARGET-UNAME@',
	  Password: '@TARGET-PASSWORD@',
	  bucketname: '@BUCKETNAME@',
	  --accesskeyId: '@ACCESS-KEY-ID@',
	  --secretaccesskey: '@SECRET-ACCESS-KEY@',
	  S3IAMRole:'@IAMROLE@',
	  Tables: '@TARGET-TABLES@',
	  uploadpolicy:'eventcount:1,interval:5s',
	  Mode:'incremental'
	) INPUT FROM sqlstream;

END APPLICATION SQLtoRedshift;
deploy application SQLtoRedshift;
start application SQLtoRedshift;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@;

create TYPE CountTYPE(numcol INT);

CREATE JUMPING WINDOW nEvents OVER @STREAM@ KEEP 10 ROWS;

CREATE STREAM TypedCountStream of CountTYPE;

CREATE CQ CountCQ INSERT INTO TypedCountStream SELECT TO_INT(data[0]) FROM nEvents;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@;


CREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  --Password_encrypted: false
 )
INPUT FROM @STREAM@;

CREATE APPLICATION  @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE  @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'JsonNodeEvent.json',
positionByEOF:false
)
PARSE USING Global.JSONParser (
 )  OUTPUT TO  @AppName@_rawstream;

CREATE CQ @BuiltinFunc@CQ
INSERT INTO  @BuiltinFunc@_Stream
SELECT @BuiltinFunc@(x, 'Sno', data.get("_id"), 'Name', data.get("firstname"))
FROM @AppName@_rawstream x;

CREATE OR REPLACE CQ cq1
INSERT INTO RemoveUserData_Stream
SELECT
removeUserData(s1, 'Sno')
FROM @BuiltinFunc@_Stream s1;

CREATE OR REPLACE TARGET  @AppName@_FileTarget USING Global.FileWriter (
  flushpolicy: 'EventCount:10000,Interval:30s',
  adapterName: 'FileWriter',
 directory: '@logs@',
  filename: '@BuiltinFunc@_JsonEventRemoveData',
  rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING Global.JSONFormatter  (
  handler: 'com.webaction.proc.JSONFormatter',
  jsonMemberDelimiter: '\n',
  EventsAsArrayOfJsonObjects: 'true',
  formatterName: 'JSONFormatter',
  jsonobjectdelimiter: '\n' )
INPUT FROM RemoveUserData_Stream;

End application  @AppName@;
Deploy application  @AppName@;
Start application  @AppName@;

CREATE APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE @AppName@_Source USING FileReader (
directory:'@dataDir@',
wildcard:'data.csv',
positionByEOF:false
)
PARSE USING DSVParser (
) OUTPUT TO @AppName@_rawstream;


CREATE CQ @BuiltinFunc@CQ
INSERT INTO @BuiltinFunc@_Stream
SELECT @BuiltinFunc@(x, 'Last_Date', data[5], 'Country', data[10])
FROM @AppName@_rawstream x;

CREATE OR REPLACE CQ cq1
INSERT INTO RemoveUserData_Stream
SELECT
removeUserData(s1, 'Last_Date')
FROM @BuiltinFunc@_Stream s1;


CREATE OR REPLACE TARGET @AppName@_FileTarget USING Global.FileWriter ( 
  flushpolicy: 'EventCount:10000,Interval:30s', 
  adapterName: 'FileWriter', 
  directory: '@logs@',
  filename: '@BuiltinFunc@_RemoveData', 
  rolloverpolicy: 'EventCount:10000,Interval:30s' ) 
FORMAT USING Global.JSONFormatter  ( 
  handler: 'com.webaction.proc.JSONFormatter', 
  jsonMemberDelimiter: '\n', 
  EventsAsArrayOfJsonObjects: 'true', 
  formatterName: 'JSONFormatter', 
  jsonobjectdelimiter: '\n' ) 
INPUT FROM RemoveUserData_Stream;

End application @AppName@;
Deploy application @AppName@; 
Start application @AppName@;

stop application logminer;
undeploy application logminer;
drop application logminer cascade;

create application logminer;

Create Source Rac11g Using OracleReader
(
 --StartTimestamp:'15-JAN-2015 13:00:40',
 Username:'miner',
 Password:'miner',
 ConnectionURL:'10.1.110.128:1521:orcl',
 --Tables:'SCOTT.SIMPLETEST',
 Tables:'QATEST.SAMPLETEST2',
 OnlineCatalog:true,
 FetchSize:1,
 QueueSize:2148,
 CommittedTransactions:false,
 Compression:false,
 FilterTransactionState:false
)
Output To LCRStream;

create target myout using sysout(name: logminer) input from LCRStream;
create Target y using logwriter(name:GitCommitInfo,filename:logminer) input from LCRStream;

end application logminer;
deploy application logminer;
start application logminer;

CREATE TARGET @TARGET_NAME@ USING SpannerWriter (
  instanceId: 'qatest',
  BatchPolicy: 'EventCount:10000,Interval:60',
  Tables: 'QATEST.ORACLETOSPANNER_SOURCETABLE%,oracletospannerdb.OracleToSpannertarget',
  adapterName: 'SpannerWriter',
  ServiceAccountKey: '/Users/jenniffer/Product2/IntegrationTests/TestData/google-gcs.json'
) INPUT FROM @STREAM@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

CREATE  SOURCE @QuerySource@1 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@1;
create Target @tgtsys@1 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@1;
 CREATE  TARGET @Querytarget@1 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@1;

CREATE  SOURCE @QuerySource@2 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@2;
create Target @tgtsys@2 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@2;
 CREATE  TARGET @Querytarget@2 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@2;

CREATE  SOURCE @QuerySource@3 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@3;
create Target @tgtsys@3 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@3;
 CREATE  TARGET @Querytarget@3 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@3;

CREATE  SOURCE @QuerySource@4 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@4;
create Target @tgtsys@4 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@4;
 CREATE  TARGET @Querytarget@4 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@4;

CREATE  SOURCE @QuerySource@5 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@5;
create Target @tgtsys@5 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@5;
 CREATE  TARGET @Querytarget@5 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@5;

CREATE  SOURCE @QuerySource@6 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@6;
create Target @tgtsys@6 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@6;
 CREATE  TARGET @Querytarget@6 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@6;

CREATE  SOURCE @QuerySource@7 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@7;
create Target @tgtsys@7 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@7;
 CREATE  TARGET @Querytarget@7 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@7;

CREATE  SOURCE @QuerySource@8 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@8;
create Target @tgtsys@8 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@8;
 CREATE  TARGET @Querytarget@8 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@8;

CREATE  SOURCE @QuerySource@9 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@9;
create Target @tgtsys@9 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@9;
 CREATE  TARGET @Querytarget@9 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@9;

CREATE  SOURCE @QuerySource@10 USING DatabaseReader  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  Query: '@SourceQuery@'
 )
OUTPUT TO @SRCQueryINPUTSTREAM@10;
create Target @tgtsys@10 using SysOut(name:Foo4) input from @SRCQueryINPUTSTREAM@10;
 CREATE  TARGET @Querytarget@10 USING DatabaseWriter  (
  ConnectionURL:'jdbc:mysql://127.0.0.1:3306/waction',
  Username:'root',
  Password:'w@ct10n',
  BatchPolicy: 'EventCount:1,Interval:0',
  Tables: 'QUERY,qatest.@TargetTable@'
 )
INPUT FROM @SRCQueryINPUTSTREAM@10;
end application @APPNAME@;

stop application @APPNAME1@;
undeploy application @APPNAME1@;
stop application @APPNAME2@;
undeploy application @APPNAME2@;
drop application @APPNAME1@ cascade;
drop application @APPNAME2@ cascade;


CREATE APPLICATION @APPNAME1@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;
CREATE OR REPLACE STREAM @SRCINPUTSTREAM@ OF Global.WAEvent PERSIST USING Global.DefaultKafkaProperties;
CREATE OR REPLACE SOURCE @SourceName@ Using PostgreSQLReader
(
   adapterName: PostgreSQLReader,
   CDDLAction: Quiesce_Cascade,
   CDDLCapture: true,
   CDDLTrackingTable:'striim.ddlcapturetable',
   ConnectionURL: jdbc:postgresql://localhost:5432/qatest,
   FilterTransactionBoundaries: true,
   Password: w@ct10n,
   ReplicationSlotName:'test_slot',
   Tables: public.sample,
   Username: sa,
)OUTPUT TO @SRCINPUTSTREAM@;

End APPLICATION @APPNAME1@;
DEPLOY APPLICATION @APPNAME1@;
START APPLICATION @APPNAME1@;

CREATE APPLICATION @APPNAME2@ recovery 5 SECOND Interval USE EXCEPTIONSTORE;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'qatest',
  BatchPolicy:'EventCount:1',
  CommitPolicy:'EventCount:1',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/XE',
  Tables:'QATEST.ORCTOORCQUIESCESOURCESOURCE,QATEST.ORCTOORCQUIESCESOURCETARGET1',
  Password:'qatest',
  CDDLAction:'Process'
) 
INPUT FROM @SRCINPUTSTREAM@;

End APPLICATION @APPNAME2@;
DEPLOY APPLICATION @APPNAME2@;
START APPLICATION @APPNAME2@;

stop application recoveryTestAgent.CSV;
undeploy application recoveryTestAgent.CSV;
drop application recoveryTestAgent.CSV cascade;

create application CSV
RECOVERY 5 SECOND INTERVAL;

CREATE FLOW AgentFlow;
create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails-recovery.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStream;
END FLOW AgentFlow;

CREATE FLOW ServerFlow;
CREATE TYPE UserDataType
(
  UserId String KEY,
  UserName String
);

CREATE STREAM UserDataStream OF UserDataType PARTITION BY UserId;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  data[0],
        data[1]
FROM CsvStream;


CREATE WACTIONSTORE UserActivityInfo
CONTEXT OF UserDataType
EVENT TYPES ( UserDataType )
@PERSIST-TYPE@

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction
INSERT INTO UserActivityInfo
SELECT * FROM UserDataStream
LINK SOURCE EVENT;
END FLOW ServerFlow;

END APPLICATION CSV;
DEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;
START CSV;

stop OracleTOFileWriterApp;
undeploy application OracleTOFileWriterApp;
drop application OracleTOFileWriterApp cascade;

CREATE APPLICATION OracleTOFileWriterApp recovery 5 second interval;

Create Source ReadFromOracle
Using OracleReader
(
Username:'qatest',
Password:'qatest',
ConnectionURL:'jdbc:oracle:thin:@localhost:1521:xe',
Tables:'QATEST.VARRAY_AS_VARCHAR',
OnlineCatalog:true,
CommittedTransactions:true
)
Output To DataStream;


CREATE OR REPLACE TARGET FileWriter1 USING FileWriter  (
filename: 'OutFile.csv',
  directory:'@DIR@/%@metadata(TableName)%',
  rolloverpolicy: 'EventCount:6,Interval:60s'
)
format using DSVFormatter (
members: 'data'
)
input from DataStream;

create target tout using sysout(name:'out') input from DataStream;

CREATE OR REPLACE TARGET FileWriter2 USING FileWriter  (
filename: 'OutFile.json',
  directory:'@DIR@/%@metadata(TableName)%',
  rolloverpolicy: 'EventCount:6,Interval:60s'
)
format using JSONFormatter (
)
input from DataStream;


CREATE OR REPLACE TARGET FileWriter3 USING FileWriter  (
filename: 'OutFile.xml',
  directory:'@DIR@/%@metadata(TableName)%',
  rolloverpolicy: 'EventCount:6,Interval:60s'
)
format using XMLFormatter (
rootelement : 'data',
FormatColumnValueAS:'xmlelement')
input from DataStream;


CREATE OR REPLACE TARGET FileWriter4 USING FileWriter  (
  filename: 'OutFile.parquet',
  directory:'@DIR@/%@metadata(TableName)%',
  rolloverpolicy: 'EventCount:6,Interval:60s' 
  )
FORMAT USING ParquetFormatter  
( 
blocksize: '128000000',
  formatAs: 'Default',
  schemaFileName: '@DIR@/parquetSchema'
  )
INPUT FROM DataStream;

CREATE OR REPLACE TARGET FileWriter5 USING FileWriter  (
  filename: 'OutFile.avro',
  directory:'@DIR@/%@metadata(TableName)%',
  rolloverpolicy: 'EventCount:6,Interval:60s' 
  )
FORMAT USING AvroFormatter  
( 
schemaFileName : '@DIR@/varray.avsc'
  )
INPUT FROM DataStream;

END APPLICATION OracleTOFileWriterApp;
deploy application OracleTOFileWriterApp in default;
start OracleTOFileWriterApp;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@ (a String, b String ,c String,g String,h String,i String)
SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]),TO_String(data[7]), TO_String(data[8]) where TO_String(data[0]) != '2';

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

stop application BigqueryBulkLoadMonMetrics;
undeploy application BigqueryBulkLoadMonMetrics;
drop application BigqueryBulkLoadMonMetrics cascade;

CREATE APPLICATION BigqueryBulkLoadMonMetrics;

CREATE FLOW BigqueryBulkLoadMonMetrics_SourceFlow;

CREATE SOURCE BigqueryBulkLoadMonMetrics_DBSource USING DatabaseReader ( 
  Username: 'qatest', 
  DatabaseProviderType: 'ORACLE', 
  FetchSize: 10000, 
  Password_encrypted: 'false', 
  QuiesceOnILCompletion: 'true', 
  Password: 'JVaLv3ZpgQDY8R2ZxS38xg==', 
  Tables: 'QATEST.EMPLOYEE', 
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe' ) 
OUTPUT TO BigqueryBulkLoadMonMetrics_OutputStream;

END FLOW BigqueryBulkLoadMonMetrics_SourceFlow;

CREATE OR REPLACE TARGET BigqueryBulkLoadMonMetrics_BigQueryTarget1 USING BigQueryWriter ( 
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  streamingUpload: 'false', 
  projectId: 'striimqa-214712', 
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:1000000, Interval:90', 
  ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 , maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30', 
  AllowQuotedNewLines: 'false', 
  optimizedMerge: 'false', 
  TransportOptions: 'connectionTimeout=300, readTimeout=120', 
  adapterName: 'BigQueryWriter', 
  Mode: 'MERGE', 
  Tables: 'QATEST.EMPLOYEE,DEV22862jen.sample', 
  StandardSQL: 'true', 
  includeInsertId: 'true', 
  QuoteCharacter: '\"', 
  ServiceAccountKey: '/Users/jenniffer/Product2/IntegrationTests/TestData/google-gcs.json' ) 
INPUT FROM BigqueryBulkLoadMonMetrics_OutputStream;

END APPLICATION BigqueryBulkLoadMonMetrics;

deploy application BigqueryBulkLoadMonMetrics;
start application BigqueryBulkLoadMonMetrics;

CREATE OR REPLACE SOURCE @APPNAME@cdcreader USING @AlterSourceName@  ( 
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: '@CDC_URL@',
  Tables: '@Source1Tables@' ) 
OUTPUT TO @APPNAME@cdcStream;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@1 ( 
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  FetchSize: 20,
  DatabaseProviderType: 'Default',
  Table: '@Source3Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')  
OF @APPNAME@cachetype;

CREATE OR REPLACE EXTERNAL CACHE @APPNAME@2 ( 
  AdapterName:'DatabaseReader',
  Username: '@SRC_USERNAME@',
  Password: '@SRC_PASSWORD@',
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
  FetchSize: 10,
  DatabaseProviderType: 'Default',
  Table: '@Source2Tables@',
  Columns: 'col1,col2,col3,col4,uniquecol',
  keytomap: 'uniquecol')  
OF @APPNAME@cachetype;

STOP APPLICATION ORACLETOBIGQUERY;
UNDEPLOY APPLICATION ORACLETOBIGQUERY;
DROP APPLICATION ORACLETOBIGQUERY CASCADE;

--create application 
CREATE APPLICATION ORACLETOBIGQUERY RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE OracleSource USING OracleReader (
 ConnectionURL: '192.168.123.12:1521/ORCL',
 Tables: 'QATEST.ORATOBIGQALLDATATYPE',
 Username: 'qatest',
 Password: 'qatest',
 FetchSize:1
) OUTPUT TO CDCStream;

CREATE OR REPLACE TARGET bqtables using BigqueryWriter(
 BQServiceAccountConfigurationPath:"/Users/karthikmurugan/Downloads/bqtest-540227c31980.json",
 projectId:"bqtest-158706",
 Tables: "QATEST.ORATOBIGQALLDATATYPE,QATEST.ORATOBIGQALLDATATYPE",
 BatchPolicy: "eventCount:1,Interval:90")
INPUT FROM CDCStream;


CREATE OR REPLACE TARGET T1 using SysOut(name : "some text") INPUT FROM CDCStream;

END APPLICATION ORACLETOBIGQUERY;

DEPLOY APPLICATION ORACLETOBIGQUERY;
START APPLICATION ORACLETOBIGQUERY;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@ recovery 5 second interval;

CREATE OR REPLACE SOURCE @SOURCENAME@ USING IncrementalBatchReader  (
  FetchSize: 1000,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: @startPosition@,
  PollingInterval: '20sec'
  )
  OUTPUT TO @STREAM@;

  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;

  CREATE TARGET @targetName@ USING DatabaseWriter(
    ConnectionURL:'@READER-URL@',
    Username:'@READER-UNAME@',
    Password:'@READER-PASSWORD@',
    BatchPolicy:'Eventcount:1000,Interval:1000',
    CommitPolicy:'Eventcount:1000,Interval:1000',
    Checkpointtable:'CHKPOINT',
    Tables:'@WATABLES@,@WATABLES@_target'
  ) INPUT FROM @STREAM@;

  END APPLICATION @APPNAME@;

  DEPLOY APPLICATION @APPNAME@;
  start application @APPNAME@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'posdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'JsonTarget',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'FileSizeRollingPolicy,filesize:333M,sequence:00'
)
format using JSONFormatter (
  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)

input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetFileSizeBig_actual.log') input from TypedCSVStream;
end application DSV;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.AvroEvent PERSIST USING @APPNAME@KafkaPropset;

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING Global.JMSReader (
  ProviderName: '',
  Provider: '',
  Ctx: '',
  QueueName: '',
  Topic:'',
  UserName: '',
  Password: '',
  EnableTransaction: '',
  transactionpolicy: ''
  )
PARSE USING Global.ParquetParser ()
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

END APPLICATION @APPNAME@;

STOP APPLICATION orrs;
UNDEPLOY APPLICATION orrs;
DROP APPLICATION orrs CASCADE;
CREATE APPLICATION orrs;
Create Type CSVType (
  merchantName String,
  companyname String
);

Create Stream TypedFileStream of CSVType;

create source CSVSource using FileReader (
	directory:'@DIRECTORY@',
	WildCard:'posdata5L.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	
)
OUTPUT TO FileStream;

CREATE CQ CsvToPosData
INSERT INTO TypedFileStream
SELECT TO_STRING(data[1]),TO_STRING(data[0])
FROM FileStream;

CREATE TARGET RSTarget USING RedshiftWriter
(
  ConnectionURL: '@TARGET_URL@',
  Username: '@USERNAME@',
  Password: '@PASSWORD@',
  bucketname: '@BUCKET-NAME@',
  --accesskeyId: '@ACCESS-KEY-ID@',
  --secretaccesskey: '@SECRET-ACCESS-KEY@',
   S3IAMRole:'@IAMROLE@',
  Tables: '@TABLES@',
  uploadpolicy: 'eventcount:10000, interval:1m'
) INPUT FROM TypedFileStream;
DEPLOY APPLICATION orrs;
START APPLICATION orrs;

Create Source @SOURCE_NAME@ Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: true,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SCHEMANAME@.@TABLENAME@_copy;@SCHEMANAME@.@TABLENAME@_copy',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  StartSCN: 'null',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream',
  SetConservativeRange: true
) Output To @STREAM@;

stop application @appName@;
undeploy application @appName@;
drop application @appName@ cascade;

CREATE OR REPLACE APPLICATION @appName@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE @appName@_Source USING PostgreSQLReader (
Username: '@Username@',
ConnectionURL: '@ConnectionURL@',
ReplicationSlotName: '@ReplicationSlotName@',
Tables: '@srcTable@',
Password: '@Password@' )
OUTPUT TO @appName@_Stream;

CREATE OR REPLACE TARGET @appName@_Target USING Global.BigQueryWriter (
ColumnDelimiter: '|',
NullMarker: 'NULL',
streamingUpload: 'false',
projectId: '@projectId@',
Encoding: 'UTF-8',
ConnectionRetryPolicy: 'totalTimeout=600, initialRetryDelay=10, retryDelayMultiplier=2.0, maxRetryDelay=60 ,
maxAttempts=5, jittered=True, initialRpcTimeout=10, rpcTimeoutMultiplier=2.0, maxRpcTimeout=30',
AllowQuotedNewLines: 'false',
CDDLAction: 'Process',
Tables: '@srcTable@,@tgtTable@',
optimizedMerge: 'false',
TransportOptions: 'connectionTimeout=300,
readTimeout=120',
adapterName: 'BigQueryWriter',
Mode: 'APPENDONLY',   StandardSQL: 'true',
ServiceAccountKey: '@GCS-AuthPath@',
BatchPolicy: 'eventCount:100',
QuoteCharacter: '\"' )
INPUT FROM @appName@_Stream;
END APPLICATION @appName@;

--
-- Recovery Test 13 with two sources, two jumping windows, and one wactionstore -- all partitioned on the same compound key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> CW(p#p) -> CQ -> WS
--

STOP Recov13Tester.RecovTest13;
UNDEPLOY APPLICATION Recov13Tester.RecovTest13;
DROP APPLICATION Recov13Tester.RecovTest13 CASCADE;
CREATE APPLICATION RecovTest13 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTest10Data.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE CsvData (
  partKey String KEY,
  serialNumber int,
  partKey2 String KEY
);

CREATE TYPE WactionData (
  partKey String KEY,
  serialNumber int
);

CREATE STREAM DataStream OF CsvData PARTITION BY partKey, partKey2;

CREATE CQ CsvToData
INSERT INTO DataStream
SELECT
    data[0],
    TO_INT(data[1]),
    data[0]
FROM CsvStream;

CREATE JUMPING WINDOW DataStreamTwoItems
OVER DataStream KEEP 2 ROWS
PARTITION BY partKey, partKey2;

CREATE WACTIONSTORE Wactions CONTEXT OF WactionData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ DataToWaction
INSERT INTO Wactions
SELECT
    first(partKey),
    to_int(first(serialNumber))
FROM DataStreamTwoItems
GROUP BY partKey, partKey2;

END APPLICATION RecovTest13;

create Target @TARGET_NAME@ using ADLSGen2Writer(
          accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'%@metadata(TableName)%',
        filename:'table.csv',
        uploadpolicy:'filesize:10M'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@',
	members:'Table=@metadata(TableName),OpName=@metadata(OperationName)'
)
input from @STREAM@;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@;

CREATE SOURCE @SOURCE_NAME@2 USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@;

STOP ttlTester.ttlApp;
UNDEPLOY APPLICATION ttlTester.ttlApp;
DROP APPLICATION ttlTester.ttlApp cascade;

CREATE APPLICATION ttlApp;


CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;


CREATE TYPE wsData
(
bankID Integer KEY,
bankName String
);


CREATE STREAM wsStream OF wsData;

--Select data from QaStream and insert into wsStream

CREATE CQ csvTowsData
INSERT INTO wsStream
SELECT TO_INT(data[0]),data[1] FROM QaStream;

--create jumping window over data in wsStream

CREATE WACTIONSTORE oneWS
CONTEXT OF wsData
EVENT TYPES(wsData )
PERSIST IMMEDIATE USING ( storageProvider: 'elasticsearch', elasticsearch.time_to_live: '10000ms' ) ;

--get data from wsStream and place into wactionStore oneWS
CREATE CQ wsToWaction
INSERT INTO oneWS
SELECT * FROM wsStream
LINK SOURCE EVENT;


END APPLICATION ttlApp;

stop PatternMatchingTcp.CSV;
undeploy application PatternMatchingTcp.CSV;
drop application PatternMatchingTCp.CSV cascade;

create application CSV;

create source TCPSource using TCPReader
(
  IpAddress:'127.0.0.1',
  PortNo:'3549'
)

PARSE USING DSVParser
(
header:'false',
metadata:'@TEST-DATA-PATH@/ctest-TCP.csv',endian : false

)
OUTPUT TO TcpStream;

create Target t1 using SysOut(name:Typed1) input from TcpStream;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  TO_INT(data[0]) as UserId,
	    TO_INT(data[1]) as temp1,
        TO_DOUBLE(data[2]) as temp2,
	    TO_STRING(data[3]) as temp3
FROM TcpStream;

-- scenario 1.1 check pattern using timer within 10 seconds and wait
CREATE CQ TypeConversionTCPCQ1
INSERT INTO TypedStream1
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN T A (W | B | C)
define T = timer(interval 10 second),
A = UserDataStream(temp1 >= 20), B= UserDataStream(temp2 < 30.40), C= UserDataStream(temp3 = 'Bret'), W = wait(T)
PARTITION BY UserId;

-- scenario 1.2 check pattern using timer within 20 seconds
CREATE CQ TypeConversionTCPCQ2
INSERT INTO TypedStream2
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN T A C
define T = timer(interval 20 second), B= UserDataStream(temp2 < 30.40), C= UserDataStream(temp3 = 'zalak'),
A = UserDataStream(temp1 >= 20)
PARTITION BY UserId;

-- scenario 1.3 check pattern using timer within 5 seconds with between values
CREATE CQ TypeConversionTCPCQ3
INSERT INTO TypedStream3
SELECT UserId as typeduserid,
	   A.temp1 as typedtemp1
from UserDataStream
MATCH_PATTERN T A
define T = timer(interval 5 second),
A = UserDataStream(temp1 between 10 and 40)
PARTITION BY UserId;

-- scenario 1.4 check pattern using timer which match no events
CREATE CQ TypeConversionTCPCQ4
INSERT INTO TypedStream4
SELECT UserId as typeduserid
from UserDataStream
MATCH_PATTERN T W
define T = timer(interval 50 second), W = wait(T)
PARTITION BY UserId;

-- scenario 1.5 check pattern using stop timer
CREATE CQ TypeConversionTCPCQ5
INSERT INTO TypedStream5
SELECT UserId as typeduserid,
       A.temp1 as typedtemp1,
       B.temp2 as typedtemp2
from UserDataStream
MATCH_PATTERN T A C T2 B
define
T = timer(interval 50 second),
A = UserDataStream(temp1 between 10 and 40),
C = stoptimer(T),
T2 = timer(interval 30 second),
B = UserDataStream(temp2 >= 20)
PARTITION BY UserId;

CREATE WACTIONSTORE UserActivityInfoTcp1
CONTEXT OF TypedStream1_Type
EVENT TYPES ( TypedStream1_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTcp2
CONTEXT OF TypedStream2_Type
EVENT TYPES ( TypedStream2_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTcp3
CONTEXT OF TypedStream3_Type
EVENT TYPES ( TypedStream3_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTcp4
CONTEXT OF TypedStream4_Type
EVENT TYPES ( TypedStream4_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTcp5
CONTEXT OF TypedStream5_Type
EVENT TYPES ( TypedStream5_Type )
@PERSIST-TYPE@

create Target t2 using SysOut(name:Typed2) input from TypedStream1;

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction1
INSERT INTO UserActivityInfoTcp1
SELECT * FROM TypedStream1
LINK SOURCE EVENT;

CREATE CQ UserWaction2
INSERT INTO UserActivityInfoTcp2
SELECT * FROM TypedStream2
LINK SOURCE EVENT;

CREATE CQ UserWaction3
INSERT INTO UserActivityInfoTcp3
SELECT * FROM TypedStream3
LINK SOURCE EVENT;

CREATE CQ UserWaction4
INSERT INTO UserActivityInfoTcp4
SELECT * FROM TypedStream4
LINK SOURCE EVENT;

CREATE CQ UserWaction5
INSERT INTO UserActivityInfoTcp5
SELECT * FROM TypedStream5
LINK SOURCE EVENT;

end application CSV;
deploy application csv;
start csv;

stop application @APPNAME@1;
undeploy application @APPNAME@1;
stop application @APPNAME@2;
undeploy application @APPNAME@2;

DROP STREAM @APPNAME@_STREAM;
DROP APPLICATION @APPNAME@1 CASCADE;
DROP APPLICATION @APPNAME@2 CASCADE;

drop propertyset PGtoPGPlatfm_App_KafkaPropset;
drop stream  PGToPGPlatfm_Stream CASCADE;


CREATE OR REPLACE PROPERTYSET @APPNAME@_KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

CREATE OR REPLACE STREAM @STREAM@ OF Global.waevent persist using @APPNAME@_KafkaPropset;
					
CREATE OR REPLACE PROPERTYVARIABLE table1='@TABLENAME@1';
create application @APPNAME@1 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@2 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@1 Using PostgreSQLReader(
  ReplicationSlotName: 'slotname',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: '$table1',
  ExcludedTables:'public.chkpoint'
 ) OUTPUT TO @STREAM@;
END FLOW @APPNAME@_AGENTFLOW1;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE SOURCE @SOURCE_NAME@2 Using PostgreSQLReader( 
  ReplicationSlotName: 'slotname',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: '@TABLENAME@2',
  ExcludedTables:'public.chkpoint'
 ) OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW1;

CREATE OR REPLACE TARGET @TARGET_NAME@1 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW1 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ1 INSERT INTO @STREAM@2 SELECT * from @APPNAME@JUMPINGWINDOW1 x
 WHERE META(x,'TableName').toString() == 'WACTION.PGToPGPLATFM_SOURCE4';

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@2;

CREATE OR REPLACE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;

CREATE OR REPLACE Target @TARGET_NAME@sysout1 using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@2;

END APPLICATION @APPNAME@1;



CREATE OR REPLACE PROPERTYVARIABLE table2='@TABLENAME@3';
CREATE OR REPLACE application @APPNAME@2 recovery 5 SECOND INTERVAL;

create or replace stream @STREAM@3 of Global.waevent;

CREATE OR REPLACE FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@3 Using PostgreSQLReader( 
  ReplicationSlotName: 'slotname',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: '$table2',
  ExcludedTables:'public.chkpoint'
 ) OUTPUT TO @STREAM@;

END FLOW @APPNAME@_AGENTFLOW2;

CREATE OR REPLACE FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@4 Using PostgreSQLReader(
  ReplicationSlotName: 'slotname',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: '@TABLENAME@4',  
  ExcludedTables:'public.chkpoint'
 ) OUTPUT TO @STREAM@;
END FLOW @APPNAME@_SERVERFLOW2;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@;

CREATE OR REPLACE JUMPING WINDOW @APPNAME@JUMPINGWINDOW2 OVER @STREAM@ KEEP 1 ROWS;
CREATE OR REPLACE CQ @APPNAME@CQ2 INSERT INTO @STREAM@3 SELECT * from @APPNAME@JUMPINGWINDOW2 y
 WHERE META(y,'TableName').toString() == 'WACTION.PGToPGPLATFM_SOURCE2';

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1',
  CommitPolicy: 'EventCount:1',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@'
  Password_encrypted: false
 ) 
INPUT FROM @STREAM@3;

CREATE OR REPLACE Target @TARGET_NAME@sysout2 using SysOut(name:@TARGET_NAME@Foo3) input from @STREAM@3;

END APPLICATION @APPNAME@2;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ Using OJet
(
  ConnectionURL: '@CONNECTION_URL@',
  CommittedTransactions: true,
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OJet',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  Username: '@SOURCE_USER@'
)OUTPUT TO @STREAM@ (a String, b String ,c String,g String)

SELECT TO_String(data[0]),TO_String(data[1]),TO_String(data[2]),TO_String(data[6]) where TO_String(data[0]) > '1' ;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE OR REPLACE APPLICATION @AppName@;
CREATE SOURCE @srcName@ USING Global.OracleReader ( 
  Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@') 
OUTPUT TO @instreamname@;

CREATE OR REPLACE TARGET @tgtName@ USING Global.SnowflakeWriter ( 
  connectionUrl: '@tgturl@', 
  tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@', 
  password: '@tgtpassword@',   
  username: '@tgtusername@', 
  uploadPolicy: 'eventcount:1,interval:5m', 
  authenticationType: 'Password',
  externalStageType: 'Local', 
  adapterName: 'SnowflakeWriter' ) 
INPUT FROM @outstreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

create Target @TARGET_NAME@ using ADLSGen1Writer(
        filename:'event_data.csv',
        directory:'%@metadata(TableName)%',
        datalakestorename:'striimdls.azuredatalakestore.net',
        clientid:'98058cd8-ce8d-4143-9cc0-fb29258fb375',
        authtokenendpoint:'https://login.microsoftonline.com/5a15b36d-ff72-4884-b039-e69ed7ea0be3/oauth2/token',
        clientkey:'ycq/U45oCPOMUTsvD8ztjchSH5Og+FftdaVHH8cSyQY=',
		rolloverpolicy:'filesize:10M',
		compressiontype: 'false'
)
format using DSVFormatter (
)
input from @STREAM@;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW RECOVERY 5 SECOND INTERVAL;


CREATE OR REPLACE SOURCE DBSource USING OracleReader  (
  Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
  QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
 )
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: '@TARGET_USER@',
  BatchPolicy: 'EventCount:1,Interval:0',
  CommitPolicy: 'EventCount:1,Interval:0',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
  Password_encrypted: false
 )
INPUT FROM Oracle_ChangeDataStream;


create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;

deploy application DBRTOCW on ANY in default;

start application DBRTOCW;

CREATE TARGET @TARGET_NAME@ USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
) INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@2 USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
) INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@3 USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
) INPUT FROM @STREAM@;


CREATE TARGET @TARGET_NAME@4 USING SpannerWriter (
	Tables: 'QATEST.%,testdb.test',
	ServiceAccountKey: '/Users/praveen/Downloads/bigquerywritertest-aadbc85c5324.json',
	instanceId: 'qatest'
) INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;;

create Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
directory:'@TEST-DATA-PATH@',
WildCard:'smallposdata.csv',
positionByEOF:false,
charset:'UTF-8'
)
parse using DSVParser (
header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
filename:'Events',
directory:'@FEATURE-DIR@/logs/',
rolloverpolicy:'eventcount:200,sequence:00'
)
format using DSVFormatter (

)
input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetEventCount_actual.log') input from TypedCSVStream;

end application DSV;

STOP APPLICATION admin.PosApp;
UNDEPLOY APPLICATION admin.PosApp;
DROP APPLICATION admin.PosApp cascade;
drop namedquery admin.PosAppMainPageMap;

drop namedquery admin.PosAppMainPageBar;

drop namedquery admin.PosAppMainPageBar2;
drop namedquery admin.PosAppMainPageScatter;

drop namedquery admin.PosAppHeatMapDrilldownHeatMap;
drop namedquery admin.PosAppHeatMapDrilldownDonuts;
drop namedquery admin.PosAppMainPageHeatMap;
drop namedquery admin.PosAppCompanyDrilldownCharts;
drop namedquery admin.PosAppHeatMapDrilldownMap;
drop namedquery admin.PosAppMainSearchBox;
drop namedquery admin.PosAppMcount;

drop user PosTester;

drop namespace PosTester cascade;
drop dashboard admin.PosAppDash;

stop application iteratortester.iteratorapp;
undeploy application iteratortester.iteratorapp;
drop application iteratortester.iteratorapp cascade;

CREATE APPLICATION iteratorapp;

create flow sourceFlow;

CREATE SOURCE JSONAccessLogSource USING FileReader(
  directory:'@TEST-DATA-PATH@',
  wildcard:'iterator2.json'
)
parse using JSONParser (
) OUTPUT TO jsonSourceStream;

end flow sourceflow;

-- ******ARRAY LIST****** --
create flow processFlow;

create type cacheType (bankID string key, bankName string);
CREATE cache dsvcache USING FileReader (
directory:'@TEST-DATA-PATH@',
wildcard:'banks.csv',
blocksize: 10240,
positionByEOF:false
)
PARSE USING DSVParser (
header:No,
trimquote:false
) QUERY (keytomap:'bankID') OF cacheType;

CREATE TYPE listType (id integer KEY, bankname string, lst java.util.List);
CREATE TYPE listStoreType (id integer KEY, bankname string, lst java.util.List, lstoflst java.util.List);

CREATE STREAM listStream of listType partition by bankname;

CREATE JUMPING WINDOW listJWindow
OVER listStream
keep 3 rows;

CREATE WINDOW listWindow
OVER listStream
keep 3 rows;

CREATE WACTIONSTORE listStore CONTEXT OF listStoreType EVENT TYPES (listStoreType ) 
@PERSIST-TYPE@

create cq updatelistStream
insert into listStream
select TO_INT(bankID), bankName, makelist(bankID,bankName) as lst 
from dsvcache;

create cq updatelistStore 
insert into listStore
select ID, bankName, lst, makelist(lst,lst) from listStream
LINK SOURCE EVENT;

create stream listTargetStream( str String);

create cq updateListTarget
insert into listTargetStream
select itr
from listStream, iterator(listStream.lst) itr order by cast(itr as java.lang.Comparable);

-- CREATE TARGET listout USING SYSOUT(name:"list") input from listStream;

-- ******JsonNode****** --

--CREATE TYPE jsonType (id integer KEY,  lst com.fasterxml.jackson.databind.JsonNode);
CREATE TYPE jsonType (int integer, bankname string, lst com.fasterxml.jackson.databind.JsonNode key);

CREATE STREAM jsonStream of jsonType partition by bankname;

CREATE JUMPING WINDOW jsonJWindow
OVER jsonStream
keep 3 rows
partition by bankname;

CREATE WINDOW jsonWindow
OVER jsonStream
keep 3 rows;

CREATE WACTIONSTORE jsonStore CONTEXT OF jsonType EVENT TYPES (jsonType ) 
@PERSIST-TYPE@

create cq updateJsonStream
insert into jsonStream
--select TO_INT(MATCH(data[0], '.*\\\\s([0-9]+)')) , makeJSON('[{"x":"a"},{"x":"b","y":"c"},{"y":"c"}]') as lst 
select TO_INT(y.bankID), y.bankName, x.data 
from JSONSourceStream x, dsvcache y;

create cq updateJsonStore 
insert into jsonStore
select * from jsonStream
LINK SOURCE EVENT;

create stream jsonTargetStream( str String);

create cq updateJsonTarget
insert into jsonTargetStream
select to_string(itr.StringJSON)
from jsonStream, iterator(jsonStream.lst) itr order by to_string(itr.StringJSON);
CREATE TARGET jsonout USING SYSOUT(name:"jlist") input from jsonStream;

end flow processflow; 

end application iteratorapp;

stop DBRTOCW;
undeploy application DBRTOCW;
drop application DBRTOCW cascade;
CREATE APPLICATION DBRTOCW ;

 create flow myagentflow;

CREATE OR REPLACE SOURCE DBSource USING DatabaseReader  ( 
  Username: '@SOURCE_USER@',
  Password_encrypted: false,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Query: "SELECT * FROM QATEST.OracToCql_alldatatypes",
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SOURCE_PASS@'
 ) 
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource2 USING DatabaseReader  ( 
  Username: '@SOURCE_USER@',
  Password_encrypted: false,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Query: "SELECT * FROM QATEST.OracToCql_alldatatypes2",
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SOURCE_PASS@'
 ) 
OUTPUT TO Oracle_ChangeDataStream;

CREATE OR REPLACE SOURCE DBSource3 USING DatabaseReader  ( 
  Username: '@SOURCE_USER@',
  Password_encrypted: false,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  Query: "SELECT * FROM QATEST.OracToCql_alldatatypes3",
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: '@SOURCE_PASS@'
 ) 
OUTPUT TO Oracle_ChangeDataStream;

end flow myagentflow;

create flow myserverflow;

CREATE OR REPLACE TARGET DBTarget USING @ADAPTER_NAME@  ( 
  DatabaseProviderType: 'Default',
  CheckPointTable: 'CHKPOINT',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:100,Interval:0',
  CommitPolicy: 'EventCount:100,Interval:0',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  Tables: '',
  Password: 'cassandra',
  Password_encrypted: false
 ) 
INPUT FROM Oracle_ChangeDataStream;

create Target t2 using SysOut(name:Foo2) input from Oracle_ChangeDataStream;

end flow myserverflow;

END APPLICATION DBRTOCW;

deploy application DBRTOCW on ALL in default with myagentflow on all in Agents, myserverflow on all in  default;

start DBRTOCW;

CREATE OR REPLACE PROPERTYVARIABLE RetryPolicy='timeOut=00,retryInterval=1,maxRetries=3';
CREATE OR REPLACE PROPERTYVARIABLE KafkaBrokerAddress='localhost:9099';
CREATE OR REPLACE PROPERTYVARIABLE KafkaConfig='request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;';

STOP APPLICATION @Appname@;
UNDEPLOY APPLICATION @Appname@;
DROP APPLICATION @Appname@ CASCADE;
CREATE APPLICATION @Appname@ @Recovery@;
CREATE FLOW @Appname@AgentFlow;
--Partitioning source stream with meta (tablename)
Create Source @Appname@s1 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
 Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss1 partition by meta(@Appname@ss1,'TableName');
-- (id integer,name1 string,name2 string) partition by sleft(name1,1)
-- select TO_INT(data[0]),TO_STRING(data[1]),TO_STRING(data[2]);


--Partitioning source stream using meta (STARTSCN)
Create Source @Appname@s2 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss2 partition by meta(@Appname@ss2,'STARTSCN');



CREATE TYPE @Appname@OpTableDataType(
  TableName String,
  STARTSCN String,
  data java.util.HashMap
);

--Partitioning typed stream inline while creation using expr
CREATE STREAM @Appname@OracleTypedStream OF @Appname@OpTableDataType partition by sright(STARTSCN,2);
Create Source @Appname@s3 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss3 partition by meta(@Appname@ss3,'STARTSCN');

CREATE CQ @Appname@ParseOracleRawStream
  INSERT INTO @Appname@OracleTypedStream
  SELECT META(@Appname@ss3, 'TableName').toString(),META(@Appname@ss3, 'STARTSCN').toString(),
    DATA(@Appname@ss3)
  FROM @Appname@ss3;

--Partitioning source stream with types defined inline using meta expr
Create Source @Appname@s4 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss4 (id integer,NAME string,COL1 string)
-- partition by meta(ss4,'STARTSCN')
select TO_INT(data[0]),TO_STRING(data[2]),TO_STRING(data[3]);

CREATE CQ @Appname@cqss4
  INSERT INTO @Appname@ss4new Partition by sright(id,1)
  SELECT * FROM @Appname@ss4;

--Partitioning source stream -(waevent-s5) with field that has null value

CREATE STREAM @Appname@s5 of Global.WAEvent PARTITION BY meta(@Appname@s5, 'TransactionName');

Create Source @Appname@src5 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss5;

END FLOW @Appname@AgentFlow;


CREATE FLOW @Appname@ServerFlow;
--CREATE STREAM modifyStream OF Global.WAEvent partition by meta(modifyStream,'STARTSCN');
CREATE STREAM @Appname@modifyStream OF Global.WAEvent partition by sleft(data[0],1);
CREATE CQ @Appname@modifycq INSERT INTO @Appname@modifyStream
SELECT * FROM @Appname@ss5
MODIFY
(
data[0] = (TO_INT(data[0]) * 10000) / 100
);


--Partitioning source stream -(waevent) with modified fields
Create Source @Appname@s6 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss6;

CREATE CQ @Appname@putUserDatacq1
INSERT INTO @Appname@newss6
PARTITION BY userdata(@Appname@newss6, 'Modified_Date')
SELECT
putUserData(x,'Modified_Date',TO_STRING(DNOW(),'yyyy-MM-dd HH:mm:ss'))
FROM @Appname@ss6 x;

--Partitioning stream with userdata
CREATE STREAM @Appname@newss7 of Global.WAEvent PARTITION BY userdata(@Appname@newss7, 'Modified_Date');

Create Source @Appname@s7 Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: '@Connectionurl@',
Tables:'qatest.oracle_PartitionStream_test%',
 FetchSize:1
)
Output To @Appname@ss7;

CREATE CQ @Appname@putUserDatacq2
INSERT INTO @Appname@newss7
SELECT
putUserData(x,'Modified_Date',TO_STRING(DNOW(),'yyyy-MM-dd HH:mm:ss'))
FROM @Appname@ss7 x;


create Target @Appname@KW1 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr01',
Mode:'Async',
KafkaConfig: '$KafkaConfig'
)
FORMAT USING jsonFormatter ()
input from @Appname@ss1;

create Target @Appname@KW2 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr02',
Mode:'Async',
KafkaConfig: '$KafkaConfig'
        )
FORMAT USING jsonFormatter ()
input from @Appname@ss2;

create Target @Appname@KW3 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr03',
Mode:'Async',
KafkaConfig: '$KafkaConfig'
        )
FORMAT USING jsonFormatter ()
input from @Appname@OracleTypedStream;

create Target @Appname@KW4 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr04',
Mode:'sync',
KafkaConfig: '$KafkaConfig'
        )
FORMAT USING jsonFormatter ()
input from @Appname@ss4new;

create Target @Appname@KW5 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr05',
Mode:'sync',
KafkaConfig: '$KafkaConfig'
        )
FORMAT USING jsonFormatter ()
input from @Appname@modifyStream;

create Target @Appname@KW6 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr06',
Mode:'sync',
KafkaConfig: '$KafkaConfig'
        )
FORMAT USING jsonFormatter ()
input from @Appname@newss6;

create Target @Appname@KW7 using KafkaWriter VERSION '2.1.0' (
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr07',
Mode:'sync',
KafkaConfig: '$KafkaConfig'
        )
FORMAT USING jsonFormatter ()
input from @Appname@newss7;
END FLOW @Appname@ServerFlow;
end application @Appname@;
--deploy application @Appname@ with @Appname@AgentFlow in Agents, @Appname@ServerFlow in default;
deploy application @Appname@;
start @Appname@;

stop application @Appname@KR;
undeploy application @Appname@KR;
drop application @Appname@KR cascade;
create application @Appname@KR;
CREATE STREAM @Appname@KafkaStream of Global.jsonnodeEvent;
alter stream @Appname@KafkaStream PARTITION BY meta(@Appname@KafkaStream, 'PartitionID');
CREATE SOURCE @Appname@KafkaSource1 USING KafkaReader Version '2.1.0'
(
brokerAddress:'$KafkaBrokerAddress',
Topic:'oracle_Expr01',
startOffset:0
)
PARSE USING jsonParser ()
OUTPUT TO @Appname@KafkaStream;

create Target @Appname@t2 using filewriter(
	filename:'%n%',
	rolloverpolicy:'EventCount:5000000',
    directory:'FEATURE-DIR/logs/%@metadata(TopicName)%/%@metadata(PartitionID)%'
)
format using jsonFormatter (
)
input from @Appname@KafkaStream;

end application @Appname@KR;
deploy application @Appname@KR;
--start @Appname@KR;

STOP APPLICATION @Appname@FR;
UNDEPLOY APPLICATION @Appname@FR;
DROP APPLICATION @Appname@FR CASCADE;
CREATE APPLICATION @Appname@FR;
CREATE SOURCE @Appname@FS0 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS0;

CREATE SOURCE @Appname@FS1 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS1;

CREATE SOURCE @Appname@FS2 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS2;

CREATE SOURCE @Appname@FS3 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS3;

CREATE SOURCE @Appname@FS4 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS4;

CREATE SOURCE @Appname@FS5 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS5;

CREATE SOURCE @Appname@FS6 USING FileReader (
    directory:'Product/IntegrationTests/TestData/',
    WildCard:'0',
	positionByEOF:false
	)
PARSE USING jsonParser (
)OUTPUT TO @Appname@FR_SS6;
end application @Appname@FR;
deploy application @Appname@FR;

--
-- Canon Test W80
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for a partitioned jumping attribute window
--
-- S -> JWa5p -> CQ -> WS
--


UNDEPLOY APPLICATION NameW80.W80;
DROP APPLICATION NameW80.W80 CASCADE;
CREATE APPLICATION W80 RECOVERY 5 SECOND INTERVAL;


CREATE FLOW DataAcquisitionW80;

CREATE SOURCE CsvSourceW80 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW80;

END FLOW DataAcquisitionW80;



CREATE FLOW DataProcessingW80;

CREATE TYPE DataTypeW80 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW80 OF DataTypeW80;

CREATE CQ CSVStreamW80_to_DataStreamW80
INSERT INTO DataStreamW80
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW80;

CREATE JUMPING WINDOW JWa5pW80
OVER DataStreamW80
KEEP WITHIN 5 MINUTE ON dateTime
PARTITION BY word;

CREATE WACTIONSTORE WactionStoreW80 CONTEXT OF DataTypeW80
EVENT TYPES ( DataTypeW80 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWa5pW80_to_WactionStoreW80
INSERT INTO WactionStoreW80
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWa5pW80
GROUP BY word;

END FLOW DataProcessingW80;



END APPLICATION W80;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.TABLE_TEST_%,@DATASET@.%',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true	
) INPUT FROM @STREAM@;

CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;;

create Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;

CREATE APPLICATION DSV RECOVERY 1 SECOND INTERVAL;

CREATE FLOW agentflow;

CREATE OR REPLACE SOURCE CSVSource USING Global.FileReader (
  charset: 'UTF-8',
  adapterName: 'FileReader',
  rolloverstyle: 'Default',
  positionByEOF: false,
  WildCard: 'posdata.csv',
  blocksize: 64,
  skipbom: true,
  includesubdirectories: false,
  directory: 'Samples/AppData' )
PARSE USING Global.DSVParser (
  trimwhitespace: false,
  linenumber: '-1',
  columndelimiter: ',',
  columndelimittill: '-1',
  trimquote: true,
  ignoreemptycolumn: false,
  separator: ':',
  parserName: 'DSVParser',
  quoteset: '\"',
  handler: 'com.webaction.proc.DSVParser_1_0',
  charset: 'UTF-8',
  ignoremultiplerecordbegin: 'true',
  ignorerowdelimiterinquote: false,
  blockascompleterecord: false,
  rowdelimiter: '\n',
  nocolumndelimiter: false,
  headerlineno: 0,
  header: true )
OUTPUT TO CsvStream;

END FLOW agentflow;

CREATE OR REPLACE TARGET t USING Global.FileWriter (
  directory: '@TEST-DATA-PATH@',
  flushpolicy: 'EventCount:10000,Interval:30s',
  members: 'data',
  rolloveronddl: 'true',
  adapterName: 'FileWriter',
  rolloverpolicy: 'TimeIntervalRollingPolicy,rotationinterval:5s',
  filename: 'Foo' )
FORMAT USING Global.DSVFormatter  (
  quotecharacter: '\"',
  handler: 'com.webaction.proc.DSVFormatter',
  columndelimiter: ',',
  formatterName: 'DSVFormatter',
  nullvalue: 'NULL',
  usequotes: 'false',
  rowdelimiter: '\n',
  standard: 'none',
  header: 'false' )
INPUT FROM CsvStream;

END APPLICATION DSV;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using DatabaseReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

stop DataGenSampleApp;
undeploy application DataGenSampleApp;
drop application DataGenSampleApp cascade;


CREATE APPLICATION DataGenSampleApp;

Create Source dataGenSrc USING OracleReader  ( 
  FetchSize: 1,
  QueueSize: 2048,
  CommittedTransactions: true,
  Compression: false,
  Username: 'qatest',
  Password: 'r+g6o0bDETs=',
  ConnectionURL: 'localhost:1521:xe',
  FilterTransactionState: true,
  DictionaryMode: 'OnlineCatalog',
  ReaderType: 'LogMiner',
  Tables: '@tableNames@',
  Password_encrypted: true
 ) 
 Output To LCRStream;

create Target dataGenTgt using SysOut(name:dataGenTgt) input from LCRStream;

END APPLICATION DataGenSampleApp;

CREATE APPLICATION @AppName@;

CREATE OR REPLACE SOURCE CP_Oracle_source USING OracleReader (
  ConnectionURL: '',
  Tables: '',
  Username: '',
  Password: '',
  Fetchsize: 1 )
OUTPUT TO CP_EndToEnd_DB_Adapter_Stream;

CREATE OR REPLACE TARGET CP_DB_Target USING Global.DeltaLakeWriter (
  connectionProfileName: '',
  useConnectionProfile: 'true',
  externalStageConnectionProfileName: '',
  Tables: 'QATEST.Test_CP,qa_reg_CDDL_1702803133916.OrcToDLAltAppendtarget1',
  uploadPolicy: 'eventcount:100000,interval:60s'
)
INPUT FROM CP_EndToEnd_DB_Adapter_Stream;
END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

--
-- Canon Test W70
-- Nicholas Keene, WebAction, Inc.
--
-- Basic test for a partitioned jumping count window
--
-- S -> JWc5p -> CQ -> WS
--


UNDEPLOY APPLICATION NameW70.W70;
DROP APPLICATION NameW70.W70 CASCADE;
CREATE APPLICATION W70 RECOVERY 5 SECOND INTERVAL;




CREATE FLOW DataAcquisitionW70;


CREATE SOURCE CsvSourceW70 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStreamW70;


END FLOW DataAcquisitionW70;




CREATE FLOW DataProcessingW70;

CREATE TYPE DataTypeW70 (
    eventID String,
    word String KEY,
    dateTime DateTime
);

CREATE STREAM DataStreamW70 OF DataTypeW70;

CREATE CQ CSVStreamW70_to_DataStreamW70
INSERT INTO DataStreamW70
SELECT
    data[0],
    data[1],
    TO_DATEF(data[2],'yyyyMMddHHmmss')
FROM CsvStreamW70;

CREATE JUMPING WINDOW JWc5pW70
OVER DataStreamW70
KEEP 5 ROWS
PARTITION BY word;

CREATE WACTIONSTORE WactionStoreW70 CONTEXT OF DataTypeW70
EVENT TYPES ( DataTypeW70 KEY(word) )
@PERSIST-TYPE@

CREATE CQ JWc5pW70_to_WactionStoreW70
INSERT INTO WactionStoreW70
SELECT
    FIRST(eventID),
    FIRST(word),
    FIRST(dateTime)
FROM JWc5pW70
GROUP BY word;

END FLOW DataProcessingW70;



END APPLICATION W70;

--
-- Recovery Test 1
-- Nicholas Keene, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP Recov1Tester.RecovTest1;
UNDEPLOY APPLICATION Recov1Tester.RecovTest1;
DROP APPLICATION Recov1Tester.RecovTest1 CASCADE;
CREATE APPLICATION RecovTest1 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE WactionType (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE Wactions CONTEXT OF WactionType
EVENT TYPES ( WactionType )
@PERSIST-TYPE@

CREATE CQ InsertWactions
INSERT INTO Wactions
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END APPLICATION RecovTest1;

stop PatternMatchingTimer.CSV;
undeploy application PatternMatchingTimer.CSV;
drop application PatternMatchingTimer.CSV cascade;

create application CSV;

create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'ctest.csv',
  columndelimiter:',',
  positionByEOF:false
)
OUTPUT TO CsvStream;

CREATE CQ ParseUserData
INSERT INTO UserDataStream
SELECT  TO_INT(data[0]) as UserId,
	    TO_INT(data[1]) as temp1,
        TO_DOUBLE(data[2]) as temp2,
	    TO_STRING(data[3]) as temp3
FROM CsvStream;

-- scenario 1.1 check pattern using timer within 10 seconds and wait
CREATE CQ TypeConversionTimerCQ1
INSERT INTO TypedStream1
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN T A (W | B | C)
define T = timer(interval 10 second),
A = UserDataStream(temp1 >= 20), B= UserDataStream(temp2 < 30.40), C= UserDataStream(temp3 = 'Bret'), W = wait(T)
PARTITION BY UserId;

-- scenario 1.2 check pattern using timer within 20 seconds
CREATE CQ TypeConversionTimerCQ2
INSERT INTO TypedStream2
SELECT UserId as typeduserid,
	A.temp1 as typedtemp1,
	B.temp2 as typedtemp2,
	C.temp3 as typedtemp3
from UserDataStream
MATCH_PATTERN T A C
define T = timer(interval 20 second), B= UserDataStream(temp2 < 30.40), C= UserDataStream(temp3 = 'zalak'),
A = UserDataStream(temp1 >= 20)
PARTITION BY UserId;

-- scenario 1.3 check pattern using timer within 5 seconds with between values
CREATE CQ TypeConversionTimerCQ3
INSERT INTO TypedStream3
SELECT UserId as typeduserid,
	   A.temp1 as typedtemp1
from UserDataStream
MATCH_PATTERN T A
define T = timer(interval 5 second),
A = UserDataStream(temp1 between 10 and 40)
PARTITION BY UserId;

-- scenario 1.4 check pattern using timer which match no events
CREATE CQ TypeConversionTimerCQ4
INSERT INTO TypedStream4
SELECT UserId as typeduserid
from UserDataStream
MATCH_PATTERN T W
define T = timer(interval 50 second), W = wait(T)
PARTITION BY UserId;

-- scenario 1.5 check pattern using stop timer
CREATE CQ TypeConversionTimerCQ5
INSERT INTO TypedStream5
SELECT UserId as typeduserid,
       A.temp1 as typedtemp1,
       B.temp2 as typedtemp2
from UserDataStream
MATCH_PATTERN T A C T2 B
define
T = timer(interval 50 second),
A = UserDataStream(temp1 between 10 and 40),
C = stoptimer(T),
T2 = timer(interval 30 second),
B = UserDataStream(temp2 >= 20)
PARTITION BY UserId;

CREATE WACTIONSTORE UserActivityInfoTimer1
CONTEXT OF TypedStream1_Type
EVENT TYPES ( TypedStream1_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTimer2
CONTEXT OF TypedStream2_Type
EVENT TYPES ( TypedStream2_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTimer3
CONTEXT OF TypedStream3_Type
EVENT TYPES ( TypedStream3_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTimer4
CONTEXT OF TypedStream4_Type
EVENT TYPES ( TypedStream4_Type )
@PERSIST-TYPE@

CREATE WACTIONSTORE UserActivityInfoTimer5
CONTEXT OF TypedStream5_Type
EVENT TYPES ( TypedStream5_Type )
@PERSIST-TYPE@

--get data from UserDataStream and place into wactionStore UserWaction
CREATE CQ UserWaction1
INSERT INTO UserActivityInfoTimer1
SELECT * FROM TypedStream1
LINK SOURCE EVENT;

CREATE CQ UserWaction2
INSERT INTO UserActivityInfoTimer2
SELECT * FROM TypedStream2
LINK SOURCE EVENT;

CREATE CQ UserWaction3
INSERT INTO UserActivityInfoTimer3
SELECT * FROM TypedStream3
LINK SOURCE EVENT;

CREATE CQ UserWaction4
INSERT INTO UserActivityInfoTimer4
SELECT * FROM TypedStream4
LINK SOURCE EVENT;

CREATE CQ UserWaction5
INSERT INTO UserActivityInfoTimer5
SELECT * FROM TypedStream5
LINK SOURCE EVENT;

end application CSV;
deploy application csv;
start csv;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING MySqlReader
(
  Compression: false,
  FetchSize: 1,
  SendBeforeImage: true,
  ConnectionURL: 'mysql://localhost:3306',
  DatabaseName: 'waction',
  Tables: 'waction.test01',
  Password: 'w@ct10n',
  Password_encrypted: 'false',
  Username: 'root',
  connectionRetryPolicy:'retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'waction.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true	
) INPUT FROM @APPNAME@_ss;
--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING IncrementalBatchReader  ( 
  FetchSize: 1,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: '@CHECKCOLUMN@',
 startPosition: '%=-1',
  PollingInterval: '5sec'
  )
  OUTPUT TO @STREAM@;

STOP APPLICATION AgenCQTester.CSV;
UNDEPLOY APPLICATION AgenCQTester.CSV;
DROP APPLICATION AgenCQTester.CSV cascade;

create application CSV;

CREATE FLOW AgentFlow;
create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails-agent.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStream;

CREATE TYPE MyType (
    PAN String,
    FNAME String
);

CREATE STREAM TypedStream of MyType;

CREATE CQ TypeConversionCQ
INSERT INTO TypedStream
SELECT data[0], data[1]
from CsvStream;

END FLOW AgentFlow;

CREATE FLOW ServerFlow;
CREATE TARGET myout1 using LogWriter(name: CQSource, filename:'@FEATURE-DIR@/logs/logCQ.txt') input from TypedStream;
END FLOW ServerFlow;

end application CSV;
DEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;
START CSV;

--
-- Canon Master Test, Unpartitioned
-- Nicholas Keene, WebAction, Inc.
--


CREATE APPLICATION MasterUnpartitioned
RECOVERY 5 SECOND INTERVAL
;

CREATE OR REPLACE TYPE N100k_JUc100_NoPersist_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100k_JUc100_NoPersist_WS  CONTEXT OF N100k_JUc100_NoPersist_WS_Type
 PERSIST NONE USING ( 
 ) ;

CREATE OR REPLACE TYPE R10_SPc10_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE SOURCE N50k USING NumberSource ( 
  lowValue: '1',
  highValue: '500000',
  delayMillis: '0',
  delayNanos: '0',
  repeat: 'false'
 ) 
OUTPUT TO NumberStream2_Out;

CREATE OR REPLACE SOURCE N100k USING NumberSource ( 
  lowValue: '1',
  highValue: '1000000',
  delayMillis: '0',
  delayNanos: '0',
  repeat: 'false'
 ) 
OUTPUT TO N100k_Out;

CREATE OR REPLACE TYPE ComplexNumberType  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY , 
valueAsDateTime org.joda.time.DateTime , 
valueMod10 java.lang.Long , 
valueMod7 java.lang.Long , 
valueMod13 java.lang.Long  
 );

CREATE OR REPLACE TYPE R10_JPc5_x7_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE TYPE N100k_JUc100_SUc10_WS  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE TYPE N100K_N50K_TwoInputs_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100K_N50K_TwoInputs  CONTEXT OF N100K_N50K_TwoInputs_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE N100K_TwoInputs_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100K_TwoInputs_WS  CONTEXT OF N100K_TwoInputs_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE N100k_JUc100_M_JUc200_JUc15_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100k_JUc100_M_JUc200_JUc15_WS  CONTEXT OF N100k_JUc100_M_JUc200_JUc15_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE N100k_JUc100_Dupe_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100k_JUc100_Dupe  CONTEXT OF N100k_JUc100_Dupe_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE N100k_JUc100_SUc10_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY , 
count java.lang.Long  
 );

CREATE OR REPLACE WACTIONSTORE N100k_JUc100_SUc10_WS  CONTEXT OF N100k_JUc100_SUc10_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE N100k_JUc100_SUc10na_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE TYPE ComplexNumberType2  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY , 
valueAsTimeStamp org.joda.time.DateTime , 
valueMod10 java.lang.Long , 
valueMod7 java.lang.Long , 
valueMod13 java.lang.Long  
 );

CREATE OR REPLACE TYPE Type1  ( f java.lang.String KEY  
 );

CREATE OR REPLACE TYPE N50k_JUc100_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N50k_JUc100_WS  CONTEXT OF N50k_JUc100_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE Type2  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY , 
valueAsDateTime org.joda.time.DateTime , 
valueMod10 java.lang.Long , 
valueMod3 java.lang.Long , 
valueMod7 java.lang.Long  
 );

CREATE OR REPLACE TYPE N100k_JUc100_JUc10_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100k_JUc100_JUc10_WS  CONTEXT OF N100k_JUc100_JUc10_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE TYPE OneNumberType  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE STREAM N100k_Stream OF OneNumberType;

CREATE OR REPLACE CQ N100k_Convert 
INSERT INTO N100k_Stream
SELECT TO_DATE(data[0]), data[1]
 	 
FROM N100k_Out;

CREATE OR REPLACE JUMPING WINDOW N100k_JUc100 OVER N100k_Stream KEEP 100 ROWS;

CREATE OR REPLACE CQ N100k_JUc100_Pull_Dupe 
INSERT INTO N100k_JUc100_Dupe
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100 w;

CREATE OR REPLACE CQ N100k_JUc100_Pull3 
INSERT INTO N100k_JUc100_NoPersist_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100 w;

CREATE OR REPLACE CQ N100k_Merge_JUc100_Pull 
INSERT INTO N100K_TwoInputs_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100 w;

CREATE OR REPLACE JUMPING WINDOW N100k_JUc200 OVER N100k_Stream KEEP 200 ROWS;

CREATE OR REPLACE CQ N100K_JUc200_TwoInputs_Pull 
INSERT INTO N100K_N50K_TwoInputs
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc200 w;

CREATE OR REPLACE STREAM N50k_Stream OF OneNumberType;

CREATE OR REPLACE JUMPING WINDOW N50k_JUc100 OVER N50k_Stream KEEP 100 ROWS;

CREATE OR REPLACE CQ N50K_JUc100_TwoInputs_Pull 
INSERT INTO N100K_N50K_TwoInputs
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N50k_JUc100 w;

CREATE OR REPLACE CQ N50k_JUc100_Pull 
INSERT INTO N50k_JUc100_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N50k_JUc100 w;

CREATE OR REPLACE CQ N50k_Convert 
INSERT INTO N50k_Stream
SELECT TO_DATE(data[0]), data[1]
 	 
FROM NumberStream2_Out;

CREATE OR REPLACE STREAM N100k_JUc100_Out OF OneNumberType;

CREATE OR REPLACE JUMPING WINDOW N100k_JUc100_JUc10 OVER N100k_JUc100_Out KEEP 10 ROWS;

CREATE OR REPLACE CQ N100k_JUc100_JUc10_Pull 
INSERT INTO N100k_JUc100_JUc10_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100_JUc10 w;

CREATE OR REPLACE CQ N100K_TwoInputs_CQ2 
INSERT INTO N100K_TwoInputs_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100_JUc10 w;

CREATE OR REPLACE CQ N100k_JUc100_Pull2 
INSERT INTO N100k_JUc100_Out
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100 w;

CREATE OR REPLACE WINDOW N100k_JUc100_SUc10 OVER N100k_JUc100_Out KEEP 10 ROWS;

CREATE OR REPLACE CQ N100k_JUc100_SUc10_Pull 
INSERT INTO N100k_JUc100_SUc10_WS
SELECT FIRST(w.timestamp), FIRST(w.value), COUNT(w)
 	 
FROM N100k_JUc100_SUc10 w;

CREATE OR REPLACE STREAM N100k_JUc100_M_JUc200 OF OneNumberType;

CREATE OR REPLACE JUMPING WINDOW N100k_JUc100_M_JUc200_JUc15 OVER N100k_JUc100_M_JUc200 KEEP 15 ROWS;

CREATE OR REPLACE CQ N100k_JUc100_M_JUc200_JUc15_Pull 
INSERT INTO N100k_JUc100_M_JUc200_JUc15_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100_M_JUc200_JUc15 w;

CREATE OR REPLACE CQ N100k_Merge_JUc200 
INSERT INTO N100k_JUc100_M_JUc200
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc200 w;

CREATE OR REPLACE CQ N100k_Merge_JUc100 
INSERT INTO N100k_JUc100_M_JUc200
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100 w;

CREATE OR REPLACE STREAM N50K_x100_SUc5_Stream OF OneNumberType;

CREATE OR REPLACE WINDOW N50K_x100_SUc5_SUc10 OVER N50K_x100_SUc5_Stream KEEP 10 ROWS;

CREATE OR REPLACE STREAM N50K_x100_Stream OF OneNumberType;

CREATE OR REPLACE WINDOW N50K_x100_SUc5 OVER N50K_x100_Stream KEEP 5 ROWS;

CREATE OR REPLACE CQ N50K_x100_SUc5_Stream_Pull2 
INSERT INTO N50K_x100_SUc5_Stream
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N50K_x100_SUc5 w;

CREATE OR REPLACE CQ N50K_x100_Pop 
INSERT INTO N50K_x100_Stream
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N50k_Stream w
WHERE (w.value % 100) == 0;

CREATE OR REPLACE TYPE R10_JPc10_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE TYPE N100k_JUc100_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N100k_JUc100_WS  CONTEXT OF N100k_JUc100_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE CQ N100k_JUc100_Pull 
INSERT INTO N100k_JUc100_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N100k_JUc100 w;

CREATE OR REPLACE TYPE ComplexNumberType3  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY , 
valueAsDateTime org.joda.time.DateTime , 
valueMod10 java.lang.Long , 
valueMod7 java.lang.Long , 
valueMod3 java.lang.Long  
 );

CREATE OR REPLACE TYPE N50K_x100_SUc5_WS_Type  ( timestamp org.joda.time.DateTime , 
value java.lang.Long KEY  
 );

CREATE OR REPLACE WACTIONSTORE N50K_x100_SUc5_WS  CONTEXT OF N50K_x100_SUc5_WS_Type
@PERSIST-TYPE@

CREATE OR REPLACE CQ N50K_x100_SUc5_Pull 
INSERT INTO N50K_x100_SUc5_WS
SELECT FIRST(w.timestamp), FIRST(w.value)
 	 
FROM N50K_x100_SUc5 w;





END APPLICATION MasterUnpartitioned;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE OR REPLACE SOURCE @parquetsrc@ USING Global.HDFSReader (
  wildcard: '',
  directory: '',
  hadoopurl: '',
  hadoopconfigurationpath: '',
  positionbyeof: false )
  PARSE USING ParquetParser (
   )
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING AvroFormatter  (
schemaFileName: 'AvroFileSchema'
)
INPUT FROM @appname@CqOut;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using AvroFormatter (
schemaFileName: 'AvroS3Schema'
)
input from @appname@CqOut;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using AvroFormatter (
schemaFileName: 'AvroAzureSchema'
)
INPUT FROM @appname@CqOut;

CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using AvroFormatter (
schemaFileName: 'AvroGCSSchema'
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

STOP APPLICATION @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;
CREATE APPLICATION @APPNAME@ RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE Source @SourceName@ USING Global.MSSqlReader (
  PollingInterval: 5,
  FetchTransactionMetadata: false,
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  TransactionSupport: true,
  StartPosition: 'NOW',
  Tables: 'dbo.sourceTable',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n',
  ConnectionPoolSize: 10,
  Compression: true,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'qatest',
  Username: 'qatest',
  FetchSize: 0,
  IntegratedSecurity: false,
  FilterTransactionBoundaries: true,
  SendBeforeImage: true,
  AutoDisableTableCDC: false )
OUTPUT TO @SRCINPUTSTREAM@;

CREATE TARGET @targetsys@ USING Global.SysOut (
  name: 'sysout' )
INPUT FROM @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING Global.DatabaseWriter (
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  ConnectionURL: 'jdbc:sqlserver://localhost:1433;databaseName=qatest',
  BatchPolicy: 'EventCount:10000,Interval:30',
  CheckPointTable: 'CHKPOINT',
  CDDLAction: 'Process',
  CommitPolicy: 'EventCount:10000,Interval:100',
  StatementCacheSize: '50',
  Password: 'w3b@ct10n',
  Username: 'qatest',
  IgnorableExceptionCode: '547,DUPLICATE_ROW_EXISTS,NO_OP_UPDATE,NO_OP_DELETE,NO_OP_PKUPDATE',
  DatabaseProviderType: 'SQLServer',
  PreserveSourceTransactionBoundary: 'false',
  Tables: 'dbo.sourceTable,dbo.targetTable',
  VendorConfiguration: 'enableIdentityInsert=true',
  adapterName: 'DatabaseWriter' )
INPUT FROM @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;

stop application @APPNAME@app4;
undeploy application @APPNAME@app4;
alter application @APPNAME@app4;
CREATE or replace TARGET @APPNAME@app4_targetNew USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS4_Alter'
) INPUT FROM @APPNAME@sourcestream;
alter application @APPNAME@app4 recompile;
deploy application @APPNAME@app4;

STOP APPLICATION bq;
UNDEPLOY APPLICATION bq;
DROP APPLICATION bq CASCADE;
CREATE APPLICATION bq recovery 1 second interval;

Create Source s Using OracleReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL: 'dockerhost:1521:orcl',
 Tables:'qatest.src_multi_target01',
 FetchSize:1
) 
Output To ss;

CREATE TARGET t1 USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
	Tables:'qatest.src_multi_target01,qatest.tgt_multi_target01 columnmap(col1=ID,col2=NAME,OperationName=@METADATA(OperationName),TableName=@METADATA(TableName))',
	datalocation: 'US',
	nullmarker: 'aaaa',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:10, Interval:30'	
) INPUT FROM ss;

CREATE TARGET t2 USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
	Tables:'qatest.src_multi_target01,qatest.tgt_multi_target02',
	datalocation: 'US',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:10, Interval:30'	
) INPUT FROM ss;

CREATE TARGET t3 USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
	Tables:'qatest.src_multi_target01,qatest.tgt_multi_target03',
	datalocation: 'US',
	nullmarker: 'NOTNULL',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:10, Interval:30'	
) INPUT FROM ss;

CREATE or replace TARGET t4 USING BigQueryWriter (
	serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
	projectId: 'bigquerywritertest',
	Tables:'qatest.src_multi_target01,qatest.tgt_multi_target04 columnmap(OperationName=@METADATA(OperationName))',
	datalocation: 'US',
	nullmarker: 'NULL',
	columnDelimiter: '|',
	BatchPolicy: 'eventCount:10, Interval:30'	
) INPUT FROM ss;

END APPLICATION bq;
DEPLOY APPLICATION bq;
START APPLICATION bq;

use PosTester;
DROP WACTIONSTORE MerchantActivity;

Stop @APPNAME@_App;
undeploy application @APPNAME@_App;
drop application @APPNAME@_App cascade;

CREATE APPLICATION @APPNAME@_App recovery 5 SECOND Interval;
CREATE SOURCE @APPNAME@_Src USING Global.Ojet(
  Username:'@SOURCE_USER@',
  Password:'@SOURCE_PASSWORD@',
  ConnectionURL:'@SOURCE_URL@',
  Tables:'@SOURCE_TABLES@',
  ConnectionRetryPolicy:'@AUTO_CONNECTION_RETRY@',
  OJetConfig: '{ "OJET" : [ "retriable_errors:ORA-26804" ] }'
)
OUTPUT TO @APPNAME@_Stream;

CREATE TARGET @APPNAME@_SYS USING SysOut (
  name: '@APPNAME@_SYS' )
INPUT FROM @APPNAME@_Stream;

End APPLICATION @APPNAME@_App;

STOP APPLICATION DBRTOCW;
UNDEPLOY APPLICATION DBRTOCW;
DROP APPLICATION DBRTOCW CASCADE;
CREATE APPLICATION DBRTOCW;

CREATE TYPE employee(
id String,
ename String
);

CREATE STREAM Oracle_ChangeDataStream of employee;

create source CSVSource using FileReader (
	directory:'/Users/jenniffer/Product2/IntegrationTests/TestData/',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (

)
OUTPUT TO FileStream;

CREATE CQ CQfilter
INSERT INTO Oracle_ChangeDataStream
select data[0],data[1] from FileStream;


create Target t2 using SysOut(name:OrgData) input from Oracle_ChangeDataStream;
CREATE OR REPLACE Target DBTarget USING @ADAPTER_NAME@  (
  DatabaseProviderType: 'Default',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1000,Interval:60',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  Tables: 'test.employee',
  Password: 'cassandra',
  Password_encrypted: false
 )INPUT FROM Oracle_ChangeDataStream;

END APPLICATION DBRTOCW;
DEPLOY APPLICATION DBRTOCW;
START APPLICATION DBRTOCW;

STOP NamedQTester.NamedQueryApp;
UNDEPLOY APPLICATION NamedQTester.NamedQueryApp;
DROP APPLICATION NamedQTester.NamedQueryApp cascade;

CREATE APPLICATION NamedQueryApp;

CREATE TYPE cacheType( EventID String, 
			Word String, 
			datetime Datetime);

CREATE CACHE adhcache using CSVReader (
  directory: '@TEST-DATA-PATH@/',
  wildcard: 'Canon1000.csv',
  header: Yes,
  columndelimiter: ',',
  trimquote:false
) QUERY (keytomap:'EventID') OF cacheType;



END APPLICATION NamedQueryApp;
DEPLOY APPLICATION NamedQueryApp;
START APPLICATION NamedQueryApp;

DROP NAMEDQUERY NamedQTester.nqone;
DROP NAMEDQUERY NamedQTester.nqtwo;
DROP NAMEDQUERY NamedQTester.nqthree;
CREATE NAMEDQUERY nqone select * from adhcache;
CREATE NAMEDQUERY nqtwo select * from adhcache;
CREATE NAMEDQUERY nqthree select * from adhcache;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@
 Using OracleReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 _h_returnDateTimeAs: 'ZonedDateTime',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSV1Source using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'MerchantNames.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO Csv1Stream;

Create Type CSV1Type (
  merchantId String,
  merchantName String
);

Create Stream TypedCSV1Stream of CSV1Type;

CREATE CQ CsvToMerchantNames
INSERT INTO TypedCSV1Stream
SELECT data[0],
       data[1]
FROM Csv1Stream;

create Target t using FileWriter(
  filename:'Merchant',
  sequence:'00',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00'
)
format using DSVFormatter (

)
input from TypedCSV1Stream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetTS_Merchant_actual.log') input from TypedCSV1Stream;

end application DSV;

create or replace Target EH_TARGET using AzureEventHubWriter (
	EventHubNamespace:'EventHubWriterTest',
	EventHubName:'test_01',
	SASPolicyName:'RootManageSharedAccessKey',
	SASKey:'CsU1JvuIJ4JKNSeEqQxSruY/nDHjfmL06UbqBFSgYiA=',
	ConsumerGroup:'test_01_cg',
	E1P:'true',
	OperationTimeoutMS:'200000',
	BatchPolicy:'Size:256000',
	ConnectionRetryPolicy:'Retries:5,RetryBackOff:1m'
)
format using jsonFormatter(
)
input from EH_SS;

create source @SOURCE_NAME@ USING MySQLReader 
(
Username: '@READER-UNAME@',
Password: '@READER-PASSWORD@',
ConnectionURL: 'jdbc:mysql://127.0.0.1:3306/@DBName@',
Tables: @WATABLES@,
sendBeforeImage:'true',
FilterTransactionBoundaries: 'true'
) 
OUTPUT TO @STREAM@;

STOP APPLICATION LongRunningQueryTester.LongRunningApp;
UNDEPLOY APPLICATION LongRunningQueryTester.LongRunningApp;
DROP APPLICATION LongRunningQueryTester.LongRunningApp cascade;

CREATE APPLICATION LongRunningApp;


 --COMMENT::   Modify type attributes as desired.
 --COMMENT::   Type must be created first before creating a source using ranReader

CREATE TYPE RandomData(
  myName String,
  streetAddress String,
  bankName String,
  bankNumber int KEY,
  bankAmount double
);

CREATE source ranDataSource USING ranReader(
  OutputType:'LongRunningQueryTester.RandomData',
  TimeInterval:5,
  NoLimit:true,
  SampleSize:10000,
  DataKey:bankName,
  NumberOfUniqueKeys:500
) OUTPUT TO CSVDataStream;


CREATE STREAM RandomDataStream OF RandomData;

CREATE CQ ParseRandomData
INSERT INTO RandomDataStream
SELECT data[0], data[1],data[2], TO_INT(data[3]), TO_DOUBLE(data[4])
FROM CSVDataStream;


CREATE JUMPING WINDOW RandomData10Rows
OVER RandomDataStream KEEP 10 ROWS
PARTITION BY bankNumber;


CREATE TYPE myData(
  myName String,
  myAddress String,
  myBankName String,
  myBankNumber int KEY,
  myBankAmount double
);

CREATE STREAM myDataStream OF myData;

CREATE CQ GetMyData
INSERT INTO MyDataStream
SELECT myName, streetAddress, bankName, bankNumber, bankAmount
FROM RandomData10Rows WHERE bankNumber > 20000 AND bankNumber < 20300;


CREATE WACTIONSTORE MyDataActivity CONTEXT OF MyData
EVENT TYPES(myData )
PERSIST EVERY 60 second USING (
JDBC_DRIVER:'@WASTORE-DRIVER@',
JDBC_URL:'@WASTORE-URL@',
JDBC_USER:'@WASTORE-UNAME@',
JDBC_PASSWORD:'@WASTORE-PASSWORD@',
DDL_GENERATION:'drop-and-CREATE-tables'
);


Create CQ TrackMyDataActivity
INSERT INTO MyDataActivity
Select * from myDataStream
LINK SOURCE EVENT;


END APPLICATION LongRunningApp;

DROP TYPE T1;
DROP CACHE C1;


CREATE  TYPE T1  ( id String ,
airport_ref String,
airport_ident String KEY ,
type1 String ,
description String ,
frequency_mhz String
 );

CREATE CACHE C1 USING FileReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: '1MB.csv',
  charset: 'UTF-8',
  blockSize: '64',
  positionbyeof: 'false'
 )
PARSE USING DSVPARSER (
  columndelimiter: ',',
  rowdelimiter: '\n:\r',
  header: 'true'
 )
QUERY (
  keytomap: 'id'
 )
 OF T1;

-- Loading Cache C1 which is accessed by DataSourceApp.tql

load cache C1;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

Create Source @SourceName@
 Using OracleReader
(
Username:'@UN@',
 Password:'@PWD@',
 ConnectionURL:'@SourceConnectURL@',
 Tables:'@SourceTable@',
 _h_useClassic: false,
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@SourceConnectURL@',
Username:'@UN@',
  Password:'@PWD@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: '@Tablemapping@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

--
-- Crash Recovery Test 4 on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> JW5 -> CQ5(aggregate) -> WS
-- S -> CQ -> JW6 -> CQ6(aggregate) -> WS
--

STOP APPLICATION N4S4CR4Tester.N4S4CRTest4;
UNDEPLOY APPLICATION N4S4CR4Tester.N4S4CRTest4;
DROP APPLICATION N4S4CR4Tester.N4S4CRTest4 CASCADE;
CREATE APPLICATION N4S4CRTest4 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest4;

CREATE SOURCE CsvSourceN4S4CRTest4 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest4;

CREATE FLOW DataProcessingN4S4CRTest4;

CREATE TYPE CsvDataN4S4CRTest4 (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE TYPE WactionDataN4S4CRTest4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);

CREATE STREAM DataStream OF CsvDataN4S4CRTest4;

CREATE CQ CsvToDataN4S4CRTest4
INSERT INTO DataStream
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE JUMPING WINDOW DataStream6Minutes
OVER DataStream KEEP WITHIN 6 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN4S4CRTest4 CONTEXT OF WactionDataN4S4CRTest4
EVENT TYPES ( CsvDataN4S4CRTest4 )
@PERSIST-TYPE@

CREATE CQ Data5ToWaction
INSERT INTO WactionsN4S4CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream5Minutes p;

CREATE CQ Data6ToWaction
INSERT INTO WactionsN4S4CRTest4
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.city)
FROM DataStream6Minutes p;

END FLOW DataProcessingN4S4CRTest4;

END APPLICATION N4S4CRTest4;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE OR REPLACE TYPE @appname@CQOUT1_Type (
 companyName java.lang.String,
 merchantId java.lang.String,
 dateTime org.joda.time.DateTime,
 hourValue java.lang.String,
 amount java.lang.String,
 zip java.lang.String,
 FileName java.lang.String);

CREATE SOURCE @parquetsrc@ USING S3Reader (
    bucketname:'',
    objectnameprefix:''
     )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE OR REPLACE STREAM @appname@CQOUT1 OF @appname@CQOUT1_Type;
CREATE OR REPLACE CQ @appname@CQ_PQEvent
INSERT INTO @appname@CQOUT1
    Select
    data.get("companyName").toString(),
    data.get("merchantId").toString(),
    TO_DATE(data.get("dateTime").toString()),
    data.get("hourValue").toString(),
    data.get("amount").toString(),
    data.get("zip").toString(),
    metadata.get("FileName").toString()
    FROM @appname@Stream p;

CREATE OR REPLACE TARGET @avrotarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING AvroFormatter  (
schemaFileName: 'AvroFileSchema'
)
INPUT FROM @appname@CQOUT1;

create Target @jsontarget@ using FileWriter(
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
format using JSONFormatter ()
INPUT FROM @appname@CQOUT1;

create Target @xmltarget@ using FileWriter(
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
format using XMLFormatter (
    rootelement:'',
    elementtuple:'',
    charset:'UTF-8'
)
INPUT FROM @appname@CQOUT1;

create Target @dsvtarget@ using FileWriter(
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
format using DSVFormatter ()
INPUT FROM @appname@CQOUT1;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;

CREATE  APPLICATION @AppName@ RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE @srcName@ USING Global.DatabaseReader (
  ConnectionURL: '@srcurl@', 
  Tables: '@srcschema@.@srctable@',
  ReturnDateTimeAs: 'String', 
  FetchSize: 30000, 
  Username: '@srcusername@', 
  QuiesceOnILCompletion: true, 
  Password: '@srcpassword@', 
  DatabaseProviderType: 'DEFAULT' ) 
OUTPUT TO @outstreamname@;

CREATE TARGET @tgtName@  USING Global.DatabaseWriter ( 
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  CheckPointTable: 'CHKPOINT',
  Username:'@tgtusername@', 
  Password:'@tgtpassword@', 
  CDDLAction: 'Process', 
  StatementCacheSize: '50', 
  CommitPolicy: 'EventCount:30000,Interval:60', 
  ConnectionURL:'@tgturl@',
  DatabaseProviderType: 'Default', 
  PreserveSourceTransactionBoundary: 'false', 
  BatchPolicy: 'EventCount:30000,Interval:60', 
  Tables:'@srcschema@.@srctable@,@tgtschema@.@tgttable@', 
  adapterName: 'DatabaseWriter' ) 
INPUT FROM @instreamname@;

End APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP Jumping1Tester.Jumping1;
UNDEPLOY APPLICATION Jumping1Tester.Jumping1;
DROP APPLICATION Jumping1Tester.Jumping1 CASCADE;
CREATE APPLICATION Jumping1;

create source CsvSource1 using FileReader
(
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'WindowsTest.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
)
 parse using DSVParser
(
	header:'yes',
	columndelimiter:','
)
OUTPUT TO CsvStream1;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);
CREATE TYPE CsvData1 (
  zip double
);

CREATE TYPE WactionData1 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData2 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData3 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData5 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData6 (
  zip double
);
CREATE TYPE WactionData7 (
  zip double
);
CREATE TYPE WactionData8 (
  zip double
);

CREATE STREAM DataStream1 OF CsvData;

CREATE STREAM DataStream2 OF CsvData
PARTITION BY companyName;

CREATE STREAM DataStream3 OF CsvData
PARTITION BY city;

CREATE STREAM DataStream4 OF CsvData;
CREATE STREAM DataStream5 OF CsvData
PARTITION BY city;

CREATE STREAM DataStream6 OF CsvData1;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData3
INSERT INTO DataStream3
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData4
INSERT INTO DataStream4
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData5
INSERT INTO DataStream5
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData6
INSERT INTO DataStream6
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

CREATE CQ CsvToData7
INSERT INTO DataStream7
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

CREATE CQ CsvToData8
INSERT INTO DataStream8
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

-- Count based jumping window
CREATE JUMPING WINDOW DataStreamCount
OVER DataStream1 KEEP 5 ROWS;

-- Time based jumping window
CREATE JUMPING WINDOW DataStreamTime OVER DataStream2 KEEP
within 40 second
PARTITION BY companyName;

-- Attribute based jumping window
CREATE JUMPING WINDOW DataStreamAtrribute
OVER DataStream3 KEEP
range 5 minute
ON dateTime
PARTITION BY city;

-- Count + time based jumping window
CREATE JUMPING WINDOW DataStreamCountTime
OVER DataStream4 KEEP
5 rows
within 8 minute;

-- Attribute + time based jumping window
CREATE JUMPING WINDOW DataStreamAttributeTime
OVER DataStream5 KEEP
range 300 second
ON dateTime
within 5 minute
PARTITION BY city;

-- Attribute + time based jumping window using COUNT
CREATE JUMPING WINDOW DataStreamAttributeTime1
OVER DataStream5 KEEP
range 300 second
ON dateTime
within 5 minute;

CREATE WACTIONSTORE Wactions1 CONTEXT OF WactionData1
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions2 CONTEXT OF WactionData2
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions3 CONTEXT OF WactionData3
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions4 CONTEXT OF WactionData4
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions5 CONTEXT OF WactionData5
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions6 CONTEXT OF WactionData6
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions7 CONTEXT OF WactionData7
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions8 CONTEXT OF WactionData8
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE CQ Data1ToWaction
INSERT INTO Wactions1
SELECT
	p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamCount p;

CREATE CQ Data2ToWaction
INSERT INTO Wactions2
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamTime p
group by companyName;

CREATE CQ Data3ToWaction
INSERT INTO Wactions3
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamAtrribute p;

CREATE CQ Data4ToWaction
INSERT INTO Wactions4
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamCountTime p;

CREATE CQ Data5ToWaction
INSERT INTO Wactions5
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamAttributeTime p
group by city;

CREATE CQ Data6ToWaction
INSERT INTO Wactions6
SELECT
    count(*)
FROM DataStreamCount p;

CREATE CQ Data7ToWaction
INSERT INTO Wactions7
SELECT
    count(*)
FROM DataStreamCountTime p;

CREATE CQ Data8ToWaction
INSERT INTO Wactions8
SELECT
    count(*)
FROM DataStreamAttributeTime1 p;


END APPLICATION Jumping1;

--
-- Crash Recovery Test 1 on Four node all server cluster 
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP N4S4CR1Tester.N4S4CRTest1;
UNDEPLOY APPLICATION N4S4CR1Tester.N4S4CRTest1;
DROP APPLICATION N4S4CR1Tester.N4S4CRTest1 CASCADE;
CREATE APPLICATION N4S4CRTest1 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest1;

CREATE SOURCE CsvSourceN4S4CRTest1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest1;

CREATE FLOW DataProcessingN4S4CRTest1;

CREATE TYPE WactionTypeN4S4CRTest1 (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE WactionsN4S4CRTest1 CONTEXT OF WactionTypeN4S4CRTest1
EVENT TYPES ( WactionTypeN4S4CRTest1 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN4S4CRTest1
INSERT INTO WactionsN4S4CRTest1
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

END FLOW DataProcessingN4S4CRTest1;

END APPLICATION N4S4CRTest1;

--
-- Kafka Stream Recovery Test 1 with FileWriter as Target
-- Amudha, Striim, Inc.
--
-- S -> CQ -> KS -> WS

STOP KStreamRecov1Tester.KStreamRecovTest1wfwr;
UNDEPLOY APPLICATION KStreamRecov1Tester.KStreamRecovTest1wfwr;
DROP APPLICATION KStreamRecov1Tester.KStreamRecovTest1wfwr CASCADE;
DROP USER KStreamRecov1Tester;
DROP NAMESPACE KStreamRecov1Tester CASCADE;
CREATE USER KStreamRecov1Tester IDENTIFIED BY KStreamRecov1Tester;
-- GRANT 'Global:create,drop:deploymentgroup:*' TO USER KStreamRecov1Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamRecov1Tester;
CONNECT KStreamRecov1Tester KStreamRecov1Tester;

CREATE APPLICATION KStreamRecovTest1wfwr RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'250');
CREATE STREAM KafkaCsvStream OF Global.waevent persist using KafkaPropset;

CREATE SOURCE CsvSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

CREATE or REPLACE TYPE CsvStreamType(
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM TypedStream OF CsvStreamType; 

CREATE CQ InsertEvents
INSERT INTO TypedStream
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

CREATE TARGET FileWrt USING FILEWRITER (
	directory:'@FEATURE-DIR@/logs/',
	FILENAME:'FileKafkaStream.log',
	flushpolicy:'eventcount:1'
--	rolloverpolicy:'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter(

) 
INPUT FROM TypedStream;


END APPLICATION KStreamRecovTest1wfwr;
DEPLOY APPLICATION KStreamRecovTest1wfwr;
START APPLICATION KStreamRecovTest1wfwr;

create application TestApp;
create source CSVSource using FileReader (
	directory:'Samples/AppData',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ TestCQ
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
	filename:'posdata_JSON',
	rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:5s'
)
format using JSONFormatter (
	members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)

input from TypedCSVStream;
end application TestApp;

deploy application TestApp;
start application TestApp;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING Global.DeltaLakeWriter (
  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.testaswin',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING Global.DeltaLakeWriter (
  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.testaswin',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@3 USING Global.DeltaLakeWriter (
  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.testaswin',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@4 USING Global.DeltaLakeWriter (
  personalAccessToken: 'dapi30ab71c2ef9704b3c0581ebe386b305f',
  hostname: 'adb-5292730997167687.7.azuredatabricks.net',
  stageLocation: '/',
  Mode: 'MERGE',
  Tables: '"QATEST"."%",DEFAULT.testaswin',
  optimizedMerge: 'false',
  uploadPolicy: 'eventcount:1,interval:10s',
  connectionUrl: 'jdbc:spark://adb-5292730997167687.7.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/5292730997167687/0228-054457-qra8earr;AuthMech=3;UID=token' )
INPUT FROM @STREAM@;


CREATE TARGET @TARGET_NAME@WAEventDump USING LogWriter(
  name: 'CDDL',
  filename:'WAEventDump.log'
)INPUT FROM @STREAM@;;

create Target @TARGET_NAME@sysout using SysOut(name:@TARGET_NAME@Foo2) input from @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@ USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
 )
INPUT FROM @STREAM@;

CREATE OR REPLACE TARGET @TARGET_NAME@2 USING DatabaseWriter  (
  Username: '@TARGET_USER@',
  ConnectionURL: '@TARGET_URL@',
  Tables: '@TARGET_TABLE@',
  Password: '@TARGET_PASS@',
 )
INPUT FROM @STREAM@2;

list servers;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCENAME@ USING IncrementalBatchReader  (
  FetchSize: 10,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:mariadb://54.193.168.113:3306/striimTest',
  Tables: 'striim.test01',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: @checkColumn@,
  startPosition: '@startPosition@',
  PollingInterval: '20sec'
  )
  OUTPUT TO @STREAM@;

  create Target @targetsys@ using SysOut(name:@targetsys@) input from @STREAM@;

  CREATE TARGET @targetName1@ USING DatabaseWriter(
    ConnectionURL:'@READER-URL@',
    Username:'@READER-UNAME@',
    Password:'@READER-PASSWORD@',
    BatchPolicy:'Eventcount:1,Interval:1',
    CommitPolicy:'Eventcount:1,Interval:1',
    Checkpointtable:'CHKPOINT',
    Tables:'@WATABLES@,@WATABLES@_target'
  ) INPUT FROM @STREAM@;

  create Target @targetsys1@ using SysOut(name:@targetsys1@) input from @STREAM@;

    CREATE TARGET @targetName2@ USING DatabaseWriter(
      ConnectionURL:'@READER-URL@',
      Username:'@READER-UNAME@',
      Password:'@READER-PASSWORD@',
      BatchPolicy:'Eventcount:1,Interval:1',
      CommitPolicy:'Eventcount:1,Interval:1',
      Checkpointtable:'CHKPOINT',
      Tables:'@WATABLES@,@WATABLES@_target1'
    ) INPUT FROM @STREAM@;

  END APPLICATION @APPNAME@;

  DEPLOY APPLICATION @APPNAME@;
  start application @APPNAME@;

--
-- Crash Recovery Test 2 on two node cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS
--

STOP APPLICATION N2S2CR2Tester.N2S2CRTest2;
UNDEPLOY APPLICATION N2S2CR2Tester.N2S2CRTest2;
DROP APPLICATION N2S2CR2Tester.N2S2CRTest2 CASCADE;
CREATE APPLICATION N2S2CRTest2 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN2S2CRTest2;

CREATE SOURCE CsvSourceN2S2CRTest2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN2S2CRTest2;

CREATE FLOW DataProcessingN2S2CRTest2;

CREATE TYPE WactionTypeN2S2CRTest2 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionTypeN2S2CRTest2;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN2S2CRTest2 CONTEXT OF WactionTypeN2S2CRTest2
EVENT TYPES ( WactionTypeN2S2CRTest2 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN2S2CRTest2
INSERT INTO WactionsN2S2CRTest2
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN2S2CRTest2;

END APPLICATION N2S2CRTest2;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ RECOVERY 1 SECOND INTERVAL;
CREATE SOURCE @srcName@ USING OracleReader (
 Username: '@srcusername@',
  Password: '@srcpassword@',
  ConnectionURL: '@srcurl@',
  Tables: '@srcschema@.@srctable@'
)
OUTPUT TO @outstreamname@;

CREATE OR REPLACE TARGET @tgtName@ USING DatabaseWriter
(
  DatabaseProviderType:'Default',
  CheckPointTable:'CHKPOINT',
  PreserveSourceTransactionBoundary:'false',
  Username:'@tgtusername@',
  BatchPolicy:'EventCount:1,Interval:0',
  CommitPolicy:'EventCount:1,Interval:0',
  ConnectionURL:'@tgturl@',
  Tables: '@srcschema@.@srctable@,@tgtschema@.@tgttable@',
  Password:'@tgtpassword@'
)
INPUT FROM @instreamname@;
END APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

IMPORT static com.webaction.runtime.converters.DateConverter.*;

DROP APPLICATION SampleCDCReaderApp cascade;

CREATE APPLICATION SampleCDCReaderApp;
create source CDCSource using SampleReader (
	AgentPortNo:'2012',
	AgentIpAddress:'127.0.0.1',
	portno:'2020',
	ipaddress:'10.10.196.103',
	Name:'testsession',
    dataFile:'../conf/data.csv',
    metaFile:'../conf/metadata.csv',
	Tables:'POS;STU') output to dbstream,
	CheckUnSupportedTypeStream MAP (table:'STU');

CREATE TYPE CDCPosData(
    BUSINESSNAME String,
    BUSINESSNAME_HEX String,
    PRIMARYACCOUNTNUMBER String,
    POSDATACODE Integer,
    DATETIME org.joda.time.DateTime,
    EXPDATE String,
    CURRENCYCODE String,
    AUTHAMOUNT Float,
    TERMINALID String,
    ZIP String,
    CITY String,
    OPR String,
    TABLENAME String
);

CREATE STREAM CDCPosDataStream OF CDCPosData;

CREATE JUMPING WINDOW CDCPosDataWindow
OVER CDCPosDataStream KEEP 9 ROWS
PARTITION BY OPR;

CREATE CQ CDCCsvToPosData
INSERT INTO CDCPosDataStream
SELECT TO_STRING(data[0],"UTF-8"),
	   TO_HEX(data[0]),
       data[2],
       TO_INT(data[3]),
	   data[4],
	   data[5],
       data[6],
       TO_FLOAT(data[7]),
	   data[8],
       data[9],
	   data[10],
	META(x,"OperationName").toString(),
	META(x, "TableName").toString()
FROM dbstream x
WHERE not(META(x,"OperationName").toString() = "BEGIN") AND not(META(x,"OperationName").toString() = "COMMIT") AND not(META(x, "TableName").toString() is null) AND META(x, "TableName").toString() = "POS";

CREATE TYPE CDCSampleOperationData(
    TableName String,
    OperationName String,
    Count Integer
);

CREATE STREAM CDCSampleOperationDataStream OF CDCSampleOperationData;
-- PARTITION BY OperationName;


CREATE CQ CDCSampleOperationCheck
INSERT INTO CDCSampleOperationDataStream
SELECT x.TABLENAME,
CASE WHEN x.OPR = 'INSERT' THEN x.OPR
     WHEN x.OPR = 'DELETE' THEN x.OPR
     WHEN x.OPR = 'UPDATE' THEN x.OPR
     ELSE 'UNSUPPORTED OPREATION' END,
CASE WHEN x.OPR = 'INSERT' THEN COUNT(x.OPR)
     WHEN x.OPR = 'DELETE' THEN COUNT(x.OPR)
     WHEN x.OPR = 'UPDATE' THEN COUNT(x.OPR)
     ELSE 0 END
FROM CDCPosDataWindow x
GROUP BY OPR;

CREATE TARGET CDCOperationLog USING LogWriter(
	name:FILECDCP,
	filename:'@FEATURE-DIR@/logs/SampleCDCReaderOperationCheck.log'
--	filename:'a1.log'
) INPUT FROM CDCSampleOperationDataStream;

CREATE TARGET CDCLog USING LogWriter(
  name:SampleCDCReaderApp,
filename:'@FEATURE-DIR@/logs/SampleCDCReaderApp.log'
--  filename:'a.log'
) INPUT FROM CDCPosDataStream;

CREATE TARGET CDCLog1 USING LogWriter(
  name:SampleCDCReaderApp,
  filename:'@FEATURE-DIR@/logs/UnsupportedColumn.log'
--  filename:'a2.log'
) INPUT FROM CDCCheckUnSupportedTypeStream;

END APPLICATION SampleCDCReaderApp;

--deploy application SampleCDCReaderApp.SampleCDCReaderApp in default;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE PROPERTYSET @APPNAME@KafkaPropset (zk.address:'localhost:@KEEPERPORT@', bootstrap.brokers:'localhost:@BROKERPORT@', partitions:'250');

CREATE STREAM @APPNAME@PersistStream@RANDOM@ OF Global.waevent PERSIST USING @APPNAME@KafkaPropset;

CREATE TYPE @APPNAME@posDataType (
 merchantId java.lang.String,
 companyName java.lang.String
 );

CREATE OR REPLACE SOURCE @APPNAME@_jmssrc USING JMSReader (
  ProviderName: '',
  UserName: '',
  transactionpolicy: '',
  Provider: '',
  Ctx: '',
  EnableTransaction: '',
  QueueName: '',
  Topic:'',
  Password: '' )
PARSE USING XMLParser (
  rootnode:'/JMSXMLIN'
  )
OUTPUT TO @APPNAME@PersistStream@RANDOM@;

CREATE STREAM @APPNAME@TypedStream OF @APPNAME@posDataType;

CREATE CQ @APPNAME@CQ
INSERT INTO @APPNAME@TypedStream
SELECT TO_STRING(data[0]) as merchantId,
  TO_STRING(data[1]) as companyName
FROM @APPNAME@PersistStream@RANDOM@;

CREATE OR REPLACE TARGET @APPNAME@_filetrgt USING Global.FileWriter (
  filename: '',
  flushpolicy: '',
  directory: '',
  rolloverpolicy: '' )
FORMAT USING JSONFormatter  ()
INPUT FROM @APPNAME@TypedStream;

END APPLICATION @APPNAME@;

drop namespace test cascade force;
create namespace test;
use test;
stop @AppName@;
undeploy application @AppName@;
drop application @AppName@ cascade;
CREATE APPLICATION @AppName@ recovery 5 second Interval;

CREATE SOURCE @srcName@ USING Global.S3Reader ( 
 secretaccesskey: '@secretaccesskey@', 
  foldername: '@sourcefoldername@', 
  blocksize: 64, 
  bucketname: '@bucketname@', 
  objectnameprefix: '@sourcefilename@', 
  accesskeyid: '@accesskeyid@' ) 
PARSE USING Global.DSVParser ( 
  trimwhitespace: false, 
  commentcharacter: '', 
  linenumber: '-1', 
  columndelimiter: ',', 
  trimquote: true, 
  columndelimittill: '-1', 
  ignoreemptycolumn: false, 
  separator: ':', 
  quoteset: '\"', 
  charset: 'UTF-8', 
  ignoremultiplerecordbegin: 'true', 
  ignorerowdelimiterinquote: false, 
  header: false, 
  blockascompleterecord: false, 
  rowdelimiter: '\n', 
  nocolumndelimiter: false, 
  headerlineno: 0 ) 
OUTPUT TO @outstreamname@;

CREATE OR REPLACE TARGET @tgtName@ USING Global.S3Writer ( 
  UploadConfigValueSeparator: '=', 
  UploadConfigPropertySeparator: ',', 
  objectname: '@targetfilename@', 
  secretaccesskey: '@secretaccesskey@', 
  ParallelThreads: '', 
  rolloveronddl: 'true', 
  bucketname: '@bucketname@', 
  foldername: '@targetfoldername@', 
  uploadpolicy: 'eventcount:5,interval:60s', 
  accesskeyid: '@accesskeyid@' ) 
FORMAT USING Global.DSVFormatter  ( 
  quotecharacter: '\"', 
  columndelimiter: ',', 
  members: 'data', 
  nullvalue: 'NULL', 
  usequotes: 'false', 
  rowdelimiter: '\n', 
  standard: 'none', 
  header: 'false' ) 
INPUT FROM @instreamname@;
End APPLICATION @AppName@;
deploy application @AppName@;
start @AppName@;

STOP TestAlertsEmail.TestAlertsEmailApp;
UNDEPLOY APPLICATION TestAlertsEmail.TestAlertsEmailApp;
DROP APPLICATION TestAlertsEmail.TestAlertsEmailApp CASCADE;

CREATE APPLICATION TestAlertsEmailApp;

CREATE source rawSource USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:No,
  wildcard:'@TESTDATAFILE@',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO rawStream;

CREATE STREAM MyAlertStream OF Global.AlertEvent;
CREATE CQ GenerateMyAlerts
INSERT INTO MyAlertStream (name, keyVal, severity, flag, message)
SELECT "Testing Alerts", data[0], data[1], data[2], data[3]
FROM rawStream s;
CREATE TARGET output2 USING SysOut(name : alertsrecevied) input FROM MyAlertStream;

CREATE SUBSCRIPTION alertSubscription USING EmailAdapter
(
SMTPUSER:'@Alerts_Smtpuser@',
--, ${alerts.smtpuser}
SMTPUSER:' ${alerts.smtpuser}',
SMTPPASSWORD:'@Alerts_Smtppassword@',
smtpurl:"@Alerts_Smtpurl@",
starttls_enable:"@Alerts_Starttls_enable@",
smtp_auth:"@Alerts_Smtp_auth@",
subject:"@Alerts_Subject@",
emailList:"@Alerts_Emaillist@",
userIds:"@Alerts_UserId@",
threadCount:"@Alerts_Threadcount@",
@CONTENTTYPE@
senderEmail:"@Alerts_SenderEmail@",
)
INPUT FROM MyAlertStream;

END APPLICATION TestAlertsEmailApp;
DEPLOY APPLICATION TestAlertsEmail.TestAlertsEmailApp;
START TestAlertsEmail.TestAlertsEmailApp;

stop application Postgres_To_PostgresApp;
undeploy application Postgres_To_PostgresApp;
drop application Postgres_To_PostgresApp cascade;

CREATE APPLICATION Postgres_To_PostgresApp RECOVERY 5 SECOND INTERVAL;

create type employee
(
id integer,
ename String,
operationname String,
LSN String
);
CREATE STREAM Postgres_TypedStream of employee;

CREATE OR REPLACE SOURCE Postgres_Src USING PostgreSQLReader  ( 
 ReplicationSlotName: 'test_slot',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgres_src'
 ) 
OUTPUT TO Postgres_Change_Data_Stream;

create stream UserdataStream1 of Global.WAEvent;

Create CQ CQUser
insert into UserdataStream1
select putuserdata (data1,'OperationName',META(data1,'OperationName').toString()) from Postgres_Change_Data_Stream data1;

create CQ Cqfilter 
insert into Postgres_TypedStream
select 
to_int(data[0]),
data[1],
META(u,'OperationName').toString(),
META(u,'LSN').toString()
from UserdataStream1 u 
where USERDATA(u,'OperationName').toString()=='INSERT';

CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Postgres_TypedStream;

CREATE TARGET Postgres_FW USING FileWriter (
  filename:'Postgres_FW.log'
)
FORMAT USING DSVFormatter ()
INPUT FROM Postgres_TypedStream;

CREATE OR REPLACE TARGET Postgres_tgt USING DatabaseWriter ( 
DatabaseProviderType: 'Default',
CheckPointTable: 'CHKPOINT',
PreserveSourceTransactionBoundary: 'false',
Username: 'waction',
Password_encrypted: 'false',
BatchPolicy: 'EventCount:1,Interval:60',
CommitPolicy: 'EventCount:1,Interval:300',
ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction?stringtype=unspecified',
Tables: 'public.postgres_tgt',
adapterName: 'DatabaseWriter',
Password: 'w@ct10n'
)INPUT FROM Postgres_TypedStream;

end application Postgres_To_PostgresApp;
deploy application Postgres_To_PostgresApp;
start Postgres_To_PostgresApp;

stop ORAToBigquery;
undeploy application ORAToBigquery;
drop application ORAToBigquery cascade;
CREATE APPLICATION ORAToBigquery RECOVERY 5 SECOND INTERVAL;
CREATE OR REPLACE SOURCE Rac11g USING OracleReader  ( 
  SupportPDB: false,
  SendBeforeImage: true,
  ReaderType: 'LogMiner',
  CommittedTransactions: false,
  FetchSize: 1,
  Password: 'manager',
  DDLTracking: 'true',
  StartTimestamp: 'null',
  OutboundServerProcessName: 'WebActionXStream',
  OnlineCatalog: true,
  ConnectionURL: '192.168.33.10:1521/XE',
  SkipOpenTransactions: false,
  Compression: false,
  QueueSize: 40000,
  RedoLogfiles: 'null',
  Tables: 'SYSTEM.POSTABLE',
  Username: 'ravi',
  FilterTransactionBoundaries: true,
  adapterName: 'OracleReader',
  XstreamTimeOut: 600,
  connectionRetryPolicy: 'timeOut=30, retryInterval=60, maxRetries=3'
 ) 
OUTPUT TO DataStream;
CREATE OR REPLACE TARGET Target1 USING SysOut ( 
  name: "dstream"
 ) 
INPUT FROM DataStream;
CREATE OR REPLACE TARGET Target2 using BigqueryWriter(
  BQServiceAccountConfigurationPath:"/Users/ravipathak/Downloads/big-querytest-1963ae421e90.json",
  projectId:"big-querytest",
  Tables: "SYSTEM.GGAUTHORIZATIONS,testing1.ggauthorisation",
  parallelismCount: 2,
  BatchPolicy: "eventCount:100000,Interval:0")
INPUT FROM DataStream;
END APPLICATION ORAToBigquery;
deploy application ORAToBigquery;
start ORAToBigquery;

CREATE APPLICATION jsonapp;

CREATE TYPE JSONAccessLogEntry (
    srcIp String KEY,
    userId String,
    sessionId String,
    accessTime long,
    request String,
    code int,
    size int,
    referrer String,
    userAgent String,
    responseTime int
);
CREATE STREAM JSONSourceStream OF JSONAccessLogEntry;

CREATE SOURCE JSONAccessLogSource USING FileReader(
  directory:'@TEST-DATA-PATH@',
  wildcard:'FileWithJSONTest.json',
  positionbyeof:false
)
parse using JSONParser (
  eventType:'admin.JSONAccessLogEntry'
) OUTPUT TO JSONSourceStream;

create Target JSONDump using LogWriter(name:Foo,filename:'@FEATURE-DIR@/logs/jsondata') input FROM JSONSourceStream;

END APPLICATION jsonapp;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;
CREATE  SOURCE @SOURCENAME@ USING OracleReader  (
  Username: 'qatest',
  Password: 'qatest',
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.emp',
  --OnlineCatalog: true,
  FetchSize: @FETCHSIZE@
 )
OUTPUT TO DataStream;

CREATE TARGET @TARGETNAME@ USING DatabaseWriter(
ConnectionURL:'jdbc:sqlserver://localhost:1433;databaseName=qatest',
Username:'qatest',
PassWord:'w3b@ct10n',
Tables: 'qatest.emp,dbo.emp',
CHECKPOINTTABLE : 'qatest.CHKPOINT',
ConnectionRetryPolicy: '@RETRY_INTERVAL@,@MAX_RETRY@',
BatchPolicy:'EventCount:@BATCH_EVENT@,Interval:@BATCH_INTERVAL@',
CommitPolicy:'EventCount:@COMMIT_EVENT@,Interval:@COMMIT_INTERVAL@'
) INPUT FROM DataStream;

end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	charset:'@charset@'
)
parse using AvroParser (
	schemaFileName:'@fname@',
	schemaRegistryURI:'@evty@'
)
OUTPUT TO CsvStream;
/*
create Target FileTarget using FileWriter(
    rolloverpolicy:'eventcount:100',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using AvroFormatter (
schemaFileName:'@fname@'
)
input from CsvStream;
*/
end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@;
CREATE  SOURCE @SourceName@ USING MSSqlReader  ( 
  Username: '@UserName@',
  Password: '@Password@',
  DatabaseName: 'qatest',
  ConnectionURL: '@SourceConnectionURL@',
  Tables: 'qatest.@SourceTable@',
  ConnectionPoolSize: 1,
  StartPosition: 'EOF',
  ReplicationSlotName: 'null'
 ) 
OUTPUT TO @SRCINPUTSTREAM@;
create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;
CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter  ( 
  ConnectionURL: '@TargetConnectionURL@',
  Username: '@UserName@',
  Password: '@Password@',
  BatchPolicy: 'EventCount:1,Interval:1',
  Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
 ) 
INPUT FROM @SRCINPUTSTREAM@;
end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

IMPORT cern.colt.list.DoubleArrayList;

IMPORT com.webaction.runtime.LocationInfo;

IMPORT com.webaction.runtime.LocationArrayList;

UNDEPLOY APPLICATION FraudTester.FraudDetectionApp;
DROP APPLICATION FraudTester.FraudDetectionApp cascade;

CREATE APPLICATION FraudDetectionApp;

CREATE SOURCE CsvDataSource USING CSVReader
(
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'fraudPosData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

CREATE TYPE PosData
(
  merchantId String,
  pan String,
  datetime DateTime,
  hourValue int,
  amount double,
  zip String
);

CREATE STREAM PosDataStream OF PosData PARTITION BY merchantId;

CREATE CQ CsvToPosData
INSERT INTO PosDataStream
SELECT data[1], data[2], TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]), data[9]
FROM CsvStream;

CREATE JUMPING WINDOW PosData5Minutes
OVER PosDataStream KEEP WITHIN 5 MINUTE ON datetime
PARTITION BY merchantId;

CREATE TYPE USAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

CREATE CACHE ZipLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'USAddresses.txt',
  header: Yes,
  columndelimiter: '	',
  trimquote:false
) QUERY (keytomap:'zip') OF USAddressData;

CREATE TYPE MerchantNameData(
  merchantId		String KEY,
  companyName		String
);

CREATE CACHE NameLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'MerchantNames.csv',
  header: Yes,
  columndelimiter: ',',
  trimquote:false
) QUERY(keytomap:'merchantId') OF MerchantNameData;

CREATE TYPE CustomerDetails(
  pan String KEY,
  fname String,
  lname String,
  address String,
  city String,
  state String,
  zip String,
  gender String
);

CREATE CACHE CustomerLookup using CSVReader (
  directory: '@TEST-DATA-PATH@',
  wildcard: 'customerdetails.csv',
  header: Yes,
  columndelimiter: ',',
  trimquote:false
) QUERY(keytomap:'pan') OF CustomerDetails;

CREATE TYPE PanWithMerchantData(
wKey String KEY,
pan String,
count int,
datetime DateTime,
locations com.webaction.runtime.LocationArrayList);

CREATE STREAM PanWithMerchantDataStream of PanWithMerchantData PARTITION BY wKey;

CREATE CQ CsvToPanWithMerchantData
INSERT into PanWithMerchantDataStream
SELECT p.pan+p.datetime, p.pan , count(*), p.datetime, locInfoList(z.latVal, z.longVal, z.city, z.zip, n.companyName)
FROM PosData5Minutes p, ZipLookup z, NameLookup n
WHERE p.zip = z.zip AND p.merchantId = n.merchantId group by p.pan having count(*) > 1;

CREATE TYPE PanWithMerchantAndCustomerData(
wKey String KEY,
pan String,
fname String,
lname String,
address String,
city String,
state String,
zip String,
gender String,
count int,
datetime DateTime,
locations com.webaction.runtime.LocationArrayList);

CREATE STREAM PanWithMerchantAndCustomerDataStream of PanWithMerchantAndCustomerData PARTITION BY wKey;

CREATE CQ PanWithMerchantToMerchantAndCustomerData
INSERT into PanWithMerchantAndCustomerDataStream SELECT ds.wKey, ds.pan, c.fname, c.lname, c.address, c.city, c.state, c.zip, c.gender, ds.count, ds.datetime, ds.locations
FROM PanWithMerchantDataStream ds, CustomerLookup c
WHERE ds.pan = c.pan;

CREATE TYPE AlertRecord(
  name String,
  keyVal String,
  severity String,
  flag String ,
  message String
);

CREATE STREAM AlertStream OF AlertRecord;

CREATE CQ GenerateAlerts
INSERT INTO AlertStream
SELECT p.pan, p.wKey,
       CASE
         WHEN p.count <= 2  THEN 'info'
      WHEN p.count > 2 THEN 'warning'
         ELSE 'error' END,
       CASE
         WHEN p.count > 1 THEN 'raise'
         ELSE 'cancel' END,
       CASE
         WHEN p.count <= 2 THEN textFromFields('PAN $2 has been used $10 times in a 5 minute period',p)
         WHEN p.count > 2 THEN textFromFields('[Abnormal Usage] PAN $2 has been used $10 times in a 5 minute period',p)
         ELSE ''
         END
FROM PanWithMerchantAndCustomerDataStream p;

CREATE TARGET SendAlert USING AlertSender(
 name: unusualActivity
) INPUT FROM AlertStream;

CREATE WACTIONSTORE FraudActivity CONTEXT OF PanWithMerchantAndCustomerData
EVENT TYPES ( PanWithMerchantAndCustomerData )
@PERSIST-TYPE@


Create CQ TrackFraudActivity
INSERT INTO FraudActivity
Select * from PanWithMerchantAndCustomerDataStream
LINK SOURCE EVENT;


END APPLICATION FraudDetectionApp;

STOP APPLICATION @WRITERAPPNAME@;
UNDEPLOY APPLICATION @WRITERAPPNAME@;
DROP APPLICATION @WRITERAPPNAME@ CASCADE;

CREATE APPLICATION @WRITERAPPNAME@ RECOVERY 1 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

CREATE SOURCE @SOURCE@ USING OracleReader
(
FetchSize:1,
Username:'@SOURCE_USER@',
Password:'85d7qFnwTW8=',
ConnectionURL:'@CONNECTION_URL@',
Tables:'@SOURCE_TABLE@',
password_encrypted: 'true'
)
OUTPUT TO @STREAM1@;


end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

CREATE OR REPLACE TYPE @TYPE@( 
datae java.util.HashMap , 
TABLE_NAME java.lang.String , 
OPS_NAME java.lang.String , 
DB_TIMESTAMP java.lang.String  ,
COMMITSCN java.lang.String ,
SCN java.lang.String ,
REC_INS_TIME java.lang.String );

CREATE CQ @CQ1@
INSERT INTO @STREAM2@
SELECT  
CASE WHEN (META(c,"OperationName").toString() == "DELETE")
THEN putUserData(c, 'isDelete', 'true') 
ELSE
putUserData(c,'isDelete', 'false')
END
FROM @STREAM1@ c;

CREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;

CREATE OR REPLACE CQ @CQ2@ 
INSERT INTO @STREAM3@
SELECT 
data(e),
META(e,"TableName").toString() as TABLE_NAME,
META(e, "OperationName").toString() as OPS_NAME,
META(e, "TimeStamp").toString() as DB_TIMESTAMP,
META(e,"COMMITSCN").toString() as COMMITSCN ,
META(e,"SCN").toString() as  SCN ,
DNOW().toString() as REC_INS_TIME
FROM @STREAM1@ e;

create Target @TARGET1@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC1',
ParallelThreads:'',
PartitionKey:'@metadata(TableName)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from @STREAM1@;


create Target @TARGET2@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC2',
ParallelThreads:'2',
PartitionKey:'TABLE_NAME',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from @STREAM3@;

create Target @TARGET3@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC3',
ParallelThreads:'',
PartitionKey:'@userdata(isDelete)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest.avsc')
input from @STREAM2@;

end application @WRITERAPPNAME@;
deploy application @WRITERAPPNAME@;
start @WRITERAPPNAME@;
stop application @READERAPPNAME@;
undeploy application @READERAPPNAME@;
drop application @READERAPPNAME@ cascade;
CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC1',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;


CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
filename:'@READERAPPNAME@_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream1;

CREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC2',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;


CREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC3',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest.avsc'
)
OUTPUT TO KafkaReaderStream3;

end flow @APPNAME@_serverflow;

end application @READERAPPNAME@;
deploy application @READERAPPNAME@;

STOP Sliding1Tester.Sliding1;
UNDEPLOY APPLICATION Sliding1Tester.Sliding1;
DROP APPLICATION Sliding1Tester.Sliding1 CASCADE;
CREATE APPLICATION Sliding1;

create source CsvSource1 using FileReader
(
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'WindowsTest.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
)
 parse using DSVParser
(
	header:'yes',
	columndelimiter:','
)
OUTPUT TO CsvStream1;

CREATE TYPE CsvData (
  companyName String KEY,
  dateTime DateTime,
  amount double,
  city String
);
CREATE TYPE CsvData1 (
  zip double
);

CREATE TYPE WactionData1 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData2 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData3 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData4 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData5 (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstCity String
);
CREATE TYPE WactionData6 (
  zip double
);
CREATE TYPE WactionData7 (
  zip double
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY companyName;

CREATE STREAM DataStream2 OF CsvData
PARTITION BY city;

CREATE STREAM DataStream3 OF CsvData;
CREATE STREAM DataStream4 OF CsvData;
CREATE STREAM DataStream5 OF CsvData;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData3
INSERT INTO DataStream3
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData4
INSERT INTO DataStream4
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData5
INSERT INTO DataStream5
SELECT
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream1;

CREATE CQ CsvToData6
INSERT INTO DataStream6
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

CREATE CQ CsvToData7
INSERT INTO DataStream7
SELECT
    TO_DOUBLE(data[8])
FROM CsvStream1;

-- Count based sliding window
CREATE WINDOW DataStreamCount
OVER DataStream1 KEEP 5 ROWS
PARTITION BY companyName;

-- Time based jumping window
CREATE WINDOW DataStreamTime OVER DataStream2 KEEP
within 350 second
PARTITION BY companyName,city;

-- Attribute based sliding window
CREATE WINDOW DataStreamAtrribute
OVER DataStream3 KEEP
range 180 second
ON dateTime;

-- Count + time based sliding window
CREATE WINDOW DataStreamCountTime
OVER DataStream4 KEEP
5 rows
within 155 second;

-- Attribute + time based sliding window
CREATE WINDOW DataStreamAttributeTime
OVER DataStream5 KEEP
range 50 second
ON dateTime
within 400 second;

CREATE WACTIONSTORE Wactions1 CONTEXT OF WactionData1
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions2 CONTEXT OF WactionData2
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions3 CONTEXT OF WactionData3
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions4 CONTEXT OF WactionData4
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions5 CONTEXT OF WactionData5
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions6 CONTEXT OF WactionData6
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE WACTIONSTORE Wactions7 CONTEXT OF WactionData7
EVENT TYPES ( CsvData1 )
@PERSIST-TYPE@

CREATE CQ Data1ToWaction
INSERT INTO Wactions1
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamCount p
group by companyName;

CREATE CQ Data2ToWaction
INSERT INTO Wactions2
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamTime p
group by companyName,city;

CREATE CQ Data3ToWaction
INSERT INTO Wactions3
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamAtrribute p;

CREATE CQ Data4ToWaction
INSERT INTO Wactions4
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamCountTime p;

CREATE CQ Data5ToWaction
INSERT INTO Wactions5
SELECT
    p.companyName,
    p.dateTime,
    p.amount,
    p.city
FROM DataStreamAttributeTime p;

CREATE CQ Data6ToWaction
INSERT INTO Wactions6
SELECT
    count(*)
FROM DataStreamCount p;

CREATE CQ Data7ToWaction
INSERT INTO Wactions7
SELECT
    count(*)
FROM DataStreamTime p;

END APPLICATION Sliding1;

-----------------------------------
stop application SourceAgentApp1;
undeploy application SourceAgentApp1;

stop application SourceAgentApp2;
undeploy application SourceAgentApp2;

stop application TargetServerApp;
undeploy application TargetServerApp;

drop application SourceAgentApp1 cascade;
drop application SourceAgentApp2 cascade;
drop application TargetServerApp cascade;


CREATE APPLICATION SourceAgentApp1;
create flow flow1;
create source CSVSource1 using FileReader (
directory: '@TEST-DATA-PATH@/tmp',
WildCard:'mybanks*',
positionByEOF: true,
charset:'UTF-8'
) parse using DSVParser (header:'no')
OUTPUT TO CsvStream;
end flow flow1;

--CREATE TARGET T USING Sysout(name:'sysout1') INPUT FROM CsvStream;

END APPLICATION SourceAgentApp1;

DEPLOY APPLICATION SourceAgentApp1 with flow1 in AGENTS;


CREATE APPLICATION SourceAgentApp2;

create flow flow2;
create source CSVSource2 using FileReader (
directory: '@TEST-DATA-PATH@/tmp',
WildCard:'mybanks*',
positionByEOF: true,
charset:'UTF-8'
) parse using DSVParser (header:'no')
OUTPUT TO CsvStream;
end flow flow2;

--CREATE TARGET T USING Sysout(name:'sysout2') INPUT FROM CsvStream;

END APPLICATION SourceAgentApp2;

DEPLOY APPLICATION SourceAgentApp2 with flow2 in AGENTS;


-- One app consuming from stream from 2 sources running in agent
CREATE APPLICATION TargetServerApp;
create flow flow3;

CREATE TARGET T5 USING FileWriter(filename:'@FEATURE-DIR@/logs/TargetServerApp_output.log', rolloverpolicy: 'EventCount:10000,Interval:30s')
FORMAT USING JSONFormatter ()
INPUT FROM CsvStream;
end flow flow3;

END APPLICATION TargetServerApp;
deploy application TargetServerApp with flow3 in default;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR recovery 5 second interval;

 CREATE OR REPLACE SOURCE TS USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01;striim.test02;striim.test03',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream ;

  CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

create target T using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream;


CREATE TARGET log USING LOGWRITER
(
 name:log,
 filename:'LOGFILENAME'
)
INPUT FROM data_stream;


END APPLICATION IR;

deploy application IR;
start IR;

CREATE OR REPLACE APPLICATION @AppName@;

Create Source @AppName@_source Using OracleReader(
  Username:'@username@',
  Password:'@password@',
  ConnectionURL:'@url@',
  Tables:'@srctableName@',
  Fetchsize:1
)
Output To @AppName@_Stream;
CREATE OR REPLACE TARGET @AppName@_SF_Target USING Global.SnowflakeWriter (
  streamingUpload: 'false',
  useConnectionProfile:'true',
  connectionProfileName: 'admin.@SFCP@',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  columnDelimiter: '|',
  tables: '@srctableName@,@trgtableName@',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:1,interval:5m',
  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',
  externalStageType: 'Local',
  adapterName: 'SnowflakeWriter',
  fileFormatOptions: 'null_if = \"\"' )
INPUT FROM @AppName@_Stream;

CREATE OR REPLACE TARGET @AppName@_SF_Target2 USING Global.SnowflakeWriter (
  streamingUpload: 'false',
  useConnectionProfile:'true',
  connectionProfileName: 'admin.@SFCP@',
  CDDLAction: 'Process',
  optimizedMerge: 'false',
  columnDelimiter: '|',
  tables: '@srctableName@,@trgtableName@',
  appendOnly: 'false',
  uploadPolicy: 'eventcount:1,interval:5m',
  UUID: '{uuidstring=01ee6b93-b50d-a941-af30-429c7981246b}',
  externalStageType: 'Local',
  adapterName: 'SnowflakeWriter',
  fileFormatOptions: 'null_if = \"\"' )
INPUT FROM @AppName@_Stream;

END APPLICATION @AppName@;
Deploy application @AppName@;
Start application @AppName@;

CREATE APPLICATION @APPNAME@;

CREATE OR REPLACE TYPE @APPNAME@oracleType (
 companyName java.lang.String,
 merchantId java.lang.String);

CREATE SOURCE @APPNAME@_Orcl USING OracleReader (
  ConnectionURL: '',
  Password: '',
  Tables: '',
  Username: ''
  )
OUTPUT TO @APPNAME@OracleOut;

CREATE OR REPLACE STREAM @APPNAME@TypedStream OF @APPNAME@oracleType;

CREATE OR REPLACE CQ @APPNAME@CQOut
INSERT INTO @APPNAME@TypedStream
SELECT
TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantID
FROM @APPNAME@OracleOut o;

CREATE OR REPLACE TARGET @APPNAME@_jmstrgt USING JMSWriter (
  QueueName: '',
  UserName: '',
  Password: '',
  Ctx: '',
  Provider: ''
  )
FORMAT USING DSVFormatter()
INPUT FROM @APPNAME@TypedStream;

END APPLICATION @APPNAME@;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
create application @APPNAME@ recovery 5 second interval;

create source @SourceName@ using MySQLReader
  (ConnectionURL: '@SourceConnectionURL@',
   Username:'@UserName@',
   Password:'@Password@',
   Tables: '@SourceTableName@'
)
output to @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo4) input from @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
   ConnectionURL:'@TargetConnectionURL@',
   Username:'@UserName@',
   Password:'@Password@',
   BatchPolicy:'EventCount:1,Interval:0',
   Tables: '@SourceTableName@,@TargetTableName@'
 
 ) INPUT FROM @SRCINPUTSTREAM@;

end application @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;

Create Source @SOURCE_NAME@
 Using DatabaseReader
(
 Username:'qatest',
 Password:'qatest',
 ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
 Tables:'QATEST.DATE_TIME',
 --Query:"SELECT * FROM qatest.oracle_alldatatypes",
 FetchSize:10000,
 QueueSize:2048,
) Output To @STREAM@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.test01',
	FetchSize: '1',
	connectionRetryPolicy:'timeOut=00,retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(id,col5) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'Eventcount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true		
) INPUT FROM @APPNAME@_ss;
--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

create flow @APPNAME@_agentflow;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

end flow @APPNAME@_agentflow;

create flow @APPNAME@_serverflow;

create Target @TARGET@ using S3Writer(
    bucketname:'@BUCKET@',
   objectname:'upgradeData.csv',
   foldername:'upgradefolder',
  uploadpolicy:'EventCount : 10000,Interval :1m '
)
format using DSVFormatter (
)
input from @STREAM@;

end flow @APPNAME@_serverflow;

end application @APPNAME@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.test01',
	FetchSize: '1'
)
OUTPUT TO @APPNAME@_SS;


CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(id,name) COLUMNMAP(id=id,col1=col11,col2=col12)',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:10',
StandardSQL:true		
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
START APPLICATION @APPNAME@;

-- CacheTestWithDB.tql
-- Author: Gopi Putty; Date: Oct,11, 2016. 
-- A cache backed by db table.
/*
Oracle:
CREATE TABLE PRODUCT_INV ( SKU INT PRIMARY KEY NOT NULL,  NAME varchar2(20) );
INSERT INTO PRODUCT_INV (SKU, NAME)
*/

USE ADMIN;
STOP APPLICATION CACHETEST2.CACHEBACKEDWITHDB;
UNDEPLOY APPLICATION CACHETEST2.CACHEBACKEDWITHDB;
DROP APPLICATION CACHETEST2.CACHEBACKEDWITHDB CASCADE;
DROP NAMESPACE CACHETEST2 CASCADE; 
CREATE NAMESPACE CACHETEST2;
USE CACHETEST2; 

CREATE APPLICATION CACHEBACKEDWITHDB;

CREATE TYPE CTYPE(
  SKU int,
  Name String
);

CREATE STREAM skuStream OF CTYPE;

CREATE OR REPLACE CACHE BKRequestLookup USING DatabaseReader (
ConnectionURL:'jdbc:oracle:thin:@10.1.186.105:1521:orcl',
Username:'GOPI',
Password:'gopi',
Query: "
SELECT  PI.SKU, PI.NAME FROM GOPI.PRODUCT_INV PI"
) QUERY (keytomap:'SKU', refreshinterval: '60000000',charset:'UTF-8',replicas:1) OF CTYPE;

END APPLICATION CACHEBACKEDWITHDB;
DEPLOY APPLICATION CACHEBACKEDWITHDB;

USE ADMIN;

STOP APPLICATION OneAgentEncryptionTester.CSV;
UNDEPLOY APPLICATION OneAgentEncryptionTester.CSV;
DROP APPLICATION OneAgentEncryptionTester.CSV cascade;

create application CSV WITH ENCRYPTION;

CREATE FLOW AgentFlow;
create source CSVSource using CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'customerdetails-agent.csv',
  columndelimiter:',',
  positionByEOF:false
) OUTPUT TO CsvStream1;

END FLOW AgentFlow;

CREATE FLOW ServerFlow;

CREATE TYPE MyTypeCsv(
PAN String,
FNAME String KEY,
LNAME String,
ADDRESS String,
CITY String,
STATE String,
ZIP String,
GENDER String
);

CREATE STREAM TypedStreamCsv of MyTypeCsv;

CREATE CQ TypeConversionCQCsv
INSERT INTO TypedStreamCsv
SELECT
data[0],
data[1],
data[2],
data[3],
data[4],
data[5],
data[6],
data[7]
from CsvStream1;

CREATE WACTIONSTORE StoreInfoCsv CONTEXT OF MyTypeCsv
EVENT TYPES ( MyTypeCsv )
@PERSIST-TYPE@

CREATE CQ StoreWactionCsv
INSERT INTO StoreInfoCsv
SELECT * FROM TypedStreamCsv
LINK SOURCE EVENT;


END FLOW ServerFlow;

end application CSV;

DEPLOY APPLICATION CSV with AgentFlow in AGENTS, ServerFlow on any in default;
START CSV;

--
-- Crash Recovery Test 2 on four node all server cluster
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ(transform) -> JW -> CQ(no aggregate) -> WS
--

STOP APPLICATION N4S4CR2Tester.N4S4CRTest2;
UNDEPLOY APPLICATION N4S4CR2Tester.N4S4CRTest2;
DROP APPLICATION N4S4CR2Tester.N4S4CRTest2 CASCADE;
CREATE APPLICATION N4S4CRTest2 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionN4S4CRTest2;

CREATE SOURCE CsvSourceN4S4CRTest2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream;

END FLOW DataAcquisitionN4S4CRTest2;

CREATE FLOW DataProcessingN4S4CRTest2;

CREATE TYPE WactionTypeN4S4CRTest2 (
  companyName String,
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE STREAM DataStream OF WactionTypeN4S4CRTest2;

CREATE CQ CsvToWaction
INSERT INTO DataStream
SELECT
    data[0],
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM CsvStream;

CREATE JUMPING WINDOW DataStream5Minutes
OVER DataStream KEEP WITHIN 5 MINUTE ON dateTime;

CREATE WACTIONSTORE WactionsN4S4CRTest2 CONTEXT OF WactionTypeN4S4CRTest2
EVENT TYPES ( WactionTypeN4S4CRTest2 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsN4S4CRTest2
INSERT INTO WactionsN4S4CRTest2
SELECT
    *
FROM DataStream5Minutes;

END FLOW DataProcessingN4S4CRTest2;

END APPLICATION N4S4CRTest2;

STOP OuterJoinTester.OJApp;
UNDEPLOY APPLICATION OuterJoinTester.OJApp;
DROP APPLICATION OuterJoinTester.OJApp cascade;

CREATE APPLICATION OJApp;

CREATE source wsSource USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'banks.csv',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  header:No,
  trimquote:false
) OUTPUT TO QaStream;

CREATE TYPE bankData
(
bankID Integer KEY,
bankName String
);

CREATE TYPE cardData
(
cardID Integer KEY,
cardName String
);

CREATE STREAM wsStream OF bankData;

CREATE CACHE cache1 USING CsvReader(
directory: '@TEST-DATA-PATH@',
wildcard: 'banks.csv',
header: No,
columndelimiter: ',',
trimquote: false
) QUERY (keytomap:'bankID') OF bankData;


CREATE CACHE cache2 USING CsvReader(
directory: '@TEST-DATA-PATH@',
wildcard: 'bankCards.csv',
header: No,
columndelimiter: ',',
trimquote: false
) QUERY (keytomap:'cardID') OF cardData;


CREATE WACTIONSTORE oneWS CONTEXT OF bankData
EVENT TYPES(bankData )
@PERSIST-TYPE@

CREATE CQ csvTobankData
INSERT INTO oneWS
SELECT TO_INT(data[0]), data[1] FROM QaStream;



END APPLICATION OJApp;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @SOURCE@
        Using OracleReader
(
  Username: '@LOGMINER-UNAME@',
  Password: '@LOGMINER-PASSWORD@',
  ConnectionURL: '@LOGMINER-URL@',
  Tables: 'qatest.test77',
  FetchSize:1,
  QueueSize:2000,
  CommittedTransactions:true,
  Compression:false
)
Output To @STREAM@;

create Target @TARGET@ using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadpolicy:'EventCount:7'
    ServiceAccountKey:'@file-path@'
)
format using DSVFormatter (
members:'data'
)
input from @STREAM@;
end application @APPNAME@;

--
-- Recovery Test 40 with two sources and two WactionStores. A variety of partitioned windows in between
-- assure that we are testing a complicated recovery scenario.
--
-- NOTE THIS APP IS INCONSISTENT AND NOT COMPATIBLE WITH THE CURRENT VERSION OF RECOVERY BECAUSE IT HAS COMBINING STREAMS
--
-- Nicholas Keene WebAction, Inc.
--
--   S1 -> JWc2 -> JWc5 -> WS1
--   S2 -> JWc2 -> JWc7 -> WS2
--

STOP Recov40Tester.RecovTest40;
UNDEPLOY APPLICATION Recov40Tester.RecovTest40;
DROP APPLICATION Recov40Tester.RecovTest40 CASCADE;
CREATE APPLICATION RecovTest40 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE STREAM DataStreamTop OF CsvData;

CREATE CQ Csv1ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream1;

CREATE CQ Csv2ToDataStreamTop
INSERT INTO DataStreamTop
SELECT
    data[1],
    data[0],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7])
FROM CsvStream2;








CREATE JUMPING WINDOW TopJWc2
OVER DataStreamTop KEEP 2 ROWS;







CREATE STREAM DataStreamLeft OF CsvData;
CREATE STREAM DataStreamRight OF CsvData;

CREATE CQ DataToLeft
INSERT INTO DataStreamLeft
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM TopJWc2 p;

CREATE CQ DataToRight
INSERT INTO DataStreamRight
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM TopJWc2 p;






CREATE JUMPING WINDOW LeftJWc5
OVER DataStreamLeft KEEP 5 ROWS;

CREATE JUMPING WINDOW RightJWc10
OVER DataStreamRight KEEP 10 ROWS;



CREATE WACTIONSTORE WactionsLeft CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE WACTIONSTORE WactionsRight CONTEXT OF CsvData
EVENT TYPES ( CsvData )
@PERSIST-TYPE@

CREATE CQ ToWactionsLeft
INSERT INTO WactionsLeft
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM LeftJWc5 p;

CREATE CQ ToWactionsRight
INSERT INTO WactionsRight
SELECT
    FIRST(p.merchantId),
    FIRST(p.companyName),
    FIRST(p.dateTime),
    TO_DOUBLE(FIRST(p.amount))
FROM RightJWc10 p;

END APPLICATION RecovTest40;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

create application @APPNAME@ RECOVERY 5 SECOND INTERVAL;

Create Source @APPNAME@_src Using OracleReader
(
 Compression: true,
  StartTimestamp: 'null',
  SupportPDB: false,
  FetchSize: 1,
 -- QuiesceMarkerTable: 'QATEST.QUIESCEMARKER',
  CommittedTransactions: true,
  QueueSize: 2048,
  FilterTransactionBoundaries: true,
  Password_encrypted: true,
  SendBeforeImage: true,
  XstreamTimeOut: 600,
  ConnectionURL: '@CONNECTION_URL@',
  Tables: '@SOURCE_TABLE@',
  adapterName: 'OracleReader',
  Password: '@SOURCE_PASS@',
  Password_encrypted: 'false',
  DictionaryMode: 'OnlineCatalog',
  FilterTransactionState: true,
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3',
  ReaderType: 'LogMiner',
  Username: '@SOURCE_USER@',
  OutboundServerProcessName: 'WebActionXStream'
) Output To @APPNAME@_stream;

create type @APPNAME@_type(
id int,
name String,
cost float,
Lvalue long,
PreviousPaid double,
dateOrder DateTime,
DwoTime Datetime,
DeliverDate Datetime,
TableName string,
OperationName String
);

create or replace stream @APPNAME@_typed_Stream of @APPNAME@_type;

Create CQ @APPNAME@_TypedCQ
insert into @APPNAME@_typed_Stream
select
to_int(data[0]),data[1],to_float(data[2]),to_long(data[3]),to_double(data[4]),
to_Date(data[5]),to_Date(data[6]),to_Date(data[7]),
meta(@APPNAME@_stream,'TableName'),
Meta(@APPNAME@_stream,'OperationName') from @APPNAME@_stream;


create Target @APPNAME@_tgt using GCSWriter(
    bucketname:'@bucketname@',
    objectname:'@objectname@',
    projectId:'@project-id@',
    uploadPolicy:'eventCount:5',
    ServiceAccountKey:'@file-path@'
)
format using ParquetFormatter (
	schemaFileName:'@FILENAME@',
	FormatAs:'@FORMATAS@'
)
input from @APPNAME@_typed_Stream;




end application @APPNAME@;
deploy application @APPNAME@;
start application @APPNAME@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING YugabyteReader  (
 ReplicationSlotName: 'slotname',
  FilterTransactionBoundaries: 'true',
  Username: 'waction',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:postgresql://localhost:5432/webaction',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  Password: 'xFzvJYZf1b8=',
  Tables: 'public.postgrestopostgres_src',
  ExcludedTables:'public.postgres_2000target'
 )
OUTPUT TO @STREAM@ ;

stop application ps1;
stop application ps2;
undeploy application ps1;
undeploy application ps2;
drop application ps1 cascade;
drop application ps2 cascade;
CREATE application ps1 recovery 5 second interval;

CREATE OR REPLACE PROPERTYSET KafkaPropset (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', kafkaversion:'0.11', batch.size:'800000',
acks:'all',  partitions:'10', replication.factor:'1', retries_config:'50', retry_backoff_ms_config:'1000');

create type type1(
  companyName String,
  merchantId String,
  city string
);

CREATE STREAM kps1 OF Global.waevent persist using KafkaPropset;
CREATE STREAM kps2 OF Global.waevent persist using KafkaPropset;
CREATE STREAM kps3 OF Global.waevent persist using KafkaPropset;
CREATE STREAM kps4 OF Global.waevent persist using KafkaPropset;
CREATE STREAM kps5_typedStream OF type1 partition by city persist using KafkaPropset;

create source s using FileReader (
        directory:'/Users/saranyad/Product/IntegrationTests/TestData',
        wildcard:'posdata.csv',
        positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:',',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO kps;

CREATE CQ cq1
INSERT INTO kps1
SELECT *
FROM kps;

CREATE CQ cq2
INSERT INTO kps2
SELECT *
FROM kps;

CREATE CQ cq3
INSERT INTO kps3
SELECT *
FROM kps;

CREATE CQ cq4
INSERT INTO kps4
SELECT *
FROM kps;


CREATE CQ cq5
INSERT INTO kps5_typedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantId,
TO_STRING(data[10]) as city
FROM kps;

end application ps1;

CREATE application ps2 recovery 5 second interval;

create type type2(
  companyName String,
  merchantId String,
  city string
);

CREATE STREAM kps1_typedStream OF type2 partition by city;
CREATE STREAM kps2_typedStream OF type2 partition by city;
CREATE STREAM kps3_typedStream OF type2 partition by city;
CREATE STREAM kps4_typedStream OF type2 partition by city;

CREATE CQ cq6
INSERT INTO kps1_typedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantId,
TO_STRING(data[10]) as city
FROM kps1;

CREATE CQ cq7
INSERT INTO kps2_typedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantId,
TO_STRING(data[10]) as city
FROM kps2;

CREATE CQ cq8
INSERT INTO kps3_typedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantId,
TO_STRING(data[10]) as city
FROM kps3;

CREATE CQ cq9
INSERT INTO kps4_typedStream
SELECT TO_STRING(data[0]).replaceAll("COMPANY ", "") as companyName,
TO_STRING(data[1]) as merchantId,
TO_STRING(data[10]) as city
FROM kps4;

CREATE TARGET target1 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'QATEST.KPS1'
) INPUT FROM kps1_typedStream;

CREATE TARGET target2 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'QATEST.KPS2'
) INPUT FROM kps2_typedStream;

CREATE TARGET target3 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'QATEST.KPS3'
) INPUT FROM kps3_typedStream;

CREATE TARGET target4 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'QATEST.KPS4'
) INPUT FROM kps4_typedStream;

CREATE TARGET target5 USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'QATEST.KPS5'
) INPUT FROM kps5_typedStream;

end application ps2;
--DEPLOY APPLICATION KafkaWriterApp with sourceFlow  ON ANY IN DEFAULT, targetFlow ON ALL IN DEFAULT;

drop user dtest;
drop namespace dtest cascade;

create user dtest identified by test;
grant Global.appuser to user dtest;
connect dtest test;
create cq permissioncq select * from banker.oneWS where ( bankID = :bI);

STOP APPLICATION oraddl;
UNDEPLOY APPLICATION oraddl;
DROP APPLICATION oraddl CASCADE;
CREATE APPLICATION oraddl recovery 5 second interval;
 
Create Source Ora Using OracleReader 
(
 Username:'@user-name@',
 Password:'@password@',
 ConnectionURL:'src_url',
 Tables:'QATEST.ORACLEDDL%',
 DictionaryMode:OfflineCatalog,
 DDLCaptureMode : 'All',
 FetchSize:1
) Output To LogminerStream;

Create Target tgt using DatabaseWriter 
(
 Username:'@username@',
 Password:'@password@',
 ConnectionURL:'TGT_URL',
 BatchPolicy:'EventCount:1,Interval:1',
 CommitPolicy:'EventCount:1,Interval:1',
 IgnorableExceptionCode: '1,2290,942',
 Tables :'QATEST.ORACLEDDL%,QATEST2.%'
) input from LogminerStream;

end application oraddl;
deploy application oraddl;
start application oraddl;

STOP application XmlFormatterTester.DSV;
undeploy application XmlFormatterTester.DSV;
drop application XmlFormatterTester.DSV cascade;

create application DSV;
create source CSVSmallPosDataSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvSmallPosDataStream;

create source CSVPosDataSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'posdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvPosDataStream;


Create Type CSVPosDataType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVSmallPosDataStream of CSVPosDataType;
Create Stream TypedCSVPosDataStream of CSVPosDataType;


CREATE CQ CsvToSmallPosData
INSERT INTO TypedCSVSmallPosDataStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvSmallPosDataStream;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVPosDataStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvPosDataStream;

/**
*  3.2.1.b FileWriter XML TimeInterval
**/
create Target XmlFormatterTimeInterval using FileWriter(
  filename:'TargetPosDataXmlTIOpt',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:1m,sequence:00'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:merchantid:text=merchantname',
  charset:'UTF-8'
)
input from TypedCSVSmallPosDataStream;

/**
* 3.2.1.c FileWriter XMLFileSize 101MB
**/
create Target XmlFormatterFileSize101 using FileWriter(
  filename:'TargetPosDataXmlFS',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'FileSizeRollingPolicy,filesize:101M,sequence:00'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:merchantid:text=merchantname',
  charset:'UTF-8'
)
input from TypedCSVPosDataStream;

/**
* 3.2.1.d FileWriter XMLDefaultFS 10 MB
**/
create Target XmlFormatterDefault using FileWriter(
  filename:'TargetPosDataXmlFSDefault',
  directory:'@FEATURE-DIR@/logs/',
    sequence:'00',
  rolloverpolicy:'FileSizeRollingPolicy'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:zip:text=merchantname',
  charset:'UTF-8'
)
input from TypedCSVSmallPosDataStream;


/**
* 3.2.1.e FileWriter XML EventCount 2000
**/
create Target XmlFormatterEventCount2000 using FileWriter(
  filename:'TargetPosDataXmlEC',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'eventcount:2000,sequence:00'
)
format using XMLFormatter (
  rootelement:'document',
  elementtuple:'MerchantName:zip:text=merchantname',
  charset:'UTF-8'
)
input from TypedCSVSmallPosDataStream;

create Target LogWriterSmallPosData using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetPosDataXmlDFS_actual.log') input from TypedCSVSmallPosDataStream;

end application DSV;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.test01',
	FetchSize: '1',
	connectionRetryPolicy:'timeOut=00,retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(id,name) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true		
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

--
-- Recovery Test 36 with two sources, two jumping attribute windows, and one wactionstore -- all partitioned on the same key
-- Nicholas Keene WebAction, Inc.
--
-- S1 -> Ja5W/p -> CQ1 -> WS
-- S2 -> Ja6W/p -> CQ2 -> WS
--

STOP NameW03.W03;
UNDEPLOY APPLICATION NameW03.W03;
DROP APPLICATION NameW03.W03 CASCADE;
CREATE APPLICATION W03 RECOVERY 5 SECOND INTERVAL;

CREATE SOURCE CsvSource1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'Canon1000.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream1;

CREATE SOURCE CsvSource2 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestDataLong.csv',
  wildcard:'Canon1000.csv',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO CsvStream2;

CREATE TYPE CsvData (
  merchantId String KEY,
  companyName String,
  dateTime DateTime,
  amount double
);

CREATE TYPE WactionData (
  firstCompanyName String KEY,
  dateTime DateTime,
  totalCompanies int,
  firstMerchantId String
);

CREATE STREAM DataStream1 OF CsvData
PARTITION BY merchantId;
CREATE STREAM DataStream2 OF CsvData
PARTITION BY merchantId;

CREATE CQ CsvToData1
INSERT INTO DataStream1
SELECT
    data[1],
    data[0],
    TO_DATEF(data[2],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[3])
FROM CsvStream1;

CREATE CQ CsvToData2
INSERT INTO DataStream2
SELECT
    data[1],
    data[0],
    TO_DATEF(data[2],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[3])
FROM CsvStream2;

CREATE JUMPING WINDOW JWc51uW03
OVER DataStream1 KEEP 51 ROWS;

CREATE JUMPING WINDOW JWc101uW03
OVER DataStream2 KEEP 101 ROWS;

CREATE WACTIONSTORE WactionsW03 CONTEXT OF WactionData
EVENT TYPES ( CsvData KEY(merchantId) )
@PERSIST-TYPE@

CREATE CQ JWc51uW03_to_WactionsW03
INSERT INTO WactionsW03
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM JWc51uW03 p
GROUP BY p.merchantId;

CREATE CQ JWc101uW03_to_WactionsW03
INSERT INTO WactionsW03
SELECT
    FIRST(p.companyName),
    FIRST(p.dateTime),
    COUNT(p.amount),
    FIRST(p.merchantId)
FROM JWc101uW03 p
GROUP BY p.merchantId;

END APPLICATION W03;

create application HTTPtest;
create source DSVCSVSource using FileReader (
	directory:'@TEST-DATA-PATH@',
	WildCard:'customerdetails.csv',
	charset: 'UTF-8',
	positionByEOF:false
)
parse using DSVParser (
	header:'no'
)
OUTPUT TO DSVCsvStream;
create Target DSVDump using CSVWriter(fileName:'@FEATURE-DIR@/logs/customerdetails') input from DSVCsvStream;
end application HTTPtest;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@;

CREATE SOURCE @SourceName@ USING PostgreSQLReader  ( 
ReplicationSlotName: 'test_slot',
adapterName: 'PostgreSQLReader',
TransactionSupport: false, 
  FetchTransactionMetadata: false, 
  Compression: false, 
  connectionRetryPolicy: 'timeOut=30, retryInterval=30, maxRetries=3', 
  Password_encrypted: 'false', 
  ConnectionURL: '@ConnectionURL@', 
  Fetchsize: 0, 
  ConnectionPoolSize: 10, 
  StartPosition: 'EOF', 
  Username: '@UN@', 
  cdcRoleName: 'STRIIM_READER', 
  Password: '@PWD@', 
  Tables: 'qatest.%', 
  FilterTransactionBoundaries: true, 
  SendBeforeImage: true, 
  AutoDisableTableCDC: false ) 
OUTPUT TO @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;


CREATE CQ @cqName@ INSERT INTO admin.sqlreader_cq_out SELECT ChangeOperationToInsert(PutUserData (x, 'OpType', META(x, 'OperationName'), 'OpTime',META(x, 'TimeStamp'))) FROM @SRCINPUTSTREAM@ x; ;

CREATE TARGET @targetName@ USING DatabaseWriter  ( 
ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3', 
  ParallelThreads: '', 
  CheckPointTable: 'CHKPOINT', 
  Password_encrypted: 'false', 
  ConnectionURL: '@ConnectionURL@', 
  CommitPolicy: 'EventCount:1,Interval:60', 
  StatementCacheSize: '50', 
  DatabaseProviderType: 'Default', 
  Username: '@UN@', 
  Password: '@PWD@', 
  PreserveSourceTransactionBoundary: 'false', 
  BatchPolicy: 'EventCount:1,Interval:60', 
  Tables: '@TableMapping@' ) 
INPUT FROM admin.sqlreader_cq_out;

END APPLICATION @APPNAME@;
deploy application @APPNAME@ in default;
start @APPNAME@;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;

CREATE SOURCE @APPNAME@_S USING MSSqlReader
(
  Compression: false,
  cdcRoleName: 'STRIIM_READER',
  DatabaseName: 'QATEST',
  connectionRetryPolicy: 'timeOut=00, retryInterval=1, maxRetries=3',
  ConnectionPoolSize: 1,
  FetchTransactionMetadata: false,
  StartPosition: 'EOF',
  Username: 'qatest',
  SendBeforeImage: true,
  AutoDisableTableCDC: true,
  ConnectionURL: 'localhost:1433',
  Tables: 'qatest.test01',
  adapterName: 'MSSqlReader',
  Password: 'w3b@ct10n'
)
OUTPUT TO @APPNAME@_SS;

CREATE or replace TARGET @APPNAME@_T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'QATEST.test01,@DATASET@.% KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'NULL',
columnDelimiter: '|',
BatchPolicy: 'eventCount:1,Interval:0',
StandardSQL:true	
) INPUT FROM @APPNAME@_ss;

END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

stop application MySQLToOracle;
undeploy application MySQLToOracle;
drop application MySQLToOracle cascade;

CREATE APPLICATION MySQLToOracle recovery 5 second interval;

create source Src1ReadFromMySQL USING MySQLReader (
Username: 'root',
  Password: 'w@ct10n',
  ConnectionURL: 'jdbc:mysql://localhost:3306/waction',
--Tables: 'waction.mytable1;waction.mytable2;qatest.mysqlmarker',
Tables: 'waction.Parent%;waction.Child%;',
--Tables: 'waction.mytable%',
BiDirectionalMarkerTable: 'waction.mysqlmarker',
compression:'false',
sendBeforeImage:True
) OUTPUT TO App1Stream;

/*
CREATE TARGET MySQLReaderOutput
USING SysOut(name:MySQLReaderOutput)
INPUT FROM App1Stream;
*/

CREATE TARGET MySQLReaderOutput using FileWriter(
  filename:'MySQLReaderOutput.log',
  flushpolicy: 'EventCount:1, Interval:30s',
  rolloverpolicy: 'Eventcount:1000, rolloverpolicy:3000s')
FORMAT USING JsonFormatter ()

INPUT FROM App1Stream;


CREATE TARGET WriteToOracle USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'EventCount:10,Interval:5',
CommitPolicy:'EventCount:15,Interval:10',
BiDirectionalMarkerTable: 'qatest.mysqlmarker',
--Tables: 'waction.mytable1,qatest.mytable1;'
--Tables: 'waction.table_1_1,qatest.table_1_1;waction.table_2_1,qatest.table_2_1;waction.table_1_2,qatest.table_1_2;waction.table_2_2,qatest.table_2_2'
--Tables: 'waction.mytable%,qatest.%'
Tables: 'waction.Parent%,qatest.%;waction.Child%,qatest.%'

)
INPUT FROM App1Stream;



--Orcle to MySQL

Create Source SrcReadFromOracle
Using OracleReader
(
Username:'qatest',
Password:'qatest',
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
--Tables:'qatest.mytable1;qatest.mytable2;qatest.mysqlmarker;qatest.mysqlmarker2',
Tables:'QATEST.PARENT%;QATEST.CHILD%',
--Tables:'qatest.mytable%',
BiDirectionalMarkerTable: 'QATEST.MYSQLMARKER',
FilterTransactionBoundaries: true,
compression:'false',
FetchSize: 1

)
Output To App2Stream;

/*
CREATE TARGET MSSqlReaderOutput
USING SysOut(name:MSSqlReaderOutput)
INPUT FROM App2Stream;
*/

CREATE TARGET OracleReaderOutput using FileWriter(
  filename:'OracleReaderOutput.log',
    flushpolicy: 'EventCount:1, Interval:30s',
  rolloverpolicy: 'Eventcount:1000, rolloverpolicy:3000s')
FORMAT USING JsonFormatter ()

INPUT FROM App2Stream;



CREATE TARGET WriteToMySQL1 USING DatabaseWriter(
ConnectionURL:'jdbc:mysql://localhost:3306/waction',
Username:'root',
Password:'w@ct10n',
BatchPolicy:'EventCount:10,Interval:10',
CommitPolicy: 'EventCount:15,Interval:12',
BiDirectionalMarkerTable: 'waction.mysqlmarker',
--Tables: 'qatest.mytable1,waction.mytable1;qatest.mytable2,waction.mytable3;'
Tables: 'QATEST.PARENT_1,waction.Parent_1;QATEST.PARENT_2,waction.Parent_2;QATEST.CHILD_1,waction.Child_1;QATEST.CHILD_2,waction.Child_2;'
--Tables: 'qatest.mytable%,waction.%'
)
INPUT FROM App2Stream;




END APPLICATION MySQLToOracle;
deploy application MySQLToOracle;
start application MySQLToOracle;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;

--CREATE APPLICATION @APPNAME@;
create application @APPNAME@ Recovery 5 second Interval;

--create or replace flow @APPNAME@_agentflow;

CREATE OR REPLACE SOURCE @APPNAME@_source USING DatabaseReader  (
  Username: 'qatest',
  Password_encrypted: false,
  ConnectionURL: 'jdbc:oracle:thin:@//localhost:1521/xe',
  Tables: 'QATEST.EMP_INIT',
  adapterName: 'DatabaseReader',
  FetchSize: 100,
  Password: 'qatest'
 )
OUTPUT TO @APPNAME@_stream ;

--end flow @APPNAME@_@APPNAME@_agentflow;

CREATE OR REPLACE TARGET @APPNAME@_target USING CassandraCosmosDBWriter  (
  AccountEndpoint: 'cassandracosmostest.cassandra.cosmos.azure.com',
  AccountKey: 'pqDZvVgbdSCg7VzIzD77dAhPG2odGRZPLhAQA1qnZbAKoIDk6RuQX5r2phbRQFnR1l54qxOcvBXNdz8DeijYIg==',
  KeySpace: 'test',
  LoadBalancingPolicy:'TokenAwarePolicy(RoundRobinPolicy())',
  Tables: 'QATEST.Source1,test.target1',
  adapterName: 'CassandraCosmosDBWriter'
 )
 INPUT FROM @APPNAME@_stream;

 END APPLICATION @APPNAME@;

deploy application @APPNAME@;
 --deploy application @APPNAME@ with agentflow in agents;
 start application @APPNAME@;

--
-- Crash Recovery Test 1 on two node cluster with Kafka Stream
-- Bert Hashemi, WebAction, Inc.
--
-- S -> CQ -> WS
--

STOP APPLICATION KStreamN2S2CR1Tester.KStreamN2S2CRTest1;
UNDEPLOY APPLICATION KStreamN2S2CR1Tester.KStreamN2S2CRTest1;
DROP APPLICATION KStreamN2S2CR1Tester.KStreamN2S2CRTest1 CASCADE;

DROP USER KStreamN2S2CR1Tester;
DROP NAMESPACE KStreamN2S2CR1Tester CASCADE;
CREATE USER KStreamN2S2CR1Tester IDENTIFIED BY KStreamN2S2CR1Tester;
GRANT create,drop ON deploymentgroup Global.* TO USER KStreamN2S2CR1Tester;
CONNECT KStreamN2S2CR1Tester KStreamN2S2CR1Tester;

CREATE APPLICATION KStreamN2S2CRTest1 RECOVERY 5 SECOND INTERVAL;

CREATE FLOW DataAcquisitionKStreamN2S2CRTest1;

CREATE OR REPLACE PROPERTYSET KafkaProps (zk.address:'localhost:2181', bootstrap.brokers:'localhost:9092', partitions:'242');
CREATE STREAM KafkaCsvStream using KafkaProps;

CREATE SOURCE CsvSourceKStreamN2S2CRTest1 USING CSVReader (
  directory:'@TEST-DATA-PATH@',
  header:Yes,
  wildcard:'RecovTestData.csv',
  columndelimiter:',',
  blocksize: 10240,
  positionByEOF:false,
  trimquote:false
) OUTPUT TO KafkaCsvStream;

END FLOW DataAcquisitionKStreamN2S2CRTest1;

CREATE FLOW DataProcessingKStreamN2S2CRTest1;

CREATE TYPE WactionTypeKStreamN2S2CRTest1 (
  merchantId String KEY,
  dateTime DateTime,
  amount double,
  city String
);

CREATE WACTIONSTORE WactionsKStreamN2S2CRTest1 CONTEXT OF WactionTypeKStreamN2S2CRTest1
EVENT TYPES ( WactionTypeKStreamN2S2CRTest1 )
@PERSIST-TYPE@

CREATE CQ InsertWactionsKStreamN2S2CRTest1
INSERT INTO WactionsKStreamN2S2CRTest1
SELECT
    data[1],
    TO_DATEF(data[4],'yyyyMMddHHmmss'),
    TO_DOUBLE(data[7]),
    data[10]
FROM KafkaCsvStream;

END FLOW DataProcessingKStreamN2S2CRTest1;

END APPLICATION KStreamN2S2CRTest1;

stop application Postgres_To_Filewriter;
undeploy application Postgres_To_Filewriter;
drop application Postgres_To_Filewriter cascade;

CREATE APPLICATION Postgres_To_Filewriter RECOVERY 5 SECOND INTERVAL;

CREATE OR REPLACE SOURCE Postgres_Src USING PostgreSQLReader  ( 
  ReplicationSlotName: '',
  FilterTransactionBoundaries: 'true',
  Username: '',
  Password_encrypted: false,
  ConnectionURL: '',
  adapterName: 'PostgreSQLReader',
  PostgresConfig: '@PGConfig@',
  ConnectionRetryPolicy: 'retryInterval=30, maxRetries=3',
  --startLSN:'758199500005967269518578814280000',
  Password: '',
  Tables: ''
 ) 
OUTPUT TO Change_Data_Stream;


CREATE OR REPLACE TARGET Postgres_Sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM Change_Data_Stream;
 
 CREATE TARGET BinaryDump USING LogWriter(
  name: 'PostgresCDCData',
  filename:'PostgresCDCData.log'
)INPUT FROM Change_Data_Stream;
 

end application Postgres_To_Filewriter;
deploy application Postgres_To_Filewriter;
start Postgres_To_Filewriter;

STOP APPLICATION @APP_NAME@;
UNDEPLOY APPLICATION @APP_NAME@;
DROP APPLICATION @APP_NAME@ CASCADE;
DROP EXCEPTIONSTORE @APP_NAME@_EXCEPTIONSTORE;

CREATE APPLICATION @APP_NAME@ @APP_PROPERTY@ USE EXCEPTIONSTORE;

CREATE SOURCE @APP_NAME@_Source Using @SOURCE_ADAPTER@ (

) OUTPUT TO @APP_NAME@DataStream;


CREATE TARGET @TARGETNAME@ USING @TARGET_ADAPTER@ (

) INPUT FROM @APP_NAME@DataStream;

END APPLICATION @APP_NAME@;

DEPLOY APPLICATION @APP_NAME@ IN DEFAULT;
START APPLICATION @APP_NAME@;

CREATE SOURCE @SOURCE_NAME@ USING Global.OracleReader
(
FetchSize:'@FETCHSIZE@',
QueueSize:'@QUEUESIZE@',
Username:'@USERNAME@',
Password:'@PASSWORD@',
ConnectionURL:'@URL@',
_h_useClassic: @CLASSICMODE@,
Tables:'@TABLES@',
CommittedTransactions:'@COMMITEDT@'
Compression:'@COMPRESSION@'
)
OUTPUT TO @STREAM@ MAP (table: '@SOURCE_SCHEMA@.@SOURCE_TABLE@1')
SELECT NUM_COL,CHAR_COL,VARCHAR2_COL,LONG_COL,DATE_COL,TIMESTAMP_COL where TO_INT(NUM_COL) > 1;

CREATE TYPE LogType(
Num_col String key,
Char_col String,
Varchar2_col String,
long_col String,
Table String,
Operation String
);

CREATE WINDOW CDCWindow
OVER @STREAM@
KEEP 1 ROWS;

CREATE WACTIONSTORE CDCWS CONTEXT of LogType
EVENT TYPES ( LogType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ ToWactionStore
INSERT INTO CDCWS
SELECT * FROM CDCWindow
LINK SOURCE EVENT;

CREATE TARGET @SOURCE_NAME@_SYS USING SysOut (
  name: '@SOURCE_NAME@_SYS' )
INPUT FROM @STREAM@;

stop application @APPNAME@app3;
undeploy application @APPNAME@app3;
alter application @APPNAME@app3;
CREATE or replace TARGET @APPNAME@app3_targetNew USING DatabaseWriter(
ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
Username:'qatest',
Password:'qatest',
BatchPolicy:'Eventcount:10000,Interval:1',
CommitPolicy:'Interval:1,Eventcount:10000',
Tables:'qatest.test01,QATEST.KPS3_Alter'
) INPUT FROM @APPNAME@sourcestream;
alter application @APPNAME@app3 recompile;
deploy application @APPNAME@app3;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @parquetsrc@ USING S3Reader (
    bucketname:'',
    objectnameprefix:'',
     )
PARSE USING ParquetParser (
)
OUTPUT TO @appname@Stream;

CREATE CQ @appname@CQ
INSERT INTO @appname@CqOut
    SELECT putUserData(p,'schemaName',p.data.getSchema().getName()) FROM @appname@Stream p;

CREATE OR REPLACE TARGET @filetarget@ USING FileWriter (
filename: '',
directory: '',
flushpolicy: 'EventCount:1,Interval:30s',
rolloverpolicy: 'EventCount:10000,Interval:30s' )
FORMAT USING ParquetFormatter  (
schemaFileName: 'ParquetFileSchema'
)
INPUT FROM @appname@CqOut;

create Target @s3target@ using S3Writer(
    bucketname:'',
    objectname:'',
    uploadpolicy:'',
    foldername:''
)
format using ParquetFormatter (
schemaFileName: 'ParquetS3Schema'
)
input from @appname@CqOut;

create Target @blobtarget@ using AzureBlobWriter(
	accountname:'',
	accountaccesskey:'',
	containername:'',
    blobname:'',
	foldername:'',
	uploadpolicy:'EventCount:10,interval:5s'
)
format using ParquetFormatter (
schemaFileName: 'ParquetAzureSchema'
)
INPUT FROM @appname@CqOut;

CREATE OR REPLACE TARGET @gcstarget@ USING GCSWriter (
    bucketname:'',
    objectname:'',
    foldername:'',
    projectId:'',
    uploadPolicy:''
)
format using ParquetFormatter (
schemaFileName: 'ParquetGCSSchema'
)
INPUT FROM @appname@CqOut;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

create Target @TARGET_NAME@ using ADLSGen2Writer(
          accountname:'adlsgen2nntest',
        sastoken:'sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-04-05T16:05:31Z&st=2019-03-22T08:05:31Z&spr=https&sig=qjToTrY8dNnzOVQDOaNhRBsLnEWGsRWQ9BSnYki6L8k%3D',
        filesystemname:'qatestadls2',
        directory:'%@metadata(TableName)%',
        filename:'table.csv',
        uploadpolicy:'filesize:10M'
)
FORMAT USING dsvFormatter()
input from @STREAM@;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING Global.GGTrailReader (
  Tables:'@TABLES@',
  CDDLCapture: false,
  TrailDirectory: '@TRAIL_FILE_DIR@',
  TrailFilePattern: '@WILDCARD@',
  Compression: false,
  SupportColumnCharset: false,
  CDDLAction: 'Process',
  FilterTransactionBoundaries: true,
  adapterName: 'GGTrailReader',
  TrailByteOrder: '@ENDIAN@' )
OUTPUT TO @STREAM@;

CREATE TARGET @SOURCE_NAME@_sysout USING Global.SysOut (
  name: '@SOURCE_NAME@_SysOut' )
INPUT FROM @STREAM@;

stop application FileWriterTest;
undeploy application FileWriterTest;
drop application FileWriterTest cascade;
create application FileWriterTest recovery 1 second interval;
create source File_SRC Using FileReader(
	directory:'@DIRECTORY@',
	WildCard:'@FILENAME@',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	header:'false'
)
OUTPUT TO CsvStream;

create Target FileTarget using FileWriter(
    rolloverpolicy:'@UPLOAD-SIZE@',
    filename :'@FILE@',
    directory : '@FOLDER@'
)
format using DSVFormatter (
charset:'@charset@',
members:'@mem@',
header:'@head@'

)
input from CsvStream;

end application FileWriterTest;
deploy application FileWriterTest on all in default;
start application FileWriterTest;

STOP APPLICATION KafkaWPTester.KWApp;
STOP APPLICATION KafkaWPTester.KRApp;
UNDEPLOY APPLICATION KafkaWPTester.KWApp;
UNDEPLOY APPLICATION KafkaWPTester.KRApp;
DROP APPLICATION KafkaWPTester.KWApp CASCADE;
DROP APPLICATION KafkaWPTester.KRApp CASCADE;

CREATE USER KafkaWPTester IDENTIFIED BY KafkaWPTester;
GRANT create,drop ON deploymentgroup Global.* TO USER KafkaWPTester;
CONNECT KafkaWPTester KafkaWPTester;


CREATE APPLICATION KWApp RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE CSVSource USING FileReader (
	directory:'/Users/saranyad/Product/IntegrationTests/TestData/kafka_tmp',
    WildCard:'mybanks*',
	positionByEOF:false,
        Charset:'UTF-8'
)
PARSE USING DSVParser (
  columndelimiter:',',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~'
)
OUTPUT TO FileStream;


CREATE TYPE AccessLogType(
Col1 String,
Col2 String
);

CREATE STREAM TypedAccessLogStream1 OF AccessLogType;
CREATE STREAM TypedAccessLogStream2 OF AccessLogType;
CREATE STREAM TypedAccessLogStream3 OF AccessLogType;
CREATE STREAM TypedAccessLogStream4 OF AccessLogType;
CREATE STREAM TypedAccessLogStream5 OF AccessLogType;
CREATE STREAM TypedAccessLogStream6 OF AccessLogType;
CREATE STREAM TypedAccessLogStream7 OF AccessLogType;
CREATE STREAM TypedAccessLogStream8 OF AccessLogType;
CREATE STREAM TypedAccessLogStream9 OF AccessLogType;
CREATE STREAM TypedAccessLogStream10 OF AccessLogType;
CREATE STREAM TypedAccessLogStream11 OF AccessLogType;
CREATE STREAM TypedAccessLogStream12 OF AccessLogType;
CREATE STREAM TypedAccessLogStream13 OF AccessLogType;
CREATE STREAM TypedAccessLogStream14 OF AccessLogType;
CREATE STREAM TypedAccessLogStream15 OF AccessLogType;
CREATE STREAM TypedAccessLogStream16 OF AccessLogType;
CREATE STREAM TypedAccessLogStream17 OF AccessLogType;
CREATE STREAM TypedAccessLogStream18 OF AccessLogType;
CREATE STREAM TypedAccessLogStream19 OF AccessLogType;
CREATE STREAM TypedAccessLogStream20 OF AccessLogType;

CREATE CQ AcceeslogCQ
INSERT INTO TypedAccessLogStream
SELECT 
TO_STRING(data[0]) as Col1,
TO_STRING(data[1]) as Col2
FROM FileStream ; 

CREATE CQ AcceeslogCQ1
INSERT INTO TypedAccessLogStream1
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS WHERE FS.Col1 = '1'; 


create Target KW1 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test01',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream1;

CREATE CQ AcceeslogCQ2
INSERT INTO TypedAccessLogStream2
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '2'; 


create Target KW2 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test02',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream2;

CREATE CQ AcceeslogCQ3
INSERT INTO TypedAccessLogStream3
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '3'; 


create Target KW3 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test03',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream3;

CREATE CQ AcceeslogCQ4
INSERT INTO TypedAccessLogStream4
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '4'; 


create Target KW4 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test04',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream4;

CREATE CQ AcceeslogCQ5
INSERT INTO TypedAccessLogStream5
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '5'; 


create Target KW5 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test05',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream5;

CREATE CQ AcceeslogCQ6
INSERT INTO TypedAccessLogStream6
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '6'; 


create Target KW6 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test06',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream6;

CREATE CQ AcceeslogCQ7
INSERT INTO TypedAccessLogStream7
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '7'; 


create Target KW7 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test07',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream7;

CREATE CQ AcceeslogCQ8
INSERT INTO TypedAccessLogStream8
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '8'; 


create Target KW8 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test08',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream8;

CREATE CQ AcceeslogCQ9
INSERT INTO TypedAccessLogStream9
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '9'; 


create Target KW9 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test09',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream9;

CREATE CQ AcceeslogCQ10
INSERT INTO TypedAccessLogStream10
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '10'; 


create Target KW10 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test10',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream10;

CREATE CQ AcceeslogCQ11
INSERT INTO TypedAccessLogStream11
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '11'; 


create Target KW11 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test11',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream11;

CREATE CQ AcceeslogCQ12
INSERT INTO TypedAccessLogStream12
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '12'; 


create Target KW12 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test12',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream12;

CREATE CQ AcceeslogCQ13
INSERT INTO TypedAccessLogStream13
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '13'; 


create Target KW13 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test13',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream13;

CREATE CQ AcceeslogCQ14
INSERT INTO TypedAccessLogStream14
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '14'; 


create Target KW14 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test14',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream14;

CREATE CQ AcceeslogCQ15
INSERT INTO TypedAccessLogStream15
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '15'; 


create Target KW15 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test15',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream15;

CREATE CQ AcceeslogCQ16
INSERT INTO TypedAccessLogStream16
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '16'; 


create Target KW16 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test16',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream16;

CREATE CQ AcceeslogCQ17
INSERT INTO TypedAccessLogStream17
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '17'; 


create Target KW17 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test17',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream17;

CREATE CQ AcceeslogCQ18
INSERT INTO TypedAccessLogStream18
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '18'; 


create Target KW18 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test18',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream18;

CREATE CQ AcceeslogCQ19
INSERT INTO TypedAccessLogStream19
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '19'; 


create Target KW19 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test19',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream19;

CREATE CQ AcceeslogCQ20
INSERT INTO TypedAccessLogStream20
SELECT FS.Col1, FS.Col2
FROM TypedAccessLogStream FS
WHERE FS.Col1 = '20'; 


create Target KW20 using KafkaWriter VERSION '0.10.0' ( 
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test20',
	    Mode:'Sync'
        )

FORMAT USING DSVFormatter ()
input from TypedAccessLogStream20;


END APPLICATION KWApp;

DEPLOY APPLICATION KWApp on any in default;



















-- Kafka Reader Apps

CREATE APPLICATION KRApp RECOVERY 1 SECOND INTERVAL;

CREATE SOURCE KR1 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test01',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;



CREATE SOURCE KR2 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test02',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream2;


CREATE SOURCE KR3 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test03',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream3;


CREATE SOURCE KR4 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test04',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream4;


CREATE SOURCE KR5 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test05',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream5;


CREATE SOURCE KR6 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test06',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream6;


CREATE SOURCE KR7 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test07',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream7;


CREATE SOURCE KR8 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test08',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream8;


CREATE SOURCE KR9 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test09',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream9;


CREATE SOURCE KR10 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test10',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream10;


CREATE SOURCE KR11 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test11',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream11;



CREATE SOURCE KR12 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test12',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream12;


CREATE SOURCE KR13 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test13',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream13;


CREATE SOURCE KR14 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test14',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream14;


CREATE SOURCE KR15 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test15',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream15;


CREATE SOURCE KR16 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test16',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream16;


CREATE SOURCE KR17 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test17',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream17;


CREATE SOURCE KR18 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test18',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream18;


CREATE SOURCE KR19 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test19',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream19;


CREATE SOURCE KR20 USING KafkaReader VERSION '0.10.0' (
        brokerAddress:'localhost:9092, localhost:9093',
        Topic:'V8_test20',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream20;




CREATE TARGET DumpKafkaReaderStream1 USING FileWriter(
  name:KafkaROuput1,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_1',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream1;


CREATE TARGET DumpKafkaReaderStream2 USING FileWriter(
  name:KafkaROuput2,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_2',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream2;


CREATE TARGET DumpKafkaReaderStream3 USING FileWriter(
  name:KafkaROuput3,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_3',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream3;


CREATE TARGET DumpKafkaReaderStream4 USING FileWriter(
  name:KafkaROuput4,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_4',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream4;


CREATE TARGET DumpKafkaReaderStream5 USING FileWriter(
  name:KafkaROuput5,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_5',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream5;


CREATE TARGET DumpKafkaReaderStream6 USING FileWriter(
  name:KafkaROuput6,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_6',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream6;


CREATE TARGET DumpKafkaReaderStream7 USING FileWriter(
  name:KafkaROuput7,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_7',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream7;


CREATE TARGET DumpKafkaReaderStream8 USING FileWriter(
  name:KafkaROuput8,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_8',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream8;


CREATE TARGET DumpKafkaReaderStream9 USING FileWriter(
  name:KafkaROuput9,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_9',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream9;


CREATE TARGET DumpKafkaReaderStream10 USING FileWriter(
  name:KafkaROuput10,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_10',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream10;


CREATE TARGET DumpKafkaReaderStream11 USING FileWriter(
  name:KafkaROuput11,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_11',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream11;


CREATE TARGET DumpKafkaReaderStream12 USING FileWriter(
  name:KafkaROuput12,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_12',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream12;


CREATE TARGET DumpKafkaReaderStream13 USING FileWriter(
  name:KafkaROuput13,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_13',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream13;


CREATE TARGET DumpKafkaReaderStream14 USING FileWriter(
  name:KafkaROuput14,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_14',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream14;


CREATE TARGET DumpKafkaReaderStream15 USING FileWriter(
  name:KafkaROuput15,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_15',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream15;


CREATE TARGET DumpKafkaReaderStream16 USING FileWriter(
  name:KafkaROuput16,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_16',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream16;


CREATE TARGET DumpKafkaReaderStream17 USING FileWriter(
  name:KafkaROuput17,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_17',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream17;


CREATE TARGET DumpKafkaReaderStream18 USING FileWriter(
  name:KafkaROuput18,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_18',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream18;


CREATE TARGET DumpKafkaReaderStream19 USING FileWriter(
  name:KafkaROuput19,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_19',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream19;


CREATE TARGET DumpKafkaReaderStream20 USING FileWriter(
  name:KafkaROuput20,
  filename:'/Users/saranyad/Product/IntegrationTests/target/test-classes/features/AllTargetWriters/KafkaWriterTests/logs/V8.DumpKafkaWriterSyncMode_20',
flushpolicy : 'flushcount:1',
rolloverpolicy : 'EventCount:10000,Interval:30s'
)
FORMAT USING DSVFormatter()
 INPUT FROM KafkaReaderStream20;

END APPLICATION KRApp;
DEPLOY APPLICATION KRApp on any in default;

Stop application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

CREATE APPLICATION DSV;

CREATE FLOW HTTPsource;

CREATE SOURCE HTTPSOURCE USING FileReader (
  directory:'@TEST-DATA-PATH@',
  wildcard:'http.txt',
  skipbom: true,
  rolloverpolicy: 'DefaultFileComparator',
  blocksize: '64',
  charset: 'UTF-8',
  positionbyeof: false
 )
 PARSE USING DSVParser (
  trimquote: true,
  columndelimiter: ' ',
  rowdelimiter: '\n',
  header: false,
  quoteset: '{}'
 )
OUTPUT TO RawHTTPStream;

END FLOW HTTPSource;

CREATE FLOW main;

CREATE TYPE HTTPLogEntry (
     start_time String,
     srcIp      String KEY,
     port       String,
     method     String,
     url        String,
     error_num  String,
     status     String,
     end_time   String,
     host       String
 );

CREATE STREAM HTTPStream OF HTTPLogEntry;

CREATE CQ ParseHTTPLog
  INSERT INTO HTTPStream
  SELECT  MATCH(data[1],'start\\s+(\\w+.\\w+)'),
          matchIP(data[2]),
          MATCH(data[3],'port\\W+(\\w+)'),
          MATCH(data[4],'method\\W+(\\w+)'),
          data[5],
          data[7],
          data[8],
          data[9],
          data[10]
  FROM RawHTTPStream;

create Target t using FileWriter(
  filename:'XmlTrimQuote',
  sequence:'00',
  directory:'@FEATURE-DIR@/logs/',
  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:10s,sequence:00'
)
format using DSVFormatter (

)
input from HTTPStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TrimQuoteTest_actual.log') input from HTTPStream;

END FLOW main;

END APPLICATION DSV;

STOP APPLICATION testApp;
UNDEPLOY APPLICATION testApp;
DROP APPLICATION testApp CASCADE;
-- DROP EXCEPTIONSTORE testApp_exceptionstore;

CREATE APPLICATION testApp WITH ENCRYPTION RECOVERY 10 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE OR REPLACE SOURCE testApp_Source USING OracleReader  (
  Username:'qatest',
  Password:'qatest',
  ConnectionURL:'jdbc:oracle:thin:@//localhost:1521/xe',
  OnlineCatalog:true,
  FetchSize:'1',
  Tables: 'QATEST.srctb'
  ) OUTPUT TO testApp_Stream  ;

CREATE OR REPLACE TARGET testApp_Target USING BigQueryWriter  (
  ColumnDelimiter: '|', 
  NullMarker: 'NULL', 
  projectId:'striimqa-214712',
  Encoding: 'UTF-8', 
  BatchPolicy: 'eventCount:5,Interval:120',
  ServiceAccountKey: '/Users/gopinaths/Product/IntegrationTests/TestData/google-gcs.json', 
  AllowQuotedNewLines: 'false', 
  adapterName: 'BigQueryWriter', 
  optimizedMerge: 'true', 
  connectionRetryPolicy: 'retryInterval=30, maxRetries=10', 
  StandardSQL: 'true', 
  QuoteCharacter: '\"', 
  Tables: 'QATEST.srctb,.oratobqtgt',
  Mode: 'MERGE',
  StandardSQL: 'true',
  _h_TransportOptions:'connectionTimeout=30s, readTimeout=12s',
  QuoteCharacter: '\"'
  ) INPUT FROM testApp_Stream;

CREATE OR REPLACE TARGET testApp_SysOut USING Global.SysOut (name: 'wa') INPUT FROM testApp_Stream;

END APPLICATION testApp;
DEPLOY APPLICATION testApp;
START testApp;

stop APPLICATION MultiLogApp;
undeploy APPLICATION MultiLogApp;
drop APPLICATION MultiLogApp cascade;

CREATE APPLICATION MultiLogApp;

-- This sample application shows how Striim could be used monitor and correlate logs 
-- from web and application server logs from the same web application. See the discussion 
-- in the "Sample Applications" section of the Striim documentation for additional 
-- discussion.


CREATE FLOW MonitorLogs;

-- MonitorLogs sets up the two log sources used by this application. In a real-world
--implementation, each source could be reading many logs from many servers.

-- The web server logs are in Apache NCSA extended/ combined log format plus response time:
-- "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-agent}i\" %D"
-- See apache.org for more information.

CREATE SOURCE AccessLogSource USING FileReader (
  directory:'Samples/Customer/MultiLogApp/appData',
  wildcard:'access_log',
  blocksize: 10240,
  positionByEOF:false
)
PARSE USING DSVParser (
  columndelimiter:' ',
  ignoreemptycolumn:'Yes',
  quoteset:'[]~"',
  separator:'~',
  trimwhitespace:true
)
OUTPUT TO RawAccessStream;

CREATE TYPE AccessLogEntry (
    srcIp String KEY,
    userId String,
    sessionId String,
    accessTime DateTime,
    request String,
    code integer,
    size integer,
    referrer String,
    userAgent String,
    responseTime integer
);
CREATE STREAM AccessStream OF AccessLogEntry;

CREATE CQ ParseAccessLog 
INSERT INTO AccessStream
SELECT data[0], data[2], MATCH(data[4], ".*jsessionId=(.*) "),
       TO_DATE(data[3], "dd/MMM/yyyy:HH:mm:ss.SSS Z"), data[4], TO_INT(data[5]), TO_INT(data[6]),
       data[7], data[8], TO_INT(data[9])
FROM RawAccessStream;

-- The application server logs are in Apache's Log4J format.

CREATE SOURCE Log4JSource USING FileReader (
  directory:'Samples/Customer/MultiLogApp/appData',
  wildcard:'log4jLog.xml',
  positionByEOF:false
) 
PARSE USING XMLParser(
  rootnode:'/log4j:event',
  columnlist:'log4j:event/@timestamp,log4j:event/@level,log4j:event/log4j:message,log4j:event/log4j:throwable,log4j:event/log4j:locationInfo/@class,log4j:event/log4j:locationInfo/@method,log4j:event/log4j:locationInfo/@file,log4j:event/log4j:locationInfo/@line'
)
OUTPUT TO RawXMLStream;

CREATE TYPE Log4JEntry (
  logTime DateTime,
  level String,
  message String,
  api String,
  sessionId String,
  userId String,
  sobject String,
  xception String,
  className String,
  method String,
  fileName String,
  lineNum String
);
CREATE STREAM Log4JStream OF Log4JEntry;

CREATE CQ ParseLog4J
INSERT INTO Log4JStream
SELECT TO_DATE(TO_LONG(data[0])), data[1], data[2], 
       MATCH(data[2], '\\\\[api=([a-zA-Z0-9]*)\\\\]'),
       MATCH(data[2], '\\\\[session=([a-zA-Z0-9\\-]*)\\\\]'),
       MATCH(data[2], '\\\\[user=([a-zA-Z0-9\\-]*)\\\\]'),
       MATCH(data[2], '\\\\[sobject=([a-zA-Z0-9]*)\\\\]'),
       data[3], data[4], data[5], data[6], data[7]
FROM RawXMLStream;

END FLOW MonitorLogs;


CREATE FLOW ErrorsAndWarnings;

-- ErrorsAndWarnings creates a sliding window (Log4JErrorWarningActivity) containing 
-- the 300 most recent errors and warnings in the application server log. The 
-- ZeroContentCheck and LargeRTCheck flows join events from this window with access log 
-- events.

-- The type Log4JEntry was already defined by the MonitorLogs flow.
CREATE STREAM Log4ErrorWarningStream OF Log4JEntry;

CREATE CQ GetLog4JErrorWarning
INSERT INTO Log4ErrorWarningStream
SELECT l FROM Log4JStream l
WHERE l.level = 'ERROR' OR l.level = 'WARN';

CREATE WINDOW Log4JErrorWarningActivity 
OVER Log4ErrorWarningStream KEEP 300 ROWS;

END FLOW ErrorsAndWarnings;


-- HackerCheck sends an alert when an access log srcIp value is on a blacklist.

CREATE FLOW HackerCheck;

CREATE TYPE IPEntry (
    ip String
);

/* CREATE CACHE BlackListLookup using CSVReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'multiLogBlackList.txt',
  header: No,
  columndelimiter: ','
) QUERY (keytomap:'ip') OF IPEntry; */

CREATE CACHE BlackListLookup using FileReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'multiLogBlackList.txt'
)
PARSE USING DSVParser ( )
QUERY (keytomap:'ip') OF IPEntry;


CREATE STREAM HackerStream OF AccessLogEntry;

CREATE CQ FindHackers
INSERT INTO HackerStream
SELECT ale 
FROM AccessStream ale, BlackListLookup bll
WHERE ale.srcIp = bll.ip;

CREATE TYPE UnusualContext (
    typeOfActivity String,
    accessTime DateTime,
    accessSessionId String,
    srcIp String KEY,
    userId String,
    country String,
    city String,
    lat double,
    lon double
);
CREATE TYPE MergedEntry (
    accessTime DateTime,
    accessSessionId String,
    srcIp String KEY,
    userId String,
    request String,
    code integer,
    size integer,
    referrer String,
    userAgent String,
    responseTime integer,
    logTime DateTime,
    logSessionId String,
    level String,
    message String,
    api String,
    sobject String,
    xception String,
    className String,
    method String,
    fileName String,
    lineNum String
);
CREATE WACTIONSTORE UnusualActivity 
CONTEXT OF UnusualContext 
EVENT TYPES (MergedEntry, AccessLogEntry)
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE CQ GenerateHackerContext
INSERT INTO UnusualActivity
SELECT 'HackAttempt', accessTime, sessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM HackerStream
LINK SOURCE EVENT;

CREATE STREAM HackingAlertStream OF Global.AlertEvent;

CREATE CQ SendHackingAlerts 
INSERT INTO HackingAlertStream 
SELECT 'HackingAlert', ''+accessTime, 'warning', 'raise', 
        'Possible Hacking Attempt from ' + srcIp + ' in ' + IP_COUNTRY(srcIp)
FROM HackerStream; 

CREATE SUBSCRIPTION HackingAlertSub USING WebAlertAdapter( ) INPUT FROM HackingAlertStream;

END FLOW HackerCheck;


-- LargeRTCheck sends an alert when an access log responseTime value exceeds 2000 
-- microseconds.

CREATE FLOW LargeRTCheck;

CREATE STREAM LargeRTStream of AccessLogEntry;

CREATE CQ FindLargeRT
INSERT INTO LargeRTStream
SELECT ale
FROM AccessStream ale
WHERE ale.responseTime > 2000;

CREATE WINDOW LargeRTActivity 
OVER LargeRTStream KEEP 100 ROWS;

CREATE STREAM LargeRTAPIStream OF MergedEntry;

CREATE CQ MergeLargeRTAPI
INSERT INTO LargeRTAPIStream
SELECT lrt.accessTime, lrt.sessionId, lrt.srcIp, lrt.userId, lrt.request, 
       lrt.code, lrt.size, lrt.referrer, lrt.userAgent, lrt.responseTime,
       log4j.logTime, log4j.sessionId, log4j.level, log4j.message, log4j.api, log4j.sobject, log4j.xception, 
       log4j.className, log4j.method, log4j.fileName, log4j.lineNum
FROM LargeRTActivity lrt, Log4JErrorWarningActivity log4j
WHERE lrt.sessionId = log4j.sessionId
      AND lrt.accessTime = log4j.logTime;   

CREATE CQ GenerateLargeRTContext
INSERT INTO UnusualActivity
SELECT 'LargeResponseTime', accessTime, accessSessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM LargeRTAPIStream
LINK SOURCE EVENT;

CREATE STREAM LargeRTAlertStream OF Global.AlertEvent;

CREATE CQ SendLargeRTAlerts 
INSERT INTO LargeRTAlertStream 
SELECT 'LargeRTAlert', ''+accessTime, 'warning', 'raise', 
        'Long response time for call from ' + userId + ' api ' + api + ' message ' + message
FROM LargeRTAPIStream; 

CREATE SUBSCRIPTION LargeRTAlertSub USING WebAlertAdapter( ) INPUT FROM LargeRTAlertStream;

END FLOW LargeRTCheck;


-- ProxyCheck sends an alert when an access log srcIP value is on a list of suspicious 
-- proxies.

CREATE FLOW ProxyCheck;

/* CREATE CACHE ProxyLookup using CSVReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'multiLogProxies.txt',
  header: No,
  columndelimiter: ','
) QUERY (keytomap:'ip') OF IPEntry; */

CREATE CACHE ProxyLookup using FileReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'multiLogProxies.txt'
)
PARSE USING DSVParser ( )
QUERY (keytomap:'ip') OF IPEntry; 


CREATE STREAM ProxyStream OF AccessLogEntry;

CREATE CQ FindProxies
INSERT INTO ProxyStream
SELECT ale 
FROM AccessStream ale, ProxyLookup pl
WHERE ale.srcIp = pl.ip;

CREATE CQ GenerateProxyContext
INSERT INTO UnusualActivity
SELECT 'ProxyAccess', accessTime, sessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM ProxyStream
LINK SOURCE EVENT;


CREATE STREAM ProxyAlertStream OF Global.AlertEvent;

CREATE CQ SendProxyAlerts 
INSERT INTO ProxyAlertStream 
SELECT 'ProxyAlert', ''+accessTime, 'warning', 'raise', 
        'Possible use of Proxy from ' + srcIp + ' in ' + IP_COUNTRY(srcIp) + ' for user ' + userId 
FROM ProxyStream; 

CREATE SUBSCRIPTION ProxyAlertSub USING WebAlertAdapter( ) INPUT FROM ProxyAlertStream;

END FLOW ProxyCheck;


-- ZeroContentCheck sends an alert when an access log entry's code value is 200 (that is,
-- the HTTP request succeeded) but the size value is 0 (the return had no content).

CREATE FLOW ZeroContentCheck;

CREATE STREAM ZeroContentStream of AccessLogEntry;

CREATE CQ FindZeroContent
INSERT INTO ZeroContentStream
SELECT ale
FROM AccessStream ale
WHERE ale.code = 200 AND ale.size = 0;

CREATE WINDOW ZeroContentActivity 
OVER ZeroContentStream KEEP 100 ROWS;

CREATE STREAM ZeroContentAPIStream OF MergedEntry;

CREATE CQ MergeZeroContentAPI
INSERT INTO ZeroContentAPIStream
SELECT zcs.accessTime, zcs.sessionId, zcs.srcIp, zcs.userId, zcs.request, 
       zcs.code, zcs.size, zcs.referrer, zcs.userAgent, zcs.responseTime,
       log4j.logTime, log4j.sessionId, log4j.level, log4j.message, log4j.api, log4j.sobject, log4j.xception, 
       log4j.className, log4j.method, log4j.fileName, log4j.lineNum
FROM ZeroContentActivity zcs, Log4JErrorWarningActivity log4j
WHERE zcs.sessionId = log4j.sessionId
      AND zcs.accessTime = log4j.logTime;   

CREATE CQ GenerateZeroContentContext
INSERT INTO UnusualActivity
SELECT 'ZeroContent', accessTime, accessSessionId, srcIp, userId,
       IP_COUNTRY(srcIp), IP_CITY(srcIP), IP_LAT(srcIP), IP_LON(srcIP)
FROM ZeroContentAPIStream
LINK SOURCE EVENT;


CREATE TYPE ZeroContentEventListType (
    srcIp String KEY,
    code integer,
    size integer,
    level String,
    message String,
    xception String);
    
CREATE WACTIONSTORE ZeroContentEventList
CONTEXT OF ZeroContentEventListType 
EVENT TYPES (ZeroContentEventListType )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );


CREATE CQ GenerateZeroContentEventList
INSERT INTO ZeroContentEventList
SELECT srcIp, code, size, level, message, xception
FROM ZeroContentAPIStream;


CREATE STREAM ZeroContentAlertStream OF Global.AlertEvent;

CREATE CQ SendZeroContentAlerts 
INSERT INTO ZeroContentAlertStream 
SELECT 'ZeroContentAlert', ''+accessTime, 'warning', 'raise', 
        'Zero content returned in call from ' + userId + ' api ' + api + ' message ' + message
FROM ZeroContentAPIStream; 

CREATE SUBSCRIPTION ZeroContentAlertSub USING WebAlertAdapter( ) INPUT FROM ZeroContentAlertStream;

END FLOW ZeroContentCheck;


-- ErrorHandling is functionally identical to ErrorFlow.SaasMonitorApp. It sends an alert 
-- when an error message appears in the application server log.

CREATE FLOW ErrorHandling;

CREATE STREAM ErrorStream OF Log4JEntry;

CREATE CQ GetErrors 
INSERT INTO ErrorStream 
SELECT log4j 
FROM Log4ErrorWarningStream log4j WHERE log4j.level = 'ERROR';

CREATE STREAM ErrorAlertStream OF Global.AlertEvent;

CREATE CQ SendErrorAlerts 
INSERT INTO ErrorAlertStream 
SELECT 'ErrorAlert', ''+logTime, 'error', 'raise', 'Error in log ' + message 
FROM ErrorStream;

CREATE SUBSCRIPTION ErrorAlertSub USING WebAlertAdapter( ) INPUT FROM ErrorAlertStream;

END FLOW ErrorHandling;


-- WarningHandling is a minor variation on WarningFlow.SaasMonitorApp. It sends an alert 
-- once an hour with the count of warnings for each api call for which there has been at 
-- least one alert.

CREATE FLOW WarningHandling;

CREATE STREAM WarningStream OF Log4JEntry;

CREATE CQ GetWarnings 
INSERT INTO WarningStream 
SELECT log4j 
FROM Log4ErrorWarningStream log4j WHERE log4j.level = 'WARN';

CREATE JUMPING WINDOW WarningWindow 
OVER WarningStream KEEP WITHIN 60 MINUTE ON logTime;

CREATE STREAM WarningAlertStream OF Global.AlertEvent;

CREATE CQ SendWarningAlerts 
INSERT INTO WarningAlertStream 
SELECT 'WarningAlert', ''+logTime, 'warning', 'raise', 
        COUNT(logTime) + ' Warnings in log for api ' + api 
FROM WarningWindow 
GROUP BY api 
HAVING count(logTime) > 1;

CREATE SUBSCRIPTION WarningAlertSub USING WebAlertAdapter( ) INPUT FROM WarningAlertStream;

END FLOW WarningHandling;


-- InfoFlow is functionally similar to InfoFlow.SaasMonitorApp. Its output is used by 
-- ApiFlow, CompanyApiFlow, and UserApiFlow.

CREATE FLOW InfoFlow;

CREATE TYPE UserInfo (
  userId String, 
  userName String, 
  company String,  
  userZip String,  
  companyZip String
);

/* CREATE CACHE MLogUserLookup using CSVReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'multiLogUser.csv',
  header: No,
  columndelimiter: ','
) QUERY (keytomap:'userId') OF UserInfo; */

CREATE CACHE MLogUserLookup using FileReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'multiLogUser.csv'
)
PARSE USING DSVParser ( )
QUERY (keytomap:'userId') OF UserInfo;

CREATE STREAM InfoStream OF Log4JEntry;

CREATE CQ GetInfo 
INSERT INTO InfoStream 
SELECT log4j 
FROM Log4JStream log4j WHERE log4j.level = 'INFO';

CREATE TYPE ApiCall (
  userId String, 
  api String, 
  sobject String, 
  logTime DateTime, 
  userName String, 
  company String,  
  userZip String,  
  companyZip String
);
CREATE STREAM ApiEnrichedStream OF ApiCall;

CREATE CQ GetUserDetails 
INSERT INTO ApiEnrichedStream 
SELECT a.userId, a.api, a.sobject, a.logTime, u.userName, u.company, u.userZip, u.companyZip 
FROM InfoStream a, MLogUserLookup u 
WHERE a.userId = u.userId;

END FLOW InfoFlow;


-- ApiFlow populates the dashboard's Detail - ApiActivity page and the pie chart on the 
-- Overview page.

CREATE FLOW ApiFlow;

CREATE TYPE ApiUsage (
  api String key, 
  sobject String, 
  count integer, 
  logTime DateTime
);

CREATE TYPE ApiContext (
  api String key, 
  count integer, 
  logTime DateTime
);

CREATE WACTIONSTORE ApiActivity 
CONTEXT OF ApiContext 
EVENT TYPES (ApiUsage )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE JUMPING WINDOW ApiWindow 
OVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime 
PARTITION BY api;

CREATE STREAM ApiUsageStream OF ApiUsage;

CREATE CQ GetApiUsage 
INSERT INTO ApiUsageStream 
SELECT a.api, a.sobject, 
       COUNT(a.userId), FIRST(a.logTime) 
FROM ApiWindow a 
GROUP BY a.api, a.sobject HAVING FIRST(a.logTime) IS NOT NULL;

CREATE JUMPING WINDOW ApiSummaryWindow 
OVER ApiUsageStream KEEP WITHIN 1 HOUR ON logTime 
PARTITION BY api;

CREATE CQ GetApiSummaryUsage 
INSERT INTO ApiActivity 
SELECT a.api,  
       sum(a.count), first(a.logTime)
FROM ApiSummaryWindow a 
GROUP BY a.api
LINK SOURCE EVENT;

END FLOW ApiFlow;


-- CompanyApiFlow populates the dashboard's Detail - CompanyApiActivity page and the bar 
-- chart on the Overview page. It also sends an alert when an API call is used by a 
-- company more than 1500 times during the flow's one-hour jumping window.

CREATE FLOW CompanyApiFlow;

CREATE TYPE CompanyApiUsage (
  company String key, 
  companyZip String, 
  companyLat double, 
  companyLong double, 
  api String, 
  count integer, 
  unusual integer,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE TYPE CompanyApiContext (
  company String key, 
  companyZip String, 
  companyLat double, 
  companyLong double, 
  count integer, 
  unusual integer,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE WACTIONSTORE CompanyApiActivity 
CONTEXT OF CompanyApiContext 
EVENT TYPES (CompanyApiUsage )
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE JUMPING WINDOW CompanyApiWindow 
OVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime 
PARTITION BY company;

CREATE STREAM CompanyApiUsageStream OF CompanyApiUsage;

CREATE TYPE MLogUSAddressData(
  country String,
  zip String KEY,
  city String,
  state String,
  stateCode String,
  fullCity String,
  someNum String,
  pad String,
  latVal double,
  longVal double,
  empty String,
  empty2 String
);

/*CREATE CACHE MLogZipLookup using CSVReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'USAddresses.txt',
  header: Yes,
  columndelimiter: ','
) QUERY (keytomap:'zip') OF MLogUSAddressData; */

CREATE CACHE MLogZipLookup using FileReader (
  directory: 'Samples/Customer/MultiLogApp/appData',
  wildcard: 'USAddresses.txt'
)
PARSE USING DSVParser (
  header: Yes
)
QUERY (keytomap:'zip') OF MLogUSAddressData;


CREATE CQ GetCompanyApiUsage 
INSERT INTO CompanyApiUsageStream 
SELECT a.company, a.companyZip, z.latVal, z.longVal, 
       a.api, COUNT(a.sobject), 
       CASE WHEN COUNT(a.sobject) > 1500 THEN 1
            ELSE 0 END,
       CASE WHEN COUNT(a.sobject) > 1500 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.sobject),
       FIRST(a.logTime) 
FROM CompanyApiWindow a, MLogZipLookup z 
WHERE a.companyZip = z.zip 
GROUP BY a.company, a.api HAVING FIRST(a.logTime) IS NOT NULL;

CREATE JUMPING WINDOW CompanyWindow 
OVER CompanyApiUsageStream KEEP WITHIN 1 HOUR ON logTime 
PARTITION BY company;

CREATE CQ GetCompanyUsage 
INSERT INTO CompanyApiActivity 
SELECT a.company, a.companyZip, a.companyLat, a.companyLong, 
       SUM(a.count), SUM(a.unusual), 
       CASE WHEN SUM(a.unusual) > 0 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.topObject),
       FIRST(a.logTime) 
FROM CompanyWindow a 
GROUP BY a.company
LINK SOURCE EVENT;

CREATE STREAM CompanyAlertStream OF Global.AlertEvent;

CREATE CQ SendCompanyApiAlerts 
INSERT INTO CompanyAlertStream 
SELECT 'CompanyAPIAlert', ''+logTime, 'warning', 'raise', 
       'Company ' + company + ' has used api ' + api + ' ' + count + ' times for ' + topObject 
FROM CompanyApiUsageStream 
WHERE unusual = 1;

CREATE SUBSCRIPTION CompanyAlertSub USING WebAlertAdapter( ) INPUT FROM CompanyAlertStream;

END FLOW CompanyApiFlow;


-- UserApiFlow populates the dashboard's Detail - UserApiActivity page and the US map on 
-- the Overview page. It also sends an alert when an API call is used by a user more than 
-- 125 times during the flow's one-hour window.

CREATE FLOW UserApiFlow;

CREATE TYPE UserApiUsage (
  userId String key, 
  userName String, 
  userZip String, 
  userLat double, 
  userLong double, 
  company String, 
  api String, 
  count integer, 
  unusual integer,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE TYPE UserApiContext (
  userId String key, 
  userName String, 
  userZip String, 
  userLat double, 
  userLong double, 
  company String, 
  count integer, 
  unusual integer,
  Category String,
  topObject String,
  logTime DateTime
);

CREATE WACTIONSTORE UserApiActivity
CONTEXT OF UserApiContext 
EVENT TYPES (  UserApiUsage ) 
PERSIST IMMEDIATE USING ( storageProvider:'elasticsearch' );

CREATE JUMPING WINDOW UserApiWindow 
OVER ApiEnrichedStream KEEP WITHIN 1 HOUR ON logTime
PARTITION BY userId;

CREATE STREAM UserApiUsageStream OF UserApiUsage;

CREATE CQ GetUserApiUsage 
INSERT INTO UserApiUsageStream 
SELECT a.userId, a.userName, a.userZip, z.latVal, z.longVal, a.company,
       a.api, COUNT(a.sobject), 
       CASE WHEN COUNT(a.sobject) > 125 THEN 1
            ELSE 0 END,
       CASE WHEN COUNT(a.sobject) > 125 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.sobject),
       FIRST(a.logTime) 
FROM UserApiWindow a, MLogZipLookup z 
WHERE a.userZip = z.zip 
GROUP BY a.userId, a.api HAVING FIRST(a.logTime) IS NOT NULL;

CREATE JUMPING WINDOW UserWindow 
OVER UserApiUsageStream KEEP WITHIN 1 HOUR ON logTime 
PARTITION BY userId;

CREATE CQ GetUserUsage 
INSERT INTO UserApiActivity 
SELECT a.userId, a.userName, a.userZip, a.userLat, a.userLong, 
       a.company, SUM(a.count), SUM(a.unusual), 
       CASE WHEN SUM(a.unusual) > 0 THEN 'UNUSUAL'
            ELSE 'OK' END,
       MAXOCCURS(a.topObject),
       FIRST(a.logTime) 
FROM UserWindow a 
GROUP BY a.userId
LINK SOURCE EVENT;

CREATE STREAM UserAlertStream OF Global.AlertEvent;

CREATE CQ SendUserApiAlerts 
INSERT INTO UserAlertStream 
SELECT 'UserAPIAlert', ''+logTime, 'warning', 'raise', 
       'User ' + userName + ' has used api ' + api + ' ' + count + ' times for ' + topObject 
FROM UserApiUsageStream 
WHERE unusual = 1;

CREATE SUBSCRIPTION UserAlertSub USING WebAlertAdapter( ) INPUT FROM UserAlertStream;

END FLOW UserApiFlow;

END APPLICATION MultiLogApp;

Deploy application MultiLogApp;
Start application MultiLogApp;

STOP @APPNAME@;
UNDEPLOY APPLICATION @APPNAME@;
DROP APPLICATION @APPNAME@ CASCADE;

CREATE APPLICATION @APPNAME@ @Recovery@;
--create flow AgentFlow;
CREATE SOURCE @APPNAME@_S USING OracleReader
(
	Username: 'qatest',
	Password: 'qatest',
	ConnectionURL: 'dockerhost:1521:xe',
	Tables: 'qatest.test01',
	FetchSize: '1',
	connectionRetryPolicy:'timeOut=00,retryInterval=1,maxRetries=3'
)
OUTPUT TO @APPNAME@_SS;
--end flow AgentFlow;
--create flow serverFlow;
--create target sys_out using sysout (name:sysoutput) input from ss;

CREATE or replace @APPNAME@_TARGET T USING BigQueryWriter (
AllowQuotedNewlines:False,
ConnectionRetryPolicy:'retryInterval=30,maxRetries=3',
serviceAccountKey: '/Users/saranyad/Documents/bigquerywritertest-5354834e7631.json',
Encoding:'UTF-8',
projectId: 'bigquerywritertest',
Tables:'qatest.test01,@DATASET@.test01 KEYCOLUMNS(id) COLUMNMAP(id=id,col1=col11,col2=col12,col3=@metadata(OperationName))',
Mode:'merge',
datalocation: 'US',
nullmarker: 'defaultNULL',
columnDelimiter: '|',
BatchPolicy: 'Eventcount:1000,Interval:30',
StandardSQL:true,
optimizedMerge:true		
) INPUT FROM @APPNAME@_ss;
--end flow serverFlow;
END APPLICATION @APPNAME@;
DEPLOY APPLICATION @APPNAME@;
--deploy application @APPNAME@ with AgentFlow in Agents, ServerFlow in default;
START APPLICATION @APPNAME@;

Stop Teradata_LogWriter;
Undeploy application Teradata_LogWriter;
drop application Teradata_LogWriter cascade;

CREATE APPLICATION Teradata_LogWriter recovery 5 second interval;

CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.TDSOURCE',
  adapterName: 'IncrementalBatchReader',
  CheckColumn: 'striim.TEST01=ID;',
  PollingInterval: '5sec',
  ReturnDateTimeAs: 'String',
  startPosition:'striim.test01=0'
  )
  OUTPUT TO data_stream;

  CREATE OR REPLACE TARGET TeraSys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

CREATE TARGET BinaryDump USING LogWriter(
  name: 'TeraData',
  filename:'TeraData.log',
  flushpolicy:'EventCount:100,Interval:30s'
)INPUT FROM data_stream;

END APPLICATION Teradata_LogWriter;

deploy application Teradata_LogWriter in default;

start application Teradata_LogWriter;

STOP APPLICATION @WRITERAPPNAME@;
UNDEPLOY APPLICATION @WRITERAPPNAME@;
DROP APPLICATION @WRITERAPPNAME@ CASCADE;
create or replace PROPERTYVARIABLE SRC_PASSWORD='@SOURCE_PASS@';
CREATE APPLICATION @WRITERAPPNAME@ RECOVERY 10 SECOND INTERVAL USE EXCEPTIONSTORE;

CREATE SOURCE @SOURCE@ USING OracleReader
(
FetchSize:1,
Username:'@SOURCE_USER@',
Password:'$SRC_PASSWORD',
ConnectionURL:'@CONNECTION_URL@',
Tables:'@SOURCE_TABLE@',
password_encrypted: 'true'
)
OUTPUT TO @STREAM1@;

CREATE OR REPLACE TYPE @TYPE@( 
datae java.util.HashMap , 
TABLE_NAME java.lang.String , 
OPS_NAME java.lang.String , 
DB_TIMESTAMP java.lang.String  ,
COMMITSCN java.lang.String ,
SCN java.lang.String ,
REC_INS_TIME java.lang.String );

CREATE CQ @CQ1@
INSERT INTO @STREAM2@
SELECT  
CASE WHEN (META(c,"OperationName").toString() == "DELETE")
THEN putUserData(c, 'isDelete', 'true') 
ELSE
putUserData(c,'isDelete', 'false')
END
FROM @STREAM1@ c;

CREATE STREAM @STREAM3@ OF @TYPE@ PARTITION BY TABLE_NAME;

CREATE OR REPLACE CQ @CQ2@ 
INSERT INTO @STREAM3@
SELECT 
data(e),
META(e,"TableName").toString() as TABLE_NAME,
META(e, "OperationName").toString() as OPS_NAME,
META(e, "TimeStamp").toString() as DB_TIMESTAMP,
META(e,"COMMITSCN").toString() as COMMITSCN ,
META(e,"SCN").toString() as  SCN ,
DNOW().toString() as REC_INS_TIME
FROM @STREAM1@ e;

create Target @TARGET1@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC1',
ParallelThreads:'',
PartitionKey:'@metadata(TableName)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING dsvFormatter ()
input from @STREAM1@;


create Target @TARGET2@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC2',
ParallelThreads:'2',
PartitionKey:'TABLE_NAME',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING jsonFormatter ()
input from @STREAM3@;

create Target @TARGET3@ using KafkaWriter VERSION @kafkaAdpVersion@ (
brokerAddress:'localhost:9099',
Topic:'@WRITERAPPNAME@_TOPIC3',
ParallelThreads:'',
PartitionKey:'@userdata(isDelete)',
KafkaConfig: 'request.timeout.ms=60001;session.timeout.ms=60000;batch.size=2000000;max.request.size=2000000;')
FORMAT USING avroFormatter (
schemaFileName:'KafkaAvroTest.avsc')
input from @STREAM2@;

end application @WRITERAPPNAME@;
deploy application @WRITERAPPNAME@;
start @WRITERAPPNAME@;
stop application @READERAPPNAME@;
undeploy application @READERAPPNAME@;
drop application @READERAPPNAME@ cascade;
CREATE APPLICATION @READERAPPNAME@ RECOVERY 1 SECOND INTERVAL;


CREATE SOURCE @SOURCE_DSV@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC1',
        startOffset:0       
)
PARSE USING DSVParser (
)
OUTPUT TO KafkaReaderStream1;


CREATE TARGET kafkaDumpDSV USING FileWriter(
name:kafkaOuputDSV,
filename:'@READERAPPNAME@_RT_DSV')
FORMAT USING DSVFormatter()
INPUT FROM KafkaReaderStream1;

CREATE SOURCE @SOURCE_JSON@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC2',
        startOffset:0          
)
PARSE USING JSONParser (
	eventType:''
)
OUTPUT TO KafkaReaderStream2;


CREATE SOURCE @SOURCE_AVRO@ USING KafkaReader VERSION @kafkaAdpVersion@ (
        brokerAddress:'localhost:9099',
        Topic:'@WRITERAPPNAME@_TOPIC3',
        startOffset:0          
)
PARSE USING AvroParser (
	schemaFileName:'KafkaAvroTest.avsc'
)
OUTPUT TO KafkaReaderStream3;


end application @READERAPPNAME@;

STOP application RollOverTester.DSV;
undeploy application RollOverTester.DSV;
drop application RollOverTester.DSV cascade;

create application DSV;
create source CSVSource using FileReader (
  directory:'@TEST-DATA-PATH@',
  WildCard:'smallposdata.csv',
  positionByEOF:false,
  charset:'UTF-8'
)
parse using DSVParser (
  header:'yes'
)
OUTPUT TO CsvStream;

Create Type CSVType (
  merchantName String,
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);

Create Stream TypedCSVStream of CSVType;

CREATE CQ CsvToPosData
INSERT INTO TypedCSVStream
SELECT data[0],data[1],
       TO_DATEF(data[4],'yyyyMMddHHmmss'),
       DHOURS(TO_DATEF(data[4],'yyyyMMddHHmmss')),
       TO_DOUBLE(data[7]),
       data[9]
FROM CsvStream;

create Target t using FileWriter(
  filename:'JsonTargetTI',
  directory:'@FEATURE-DIR@/logs/',
  sequence:'00',
  rolloverpolicy:'TimeIntervalRollingPolicy,rotationinterval:10-89s'
)
format using JSONFormatter (
  members:'merchantname,merchantid,dateTime,hourValue,amount,zip'
)

input from TypedCSVStream;

create Target x using LogWriter(name:DSVTarget,filename:'@FEATURE-DIR@/logs/TargetJsonTIAddition_actual.log') input from TypedCSVStream;

end application DSV;

stop application @appname@;
undeploy application @appname@;
drop application @appname@ cascade;

CREATE APPLICATION @appname@;

CREATE SOURCE @kafkasrc@ USING KafkaReader VERSION @KAFKAVERSION@ (
  brokerAddress: '',
  Topic: '',
  startOffset: '0' )
PARSE USING AvroParser (
  schemaregistryurl: 'http://localhost:8081/' )
OUTPUT TO @appname@Stream2;

CREATE TYPE @appname@ElementsOfNativeRecord1 (
 datarecord com.fasterxml.jackson.databind.JsonNode,
 before com.fasterxml.jackson.databind.JsonNode,
 metadata com.fasterxml.jackson.databind.JsonNode,
 userdata com.fasterxml.jackson.databind.JsonNode,
 datapresenceinfo com.fasterxml.jackson.databind.JsonNode,
 beforepresenceinfo com.fasterxml.jackson.databind.JsonNode);

CREATE TYPE @appname@completeRecord1 (
 completedata com.fasterxml.jackson.databind.JsonNode);

CREATE STREAM @appname@NativeRecordStream1 OF @appname@ElementsOfNativeRecord1;

CREATE STREAM @appname@CompleteRecordInJSONStream1 OF @appname@completeRecord1;

CREATE TARGET @filetarget@ USING FileWriter (
  filename: 'kafkaout',
  directory: ''
  rolloverpolicy: 'EventCount:10' )
FORMAT USING JSONFormatter  (
  members: 'datarecord' )
INPUT FROM @appname@NativeRecordStream1;

CREATE CQ @appname@GetNativeRecordInJSONCQ1
INSERT INTO @appname@NativeRecordStream1
SELECT
 completedata.get("data"),
 completedata.get("before"),
 completedata.get("metadata"),
 completedata.get("userdata"),
 completedata.get("datapresenceinfo"),
 completedata.get("beforepresenceinfo")
FROM @appname@CompleteRecordInJSONStream1;

CREATE CQ @appname@CQ2
INSERT INTO @appname@CompleteRecordInJSONStream1
SELECT
 AvroToJson(y.data)
 from @appname@Stream2 y;

END APPLICATION @appname@;
deploy application @appname@ on all in default;
start application @appname@;

stop application @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;


create application @APPNAME@;

CREATE OR REPLACE SOURCE CCBReader USING FileReader (
  wildcard: '@WILDCARD@',
  positionbyeof: false,
  directory: '@TESTDIR@'
  )
PARSE USING Global.CobolCopybookParser (
  copybookDialect: 'Mainframe',
  DataHandlingScheme: '@DATAHANDLINGSCHEMA@',
  GroupPolicy: '@GRPPOLICY@',
  TimeoutPolicy: '1s',
  dataFileOrganization: 'Text',
  ProcessCopyBookFileAs: 'MultipleEvents',
  skipIndent: 0,
  dataFileFont: 'utf8',
  CopybookFileFormat: 'USE_STANDARD_COLUMNS',
  copybookSplit: 'None',
  copybookFileName: '@CCBFILE@'
   )
OUTPUT TO CCBStream;

CREATE OR REPLACE TARGET JSONWriter USING FileWriter (
  filename: '%@metadata(FileName)%',
  directory: '@DIR@',
  rolloverpolicy: 'EventCount:10000,Interval:30s',
  flushpolicy: 'EventCount:1,Interval:30s'
  )
FORMAT USING JSONFormatter  ()
INPUT FROM CCBStream;

end application @APPNAME@;
deploy application @APPNAME@ on all in default;
start application @APPNAME@;

-- stop application Metadata_Application;
-- undeploy application Metadata_Application;
drop application Metadata_Application force;
CREATE APPLICATION Metadata_Application;

CREATE OR REPLACE TYPE app1_AccessSource_Type (
 timestamp org.joda.time.DateTime,
 src_ip java.lang.String,
 dest_ip java.lang.String,
 http_code java.lang.String);


CREATE OR REPLACE SOURCE app1_AccessSource USING ContinuousGenerator ( 
  OutputType: 'admin.TYPE.app1_AccessSource_Type', 
  Throughput: 'Unrestricted' ) 
OUTPUT TO app1_OutputStream;


CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

CREATE OR REPLACE CQ app1_Query 
INSERT INTO app1_AccessStream 
SELECT DNOW() as Time,
       TO_STRING(data[1]) as Source_IP,
       TO_STRING(data[2]) as Destination_IP,
       TO_STRING(data[3]) as HTTP_CODE
FROM app1_OutputStream;

END APPLICATION Metadata_Application;



-- stop application app2;
-- undeploy application Metadata_Application2;
drop application Metadata_Application2 force;

CREATE APPLICATION Metadata_Application2;

CREATE OR REPLACE CQ app2_Query 
INSERT INTO app2_AccessStream 
SELECT *
FROM app1_AccessStream;

END APPLICATION Metadata_Application2;

STOP DSLAPP;
UNDEPLOY APPLICATION DSLAPP;
DROP APPLICATION DSLAPP CASCADE;

CREATE APPLICATION DSLAPP;

-- CacheWaction WACTIONSTORE is being loaded from DSCache

CREATE OR REPLACE WACTIONSTORE CacheWactionDSL CONTEXT OF DS.T1
EVENT TYPES ( DS.T1 )
@PERSIST-TYPE@

CREATE CQ DSLDerby
INSERT INTO CacheWactionDSL
select * from DS.C1
LINK SOURCE EVENT;

END APPLICATION DSLAPP;
DEPLOY APPLICATION DSLAPP;
START APPLICATION DSLAPP;

CREATE CQ @CQ_NAME@
INSERT INTO @EMB_STREAM@
@SELECT_QUERY@
FROM @STREAM@ e;

CREATE TARGET @CQ_NAME@_sysout USING Global.SysOut (
  name: '@CQ_NAME@_sysout' )
INPUT FROM @EMB_STREAM@;

stop IR;
undeploy application IR;
drop application IR cascade;
CREATE APPLICATION IR;

 CREATE OR REPLACE SOURCE Teradata_source USING IncrementalBatchReader  ( 
  FetchSize: 100,
  Username: 'striim',
  Password: 'striim',
  ConnectionURL: 'jdbc:teradata://10.55.11.5/DBS_PORT=1025,DATABASE=striim',
  Tables: 'striim.test01;striim.test02;striim.test03',
  adapterName: 'IncrementalBatchReader',
 CheckColumn: 'striim.test01=id;striim.test02=t2;striim.test03=t1',
 startPosition: 'striim.test01=1;striim.test02=-1;%=0',
  PollingInterval: '5sec'
  )
  OUTPUT TO data_stream ;

  CREATE OR REPLACE TARGET sys USING SysOut  ( 
  name: 'ora12_out'
 ) INPUT FROM data_stream;

create target AzureSQLDWHTarget using AzureSQLDWHWriter(
		ConnectionURL: 'jdbc:sqlserver://qatestazuresqldwh.database.windows.net:1433;database=QA-DBW',
        username: 'striim',
        password: 'W3b@ct10n',
        accountname: 'striimqatestdonotdelete',
        AccountAccessKey: '/mqWfx9/wW3M0qJPJfoAStzAGNg9RjMXTVy6whL6VJltXABl4+xl+DuozpMxcuAi4XxV1b3lLwBHf+j/1WN0dg==',
        Tables: 'striim.test01,dbo.test1;striim.test02,dbo.test2;striim.test03,dbo.test3;',
        uploadpolicy:'eventcount:1,interval:10s'
) INPUT FROM data_stream;


CREATE TARGET log USING LOGWRITER
(
 name:log,
 filename:'LOGFILENAME'
)
INPUT FROM data_stream;


END APPLICATION IR;

deploy application IR;
start IR;

STOP APPLICATION DBRTOCW;
UNDEPLOY APPLICATION DBRTOCW;
DROP APPLICATION DBRTOCW CASCADE;
CREATE APPLICATION DBRTOCW;

create source CSVSource using FileReader (
	directory:'/Users/jenniffer/Product2/IntegrationTests/TestData/',
	WildCard:'posdata.csv',
	positionByEOF:false,
	charset:'UTF-8'
)
parse using DSVParser (
	
)
OUTPUT TO FileStream
(
id String,
ename String
)
select 
data[2],
data[0];

create Target t2 using SysOut(name:OrgData) input from FileStream;
CREATE OR REPLACE Target DBTarget USING DatabaseWriter  ( 
  DatabaseProviderType: 'Default',
  PreserveSourceTransactionBoundary: 'false',
  Username: 'cassandra',
  BatchPolicy: 'EventCount:1000,Interval:60',
  CommitPolicy: 'EventCount:1000,Interval:60',
  ConnectionURL: 'jdbc:cassandra://127.0.0.1:9042/test',
  Tables: 'test.employee',
  Password: 'cassandra',
  Password_encrypted: false
 )INPUT FROM FileStream;

END APPLICATION DBRTOCW;
DEPLOY APPLICATION DBRTOCW;
START APPLICATION DBRTOCW;

CREATE OR REPLACE SOURCE @SOURCE_NAME@ USING FileReader  (
  blocksize: 64,
  positionbyeof: false,
  rolloverstyle: 'Default',
  includesubdirectories: false,
  adapterName: 'FileReader',
  directory: '/Users/jenniffer/Downloads',
  skipbom: true,
  wildcard: 'dk000000000'
 )
 PARSE USING GGTrailParser  (
  handler: 'com.webaction.proc.GGTrailParser_1_0',
  metadata: '@METADATA@',
  FilterTransactionBoundaries: true,
  TrailByteOrder: '@BYTEORDDER@',
  Tables: '@TABLES@',
  parserName: 'GGTrailParser',
  _h_ReturnDateTimeAs: '@DATETIME@',
  Compression:'@COMPRESSION@'
 )OUTPUT to @STREAM@;

CREATE TYPE PosData(
  merchantId String,
  dateTime DateTime,
  hourValue int,
  amount double,
  zip String
);
CREATE STREAM PosDataStream OF PosData PARTITION BY merchantId;

stop @APPNAME@;
undeploy application @APPNAME@;
drop application @APPNAME@ cascade;
CREATE APPLICATION @APPNAME@ recovery 5 second interval;

Create Source @SourceName@ Using MySQLReader
(
 Username:'@UserName@',
 Password:'@Password@',
 ConnectionURL:'@SourceConnectionURL@',
 Tables:'qatest.@SourceTable@',
 Fetchsize:1
)
Output To @SRCINPUTSTREAM@;

CREATE OR REPLACE TARGET @targetName@ USING DatabaseWriter(
ConnectionURL:'@TargetConnectionURL@',
  Username:'@UserName@',
  Password:'@Password@',
  BatchPolicy:'EventCount:1,Interval:1',
 Tables: 'qatest.@SourceTable@,qatest.@TargetTable@'
) INPUT FROM @SRCINPUTSTREAM@;

create Target @targetsys@ using SysOut(name:Foo2) input from @SRCINPUTSTREAM@;

END APPLICATION @APPNAME@;
deploy application @APPNAME@;
start @APPNAME@;